{"task_id": 1, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the dot product of a matrix and a vector. The dot product of a matrix and a vector is a vector where each element is the sum of the products of corresponding elements from each row of the matrix and the vector. If the dimensions of the matrix and vector are incompatible, we return -1.\n\n### Approach\n1. **Check for Empty Matrix**: If the matrix is empty, we check if the vector is also empty. If both are empty, we return an empty list. If the matrix is empty but the vector is not, we return -1.\n2. **Determine Number of Columns**: For a non-empty matrix, the number of columns is determined by the length of the first row.\n3. **Dimension Compatibility Check**: We check if the number of columns in the matrix matches the length of the vector. If they do not match, we return -1.\n4. **Compute Dot Product**: For each row in the matrix, compute the dot product with the vector by summing the products of corresponding elements. Append each result to the resulting vector.\n\n### Solution Code\n```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # Determine the number of columns in the matrix\n    if not a:\n        num_cols = 0\n    else:\n        num_cols = len(a[0])\n    \n    # Check if the vector's length matches the number of columns\n    if len(b) != num_cols:\n        return -1\n    \n    # Compute the dot product for each row\n    result = []\n    for row in a:\n        dot_product = 0\n        for i in range(num_cols):\n            dot_product += row[i] * b[i]\n        result.append(dot_product)\n    \n    return result\n```\n\n### Explanation\n- **Empty Matrix Handling**: The function first checks if the matrix is empty. If it is, it checks the vector's length and returns an appropriate result.\n- **Column Count**: The number of columns is determined from the first row of the matrix if it is non-empty.\n- **Dimension Check**: The function ensures the matrix columns and vector length match, returning -1 if they do not.\n- **Dot Product Calculation**: For each row in the matrix, the function computes the dot product with the vector by iterating through each element, multiplying corresponding elements, and summing the results.\n\nThis approach efficiently handles edge cases and ensures correct computation of the dot product while validating input dimensions."}
{"task_id": 2, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the transpose of a given matrix. The transpose of a matrix is formed by converting its rows into columns and columns into rows. This means that the element at position [i][j] in the original matrix will be at position [j][i] in the transposed matrix.\n\n### Approach\nThe approach to solve this problem involves using Python's built-in `zip` function, which is well-suited for transposing matrices. The `zip` function takes iterables, aggregates them into tuples, and returns an iterator of these tuples. When applied to the rows of a matrix, `zip` effectively groups elements from each column into tuples, which can then be converted into lists to form the transposed matrix.\n\nHere's a step-by-step breakdown of the approach:\n1. **Use `zip(*a)`**: This expression unpacks the matrix `a` into its rows and then zips these rows together. The result is an iterator of tuples where each tuple represents a column from the original matrix.\n2. **Convert Tuples to Lists**: Each tuple returned by `zip` is converted into a list to form the rows of the transposed matrix.\n3. **Return the Result**: The resulting list of lists is returned as the transposed matrix.\n\nThis approach efficiently handles both square and rectangular matrices, ensuring that the transpose is computed correctly regardless of the matrix dimensions.\n\n### Solution Code\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    return [list(row) for row in zip(*a)]\n```\n\n### Explanation\n- **zip(*a)**: This expression unpacks the matrix `a` into its individual rows and then zips these rows together. For example, if `a` is `[[1, 2, 3], [4, 5, 6]]`, `zip(*a)` will produce tuples `(1, 4)`, `(2, 5)`, and `(3, 6)`.\n- **List Comprehension**: The list comprehension `[list(row) for row in zip(*a)]` converts each tuple returned by `zip` into a list, resulting in the transposed matrix.\n- **Efficiency**: This method is efficient and concise, leveraging Python's built-in functions to achieve the desired result with minimal code.\n\nThis solution is both elegant and efficient, making it well-suited for transposing matrices of any size."}
{"task_id": 3, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that reshapes a given matrix into a specified shape. If the reshaping is not possible, the function should return an empty list.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Convert the Input Matrix to a Numpy Array**: This allows us to use numpy's built-in functions for reshaping.\n2. **Calculate the Total Number of Elements**: Determine the total number of elements in the original matrix and the target shape.\n3. **Check Reshaping Feasibility**: Compare the total number of elements in the original matrix with the product of the dimensions in the new shape. If they do not match, reshaping is not possible.\n4. **Reshape the Matrix**: If reshaping is feasible, use numpy's reshape function to change the matrix's shape.\n5. **Convert Back to List of Lists**: After reshaping, convert the numpy array back to a list of lists using numpy's tolist() method.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Convert the input matrix to a numpy array\n    arr = np.array(a)\n    # Calculate the total number of elements in the original matrix\n    original_size = arr.size\n    # Calculate the required number of elements for the new shape\n    required_size = new_shape[0] * new_shape[1]\n    # Check if reshaping is possible\n    if original_size != required_size:\n        return []\n    # Reshape the numpy array\n    reshaped_arr = arr.reshape(new_shape)\n    # Convert the reshaped array back to a list of lists\n    return reshaped_arr.tolist()\n```\n\n### Explanation\n1. **Conversion to Numpy Array**: The input matrix is converted to a numpy array to leverage numpy's efficient array operations.\n2. **Element Count Check**: The total number of elements in the original matrix is compared with the product of the dimensions in the new shape. This ensures that reshaping is possible without losing or adding elements.\n3. **Reshaping**: If the element counts match, the matrix is reshaped using numpy's reshape function.\n4. **Conversion Back to List**: The reshaped numpy array is converted back to a list of lists to meet the problem's output requirements.\n\nThis approach efficiently handles the reshaping of matrices and ensures that only valid reshapes are performed, returning an empty list when reshaping is not possible."}
{"task_id": 4, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that calculates the mean of a matrix either by row or by column based on a specified mode. The function will take a matrix (a list of lists of floats) and a mode ('row' or 'column') as input and return a list of means according to the specified mode.\n\n### Approach\n1. **Check for Empty Matrix**: If the input matrix is empty, the function should return an empty list immediately.\n2. **Row Mean Calculation**: If the mode is 'row', iterate through each row of the matrix, compute the mean of each row by summing the elements and dividing by the number of elements, and collect these means in a result list.\n3. **Column Mean Calculation**: If the mode is 'column', determine the number of columns from the first row of the matrix. For each column index, sum the elements from each row at that index, compute the mean by dividing the sum by the number of rows, and collect these means in a result list.\n\n### Solution Code\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if not matrix:\n        return []\n    \n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    else:\n        num_columns = len(matrix[0])\n        return [sum(row[col] for row in matrix) / len(matrix) for col in range(num_columns)]\n```\n\n### Explanation\n- **Empty Matrix Check**: The function first checks if the matrix is empty. If it is, it returns an empty list.\n- **Row Mean Calculation**: For each row in the matrix, the mean is calculated by summing the elements of the row and dividing by the number of elements in the row. This is done using a list comprehension for concise and efficient code.\n- **Column Mean Calculation**: For each column index, the function sums the elements from each row at that index. The mean is then calculated by dividing this sum by the number of rows. This is also done using a list comprehension for efficiency.\n\nThis approach ensures that the function efficiently computes the required means whether the mode is 'row' or 'column', handling both cases with clear and concise logic."}
{"task_id": 5, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that multiplies each element of a given matrix by a scalar value and returns the resulting matrix. Scalar multiplication is a fundamental operation in linear algebra where each element of the matrix is multiplied by a scalar, resulting in a new matrix of the same dimensions.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Understand Scalar Multiplication**: Scalar multiplication involves multiplying each element of the matrix by a given scalar value. This means that for each element in the matrix, we replace it with the product of itself and the scalar.\n2. **Iterate Through the Matrix**: We will loop through each row of the matrix and then through each element in the row. For each element, we will compute the product with the scalar and construct a new row with these products.\n3. **Construct the Result Matrix**: As we process each row, we will append the new row (with elements multiplied by the scalar) to the result matrix.\n\nThis approach ensures that we correctly handle each element and construct the resulting matrix efficiently.\n\n### Solution Code\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    return [[element * scalar for element in row] for row in matrix]\n```\n\n### Explanation\n- **Matrix and Scalar Input**: The function takes two parameters: `matrix`, which is a list of lists containing integers or floats, and `scalar`, which is an integer or float.\n- **List Comprehensions**: The function uses nested list comprehensions to iterate over each row and each element within the row. For each element, it multiplies the element by the scalar and constructs a new row.\n- **Result Construction**: The outer list comprehension iterates over each row, and the inner list comprehension processes each element within the row. The result is a new matrix where each element is the product of the original element and the scalar.\n\nThis solution is efficient and concise, leveraging Python's list comprehensions to handle the iteration and construction of the resulting matrix in a clear and readable manner."}
{"task_id": 6, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the eigenvalues of a given 2x2 matrix and return them sorted from highest to lowest. Eigenvalues are scalar values associated with a linear system of equations, and they can be found using the characteristic equation of the matrix.\n\n### Approach\n1. **Extract Matrix Elements**: Extract the elements of the matrix, which are denoted as a, b, c, and d for a 2x2 matrix.\n2. **Calculate Trace and Determinant**: Compute the trace (sum of the diagonal elements) and the determinant (ad - bc) of the matrix.\n3. **Compute Discriminant**: Use the trace and determinant to compute the discriminant, which is given by the formula \\( \\text{trace}^2 - 4 \\times \\text{determinant} \\).\n4. **Find Eigenvalues**: Using the quadratic formula, compute the two eigenvalues. The eigenvalues are given by \\( \\frac{\\text{trace} \\pm \\sqrt{\\text{discriminant}}}{2} \\).\n5. **Sort Eigenvalues**: Sort the computed eigenvalues in descending order and return them as a list.\n\n### Solution Code\n```python\nimport math\n\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    \n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    \n    sqrt_discriminant = math.sqrt(discriminant)\n    \n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n    \n    eigenvalues = [eigenvalue1, eigenvalue2]\n    eigenvalues.sort(reverse=True)\n    \n    return eigenvalues\n```\n\n### Explanation\n1. **Extract Matrix Elements**: The matrix is given as a list of lists. We extract the elements a, b, c, and d from their respective positions.\n2. **Trace and Determinant**: The trace is the sum of the diagonal elements (a + d), and the determinant is calculated as (a*d - b*c).\n3. **Discriminant**: The discriminant is computed using the trace and determinant. This value helps determine the nature of the eigenvalues (real or complex).\n4. **Eigenvalues Calculation**: Using the quadratic formula, we compute the two eigenvalues. These values are derived from the characteristic equation of the matrix.\n5. **Sorting**: The eigenvalues are sorted in descending order to meet the problem's requirement of returning the highest values first.\n\nThis approach ensures that we accurately compute the eigenvalues and return them in the desired order, handling matrices that yield real eigenvalues."}
{"task_id": 7, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to transform a given matrix \\( A \\) using the operation \\( T^{-1}AS \\), where \\( T \\) and \\( S \\) are invertible matrices. The function should first validate if \\( T \\) and \\( S \\) are invertible and then perform the transformation. If either matrix is not invertible or the transformation is not possible, the function should return -1.\n\n### Approach\n1. **Check if Matrices are Square**: Both \\( T \\) and \\( S \\) must be square matrices to be invertible. If either is not square, return -1.\n2. **Check Invertibility**: Compute the determinants of \\( T \\) and \\( S \\). If either determinant is zero, the matrix is not invertible, so return -1.\n3. **Compute Inverses**: Calculate the inverse of \\( T \\) (since \\( S \\) is used as is in the transformation).\n4. **Matrix Multiplication**: Perform the matrix multiplication \\( T^{-1} \\times A \\times S \\). If the dimensions are incompatible for multiplication, return -1.\n5. **Rounding and Conversion**: Round the resulting matrix to four decimal places and convert it to a Python list using numpy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Convert input lists to numpy arrays with float dtype\n    A_np = np.array(A, dtype=float)\n    T_np = np.array(T, dtype=float)\n    S_np = np.array(S, dtype=float)\n    \n    # Check if T and S are square matrices\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    \n    # Calculate determinants to check invertibility\n    det_T = np.linalg.det(T_np)\n    det_S = np.linalg.det(S_np)\n    \n    # If any determinant is zero, matrices are not invertible\n    if det_T == 0 or det_S == 0:\n        return -1\n    \n    # Compute the inverse of T\n    T_inv = np.linalg.inv(T_np)\n    \n    # Attempt the matrix multiplication\n    try:\n        transformed = T_inv @ A_np @ S_np\n    except ValueError:\n        # If matrix dimensions are incompatible for multiplication\n        return -1\n    \n    # Round the result to four decimal places\n    transformed_rounded = np.round(transformed, 4)\n    \n    # Convert the numpy array back to a Python list\n    result = transformed_rounded.tolist()\n    \n    return result\n```\n\n### Explanation\n1. **Conversion to Numpy Arrays**: The input matrices are converted to numpy arrays for efficient matrix operations.\n2. **Square Matrix Check**: The function checks if \\( T \\) and \\( S \\) are square matrices. If not, it returns -1.\n3. **Determinant Calculation**: The determinants of \\( T \\) and \\( S \\) are computed. If either determinant is zero, the function returns -1.\n4. **Inverse Calculation**: The inverse of \\( T \\) is computed using numpy's `linalg.inv` function.\n5. **Matrix Multiplication**: The transformation \\( T^{-1} \\times A \\times S \\) is performed. If the dimensions are incompatible, a ValueError is caught, and the function returns -1.\n6. **Rounding and Conversion**: The result is rounded to four decimal places and converted to a Python list for the final output.\n\nThis approach ensures that all edge cases are handled, including non-invertible matrices and incompatible dimensions for matrix multiplication."}
{"task_id": 8, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the inverse of a 2x2 matrix. If the matrix is not invertible (i.e., its determinant is zero), we should return `None`.\n\n### Approach\n1. **Extract Elements**: Extract the elements of the matrix. For a matrix `[[a, b], [c, d]]`, the elements are `a`, `b`, `c`, and `d`.\n2. **Calculate Determinant**: Compute the determinant using the formula `det = a*d - b*c`.\n3. **Check Invertibility**: If the determinant is zero, the matrix is not invertible, so return `None`.\n4. **Compute Inverse**: If the determinant is non-zero, compute the inverse using the formula for the inverse of a 2x2 matrix, which is `(1/det) * [[d, -b], [-c, a]]`.\n\n### Solution Code\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_det = 1.0 / det\n    return [\n        [d * inv_det, -b * inv_det],\n        [-c * inv_det, a * inv_det]\n    ]\n```\n\n### Explanation\n1. **Extract Elements**: The elements `a`, `b`, `c`, and `d` are extracted from the input matrix.\n2. **Calculate Determinant**: The determinant is calculated as `a*d - b*c`. This value determines if the matrix is invertible.\n3. **Check Invertibility**: If the determinant is zero, the function returns `None` because the matrix does not have an inverse.\n4. **Compute Inverse**: If the determinant is non-zero, the inverse matrix is computed by scaling the matrix `[[d, -b], [-c, a]]` by the reciprocal of the determinant. This results in the inverse matrix, which is then returned.\n\nThis approach efficiently handles both invertible and non-invertible 2x2 matrices, ensuring correct results for all valid inputs."}
{"task_id": 9, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that multiplies two matrices. Matrix multiplication is only possible when the number of columns in the first matrix is equal to the number of rows in the second matrix. If this condition is not met, the function should return -1.\n\n### Approach\n1. **Check for Empty Matrices**: If either of the input matrices is empty, return -1 immediately as matrix multiplication is not possible.\n2. **Get Matrix Dimensions**: Determine the number of rows and columns for both matrices. The number of columns in the first matrix (matrix `a`) must match the number of rows in the second matrix (matrix `b`).\n3. **Check Multiplication Feasibility**: If the number of columns in `a` does not match the number of rows in `b`, return -1.\n4. **Initialize Result Matrix**: Create a result matrix with dimensions equal to the number of rows of `a` and the number of columns of `b`, initialized with zeros.\n5. **Matrix Multiplication**: Compute each element of the result matrix as the dot product of the corresponding row from `a` and column from `b`.\n\n### Solution Code\n```python\ndef matrixmul(a: list[list[int|float]], b: list[list[int|float]]) -> list[list[int|float]]:\n    # Check if either matrix is empty\n    if not a or not b:\n        return -1\n    \n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    \n    # Check if multiplication is possible\n    if cols_a != rows_b:\n        return -1\n    \n    # Create the result matrix filled with zeros\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    \n    # Perform matrix multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            sum_val = 0\n            for k in range(cols_a):\n                sum_val += a[i][k] * b[k][j]\n            result[i][j] = sum_val\n    \n    return result\n```\n\n### Explanation\n1. **Empty Matrix Check**: The function first checks if either input matrix is empty. If so, it returns -1.\n2. **Dimension Extraction**: The dimensions of both matrices are extracted to determine their rows and columns.\n3. **Feasibility Check**: The function checks if the number of columns in the first matrix matches the number of rows in the second matrix. If not, it returns -1.\n4. **Result Initialization**: A result matrix is initialized with zeros, having the same number of rows as the first matrix and columns as the second matrix.\n5. **Dot Product Calculation**: Each element in the result matrix is computed as the sum of the products of corresponding elements from the rows of the first matrix and columns of the second matrix.\n\nThis approach ensures that the function correctly handles matrix multiplication and returns the appropriate result or -1 when multiplication is not feasible."}
{"task_id": 10, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the covariance matrix for a given set of vectors. Each vector represents a feature with its observations. The covariance matrix is a square matrix where each element (i, j) represents the covariance between the i-th and j-th features.\n\n### Approach\n1. **Compute the Mean**: For each feature vector, compute the mean of its observations. This is done by summing all the observations and dividing by the number of observations.\n2. **Calculate Covariance**: For each pair of features (i, j), compute the covariance using the formula:\n   \\[\n   \\text{cov}(i, j) = \\frac{\\sum_{k=1}^{n} (x_{ik} - \\text{mean}_i) \\cdot (x_{jk} - \\text{mean}_j)}{n-1}\n   \\]\n   where \\( n \\) is the number of observations, \\( x_{ik} \\) and \\( x_{jk} \\) are the k-th observations of features i and j, respectively, and \\( \\text{mean}_i \\) and \\( \\text{mean}_j \\) are the means of features i and j, respectively.\n3. **Construct the Matrix**: Organize the computed covariances into a square matrix where the element at position (i, j) is the covariance between feature i and feature j.\n\n### Solution Code\n```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    if not vectors:\n        return []\n    n = len(vectors[0])  # Number of observations\n    m = len(vectors)     # Number of features\n    if n == 0:\n        return []\n    \n    # Calculate the mean for each feature\n    means = [sum(vec) / n for vec in vectors]\n    \n    # Initialize the covariance matrix with zeros\n    cov_matrix = [[0.0 for _ in range(m)] for _ in range(m)]\n    \n    # Compute covariance for each pair of features\n    for i in range(m):\n        for j in range(m):\n            covariance = 0.0\n            for k in range(n):\n                covariance += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            covariance /= (n - 1)  # Using sample covariance (divided by n-1)\n            cov_matrix[i][j] = covariance\n    \n    return cov_matrix\n```\n\n### Explanation\n1. **Mean Calculation**: The mean of each feature is computed by summing all its observations and dividing by the number of observations. This is stored in the `means` list.\n2. **Covariance Calculation**: For each pair of features (i, j), the covariance is calculated by iterating through each observation, computing the product of the deviations from the mean for both features, summing these products, and then dividing by \\( n-1 \\) (for sample covariance).\n3. **Matrix Construction**: The computed covariances are stored in a 2D list `cov_matrix`, which is then returned as the result.\n\nThis approach ensures that we correctly compute the covariance matrix, handling both the diagonal elements (variances) and off-diagonal elements (covariances between different features). The solution efficiently handles the input by iterating through each observation and feature pair, ensuring accurate results."}
{"task_id": 11, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Jacobi method, an iterative technique for solving a system of linear equations given by Ax = b. The method involves updating each component of the solution vector simultaneously based on the previous iteration's values. The solution is approximated after a specified number of iterations, with each intermediate result rounded to four decimal places.\n\n### Approach\n1. **Initialization**: Start with an initial guess for the solution vector x, which is a zero vector of the same size as b.\n2. **Decomposition**: Extract the diagonal elements of matrix A to form a diagonal matrix D. Compute the inverse of D, which is used to scale the residual vector in each iteration.\n3. **Iteration**: For each iteration, compute the residual vector r as b - Ax. Scale this residual using the inverse of D to get an update vector. Update the solution vector x by adding this scaled residual.\n4. **Rounding**: After each iteration, round the updated solution vector to four decimal places to ensure precision.\n5. **Return Result**: After completing the specified number of iterations, convert the solution vector to a list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    x = np.zeros_like(b, dtype=float)\n    D = np.diag(A)\n    D_inv = 1.0 / D\n    for _ in range(n):\n        r = b - np.dot(A, x)\n        D_inv_r = r * D_inv\n        x += D_inv_r\n        x = np.round(x, 4)\n    return x.tolist()\n```\n\n### Explanation\n- **Initialization**: The solution vector x is initialized to a zero vector of the same size as b.\n- **Decomposition**: The diagonal elements of A are extracted to form D, and its inverse is computed for scaling the residual vector.\n- **Iteration**: For each iteration, the residual vector r is computed. This residual is scaled by the inverse of D to produce an update vector, which is then added to the current solution vector x.\n- **Rounding**: After each update, the solution vector is rounded to four decimal places to maintain precision.\n- **Result**: The final solution vector, after n iterations, is converted to a list and returned.\n\nThis approach ensures that each iteration uses the most recent values of the solution vector, adhering to the Jacobi method's simultaneous update rule. The method is efficient and straightforward, making it suitable for solving systems of linear equations iteratively."}
{"task_id": 12, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the singular values of a 2x2 matrix using the Jacobian method. The singular values are derived from the eigenvalues of the matrix product of the transpose of the matrix and the matrix itself. This approach ensures that we do not use the built-in `numpy svd` function, allowing us to understand the underlying process.\n\n### Approach\n1. **Compute the Matrix Product**: First, we compute the matrix product of the transpose of the given matrix \\( A \\) and \\( A \\) itself, resulting in a new matrix \\( B \\).\n2. **Extract Elements**: Extract the elements of matrix \\( B \\) to compute the trace and determinant.\n3. **Calculate Eigenvalues**: Using the trace and determinant of \\( B \\), compute the eigenvalues. For a 2x2 matrix, the eigenvalues can be found using the quadratic formula.\n4. **Compute Singular Values**: The singular values are the square roots of the eigenvalues of \\( B \\).\n5. **Sort and Round**: Ensure the singular values are sorted in descending order and rounded to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Compute A^T A\n    B = np.dot(A.T, A)\n    a = B[0, 0]\n    b = B[0, 1]\n    c = B[1, 0]\n    d = B[1, 1]\n    \n    # Calculate trace and determinant of B\n    trace = a + d\n    det = a * d - b * c\n    \n    # Compute the square root part of the eigenvalue formula\n    sqrt_part = np.sqrt(trace**2 - 4 * det)\n    \n    # Calculate the eigenvalues\n    lambda1 = (trace + sqrt_part) / 2\n    lambda2 = (trace - sqrt_part) / 2\n    \n    # Compute singular values as square roots of eigenvalues\n    sigma1 = np.sqrt(lambda1)\n    sigma2 = np.sqrt(lambda2)\n    \n    # Round to four decimal places\n    rounded_sigma1 = round(sigma1, 4)\n    rounded_sigma2 = round(sigma2, 4)\n    \n    # Return as a tuple, ensuring the order is correct\n    return (rounded_sigma1, rounded_sigma2)\n```\n\n### Explanation\n1. **Matrix Product**: The product of the transpose of \\( A \\) and \\( A \\) itself results in a symmetric matrix \\( B \\), which is positive semi-definite. This ensures that all eigenvalues of \\( B \\) are non-negative.\n2. **Eigenvalues Calculation**: For a 2x2 matrix, the eigenvalues are computed using the trace and determinant, which are derived from the elements of \\( B \\). The quadratic formula is used to find the eigenvalues.\n3. **Singular Values**: The singular values are the square roots of these eigenvalues, ensuring they are non-negative and ordered in descending order.\n4. **Rounding**: The singular values are rounded to four decimal places to meet the problem's output constraints.\n\nThis approach efficiently computes the singular values using fundamental linear algebra operations, providing a clear understanding of the underlying mathematics without relying on built-in SVD functions."}
{"task_id": 13, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the determinant of a 4x4 matrix using Laplace's Expansion method. This method involves expanding the determinant along a row or column and computing the determinants of smaller minor matrices recursively.\n\n### Approach\n1. **Laplace's Expansion**: The determinant of a matrix can be computed by expanding along any row or column. For simplicity, we expand along the first row.\n2. **Minor Matrices**: For each element in the first row, we create a minor matrix by removing the row and column of the current element. This results in a 3x3 matrix.\n3. **Recursive Determinant Calculation**: The determinant of each 3x3 minor matrix is computed using a helper function. This helper function directly calculates the determinant of a 3x3 matrix using the standard formula.\n4. **Sign Alternation**: Each term in the expansion is multiplied by a sign factor of (-1)^(row+column). Since we are expanding along the first row (row=0), the sign alternates as (+1, -1, +1, -1) for columns 0 to 3 respectively.\n\n### Solution Code\n```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    def determinant_3x3(m: list[list[int|float]]) -> float:\n        a, b, c = m[0]\n        d, e, f = m[1]\n        g, h, i = m[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    \n    det = 0.0\n    for col in range(4):\n        # Construct the minor matrix\n        minor = []\n        for r in range(1, 4):  # Skip the first row\n            row = []\n            for c in range(4):\n                if c != col:\n                    row.append(matrix[r][c])\n            minor.append(row)\n        # Calculate the sign\n        sign = (-1) ** col\n        # Calculate the determinant of the minor\n        minor_det = determinant_3x3(minor)\n        # Accumulate the result\n        det += matrix[0][col] * sign * minor_det\n    return det\n```\n\n### Explanation\n1. **Helper Function for 3x3 Matrix**: The `determinant_3x3` function computes the determinant of a 3x3 matrix using the standard formula for 3x3 determinants.\n2. **Minor Matrix Construction**: For each element in the first row of the 4x4 matrix, we construct a 3x3 minor matrix by excluding the current column and the first row.\n3. **Sign Calculation**: The sign for each term is determined by (-1) raised to the power of the column index.\n4. **Expansion and Summation**: Each term in the expansion is computed as the product of the element, its sign, and the determinant of its minor matrix. These terms are summed to get the final determinant.\n\nThis approach efficiently breaks down the problem into smaller subproblems, leveraging recursion and helper functions to manage the complexity of matrix operations."}
{"task_id": 14, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs linear regression using the normal equation. The normal equation provides a direct method to compute the coefficients of the linear regression model without using iterative methods like gradient descent.\n\n### Approach\nThe normal equation for linear regression is given by:\n\\[ \\theta = (X^T X)^{-1} X^T y \\]\nwhere:\n- \\( X \\) is the feature matrix.\n- \\( y \\) is the target vector.\n- \\( X^T \\) is the transpose of matrix \\( X \\).\n- \\( \\theta \\) is the vector of coefficients we want to compute.\n\nThe steps to solve the problem are as follows:\n1. Convert the input lists \\( X \\) and \\( y \\) into numpy arrays for efficient matrix operations.\n2. Compute the transpose of matrix \\( X \\).\n3. Calculate the product of \\( X^T \\) and \\( X \\) to get the matrix \\( X^T X \\).\n4. Invert the matrix \\( X^T X \\).\n5. Multiply the inverse matrix by \\( X^T \\) and then by \\( y \\) to get the coefficients \\( \\theta \\).\n6. Round the coefficients to four decimal places and convert the result to a list of floats.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_array = np.array(X, dtype=float)\n    y_array = np.array(y, dtype=float)\n    \n    X_trans = X_array.T\n    X_trans_X = np.dot(X_trans, X_array)\n    \n    # Compute the inverse of X_trans_X\n    inverse = np.linalg.inv(X_trans_X)\n    \n    # Calculate theta using the normal equation\n    theta = np.dot(inverse, X_trans)\n    theta = np.dot(theta, y_array)\n    \n    # Reshape to ensure it's a 1D array and round to four decimal places\n    theta = theta.reshape(-1)\n    theta_rounded = np.round(theta, 4)\n    \n    return theta_rounded.tolist()\n```\n\n### Explanation\n1. **Matrix Conversion**: The input lists \\( X \\) and \\( y \\) are converted to numpy arrays to facilitate matrix operations.\n2. **Transpose and Matrix Multiplication**: The transpose of \\( X \\) is computed, and then the product \\( X^T X \\) is calculated.\n3. **Matrix Inversion**: The matrix \\( X^T X \\) is inverted. This step is crucial as it allows us to solve the system of linear equations directly.\n4. **Coefficient Calculation**: Using the normal equation, the coefficients \\( \\theta \\) are computed by multiplying the inverse matrix with \\( X^T \\) and then with \\( y \\).\n5. **Rounding and Conversion**: The resulting coefficients are rounded to four decimal places and converted to a list of floats for the final output.\n\nThis approach efficiently computes the coefficients using matrix operations, ensuring accuracy and correctness for the linear regression model."}
{"task_id": 15, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs linear regression using gradient descent. The function will take in the feature matrix \\(X\\), target vector \\(y\\), learning rate \\(\\alpha\\), and the number of iterations. It will return the coefficients of the linear regression model as a list, rounded to four decimal places.\n\n### Approach\n1. **Initialization**: Start by initializing the coefficients (theta) to zeros. The size of theta is determined by the number of features in \\(X\\).\n2. **Gradient Descent Iterations**: For each iteration, compute the predictions using the current coefficients, calculate the error, and then determine the gradient of the cost function with respect to each coefficient. Update the coefficients by moving in the direction that reduces the error, scaled by the learning rate \\(\\alpha\\).\n3. **Rounding and Conversion**: After completing the specified number of iterations, round the coefficients to four decimal places and convert them to a list for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    m = X.shape[0]\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = (1.0 / m) * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: The coefficients (theta) are initialized to a zero vector of appropriate size.\n2. **Predictions and Error Calculation**: For each iteration, predictions are made using the current theta. The error is the difference between these predictions and the actual target values.\n3. **Gradient Calculation**: The gradient of the cost function is computed using the transpose of \\(X\\) and the error vector. This gradient indicates the direction of the steepest ascent of the cost function, which we use to adjust theta in the opposite direction to minimize the cost.\n4. **Update Coefficients**: The coefficients are updated by subtracting the product of the learning rate \\(\\alpha\\) and the gradient from the current theta.\n5. **Rounding and Conversion**: After all iterations, the coefficients are rounded to four decimal places and converted to a list for the final output.\n\nThis approach ensures that we iteratively adjust the coefficients to best fit the data, minimizing the sum of squared errors using gradient descent. The final coefficients are returned as a list, rounded to four decimal places for precision."}
{"task_id": 16, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs feature scaling on a dataset using both standardization and min-max normalization. The function will take a 2D NumPy array as input and return two 2D lists: one scaled by standardization and one by min-max normalization. Each value in the resulting lists will be rounded to the nearest fourth decimal.\n\n### Approach\n1. **Standardization (Z-score Normalization)**:\n   - For each feature (column), compute the mean and standard deviation.\n   - Subtract the mean from each value and divide by the standard deviation. This centers the data around zero with unit variance.\n   - Handle division by zero by setting the result to zero if the standard deviation is zero.\n\n2. **Min-Max Normalization**:\n   - For each feature, compute the minimum and maximum values.\n   - Scale each value to the range [0, 1] using the formula \\((X - \\text{min}) / (\\text{max} - \\text{min})\\).\n   - Handle division by zero by setting the result to zero if the maximum and minimum values are the same.\n\n3. **Rounding and Conversion**:\n   - Round the results of both scaling methods to four decimal places.\n   - Convert the resulting NumPy arrays to lists using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    standardized = (data - means) / stds\n    # Handle division by zero in standardization (when std is zero)\n    standardized = np.where(stds == 0, 0, standardized)\n    standardized_rounded = np.round(standardized, 4)\n    \n    # Min-Max Normalization\n    mins = np.min(data, axis=0)\n    maxs = np.max(data, axis=0)\n    denominator = maxs - mins\n    normalized = (data - mins) / denominator\n    # Handle division by zero in normalization (when max == min)\n    normalized = np.where(denominator == 0, 0, normalized)\n    normalized_rounded = np.round(normalized, 4)\n    \n    # Convert to lists\n    standardized_list = standardized_rounded.tolist()\n    normalized_list = normalized_rounded.tolist()\n    \n    return (standardized_list, normalized_list)\n```\n\n### Explanation\n- **Standardization**: This method transforms the data such that each feature has a mean of 0 and a standard deviation of 1. This is useful for making features comparable in scale.\n- **Min-Max Normalization**: This method scales the data to a specific range, typically between 0 and 1, which can be useful for algorithms that require input data to be within a specific range.\n- **Handling Edge Cases**: The function handles cases where the standard deviation or the difference between max and min is zero by setting the scaled values to zero, avoiding division by zero errors.\n- **Rounding and Conversion**: The results are rounded to four decimal places for precision and converted to lists for easier handling in subsequent steps.\n\nThis approach ensures that the data is scaled appropriately for different machine learning algorithms, making it a robust solution for feature scaling."}
{"task_id": 17, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the k-Means clustering algorithm. This algorithm partitions a set of points into k clusters such that each cluster is represented by its centroid. The goal is to group similar points together and represent each group by its center.\n\n### Approach\nThe k-Means clustering algorithm works as follows:\n\n1. **Initialization**: Start with a set of initial centroids.\n2. **Iteration**: For each iteration, perform the following steps until the centroids converge or the maximum number of iterations is reached:\n   - **Assignment**: Assign each point to the closest centroid based on the Euclidean distance.\n   - **Update**: Compute new centroids as the mean of all points assigned to each cluster.\n3. **Convergence Check**: If the centroids do not change significantly between iterations, stop early.\n4. **Rounding**: Round the final centroids to four decimal places for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids, dtype=np.float64)\n    \n    for _ in range(max_iterations):\n        # Calculate distances from each point to each centroid\n        distances = np.sqrt(np.sum((points_array[:, np.newaxis] - centroids) ** 2, axis=2))\n        # Assign each point to the closest centroid\n        labels = np.argmin(distances, axis=1)\n        # Compute new centroids\n        new_centroids = np.array([points_array[labels == i].mean(axis=0) for i in range(k)])\n        # Check for convergence\n        if np.allclose(centroids, new_centroids, atol=1e-6):\n            break\n        centroids = new_centroids\n    \n    # Round the centroids to four decimal places\n    rounded_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return rounded_centroids\n```\n\n### Explanation\n1. **Initialization**: Convert the input points and initial centroids into NumPy arrays for efficient computation.\n2. **Distance Calculation**: For each point, compute the distance to each centroid. This is done using NumPy's broadcasting to efficiently compute distances for all points and centroids.\n3. **Cluster Assignment**: Determine the closest centroid for each point and assign the point to the corresponding cluster.\n4. **Centroid Update**: Compute the new centroids as the mean of all points in each cluster. If the new centroids are very close to the previous ones, the algorithm stops early to avoid unnecessary iterations.\n5. **Rounding**: The final centroids are rounded to four decimal places to meet the output requirements.\n\nThis approach ensures that the algorithm efficiently clusters the points and converges to the optimal centroids within the given constraints."}
{"task_id": 18, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that generates train and test splits for K-Fold Cross-Validation. This technique is widely used in machine learning to evaluate the performance of a model by dividing the dataset into k folds and using each fold as a test set once while the remaining folds form the training set.\n\n### Approach\n1. **Input Validation**: Ensure that the number of folds (k) is at least 2. If not, raise a ValueError.\n2. **Index Initialization**: Create an array of indices representing each sample in the dataset.\n3. **Shuffling**: If shuffling is required, shuffle the indices using a random seed for reproducibility.\n4. **Fold Creation**: Split the indices into k contiguous blocks (folds). If the number of samples is not perfectly divisible by k, the first few folds will have one more sample than the others.\n5. **Generate Splits**: For each fold, create a tuple where the current fold is the test set and the remaining folds are combined to form the training set.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generates train and test splits for K-Fold Cross-Validation.\n\n    Args:\n        X (np.ndarray): The feature matrix.\n        y (np.ndarray): The target vector.\n        k (int, optional): The number of folds. Defaults to 5.\n        shuffle (bool, optional): Whether to shuffle the data before splitting. Defaults to True.\n        random_seed (int, optional): Seed for random shuffling. Defaults to None.\n\n    Returns:\n        list: A list of tuples, each containing (train_indices, test_indices) for each fold.\n\n    Raises:\n        ValueError: If k is less than 2.\n    \"\"\"\n    n_samples = X.shape[0]\n    if y.shape[0] != n_samples:\n        raise ValueError(\"X and y must have the same number of samples.\")\n    if k < 2:\n        raise ValueError(\"k must be at least 2.\")\n    \n    indices = np.arange(n_samples)\n    \n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    \n    m = n_samples // k\n    rem = n_samples % k\n    \n    folds = []\n    current = 0\n    for i in range(k):\n        if i < rem:\n            size = m + 1\n        else:\n            size = m\n        end = current + size\n        folds.append(indices[current:end])\n        current = end\n    \n    result = []\n    for i in range(k):\n        test = folds[i]\n        train = np.concatenate([folds[j] for j in range(k) if j != i])\n        result.append((train, test))\n    \n    return result\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the number of folds (k) is valid (at least 2) and if the feature matrix (X) and target vector (y) have the same number of samples.\n2. **Index Initialization**: An array of indices from 0 to n_samples-1 is created to represent each sample.\n3. **Shuffling**: If shuffling is enabled, the indices are shuffled using a random seed to ensure reproducibility.\n4. **Fold Creation**: The indices are split into k contiguous blocks. If the number of samples isn't perfectly divisible by k, the first few blocks will have one more sample than the others.\n5. **Generate Splits**: For each fold, the current block is used as the test set, and the remaining blocks are combined to form the training set. This results in a list of tuples, each containing the training and test indices for a fold.\n\nThis approach ensures that each sample is used exactly once as part of the test set, providing a robust way to evaluate model performance across different subsets of the data."}
{"task_id": 19, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs Principal Component Analysis (PCA) from scratch. The function will take a 2D NumPy array as input, where each row represents a data sample and each column represents a feature. The function will also take an integer `k` representing the number of principal components to return. The goal is to standardize the dataset, compute the covariance matrix, find the eigenvalues and eigenvectors, and return the principal components corresponding to the largest eigenvalues.\n\n### Approach\n1. **Standardize the Data**: Subtract the mean of each feature from the data and divide by the standard deviation to ensure each feature has zero mean and unit variance.\n2. **Compute the Covariance Matrix**: Calculate the covariance matrix from the standardized data. This matrix represents the variance and covariance between different features.\n3. **Compute Eigenvalues and Eigenvectors**: Use the covariance matrix to compute eigenvalues and eigenvectors. The eigenvectors corresponding to the largest eigenvalues are the principal components.\n4. **Sort Eigenvalues and Eigenvectors**: Sort the eigenvalues in descending order and reorder the eigenvectors accordingly.\n5. **Select Top k Eigenvectors**: Extract the top `k` eigenvectors corresponding to the largest eigenvalues.\n6. **Format the Result**: Convert the selected eigenvectors into a list of lists, rounding each element to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Standardize the data\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0, ddof=1)  # Using sample standard deviation\n    standardized = (data - mean) / std\n    \n    # Compute the covariance matrix\n    n = data.shape[0]\n    cov_matrix = (standardized.T @ standardized) / (n - 1)\n    \n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues_sorted = eigenvalues[sorted_indices]\n    eigenvectors_sorted = eigenvectors[:, sorted_indices]\n    \n    # Select the top k eigenvectors\n    principal_components = eigenvectors_sorted[:, :k]\n    \n    # Convert to list of lists with rounding\n    result = []\n    for pc in principal_components.T:\n        rounded_pc = [round(x, 4) for x in pc]\n        result.append(rounded_pc)\n    \n    return result\n```\n\n### Explanation\n1. **Standardization**: The data is standardized to ensure that each feature has a mean of 0 and a standard deviation of 1. This is done by subtracting the mean and dividing by the standard deviation for each feature.\n2. **Covariance Matrix**: The covariance matrix is computed from the standardized data. This matrix helps in understanding the variance and covariance between different features.\n3. **Eigenvalues and Eigenvectors**: Using the covariance matrix, eigenvalues and eigenvectors are computed. The eigenvectors corresponding to the largest eigenvalues are the principal components.\n4. **Sorting and Selection**: The eigenvalues and eigenvectors are sorted in descending order of eigenvalues. The top `k` eigenvectors are selected as the principal components.\n5. **Result Formatting**: The selected eigenvectors are converted into a list of lists, with each element rounded to four decimal places for clarity.\n\nThis approach ensures that the function correctly performs PCA from scratch, providing the principal components in the required format."}
{"task_id": 20, "completion_id": 0, "completion": "\n\nHere's a Python function that implements the decision tree learning algorithm for classification using recursive binary splitting based on entropy and information gain:\n\n```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    # Base case: All examples have the same target value\n    target_values = [example[target_attr] for example in examples]\n    if all(val == target_values[0] for val in target_values):\n        return target_values[0]\n    \n    # Base case: No attributes left, return the most common target value\n    if not attributes:\n        counts = Counter(target_values)\n        return counts.most_common(1)[0][0]\n    \n    # Calculate entropy of the current examples\n    def entropy(examples, target_attr):\n        target_counts = Counter(example[target_attr] for example in examples)\n        total = len(examples)\n        ent = 0.0\n        for count in target_counts.values():\n            prob = count / total\n            ent -= prob * math.log2(prob)\n        return ent\n    \n    parent_entropy = entropy(examples, target_attr)\n    \n    # If entropy is zero, return the target value as a leaf\n    if parent_entropy == 0:\n        return examples[0][target_attr]\n    \n    # Find the attribute with maximum information gain\n    max_info_gain = -1\n    best_attr = None\n    \n    for attr in attributes:\n        attr_values = set(example[attr] for example in examples)\n        total = len(examples)\n        attr_entropy = 0.0\n        for value in attr_values:\n            subset = [e for e in examples if e[attr] == value]\n            subset_entropy = entropy(subset, target_attr)\n            attr_entropy += (len(subset) / total) * subset_entropy\n        info_gain = parent_entropy - attr_entropy\n        if info_gain > max_info_gain:\n            max_info_gain = info_gain\n            best_attr = attr\n    \n    # Create the node with the best attribute\n    node = {best_attr: {}}\n    \n    # Split examples by each value of the best attribute and build subtrees\n    for value in set(example[best_attr] for example in examples):\n        subset = [e for e in examples if e[best_attr] == value]\n        remaining_attrs = [a for a in attributes if a != best_attr]\n        subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n        node[best_attr][value] = subtree\n    \n    return node\n```\n\nThis function constructs a decision tree by recursively partitioning the training examples based on the attribute that provides the highest information gain. The tree is represented as a nested dictionary where each node corresponds to an attribute, and its branches correspond to the attribute's values, leading to either another node or a leaf (target value)."}
{"task_id": 22, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the output of the sigmoid activation function given an input value. The sigmoid function is a mathematical function that maps any real-valued number to a value between 0 and 1. This function is widely used in machine learning models, especially in binary classification problems, due to its S-shaped curve which helps in converting a linear model's predictions into probabilities.\n\n### Approach\nThe sigmoid function is defined by the formula:\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\nwhere \\( e \\) is the base of the natural logarithm (approximately equal to 2.71828).\n\nThe steps to compute the sigmoid function are as follows:\n1. Compute the exponent of the negative input value, \\( e^{-z} \\).\n2. Add 1 to the result from step 1.\n3. Take the reciprocal of the result from step 2.\n4. Round the result to four decimal places to get the final output.\n\n### Solution Code\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    exp_val = math.exp(-z)\n    sig = 1 / (1 + exp_val)\n    return round(sig, 4)\n```\n\n### Explanation\n1. **Exponent Calculation**: The function first calculates \\( e^{-z} \\) using Python's `math.exp()` function. This step is crucial as it transforms the input value into a form that the sigmoid function can process.\n2. **Denominator Calculation**: The result from the exponent calculation is then added to 1. This step ensures that the denominator is always positive, avoiding division by zero and ensuring the function's output is between 0 and 1.\n3. **Sigmoid Value Calculation**: The reciprocal of the denominator is computed to get the sigmoid value. This step maps the input value to a probability between 0 and 1.\n4. **Rounding**: The final result is rounded to four decimal places using Python's `round()` function to meet the problem's requirement for precision.\n\nThis approach efficiently computes the sigmoid function and ensures the result is precise and formatted correctly."}
{"task_id": 23, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the softmax activation for a given list of scores. The softmax function is commonly used in machine learning to convert a vector of real numbers into a probability distribution, ensuring each value is between 0 and 1, and the sum of all values is 1.\n\n### Approach\nThe approach to compute the softmax values involves the following steps:\n\n1. **Compute Exponentials**: For each score in the input list, compute the exponential value using the formula \\( e^s \\), where \\( s \\) is the score.\n2. **Sum of Exponentials**: Calculate the sum of all the computed exponential values. This sum serves as the denominator in the softmax formula.\n3. **Compute Softmax Values**: For each exponential value, divide it by the sum of exponentials to get the softmax value.\n4. **Rounding**: Round each resulting softmax value to four decimal places to ensure the output meets the specified precision.\n\n### Solution Code\n\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(s) for s in scores]\n    sum_exp = sum(exp_scores)\n    result = [s / sum_exp for s in exp_scores]\n    rounded = [round(num, 4) for num in result]\n    return rounded\n```\n\n### Explanation\n1. **Exponential Calculation**: The function starts by computing the exponential of each score using `math.exp()`, which is stored in a list called `exp_scores`.\n2. **Sum Calculation**: The sum of all values in `exp_scores` is computed to determine the denominator for the softmax formula.\n3. **Softmax Computation**: Each value in `exp_scores` is divided by the computed sum to get the individual softmax values.\n4. **Rounding**: Each computed softmax value is rounded to four decimal places to ensure the result is precise and formatted as required.\n\nThis approach ensures that the output is a list of probabilities that sum up to 1, each rounded to four decimal places, making it suitable for use in various machine learning applications."}
{"task_id": 24, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to simulate a single neuron with a sigmoid activation function for binary classification. The neuron will handle multidimensional input features and compute predicted probabilities and the mean squared error (MSE) between these predictions and the true labels.\n\n### Approach\n1. **Compute Predictions**: For each feature vector, compute the weighted sum of the features using the given weights, add the bias, and then apply the sigmoid activation function to get the predicted probability.\n2. **Sigmoid Function**: The sigmoid function is used to convert the weighted sum into a probability between 0 and 1.\n3. **Mean Squared Error (MSE)**: Calculate the MSE between the predicted probabilities and the true labels. This involves squaring the difference between each predicted probability and the corresponding true label, summing these squared differences, and then dividing by the number of examples.\n4. **Rounding**: Both the predicted probabilities and the MSE are rounded to four decimal places for the final output.\n\n### Solution Code\n```python\nimport math\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    predictions = []\n    for feature_vector in features:\n        z = sum(f * w for f, w in zip(feature_vector, weights)) + bias\n        sigmoid = 1 / (1 + math.exp(-z))\n        predictions.append(sigmoid)\n    \n    total_error = 0.0\n    for i in range(len(labels)):\n        error = predictions[i] - labels[i]\n        total_error += error ** 2\n    \n    mse = total_error / len(labels)\n    \n    rounded_predictions = [round(p, 4) for p in predictions]\n    rounded_mse = round(mse, 4)\n    \n    return rounded_predictions, rounded_mse\n```\n\n### Explanation\n1. **Predictions Calculation**: For each feature vector, the code computes the dot product with the weights, adds the bias, and applies the sigmoid function to get the predicted probability. This is done using a loop over each feature vector.\n2. **MSE Calculation**: The code then computes the MSE by iterating over each predicted probability and true label, calculating the squared error for each pair, summing these errors, and dividing by the number of examples.\n3. **Rounding**: The predicted probabilities are rounded to four decimal places using a list comprehension, and the MSE is also rounded to four decimal places.\n\nThis approach ensures that we accurately simulate the behavior of a single neuron with a sigmoid activation function, providing both the predicted probabilities and the MSE in the required format."}
{"task_id": 25, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to simulate a single neuron with a sigmoid activation function and implement backpropagation to update the neuron's weights and bias using gradient descent based on the Mean Squared Error (MSE) loss. The goal is to return the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places.\n\n### Approach\n1. **Initialization**: Start with the given initial weights and bias. Make a copy of the initial weights to avoid modifying the original array.\n2. **Forward Pass**: For each epoch, compute the output of the neuron using the current weights and bias. The output is calculated using the sigmoid activation function.\n3. **Loss Calculation**: Compute the MSE loss for the current epoch. This is done by comparing the predicted output with the true labels.\n4. **Backward Pass**: Calculate the gradients of the loss with respect to the weights and bias using the chain rule. This involves computing the delta term, which is the product of the error and the derivative of the sigmoid function.\n5. **Weight Update**: Update the weights and bias using the computed gradients and the learning rate.\n6. **Store Results**: Keep track of the MSE for each epoch, rounding it to four decimal places before storing.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    n_samples = features.shape[0]\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_history = []\n    \n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        y_pred = 1 / (1 + np.exp(-z))\n        \n        # Compute MSE and round it\n        mse = np.mean((y_pred - labels) ** 2)\n        mse_rounded = round(mse, 4)\n        mse_history.append(mse_rounded)\n        \n        # Compute delta\n        delta = (y_pred - labels) * y_pred * (1 - y_pred)\n        \n        # Compute gradients\n        grad_weights = (features.T @ delta) / n_samples\n        grad_bias = np.mean(delta)\n        \n        # Update weights and bias\n        weights -= learning_rate * grad_weights\n        bias -= learning_rate * grad_bias\n    \n    return weights, bias, mse_history\n```\n\n### Explanation\n- **Initialization**: The weights and bias are initialized with the provided values. We make a copy of the initial weights to ensure the original array is not modified.\n- **Forward Pass**: For each epoch, the neuron's output is computed using the current weights and bias. The sigmoid function is applied to the linear combination of inputs and weights.\n- **Loss Calculation**: The MSE loss is computed by taking the mean of the squared differences between the predicted and true labels.\n- **Backward Pass**: The gradients are computed using the chain rule. The delta term is calculated as the product of the error (difference between predicted and true labels) and the derivative of the sigmoid function.\n- **Weight Update**: The weights and bias are updated by subtracting the product of the learning rate and the computed gradients.\n- **Result Storage**: The MSE for each epoch is rounded to four decimal places and stored in a list, which is returned along with the updated weights and bias.\n\nThis approach ensures that the neuron's weights and bias are adjusted iteratively to minimize the loss, and the process is repeated for the specified number of epochs. The result is a neuron that has learned to predict the labels based on the input features."}
{"task_id": 26, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Python class called `Value` that can handle basic autograd operations such as addition, multiplication, and ReLU activation. The class should manage scalar values and compute gradients automatically through automatic differentiation.\n\n### Approach\nThe `Value` class is designed to create nodes in a computation graph. Each node keeps track of its data, gradient, and the operation it performs. The key operations (addition, multiplication, and ReLU) are implemented as methods that return new `Value` instances. Each operation defines its own backward pass function to compute gradients correctly using the chain rule.\n\n1. **Initialization**: The `Value` class is initialized with data, children nodes, and the operation name. It also initializes the gradient to zero and sets up a default backward function that does nothing.\n2. **Addition Operation**: The `add` method creates a new `Value` node representing the sum of two nodes. The backward function for addition adds the gradient from the result to both input nodes.\n3. **Multiplication Operation**: The `mul` method creates a new `Value` node representing the product of two nodes. The backward function for multiplication uses the product rule to compute gradients for both input nodes.\n4. **ReLU Activation**: The `relu` method creates a new `Value` node applying the ReLU function. The backward function for ReLU passes the gradient only if the input was positive.\n5. **Backward Pass**: The `backward` method initiates the backward pass starting from the root node, triggering the computation of gradients through the computation graph.\n\n### Solution Code\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def add(self, other):\n        new_data = self.data + other.data\n        new_node = Value(new_data, (self, other), 'add')\n        def _backward():\n            self.grad += new_node.grad * 1\n            other.grad += new_node.grad * 1\n            self._backward()\n            other._backward()\n        new_node._backward = _backward\n        return new_node\n\n    def mul(self, other):\n        new_data = self.data * other.data\n        new_node = Value(new_data, (self, other), 'mul')\n        def _backward():\n            self.grad += new_node.grad * other.data\n            other.grad += new_node.grad * self.data\n            self._backward()\n            other._backward()\n        new_node._backward = _backward\n        return new_node\n\n    def relu(self):\n        new_data = max(0, self.data)\n        new_node = Value(new_data, (self,), 'relu')\n        def _backward():\n            if self.data > 0:\n                self.grad += new_node.grad\n            self._backward()\n        new_node._backward = _backward\n        return new_node\n\n    def backward(self):\n        self._backward()\n```\n\n### Explanation\n- **Initialization**: Each `Value` node is initialized with data, child nodes, and an operation name. The gradient is set to zero, and a default backward function is provided.\n- **Addition**: The `add` method creates a new node with the sum of two nodes. The backward function adds the gradient from the result to both input nodes and calls their backward functions.\n- **Multiplication**: The `mul` method creates a new node with the product of two nodes. The backward function uses the product rule to compute gradients for both inputs.\n- **ReLU**: The `relu` method creates a new node applying the ReLU function. The backward function passes the gradient only if the input was positive.\n- **Backward Pass**: The `backward` method starts the gradient computation from the root node, propagating gradients through the computation graph.\n\nThis approach ensures that each operation correctly computes gradients using automatic differentiation, allowing for efficient computation of derivatives in a computation graph."}
{"task_id": 27, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the transformation matrix \\( P \\) that converts a vector from basis \\( B \\) to basis \\( C \\) in \\( \\mathbb{R}^3 \\). The transformation matrix \\( P \\) is derived by expressing each basis vector of \\( B \\) in terms of the basis vectors of \\( C \\).\n\n### Approach\n1. **Convert Input to NumPy Arrays**: Convert the given lists of basis vectors \\( B \\) and \\( C \\) into NumPy arrays for easier matrix operations.\n2. **Compute Inverse of Matrix \\( C \\)**: The transformation matrix \\( P \\) is found by expressing each basis vector of \\( B \\) in terms of \\( C \\). This is done by computing the inverse of matrix \\( C \\) (denoted as \\( C^{-1} \\)).\n3. **Matrix Multiplication**: Multiply the inverse of matrix \\( C \\) with matrix \\( B \\) to get the transformation matrix \\( P \\). This is because each column of \\( P \\) represents the coordinates of the corresponding basis vector of \\( B \\) in terms of \\( C \\).\n4. **Rounding and Conversion**: Round the elements of the resulting matrix \\( P \\) to four decimal places and convert it back to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_array = np.array(B, dtype=float)\n    C_array = np.array(C, dtype=float)\n    C_inv = np.linalg.inv(C_array)\n    P = np.dot(C_inv, B_array)\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The input lists \\( B \\) and \\( C \\) are converted to NumPy arrays to facilitate matrix operations.\n2. **Inverse Calculation**: The inverse of matrix \\( C \\) is computed using `np.linalg.inv()`, which is necessary to express the basis vectors of \\( B \\) in terms of \\( C \\).\n3. **Matrix Multiplication**: The transformation matrix \\( P \\) is obtained by multiplying \\( C^{-1} \\) with \\( B \\). This step effectively transforms each basis vector of \\( B \\) into the coordinate system defined by \\( C \\).\n4. **Rounding and Conversion**: The resulting matrix \\( P \\) is rounded to four decimal places to meet the precision requirement and then converted back to a list of lists using `tolist()` for the final output.\n\nThis approach ensures that we accurately compute the transformation matrix \\( P \\) and present it in the required format."}
{"task_id": 29, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that randomly shuffles two numpy arrays, X and y, while maintaining the correspondence between their elements. The function should also provide an optional seed parameter for reproducibility and return the shuffled arrays as a list of lists.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Set Random Seed for Reproducibility**: If a seed is provided, we set the random seed using numpy's random seed function to ensure that the shuffling can be reproduced.\n\n2. **Generate Permutation of Indices**: We generate a random permutation of indices using numpy's `random.permutation` function. This permutation will be used to shuffle both arrays X and y.\n\n3. **Shuffle Arrays Using Permutation**: Apply the generated permutation to both arrays X and y. This ensures that both arrays are shuffled in the same order.\n\n4. **Convert to Lists**: Convert the shuffled numpy arrays to lists using the `tolist()` method.\n\n5. **Return Result**: Return the shuffled arrays as a list containing two elements, where the first element is the shuffled X array and the second element is the shuffled y array.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.random.permutation(len(X))\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]\n```\n\n### Explanation\n- **Setting the Random Seed**: By setting the random seed, we ensure that the shuffling process is reproducible. This is particularly useful for debugging and testing purposes where consistent results are needed.\n\n- **Generating Permutation of Indices**: The `np.random.permutation` function generates a random order of indices that can be used to shuffle the arrays. This ensures that both X and y are shuffled in the same way.\n\n- **Shuffling Arrays**: Using the permutation indices, we shuffle both X and y. This maintains the correspondence between the elements of X and y.\n\n- **Converting to Lists**: The `tolist()` method converts the numpy arrays to Python lists, which is the required output format.\n\nThis approach efficiently shuffles the arrays while maintaining their correspondence and provides reproducibility when needed."}
{"task_id": 30, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that iterates over a dataset in batches. The function should handle both cases where the target data is provided and where it is not. Each batch should be converted from a numpy array to a Python list before being yielded.\n\n### Approach\n1. **Problem Analysis**: The task is to implement a generator function that yields batches of data. If target data is provided, each batch should be a tuple of input and target data. Otherwise, each batch should only contain the input data. The function should handle cases where the total number of samples is not a multiple of the batch size, ensuring that the last batch is yielded even if it is smaller than the specified batch size.\n\n2. **Intuition**: The function will iterate over the dataset in steps of the specified batch size. For each step, it will extract the corresponding batch from the input data and, if provided, the target data. Each batch will be converted to a list using numpy's `tolist()` method before being yielded.\n\n3. **Algorithm Selection**: A while loop is suitable for this task as it allows us to control the indices directly, ensuring that we handle the end of the dataset correctly. The loop will continue until all samples have been processed.\n\n4. **Complexity Analysis**: The time complexity of this function is O(n), where n is the number of samples. This is because each sample is processed exactly once. The space complexity is O(batch_size), as we only store one batch in memory at a time.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    n_samples = X.shape[0]\n    i = 0\n    while i < n_samples:\n        end = i + batch_size\n        if end > n_samples:\n            end = n_samples\n        X_batch = X[i:end]\n        if y is not None:\n            y_batch = y[i:end]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()\n        i += batch_size\n```\n\n### Explanation\n- **Initialization**: The function starts by determining the number of samples in the input data `X`.\n- **Loop**: A while loop is used to iterate over the dataset. The loop starts at index 0 and continues until all samples are processed.\n- **Batch Extraction**: For each iteration, the function calculates the end index for the current batch. If the end index exceeds the number of samples, it is adjusted to the last sample index.\n- **Conversion and Yielding**: The current batch of input data is extracted and converted to a list. If target data is provided, the corresponding batch of target data is also extracted and converted to a list. The batches are then yielded as a tuple if target data is present, or as a single list if not.\n- **Index Update**: The loop index is updated by adding the batch size to move to the next batch.\n\nThis approach ensures that the function efficiently processes the dataset in batches, handling both the presence and absence of target data gracefully."}
{"task_id": 31, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that divides a dataset into two subsets based on whether the value of a specified feature meets or exceeds a given threshold. This function will return two subsets: one where the feature values are greater than or equal to the threshold, and another where they are not.\n\n### Approach\n1. **Extract the Feature Column**: Identify the column of the dataset corresponding to the specified feature index.\n2. **Create a Boolean Mask**: Generate a boolean mask where each entry is `True` if the corresponding feature value is greater than or equal to the threshold, and `False` otherwise.\n3. **Filter the Dataset**: Use the boolean mask to split the dataset into two subsets. The first subset contains samples where the feature value meets the condition, and the second subset contains the remaining samples.\n4. **Convert to Lists**: Convert the resulting NumPy arrays into Python lists using the `tolist()` method to meet the output constraints.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    # Create a boolean mask based on the threshold\n    mask = X[:, feature_i] >= threshold\n    # Split the dataset into two subsets\n    subset1 = X[mask]\n    subset2 = X[~mask]\n    # Convert numpy arrays to lists and return\n    return [subset1.tolist(), subset2.tolist()]\n```\n\n### Explanation\n1. **Boolean Mask Creation**: The mask is created by comparing each value in the specified feature column against the threshold. This results in a boolean array where each entry indicates whether the corresponding sample meets the condition.\n2. **Dataset Filtering**: Using the boolean mask, the dataset is split into two parts. The first part includes all samples where the feature value is greater than or equal to the threshold, and the second part includes the rest.\n3. **Conversion to Lists**: The resulting subsets, which are NumPy arrays, are converted into Python lists to ensure the output format is as required.\n\nThis approach efficiently divides the dataset using vectorized operations, ensuring clarity and performance."}
{"task_id": 32, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate polynomial features for a given dataset. Polynomial features are created by taking each existing feature and raising it to powers up to a specified degree, as well as multiplying different features together. This approach is useful in machine learning to transform the data into a higher-dimensional space, which can help improve model performance.\n\n### Approach\n1. **Understanding Polynomial Features**: Polynomial features involve creating new features by taking each existing feature and raising it to powers up to the given degree, as well as multiplying different features together. For example, if the degree is 2, each feature \\( x_i \\) will be squared, and each pair of features \\( x_i \\) and \\( x_j \\) (where \\( i \\leq j \\)) will be multiplied together.\n\n2. **Generating Combinations**: For each degree from 1 to the specified degree, generate all possible combinations of the feature indices with replacement. This ensures that we consider all possible products of the features up to the specified degree.\n\n3. **Computing Products**: For each combination of feature indices, compute the product of the corresponding elements in the dataset. This product represents a new polynomial feature.\n\n4. **Constructing the Result**: Collect all the computed products and construct a new feature matrix by concatenating these products column-wise. Finally, return this matrix as a Python list.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    n_samples, n_features = X.shape\n    new_features = []\n    \n    for d in range(1, degree + 1):\n        for combo in combinations_with_replacement(range(n_features), d):\n            product = np.ones(n_samples)\n            for idx in combo:\n                product *= X[:, idx]\n            new_features.append(product)\n    \n    new_features_array = np.array(new_features)\n    result = new_features_array.T\n    return result.tolist()\n```\n\n### Explanation\n1. **Initialization**: The function starts by determining the number of samples and features in the input dataset \\( X \\).\n\n2. **Loop Through Degrees**: For each degree from 1 to the specified degree, the function generates all possible combinations of feature indices with replacement using `combinations_with_replacement`.\n\n3. **Compute Products**: For each combination of indices, the function computes the product of the corresponding elements in \\( X \\). This product is added as a new feature.\n\n4. **Construct Result Matrix**: After collecting all the new features, they are stacked column-wise to form the new feature matrix. This matrix is then transposed to ensure each row represents a sample and each column represents a feature.\n\n5. **Return Result**: The resulting matrix is converted to a Python list using `tolist()` and returned.\n\nThis approach efficiently generates all necessary polynomial features up to the specified degree, ensuring that the dataset is transformed appropriately for higher-dimensional modeling."}
{"task_id": 33, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate random subsets of a given dataset. Each subset should be a tuple containing a 2D numpy array of features and a 1D numpy array of target values. The subsets can be generated with or without replacement, and the number of subsets is specified by the user.\n\n### Approach\n1. **Problem Analysis**: The task requires generating multiple random subsets from a given dataset. Each subset should maintain the structure of the original dataset but with randomly selected or shuffled samples. The key parameters are the number of subsets, whether to sample with replacement, and a seed for reproducibility.\n\n2. **Intuitions and Insights**: \n   - When sampling with replacement (bootstrap sampling), each subset will have the same number of samples as the original dataset, but some samples may be repeated.\n   - When sampling without replacement, each subset is a permutation of the original dataset, meaning each sample appears exactly once but in a different order.\n\n3. **Algorithm Selection**: \n   - Use `np.random.choice` with replacement to generate bootstrap samples.\n   - Use `np.random.permutation` to generate shuffled indices without replacement.\n\n4. **Complexity Analysis**: The time complexity is dominated by the random sampling operations. Each call to `np.random.choice` or `np.random.permutation` is O(N), where N is the number of samples. Generating `n_subsets` such samples results in a time complexity of O(n_subsets * N).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    N = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(N, size=N, replace=True)\n        else:\n            indices = np.random.permutation(N)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\n```\n\n### Explanation\n1. **Setting the Seed**: The random seed is set to ensure reproducibility of the results.\n2. **Generating Indices**: For each subset, indices are generated either by random choice with replacement or by permutation without replacement.\n3. **Creating Subsets**: Using the generated indices, the corresponding features and target values are selected from the original dataset.\n4. **Conversion to Lists**: Each subset is converted from numpy arrays to lists using `tolist()` before being added to the result list.\n\nThis approach efficiently generates the required random subsets, ensuring that each subset meets the specified criteria of being with or without replacement."}
{"task_id": 34, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform one-hot encoding on a 1D numpy array of integer values. One-hot encoding is a technique used to convert nominal values into a binary vector format, which is useful for various machine learning algorithms.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Check for Empty Input**: If the input array is empty, return an empty list immediately.\n2. **Determine Number of Columns**: If the number of columns (`n_col`) is not provided, compute it as the maximum value in the input array plus one. This ensures that each unique value in the input array gets its own column in the resulting one-hot encoded array.\n3. **Initialize Result Array**: Create a 2D numpy array of zeros with dimensions `(len(x), n_col)`, where `len(x)` is the number of elements in the input array and `n_col` is the number of columns determined in the previous step.\n4. **Set One-Hot Encoded Values**: For each element in the input array, set the corresponding position in the result array to 1. This is done using numpy indexing for efficiency.\n5. **Convert to List**: Convert the resulting numpy array to a Python list using the `tolist()` method and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    if len(x) == 0:\n        return []\n    if n_col is None:\n        n_col = max(x) + 1\n    result = np.zeros((len(x), n_col), dtype=int)\n    for i in range(len(x)):\n        result[i, x[i]] = 1\n    return result.tolist()\n```\n\n### Explanation\n- **Empty Input Handling**: The function first checks if the input array is empty and returns an empty list if true.\n- **Automatic Column Determination**: If `n_col` is not provided, it is calculated as the maximum value in the input array plus one. This ensures that each unique value in the input array is represented as a column in the one-hot encoded result.\n- **Efficient Array Initialization and Update**: A 2D numpy array of zeros is created, and each element of the input array is used to set the corresponding position in the result array to 1. This is done efficiently using numpy indexing.\n- **Conversion to List**: The final result is converted to a Python list using `tolist()` to meet the problem's output requirements.\n\nThis approach ensures that the function handles both provided and automatically determined column counts efficiently, while correctly transforming the input array into its one-hot encoded form."}
{"task_id": 35, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a 1D numpy array into a diagonal matrix. A diagonal matrix is a square matrix where all the elements outside the main diagonal are zero, and the elements on the main diagonal are from the given 1D array.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Understand the Problem**: We need to transform a 1D array into a 2D diagonal matrix. The diagonal matrix should have the same length as the input array, with the elements of the array placed along the main diagonal.\n2. **Use numpy's diag Function**: The numpy library provides a convenient function `np.diag` that directly converts a 1D array into a diagonal matrix. This function creates a square matrix of size n x n (where n is the length of the input array) with the elements of the array on the main diagonal and zeros elsewhere.\n3. **Convert to Python List**: After creating the diagonal matrix using `np.diag`, we convert the resulting numpy array into a Python list using the `tolist()` method.\n\nThis approach is efficient and concise, leveraging numpy's built-in functions to achieve the desired result with minimal code.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    return np.diag(x).tolist()\n```\n\n### Explanation\n1. **Function Definition**: The function `make_diagonal` takes a single argument `x`, which is a 1D numpy array.\n2. **Create Diagonal Matrix**: Using `np.diag(x)`, we create a diagonal matrix where the elements of `x` form the main diagonal.\n3. **Convert to List**: The resulting diagonal matrix (a numpy array) is converted to a Python list using the `tolist()` method before being returned.\n\nThis solution efficiently handles the conversion and ensures the output is in the required format. The use of numpy's optimized functions ensures that the solution is both concise and performant."}
{"task_id": 36, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the accuracy score of a model's predictions. The accuracy score is a measure of how well the model's predictions match the true labels. It is defined as the number of correct predictions divided by the total number of predictions.\n\n### Approach\n1. **Compare True and Predicted Labels**: We start by comparing each element of the true labels array (`y_true`) with the corresponding element of the predicted labels array (`y_pred`). This comparison results in a boolean array where each entry is `True` if the corresponding elements in `y_true` and `y_pred` are equal, and `False` otherwise.\n2. **Count Correct Predictions**: Using the boolean array, we sum up all the `True` values. In numpy, `True` is treated as 1 and `False` as 0 when summed, so this sum gives the count of correct predictions.\n3. **Calculate Total Predictions**: The total number of predictions is simply the number of elements in the `y_true` array, which can be obtained using the `size` attribute.\n4. **Compute Accuracy**: The accuracy is then calculated by dividing the count of correct predictions by the total number of predictions.\n5. **Round the Result**: Finally, we round the computed accuracy to four decimal places to meet the specified precision requirement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    correct = (y_true == y_pred)\n    sum_correct = np.sum(correct)\n    total = y_true.size\n    accuracy = sum_correct / total\n    return round(accuracy, 4)\n```\n\n### Explanation\n- **Comparison of Arrays**: The comparison `(y_true == y_pred)` generates a boolean array indicating where the true and predicted values match.\n- **Summing Correct Predictions**: Using `np.sum(correct)`, we count how many times the predictions were correct.\n- **Total Predictions**: The total number of elements in `y_true` is obtained using `y_true.size`.\n- **Accuracy Calculation**: Dividing the count of correct predictions by the total number of predictions gives the accuracy score.\n- **Rounding**: The result is rounded to four decimal places using `round(accuracy, 4)` to ensure the output meets the specified precision.\n\nThis approach efficiently computes the accuracy score using numpy operations, ensuring both correctness and optimal performance."}
{"task_id": 37, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that calculates the correlation matrix for a given dataset. The function should handle both cases where a second dataset is provided and where it is not. The result should be a matrix of correlation coefficients rounded to four decimal places, returned as a Python list.\n\n### Approach\n1. **Problem Analysis**: The task is to compute the correlation matrix between two datasets, X and Y. If Y is not provided, the function should compute the correlation matrix of X with itself. The correlation matrix is a square matrix where each element represents the correlation coefficient between two variables.\n2. **Intuition**: The correlation coefficient measures the strength and direction of a linear relationship between two variables. Using numpy's `corrcoef` function simplifies the computation as it directly provides the correlation matrix for given input arrays.\n3. **Algorithm Selection**: We use numpy's `corrcoef` function because it efficiently computes the correlation matrix. If Y is not provided, we set Y to be the same as X, ensuring the function computes the correlation matrix for X with itself.\n4. **Rounding and Conversion**: After computing the correlation matrix, we round each element to four decimal places using numpy's `round` function and convert the result to a Python list using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    corr = np.corrcoef(X, Y)\n    rounded = np.round(corr, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Input Handling**: The function takes two parameters, X and Y. If Y is not provided (i.e., Y is None), it is set to X, ensuring the correlation matrix is computed for X against itself.\n2. **Correlation Matrix Calculation**: Using `np.corrcoef(X, Y)`, we compute the correlation matrix. This function returns a matrix where each element [i, j] is the correlation coefficient between the i-th column of X and the j-th column of Y.\n3. **Rounding**: The computed correlation matrix is rounded to four decimal places using `np.round(corr, 4)`.\n4. **Conversion to List**: The rounded matrix is converted to a Python list using the `tolist()` method, ensuring the output is in the required format.\n\nThis approach efficiently handles both cases where Y is provided or not, computes the correlation matrix, and ensures the result is formatted correctly."}
{"task_id": 39, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the log-softmax function, which is a common technique used in machine learning for numerical stability when dealing with large numbers. The log-softmax function converts a vector of scores into probabilities by taking the logarithm of the softmax function.\n\n### Approach\nThe approach to compute the log-softmax involves the following steps:\n\n1. **Compute the Maximum Score**: To prevent overflow when computing exponentials, we first find the maximum score in the input array and subtract this value from each score. This ensures that the largest exponent is 0, avoiding very large numbers.\n\n2. **Shift Scores**: Subtract the maximum score from each element in the input array to get shifted scores.\n\n3. **Compute Exponentials**: Calculate the exponentials of these shifted scores.\n\n4. **Sum of Exponentials**: Compute the sum of these exponentials.\n\n5. **Logarithm of Sum**: Take the natural logarithm of the sum of exponentials.\n\n6. **Compute Log-Softmax**: For each element, subtract the logarithm of the sum from the shifted score to get the log-softmax value.\n\n7. **Rounding and Conversion**: Round the results to the nearest fourth decimal and convert the result to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    max_score = scores_array.max()\n    shifted_scores = scores_array - max_score\n    exp_shifted = np.exp(shifted_scores)\n    sum_exp = np.sum(exp_shifted)\n    log_sum = np.log(sum_exp)\n    log_softmax = shifted_scores - log_sum\n    rounded = np.round(log_softmax, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Compute Maximum Score**: The maximum score is found using `np.max(scores_array)`. This helps in preventing overflow by normalizing the scores.\n\n2. **Shift Scores**: Each score is reduced by the maximum score to create shifted scores, which helps in stabilizing the computation of exponentials.\n\n3. **Exponentials and Sum**: The shifted scores are exponentiated and summed up. This sum is used to compute the denominator in the softmax function.\n\n4. **Logarithm of Sum**: The natural logarithm of the sum of exponentials is computed to avoid dealing with very large numbers directly.\n\n5. **Log-Softmax Calculation**: Each shifted score is adjusted by subtracting the logarithm of the sum, resulting in the log-softmax values.\n\n6. **Rounding and Conversion**: The results are rounded to four decimal places for precision and converted to a list for the final output.\n\nThis approach ensures numerical stability and efficient computation of the log-softmax function, making it suitable for large input arrays."}
{"task_id": 40, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a custom Dense layer for a neural network in Python. The Dense layer, also known as a fully connected layer, is a fundamental component in neural networks where each neuron is connected to every neuron in the previous layer. This implementation extends a base `Layer` class and includes methods for weight initialization, parameter counting, forward and backward passes, and output shape determination.\n\n### Approach\n1. **Weight Initialization**: The weights are initialized using a uniform distribution with a limit of \\(1 / \\sqrt{\\text{input\\_shape}[0]}\\). The bias terms are initialized to zero. Additionally, optimizers for both weights and biases are set up using a simple SGD (Stochastic Gradient Descent) optimizer.\n\n2. **Parameter Count**: This method returns the total number of trainable parameters in the layer, which includes both the weights and biases.\n\n3. **Forward Pass**: This method computes the output of the layer by performing a dot product between the input and the weight matrix, then adding the bias vector.\n\n4. **Backward Pass**: This method calculates the gradient with respect to the input and updates the weights and biases using the optimizer's update rule.\n\n5. **Output Shape**: This method returns the shape of the output produced by the forward pass.\n\n### Solution Code\n```python\nimport numpy as np\n\nclass Optimizer:\n    def __init__(self, learning_rate=0.01):\n        self.lr = learning_rate\n    \n    def update(self, param, grad):\n        param -= self.lr * grad\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n    \n    def layer_name(self):\n        return self.__class__.__name__\n    \n    def parameters(self):\n        return 0\n    \n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n    \n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n    \n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n    \n    def initialize(self):\n        if self.input_shape is None:\n            raise ValueError(\"Input shape must be set before initializing the layer.\")\n        \n        limit = 1 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((self.n_units,))\n        \n        self.optimizer_W = Optimizer()\n        self.optimizer_w0 = Optimizer()\n    \n    def parameters(self):\n        if self.W is None or self.w0 is None:\n            return 0\n        return self.W.size + self.w0.size\n    \n    def forward_pass(self, X, training):\n        self.layer_input = X\n        output = X.dot(self.W) + self.w0\n        return output\n    \n    def backward_pass(self, accum_grad):\n        if self.trainable:\n            grad_W = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0)\n            \n            self.optimizer_W.update(self.W, grad_W)\n            self.optimizer_w0.update(self.w0, grad_w0)\n        \n        dX = accum_grad.dot(self.W.T)\n        return dX\n    \n    def output_shape(self):\n        return (self.n_units,)\n```\n\n### Explanation\n1. **Weight Initialization**: The `initialize` method sets up the weights and biases. Weights are initialized using a uniform distribution to ensure that the variance of the activations remains consistent across layers. Biases are initialized to zero.\n\n2. **Parameter Count**: The `parameters` method calculates the total number of trainable parameters by summing the number of elements in the weight matrix and the bias vector.\n\n3. **Forward Pass**: The `forward_pass` method computes the output of the layer using matrix multiplication of the input and weights, followed by the addition of the bias vector.\n\n4. **Backward Pass**: The `backward_pass` method computes the gradients of the loss with respect to the weights and biases using the chain rule. It then updates the weights and biases using the SGD optimizer.\n\n5. **Output Shape**: The `output_shape` method returns the shape of the output produced by the layer, which is determined by the number of neurons in the layer.\n\nThis implementation provides a basic but functional Dense layer that can be integrated into a neural network framework, supporting both forward and backward propagation with weight updates."}
{"task_id": 41, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a 2D convolutional layer in Python. This function will process an input matrix using a specified convolutional kernel, padding, and stride. The goal is to slide the kernel over the input matrix, compute the dot product at each position, and return the result as a list of values rounded to the nearest fourth decimal.\n\n### Approach\n1. **Padding the Input**: The input matrix is padded with zeros on all sides to handle the boundaries. This ensures that the kernel can slide over the edges of the input matrix smoothly.\n2. **Compute Output Dimensions**: The output dimensions are calculated based on the padded input size, kernel size, and stride. This determines how many positions the kernel will be slid over.\n3. **Initialize Output Matrix**: An output matrix of zeros is initialized to store the results of the convolution.\n4. **Sliding the Kernel**: For each position in the output matrix, extract the corresponding submatrix from the padded input, compute the dot product with the kernel, and store the result in the output matrix.\n5. **Rounding and Conversion**: The results are rounded to four decimal places and converted to a Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Pad the input matrix with zeros on all sides\n    padded = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    \n    # Get the dimensions of the kernel\n    kernel_height, kernel_width = kernel.shape\n    \n    # Calculate the height and width of the padded input\n    padded_height, padded_width = padded.shape\n    \n    # Compute the output dimensions\n    output_height = (padded_height - kernel_height) // stride + 1\n    output_width = (padded_width - kernel_width) // stride + 1\n    \n    # Initialize the output matrix with zeros\n    output = np.zeros((output_height, output_width))\n    \n    # Iterate over each position in the output\n    for i in range(output_height):\n        for j in range(output_width):\n            # Calculate the starting and ending indices for the submatrix\n            start_row = i * stride\n            end_row = start_row + kernel_height\n            start_col = j * stride\n            end_col = start_col + kernel_width\n            \n            # Extract the submatrix from the padded input\n            submatrix = padded[start_row:end_row, start_col:end_col]\n            \n            # Compute the convolution value by element-wise multiplication and summation\n            conv_value = np.sum(submatrix * kernel)\n            \n            # Assign the computed value to the output matrix\n            output[i, j] = conv_value\n    \n    # Round the output values to the nearest fourth decimal place\n    output = np.round(output, 4)\n    \n    # Convert the numpy array to a Python list and return\n    return output.tolist()\n```\n\n### Explanation\n1. **Padding**: The input matrix is padded with zeros to ensure the kernel can slide over the edges without index errors. This is done using `np.pad`.\n2. **Output Dimensions**: The output height and width are calculated using the formula `(padded_size - kernel_size) // stride + 1` for both dimensions.\n3. **Sliding Window**: For each position in the output matrix, a submatrix of the same size as the kernel is extracted from the padded input. The dot product of this submatrix and the kernel is computed and stored in the output matrix.\n4. **Rounding and Conversion**: The results are rounded to four decimal places and converted to a list using `numpy.tolist()` for the final output.\n\nThis approach efficiently handles the convolution operation with given padding and stride, ensuring accurate results for various input and kernel configurations."}
{"task_id": 42, "completion_id": 0, "completion": "\n\nTo implement the ReLU activation function, we need a function that returns the input value if it is positive and zero otherwise. This can be efficiently achieved using a conditional expression.\n\n**Step-by-step Explanation:**\n\n1. **Understand ReLU Function**: The ReLU function outputs the input if it is greater than zero, otherwise, it outputs zero.\n2. **Function Definition**: Define a function `relu` that takes a single float argument `z` and returns a float.\n3. **Conditional Check**: Use a conditional expression to check if `z` is greater than zero. If true, return `z`; otherwise, return 0.\n\n**Code:**\n\n```python\ndef relu(z: float) -> float:\n    return z if z > 0 else 0\n```\n\nThis implementation efficiently checks the condition and returns the appropriate value, handling all possible cases for the input `z`."}
{"task_id": 43, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Ridge Regression loss function, which combines the Mean Squared Error (MSE) with a regularization term. This function will help in evaluating the performance of a model while penalizing large weights to prevent overfitting.\n\n### Approach\nThe Ridge Regression loss function is designed to minimize both the error between predicted and actual values and the magnitude of the model's coefficients. The loss is calculated as the sum of the Mean Squared Error (MSE) and a regularization term.\n\n1. **Mean Squared Error (MSE)**: This is calculated as the average of the squared differences between the predicted values and the true values. The predicted values are obtained by multiplying the feature matrix \\(X\\) with the coefficient vector \\(w\\).\n2. **Regularization Term**: This term is added to the loss to penalize large weights. It is computed as the sum of the squares of the coefficients multiplied by a regularization parameter \\(\\alpha\\).\n\nThe total loss is the sum of the MSE and the regularization term. The result is then rounded to four decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    errors = y_true - y_pred\n    squared_errors = errors ** 2\n    mse = np.mean(squared_errors)\n    reg_term = alpha * np.sum(w ** 2)\n    loss = mse + reg_term\n    return round(loss, 4)\n```\n\n### Explanation\n1. **Compute Predictions**: The predicted values \\(y_{\\text{pred}}\\) are obtained using the matrix multiplication of the feature matrix \\(X\\) and the coefficient vector \\(w\\).\n2. **Calculate Errors**: The errors are the differences between the true labels \\(y_{\\text{true}}\\) and the predicted values \\(y_{\\text{pred}}\\).\n3. **Compute Squared Errors**: Each error is squared to compute the squared errors.\n4. **Mean Squared Error (MSE)**: The mean of the squared errors is calculated to get the MSE.\n5. **Regularization Term**: The sum of the squares of the coefficients \\(w\\) is computed and multiplied by the regularization parameter \\(\\alpha\\).\n6. **Total Loss**: The total loss is the sum of the MSE and the regularization term.\n7. **Rounding**: The result is rounded to four decimal places to ensure precision.\n\nThis approach ensures that the model not only fits the training data well (minimizing MSE) but also keeps the model weights small (minimizing the regularization term), thus preventing overfitting."}
{"task_id": 44, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function. This function is a variation of the Rectified Linear Unit (ReLU) activation function, which is commonly used in neural networks. The key difference is that Leaky ReLU allows a small gradient for negative values, which can help in training deep neural networks by preventing the \"dying ReLU\" problem.\n\n### Approach\nThe Leaky ReLU function is defined as follows:\n- If the input value \\( z \\) is positive or zero, the function returns \\( z \\) as is.\n- If the input value \\( z \\) is negative, the function returns \\( \\alpha \\times z \\), where \\( \\alpha \\) is a small positive slope (default value is 0.01).\n\nThe function takes two parameters:\n1. \\( z \\): The input value, which is a float.\n2. \\( \\alpha \\): The slope for negative values, which is an optional float with a default value of 0.01.\n\nThe function returns the result after applying the Leaky ReLU transformation.\n\n### Solution Code\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    if z >= 0:\n        return z\n    else:\n        return alpha * z\n```\n\n### Explanation\n1. **Input Handling**: The function takes a float \\( z \\) and an optional float \\( \\alpha \\) with a default value of 0.01.\n2. **Condition Check**: The function checks if \\( z \\) is non-negative. If true, it returns \\( z \\) directly.\n3. **Negative Handling**: If \\( z \\) is negative, the function returns the product of \\( \\alpha \\) and \\( z \\), allowing a small gradient for negative values.\n\nThis approach ensures that the function behaves like ReLU for positive values and provides a small slope for negative values, thus implementing the Leaky ReLU activation function effectively."}
{"task_id": 45, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the linear kernel between two input vectors. The linear kernel is defined as the dot product (or inner product) of the two vectors. This can be efficiently calculated using numpy's `dot` function.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Import the numpy library**: This is necessary because we will use numpy's `dot` function to compute the dot product of the two vectors.\n2. **Define the function**: The function `kernel_function` takes two parameters, `x1` and `x2`, which are the input vectors.\n3. **Compute the dot product**: Using numpy's `dot` function, we compute the dot product of `x1` and `x2` and return the result.\n\nThis approach is efficient because numpy's `dot` function is optimized for performance and handles the computation of the dot product in a concise manner.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)\n```\n\n### Explanation\n- **Importing numpy**: The numpy library is imported to utilize its efficient numerical operations, specifically the `dot` function.\n- **Function Definition**: The function `kernel_function` is defined to take two parameters, `x1` and `x2`, which are expected to be numpy arrays.\n- **Dot Product Calculation**: The `np.dot(x1, x2)` function call computes the dot product of the two vectors. The dot product is the sum of the products of the corresponding entries of the two sequences of numbers, which is exactly what the linear kernel requires.\n\nThis solution is efficient and concise, leveraging numpy's capabilities to perform the necessary computation in an optimal manner."}
{"task_id": 46, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the precision metric for binary classification using two numpy arrays: `y_true` (true labels) and `y_pred` (predicted labels). Precision is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP).\n\n### Approach\n1. **True Positives (TP)**: These are the cases where both the true label and the predicted label are 1. We can compute TP by summing the element-wise product of `y_true` and `y_pred`.\n2. **False Positives (FP)**: These are the cases where the true label is 0 but the predicted label is 1. We can compute FP by summing the values of `y_pred` where `y_true` is 0.\n3. **Precision Calculation**: Precision is calculated as the ratio of TP to the sum of TP and FP. If the sum of TP and FP is zero (indicating no positive predictions), we return 0 to avoid division by zero.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    tp = np.sum(y_true * y_pred)\n    fp = np.sum(y_pred[y_true == 0])\n    denominator = tp + fp\n    if denominator == 0:\n        return 0.0\n    return tp / denominator\n```\n\n### Explanation\n1. **True Positives (TP)**: By multiplying `y_true` and `y_pred` element-wise and summing the result, we get the count of true positives.\n2. **False Positives (FP)**: By summing the values of `y_pred` where `y_true` is 0, we get the count of false positives.\n3. **Precision Calculation**: The precision is then computed as the ratio of TP to the sum of TP and FP. If there are no positive predictions (i.e., TP + FP = 0), the function returns 0 to handle the division by zero case.\n\nThis approach efficiently computes the precision metric using basic numpy operations, ensuring clarity and correctness in the calculation."}
{"task_id": 47, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that can perform three variants of gradient descent\u2014Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent\u2014using Mean Squared Error (MSE) as the loss function. The function will take an additional parameter to specify which variant to use.\n\n### Approach\n1. **Problem Analysis**: The task involves implementing gradient descent variants to minimize the MSE loss. Each variant differs in how it selects the data batch for gradient computation:\n   - **Batch Gradient Descent**: Uses the entire dataset in each iteration.\n   - **Stochastic Gradient Descent (SGD)**: Uses a single example in each iteration.\n   - **Mini-Batch Gradient Descent**: Uses a small batch of examples in each iteration.\n\n2. **Mathematical Insight**: The gradient of the MSE loss with respect to the weights is computed as the dot product of the transpose of the feature matrix and the error vector, scaled by the number of examples in the batch.\n\n3. **Algorithm**:\n   - Initialize the weights.\n   - For each iteration, select the appropriate batch based on the method.\n   - Compute predictions, errors, and the gradient.\n   - Update the weights using the gradient and learning rate.\n   - Return the weights rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    weights = weights.copy()\n    n_samples = X.shape[0]\n    for _ in range(n_iterations):\n        if method == 'batch':\n            X_batch = X\n            y_batch = y\n        elif method == 'stochastic':\n            idx = np.random.randint(0, n_samples)\n            X_batch = X[idx:idx+1, :]\n            y_batch = y[idx:idx+1]\n        else:  # mini-batch\n            idx = np.random.choice(n_samples, batch_size, replace=False)\n            X_batch = X[idx, :]\n            y_batch = y[idx]\n        \n        predictions = np.dot(X_batch, weights)\n        errors = predictions - y_batch\n        gradient = np.dot(X_batch.T, errors) / len(y_batch)\n        weights -= learning_rate * gradient\n    \n    weights_rounded = np.round(weights, 4)\n    return weights_rounded.tolist()\n```\n\n### Explanation\n- **Initialization**: The function starts by copying the initial weights to avoid modifying the original array.\n- **Iteration Loop**: For each iteration, the function selects the appropriate batch of data based on the specified method.\n  - **Batch Method**: Uses the entire dataset.\n  - **Stochastic Method**: Randomly selects a single example.\n  - **Mini-Batch Method**: Randomly selects a batch of specified size.\n- **Gradient Computation**: For the selected batch, the function computes the predictions, errors, and the gradient of the loss with respect to the weights.\n- **Weight Update**: The weights are updated by subtracting the product of the learning rate and the gradient.\n- **Result**: After completing the iterations, the weights are rounded to four decimal places and returned as a list.\n\nThis approach ensures that the function efficiently computes the weights using the specified gradient descent variant, providing a flexible solution for different scenarios."}
{"task_id": 49, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Adam optimization algorithm, which is a popular stochastic gradient descent algorithm that adapts the learning rate for each parameter. The algorithm uses estimates of the first and second moments of the gradient to update the parameters iteratively.\n\n### Approach\nThe Adam optimization algorithm can be broken down into the following steps:\n\n1. **Initialization**: Start with the initial parameter values and initialize the first and second moment estimates to zero.\n2. **Iterative Updates**: For each iteration, compute the gradient of the objective function at the current parameter values. Update the first and second moment estimates using exponential decay rates.\n3. **Bias Correction**: Correct the bias in the moment estimates to account for the fact that the initial estimates are zero.\n4. **Parameter Update**: Use the bias-corrected moment estimates to update the parameters. The update step normalizes the gradient by the square root of the second moment estimate, ensuring stable updates.\n5. **Return Result**: After completing the specified number of iterations, return the optimized parameters rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    x = np.array(x0, copy=True)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    \n    for i in range(num_iterations):\n        t = i + 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        \n        x = x - learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon))\n    \n    x = np.round(x, 4)\n    return x.tolist()\n```\n\n### Explanation\n1. **Initialization**: The parameters `x` are initialized from `x0`, and the moment estimates `m` and `v` are initialized to zero arrays of the same shape as `x`.\n2. **Gradient Computation**: For each iteration, the gradient `g` of the objective function at the current parameter values is computed using the provided `grad` function.\n3. **Moment Updates**: The first moment estimate `m` is updated using the current gradient and the decay rate `beta1`. Similarly, the second moment estimate `v` is updated using the square of the gradient and the decay rate `beta2`.\n4. **Bias Correction**: The moment estimates are bias-corrected by dividing them by `(1 - beta1^t)` and `(1 - beta2^t)`, where `t` is the current iteration.\n5. **Parameter Update**: The parameters are updated using the bias-corrected moments. The update step is normalized by the square root of the second moment estimate plus a small constant `epsilon` for numerical stability.\n6. **Result**: After completing the iterations, the parameters are rounded to four decimal places and returned as a list.\n\nThis approach ensures that the parameters are optimized efficiently using the Adam algorithm, which adapts the learning rate for each parameter based on the gradient's moment estimates."}
{"task_id": 50, "completion_id": 0, "completion": "\n\nTo implement Lasso Regression using Gradient Descent, we follow these steps:\n\n1. **Initialize Weights and Bias**: Start with random weights and bias.\n2. **Iterative Gradient Descent**: For each iteration, compute the predicted values, errors, and gradients. Update the weights and bias using the gradients.\n3. **L1 Regularization**: Add the gradient from the L1 penalty to the weight gradient.\n4. **Convergence Check**: Stop iterations if the change in weights and bias is below a tolerance level.\n5. **Rounding**: Round the final weights and bias to four decimal places for the result.\n\nHere is the Python code implementing these steps:\n\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    n, p = X.shape\n    w = np.random.randn(p)\n    b = np.random.randn()\n    \n    for _ in range(max_iter):\n        y_hat = np.dot(X, w) + b\n        error = y - y_hat\n        \n        grad_w = (-np.dot(X.T, error)) / n\n        grad_b = (-np.sum(error)) / n\n        \n        # Add L1 regularization gradient\n        grad_w += alpha * np.sign(w)\n        \n        # Update parameters\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n        \n        # Check for convergence\n        prev_w = w.copy()\n        prev_b = b\n        \n        # Compute changes\n        change_w = np.abs(w - prev_w)\n        change_b = np.abs(b - prev_b)\n        \n        max_change = max(np.max(change_w), change_b)\n        \n        if max_change < tol:\n            break\n    \n    # Round the results to 4 decimal places\n    w = np.round(w, 4)\n    b = np.round(b, 4)\n    \n    return (w.tolist(), b)\n```\n\nThis code initializes the model parameters, iteratively updates them using gradient descent with L1 regularization, checks for convergence, and returns the rounded coefficients."}
{"task_id": 51, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Optimal String Alignment (OSA) distance between two given strings. The OSA distance represents the minimum number of edits required to transform one string into another, where the allowed edit operations are insert, delete, substitute, and transpose adjacent characters. Each operation costs 1 unit.\n\n### Approach\nThe approach to solve this problem involves using dynamic programming (DP) to build a table that stores the minimum edit costs for substrings of the source and target strings. The key steps are as follows:\n\n1. **Initialize a DP Table**: Create a 2D table where `DP[i][j]` represents the minimum number of edits needed to convert the first `i` characters of the source string to the first `j` characters of the target string.\n\n2. **Base Cases**: \n   - If the source string is empty, the cost to convert it to the target string is simply the length of the target string (all insertions).\n   - If the target string is empty, the cost is the length of the source string (all deletions).\n\n3. **Fill the DP Table**: For each character in the source and target strings, compute the minimum cost considering four possible operations:\n   - **Delete**: The cost to delete a character from the source string.\n   - **Insert**: The cost to insert a character into the source string.\n   - **Substitute**: The cost to replace a character in the source string with a character from the target string.\n   - **Transpose**: The cost to swap two adjacent characters in the source string, if applicable.\n\n4. **Transpose Check**: For each pair of characters, check if transposing two adjacent characters in the source string can match the corresponding characters in the target string. If so, consider this operation in the cost calculation.\n\n### Solution Code\n```python\ndef OSA(source: str, target: str) -> int:\n    m = len(source)\n    n = len(target)\n    # Create a DP table with (m+1) rows and (n+1) columns\n    DP = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    # Initialize the base cases\n    for i in range(m + 1):\n        DP[i][0] = i  # Deleting all characters from source\n    for j in range(n + 1):\n        DP[0][j] = j  # Inserting all characters into source\n    \n    # Fill the DP table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            # Calculate substitution cost\n            sub_cost = 0 if source[i-1] == target[j-1] else 1\n            \n            # Option 1: Delete the current character in source\n            option1 = DP[i-1][j] + 1\n            # Option 2: Insert the current character in target\n            option2 = DP[i][j-1] + 1\n            # Option 3: Substitute the current characters\n            option3 = DP[i-1][j-1] + sub_cost\n            # Option 4: Transpose adjacent characters if applicable\n            option4 = float('inf')\n            if i >= 2 and j >= 2:\n                if source[i-1] == target[j-2] and source[i-2] == target[j-1]:\n                    option4 = DP[i-2][j-2] + 1\n            \n            # Choose the minimum cost among all options\n            DP[i][j] = min(option1, option2, option3, option4)\n    \n    return DP[m][n]\n```\n\n### Explanation\nThe solution uses dynamic programming to efficiently compute the minimum edit distance by considering all possible operations (insert, delete, substitute, transpose) and selecting the one with the minimum cost at each step. The DP table is filled in a bottom-up manner, ensuring that each cell `DP[i][j]` is computed based on previously computed values, leading to an optimal solution. This approach ensures that we explore all possible ways to transform the source string into the target string with the minimum number of edits."}
{"task_id": 52, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the recall metric for binary classification. Recall measures how effectively a model identifies positive instances from all the actual positive cases in a dataset. It is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN).\n\n### Approach\n1. **Understanding Recall**: Recall is defined as the ratio of true positives to the sum of true positives and false negatives. Mathematically, it is represented as:\n   \\[\n   \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n   \\]\n   where TP is the count of true positives and FN is the count of false negatives.\n\n2. **Counting TP and FN**: We iterate through the lists of true labels (`y_true`) and predicted labels (`y_pred`). For each pair:\n   - If the true label is 1 and the predicted label is 1, it is a true positive (TP).\n   - If the true label is 1 and the predicted label is 0, it is a false negative (FN).\n\n3. **Handling Division by Zero**: If the sum of TP and FN is zero, we return 0.0 to avoid division by zero.\n\n4. **Rounding the Result**: The result is rounded to three decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    TP = 0\n    FN = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1:\n            if yp == 1:\n                TP += 1\n            else:\n                FN += 1\n    denominator = TP + FN\n    if denominator == 0:\n        return 0.0\n    else:\n        rec = TP / denominator\n        return round(rec, 3)\n```\n\n### Explanation\n- **Initialization**: We start by initializing counters for true positives (TP) and false negatives (FN) to zero.\n- **Iteration**: We loop through each pair of true and predicted labels. For each pair where the true label is 1, we check the predicted label to update TP or FN accordingly.\n- **Denominator Check**: After counting, we check if the sum of TP and FN is zero to avoid division by zero, returning 0.0 if true.\n- **Recall Calculation**: If the denominator is non-zero, we compute the recall as the ratio of TP to the sum of TP and FN, then round the result to three decimal places.\n\nThis approach ensures that we accurately compute the recall metric while handling edge cases such as division by zero."}
{"task_id": 53, "completion_id": 0, "completion": "\n\nTo implement the self-attention mechanism, we follow these steps:\n\n1. **Compute Queries, Keys, and Values**: Multiply the input matrix `X` with the weight matrices `W_q`, `W_k`, and `W_v` to obtain `Q`, `K`, and `V` respectively.\n2. **Compute Attention Scores**: Calculate the dot product of `Q` and the transpose of `K`, then scale by the square root of the dimension size to prevent large values.\n3. **Apply Softmax**: Convert the attention scores into probabilities using the softmax function.\n4. **Compute Output**: Multiply the softmax weights with `V` to get the final output.\n5. **Round and Convert**: Round the output to four decimal places and convert it to a Python list.\n\nHere is the code:\n\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    d = W_q.shape[1]\n    scale = 1.0 / np.sqrt(d)\n    scores = np.dot(Q, K.T) * scale\n    \n    exp_scores = np.exp(scores)\n    sum_exp = np.sum(exp_scores, axis=1, keepdims=True)\n    weights = exp_scores / sum_exp\n    \n    output = np.dot(weights, V)\n    rounded_output = np.round(output, 4)\n    \n    return rounded_output.tolist()\n```\n\nThis function computes the self-attention mechanism as described, ensuring the output is rounded and converted to a list."}
{"task_id": 54, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simple Recurrent Neural Network (RNN) cell that processes a sequence of input vectors and produces the final hidden state. The RNN uses the tanh activation function for updating the hidden states. The function takes the input sequence, initial hidden state, weight matrices, and bias vector as inputs and returns the final hidden state rounded to four decimal places.\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Since the function parameters are provided as lists, we first convert them into numpy arrays for efficient matrix operations.\n2. **Iterate Over Input Sequence**: For each input vector in the sequence, compute the preactivation using the formula: `preact = Wx @ x_t + Wh @ h_prev + b`.\n3. **Apply Activation Function**: Use the tanh activation function on the preactivation to update the hidden state.\n4. **Update Hidden State**: After processing each input vector, update the hidden state to the newly computed value.\n5. **Round and Convert Result**: After processing all input vectors, round the final hidden state to four decimal places and convert it back to a list using numpy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Convert initial hidden state and weights to numpy arrays\n    h_prev = np.array(initial_hidden_state, dtype=np.float32)\n    Wx = np.array(Wx, dtype=np.float32)\n    Wh = np.array(Wh, dtype=np.float32)\n    b = np.array(b, dtype=np.float32)\n    \n    # Iterate through each input vector in the sequence\n    for x_t in input_sequence:\n        x_t = np.array(x_t, dtype=np.float32)\n        # Compute the preactivation\n        preact = np.dot(Wx, x_t) + np.dot(Wh, h_prev) + b\n        # Apply tanh activation function\n        h_current = np.tanh(preact)\n        # Update the hidden state for the next time step\n        h_prev = h_current\n    \n    # Round the final hidden state to four decimal places\n    h_final = np.round(h_prev, 4)\n    # Convert numpy array back to a list\n    return h_final.tolist()\n```\n\n### Explanation\n- **Conversion to Numpy Arrays**: The function starts by converting the initial hidden state, weight matrices, and bias vector into numpy arrays to facilitate matrix operations.\n- **Processing Each Input Vector**: For each input vector in the sequence, the function computes the preactivation by multiplying the input vector with the input-to-hidden weight matrix (Wx), adding the product of the previous hidden state with the hidden-to-hidden weight matrix (Wh), and then adding the bias vector (b).\n- **Tanh Activation**: The preactivation is passed through the tanh activation function to produce the new hidden state.\n- **Updating Hidden State**: The new hidden state is used as the previous hidden state for the next iteration.\n- **Rounding and Conversion**: After processing all input vectors, the final hidden state is rounded to four decimal places and converted back to a list for the output.\n\nThis approach ensures that the RNN processes each input vector correctly, updating the hidden state at each time step and producing the final hidden state after the entire sequence is processed."}
{"task_id": 55, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that applies a 2D translation to a set of points. The translation will move each point by a specified distance in the x and y directions. \n\n### Approach\nThe approach involves using numpy to efficiently perform the translation operation. Here are the key steps:\n\n1. **Convert Points to Numpy Array**: Convert the input list of points into a numpy array for efficient vectorized operations.\n2. **Create Translation Vector**: Define a translation vector using the given translation distances `tx` and `ty`.\n3. **Apply Translation**: Add the translation vector to each point in the numpy array. This operation is efficiently handled by numpy's broadcasting, ensuring that each point is translated correctly.\n4. **Convert Back to List**: Convert the resulting numpy array back to a Python list using numpy's `tolist()` method.\n\nThis approach ensures that the translation is applied efficiently even for a large number of points.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    points_array = np.array(points)\n    translated = points_array + np.array([tx, ty])\n    return translated.tolist()\n```\n\n### Explanation\n1. **Convert Points to Numpy Array**: The input list `points` is converted into a numpy array `points_array` to leverage numpy's efficient array operations.\n2. **Create Translation Vector**: The translation vector is created as a numpy array `[tx, ty]`.\n3. **Apply Translation**: The translation vector is added to `points_array`. Numpy's broadcasting ensures that each element in `points_array` is incremented by `tx` and `ty` respectively.\n4. **Convert Back to List**: The translated numpy array is converted back to a Python list using `tolist()` to meet the problem's output requirements.\n\nThis method efficiently handles the translation of points in 2D space using vectorized operations, making it both concise and performant."}
{"task_id": 56, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Kullback-Leibler (KL) divergence between two normal distributions. The KL divergence is a measure of how one probability distribution differs from a reference probability distribution. \n\n### Approach\nThe KL divergence between two normal distributions \\( P \\sim N(\\mu_P, \\sigma_P^2) \\) and \\( Q \\sim N(\\mu_Q, \\sigma_Q^2) \\) can be computed using a closed-form formula. The formula involves the means (\\(\\mu_P\\) and \\(\\mu_Q\\)) and the variances (\\(\\sigma_P^2\\) and \\(\\sigma_Q^2\\)) of the two distributions.\n\nThe formula for the KL divergence is:\n\\[ D_{KL}(P || Q) = 0.5 \\left[ \\frac{(\\mu_P - \\mu_Q)^2}{\\sigma_Q^2} + \\frac{\\sigma_P^2}{\\sigma_Q^2} - 1 \\right] \\]\n\nThis formula is derived from the properties of normal distributions and involves the following steps:\n1. Compute the squared difference of the means divided by the variance of the second distribution.\n2. Compute the ratio of the variances of the two distributions.\n3. Combine these results and adjust by subtracting 1 and then multiplying by 0.5.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    term1 = (mu_p - mu_q) ** 2 / (sigma_q ** 2)\n    term2 = (sigma_p ** 2) / (sigma_q ** 2)\n    kl = 0.5 * (term1 + term2 - 1)\n    return kl\n```\n\n### Explanation\n1. **Compute the squared difference of the means**: This term, \\((\\mu_P - \\mu_Q)^2 / \\sigma_Q^2\\), measures how far apart the means of the two distributions are, scaled by the variance of the second distribution.\n2. **Compute the ratio of the variances**: This term, \\(\\sigma_P^2 / \\sigma_Q^2\\), measures how the variances of the two distributions compare.\n3. **Combine the terms**: The combined terms are adjusted by subtracting 1 and then multiplying by 0.5 to yield the final KL divergence value.\n\nThis approach efficiently computes the KL divergence using basic arithmetic operations, making it both simple and computationally efficient."}
{"task_id": 57, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Gauss-Seidel method, an iterative technique for solving systems of linear equations. The method updates the solution vector iteratively using the most recent values available during each iteration.\n\n### Approach\nThe Gauss-Seidel method is an iterative technique used to solve a system of linear equations. The approach involves the following steps:\n\n1. **Initialization**: Start with an initial guess for the solution vector. If no initial guess is provided, use a vector of zeros.\n2. **Iteration**: For each iteration, update each element of the solution vector using the most recent values. Specifically, for each element, compute the sum of the products of the matrix elements and the current solution values, excluding the diagonal element. Update the current element using this sum.\n3. **Rounding**: After completing the specified number of iterations, round the solution vector to four decimal places.\n4. **Conversion**: Convert the solution vector from a numpy array to a Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    m = A.shape[0]\n    if x_ini is None:\n        x = np.zeros(m, dtype=np.float64)\n    else:\n        x = np.asarray(x_ini, dtype=np.float64)\n    \n    for _ in range(n):\n        for i in range(m):\n            s = 0.0\n            for j in range(m):\n                if j != i:\n                    s += A[i, j] * x[j]\n            x[i] = (b[i] - s) / A[i, i]\n    \n    x = np.round(x, 4)\n    return x.tolist()\n```\n\n### Explanation\n1. **Initialization**: The function starts by determining the size of the matrix `A` and initializing the solution vector `x`. If an initial guess `x_ini` is provided, it is used; otherwise, a vector of zeros is used.\n2. **Iteration Process**: For each iteration, the function iterates over each row of the matrix. For each row, it computes the sum of the products of the matrix elements and the current solution values, excluding the diagonal element. This sum is then used to update the current element of the solution vector.\n3. **Rounding and Conversion**: After completing the specified number of iterations, the solution vector is rounded to four decimal places and converted to a Python list using numpy's `tolist()` method.\n\nThis approach ensures that the solution vector is updated iteratively using the most recent values, which is a key feature of the Gauss-Seidel method. The method is efficient for sparse matrices and converges quickly for diagonally dominant matrices."}
{"task_id": 58, "completion_id": 0, "completion": "\n\nTo solve the task of implementing Gaussian Elimination with partial pivoting, we follow these steps:\n\n1. **Augmented Matrix Creation**: Combine the coefficient matrix `A` and the constant vector `b` into an augmented matrix for easier manipulation during elimination.\n\n2. **Partial Pivoting**: For each pivot element, find the row with the maximum absolute value in the current column (from the current row down). Swap this row with the current pivot row if necessary to ensure numerical stability.\n\n3. **Elimination**: For each pivot, eliminate the entries below it by subtracting a multiple of the pivot row from each row below, transforming the matrix into an upper triangular form.\n\n4. **Back Substitution**: Solve for the variables starting from the last equation and moving upwards, using the upper triangular matrix to compute each variable step-by-step.\n\nHere is the Python code implementing these steps:\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    aug = np.c_[A, b]\n    \n    for i in range(n):\n        # Partial pivoting to find the maximum element in column i\n        max_row = i\n        for j in range(i, n):\n            if abs(aug[j, i]) > abs(aug[max_row, i]):\n                max_row = j\n        # Swap the max row with the current row\n        if max_row != i:\n            aug[[i, max_row]] = aug[[max_row, i]]\n        \n        # Eliminate all rows below the pivot\n        for j in range(i + 1, n):\n            factor = aug[j, i] / aug[i, i]\n            aug[j] = aug[j] - factor * aug[i]\n    \n    # Back substitution to find the solution vector x\n    x = np.zeros(n, dtype=float)\n    for i in range(n - 1, -1, -1):\n        s = 0.0\n        for j in range(i + 1, n):\n            s += aug[i, j] * x[j]\n        x[i] = (aug[i, n] - s) / aug[i, i]\n    \n    # Round to 4 decimal places and return as a list\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\n```\n\nThis function takes the coefficient matrix `A` and the constant vector `b` as inputs, performs Gaussian Elimination with partial pivoting, and returns the solution vector `x` rounded to four decimal places as a Python list."}
{"task_id": 59, "completion_id": 0, "completion": "\n\nHere's the implementation of the LSTM network's forward method:\n\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states at each time step,\n        the final hidden state, and the final cell state.\n        \"\"\"\n        n_steps = x.shape[0]\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        hidden_states = []\n\n        for t in range(n_steps):\n            x_t = x[t, :].reshape(self.input_size, 1)\n            concatenated = np.vstack((x_t, h_prev))\n            \n            # Compute gates\n            f = np.sigmoid(np.dot(self.Wf, concatenated) + self.bf)\n            i = np.sigmoid(np.dot(self.Wi, concatenated) + self.bi)\n            c_candidate = np.tanh(np.dot(self.Wc, concatenated) + self.bc)\n            o = np.sigmoid(np.dot(self.Wo, concatenated) + self.bo)\n            \n            # Update cell state and hidden state\n            c_current = f * c_prev + i * c_candidate\n            h_current = o * np.tanh(c_current)\n            \n            hidden_states.append(h_current)\n            h_prev = h_current\n            c_prev = c_current\n        \n        # Prepare the output by rounding to 4 decimal places\n        hidden_states = [np.round(h.reshape(self.hidden_size), 4).tolist() for h in hidden_states]\n        final_h = np.round(h_current.reshape(self.hidden_size), 4).tolist()\n        final_c = np.round(c_current.reshape(self.hidden_size), 4).tolist()\n        \n        return hidden_states, final_h, final_c\n```\n\nThis code defines an LSTM class with an `__init__` method to initialize weights and biases, and a `forward` method to process the input sequence. The `forward` method iterates through each time step, computes the necessary gates, updates the cell and hidden states, and returns the hidden states at each time step along with the final states, all rounded to four decimal places."}
{"task_id": 60, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the TF-IDF (Term Frequency-Inverse Document Frequency) scores for a given query against a corpus of documents. The TF-IDF score is a statistical measure used to evaluate the importance of a word in a document relative to a corpus.\n\n### Approach\n1. **Check for Empty Corpus**: If the corpus is empty, return an empty list immediately.\n2. **Compute Document Frequencies (DF)**: For each word in the query, count how many documents in the corpus contain that word. This helps in calculating the Inverse Document Frequency (IDF).\n3. **Compute Inverse Document Frequency (IDF)**: Using the document frequencies, compute the IDF for each query word. The formula used is `IDF = log((N + 1) / (DF + 1))`, where `N` is the total number of documents. This formula includes smoothing to avoid division by zero.\n4. **Compute Term Frequency (TF)**: For each document, compute the TF for each query word. TF is the ratio of the count of the word in the document to the total number of words in the document.\n5. **Compute TF-IDF Scores**: For each document and each query word, compute the TF-IDF score by multiplying the TF and IDF values. Round each score to five decimal places.\n6. **Handle Edge Cases**: Gracefully handle cases such as empty documents or query words not present in the corpus.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    \n    # Calculate document frequency for each query word\n    df_dict = {}\n    for w in query:\n        count = 0\n        for doc in corpus:\n            if w in doc:\n                count += 1\n        df_dict[w] = count\n    \n    N = len(corpus)\n    idf_dict = {}\n    for w in query:\n        df = df_dict[w]\n        idf = np.log((N + 1) / (df + 1))\n        idf_dict[w] = idf\n    \n    result = []\n    for doc in corpus:\n        if not doc:\n            scores = [0.0 for _ in query]\n        else:\n            scores = []\n            for w in query:\n                count = doc.count(w)\n                tf = count / len(doc)\n                tf_idf = tf * idf_dict[w]\n                scores.append(round(tf_idf, 5))\n        result.append(scores)\n    \n    return result\n```\n\n### Explanation\n1. **Empty Corpus Check**: The function first checks if the corpus is empty and returns an empty list if true.\n2. **Document Frequency Calculation**: For each word in the query, the function counts how many documents contain the word. This count is stored in a dictionary.\n3. **IDF Calculation**: Using the document frequencies, the function computes the IDF for each query word using a smoothed formula to avoid division by zero.\n4. **TF-IDF Calculation**: For each document, the function calculates the TF for each query word. It then multiplies this TF by the precomputed IDF to get the TF-IDF score, which is rounded to five decimal places.\n5. **Edge Cases Handling**: The function handles empty documents by setting all TF-IDF scores to zero. It also handles query words not present in the corpus by setting their TF to zero, resulting in a TF-IDF score of zero.\n\nThis approach ensures that the function efficiently computes the TF-IDF scores while handling various edge cases and potential issues such as division by zero."}
{"task_id": 61, "completion_id": 0, "completion": "\n\nTo solve this task, we need to implement a function that calculates the F-Score for a binary classification task. The F-Score is a balanced measure of both Precision and Recall, and it can be adjusted using a parameter beta to weight the importance of Precision and Recall differently.\n\n### Approach\n1. **Understanding F-Score**: The F-Score is calculated using the formula:\n   \\[\n   F_{\\beta} = \\frac{(1 + \\beta^2) \\cdot (Precision \\cdot Recall)}{\\beta^2 \\cdot Precision + Recall}\n   \\]\n   where Precision is the ratio of true positives to the sum of true positives and false positives, and Recall is the ratio of true positives to the sum of true positives and false negatives.\n\n2. **Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)**: These are computed by comparing the true labels (`y_true`) and predicted labels (`y_pred`).\n\n3. **Compute Precision and Recall**: Precision is calculated as TP / (TP + FP) and Recall as TP / (TP + FN). If the denominators are zero, Precision and Recall are set to zero to avoid division by zero errors.\n\n4. **Handle Division by Zero**: When computing the F-Score, if the denominator in the formula is zero, the F-Score is set to zero to avoid division by zero errors.\n\n5. **Return the Result**: The computed F-Score is rounded to three decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate true positives, false positives, and false negatives\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Compute precision, avoiding division by zero\n    if (TP + FP) == 0:\n        precision = 0.0\n    else:\n        precision = TP / (TP + FP)\n    \n    # Compute recall, avoiding division by zero\n    if (TP + FN) == 0:\n        recall = 0.0\n    else:\n        recall = TP / (TP + FN)\n    \n    # Calculate the numerator and denominator for F-score\n    numerator = (1 + beta**2) * precision * recall\n    denominator = (beta**2 * precision) + recall\n    \n    # Handle division by zero in the F-score calculation\n    if np.isclose(denominator, 0):\n        f = 0.0\n    else:\n        f = numerator / denominator\n    \n    # Round the result to three decimal places\n    return round(f, 3)\n```\n\n### Explanation\n- **True Positives (TP)**: Count of correctly predicted positive instances.\n- **False Positives (FP)**: Count of incorrectly predicted positive instances (actual negative).\n- **False Negatives (FN)**: Count of incorrectly predicted negative instances (actual positive).\n- **Precision**: Ratio of TP to the sum of TP and FP, indicating how precise the model is.\n- **Recall**: Ratio of TP to the sum of TP and FN, indicating how well the model detects all positive instances.\n- **F-Score**: Combines Precision and Recall into a single metric, allowing adjustment of their weights using beta.\n\nThis approach ensures that the function handles edge cases such as division by zero and provides a robust measure of classification performance."}
{"task_id": 63, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Conjugate Gradient (CG) method, an efficient iterative algorithm for solving large, sparse, symmetric, positive-definite linear systems. The goal is to find the solution vector `x` for the system `Ax = b` using this method.\n\n### Approach\nThe Conjugate Gradient method is an iterative technique that efficiently solves systems of linear equations where the matrix `A` is symmetric and positive-definite. The method works by iteratively updating the solution using conjugate directions, which ensures that each update is orthogonal to the previous ones, leading to a minimum residual in each step.\n\nThe steps involved in the CG method are as follows:\n1. **Initialization**: Start with an initial guess `x0` for the solution vector. If `x0` is not provided, use a zero vector.\n2. **Compute Initial Residual**: Calculate the initial residual vector `r` as `b - Ax`.\n3. **Direction Vector**: Initialize the direction vector `p` as the initial residual `r`.\n4. **Iterative Update**: For each iteration:\n   - Compute the step size `alpha` using the residual and direction vector.\n   - Update the solution vector `x` using `alpha` and the direction vector `p`.\n   - Compute the new residual vector `r_new`.\n   - Compute the update factor `beta` for the direction vector.\n   - Update the direction vector `p` using `beta` and the new residual `r_new`.\n5. **Stopping Criteria**: Check if the norm of the residual vector is below a specified tolerance. If it is, stop the iteration early.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array = None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    \n    r = b - np.dot(A, x)\n    p = r.copy()\n    \n    for _ in range(n):\n        r_dot_r = np.dot(r, r)\n        if r_dot_r == 0:\n            break\n        \n        A_p = np.dot(A, p)\n        p_dot_A_p = np.dot(p, A_p)\n        if p_dot_A_p == 0:\n            break\n        \n        alpha = r_dot_r / p_dot_A_p\n        \n        x += alpha * p\n        \n        r_new = r - alpha * A_p\n        \n        if np.linalg.norm(r_new) < tol:\n            r = r_new\n            break\n        \n        beta = np.dot(r_new, r_new) / r_dot_r\n        p = r_new + beta * p\n        r = r_new\n    \n    x_rounded = np.round(x, 8)\n    return x_rounded.tolist()\n```\n\n### Explanation\n- **Initialization**: The function starts by initializing the solution vector `x` to `x0` if provided, otherwise to a zero vector of the same size as `b`.\n- **Residual Calculation**: The initial residual vector `r` is computed as `b - Ax`.\n- **Direction Vector**: The direction vector `p` is initialized to the residual vector `r`.\n- **Iterative Process**: For each iteration, the step size `alpha` is computed to update the solution vector `x`. The new residual `r_new` is then calculated, and the direction vector `p` is updated using the factor `beta`.\n- **Stopping Criteria**: The iteration stops early if the norm of the residual vector `r_new` falls below the specified tolerance `tol`.\n\nThis approach ensures that the solution is found efficiently, even for large systems, by leveraging the properties of symmetric positive-definite matrices and the conjugate direction method. The solution is returned as a list of values rounded to 8 decimal places."}
{"task_id": 64, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the Gini Impurity for a given set of class labels. Gini Impurity is a measure used in decision tree algorithms to determine the impurity or disorder within a node. It helps in deciding how well a split separates the training examples according to their classes.\n\n### Approach\nThe approach to calculate the Gini Impurity involves the following steps:\n\n1. **Count Class Frequencies**: Use numpy's `bincount` function to count the frequency of each class in the given list of class labels.\n2. **Calculate Probabilities**: Convert these counts into probabilities by dividing each count by the total number of elements in the list.\n3. **Compute Sum of Squared Probabilities**: Square each probability and sum them up.\n4. **Calculate Gini Impurity**: Subtract the sum of squared probabilities from 1 to get the Gini Impurity.\n5. **Round the Result**: Round the resulting Gini Impurity to three decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0  # Return 0.0 if the list is empty (though typically y is non-empty)\n    counts = np.bincount(y)\n    total = len(y)\n    sum_of_squares = np.sum((counts / total) ** 2)\n    gini = 1 - sum_of_squares\n    return round(gini, 3)\n```\n\n### Explanation\n1. **Counting Frequencies**: The function uses `np.bincount(y)` to efficiently count the occurrences of each class in the list `y`. This returns an array where each index represents a class, and the value at that index is the count of that class.\n2. **Calculating Probabilities**: Each count is divided by the total number of elements in `y` to get the probability of each class.\n3. **Sum of Squared Probabilities**: Each probability is squared, and these squared values are summed up. This sum represents the confidence in the distribution of classes.\n4. **Gini Impurity Calculation**: The Gini Impurity is calculated as 1 minus the sum of squared probabilities. This value ranges from 0 (pure node) to 1 (completely impure node).\n5. **Rounding**: The result is rounded to three decimal places to provide a clean and precise output.\n\nThis approach ensures that the Gini Impurity is computed efficiently and accurately, making it suitable for use in decision tree algorithms to evaluate the quality of splits."}
{"task_id": 65, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a given dense matrix into the Compressed Row Sparse (CSR) format. The CSR format is an efficient way to store sparse matrices by only keeping track of non-zero elements and their positions, thus reducing memory usage significantly.\n\n### Approach\nThe approach to convert a dense matrix into CSR format involves the following steps:\n\n1. **Initialize Data Structures**: Create three lists to store the non-zero values, their column indices, and the cumulative count of non-zero elements per row (row pointers).\n\n2. **Iterate Through Rows and Columns**: For each row in the dense matrix, iterate through each element. If an element is non-zero, add it to the values list and record its column index. Keep a count of non-zero elements for each row.\n\n3. **Build Row Pointers**: After processing all rows, construct the row pointers array. This array starts with 0 and each subsequent element is the cumulative sum of the counts of non-zero elements up to that row.\n\n### Solution Code\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_counts = []\n\n    for row in dense_matrix:\n        count = 0\n        for col_idx, val in enumerate(row):\n            if val != 0:\n                values.append(val)\n                column_indices.append(col_idx)\n                count += 1\n        row_counts.append(count)\n\n    row_pointers = [0]\n    current = 0\n    for count in row_counts:\n        current += count\n        row_pointers.append(current)\n\n    return (values, column_indices, row_pointers)\n```\n\n### Explanation\n1. **Initialization**: The function starts by initializing three empty lists: `values` to store non-zero elements, `column_indices` to store the column indices of these non-zero elements, and `row_counts` to keep track of the number of non-zero elements in each row.\n\n2. **Processing Each Row**: For each row in the dense matrix, the function iterates through each element. If an element is non-zero, it is added to the `values` list, its column index is added to `column_indices`, and a count of non-zero elements is maintained.\n\n3. **Building Row Pointers**: After processing all rows, the function constructs the `row_pointers` array. This array starts with 0 and each subsequent element is the cumulative sum of the counts from `row_counts`, indicating the starting index of each row in the `values` array.\n\nThis approach efficiently converts a dense matrix into the CSR format, which is particularly useful for sparse matrices as it significantly reduces memory usage by only storing non-zero elements and their positions."}
{"task_id": 66, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the orthogonal projection of a vector `v` onto another vector `L`. The orthogonal projection of `v` onto `L` is the vector on `L` that is closest to `v`. This can be achieved using the formula from linear algebra, which involves the dot product and the magnitude of the vector `L`.\n\n### Approach\n1. **Compute the Dot Product**: The dot product of vectors `v` and `L` is calculated by summing the products of their corresponding components.\n2. **Compute the Magnitude Squared**: The squared magnitude of vector `L` is computed by summing the squares of its components.\n3. **Compute the Scalar Factor**: This factor is obtained by dividing the dot product by the squared magnitude of `L`.\n4. **Compute the Projection Vector**: Multiply each component of `L` by the scalar factor to get the projection vector.\n5. **Round the Result**: Each component of the projection vector is rounded to three decimal places for the final result.\n\n### Solution Code\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = sum(a * b for a, b in zip(v, L))\n    magnitude_squared = sum(x ** 2 for x in L)\n    scalar = dot_product / magnitude_squared\n    projection = [scalar * x for x in L]\n    return [round(x, 3) for x in projection]\n```\n\n### Explanation\n1. **Dot Product Calculation**: The dot product of `v` and `L` is computed to determine how aligned the vectors are. This is done by summing the products of corresponding components.\n2. **Magnitude Squared Calculation**: The squared magnitude of `L` is computed to normalize the projection. This is done by summing the squares of each component of `L`.\n3. **Scalar Factor Calculation**: The scalar factor is derived by dividing the dot product by the squared magnitude of `L`. This factor scales the vector `L` appropriately.\n4. **Projection Vector Calculation**: Each component of `L` is multiplied by the scalar factor to project `v` onto `L`.\n5. **Rounding**: The resulting projection vector is rounded to three decimal places for precision.\n\nThis approach ensures that we accurately compute the orthogonal projection using fundamental linear algebra operations, providing an efficient and clear solution to the problem."}
{"task_id": 67, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a dense matrix into its Compressed Column Sparse (CSC) representation. The CSC format is efficient for storing matrices with a high number of zero elements by only keeping track of non-zero elements and their positions.\n\n### Approach\nThe CSC format consists of three lists:\n1. `values`: A list of non-zero elements stored in column-major order.\n2. `row_indices`: A list of row indices corresponding to each value in the `values` list.\n3. `column_pointer`: A list that indicates the starting index of each column in the `values` array.\n\nThe approach involves the following steps:\n1. **Initialization**: Start with empty lists for `values` and `row_indices`, and initialize `column_pointer` with a single element 0.\n2. **Column-wise Processing**: For each column in the dense matrix, iterate through each row to collect non-zero elements. For each non-zero element found, add it to `values` and record its row index in `row_indices`.\n3. **Update Column Pointers**: After processing each column, update `column_pointer` to include the starting index of the next column, which is the current length of `values`.\n\nThis method ensures that we efficiently capture the sparse structure of the matrix by only processing non-zero elements and maintaining their positions.\n\n### Solution Code\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix:\n        return [], [], []\n    \n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    \n    values = []\n    row_indices = []\n    column_pointer = [0]  # Starting index for the first column\n    \n    for c in range(num_cols):\n        for r in range(num_rows):\n            val = dense_matrix[r][c]\n            if val != 0:\n                values.append(val)\n                row_indices.append(r)\n        column_pointer.append(len(values))\n    \n    return (values, row_indices, column_pointer)\n```\n\n### Explanation\n- **Initialization**: We start by checking if the input matrix is empty. If it is, we return empty lists for all three components. Otherwise, we initialize the necessary variables.\n- **Column-wise Processing**: For each column, we iterate through each row to check for non-zero elements. Each non-zero element is added to `values`, and its row index is recorded in `row_indices`.\n- **Column Pointers Update**: After processing each column, we update `column_pointer` to reflect the starting index of the next column in the `values` list. This helps in quickly accessing the elements of each column in the sparse format.\n\nThis approach efficiently constructs the CSC representation by focusing on non-zero elements and their positions, making it suitable for sparse matrices."}
{"task_id": 68, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the image (or column space) of a given matrix \\( A \\). The column space is the span of all columns of \\( A \\), and it can be determined by identifying the independent columns that form a basis for this space. This is achieved by converting the matrix into its Reduced Row Echelon Form (RREF) and extracting the pivot columns from the original matrix.\n\n### Approach\n1. **Convert to Reduced Row Echelon Form (RREF):** Perform row operations on the matrix to transform it into its RREF. This involves:\n   - Swapping rows to position the pivot element.\n   - Normalizing the pivot row so that the leading entry is 1.\n   - Eliminating all other entries in the pivot column using row operations.\n   \n2. **Identify Pivot Columns:** From the RREF matrix, identify the columns that contain the pivot elements. These columns correspond to the independent columns in the original matrix.\n\n3. **Extract Basis Vectors:** Extract the identified pivot columns from the original matrix \\( A \\). These columns form the basis vectors of the column space.\n\n4. **Format the Result:** Round the basis vectors to 8 decimal places and convert them into a list format for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    # Convert the input to a numpy array of floats\n    A = np.array(A, dtype=float)\n    mat = A.copy()\n    m, n = mat.shape\n    pivot_row = 0\n\n    for pivot_col in range(n):\n        # Find the pivot row with the maximum absolute value in the current column\n        max_row = np.argmax(np.abs(mat[pivot_row:, pivot_col])) + pivot_row\n        if mat[max_row, pivot_col] == 0:\n            continue  # Move to next column if all elements are zero\n        \n        # Swap the current row with the max row\n        mat[[pivot_row, max_row]] = mat[[max_row, pivot_row]]\n        \n        # Normalize the pivot row\n        pivot = mat[pivot_row, pivot_col]\n        mat[pivot_row] = mat[pivot_row] / pivot\n        \n        # Eliminate all other entries in the pivot column\n        for r in range(m):\n            if r != pivot_row:\n                factor = mat[r, pivot_col]\n                mat[r] = mat[r] - factor * mat[pivot_row]\n        \n        pivot_row += 1\n        if pivot_row >= m:\n            break  # No more rows to process\n\n    # Identify the pivot columns from the RREF matrix\n    pivot_cols = []\n    for r in range(m):\n        row = mat[r]\n        for c in range(n):\n            if abs(row[c]) > 1e-8:  # Check for non-zero with a small tolerance\n                pivot_cols.append(c)\n                break  # Only the first non-zero element in the row is considered\n\n    # Extract the corresponding columns from the original matrix\n    if not pivot_cols:\n        return []\n    basis = A[:, pivot_cols]\n    \n    # Round to 8 decimal places and convert to a list\n    rounded_basis = np.round(basis, 8)\n    return rounded_basis.tolist()\n```\n\n### Explanation\n1. **RREF Transformation:** The matrix is transformed into its RREF using row operations. This step ensures that each leading coefficient (pivot) is 1 and is the only non-zero entry in its column.\n2. **Pivot Identification:** After transforming the matrix, the pivot columns are identified. These columns are the ones that contain the leading 1's in the RREF matrix.\n3. **Basis Extraction:** The identified pivot columns from the RREF matrix are extracted from the original matrix \\( A \\) to form the basis vectors of the column space.\n4. **Result Formatting:** The basis vectors are rounded to 8 decimal places and converted into a list format for the final output.\n\nThis approach efficiently computes the column space by leveraging linear algebra techniques and ensures numerical stability through careful handling of floating-point operations."}
{"task_id": 69, "completion_id": 0, "completion": "\n\nTo calculate the R-squared value for regression analysis, we use the formula \\( R^2 = 1 - \\frac{SSE}{SST} \\), where \\( SSE \\) is the sum of squared errors and \\( SST \\) is the total sum of squares. The function handles edge cases where the total variance is zero to avoid division errors.\n\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    mean_y = np.mean(y_true)\n    sst = np.sum((y_true - mean_y) ** 2)\n    sse = np.sum((y_true - y_pred) ** 2)\n    \n    if np.isclose(sst, 0):\n        if np.isclose(sse, 0):\n            r2 = 1.0\n        else:\n            r2 = 0.0\n    else:\n        r2 = 1 - (sse / sst)\n    \n    return round(r2, 3)\n```"}
{"task_id": 70, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the average brightness of a grayscale image represented as a 2D matrix. Each element in the matrix represents a pixel value between 0 (black) and 255 (white). The function should handle various edge cases and return the average brightness rounded to two decimal places.\n\n### Approach\n1. **Check for Empty Image**: If the image matrix is empty, return -1.\n2. **Check Row Consistency**: Ensure all rows in the matrix have the same length. If any row has a different length, return -1.\n3. **Check Pixel Validity**: Verify that each pixel value is within the valid range (0-255). If any pixel is outside this range, return -1.\n4. **Calculate Average Brightness**: If all checks pass, compute the sum of all pixel values, divide by the total number of pixels, and return the result rounded to two decimal places.\n\n### Solution Code\n```python\ndef calculate_brightness(img):\n    # Check if the image is empty\n    if not img:\n        return -1\n    \n    num_rows = len(img)\n    num_cols = len(img[0])\n    \n    # Check if the first row is empty\n    if num_cols == 0:\n        return -1\n    \n    # Check if all rows have the same length\n    for row in img:\n        if len(row) != num_cols:\n            return -1\n    \n    # Check if all pixel values are within the valid range\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n    \n    # Calculate the total sum of all pixels\n    total = 0\n    for row in img:\n        total += sum(row)\n    \n    # Calculate the average brightness\n    average = total / (num_rows * num_cols)\n    \n    # Round to two decimal places and return\n    return round(average, 2)\n```\n\n### Explanation\n1. **Empty Image Check**: The function first checks if the image matrix is empty. If it is, the function returns -1.\n2. **Row Consistency Check**: The function then checks if all rows have the same length by comparing each row's length to the length of the first row. If any row has a different length, the function returns -1.\n3. **Pixel Validity Check**: Each pixel value is checked to ensure it falls within the valid range of 0 to 255. If any pixel is outside this range, the function returns -1.\n4. **Brightness Calculation**: If all checks are passed, the function calculates the sum of all pixel values, computes the average by dividing the sum by the total number of pixels, and returns the result rounded to two decimal places.\n\nThis approach ensures that all edge cases are handled appropriately and the average brightness is computed efficiently."}
{"task_id": 71, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Root Mean Square Error (RMSE) between two arrays of actual and predicted values. RMSE is a commonly used metric to evaluate the accuracy of regression models. The function should handle various edge cases such as mismatched array shapes, empty arrays, and invalid input types.\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: First, we convert the input arrays into numpy arrays to handle array operations efficiently. If the conversion fails, we raise a TypeError.\n2. **Check Array Shapes**: Ensure that both arrays have the same shape. If they don't, we raise a ValueError.\n3. **Check for Empty Arrays**: If either of the arrays is empty, we raise a ValueError.\n4. **Calculate RMSE**: Compute the RMSE using the formula:\n   \\[\n   RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2}\n   \\]\n   where \\( n \\) is the number of observations.\n5. **Round the Result**: Return the RMSE value rounded to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    try:\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n    except TypeError:\n        raise TypeError(\"Inputs must be array-like.\")\n    \n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    if y_true.size == 0:\n        raise ValueError(\"Input arrays are empty.\")\n    \n    difference = y_true - y_pred\n    squared_diff = difference ** 2\n    mean_squared = np.mean(squared_diff)\n    rmse_value = np.sqrt(mean_squared)\n    \n    return round(rmse_value, 3)\n```\n\n### Explanation\n1. **Conversion to Numpy Arrays**: The function starts by converting the input arrays into numpy arrays. This allows us to use numpy's efficient array operations.\n2. **Shape Check**: The function checks if the shapes of the two arrays are the same. If not, it raises a ValueError.\n3. **Empty Array Check**: If either array is empty, the function raises a ValueError since RMSE cannot be computed with zero observations.\n4. **RMSE Calculation**: The function computes the element-wise difference between the actual and predicted values, squares these differences, computes the mean of these squared differences, and then takes the square root to get the RMSE.\n5. **Rounding**: The result is rounded to three decimal places for precision.\n\nThis approach ensures that the function handles various edge cases and efficiently computes the RMSE using numpy operations."}
{"task_id": 72, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Jaccard Index for binary classification. The Jaccard Index is a measure of similarity between two sets, and it is defined as the size of the intersection divided by the size of the union of the sets. This index is particularly useful in evaluating the overlap between predicted and true labels in binary classification tasks.\n\n### Approach\n1. **Problem Analysis**: The Jaccard Index is calculated using the formula:\n   \\[\n   J = \\frac{\\text{Number of elements in the intersection of } y_{\\text{true}} \\text{ and } y_{\\text{pred}}}{\\text{Number of elements in the union of } y_{\\text{true}} \\text{ and } y_{\\text{pred}}}\n   \\]\n   Here, \\( y_{\\text{true}} \\) and \\( y_{\\text{pred}} \\) are binary arrays representing true and predicted labels, respectively.\n\n2. **Key Insights**:\n   - The intersection of two binary arrays is the count of positions where both arrays have 1s.\n   - The union of two binary arrays is the count of positions where either array has a 1.\n   - If both arrays are all zeros, the Jaccard Index is defined as 0 to handle this edge case.\n\n3. **Algorithm**:\n   - Compute the sum of true labels (sum_true) and the sum of predicted labels (sum_pred).\n   - Calculate the intersection as the sum of element-wise multiplication of the two arrays.\n   - Calculate the union using the formula: \\( \\text{union} = \\text{sum_true} + \\text{sum_pred} - \\text{intersection} \\).\n   - If the union is zero, return 0.0. Otherwise, compute the Jaccard Index as the ratio of intersection to union and round it to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    union = sum_true + sum_pred - intersection\n    if union == 0:\n        return 0.0\n    j = intersection / union\n    return round(j, 3)\n```\n\n### Explanation\n- **sum_true**: This is the count of 1s in the true labels array.\n- **sum_pred**: This is the count of 1s in the predicted labels array.\n- **intersection**: This is the count of positions where both true and predicted labels are 1, calculated using element-wise multiplication and summation.\n- **union**: This is the count of positions where either true or predicted labels are 1, calculated using the formula to avoid double-counting the intersection.\n- **Edge Case Handling**: If both arrays are all zeros, the union is zero, and the function returns 0.0 to handle this case.\n\nThis approach efficiently computes the Jaccard Index using basic arithmetic operations and handles all edge cases, ensuring accurate results for binary classification evaluation."}
{"task_id": 73, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Dice Score, also known as the S\u00f8rensen-Dice coefficient, for binary classification. This score measures the similarity between two sets, making it particularly useful in tasks like image segmentation and binary classification.\n\n### Approach\nThe Dice Score is defined as:\n\n\\[ \\text{Dice Score} = \\frac{2 \\times (\\text{Number of elements in the intersection of } y_{\\text{true}} \\text{ and } y_{\\text{pred}})}{\\text{Number of elements in } y_{\\text{true}} + \\text{Number of elements in } y_{\\text{pred}}} \\]\n\nHere, \\( y_{\\text{true}} \\) and \\( y_{\\text{pred}} \\) are binary arrays representing true and predicted labels, respectively. The result ranges from 0 (no overlap) to 1 (perfect overlap).\n\nThe steps to compute the Dice Score are as follows:\n1. Compute the number of true positives (TP), which is the count of elements where both \\( y_{\\text{true}} \\) and \\( y_{\\text{pred}} \\) are 1.\n2. Compute the sum of elements in \\( y_{\\text{true}} \\) (sum_true) and the sum of elements in \\( y_{\\text{pred}} \\) (sum_pred).\n3. Calculate the denominator as the sum of sum_true and sum_pred.\n4. If the denominator is zero, return 1.0 because both arrays have no positive elements, indicating perfect agreement.\n5. Otherwise, compute the Dice Score using the formula and return the result rounded to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    tp = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    denominator = sum_true + sum_pred\n    if denominator == 0:\n        return 1.0\n    else:\n        dice = (2 * tp) / denominator\n        return round(dice, 3)\n```\n\n### Explanation\n- **True Positives (TP):** Calculated as the sum of the element-wise product of \\( y_{\\text{true}} \\) and \\( y_{\\text{pred}} \\). This gives the count of elements where both arrays have a value of 1.\n- **Sum of True and Predicted Positives:** These are the counts of 1s in \\( y_{\\text{true}} \\) and \\( y_{\\text{pred}} \\), respectively.\n- **Denominator Handling:** If both arrays have no positive elements, the denominator is zero, and we return 1.0 to indicate perfect agreement.\n- **Dice Score Calculation:** The score is computed using the formula and rounded to three decimal places for the final result.\n\nThis approach efficiently handles all edge cases, including when there are no true or predicted positives, ensuring accurate and reliable results."}
{"task_id": 75, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a confusion matrix for a binary classification problem. The confusion matrix is a 2x2 table that summarizes the predictions against the actual outcomes, helping to evaluate the accuracy of a classification model.\n\n### Approach\nThe confusion matrix for binary classification consists of four key metrics:\n1. **True Negatives (TN)**: The number of correctly predicted negative outcomes.\n2. **False Positives (FP)**: The number of incorrectly predicted positive outcomes.\n3. **False Negatives (FN)**: The number of incorrectly predicted negative outcomes.\n4. **True Positives (TP)**: The number of correctly predicted positive outcomes.\n\nThe matrix is structured as follows:\n- The first row represents the actual negative outcomes (0), with the first element being TN and the second being FP.\n- The second row represents the actual positive outcomes (1), with the first element being FN and the second being TP.\n\nThe approach involves:\n1. Extracting pairs of actual and predicted labels from the input data.\n2. Using a counter to tally the occurrences of each (actual, predicted) pair.\n3. Extracting the counts for each of the four metrics (TN, FP, FN, TP) from the counter.\n4. Constructing the confusion matrix using these counts.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    pairs = [(y[0], y[1]) for y in data]\n    counts = Counter(pairs)\n    tn = counts.get((0, 0), 0)\n    fp = counts.get((0, 1), 0)\n    fn = counts.get((1, 0), 0)\n    tp = counts.get((1, 1), 0)\n    return [[tn, fp], [fn, tp]]\n```\n\n### Explanation\n1. **Extracting Pairs**: Convert each element in the input data into a tuple of (actual label, predicted label).\n2. **Counting Occurrences**: Use the `Counter` from the `collections` module to count how many times each (actual, predicted) pair appears in the data.\n3. **Extracting Metrics**: Retrieve the counts for each of the four possible pairs (TN, FP, FN, TP) from the counter. If a pair does not appear in the data, its count defaults to zero.\n4. **Constructing the Matrix**: Arrange the counts into a 2x2 matrix in the order of TN, FP, FN, and TP.\n\nThis approach efficiently computes the confusion matrix by leveraging the counting capabilities of the `Counter` class, ensuring that all possible cases are handled correctly, including when some pairs are missing from the data."}
{"task_id": 76, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the cosine similarity between two vectors. Cosine similarity is a measure of the angle between two vectors in a multi-dimensional space and is often used in machine learning and data analysis to understand the directional similarity between vectors.\n\n### Approach\nThe approach to calculate cosine similarity involves the following steps:\n\n1. **Dot Product Calculation**: Compute the dot product of the two vectors. The dot product measures the sum of the products of corresponding elements in the two vectors.\n2. **Magnitude Calculation**: Compute the magnitudes (Euclidean norms) of both vectors. The magnitude of a vector is the square root of the sum of the squares of its elements.\n3. **Cosine Similarity Formula**: Use the formula for cosine similarity, which is the dot product of the vectors divided by the product of their magnitudes.\n4. **Rounding**: Round the result to three decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)\n```\n\n### Explanation\n1. **Dot Product Calculation**: The dot product of two vectors `v1` and `v2` is computed using `np.dot(v1, v2)`. This gives a scalar value representing the sum of the products of corresponding elements.\n2. **Magnitude Calculation**: The magnitudes of the vectors are computed using `np.linalg.norm(v1)` and `np.linalg.norm(v2)`. This function returns the Euclidean norm, which is the square root of the sum of the squares of the vector elements.\n3. **Cosine Similarity Formula**: The cosine similarity is calculated by dividing the dot product by the product of the magnitudes of the two vectors. This gives the cosine of the angle between the vectors.\n4. **Rounding**: The result is rounded to three decimal places using `round(similarity, 3)` to ensure the output meets the specified precision.\n\nThis approach efficiently computes the cosine similarity using numpy functions, ensuring both accuracy and performance. The constraints are handled by assuming valid input vectors as per the problem statement."}
{"task_id": 77, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute various performance metrics for a binary classification model. These metrics include the confusion matrix, accuracy, F1 score, specificity, and negative predictive value. The function will take two lists of actual and predicted class labels and return these metrics in a tuple.\n\n### Approach\n1. **Confusion Matrix**: This is a 2x2 matrix that summarizes the predictions against the actual outcomes. It consists of true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN).\n2. **Accuracy**: This is the proportion of correctly classified instances out of all instances. It is calculated as (TP + TN) / (TP + TN + FP + FN).\n3. **F1 Score**: This is the harmonic mean of precision and recall. It is calculated using the formula: F1 = 2 * TP / (2 * TP + FP + FN).\n4. **Specificity**: This is the number of true negatives divided by the sum of true negatives and false positives. It is calculated as TN / (TN + FP).\n5. **Negative Predictive Value (NPV)**: This is the number of true negatives divided by the sum of true negatives and false negatives. It is calculated as TN / (TN + FN).\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    pairs = list(zip(actual, predicted))\n    counts = Counter(pairs)\n    \n    TP = counts.get((1, 1), 0)\n    FP = counts.get((0, 1), 0)\n    FN = counts.get((1, 0), 0)\n    TN = counts.get((0, 0), 0)\n    \n    confusion_matrix = [[TN, FP], [FN, TP]]\n    \n    total = TP + TN + FP + FN\n    if total == 0:\n        accuracy = 0.0\n    else:\n        accuracy = (TP + TN) / total\n    accuracy_rounded = round(accuracy, 3)\n    \n    denominator_f1 = 2 * TP + FP + FN\n    if denominator_f1 == 0:\n        f1 = 0.0\n    else:\n        f1 = (2 * TP) / denominator_f1\n    f1_rounded = round(f1, 3)\n    \n    denominator_specificity = TN + FP\n    if denominator_specificity == 0:\n        specificity = 0.0\n    else:\n        specificity = TN / denominator_specificity\n    specificity_rounded = round(specificity, 3)\n    \n    denominator_npv = TN + FN\n    if denominator_npv == 0:\n        npv = 0.0\n    else:\n        npv = TN / denominator_npv\n    npv_rounded = round(npv, 3)\n    \n    return (confusion_matrix, accuracy_rounded, f1_rounded, specificity_rounded, npv_rounded)\n```\n\n### Explanation\n1. **Confusion Matrix**: We use a Counter to count occurrences of each (actual, predicted) pair. From these counts, we derive TP, FP, FN, and TN, which are used to construct the confusion matrix.\n2. **Accuracy**: This is calculated as the sum of correct predictions (TP + TN) divided by the total number of samples.\n3. **F1 Score**: This is computed using the formula that avoids division by zero by checking if the denominator is zero.\n4. **Specificity**: This measures the model's ability to correctly predict negative classes and is calculated as TN divided by the sum of TN and FP.\n5. **Negative Predictive Value (NPV)**: This measures the model's ability to correctly predict negative classes and is calculated as TN divided by the sum of TN and FN.\n\nEach metric is rounded to three decimal places for the final output. The function returns these metrics in a tuple, providing a comprehensive evaluation of the classification model's performance."}
{"task_id": 78, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that calculates various descriptive statistics for a given dataset. The function should return a dictionary containing metrics such as mean, median, mode, variance, standard deviation, percentiles, and interquartile range (IQR). \n\n### Approach\n1. **Import Necessary Libraries**: We use NumPy for efficient numerical computations.\n2. **Compute Each Metric**:\n   - **Mean**: The average value of the dataset.\n   - **Median**: The middle value when the data is ordered.\n   - **Mode**: The most frequently occurring value. If there are multiple modes, the smallest one is chosen.\n   - **Variance**: The average of the squared differences from the mean.\n   - **Standard Deviation**: The square root of the variance.\n   - **Percentiles**: The 25th, 50th, and 75th percentiles, which divide the data into four equal parts.\n   - **Interquartile Range (IQR)**: The difference between the 75th and 25th percentiles.\n3. **Round Values**: All computed metrics except the mode are rounded to four decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef descriptive_statistics(data):\n    mean_val = np.mean(data)\n    median_val = np.median(data)\n    mode_val = np.mode(data)[0]\n    variance_val = np.var(data)\n    std_dev_val = np.std(data)\n    p25 = np.percentile(data, 25)\n    p50 = np.percentile(data, 50)\n    p75 = np.percentile(data, 75)\n    iqr_val = p75 - p25\n\n    stats = {\n        'mean': round(mean_val, 4),\n        'median': round(median_val, 4),\n        'mode': mode_val,\n        'variance': round(variance_val, 4),\n        'standard_deviation': round(std_dev_val, 4),\n        '25th_percentile': round(p25, 4),\n        '50th_percentile': round(p50, 4),\n        '75th_percentile': round(p75, 4),\n        'interquartile_range': round(iqr_val, 4)\n    }\n\n    return stats\n```\n\n### Explanation\n- **Mean**: Calculated using `np.mean(data)`, which gives the average value.\n- **Median**: Calculated using `np.median(data)`, which finds the middle value.\n- **Mode**: Determined using `np.mode(data)[0]`, which returns the smallest most frequent value.\n- **Variance and Standard Deviation**: Computed using `np.var(data)` and `np.std(data)`, respectively.\n- **Percentiles**: Found using `np.percentile(data, q)` for q = 25, 50, 75.\n- **IQR**: Calculated as the difference between the 75th and 25th percentiles.\n\nThe function returns a dictionary with all these metrics, ensuring that all values except the mode are rounded to four decimal places for clarity and precision."}
{"task_id": 79, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the probability of achieving exactly k successes in n independent Bernoulli trials, where each trial has a probability p of success. This can be efficiently computed using the Binomial distribution formula.\n\n### Approach\nThe Binomial distribution formula is given by:\n\\[ P(k) = C(n, k) \\times p^k \\times (1-p)^{n-k} \\]\nwhere:\n- \\( C(n, k) \\) is the number of combinations of n items taken k at a time, calculated using the combination formula.\n- \\( p^k \\) is the probability of k successes.\n- \\( (1-p)^{n-k} \\) is the probability of (n-k) failures.\n\nThe steps to compute the probability are as follows:\n1. Calculate the combination \\( C(n, k) \\) using the `math.comb` function.\n2. Compute \\( p^k \\) and \\( (1-p)^{n-k} \\).\n3. Multiply these values together to get the probability.\n4. Round the result to five decimal places for the final output.\n\n### Solution Code\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials rounded to five decimal places\n    \"\"\"\n    comb = math.comb(n, k)\n    prob = comb * (p ** k) * ((1 - p) ** (n - k))\n    return round(prob, 5)\n```\n\n### Explanation\n- **Combination Calculation**: The function uses `math.comb(n, k)` to compute the binomial coefficient, which represents the number of ways to choose k successes out of n trials.\n- **Probability Calculation**: The probability of k successes and (n-k) failures is computed using the terms \\( p^k \\) and \\( (1-p)^{n-k} \\).\n- **Rounding**: The result is rounded to five decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the desired probability using the Binomial distribution formula and handles edge cases such as when p is 0 or 1, ensuring accurate results across all valid input ranges."}
{"task_id": 80, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the probability density function (PDF) of the normal distribution for a given value, mean, and standard deviation. The normal distribution is a fundamental concept in statistics and is widely used in various fields. The PDF of the normal distribution gives the relative likelihood for a value to occur and is defined by a specific mathematical formula.\n\n### Approach\nThe approach to solve this problem involves using the mathematical formula of the normal distribution to compute the PDF. The formula for the PDF of a normal distribution is:\n\n\\[ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\]\n\nWhere:\n- \\( x \\) is the value at which the PDF is evaluated.\n- \\( \\mu \\) is the mean of the distribution.\n- \\( \\sigma \\) is the standard deviation of the distribution.\n\nThe steps to compute the PDF are as follows:\n1. Compute the exponent part of the formula, which is \\(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\).\n2. Calculate the exponential term using the computed exponent.\n3. Compute the coefficient part of the formula, which is \\(\\frac{1}{\\sigma \\sqrt{2\\pi}}\\).\n4. Multiply the coefficient by the exponential term to get the PDF value.\n5. Round the result to 5 decimal places for precision.\n\n### Solution Code\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -((x - mean) ** 2) / (2 * (std_dev ** 2))\n    exp_term = math.exp(exponent)\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf_value = coefficient * exp_term\n    return round(pdf_value, 5)\n```\n\n### Explanation\nThe function `normal_pdf` takes three parameters: `x`, `mean`, and `std_dev`. It calculates the PDF using the normal distribution formula. Here's a detailed breakdown of the steps involved:\n\n1. **Exponent Calculation**: The exponent is computed as \\(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\). This part of the formula measures how far the value \\( x \\) is from the mean in terms of standard deviations.\n2. **Exponential Term**: Using the computed exponent, the function calculates \\( e \\) raised to the power of the exponent. This term adjusts the PDF value based on how far \\( x \\) is from the mean.\n3. **Coefficient Calculation**: The coefficient is computed as \\(\\frac{1}{\\sigma \\sqrt{2\\pi}}\\). This part normalizes the PDF so that the total area under the curve equals 1.\n4. **PDF Value**: The final PDF value is obtained by multiplying the coefficient with the exponential term.\n5. **Rounding**: The result is rounded to 5 decimal places to ensure precision and readability.\n\nThis approach efficiently computes the PDF using the mathematical properties of the normal distribution and ensures the result is precise and formatted correctly."}
{"task_id": 81, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the probability of observing exactly k events in a fixed interval using the Poisson distribution formula. The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space. The formula for the Poisson probability mass function is given by:\n\n\\[ P(k) = \\frac{\\lambda^k \\cdot e^{-\\lambda}}{k!} \\]\n\nwhere:\n- \\( k \\) is the number of occurrences (non-negative integer)\n- \\( \\lambda \\) is the average rate (mean) of occurrences in the interval\n- \\( e \\) is the base of the natural logarithm (approximately equal to 2.71828)\n- \\( k! \\) is the factorial of k\n\n### Approach\n1. **Understand the Formula**: The Poisson probability formula involves three main components: \\( \\lambda^k \\), \\( e^{-\\lambda} \\), and \\( k! \\). Each of these components needs to be computed accurately.\n2. **Compute Each Component**: \n   - Calculate \\( \\lambda^k \\) using exponentiation.\n   - Calculate \\( e^{-\\lambda} \\) using the exponential function.\n   - Calculate \\( k! \\) using the factorial function.\n3. **Combine Components**: Use the formula to combine these components into the probability value.\n4. **Handle Edge Cases**: Ensure that the function handles cases where \\( k \\) is 0 or \\( \\lambda \\) is 0 correctly.\n5. **Round the Result**: The result is rounded to five decimal places for the final output.\n\n### Solution Code\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability rounded to 5 decimal places\n    \"\"\"\n    prob = (lam ** k) * math.exp(-lam) / math.factorial(k)\n    return round(prob, 5)\n```\n\n### Explanation\n- **Exponentiation**: The term \\( \\lambda^k \\) is computed using `lam ** k`.\n- **Exponential Function**: The term \\( e^{-\\lambda} \\) is computed using `math.exp(-lam)`.\n- **Factorial**: The term \\( k! \\) is computed using `math.factorial(k)`.\n- **Probability Calculation**: The probability is calculated by combining these terms as per the Poisson formula.\n- **Rounding**: The result is rounded to five decimal places using `round(prob, 5)`.\n\nThis approach ensures that the function correctly computes the Poisson probability for any valid input values of \\( k \\) and \\( \\lambda \\), handling edge cases and providing the result rounded to five decimal places."}
{"task_id": 82, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the contrast of a grayscale image using the difference between the maximum and minimum pixel values. The contrast is a measure of how much the pixel values vary in the image, which can be determined by finding the range of pixel intensities.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Extract Maximum Pixel Value**: Identify the highest pixel value in the grayscale image. This value represents the brightest part of the image.\n2. **Extract Minimum Pixel Value**: Identify the lowest pixel value in the grayscale image. This value represents the darkest part of the image.\n3. **Calculate Contrast**: Compute the contrast by subtracting the minimum pixel value from the maximum pixel value. This difference gives the range of pixel intensities, which is the contrast of the image.\n\nUsing numpy functions `np.max()` and `np.min()`, we can efficiently find the maximum and minimum values in the 2D array representing the image. The contrast is then simply the difference between these two values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    return np.max(img) - np.min(img)\n```\n\n### Explanation\n- **Extracting Maximum and Minimum Values**: The functions `np.max(img)` and `np.min(img)` are used to find the maximum and minimum pixel values in the 2D array `img`. These functions efficiently traverse the entire array to compute these values.\n- **Calculating Contrast**: The contrast is calculated by subtracting the minimum pixel value from the maximum pixel value. This gives the range of pixel intensities, which is a direct measure of the image's contrast.\n\nThis approach ensures that we efficiently and accurately compute the contrast of the grayscale image using numpy's optimized functions. The solution handles any valid 2D array input, providing the correct contrast value based on the pixel intensity range."}
{"task_id": 83, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the dot product of two vectors using NumPy. The dot product is a fundamental operation in linear algebra and is computed as the sum of the products of the corresponding entries of the two vectors.\n\n### Approach\nThe approach to solve this problem involves using the built-in `np.dot()` function from the NumPy library. This function efficiently computes the dot product of two vectors, handling the multiplication and summation internally. The function takes two 1D NumPy arrays as input and returns the dot product as a single number. \n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    return np.dot(vec1, vec2)\n```\n\n### Explanation\n1. **Function Definition**: The function `calculate_dot_product` is defined to take two parameters, `vec1` and `vec2`, which are both 1D NumPy arrays.\n2. **Dot Product Calculation**: Inside the function, `np.dot(vec1, vec2)` is used to compute the dot product. This function multiplies corresponding elements of the two vectors and sums them up to produce a single scalar value.\n3. **Return Value**: The result of `np.dot(vec1, vec2)` is returned as the output of the function.\n\nThis approach ensures that the computation is efficient and concise, leveraging NumPy's optimized operations for numerical computations."}
{"task_id": 84, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform a Phi Transformation on a list of numerical data. This transformation maps each data point into a higher-dimensional space by generating polynomial features up to a specified degree. This allows models like linear regression to fit nonlinear data by introducing new feature dimensions that represent polynomial combinations of the original input features.\n\n### Approach\n1. **Check for Valid Degree**: If the given degree is less than 0, return an empty list as the transformation is not possible.\n2. **Generate Polynomial Features**: For each data point, generate polynomial features from degree 0 up to the specified degree. The 0th degree term is always 1, representing the constant term. For higher degrees, compute the power of the data point.\n3. **Round Features**: Each generated feature is rounded to 8 decimal places to ensure precision.\n4. **Collect Results**: Collect the transformed features for each data point into a list and return this list.\n\n### Solution Code\n```python\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed = []\n    for x in data:\n        features = []\n        for i in range(degree + 1):\n            if i == 0:\n                features.append(1.0)\n            else:\n                features.append(x ** i)\n        # Round each feature to 8 decimal places\n        rounded_features = [round(f, 8) for f in features]\n        transformed.append(rounded_features)\n    return transformed\n```\n\n### Explanation\n- **Check for Valid Degree**: The function first checks if the degree is negative. If it is, the function returns an empty list immediately.\n- **Generate Polynomial Features**: For each data point, the function iterates from 0 to the specified degree. For each iteration, it computes the polynomial term. The 0th degree term is set to 1.0, and higher-degree terms are computed as the data point raised to the power of the current degree.\n- **Round Features**: Each computed feature is rounded to 8 decimal places to maintain precision and consistency in the output.\n- **Collect Results**: The transformed features for each data point are collected into a list, which is then returned as the final result.\n\nThis approach ensures that each data point is transformed into a higher-dimensional space, allowing models to capture nonlinear relationships in the data."}
{"task_id": 85, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the positional encoding for a given sequence length and model dimensionality using sine and cosine functions as specified in the Transformer architecture. The function should return -1 if the sequence length is 0 or if the model dimensionality is less than or equal to 0.\n\n### Approach\n1. **Input Validation**: Check if the sequence length (`position`) is 0 or if the model dimensionality (`d_model`) is less than or equal to 0. If either condition is met, return -1.\n2. **Initialize Encoding Array**: Create a numpy array of size `d_model` with dtype `np.float16` to store the positional encodings.\n3. **Calculate Positional Encodings**: For each index in the array:\n   - Compute the index `i` as the integer division of the current index by 2.\n   - Calculate the exponent as `(2 * i) / d_model`.\n   - Compute the denominator as `10000` raised to the power of the exponent.\n   - Calculate the value as the sequence length divided by the denominator.\n   - Assign the sine of the value to even indices and the cosine of the value to odd indices.\n4. **Return Result**: Convert the numpy array to a list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    encoding = np.zeros(d_model, dtype=np.float16)\n    for j in range(d_model):\n        i = j // 2\n        exponent = (2 * i) / d_model\n        denominator = 10000 ** exponent\n        value = position / denominator\n        if j % 2 == 0:\n            encoding[j] = np.sin(value)\n        else:\n            encoding[j] = np.cos(value)\n    return encoding.tolist()\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the input values are valid. If `position` is 0 or `d_model` is less than or equal to 0, it returns -1 immediately.\n- **Array Initialization**: A numpy array of size `d_model` is initialized to store the positional encodings.\n- **Encoding Calculation**: For each index in the array, the function calculates the appropriate sine or cosine value based on the index's parity (even or odd). The calculations follow the formula specified in the Transformer architecture, ensuring the correct positional encoding values are generated.\n- **Result Conversion**: The numpy array is converted to a list before being returned to meet the output requirements.\n\nThis approach ensures that the positional encodings are computed efficiently and correctly, adhering to the constraints and requirements specified."}
{"task_id": 86, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine whether a machine learning model is overfitting, underfitting, or performing well based on its training and test accuracy values. The function should return 1 for overfitting, -1 for underfitting, and 0 if the model is performing well.\n\n### Approach\nThe approach to solve this problem involves checking two main conditions in sequence:\n\n1. **Overfitting Check**: The model is considered to be overfitting if the training accuracy is significantly higher than the test accuracy by a margin greater than 0.2. This is determined by checking if the difference between training accuracy and test accuracy exceeds 0.2.\n\n2. **Underfitting Check**: The model is considered to be underfitting if both the training and test accuracies are below 0.7. This is checked by verifying if both values are less than 0.7.\n\nIf neither of the above conditions is met, the model is considered to be performing well, and the function returns 0.\n\n### Solution Code\n```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for overfitting: training accuracy is significantly higher than test accuracy\n    if (training_accuracy - test_accuracy) > 0.2:\n        return 1\n    # Check for underfitting: both accuracies are below 0.7\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    # Otherwise, the model is a good fit\n    else:\n        return 0\n```\n\n### Explanation\n- **Overfitting**: This occurs when the model performs much better on the training data than on the test data. This is checked by ensuring the difference between training and test accuracy is more than 0.2.\n- **Underfitting**: This happens when the model performs poorly on both the training and test data, indicating that it has not learned the underlying patterns of the data. This is checked by ensuring both accuracies are below 0.7.\n- **Good Fit**: If neither overfitting nor underfitting conditions are met, the model is considered to be performing well.\n\nThis approach ensures that we correctly identify the model's performance based on the given accuracy values."}
{"task_id": 87, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Adam optimizer update step function. The Adam optimizer is a popular stochastic gradient descent algorithm that adapts the learning rate for each parameter based on the magnitude of the gradient. It uses moving averages of the gradient and the squared gradient to adjust the learning rate, which helps in converging to the optimal solution faster and more reliably.\n\n### Approach\nThe Adam optimizer update step involves the following key steps:\n\n1. **Update the First Moment Estimate (m):** This is the moving average of the gradients. It is updated using the formula:\n   \\[\n   m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot \\text{gradient}\n   \\]\n   where \\(\\beta_1\\) is the decay rate for the first moment estimate.\n\n2. **Update the Second Moment Estimate (v):** This is the moving average of the squared gradients. It is updated using the formula:\n   \\[\n   v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot \\text{gradient}^2\n   \\]\n   where \\(\\beta_2\\) is the decay rate for the second moment estimate.\n\n3. **Bias Correction:** Since the moving averages are initialized to zero, they have a bias which needs to be corrected. The bias-corrected estimates are computed as:\n   \\[\n   \\hat{m} = \\frac{m_t}{1 - \\beta_1^t}\n   \\]\n   \\[\n   \\hat{v} = \\frac{v_t}{1 - \\beta_2^t}\n   \\]\n\n4. **Update the Parameter:** The parameter is updated using the bias-corrected estimates:\n   \\[\n   \\theta_t = \\theta_{t-1} - \\text{learning\\_rate} \\cdot \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon}\n   \\]\n   where \\(\\epsilon\\) is a small constant to prevent division by zero.\n\n5. **Rounding and Conversion:** The updated parameter, first moment estimate, and second moment estimate are rounded to 5 decimal places and converted to lists for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Compute the new moving averages\n    new_m = beta1 * m + (1 - beta1) * grad\n    new_v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias correction factors\n    denominator_beta1 = 1 - (beta1 ** t)\n    denominator_beta2 = 1 - (beta2 ** t)\n    \n    # Bias-corrected estimates\n    m_hat = new_m / denominator_beta1\n    v_hat = new_v / denominator_beta2\n    \n    # Update the parameter\n    updated_parameter = parameter - (learning_rate * m_hat) / (np.sqrt(v_hat) + epsilon)\n    \n    # Round the results to 5 decimal places\n    updated_parameter = np.round(updated_parameter, 5)\n    new_m = np.round(new_m, 5)\n    new_v = np.round(new_v, 5)\n    \n    # Convert numpy arrays to lists\n    updated_parameter_list = updated_parameter.tolist()\n    new_m_list = new_m.tolist()\n    new_v_list = new_v.tolist()\n    \n    return (updated_parameter_list, new_m_list, new_v_list)\n```\n\n### Explanation\nThe function `adam_optimizer` takes the current parameter value, gradient, moving averages of the gradient and squared gradient, timestep, and other hyperparameters as inputs. It computes the updated parameter value and new moving averages using the Adam optimizer algorithm. The results are rounded to 5 decimal places and converted to lists before being returned. This ensures that the function handles both scalar and array inputs efficiently and returns the results in the required format."}
{"task_id": 88, "completion_id": 0, "completion": "\n\nHere's a self-contained Python script that implements a simplified GPT-2-like text generation function:\n\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    \n    generated_tokens = encoder.encode(prompt)\n    \n    for _ in range(n_tokens_to_generate):\n        # Get token embeddings\n        token_embeddings = params['wte'][generated_tokens]\n        \n        # Get positional embeddings\n        pos_embeddings = params['wpe'][:len(generated_tokens)]\n        \n        # Combine token and positional embeddings\n        embeddings = token_embeddings + pos_embeddings\n        \n        # Apply layer normalization\n        mean = np.mean(embeddings, axis=-1, keepdims=True)\n        std = np.std(embeddings, axis=-1, keepdims=True)\n        normalized = (embeddings - mean) / (std + 1e-5)\n        normalized = normalized * params['ln_f']['g'] + params['ln_f']['b']\n        \n        # Get last token's embedding\n        last_embedding = normalized[-1]\n        \n        # Compute logits\n        logits = np.dot(last_embedding, params['wte'].T)\n        \n        # Apply softmax to get probabilities\n        probs = np.exp(logits) / np.sum(np.exp(logits))\n        \n        # Sample next token\n        next_token = np.random.choice(len(probs), p=probs)\n        generated_tokens.append(next_token)\n    \n    # Decode generated tokens back to text\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text\n\n# Example usage:\nif __name__ == \"__main__\":\n    prompt = \"hello world\"\n    generated = gen_text(prompt, n_tokens_to_generate=10)\n    print(f\"Generated text: {generated}\")\n```\n\nThis script defines a simplified GPT-2-like text generation function that uses token embeddings, positional embeddings, layer normalization, and basic token generation logic. The `gen_text` function takes a prompt and the number of tokens to generate, then iteratively generates text based on the model's components. The example usage at the end demonstrates how to use the function with a sample prompt."}
{"task_id": 89, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simplified self-attention mechanism to analyze the relationships between crystals. Each crystal has a unique numeric value, and we need to compute the attention scores using the softmax function to derive the final weighted pattern for each crystal.\n\n### Approach\n1. **Softmax Function**: The softmax function is used to normalize the attention scores so that they sum up to 1. This helps in converting the scores into probabilities, which can then be used as weights.\n2. **Self-Attention Mechanism**: For each crystal, compute its relationship with every other crystal. This involves calculating attention scores using the dot product of the crystal values, scaled by the square root of the given dimension. Apply the softmax function to these scores to get the weights.\n3. **Weighted Sum Calculation**: Using the weights obtained from the softmax function, compute the weighted sum of the crystal values. This gives the final enhanced pattern for each crystal.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        values = np.array(values)\n        e_x = np.exp(values - np.max(values))  # To prevent overflow\n        sum_e_x = np.sum(e_x)\n        return (e_x / sum_e_x).tolist()\n    \n    sqrt_dim = np.sqrt(dimension)\n    result = []\n    for i in range(n):\n        scores = []\n        for j in range(n):\n            score = (crystal_values[i] * crystal_values[j]) / sqrt_dim\n            scores.append(score)\n        weights = softmax(scores)\n        total = 0.0\n        for j in range(n):\n            total += weights[j] * crystal_values[j]\n        result.append(total)\n    return [round(val, 4) for val in result]\n```\n\n### Explanation\n1. **Softmax Function**: The function takes a list of values, computes their exponentials, and normalizes them by dividing by the sum of these exponentials. This ensures the output is a probability distribution.\n2. **Self-Attention Calculation**: For each crystal, compute the attention scores with all other crystals. These scores are calculated as the product of the crystal values divided by the square root of the dimension. This scaling helps in preventing the scores from becoming too large.\n3. **Weighted Sum**: Using the weights from the softmax function, compute the weighted sum of the crystal values. This sum represents the influence of each crystal on others, giving the final enhanced pattern.\n\nThis approach efficiently computes the relationships between crystals using a simplified self-attention mechanism, providing the enhanced patterns as required."}
{"task_id": 90, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the BM25 ranking function, which is an advanced variation of TF-IDF used in information retrieval. BM25 incorporates term frequency saturation, document length normalization, and a configurable penalty for document length effects. The goal is to calculate document scores for a given query in a corpus.\n\n### Approach\nThe BM25 ranking function involves several steps to compute the score for each document in the corpus based on a query. Here's a detailed breakdown of the approach:\n\n1. **Preprocess the Corpus and Query**: Convert each document in the corpus into a list of words and split the query into individual words.\n\n2. **Compute Document Lengths and Average Document Length**: Calculate the length of each document and the average length across all documents in the corpus.\n\n3. **Calculate Term Frequencies (TF)**: For each document, count the occurrences of each word to determine the term frequency.\n\n4. **Compute Inverse Document Frequencies (IDF)**: For each term in the query, determine how many documents it appears in. Use this to compute the IDF, which is the logarithm of the ratio of the total number of documents to the number of documents containing the term.\n\n5. **Apply BM25 Formula**: For each document, compute the BM25 score by summing the contributions of each query term. The contribution of each term is calculated using the BM25 formula, which considers TF, IDF, and document length normalization.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter, defaultdict\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    # Split the query into individual words\n    query_words = query.split()\n    \n    # Preprocess each document into a list of words\n    documents = [doc.split() for doc in corpus]\n    \n    # Calculate the total number of documents\n    N = len(documents)\n    if N == 0:\n        return []\n    \n    # Compute the average document length\n    total_length = sum(len(doc) for doc in documents)\n    avg_dl = total_length / N\n    \n    # Calculate how many documents each term appears in (term_docs)\n    term_docs = defaultdict(int)\n    for doc in documents:\n        unique_terms = set(doc)\n        for term in unique_terms:\n            term_docs[term] += 1\n    \n    # Compute IDF for each term in the query\n    idf = {}\n    for term in query_words:\n        df = term_docs.get(term, 0)\n        if df == 0:\n            idf[term] = 0.0\n        else:\n            idf[term] = np.log(N / df)\n    \n    # Calculate BM25 scores for each document\n    scores = []\n    for doc in documents:\n        dl = len(doc)\n        tf = Counter(doc)\n        score = 0.0\n        \n        # Consider only terms present in both the document and the query\n        for term in set(query_words) & set(tf.keys()):\n            tf_val = tf[term]\n            denominator = tf_val + k1 * (1 - b + b * (dl / avg_dl))\n            if denominator == 0:\n                contribution = 0.0\n            else:\n                contribution = (tf_val * (k1 + 1)) / denominator * idf[term]\n            score += contribution\n        \n        # Round the score to three decimal places\n        scores.append(round(score, 3))\n    \n    return scores\n```\n\n### Explanation\n1. **Preprocessing**: The corpus is converted into a list of word lists, and the query is split into individual words for processing.\n\n2. **Document Lengths**: The length of each document and the average document length are computed to normalize document scores.\n\n3. **Term Frequencies**: Using the `Counter` from the `collections` module, we count the occurrences of each word in every document.\n\n4. **Inverse Document Frequencies**: For each term in the query, we determine how many documents it appears in and compute its IDF using the formula `IDF = log(N / df)`, where `N` is the total number of documents and `df` is the document frequency of the term.\n\n5. **BM25 Calculation**: For each document, the BM25 score is computed by iterating over each query term present in the document. The score for each term is calculated using the BM25 formula, which considers TF, IDF, and document length normalization. The contributions of all query terms are summed to get the final score for the document.\n\nThis approach ensures that each document is scored based on its relevance to the query, taking into account term frequency, document length, and inverse document frequency, providing a robust ranking mechanism for information retrieval tasks."}
{"task_id": 91, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the F1 score, which is a widely used metric in machine learning to evaluate the performance of classification models. The F1 score is the harmonic mean of precision and recall, providing a balanced measure of both.\n\n### Approach\n1. **Understanding F1 Score**: The F1 score is calculated as the harmonic mean of precision and recall. Precision is the ratio of true positives to the sum of true positives and false positives, while recall is the ratio of true positives to the sum of true positives and false negatives. The F1 score is given by the formula:\n   \\[\n   F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n   \\]\n2. **Counting True Positives, False Positives, and False Negatives**: We iterate through the true labels and predicted labels to count the number of true positives (tp), false positives (fp), and false negatives (fn).\n3. **Handling Edge Cases**: We need to handle cases where the denominators for precision or recall are zero to avoid division by zero errors. In such cases, the respective metric is set to zero.\n4. **Computing Precision and Recall**: Using the counts from the previous step, we compute precision and recall. If the denominator for either is zero, we set the metric to zero.\n5. **Calculating F1 Score**: Using the precision and recall values, we compute the F1 score. If the sum of precision and recall is zero, the F1 score is set to zero to avoid division by zero.\n6. **Rounding the Result**: The final F1 score is rounded to three decimal places for the result.\n\n### Solution Code\n```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n    \n    for y_t, y_p in zip(y_true, y_pred):\n        if y_t == 1 and y_p == 1:\n            tp += 1\n        elif y_t == 0 and y_p == 1:\n            fp += 1\n        elif y_t == 1 and y_p == 0:\n            fn += 1\n    \n    # Calculate precision\n    if (tp + fp) == 0:\n        precision = 0.0\n    else:\n        precision = tp / (tp + fp)\n    \n    # Calculate recall\n    if (tp + fn) == 0:\n        recall = 0.0\n    else:\n        recall = tp / (tp + fn)\n    \n    # Calculate F1 score\n    if (precision + recall) == 0:\n        f1 = 0.0\n    else:\n        f1 = (2 * precision * recall) / (precision + recall)\n    \n    # Round to three decimal places\n    return round(f1, 3)\n```\n\n### Explanation\n- **Counting Metrics**: The code iterates through each pair of true and predicted labels to count true positives, false positives, and false negatives.\n- **Precision and Recall Calculation**: Precision is calculated as the ratio of true positives to the sum of true positives and false positives. Recall is calculated as the ratio of true positives to the sum of true positives and false negatives. Both are handled to avoid division by zero.\n- **F1 Score Calculation**: The F1 score is computed using the harmonic mean formula, ensuring to handle cases where precision and recall might both be zero.\n- **Rounding**: The result is rounded to three decimal places to meet the problem requirements.\n\nThis approach ensures that the F1 score is accurately computed and handles all edge cases, providing a reliable measure of classification performance."}
{"task_id": 92, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to forecast the power consumption for day 15 of a Mars colony, considering both a linear trend and a daily fluctuation. The approach involves detrending the data, fitting a linear regression model, predicting the base consumption, adding back the fluctuation, and including a safety margin.\n\n### Approach\n1. **Detrend the Data**: Subtract the known daily fluctuation from each data point. The fluctuation is given by the formula \\( f(i) = 10 \\times \\sin(2\\pi i / 10) \\), where \\( i \\) is the day number.\n2. **Linear Regression**: Fit a linear regression model to the detrended data to identify the linear trend. The model is of the form \\( y = a \\times x + b \\), where \\( x \\) is the day number and \\( y \\) is the detrended power consumption.\n3. **Predict Base Consumption**: Use the linear regression model to predict the base power consumption for day 15.\n4. **Add Fluctuation for Day 15**: Compute the fluctuation for day 15 and add it to the predicted base consumption.\n5. **Safety Margin**: Round the result, add a 5% safety margin, and then round up to ensure the result is an integer.\n\n### Solution Code\n```python\nimport math\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Step 1: Subtract the daily fluctuation from each data point\n    detrended = []\n    for i in range(1, 11):  # Days 1 to 10\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_value = consumption_data[i-1] - fluctuation\n        detrended.append(detrended_value)\n    \n    # Step 2: Perform linear regression on the detrended data\n    X = list(range(1, 11))  # Days 1 to 10\n    Y = detrended\n    \n    x_mean = sum(X) / len(X)\n    y_mean = sum(Y) / len(Y)\n    \n    numerator = 0.0\n    denominator = 0.0\n    for x, y in zip(X, Y):\n        numerator += (x - x_mean) * (y - y_mean)\n        denominator += (x - x_mean) ** 2\n    \n    a = numerator / denominator\n    b = y_mean - a * x_mean\n    \n    # Step 3: Predict day 15's base consumption\n    x_pred = 15\n    y_pred = a * x_pred + b  # Base prediction without fluctuation\n    \n    # Step 4: Add the fluctuation for day 15\n    fluctuation_day15 = 10 * math.sin(2 * PI * 15 / 10)\n    total_pred = y_pred + fluctuation_day15  # This adds 0 in this case\n    \n    # Step 5: Round, add 5% safety margin, then round up\n    rounded_pred = round(total_pred)\n    safety_margin = rounded_pred * 1.05\n    final_value = math.ceil(safety_margin)\n    \n    return final_value\n```\n\n### Explanation\n1. **Detrending**: Each data point is adjusted by subtracting the known fluctuation, resulting in a detrended dataset that reflects the underlying linear trend.\n2. **Linear Regression**: The detrended data is used to compute the coefficients of a linear model, which captures the trend in power consumption.\n3. **Prediction**: The linear model predicts the base power consumption for day 15.\n4. **Fluctuation Adjustment**: The fluctuation for day 15 is computed and added to the base prediction. For day 15, the fluctuation is zero.\n5. **Safety Margin**: The result is rounded, a 5% safety margin is added, and the final value is rounded up to ensure it meets the colony's needs.\n\nThis approach ensures accurate forecasting by accounting for both the linear trend and daily fluctuations, with an added safety margin to handle uncertainties."}
{"task_id": 93, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Mean Absolute Error (MAE) between two arrays of actual and predicted values. MAE is a metric used to measure the average magnitude of errors in a set of predictions without considering their direction.\n\n### Approach\nThe approach to calculate the MAE involves the following steps:\n1. **Compute Absolute Differences**: Calculate the absolute differences between corresponding elements in the actual values array (`y_true`) and the predicted values array (`y_pred`). This is done using the formula `|y_true - y_pred|`.\n2. **Sum the Differences**: Sum all the absolute differences computed in the previous step.\n3. **Compute the Mean**: Divide the sum of absolute differences by the number of elements to get the mean. This gives the Mean Absolute Error.\n4. **Round the Result**: Round the resulting MAE value to three decimal places for the final output.\n\nThis approach ensures that we efficiently compute the MAE using vectorized operations provided by the NumPy library, which is optimized for performance.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    mae_value = np.mean(np.abs(y_true - y_pred))\n    return round(mae_value, 3)\n```\n\n### Explanation\n- **Absolute Differences**: The function uses `np.abs(y_true - y_pred)` to compute the element-wise absolute differences between the actual and predicted values.\n- **Mean Calculation**: The `np.mean()` function is used to compute the average of these absolute differences, which directly gives the MAE.\n- **Rounding**: The result is rounded to three decimal places using the `round()` function to meet the specified output requirement.\n\nThis implementation is efficient and concise, leveraging NumPy's capabilities for vectorized operations to ensure optimal performance even for large arrays."}
{"task_id": 94, "completion_id": 0, "completion": "\n\nHere's the implementation of the multi-head attention mechanism using the specified functions:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the query, key, and value matrices.\n    \n    Args:\n    X (np.ndarray): Input data.\n    W_q (np.ndarray): Weight matrix for queries.\n    W_k (np.ndarray): Weight matrix for keys.\n    W_v (np.ndarray): Weight matrix for values.\n    \n    Returns:\n    tuple: A tuple containing the query, key, and value matrices.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray = None) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention for a single head.\n    \n    Args:\n    Q (np.ndarray): Query matrix.\n    K (np.ndarray): Key matrix.\n    V (np.ndarray): Value matrix.\n    mask (np.ndarray, optional): Mask to apply (if any). Defaults to None.\n    \n    Returns:\n    np.ndarray: The output of the self-attention mechanism.\n    \"\"\"\n    d_k = Q.shape[1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    \n    if mask is not None:\n        scores = scores * mask + (1 - mask) * (-1e9)\n    \n    # Compute softmax\n    e_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    sum_e = np.sum(e_scores, axis=1, keepdims=True)\n    attention_weights = e_scores / sum_e\n    \n    output = attention_weights @ V\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention.\n    \n    Args:\n    X (np.ndarray): Input data.\n    W_q (np.ndarray): Weight matrix for queries.\n    W_k (np.ndarray): Weight matrix for keys.\n    W_v (np.ndarray): Weight matrix for values.\n    n_heads (int): Number of attention heads.\n    \n    Returns:\n    list: The output of the multi-head attention mechanism as a list.\n    \"\"\"\n    d_k = W_q.shape[1] // n_heads\n    outputs = []\n    \n    for h in range(n_heads):\n        start = h * d_k\n        end = (h + 1) * d_k\n        \n        W_q_head = W_q[:, start:end]\n        W_k_head = W_k[:, start:end]\n        W_v_head = W_v[:, start:end]\n        \n        Q, K, V = compute_qkv(X, W_q_head, W_k_head, W_v_head)\n        output = self_attention(Q, K, V)\n        outputs.append(output)\n    \n    concatenated = np.concatenate(outputs, axis=1)\n    rounded = np.round(concatenated, 4)\n    return rounded.tolist()\n```\n\nThis code implements the multi-head attention mechanism by first computing the query, key, and value matrices for each head, then applying self-attention for each head, and finally concatenating the outputs from all heads. The result is rounded to four decimal places and returned as a list."}
{"task_id": 95, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Phi coefficient, which is a measure of the correlation between two binary variables. The Phi coefficient is particularly useful for assessing the correlation between two dichotomous variables.\n\n### Approach\nThe Phi coefficient is calculated using a 2x2 contingency table derived from the input lists. The steps to compute the Phi coefficient are as follows:\n\n1. **Create a Contingency Table**: Construct a 2x2 table where each cell represents the count of occurrences of each combination of values from the two input lists. Specifically, the cells are:\n   - a: Count of instances where both variables are 1.\n   - b: Count of instances where the first variable is 1 and the second is 0.\n   - c: Count of instances where the first variable is 0 and the second is 1.\n   - d: Count of instances where both variables are 0.\n\n2. **Calculate the Numerator and Denominator**: Use the counts from the contingency table to compute the numerator and denominator of the Phi coefficient formula. The numerator is given by \\(ad - bc\\), and the denominator is the square root of the product of the marginal totals, which is \\(\\sqrt{(a+b)(a+c)(b+d)(c+d)}\\).\n\n3. **Handle Edge Cases**: If the denominator is zero, it indicates that there is no variation in one or both variables, and the Phi coefficient is undefined. In such cases, we return 0.0.\n\n4. **Compute the Phi Coefficient**: If the denominator is non-zero, compute the Phi coefficient as the ratio of the numerator to the denominator and round the result to four decimal places.\n\n### Solution Code\n```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    a = sum(1 for xi, yi in zip(x, y) if xi == 1 and yi == 1)\n    b = sum(1 for xi, yi in zip(x, y) if xi == 1 and yi == 0)\n    c = sum(1 for xi, yi in zip(x, y) if xi == 0 and yi == 1)\n    d = sum(1 for xi, yi in zip(x, y) if xi == 0 and yi == 0)\n    \n    numerator = a * d - b * c\n    denominator = ((a + b) * (c + d) * (a + c) * (b + d)) ** 0.5\n    \n    if denominator == 0:\n        return 0.0\n    else:\n        phi = numerator / denominator\n        return round(phi, 4)\n```\n\n### Explanation\n- **Contingency Table Construction**: The code iterates through the input lists to count the occurrences of each combination of values, storing these counts in variables a, b, c, and d.\n- **Numerator and Denominator Calculation**: Using the counts from the contingency table, the code computes the numerator and denominator of the Phi coefficient formula.\n- **Edge Case Handling**: If the denominator is zero, indicating no variation in one or both variables, the function returns 0.0.\n- **Phi Coefficient Calculation**: The function computes the Phi coefficient and rounds it to four decimal places for the final result.\n\nThis approach ensures that the Phi coefficient is accurately calculated and handles edge cases where the coefficient might be undefined."}
{"task_id": 96, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Hard Sigmoid activation function, which is a computationally efficient approximation of the standard sigmoid function. The Hard Sigmoid function is defined as a piecewise linear function that maps input values to outputs between 0 and 1.\n\n### Approach\nThe Hard Sigmoid function is defined as follows:\n- For input values less than or equal to -2.5, the output is 0.\n- For input values greater than or equal to 2.5, the output is 1.\n- For input values between -2.5 and 2.5, the output is a linear function given by \\((x + 2.5) / 5\\).\n\nThis approach ensures that the function is computationally efficient and avoids the exponential computations required by the standard sigmoid function.\n\n### Solution Code\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return (x + 2.5) / 5.0\n```\n\n### Explanation\nThe function `hard_sigmoid` takes a single float input `x` and returns the corresponding output based on the defined piecewise conditions:\n1. If `x` is less than or equal to -2.5, the function returns 0.0.\n2. If `x` is greater than or equal to 2.5, the function returns 1.0.\n3. For values of `x` between -2.5 and 2.5, the function computes the output using the linear formula \\((x + 2.5) / 5.0\\).\n\nThis implementation efficiently handles all possible input values and ensures the output is within the range [0, 1], making it suitable for various applications in machine learning and neural networks where an efficient activation function is required."}
{"task_id": 97, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the ELU (Exponential Linear Unit) activation function. ELU is designed to address the limitations of the ReLU activation function by providing non-zero gradients for negative inputs, which helps in training neural networks more effectively.\n\n### Approach\nThe ELU activation function is defined as follows:\n- For positive inputs (x \u2265 0), the output is the same as the input.\n- For negative inputs (x < 0), the output is computed using an exponential function scaled by a parameter alpha.\n\nThe formula for ELU is:\n- ELU(x) = x, if x \u2265 0\n- ELU(x) = alpha * (e^x - 1), if x < 0\n\nHere, alpha is a hyperparameter that controls the saturation value for negative inputs. The default value of alpha is 1.0.\n\nThe steps to implement the ELU function are:\n1. Check if the input x is non-negative. If true, return x.\n2. If x is negative, compute the value using the exponential function and scale it by alpha.\n3. Round the result to four decimal places before returning it.\n\n### Solution Code\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to the 4th decimal place\n    \"\"\"\n    if x >= 0:\n        value = x\n    else:\n        value = alpha * (math.exp(x) - 1)\n    return round(value, 4)\n```\n\n### Explanation\n- The function first checks if the input x is non-negative. If it is, the function returns x directly.\n- For negative inputs, the function computes the value using the exponential of x, subtracts 1, and then scales the result by alpha.\n- The result is then rounded to four decimal places to meet the specified precision requirement.\n\nThis approach ensures that the ELU function behaves as expected for both positive and negative inputs, providing a smooth transition and mitigating the dying ReLU problem by allowing gradients for negative values."}
{"task_id": 98, "completion_id": 0, "completion": "\n\nTo implement the PReLU activation function, we need to handle both positive and negative input values differently. For positive values, the output remains the same as the input, while for negative values, the output is scaled by a learnable parameter, alpha.\n\n**Step-by-Step Explanation:**\n\n1. **Understand the PReLU Function**: The PReLU function is defined as:\n   - \\( f(x) = x \\) for \\( x \\geq 0 \\)\n   - \\( f(x) = \\alpha \\times x \\) for \\( x < 0 \\)\n   Here, \\( \\alpha \\) is a learnable parameter that determines the slope for negative values.\n\n2. **Function Implementation**: The function takes two parameters: the input value `x` and the slope parameter `alpha` (defaulting to 0.25 if not provided).\n\n3. **Conditional Check**: The function checks if `x` is non-negative. If true, it returns `x`. If false, it returns the product of `alpha` and `x`.\n\n4. **Edge Cases**: The function correctly handles the case where `x` is zero by treating it as a non-negative value, thus returning zero.\n\n**Code:**\n\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x\n```\n\nThis implementation efficiently computes the PReLU activation by using a conditional expression, ensuring clarity and conciseness."}
{"task_id": 99, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Softplus activation function, which is a smooth approximation of the ReLU function. The Softplus function is defined as log(1 + e^x). However, we need to handle edge cases to prevent numerical overflow or underflow.\n\n### Approach\nThe Softplus function is mathematically defined as log(1 + e^x). However, directly computing this for very large or very small values of x can lead to numerical issues. To address this, we handle the following cases:\n\n1. **Large Positive x (x >= 20):** For very large positive values of x, e^x becomes extremely large, making 1 negligible in comparison. Thus, log(1 + e^x) approximates to x. Therefore, for x >= 20, we directly return x.\n\n2. **Large Negative x (x <= -20):** For very large negative values of x, e^x becomes extremely small, making 1 + e^x approximately 1. Thus, log(1 + e^x) approximates to 0. Therefore, for x <= -20, we return 0.\n\n3. **Intermediate Values (-20 < x < 20):** For values of x within this range, we compute the Softplus function directly using the formula log(1 + e^x).\n\nAfter computing the result, we round it to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to four decimal places.\n    \"\"\"\n    if x >= 20:\n        result = x\n    elif x <= -20:\n        result = 0.0\n    else:\n        result = math.log(1 + math.exp(x))\n    return round(result, 4)\n```\n\n### Explanation\n- **Handling Large Positive x:** For x >= 20, the function returns x directly because e^x is so large that 1 becomes negligible, making log(1 + e^x) approximately equal to x.\n- **Handling Large Negative x:** For x <= -20, the function returns 0 because e^x is so small that 1 + e^x is approximately 1, making log(1 + e^x) approximately 0.\n- **Intermediate Values:** For values of x between -20 and 20, the function computes log(1 + e^x) directly, ensuring numerical stability and accuracy.\n- **Rounding:** The result is rounded to four decimal places to meet the specified output constraints.\n\nThis approach efficiently handles all edge cases and ensures that the Softplus function is computed accurately without numerical overflow or underflow."}
{"task_id": 100, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Softsign activation function, which is a smooth activation function used in neural networks. The Softsign function ensures that the output is bounded between -1 and 1, making it suitable for various applications in machine learning.\n\n### Approach\nThe Softsign activation function is defined by the formula:\n\\[ f(x) = \\frac{x}{1 + |x|} \\]\nThis formula ensures that the output is always between -1 and 1. For large positive values of \\( x \\), the output approaches 1, and for large negative values, it approaches -1. The function is smooth and continuous, making it a good choice for neural networks where activation functions need to be differentiable.\n\nThe steps to implement the Softsign function are as follows:\n1. Compute the absolute value of the input \\( x \\).\n2. Add 1 to this absolute value to form the denominator.\n3. Divide the input \\( x \\) by this computed denominator.\n4. Round the result to four decimal places to meet the specified precision requirement.\n\n### Solution Code\n```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to four decimal places.\n    \"\"\"\n    denominator = 1 + abs(x)\n    result = x / denominator\n    return round(result, 4)\n```\n\n### Explanation\n1. **Compute the Denominator**: The denominator is calculated as \\( 1 + |x| \\). This ensures that the denominator is always positive and avoids division by zero.\n2. **Compute the Result**: The result is obtained by dividing \\( x \\) by the computed denominator. This step ensures that the output is scaled between -1 and 1.\n3. **Round the Result**: The result is rounded to four decimal places using Python's `round` function to meet the precision requirement.\n\nThis approach efficiently computes the Softsign activation function, ensuring the output is smooth and bounded, which is crucial for training neural networks effectively."}
{"task_id": 101, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the GRPO (Group Relative Policy Optimization) objective function, which is used to optimize policy parameters in reinforcement learning. The GRPO objective function combines a clipped likelihood ratio term with a KL divergence penalty to ensure training stability.\n\n### Approach\nThe GRPO objective function is designed to maximize the expected advantage while penalizing deviations from a reference policy using a KL divergence term. The key steps involved in computing the GRPO objective are:\n\n1. **Clipping Likelihood Ratios**: Compute the clipped likelihood ratios to ensure they lie within a specified range, which helps in stabilizing the training process.\n2. **Advantage Calculation**: Calculate the advantage for each sample using both the original and clipped likelihood ratios.\n3. **Minimum Advantage Selection**: For each sample, take the minimum value between the original and clipped advantages to form the objective function's primary term.\n4. **KL Divergence Penalty**: Compute the KL divergence between the old policy and the reference policy to penalize deviations from the reference policy.\n5. **Combine Terms**: Combine the average of the minimum advantages with the KL divergence penalty to form the final objective function value.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert input lists to numpy arrays\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Compute clipped likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    \n    # Calculate advantages for both original and clipped rhos\n    advantages = rhos * A\n    clipped_advantages = clipped_rhos * A\n    \n    # Take the minimum of original and clipped advantages element-wise\n    min_advantages = np.minimum(advantages, clipped_advantages)\n    \n    # Compute the average of the minimum advantages\n    avg_min_adv = np.mean(min_advantages)\n    \n    # Calculate the KL divergence term\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    \n    # Compute the GRPO objective\n    objective = avg_min_adv - beta * kl_divergence\n    \n    # Round the result to the nearest 6th decimal place\n    return round(objective, 6)\n```\n\n### Explanation\n1. **Clipping Likelihood Ratios**: The likelihood ratios are clipped to ensure they lie within the range [1-epsilon, 1+epsilon]. This prevents large updates and stabilizes training.\n2. **Advantage Calculation**: The advantage for each sample is computed using both the original and clipped likelihood ratios. This helps in forming a stable estimate of the policy's performance.\n3. **Minimum Advantage Selection**: By taking the minimum of the original and clipped advantages, we ensure that the policy updates are conservative and do not deviate too far from the previous policy.\n4. **KL Divergence Penalty**: The KL divergence between the old policy and the reference policy is computed to penalize deviations from the reference policy, ensuring that the new policy does not drift too far from the desired behavior.\n5. **Combining Terms**: The final objective function combines the average of the minimum advantages with the KL divergence penalty, weighted by a coefficient beta, to form a balanced optimization criterion.\n\nThis approach ensures that the policy is optimized for maximum expected advantage while maintaining stability and adherence to the reference policy."}
{"task_id": 102, "completion_id": 0, "completion": "\n\nTo implement the Swish activation function, we follow these steps:\n\n1. **Understand the Formula**: The Swish function is defined as \\( f(x) = x \\cdot \\text{sigmoid}(\\beta x) \\), where \\( \\beta \\) is typically set to 1 for simplicity.\n2. **Compute the Sigmoid**: The sigmoid function is calculated as \\( \\text{sigmoid}(x) = \\frac{1}{1 + e^{-x}} \\).\n3. **Multiply by Input**: Multiply the input \\( x \\) by the computed sigmoid value.\n4. **Round the Result**: Ensure the result is rounded to four decimal places for precision.\n\nHere is the Python code implementing the Swish activation function:\n\n```python\nimport math\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    beta = 1.0\n    sigmoid = 1.0 / (1.0 + math.exp(-beta * x))\n    result = x * sigmoid\n    return round(result, 4)\n```\n\nThis function computes the Swish value for a given input \\( x \\) using the specified formula and returns the result rounded to four decimal places."}
{"task_id": 103, "completion_id": 0, "completion": "\n\nTo implement the SELU activation function, we follow these steps:\n\n1. **Check the Input Sign**: Determine if the input `x` is non-negative or negative.\n2. **Compute SELU Value**:\n   - For non-negative `x`, the output is `scale * x`.\n   - For negative `x`, compute `scale * alpha * (exp(x) - 1)`.\n3. **Ensure Numerical Stability**: Use the exponential function from the math library.\n4. **Round the Result**: Round the computed value to four decimal places for consistency.\n\nHere is the Python code implementing the SELU function:\n\n```python\nimport math\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal.\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    if x >= 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * alpha * (math.exp(x) - 1)\n    \n    # Round to the nearest 4th decimal\n    selu_value = round(selu_value, 4)\n    return selu_value\n```\n\n**Example Use Cases**:\n\n- For `x = 0`, the output is `0.0`.\n- For `x = 1`, the output is approximately `1.0507`.\n- For `x = -1`, the output is approximately `-1.1117`.\n\nThis implementation ensures that the SELU function is computed accurately and returns values rounded to four decimal places, maintaining numerical stability across all input ranges."}
{"task_id": 104, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a prediction function for binary classification using logistic regression. The function will compute class probabilities using the sigmoid function and return binary predictions based on a threshold of 0.5.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Compute the Linear Combination**: Calculate the linear combination of the input features with the model weights and add the bias term. This is done using matrix multiplication followed by the addition of the bias.\n2. **Apply Sigmoid Function**: Convert the linear combination results into probabilities using the sigmoid function. The sigmoid function maps any real number to a value between 0 and 1.\n3. **Threshold Probabilities**: Convert these probabilities into binary predictions using a threshold of 0.5. If the probability is 0.5 or higher, the prediction is 1; otherwise, it is 0.\n4. **Return Results**: Convert the resulting predictions into a list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of X and weights, then add bias\n    z = np.dot(X, weights) + bias\n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-z))\n    # Threshold the probabilities to get binary predictions\n    predictions = np.where(probabilities >= 0.5, 1, 0)\n    # Convert numpy array to list and return\n    return predictions.tolist()\n```\n\n### Explanation\n1. **Linear Combination Calculation**: The function starts by computing the linear combination of the input features (X) and the model weights. This is done using `np.dot(X, weights)`, which performs matrix multiplication, and then adds the bias term.\n2. **Sigmoid Function Application**: The result from the linear combination is passed through the sigmoid function, which converts these values into probabilities between 0 and 1.\n3. **Thresholding**: The probabilities are then thresholded at 0.5. Values equal to or greater than 0.5 are classified as 1, and values less than 0.5 are classified as 0.\n4. **Result Conversion**: The final predictions are converted from a numpy array to a list using `tolist()` before being returned.\n\nThis approach ensures that we accurately predict binary outcomes based on the given logistic regression model parameters."}
{"task_id": 105, "completion_id": 0, "completion": "\n\nHere's the implementation of the gradient descent-based training algorithm for Softmax regression:\n\n```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Args:\n        X (np.ndarray): Input features of shape (N, M) where N is the number of samples and M is the number of features.\n        y (np.ndarray): Target vector of shape (N,) containing class labels.\n        learning_rate (float): Learning rate for gradient descent.\n        iterations (int): Number of iterations to run gradient descent.\n\n    Returns:\n        tuple[list[float], list[float]]: A tuple containing the optimized coefficients B as a flattened list and the collected losses over iterations, both rounded to 4 decimal places.\n    \"\"\"\n    # Get the number of samples and features\n    N, M = X.shape\n    \n    # Determine the number of classes and create one-hot encoding for y\n    unique_ys = np.unique(y)\n    C = len(unique_ys)\n    class_to_idx = {cls: i for i, cls in enumerate(unique_ys)}\n    y_mapped = np.array([class_to_idx[c] for c in y])\n    Y_one_hot = np.zeros((N, C))\n    Y_one_hot[np.arange(N), y_mapped] = 1\n    \n    # Initialize parameters B with small random values\n    B = np.random.randn(C, M) * 0.1\n    \n    # List to collect losses over iterations\n    losses = []\n    \n    for _ in range(iterations):\n        # Compute scores\n        scores = X @ B\n        \n        # Stabilize scores to prevent overflow\n        max_scores = np.max(scores, axis=1, keepdims=True)\n        stabilized_scores = scores - max_scores\n        \n        # Compute exponentials and apply softmax\n        exp_scores = np.exp(stabilized_scores)\n        sum_scores = np.sum(exp_scores, axis=1, keepdims=True)\n        probs = exp_scores / sum_scores\n        \n        # Compute cross-entropy loss\n        loss = -np.mean(np.log(probs[np.arange(N), y_mapped]))\n        \n        # Compute gradient\n        gradient = (X.T @ (probs - Y_one_hot)) / N\n        \n        # Update parameters\n        B -= learning_rate * gradient\n        \n        # Collect loss rounded to 4 decimal places\n        losses.append(round(loss, 4))\n    \n    # Round the parameters and convert to list\n    B_rounded = np.round(B, 4)\n    B_list = B_rounded.reshape(-1).tolist()\n    \n    # Round losses to 4 decimal places (already done during collection)\n    \n    return B_list, losses\n```\n\nThis code implements Softmax regression using gradient descent with the following steps:\n\n1. **One-Hot Encoding**: Converts the target vector `y` into a one-hot encoded matrix for multi-class classification.\n2. **Parameter Initialization**: Initializes the parameter matrix `B` with small random values.\n3. **Gradient Descent Loop**: For each iteration:\n   - Computes the scores for each class.\n   - Applies the softmax function to obtain probabilities.\n   - Calculates the cross-entropy loss.\n   - Computes the gradient of the loss with respect to the parameters.\n   - Updates the parameters using the gradient and learning rate.\n4. **Loss Collection**: Collects the loss at each iteration, rounded to four decimal places.\n5. **Result Preparation**: Rounds the final parameters and converts them into a flattened list for the result.\n\nThe function returns the optimized parameters as a flattened list and the collected losses over iterations, both rounded to four decimal places."}
{"task_id": 106, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient descent-based training algorithm for logistic regression using Binary Cross Entropy loss. The goal is to compute the model parameters (weights and bias) and return the optimized coefficients along with the loss values collected over the iterations, both rounded to four decimal places.\n\n### Approach\n1. **Initialization**: Start by initializing the model parameters (weights and bias). Weights are initialized with small random values to avoid symmetry issues, and the bias is initialized to zero.\n2. **Predictions**: For each iteration, compute the predicted probabilities using the logistic function (sigmoid).\n3. **Loss Calculation**: Compute the Binary Cross Entropy loss, which measures the difference between the predicted probabilities and the actual labels. Clip the predicted probabilities to a small range to avoid numerical instability.\n4. **Gradient Computation**: Calculate the gradients of the loss with respect to the weights and bias. The gradient for the weights is computed using the matrix product of the transpose of the feature matrix and the difference between predicted probabilities and actual labels, averaged over all samples. The gradient for the bias is the average of the difference between predicted probabilities and actual labels.\n5. **Parameter Update**: Update the weights and bias using the computed gradients and the given learning rate.\n6. **Collection of Results**: After each iteration, collect the loss value. After completing all iterations, collect the optimized coefficients (weights and bias).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    n_samples, n_features = X.shape\n    weights = np.random.randn(n_features) * 0.01  # Initialize weights with small random values\n    bias = 0.0\n    loss_list = []\n\n    for _ in range(iterations):\n        # Compute linear combination\n        z = X.dot(weights) + bias\n        # Compute predictions with sigmoid function\n        p = 1 / (1 + np.exp(-z))\n        # Clip probabilities to avoid log(0)\n        p = np.clip(p, 1e-8, 1 - 1e-8)\n        # Compute Binary Cross Entropy loss\n        loss = (-y * np.log(p) - (1 - y) * np.log(1 - p)).mean()\n        loss_list.append(loss)\n\n        # Compute gradients\n        gradient_weights = (X.T.dot(p - y)) / n_samples\n        gradient_bias = np.mean(p - y)\n\n        # Update parameters\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n\n    # Prepare the coefficients by rounding to 4 decimal places\n    coefficients = np.append(weights, bias)\n    coefficients = np.round(coefficients, 4).tolist()\n    # Round loss values to 4 decimal places\n    loss_list = np.round(loss_list, 4).tolist()\n\n    return (loss_list, coefficients)\n```\n\n### Explanation\n- **Initialization**: Weights are initialized with small random values to ensure that the model does not start with all zeros, which could lead to slow convergence. The bias is initialized to zero.\n- **Predictions**: The logistic function (sigmoid) is used to convert the linear combination of features into probabilities.\n- **Loss Calculation**: The Binary Cross Entropy loss is computed as the average loss over all samples. Predicted probabilities are clipped to avoid numerical issues when computing logarithms.\n- **Gradient Computation**: The gradients are computed using matrix operations to efficiently update the parameters in the direction that minimizes the loss.\n- **Parameter Update**: The weights and bias are updated iteratively using the gradients and the learning rate, which controls the step size in the parameter space.\n- **Result Collection**: The loss values are collected after each iteration to monitor the convergence of the model. The final coefficients (weights and bias) are rounded and returned along with the loss values.\n\nThis approach ensures that the model parameters are optimized using gradient descent, minimizing the Binary Cross Entropy loss effectively. The solution is efficient and handles numerical stability issues by clipping predicted probabilities."}
{"task_id": 107, "completion_id": 0, "completion": "\n\nTo implement masked self-attention, we follow these steps:\n\n1. **Compute Attention Scores**: Calculate the dot product of the query (Q) and key (K) matrices, scaled by the square root of the key dimension.\n2. **Apply Attention Mask**: Modify the attention scores by setting masked positions to negative infinity to prevent them from contributing to the attention weights.\n3. **Compute Softmax**: Apply the softmax function to the masked scores to obtain the attention weights.\n4. **Compute Output**: Multiply the attention weights with the value (V) matrix to get the final output.\n\nHere is the implementation:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> list:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = K.shape[2]\n    K_transposed = K.transpose(0, 2, 1)\n    scores = np.matmul(Q, K_transposed) / np.sqrt(d_k)\n    \n    # Apply mask\n    mask_expanded = mask[np.newaxis, :, :]  # Add batch dimension\n    scores = np.where(mask_expanded == 0, -np.inf, scores)\n    \n    # Compute attention weights\n    attention_weights = np.softmax(scores, axis=-1)\n    \n    # Compute output\n    output = np.matmul(attention_weights, V)\n    \n    return output.tolist()\n```\n\n**Explanation:**\n\n- **compute_qkv Function**: This function computes the Query, Key, and Value matrices by multiplying the input matrix X with the respective weight matrices W_q, W_k, and W_v.\n- **masked_attention Function**:\n  - **Compute Scores**: The scores are computed by taking the dot product of Q and the transposed K matrix, then scaling by the square root of the key dimension.\n  - **Apply Mask**: The mask is expanded to match the batch dimension and applied to the scores, setting masked positions to -inf.\n  - **Softmax**: The softmax function is applied to the scores to get the attention weights.\n  - **Output Calculation**: The final output is obtained by multiplying the attention weights with the Value matrix V.\n\nThe function returns the output as a Python list by converting the numpy array using `tolist()`."}
{"task_id": 108, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the disorder of apples in a basket based on their colors. The disorder should be 0 if all apples are the same color and should increase as the variety of colors increases. \n\n### Approach\nThe approach to solve this problem involves using the concept of entropy from information theory. Entropy measures the amount of uncertainty or randomness in a system, which makes it a suitable metric for our problem. Here's a detailed breakdown of the approach:\n\n1. **Count Frequencies**: First, we count the frequency of each color in the list of apples. This helps us understand how many apples there are of each color.\n2. **Calculate Probabilities**: Using the counts, we calculate the probability of each color by dividing the count of each color by the total number of apples.\n3. **Compute Entropy**: Using the probabilities, we compute the entropy using the Shannon entropy formula. The formula for Shannon entropy is given by:\n   \\[\n   H = -\\sum (p_i \\log_2 p_i)\n   \\]\n   where \\( p_i \\) is the probability of each color. This formula gives a measure of the uncertainty or randomness in the distribution of colors.\n4. **Round the Result**: Finally, we round the computed entropy to four decimal places to get the disorder measure.\n\n### Solution Code\n```python\nfrom collections import Counter\nimport math\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The disorder is measured using Shannon entropy, which increases with more variety in colors.\n    \"\"\"\n    counts = Counter(apples)\n    total = len(apples)\n    if total == 0:\n        return 0.0\n    entropy = 0.0\n    for count in counts.values():\n        p = count / total\n        entropy -= p * math.log2(p)\n    return round(entropy, 4)\n```\n\n### Explanation\n- **Counting Frequencies**: We use the `Counter` class from the `collections` module to count how many times each color appears in the list.\n- **Calculating Probabilities**: For each color, we determine its probability by dividing its count by the total number of apples.\n- **Computing Entropy**: Using the probabilities, we compute the entropy. Each probability is multiplied by the logarithm (base 2) of itself, and the sum of these values is negated to get the entropy.\n- **Rounding**: The result is rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the disorder measure is 0 when all apples are the same color and increases as the variety of colors increases, satisfying all the given conditions."}
{"task_id": 109, "completion_id": 0, "completion": "\n\nTo implement layer normalization for sequence data, we need to normalize the input tensor across the feature dimension for each sample and each time step, then apply scaling and shifting parameters. This involves computing the mean and variance for each feature vector, normalizing the data, and then scaling and shifting it.\n\n### Approach\n1. **Compute Mean and Variance**: For each sample in the batch and each time step in the sequence, compute the mean and variance across the feature dimension.\n2. **Normalize**: Subtract the mean from each feature and divide by the standard deviation (sqrt of variance plus a small epsilon for stability).\n3. **Scale and Shift**: Multiply the normalized values by the scaling parameter (gamma) and add the shifting parameter (beta).\n4. **Round and Convert**: Round the result to 5 decimal places and convert it to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    mean = X.mean(axis=2, keepdims=True)\n    var = X.var(axis=2, keepdims=True)\n    std = np.sqrt(var + epsilon)\n    normalized_X = (X - mean) / std\n    scaled_X = gamma * normalized_X + beta\n    rounded = np.round(scaled_X, 5)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Mean and Variance Calculation**: The mean and variance are computed along the feature dimension (axis=2) for each sample and time step. Using `keepdims=True` ensures the result has the same dimensions as the input, making broadcasting easier.\n2. **Normalization**: Each feature is normalized by subtracting the mean and dividing by the standard deviation, which is the square root of the variance plus a small epsilon to avoid division by zero.\n3. **Scaling and Shifting**: The normalized values are scaled by gamma and shifted by beta, which are applied element-wise across the feature dimension.\n4. **Rounding and Conversion**: The final result is rounded to 5 decimal places for precision and converted to a list using `tolist()` for the required output format.\n\nThis approach efficiently handles the normalization and transformation of sequence data, ensuring each feature vector is properly scaled and shifted for each sample and time step."}
{"task_id": 111, "completion_id": 0, "completion": "\n\nTo compute the Pointwise Mutual Information (PMI), we use the formula that measures the association between two events by comparing their joint occurrence with their individual occurrences. The PMI is calculated using the logarithm base 2 of the ratio of the joint probability to the product of the individual probabilities.\n\n### Approach\n1. **Calculate the Denominator**: Compute the product of the total counts of the individual events.\n2. **Handle Zero Denominator**: If the denominator is zero, return 0.0 as the PMI is undefined in this context.\n3. **Compute the Ratio**: Calculate the ratio of the joint occurrence adjusted by the total number of samples to the product of individual counts.\n4. **Compute PMI**: Use the natural logarithm base 2 of the computed ratio.\n5. **Round the Result**: Return the PMI value rounded to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    denominator = total_counts_x * total_counts_y\n    if denominator == 0:\n        return 0.0\n    ratio = (joint_counts * total_samples) / denominator\n    pmi = np.log2(ratio)\n    return round(pmi, 3)\n```\n\n### Explanation\n- **Denominator Calculation**: The product of the total counts of the individual events (total_counts_x and total_counts_y) is computed to avoid division by zero errors.\n- **Zero Denominator Handling**: If either of the individual counts is zero, the PMI is returned as 0.0 since the events cannot co-occur.\n- **Ratio Calculation**: The ratio is computed to determine how much the joint occurrence differs from the expected chance occurrence.\n- **Logarithm and Rounding**: The logarithm base 2 of the ratio is taken to compute the PMI, which is then rounded to three decimal places for the final result.\n\nThis approach ensures that the PMI is computed efficiently and handles edge cases where the denominator might be zero, providing a robust solution."}
{"task_id": 112, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs Min-Max normalization on a list of integers. This normalization technique scales all values to the range [0, 1], ensuring that all features contribute equally to a model by scaling them to a common range.\n\n### Approach\nThe Min-Max normalization technique involves transforming each value in the list such that the smallest value is scaled to 0 and the largest value is scaled to 1. The formula used for this transformation is:\n\n\\[ \\text{normalized value} = \\frac{x - \\text{min}}{\\text{max} - \\text{min}} \\]\n\nwhere:\n- \\( x \\) is the current value being normalized.\n- \\( \\text{min} \\) is the minimum value in the list.\n- \\( \\text{max} \\) is the maximum value in the list.\n\nSteps to implement the solution:\n1. Compute the minimum value (\\( \\text{min} \\)) and the maximum value (\\( \\text{max} \\)) of the input list.\n2. Check if all values in the list are the same. If they are, return a list of 0.0s since no scaling is possible.\n3. For each value in the list, apply the Min-Max normalization formula.\n4. Round each normalized value to 4 decimal places.\n5. Return the list of normalized values.\n\n### Solution Code\n```python\ndef min_max(x: list[int]) -> list[float]:\n    min_x = min(x)\n    max_x = max(x)\n    if min_x == max_x:\n        return [0.0 for _ in x]\n    normalized = [(num - min_x) / (max_x - min_x) for num in x]\n    return [round(value, 4) for value in normalized]\n```\n\n### Explanation\n1. **Compute Min and Max**: The function starts by finding the minimum and maximum values of the input list. These values are crucial for the normalization process.\n2. **Check for Uniform Values**: If all values in the list are the same, the function returns a list of 0.0s to avoid division by zero and because no meaningful scaling is possible.\n3. **Normalize Each Value**: For each value in the list, the function applies the Min-Max normalization formula to scale it between 0 and 1.\n4. **Round Values**: Each normalized value is rounded to 4 decimal places to meet the specified output constraints.\n5. **Return Result**: The function returns the list of normalized and rounded values.\n\nThis approach ensures that the input list is transformed into a scaled version where all values lie within the range [0, 1], making it suitable for various machine learning algorithms that require feature scaling."}
{"task_id": 113, "completion_id": 0, "completion": "\n\nTo implement a simple residual block using NumPy, we follow these steps:\n\n1. **First Linear Transformation**: Multiply the input `x` by the weight matrix `w1`.\n2. **ReLU Activation**: Apply the ReLU activation function to the result of the first transformation.\n3. **Second Linear Transformation**: Multiply the result of the ReLU activation by the weight matrix `w2`.\n4. **Shortcut Connection**: Add the original input `x` to the result of the second transformation.\n5. **Final ReLU Activation**: Apply the ReLU activation function to the sum obtained from the shortcut connection.\n6. **Output Processing**: Round the final result to 4 decimal places and convert it to a list.\n\nHere is the code implementing these steps:\n\n```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    # First linear transformation\n    y1 = np.dot(w1, x)\n    # Apply ReLU activation\n    y1_relu = np.maximum(y1, 0)\n    # Second linear transformation\n    y2 = np.dot(w2, y1_relu)\n    # Add the original input (shortcut connection)\n    y2_plus_x = y2 + x\n    # Apply final ReLU activation\n    output = np.maximum(y2_plus_x, 0)\n    # Round to 4 decimal places and convert to list\n    return output.round(4).tolist()\n```\n\nThis function processes the input through two linear layers with ReLU activations, adds the original input as a shortcut, applies a final ReLU, and returns the result as a list rounded to four decimal places."}
{"task_id": 114, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs Global Average Pooling (GAP) on a 3D NumPy array. The function will take an input array of shape (height, width, channels) and return a 1D array where each element is the average of the corresponding feature map in the input.\n\n### Approach\nThe key idea behind Global Average Pooling is to reduce each feature map (which is a 2D array) into a single value by computing the average of all its elements. This is done for each channel in the input array. \n\n1. **Understanding the Input Shape**: The input array has a shape of (height, width, channels). Each channel represents a feature map, which is a 2D array of size height x width.\n2. **Computing the Average**: For each feature map (channel), we compute the average of all its elements. This can be efficiently done using NumPy's `mean` function, which allows us to specify the axes along which to compute the mean.\n3. **Using NumPy's mean Function**: By computing the mean along the first two axes (height and width), we effectively reduce each feature map to a single average value. This results in a 1D array where each element corresponds to the average of a feature map.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    return np.mean(x, axis=(0, 1))\n```\n\n### Explanation\n- **Input**: The function takes a 3D NumPy array `x` of shape (height, width, channels).\n- **Processing**: The `np.mean` function is used with `axis=(0, 1)`, which means the mean is computed along the height and width dimensions for each channel. This reduces each feature map to a single average value.\n- **Output**: The result is a 1D NumPy array of shape (channels,), where each element is the average of the corresponding feature map in the input array.\n\nThis approach efficiently computes the global average pooling using NumPy's optimized functions, ensuring both clarity and performance."}
{"task_id": 115, "completion_id": 0, "completion": "\n\nTo implement batch normalization for a 4D NumPy array in BCHW format, we follow these steps:\n\n1. **Compute the Mean**: Calculate the mean for each channel across the batch, height, and width dimensions.\n2. **Compute the Variance**: Calculate the variance for each channel using the population variance (divided by N).\n3. **Normalize**: Subtract the mean and divide by the square root of (variance + epsilon) to normalize the data.\n4. **Scale and Shift**: Apply the scale (gamma) and shift (beta) parameters to the normalized data.\n5. **Round and Convert**: Round the result to four decimal places and convert it to a list.\n\nHere is the implementation:\n\n```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Compute the mean for each channel across batch, height, and width\n    mean = X.mean(axis=(0, 2, 3))\n    \n    # Compute the variance for each channel using population variance (ddof=0)\n    var = X.var(axis=(0, 2, 3), ddof=0)\n    \n    # Normalize the input\n    normalized = (X - mean[:, np.newaxis, np.newaxis, np.newaxis]) / np.sqrt(var[:, np.newaxis, np.newaxis, np.newaxis] + epsilon)\n    \n    # Apply scale and shift\n    result = gamma[:, np.newaxis, np.newaxis, np.newaxis] * normalized + beta[:, np.newaxis, np.newaxis, np.newaxis]\n    \n    # Round to 4 decimal places and convert to list\n    return result.round(4).tolist()\n```\n\nThis function efficiently handles the batch normalization process, ensuring numerical stability with epsilon and correctly applying the normalization, scaling, and shifting operations."}
{"task_id": 116, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the derivative of a polynomial term of the form \\( c \\times x^n \\) at a given point \\( x \\). The derivative is calculated using the power rule from calculus, which states that the derivative of \\( x^n \\) is \\( n \\times x^{n-1} \\). \n\n### Approach\n1. **Understand the Power Rule**: The power rule in calculus tells us that the derivative of \\( x^n \\) with respect to \\( x \\) is \\( n \\times x^{n-1} \\). Therefore, the derivative of \\( c \\times x^n \\) is \\( c \\times n \\times x^{n-1} \\).\n2. **Handle Special Cases**: If the exponent \\( n \\) is zero, the term \\( c \\times x^0 \\) simplifies to \\( c \\times 1 \\), and its derivative is zero.\n3. **Compute the Derivative**: For non-zero exponents, compute the derivative using the formula \\( c \\times n \\times x^{n-1} \\).\n4. **Round the Result**: The result is rounded to four decimal places to meet the specified output constraints.\n\n### Solution Code\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    if n == 0:\n        return 0.0\n    else:\n        derivative = c * n * (x ** (n - 1))\n        return round(derivative, 4)\n```\n\n### Explanation\n1. **Check for Zero Exponent**: The function first checks if the exponent \\( n \\) is zero. If it is, the function returns 0.0 immediately since the derivative of a constant term is zero.\n2. **Compute the Derivative**: For non-zero exponents, the function calculates the derivative using the formula \\( c \\times n \\times x^{n-1} \\).\n3. **Round the Result**: The computed derivative is then rounded to four decimal places using Python's `round` function to ensure the output meets the specified precision.\n\nThis approach efficiently handles all edge cases, including when the exponent is zero or when the result needs to be rounded to four decimal places. The solution is both concise and efficient, ensuring accurate results for a wide range of inputs."}
{"task_id": 117, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process. This process helps in transforming a set of vectors into a set of orthogonal vectors, which can then be normalized to form an orthonormal basis. This is particularly useful in various applications, including machine learning for feature orthogonalization.\n\n### Approach\nThe Gram-Schmidt process involves the following steps:\n1. **Initialization**: Start with an empty list to hold the orthonormal basis vectors.\n2. **Processing Each Vector**: For each vector in the input list:\n   - Compute the projection of the current vector onto each of the previously computed orthonormal vectors.\n   - Subtract these projections from the current vector to obtain a new orthogonal vector.\n   - Normalize this orthogonal vector to form a unit vector.\n3. **Check for Linear Independence**: If the norm of the resulting vector is below a specified tolerance, it indicates that the vector is linearly dependent on the current basis and is thus skipped.\n4. **Normalization**: Convert each orthogonal vector into a unit vector to form the orthonormal basis.\n5. **Rounding and Conversion**: Round each basis vector to four decimal places and convert it to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    basis = []\n    for v in vectors:\n        v_np = np.array(v, dtype=float)\n        u = v_np.copy()\n        for e in basis:\n            proj = np.dot(u, e) * e\n            u -= proj\n        norm = np.linalg.norm(u)\n        if norm < tol:\n            continue\n        e = u / norm\n        basis.append(e)\n    rounded_basis = [np.round(vec, 4).tolist() for vec in basis]\n    return rounded_basis\n```\n\n### Explanation\n1. **Initialization**: The function starts by initializing an empty list `basis` to store the orthonormal vectors.\n2. **Vector Processing**: For each vector in the input list, it is converted to a numpy array for easier manipulation. The vector is then processed to subtract projections onto all previously computed basis vectors.\n3. **Projection and Subtraction**: For each basis vector computed so far, the projection of the current vector onto this basis vector is calculated and subtracted from the current vector. This ensures the resulting vector is orthogonal to all previous basis vectors.\n4. **Normalization**: The resulting orthogonal vector is normalized to form a unit vector, which is then added to the basis if its norm exceeds the specified tolerance.\n5. **Rounding and Conversion**: Each basis vector is rounded to four decimal places and converted to a list before being returned as the final result.\n\nThis approach efficiently computes the orthonormal basis, ensuring that the resulting vectors are both orthogonal and normalized, while handling cases of linear dependence appropriately."}
{"task_id": 118, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the cross product of two 3-dimensional vectors. The cross product results in a third vector that is perpendicular to both input vectors and follows the right-hand rule. This concept is fundamental in various fields such as physics, engineering, and 3D graphics.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Compute the Cross Product**: Utilize the `numpy.cross` function to compute the cross product of the two input vectors. This function efficiently handles the computation using optimized underlying routines.\n2. **Round the Result**: The resulting vector from the cross product is then rounded to four decimal places to meet the specified precision requirement.\n3. **Convert to List**: Finally, the rounded result is converted to a list format before being returned.\n\nThis approach leverages numpy's capabilities for efficient numerical computations, ensuring both accuracy and performance.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    c = np.cross(a, b)\n    c_rounded = np.round(c, 4)\n    return c_rounded.tolist()\n```\n\n### Explanation\n1. **Cross Product Calculation**: The `numpy.cross` function computes the cross product of two vectors `a` and `b`. This function internally handles the computation using the standard formula for the cross product in three dimensions, which results in a vector perpendicular to both input vectors.\n2. **Rounding**: The result of the cross product is rounded to four decimal places using `numpy.round` to ensure the required precision.\n3. **Conversion to List**: The rounded result, which is a numpy array, is converted to a Python list using the `tolist()` method before being returned.\n\nThis method ensures that the cross product is computed efficiently and accurately, with the result formatted as required."}
{"task_id": 119, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that solves a system of linear equations using Cramer's Rule. The function will take a square coefficient matrix \\( A \\) and a constant vector \\( b \\), and return the solution vector \\( x \\). If the system has no unique solution (i.e., the determinant of \\( A \\) is zero), the function should return -1.\n\n### Approach\n1. **Compute the Determinant of \\( A \\)**: The first step is to compute the determinant of the coefficient matrix \\( A \\). If the determinant is zero, the system does not have a unique solution, and we return -1.\n2. **Cramer's Rule Application**: For each variable \\( x_i \\), replace the \\( i \\)-th column of \\( A \\) with the vector \\( b \\) to form a new matrix \\( A_i \\). Compute the determinant of each \\( A_i \\) and then calculate \\( x_i \\) as the ratio of the determinant of \\( A_i \\) to the determinant of \\( A \\).\n3. **Rounding the Results**: After computing all \\( x_i \\) values, round each value to the nearest fourth decimal place.\n4. **Return the Result**: Convert the result to a Python list using numpy's `tolist()` method and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    det_A = np.linalg.det(A)\n    epsilon = 1e-10\n    if abs(det_A) < epsilon:\n        return -1\n    n = A.shape[0]\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_Ai = np.linalg.det(A_i)\n        x[i] = det_Ai / det_A\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\n```\n\n### Explanation\n1. **Determinant Check**: The function starts by computing the determinant of matrix \\( A \\). If the determinant is close to zero (considering floating point precision issues), it returns -1, indicating no unique solution.\n2. **Matrix Modification and Determinant Calculation**: For each column in \\( A \\), the function creates a new matrix by replacing the current column with the vector \\( b \\). It then computes the determinant of this new matrix.\n3. **Solution Calculation**: Each element of the solution vector \\( x \\) is calculated as the ratio of the determinant of the modified matrix to the determinant of \\( A \\).\n4. **Rounding and Conversion**: The solution vector is rounded to four decimal places and converted to a Python list before being returned.\n\nThis approach ensures that the system of linear equations is solved efficiently using Cramer's Rule, with proper handling of numerical precision and edge cases."}
{"task_id": 120, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Bhattacharyya distance between two discrete probability distributions. The Bhattacharyya distance is a measure of similarity between two probability distributions and is particularly useful in statistics and machine learning for comparing distributions.\n\n### Approach\nThe Bhattacharyya distance is defined as the square root of the sum of the squared differences between the square roots of the corresponding probabilities of the two distributions. The steps to compute this distance are as follows:\n\n1. **Check Validity**: Ensure that the input lists `p` and `q` are non-empty and of the same length. If not, return 0.0.\n2. **Compute Sum of Square Roots**: Calculate the sum of the square roots of the product of corresponding elements from `p` and `q`.\n3. **Compute Bhattacharyya Coefficient**: Subtract the computed sum from 1 to get the Bhattacharyya coefficient.\n4. **Calculate Distance**: Take the square root of the Bhattacharyya coefficient to get the distance.\n5. **Round Result**: Round the result to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    sum_sqrts = 0.0\n    for pi, qi in zip(p, q):\n        sum_sqrts += np.sqrt(pi * qi)\n    value = max(0.0, 1.0 - sum_sqrts)\n    distance = np.sqrt(value)\n    return round(distance, 4)\n```\n\n### Explanation\n1. **Validity Check**: The function first checks if the input lists are valid (non-empty and of the same length). If they are not, it returns 0.0 immediately.\n2. **Sum of Square Roots**: The function iterates through each pair of corresponding elements from `p` and `q`, computes the product, takes the square root of the product, and accumulates these values.\n3. **Bhattacharyya Coefficient**: The sum of the square roots is subtracted from 1. If this value is negative (due to floating-point precision issues), it is clamped to 0.\n4. **Distance Calculation**: The square root of the resulting value gives the Bhattacharyya distance.\n5. **Rounding**: The result is rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the function efficiently computes the Bhattacharyya distance while handling edge cases such as invalid inputs and numerical precision issues."}
{"task_id": 121, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the element-wise sum of two vectors (lists) and return the resulting vector. If the vectors have different lengths, the operation is invalid, and we should return -1.\n\n### Approach\n1. **Check Vector Lengths**: First, we check if the lengths of the two input vectors are the same. If they are not, we return -1 as the operation is invalid.\n2. **Element-wise Sum**: If the lengths are the same, we compute the element-wise sum using a list comprehension and the `zip` function. The `zip` function pairs elements from both vectors, and we sum each pair to form the resulting vector.\n\n### Solution Code\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for x, y in zip(a, b)]\n```\n\n### Explanation\n- **Check Lengths**: The function starts by comparing the lengths of the two input lists `a` and `b`. If they are not equal, it immediately returns -1.\n- **Element-wise Sum Calculation**: If the lengths are equal, the function uses a list comprehension with `zip` to iterate over pairs of elements from `a` and `b`. Each pair is summed, and the results are collected into a new list which is then returned.\n\nThis approach ensures that we efficiently and correctly handle both valid and invalid cases for element-wise vector addition."}
{"task_id": 122, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the policy gradient estimator using the REINFORCE algorithm. The goal is to compute the average gradient of the log-policy multiplied by the return at each time step for a given policy parameterized by a 2D NumPy array `theta`.\n\n### Approach\n1. **Problem Analysis**: The policy gradient method using REINFORCE involves calculating the gradient of the log-policy for each state-action pair and multiplying it by the cumulative return from that time step onwards. The policy for each state is obtained using a softmax function over the corresponding row of `theta`.\n\n2. **Return Calculation**: For each episode, compute the cumulative return for each time step starting from the end of the episode and moving backwards. This ensures that each time step's return is the sum of all subsequent rewards.\n\n3. **Gradient Calculation**: For each state-action pair in each episode, compute the gradient of the log-policy. The gradient for each action in the state is calculated based on the softmax probabilities. The gradient for the chosen action is `1 - pi[a]` and `-pi[a']` for all other actions.\n\n4. **Averaging Gradients**: Sum the gradients across all episodes and then average by dividing by the number of episodes to get the final policy gradient.\n\n5. **Rounding and Conversion**: Round the resulting gradients to four decimal places and convert the result to a list using NumPy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta)\n    num_episodes = len(episodes)\n    \n    for episode in episodes:\n        # Compute returns for each step in the episode\n        returns = []\n        total_return = 0.0\n        for step in reversed(episode):\n            total_return += step[2]\n            returns.append(total_return)\n        returns = returns[::-1]  # Reverse to get the correct order\n        \n        # Now, for each step in the episode\n        for t in range(len(episode)):\n            s, a, _ = episode[t]\n            Gt = returns[t]\n            \n            # Compute pi for state s\n            theta_s = theta[s]\n            exp_theta = np.exp(theta_s)\n            pi = exp_theta / exp_theta.sum()\n            \n            # Compute gradient for this (s,a)\n            grad = np.zeros(num_actions)\n            grad[a] = 1.0 - pi[a]\n            for a_prime in range(num_actions):\n                if a_prime != a:\n                    grad[a_prime] = -pi[a_prime]\n            \n            # Multiply by Gt and add to gradient\n            gradient[s] += grad * Gt\n    \n    # Average the gradient over all episodes\n    gradient /= num_episodes\n    \n    # Round to 4 decimal places\n    gradient = np.round(gradient, 4)\n    \n    # Convert to list and return\n    return gradient.tolist()\n```\n\n### Explanation\n1. **Initialization**: The function initializes a zero matrix `gradient` with the same shape as `theta` to accumulate the gradients.\n\n2. **Return Calculation**: For each episode, the returns are computed in reverse order to accumulate the rewards from the end of the episode to the beginning.\n\n3. **Policy and Gradient Calculation**: For each state-action pair in the episode, the policy probabilities are computed using the softmax function. The gradient of the log-policy is then calculated and scaled by the return for that time step.\n\n4. **Averaging**: The accumulated gradients are averaged over all episodes to obtain the final policy gradient.\n\n5. **Rounding and Conversion**: The gradients are rounded to four decimal places and converted to a list for the final output.\n\nThis approach ensures that we correctly compute the policy gradient using the REINFORCE algorithm, taking into account the cumulative returns and averaging over all episodes."}
{"task_id": 123, "completion_id": 0, "completion": "\n\nTo calculate the computational efficiency of a Mixture of Experts (MoE) layer compared to a dense layer, we need to determine the floating-point operations (FLOPs) for both and then compute the percentage savings.\n\n### Approach\n1. **Dense Layer FLOPs Calculation**: A dense layer with input dimension `d_in` and output dimension `d_out` performs `d_in * d_out` FLOPs.\n2. **MoE Layer FLOPs Calculation**: Each expert in the MoE layer handles a subset of the output dimensions. With `n_experts` experts, each expert's output dimension is `d_out / n_experts`. For each active expert, the FLOPs are `d_in * (d_out / n_experts)`. Since `k_active` experts are active, the total FLOPs for the MoE layer are `(k_active * d_in * d_out) / n_experts`.\n3. **Savings Calculation**: The computational savings are computed as the percentage reduction in FLOPs from the dense layer to the MoE layer.\n\n### Solution Code\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = d_in * d_out\n    moe_flops = (k_active * d_in * d_out) / n_experts\n    savings = ((dense_flops - moe_flops) / dense_flops) * 100\n    return round(savings, 1)\n```\n\n### Explanation\n- **Dense Layer FLOPs**: The number of operations is the product of the input and output dimensions.\n- **MoE Layer FLOPs**: Each expert processes a fraction of the output, reducing the total operations by a factor of `n_experts`, but only `k_active` experts are used.\n- **Savings**: The percentage reduction in FLOPs indicates the computational efficiency gain of using MoE over a dense layer.\n\nThis approach efficiently computes the savings, ensuring results are rounded to the nearest tenth for clarity."}
{"task_id": 124, "completion_id": 0, "completion": "\n\nTo implement the Noisy Top-K Gating Function, we follow these steps:\n\n1. **Compute Initial Scores**: Multiply the input matrix `X` with the gating weight matrix `W_g` to get the initial scores for each expert.\n2. **Add Noise**: Introduce noise by scaling the pre-sampled noise matrix `N` with `W_noise` and add it to the initial scores.\n3. **Select Top-K Experts**: For each input sample, identify the top-K experts based on the noisy scores.\n4. **Compute Softmax Probabilities**: Normalize the top-K scores using the softmax function to obtain the gating probabilities.\n5. **Construct Gating Matrix**: Create a sparse matrix where only the top-K experts have non-zero probabilities, which are the softmax values.\n\nHere is the Python code implementing these steps:\n\n```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Compute the initial scores\n    scores = np.dot(X, W_g)\n    \n    # Add noise to the scores\n    noisy_scores = scores + W_noise * N\n    \n    # Function to process each row\n    def process_row(row):\n        # Get the indices of the top k elements\n        inds = np.argpartition(row, -k)[-k:]\n        # Get the top k values\n        topk = row[inds]\n        # Compute softmax\n        exp_topk = np.exp(topk)\n        sum_exp = np.sum(exp_topk)\n        if sum_exp == 0:\n            # Handle potential division by zero by assigning zero probabilities\n            probs = np.zeros_like(exp_topk)\n        else:\n            probs = exp_topk / sum_exp\n        # Create the gating row\n        gating_row = np.zeros_like(row)\n        gating_row[inds] = probs\n        return gating_row\n    \n    # Apply the processing to each row\n    gating = np.apply_along_axis(process_row, axis=1, arr=noisy_scores)\n    \n    # Round to the nearest 4th decimal\n    gating = np.round(gating, 4)\n    \n    # Convert to a Python list\n    return gating.tolist()\n```\n\n**Explanation:**\n\n- **Initial Scores Calculation**: The scores are computed by taking the dot product of the input matrix `X` and the gating weight matrix `W_g`.\n- **Noise Addition**: The noise is added by scaling the pre-sampled noise matrix `N` with `W_noise` and adding it to the initial scores.\n- **Top-K Selection**: For each row in the noisy scores, the top-K indices are found using `np.argpartition`, which efficiently finds the top elements without full sorting.\n- **Softmax Calculation**: The top-K values are normalized using the softmax function to ensure they sum to 1, forming the gating probabilities.\n- **Result Construction**: A sparse matrix is constructed where only the top-K experts have non-zero probabilities, which are the computed softmax values. The result is rounded to four decimal places and converted to a list for the final output.\n\nThis approach efficiently handles the computation of the gating probabilities while ensuring sparsity and proper normalization."}
{"task_id": 125, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Sparse Mixture of Experts (MoE) layer using softmax gating and top-k routing. The goal is to compute the final output by selecting the top-k experts for each token, applying their transformations, and aggregating the results weighted by normalized gating probabilities.\n\n### Approach\n1. **Compute Expert Outputs**: Each expert's output is computed by multiplying the input tensor with the corresponding expert weight matrix. This results in a tensor where each element represents the output of an expert for each token.\n2. **Compute Gating Probabilities**: Using the gating weight matrix, compute the gating scores for each expert. These scores are then converted into probabilities using the softmax function.\n3. **Select Top-k Experts**: For each token, select the top-k experts based on the highest gating probabilities.\n4. **Normalize Probabilities**: Normalize the selected top-k probabilities to ensure they sum to 1 for each token.\n5. **Aggregate Outputs**: Compute the weighted sum of the outputs from the selected top-k experts using their normalized probabilities to get the final output for each token.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Compute expert outputs\n    We_transposed = We.transpose(1, 0, 2)\n    E = x.dot(We_transposed)\n    \n    # Compute gating probabilities\n    g = x.dot(Wg)\n    g = g - g.max(axis=1, keepdims=True)  # Numerical stability\n    g_exp = np.exp(g)\n    g_softmax = g_exp / g_exp.sum(axis=1, keepdims=True)\n    \n    # Get top-k indices\n    top_k_indices = np.argsort(-g_softmax, axis=1)[:, :top_k]\n    \n    # Gather selected expert outputs and their probabilities\n    batch_size = x.shape[0]\n    selected_E = E[np.arange(batch_size)[:, None], top_k_indices, :]\n    selected_probs = g_softmax[np.arange(batch_size)[:, None], top_k_indices]\n    \n    # Normalize probabilities\n    sum_probs = selected_probs.sum(axis=1, keepdims=True)\n    normalized_probs = selected_probs / sum_probs\n    \n    # Compute the final output\n    output = (selected_E * normalized_probs[:, :, None]).sum(axis=1)\n    \n    # Round to 4 decimal places\n    output = np.round(output, 4)\n    \n    # Return as a list\n    return output.tolist()\n```\n\n### Explanation\n1. **Expert Outputs**: The input tensor is multiplied by the transposed expert weight matrix to compute the outputs for all experts. This results in a 3D tensor where each element represents the output of an expert for each token.\n2. **Gating Probabilities**: The input tensor is multiplied by the gating weight matrix to compute raw scores. These scores are converted into probabilities using the softmax function, ensuring they sum to 1 for each token.\n3. **Top-k Selection**: For each token, the top-k experts with the highest gating probabilities are selected. This is done using efficient indexing operations.\n4. **Normalization**: The selected top-k probabilities are normalized to ensure they sum to 1, which is crucial for correct aggregation.\n5. **Aggregation**: The outputs from the selected experts are weighted by their normalized probabilities and summed to produce the final output for each token. The results are rounded to four decimal places and returned as a list.\n\nThis approach efficiently handles the computation using vectorized operations, ensuring scalability and performance even for larger inputs."}
{"task_id": 126, "completion_id": 0, "completion": "\n\nTo implement Group Normalization, we need to normalize the input tensor across specified groups of channels, then apply learned scaling and shifting parameters. Here's the step-by-step approach:\n\n1. **Reshape the Input Tensor**: Divide the channels into groups and reshape the tensor to facilitate group-wise operations.\n2. **Compute Mean and Variance**: For each group, compute the mean and variance across the spatial dimensions and all samples in the batch.\n3. **Normalize**: Subtract the mean and divide by the square root of the variance plus a small epsilon to avoid division by zero.\n4. **Apply Scale and Shift**: Multiply the normalized values by the learned scale parameters (gamma) and add the shift parameters (beta).\n5. **Reshape Back**: Return the tensor to its original shape and round the results to four decimal places.\n\nHere is the Python function implementing Group Normalization:\n\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    B, C, H, W = X.shape\n    G = C // num_groups\n    X_reshaped = X.reshape(B, num_groups, G, H, W)\n    \n    mean = X_reshaped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_reshaped.var(axis=(2, 3, 4), keepdims=True)\n    \n    normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    \n    gamma_reshaped = gamma.reshape(1, num_groups, 1, 1, 1)\n    beta_reshaped = beta.reshape(1, num_groups, 1, 1, 1)\n    \n    result = normalized * gamma_reshaped + beta_reshaped\n    result = result.reshape(B, C, H, W)\n    \n    # Round to 4 decimal places\n    result = np.round(result, 4)\n    \n    return result.tolist()\n```\n\nThis function processes the input tensor by normalizing each group of channels separately, then applies the scaling and shifting parameters to produce the final normalized tensor. The result is rounded to four decimal places and returned as a Python list."}
{"task_id": 127, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the minimum value of the function \\( f(x) = x^4 - 3x^3 + 2 \\) using the gradient descent algorithm. The goal is to determine the value of \\( x \\) where this function reaches its minimum, starting from a given initial position.\n\n### Approach\nThe gradient descent algorithm is an iterative optimization technique used to find the minimum of a function. It works by adjusting the parameters in the direction of the negative gradient, which leads towards the minimum. Here's a step-by-step breakdown of the approach:\n\n1. **Compute the Derivative**: The first derivative of the function \\( f(x) \\) is \\( f'(x) = 4x^3 - 9x^2 \\). This derivative helps determine the direction of the steepest descent.\n2. **Initialize Parameters**: Start with an initial value of \\( x \\) (given as `start_x`), a learning rate (which controls the step size in each iteration), a tolerance level (to determine convergence), and a maximum number of iterations.\n3. **Iterative Update**: In each iteration, update the current value of \\( x \\) using the gradient descent update rule: \\( x_{\\text{new}} = x_{\\text{current}} - \\text{learning_rate} \\times f'(x_{\\text{current}}) \\).\n4. **Convergence Check**: After each update, check if the change in \\( x \\) is below the specified tolerance. If it is, the algorithm has converged, and we stop the iterations.\n5. **Return Result**: Once the algorithm converges or reaches the maximum number of iterations, return the value of \\( x \\) rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    current_x = start_x\n    for _ in range(max_iters):\n        derivative = 4 * current_x**3 - 9 * current_x**2\n        next_x = current_x - learning_rate * derivative\n        change = abs(next_x - current_x)\n        if change < tolerance:\n            current_x = next_x\n            break\n        current_x = next_x\n    return round(current_x, 4)\n```\n\n### Explanation\n- **Derivative Calculation**: The derivative \\( f'(x) = 4x^3 - 9x^2 \\) is computed to determine the slope of the function at each point, guiding the direction of the update.\n- **Gradient Descent Update**: The update rule adjusts the current value of \\( x \\) by subtracting the product of the learning rate and the derivative, moving \\( x \\) towards the minimum.\n- **Convergence Check**: The algorithm stops early if the change in \\( x \\) between iterations is smaller than the specified tolerance, ensuring efficient computation.\n- **Rounding**: The result is rounded to four decimal places to meet the problem's output requirements.\n\nThis approach efficiently finds the minimum of the given function using gradient descent, ensuring convergence within a reasonable number of iterations and handling various starting points effectively."}
{"task_id": 128, "completion_id": 0, "completion": "\n\nTo implement the Dynamic Tanh (DyT) function, we follow the mathematical formulation that transforms each element of the input array using the given parameters. The function is designed to mimic the squashing behavior of the Tanh function without requiring normalization, making it suitable for use in Transformer architectures.\n\n### Approach\nThe Dynamic Tanh function is defined as:\n\\[ \\text{DyT}(x) = \\frac{\\gamma x + \\beta}{1 + \\alpha e^{-\\gamma x} + e^{\\gamma x}} \\]\n\nHere, \\(\\gamma\\), \\(\\alpha\\), and \\(\\beta\\) are learnable parameters that allow the function to adapt during training. The function transforms each element \\(x\\) of the input array by:\n1. Scaling \\(x\\) by \\(\\gamma\\) and adding \\(\\beta\\) to form the numerator.\n2. Computing the denominator using exponential terms scaled by \\(\\alpha\\) and \\(\\gamma\\).\n3. Dividing the numerator by the denominator to get the transformed value.\n4. Rounding the result to four decimal places and returning it as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    gamma_x = gamma * x\n    exp_gamma_x = np.exp(gamma_x)\n    exp_neg_gamma_x = np.exp(-gamma_x)\n    \n    numerator = gamma_x + beta\n    denominator = 1 + alpha * exp_neg_gamma_x + exp_gamma_x\n    \n    result = numerator / denominator\n    rounded_result = np.round(result, 4)\n    \n    return rounded_result.tolist()\n```\n\n### Explanation\n1. **Compute Intermediate Values**: For each element in the input array, compute \\(\\gamma x\\), \\(e^{\\gamma x}\\), and \\(e^{-\\gamma x}\\).\n2. **Form Numerator and Denominator**: The numerator is \\(\\gamma x + \\beta\\), and the denominator is \\(1 + \\alpha e^{-\\gamma x} + e^{\\gamma x}\\).\n3. **Transform and Round**: Divide the numerator by the denominator to get the transformed value, round to four decimal places, and convert the result to a list.\n\nThis approach ensures that the function efficiently transforms the input array while maintaining numerical stability and the desired squashing behavior."}
