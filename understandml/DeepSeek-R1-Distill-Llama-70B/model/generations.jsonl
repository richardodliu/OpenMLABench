{"task_id": 218, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to evaluate the outcome of a finished round of Blackjack based on the final hands of the player and the dealer. The goal is to determine the reward for the player according to specific rules.\n\n### Approach\n1. **Calculate Hand Scores**: The score of a hand is determined by summing the values of the cards. If the hand contains an Ace (represented by 1), it can be counted as 11 if it helps achieve a higher score without exceeding 21. This is done by checking if adding 10 to the sum (treating one Ace as 11) results in a valid score (\u2264 21).\n2. **Check for Natural Blackjack**: A natural blackjack is a two-card hand consisting of an Ace and a 10-value card. This is checked by verifying if the hand has exactly two cards, one of which is an Ace and the other is a 10.\n3. **Determine Outcome**: Based on the scores and whether either hand is a natural blackjack, the outcome is determined using the following rules:\n   - If the player's score exceeds 21, the player loses immediately.\n   - If the dealer's score exceeds 21 and the player's score does not, the player wins.\n   - If both scores are valid, the higher score wins. If the scores are equal, it's a draw.\n   - A player with a natural blackjack wins with a bonus reward if their score is higher than the dealer's.\n\n### Solution Code\n```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n    def calculate_score(hand):\n        sum_cards = sum(hand)\n        num_aces = hand.count(1)\n        if num_aces >= 1 and (sum_cards + 10) <= 21:\n            return sum_cards + 10\n        else:\n            return sum_cards\n\n    player_score = calculate_score(player)\n    dealer_score = calculate_score(dealer)\n\n    def is_natural(hand):\n        return len(hand) == 2 and (1 in hand) and (10 in hand)\n\n    player_natural = is_natural(player)\n    dealer_natural = is_natural(dealer)\n\n    if player_score > 21:\n        return -1.0\n    elif dealer_score > 21:\n        return 1.0\n    else:\n        if player_score > dealer_score:\n            if player_natural:\n                return 1.5\n            else:\n                return 1.0\n        elif dealer_score > player_score:\n            return -1.0\n        else:\n            return 0.0\n```\n\n### Explanation\n1. **Score Calculation**: The `calculate_score` function computes the score of a hand by summing the card values and adjusting for Aces if they can be counted as 11 without busting.\n2. **Natural Blackjack Check**: The `is_natural` function checks if a hand is a natural blackjack by verifying the presence of an Ace and a 10-value card in a two-card hand.\n3. **Outcome Determination**: The main function evaluates the scores and natural blackjack status to determine the reward based on the rules provided. It handles cases where either the player or dealer busts, compares scores, and applies the bonus for a natural blackjack win."}
{"task_id": 221, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Mean Squared Logarithmic Error (MSLE) metric, which is commonly used in regression tasks to evaluate model performance. The MSLE is calculated as the mean of the squared differences between the logarithms of the actual and predicted values.\n\n### Approach\nThe approach involves two main functions:\n1. **Helper Function (`squared_log_error`)**: This function computes the element-wise squared logarithmic errors between the actual and predicted values. It returns a NumPy array where each element is the squared error of the corresponding actual and predicted values.\n2. **Main Function (`mean_squared_log_error`)**: This function performs input validation, computes the squared errors using the helper function, and then calculates the mean of these squared errors. The result is rounded to four decimal places.\n\nThe steps for the main function are as follows:\n1. Convert the input sequences into NumPy arrays.\n2. Check if the shapes (lengths) of the actual and predicted arrays are the same.\n3. Ensure all values in both arrays are non-negative.\n4. Check if the arrays are non-empty.\n5. Compute the squared logarithmic errors using the helper function.\n6. Calculate the mean of these squared errors and round it to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"Helper function to compute element-wise squared log errors.\"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    log_actual = np.log(1 + actual)\n    log_predicted = np.log(1 + predicted)\n    errors = log_actual - log_predicted\n    squared_errors = errors ** 2\n    return squared_errors\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    \n    # Check if shapes are the same\n    if actual.shape != predicted.shape:\n        return -1\n    \n    # Check for non-negative values\n    if np.any(actual < 0) or np.any(predicted < 0):\n        return -1\n    \n    # Check for empty arrays\n    if actual.size == 0:\n        return -1\n    \n    # Compute squared errors\n    squared_errors = squared_log_error(actual, predicted)\n    \n    # Calculate the mean\n    msle = np.mean(squared_errors)\n    \n    # Round to four decimal places\n    msle_rounded = round(msle, 4)\n    \n    return msle_rounded\n```\n\n### Explanation\n1. **Helper Function (`squared_log_error`)**:\n   - Converts the input sequences into NumPy arrays.\n   - Computes the logarithm of each element after adding 1 to avoid taking the logarithm of zero or negative numbers.\n   - Calculates the element-wise differences between the logarithms of actual and predicted values.\n   - Squares these differences to get the squared errors and returns them as a NumPy array.\n\n2. **Main Function (`mean_squared_log_error`)**:\n   - Converts the input sequences into NumPy arrays for uniform processing.\n   - Validates the inputs to ensure they have the same shape, contain only non-negative values, and are non-empty.\n   - Uses the helper function to compute the squared errors.\n   - Computes the mean of these squared errors and rounds the result to four decimal places for the final output.\n\nThis approach ensures that the MSLE is computed accurately while handling various edge cases such as invalid inputs and empty arrays."}
{"task_id": 222, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the sigmoid activation function, which is commonly used in machine learning models. The sigmoid function maps any real-valued number to a value between 0 and 1. The challenge is to ensure the function works efficiently for both small and very large inputs, maintaining numerical stability.\n\n### Approach\nThe sigmoid function is defined as:\n\\[ \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} \\]\n\nHowever, directly computing this for very large positive or negative values can lead to numerical instability. To address this, we use a numerically stable approach by computing the function differently based on whether the input is non-negative or negative:\n- For \\( z \\geq 0 \\), compute \\( \\frac{1}{1 + e^{-z}} \\).\n- For \\( z < 0 \\), compute \\( \\frac{e^z}{1 + e^z} \\).\n\nThis approach avoids underflow and overflow issues for large values of \\( z \\).\n\nThe function should handle scalar values, lists, and NumPy arrays, returning the results in the appropriate format. For scalar inputs, the result is a float rounded to four decimal places. For lists or arrays, the result is a list with the same structure, each element rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    if isinstance(z, (int, float, np.number)):\n        # Handle scalar input\n        if z >= 0:\n            sig = 1.0 / (1.0 + math.exp(-z))\n        else:\n            sig = math.exp(z) / (1.0 + math.exp(z))\n        return round(sig, 4)\n    else:\n        # Handle array-like input\n        z_array = np.asarray(z, dtype=np.float64)\n        mask = z_array >= 0\n        sig = np.zeros_like(z_array)\n        # Compute for z >= 0\n        sig[mask] = 1.0 / (1.0 + np.exp(-z_array[mask]))\n        # Compute for z < 0\n        sig[~mask] = np.exp(z_array[~mask]) / (1.0 + np.exp(z_array[~mask]))\n        # Round to 4 decimal places\n        rounded = np.round(sig, 4)\n        # Convert to Python list and return\n        return rounded.tolist()\n```\n\n### Explanation\n1. **Scalar Handling**: If the input is a scalar (int, float, or NumPy number), the function computes the sigmoid using the stable approach for non-negative and negative values separately. The result is then rounded to four decimal places and returned as a float.\n\n2. **Array Handling**: For array-like inputs (lists or NumPy arrays), the function converts the input into a NumPy array. It then computes the sigmoid for each element using the stable approach, rounds each result to four decimal places, and converts the result back to a Python list before returning.\n\n3. **Numerical Stability**: By using different formulas for non-negative and negative inputs, the function avoids numerical underflow and overflow, ensuring stability even for very large values.\n\nThis approach ensures that the sigmoid function is both efficient and numerically stable, handling a wide range of input types and values effectively."}
{"task_id": 224, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function, which is commonly used in neural networks to avoid the \"dying\" neuron problem. The function should handle various input types, including scalars, lists (possibly nested), and numpy arrays, and return the activated values in the same structure as the input.\n\n### Approach\nThe Leaky ReLU activation function is defined as follows:\n- For an input \\( x \\geq 0 \\), the output is \\( x \\).\n- For an input \\( x < 0 \\), the output is \\( a \\times x \\), where \\( a \\) is a slope parameter in the range [0, 1).\n\nThe function should:\n1. Accept input `z` as a scalar, list, or numpy array.\n2. Accept an optional parameter `a` with a default value of 0.01.\n3. Return the activated values in the same structure as `z`, converted to a plain Python list if necessary.\n\nThe approach involves:\n1. Checking if the input is a numpy array and processing it using vectorized operations.\n2. If the input is a list, recursively processing each element.\n3. If the input is a scalar, directly applying the Leaky ReLU formula.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(element, a) for element in z]\n    else:\n        if z >= 0:\n            return z\n        else:\n            return a * z\n```\n\n### Explanation\n1. **Handling Numpy Arrays**: If the input `z` is a numpy array, the function uses `np.where` to apply the Leaky ReLU activation element-wise. The result is then converted to a Python list using `tolist()`, which preserves the structure for multi-dimensional arrays.\n\n2. **Handling Lists**: If the input `z` is a list, the function recursively processes each element. This allows it to handle nested lists of arbitrary depth.\n\n3. **Handling Scalars**: For scalar values, the function directly applies the Leaky ReLU formula. If the value is non-negative, it is returned as is; otherwise, it is multiplied by the slope parameter `a`.\n\nThis approach ensures that the function correctly processes all valid input types and returns the activated values in the same structure as the input, converting numpy arrays to plain Python lists where necessary."}
{"task_id": 243, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a small fully-connected Actor-Critic network. The network consists of a shared feature extractor with two separate output heads for the actor and critic. The weights and biases are fixed, which simplifies the computations.\n\n### Approach\n1. **Network Architecture**: The network has three hidden layers (Dense-1, Dense-2, Dense-3) each with 4 neurons and ReLU activation. The actor head outputs 3 neurons with a softmax activation, and the critic head outputs a single neuron with no activation.\n2. **Weights and Biases**: All weights are set to 1.0 and all biases are set to 0.0. This makes the network deterministic and simplifies the computations.\n3. **Forward Pass Computations**:\n   - Compute the sum of the input state vector.\n   - Apply ReLU activation to this sum to get the output of the first hidden layer.\n   - Propagate through the subsequent hidden layers, each time applying ReLU activation.\n   - Compute the logits for the actor head and apply softmax to get action probabilities.\n   - Compute the state value using the critic head.\n4. **Output**: Return the action probabilities and state value, each rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    sum_state = sum(state)\n    a = max(0, sum_state)\n    action_probs = [1/3, 1/3, 1/3]\n    state_value = 64 * a\n    # Round each action probability and the state value to four decimal places\n    action_probs = [round(prob, 4) for prob in action_probs]\n    state_value = round(state_value, 4)\n    return action_probs, state_value\n```\n\n### Explanation\n1. **Sum of State**: The sum of the input state vector is computed. This sum is used to determine the output of the first hidden layer after applying ReLU activation.\n2. **ReLU Activation**: The ReLU activation ensures that any negative sum is converted to zero, which affects the subsequent computations.\n3. **Action Probabilities**: Since all weights and biases are fixed, the logits for the actor head are identical, leading to equal action probabilities after applying the softmax function. Each action probability is 1/3.\n4. **State Value**: The state value is computed by propagating through the critic head, resulting in a scalar value that is 64 times the ReLU-activated sum of the state.\n\nThis approach efficiently computes the forward pass using basic NumPy operations, ensuring deterministic behavior and precise results."}
{"task_id": 253, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute both the Elastic-Net penalty value and its gradient for a given weight vector. The Elastic-Net penalty is a combination of L1 and L2 regularizations, which helps in reducing model complexity and preventing over-fitting.\n\n### Approach\n1. **Understand the Elastic-Net Penalty**: The penalty is defined as a convex combination of L1 and L2 regularizations. The formula for the penalty is:\n   \\[\n   R(w) = \\alpha \\cdot [\\lambda \\cdot ||w||_1 + (1 - \\lambda) \\cdot 0.5 \\cdot w^T w]\n   \\]\n   where \\(||w||_1\\) is the L1 norm (sum of absolute values) and \\(w^T w\\) is the squared L2 norm.\n\n2. **Gradient Calculation**: The gradient of the Elastic-Net penalty with respect to the weight vector \\(w\\) is given by:\n   \\[\n   \\nabla R(w) = \\alpha \\cdot [\\lambda \\cdot \\text{sign}(w) + (1 - \\lambda) \\cdot w]\n   \\]\n   where \\(\\text{sign}(w)\\) is the element-wise sign function.\n\n3. **Implementation Steps**:\n   - Convert the input weight vector to a NumPy array for easier manipulation.\n   - Compute the L1 norm and the L2 term.\n   - Calculate the penalty value using the given formula.\n   - If the gradient flag is set, compute the gradient using the provided formula and return it as a list of rounded values. Otherwise, return the penalty value rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w = np.asarray(w)\n    L1 = np.sum(np.abs(w))\n    L2 = 0.5 * np.sum(w ** 2)\n    penalty = alpha * (l1_ratio * L1 + (1 - l1_ratio) * L2)\n    \n    if not gradient:\n        return round(penalty, 4)\n    else:\n        grad = alpha * (l1_ratio * np.sign(w) + (1 - l1_ratio) * w)\n        grad = np.round(grad, 4)\n        return grad.tolist()\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input weight vector is converted to a NumPy array to facilitate efficient element-wise operations.\n- **L1 and L2 Calculations**: The L1 norm is computed as the sum of absolute values, and the L2 term is computed as half the sum of squared values.\n- **Penalty Calculation**: The penalty is computed using the given formula, combining the L1 and L2 terms with the provided regularization strength and ratio.\n- **Gradient Calculation**: If required, the gradient is computed using the element-wise sign function and the weight vector itself, then rounded and converted to a list for the result.\n\nThis approach ensures that both the penalty value and its gradient are computed efficiently and accurately, handling various edge cases such as zero weights and different regularization strengths."}
{"task_id": 256, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a numerical gradient checker for a vanilla Recurrent Neural Network (RNN) using the centered finite-difference method. This method helps in verifying the correctness of the gradients computed during back-propagation by approximating them numerically.\n\n### Approach\n1. **Parameter Handling**: Check if the parameter name is \"X\" or \"y\", which are not trainable, and return `None` immediately. For parameters \"Ba\" or \"Bx\", convert them to their lowercase variants \"ba\" or \"bx\" respectively.\n2. **Deep Copying the Model**: For each element in the parameter array, create deep copies of the model to ensure that each perturbation run is independent and starts from the same initial state.\n3. **Perturbation and Forward Pass**: For each element, perturb the parameter by `+\u03b5` and `-\u03b5`, run the forward pass for all time steps, collect predictions, and compute the loss for both perturbations.\n4. **Gradient Calculation**: Use the centered finite-difference formula to approximate the gradient for each parameter element.\n5. **Result Construction**: Store the computed gradients in an array and return its transpose as the final result.\n\n### Solution Code\n```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\"\"\"\n    if param_name in ['X', 'y']:\n        return None\n    \n    # Handle parameter name case for 'Ba' and 'Bx'\n    if param_name in ['Ba', 'Bx']:\n        param_key = param_name.lower()\n    else:\n        param_key = param_name\n    \n    # Retrieve the parameter array\n    theta = model.parameters[param_key]\n    # Initialize gradients array with the same shape and dtype\n    grads = np.zeros_like(theta, dtype=np.float64)\n    \n    # Iterate over each element in the parameter array\n    for idx in np.ndenumerate(theta):\n        # Save the original parameter value\n        original_value = theta[idx]\n        \n        # Create a deep copy of the model for the +epsilon perturbation\n        model_plus = deepcopy(model)\n        model_plus.flush_gradients()\n        plus_params = model_plus.parameters[param_key]\n        plus_params[idx] += epsilon\n        \n        # Run forward pass for n_t steps and collect predictions\n        predictions_plus = []\n        for t in range(n_t):\n            x_t = X[:, :, t]\n            pred = model_plus.forward(x_t)\n            predictions_plus.append(pred)\n        loss_plus = loss_func(predictions_plus)\n        \n        # Create a deep copy of the model for the -epsilon perturbation\n        model_minus = deepcopy(model)\n        model_minus.flush_gradients()\n        minus_params = model_minus.parameters[param_key]\n        minus_params[idx] -= epsilon\n        \n        # Run forward pass for n_t steps and collect predictions\n        predictions_minus = []\n        for t in range(n_t):\n            x_t = X[:, :, t]\n            pred = model_minus.forward(x_t)\n            predictions_minus.append(pred)\n        loss_minus = loss_func(predictions_minus)\n        \n        # Compute the numerical gradient using centered finite difference\n        grad = (loss_plus - loss_minus) / (2 * epsilon)\n        grads[idx] = grad\n    \n    # Return the transposed gradients\n    return grads.T\n```\n\n### Explanation\n1. **Parameter Handling**: The function first checks if the parameter is non-trainable (\"X\" or \"y\") and returns `None`. It then maps \"Ba\" and \"Bx\" to their lowercase variants.\n2. **Deep Copying**: For each parameter element, deep copies of the model are created to ensure each perturbation run starts fresh.\n3. **Perturbation and Forward Pass**: Each perturbed model runs the forward pass for all time steps, collecting predictions to compute the loss.\n4. **Gradient Calculation**: The centered finite-difference formula approximates the gradient for each parameter element.\n5. **Result Construction**: The gradients are stored in an array and returned as a transpose to match the expected output format.\n\nThis approach ensures that each perturbation is independent and accurately computes the numerical gradients for debugging purposes."}
{"task_id": 261, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Glorot (Xavier) normal weight initialization function. This initialization is crucial for training neural networks as it helps in stabilizing the training process by normalizing the weights appropriately.\n\n### Approach\nThe approach to implement the Glorot normal initialization involves the following steps:\n\n1. **Compute Fan-in and Fan-out**: \n   - For a 2-D shape, the fan-in is the second dimension and the fan-out is the first dimension.\n   - For higher dimensions (e.g., convolutional layers), the first two dimensions are the output and input channels, respectively. The remaining dimensions are multiplied to get the receptive field size. The fan-in is the product of the input channels and the receptive field size, while the fan-out is the product of the output channels and the receptive field size.\n\n2. **Calculate Standard Deviation**: \n   - The standard deviation is computed as the square root of 2 divided by the sum of the fan-in and fan-out.\n\n3. **Generate Weight Matrix**: \n   - Using the computed standard deviation, generate a NumPy array of the specified shape where each element is drawn from a normal distribution with mean 0 and the computed standard deviation. This is done without modifying the global NumPy random state.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    # Compute fan_in and fan_out\n    if len(shape) == 2:\n        fan_in = shape[1]\n        fan_out = shape[0]\n    else:\n        in_channels = shape[1]\n        out_channels = shape[0]\n        receptive_field = 1\n        for d in shape[2:]:\n            receptive_field *= d\n        fan_in = in_channels * receptive_field\n        fan_out = out_channels * receptive_field\n\n    # Calculate standard deviation\n    s = np.sqrt(2 / (fan_in + fan_out))\n\n    # Generate the array using a new RNG to avoid affecting the global state\n    rng = np.random.default_rng()\n    return rng.normal(0, s, shape)\n```\n\n### Explanation\n- **Fan-in and Fan-out Calculation**: The function first determines the fan-in and fan-out based on the shape of the tensor. For 2-D shapes, it directly uses the dimensions. For higher dimensions, it calculates the receptive field size by multiplying the trailing dimensions and then computes fan-in and fan-out using the input and output channels.\n- **Standard Deviation**: The standard deviation is calculated using the formula derived from the Glorot initialization, ensuring that the weights are scaled appropriately to maintain consistent variance across layers.\n- **Weight Matrix Generation**: The function generates the weight matrix using a normal distribution with the computed standard deviation, ensuring that the global random state is not affected by using a new random number generator for each call.\n\nThis approach ensures that the weights are initialized in a way that helps in stabilizing the training process, making it easier to converge to an optimal solution."}
{"task_id": 266, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert an undirected, un-weighted graph given by its vertex list `V` and edge list `E` into an adjacency list representation. The adjacency list should maintain the order of vertices as specified in `V` and ensure that each vertex's neighbors are sorted according to their appearance in `V`.\n\n### Approach\n1. **Index Mapping**: Create a dictionary to map each vertex to its index in the list `V`. This helps in quickly finding the position of any vertex in `V`.\n2. **Adjacency Sets**: Initialize a list of sets where each set corresponds to a vertex and stores its neighbors. Using sets ensures that there are no duplicate neighbors.\n3. **Process Edges**: For each edge in `E`, add each vertex to the other's adjacency set. This step ensures that both directions of the undirected edge are considered.\n4. **Sort Neighbors**: Convert each set of neighbors into a sorted list. The sorting is based on the order of vertices in `V` to maintain determinism and ease of testing.\n\n### Solution Code\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples `(u, v)` describing undirected edges that connect\n        vertices `u` and `v`.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list `G` where `G[i]` contains all vertices adjacent to `V[i]`.\n        \u2022 The outer list has the same length and order as `V`.\n        \u2022 Inner lists contain no duplicates and are ordered by their\n          appearance order in `V`.\n    \"\"\"\n    # Create a mapping from each vertex to its index in V\n    index_map = {v: i for i, v in enumerate(V)}\n    \n    # Initialize adjacency sets for each vertex\n    adj_sets = [set() for _ in range(len(V))]\n    \n    # Populate adjacency sets based on edges\n    for u, v in E:\n        adj_sets[index_map[u]].add(v)\n        adj_sets[index_map[v]].add(u)\n    \n    # Build the adjacency list with sorted neighbors\n    G = []\n    for i in range(len(V)):\n        # Convert the set to a sorted list based on V's order\n        neighbors = sorted(adj_sets[i], key=lambda x: index_map[x])\n        G.append(neighbors)\n    \n    return G\n```\n\n### Explanation\n1. **Index Mapping**: The dictionary `index_map` is created to quickly find the index of each vertex in `V`, which is essential for maintaining the order.\n2. **Adjacency Sets**: Each vertex's neighbors are stored in a set to avoid duplicates. This ensures that each neighbor appears exactly once.\n3. **Edge Processing**: For each edge `(u, v)`, both `u` is added to `v`'s adjacency set and `v` is added to `u`'s adjacency set, reflecting the undirected nature of the graph.\n4. **Sorting Neighbors**: The neighbors of each vertex are converted from a set to a list and sorted based on their indices in `V`. This ensures the neighbors are in the correct order as specified by `V`.\n\nThis approach efficiently constructs the adjacency list while ensuring the required constraints are met, providing a clear and deterministic solution."}
{"task_id": 286, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a utility function that retrieves a weight-initialization function based on its name. This function will look up the name in the global namespace, check if it is callable, and return it if valid. If the name does not correspond to a valid initializer, it will raise an appropriate error.\n\n### Approach\n1. **Lookup in Global Namespace**: The function will use Python's `globals()` to access the global namespace and retrieve the object corresponding to the given name.\n2. **Check Validity**: It will check if the retrieved object is callable. If the object is not found or is not callable, it will raise a `ValueError`.\n3. **Return Callable**: If the object is valid and callable, it will be returned as the initializer function.\n\n### Solution Code\n```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    return [[0.0 for _ in range(shape[1])] for _ in range(shape[0])]\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    return [[1.0 for _ in range(shape[1])] for _ in range(shape[0])]\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    random.seed(seed)\n    return [[random.uniform(low, high) for _ in range(shape[1])] for _ in range(shape[0])]\n\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    initializer = globals().get(name)\n    if initializer is None or not callable(initializer):\n        raise ValueError(\"Invalid initialization function.\")\n    return initializer\n```\n\n### Explanation\n- **zeros_init**: This function creates a matrix of zeros with the specified shape. It uses list comprehensions to generate the matrix.\n- **ones_init**: This function creates a matrix of ones with the specified shape, similar to zeros_init.\n- **random_uniform_init**: This function generates a matrix with random numbers uniformly distributed between a specified low and high value. It uses the random module and sets a seed for reproducibility.\n- **get_initializer**: This function retrieves the initializer by name from the global namespace. It checks if the retrieved object is callable and raises an error if it is not valid. This ensures that only valid initializers are returned.\n\nThis approach efficiently maps initializer names to their corresponding functions, ensuring that the correct function is retrieved and used for weight initialization in deep learning models."}
{"task_id": 290, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if two binary decision trees are equivalent. The trees are composed of internal nodes (Node) and terminal nodes (Leaf). Two trees are considered equivalent if they have the same structure, their internal nodes use the same feature indices and thresholds, and their leaf values are numerically equal within a specified tolerance.\n\n### Approach\n1. **Base Case for Leaf Nodes**: If both nodes are Leaf nodes, compare their values using `numpy.allclose` with a tolerance of \\(1 \\times 10^{-8}\\).\n2. **Base Case for Node Nodes**: If both nodes are Node nodes, check if their feature indices and thresholds are equal. The feature index must be exactly the same, while the threshold must be numerically equal within the specified tolerance.\n3. **Recursive Check for Children**: For each Node, recursively check if their left and right children are equivalent.\n4. **Type Mismatch**: If one node is a Leaf and the other is a Node, they are not equivalent.\n\n### Solution Code\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-8)\n    elif isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8):\n            return False\n        return compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    else:\n        return False\n```\n\n### Explanation\n- **Leaf Nodes Comparison**: The function uses `numpy.allclose` to compare the values of Leaf nodes, ensuring numerical equality within a tolerance of \\(1 \\times 10^{-8}\\).\n- **Node Nodes Comparison**: The function checks if the feature indices are the same and if the thresholds are numerically close. It then recursively checks the left and right children.\n- **Recursive Structure Check**: By recursively comparing each node's children, the function ensures that the structure of both trees is identical.\n\nThis approach efficiently verifies the equivalence of two decision trees by leveraging recursion and numerical comparison functions, ensuring both structural and value-based equivalence."}
{"task_id": 292, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform a single-point column crossover on two parent neural network weight matrices to generate two offspring matrices. This crossover operation involves swapping columns from a specified cutoff point onwards between the two parents.\n\n### Approach\n1. **Validation Check**: First, we need to ensure that both parent matrices have the same shape. If they do not, the function returns -1.\n2. **Conversion to NumPy Arrays**: Convert the input lists into NumPy arrays for efficient slicing and concatenation operations.\n3. **Column Crossover Operation**: \n   - For the first child, columns before the cutoff are taken from the first parent, and columns from the cutoff onwards are taken from the second parent.\n   - For the second child, columns before the cutoff are taken from the second parent, and columns from the cutoff onwards are taken from the first parent.\n4. **Conversion Back to Lists**: Convert the resulting NumPy arrays back into ordinary Python lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]] | int:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    # Convert the parent lists to NumPy arrays\n    p1 = np.array(parent1)\n    p2 = np.array(parent2)\n    \n    # Check if the shapes are the same\n    if p1.shape != p2.shape:\n        return -1\n    \n    # Get the number of columns\n    num_cols = p1.shape[1]\n    \n    # Proceed to create the children\n    # For child1: before cutoff from p1, after from p2\n    child1 = np.concatenate((p1[:, :cutoff], p2[:, cutoff:]), axis=1)\n    # For child2: before cutoff from p2, after from p1\n    child2 = np.concatenate((p2[:, :cutoff], p1[:, cutoff:]), axis=1)\n    \n    # Convert back to Python lists\n    child1_list = child1.tolist()\n    child2_list = child2.tolist()\n    \n    return (child1_list, child2_list)\n```\n\n### Explanation\n1. **Validation Check**: The function first converts the input lists into NumPy arrays and checks if their shapes are identical. If not, it returns -1.\n2. **Column Crossover**: Using NumPy's array slicing and concatenation, the function constructs two new matrices. Each child matrix is formed by taking columns before the cutoff from one parent and columns from the cutoff onwards from the other parent.\n3. **Conversion to Lists**: The resulting NumPy arrays are converted back to Python lists using the `tolist()` method to meet the output requirements.\n\nThis approach ensures efficient and correct implementation of the single-point column crossover operation, handling edge cases such as cutoff values at the boundaries (0 or equal to the number of columns)."}
{"task_id": 296, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if all elements in a given NumPy array are binary, meaning each element is either 0 or 1. The function should handle arrays of integer, float, or boolean types and return `True` if all elements are binary, otherwise `False`.\n\n### Approach\n1. **Convert Input to NumPy Array**: The function first converts the input into a NumPy array to handle any array-like input uniformly.\n2. **Create a Boolean Mask**: Using NumPy's vectorized operations, we create a boolean mask where each element is `True` if it is 0 or 1, and `False` otherwise.\n3. **Check All Elements**: The function then checks if all elements in the boolean mask are `True` using the `all()` method, which returns `True` if every element is `True` and `False` otherwise.\n\nThis approach efficiently handles different data types and ensures that the function works correctly for any valid input.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    x = np.asarray(x)\n    mask = (x == 0) | (x == 1)\n    return mask.all()\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input is converted to a NumPy array to ensure uniform handling of different input types.\n- **Boolean Mask Creation**: The mask `(x == 0) | (x == 1)` is created to identify elements that are either 0 or 1. This mask is a boolean array where each element is `True` if the corresponding element in the input array is 0 or 1, and `False` otherwise.\n- **Check All Elements**: The `all()` method is used on the boolean mask to check if all elements are `True`. If they are, the function returns `True`; otherwise, it returns `False`.\n\nThis solution efficiently handles various data types and ensures that the function works correctly for any valid input, providing a robust and reliable way to validate binary arrays."}
{"task_id": 303, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that formats a debugging string to compare the current output of a program with the expected gold-standard output. The function should produce a well-formatted, multi-line string with specific sections for the previous and current outputs, along with any additional warning messages.\n\n### Approach\n1. **Identify Previous and Current Elements**: Determine the previous and current elements from the list `params` based on the given index `ix`. The previous element is the one at `max(ix-1, 0)` to handle cases where `ix` is 0.\n2. **Extract Values**: Extract the mine values and labels for both the previous and current elements from `params`. Also, retrieve the corresponding gold-standard values from the dictionary `golds`.\n3. **Construct Lines**: Build each line of the debugging string according to the specified format. This includes sections for the previous and current mine values, their respective gold-standard values, and any additional warning messages.\n4. **Handle Warning String**: Append the warning string exactly as provided, which may include newlines or other formatting.\n5. **Assemble the String**: Combine all the constructed lines into a single string with appropriate line breaks and separators.\n\n### Solution Code\n```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\"\"\"\n    prev_ix = max(ix - 1, 0)\n    prev_tuple = params[prev_ix]\n    curr_tuple = params[ix]\n    \n    prev_mine, prev_label = prev_tuple\n    curr_mine, curr_label = curr_tuple\n    \n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    \n    top_line = '-' * 25 + ' DEBUG ' + '-' * 25\n    end_line = '-' * 23 + ' END DEBUG ' + '-' * 23\n    \n    lines = [\n        top_line,\n        f\"Mine (prev) [{prev_label}]:\",\n        prev_mine,\n        f\"Theirs (prev) [{prev_label}]:\",\n        prev_gold,\n        f\"Mine [{curr_label}]:\",\n        curr_mine,\n        f\"Theirs [{curr_label}]:\",\n        curr_gold + warn_str,\n        end_line\n    ]\n    \n    return '\\n'.join(lines)\n```\n\n### Explanation\n- **Top and End Lines**: These lines are constructed with specific numbers of dashes and the words \"DEBUG\" and \"END DEBUG\" to form the header and footer of the debugging string.\n- **Previous and Current Sections**: Each section includes the mine value and the corresponding gold-standard value, formatted with their respective labels.\n- **Warning String Handling**: The warning string is appended directly to the current gold-standard value line, ensuring it appears exactly as provided.\n- **String Construction**: The lines are joined with newline characters to form the final multi-line string, ensuring the correct format and structure.\n\nThis approach ensures that the debugging string is both informative and correctly formatted, making it easier to compare outputs and diagnose issues."}
{"task_id": 312, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a Blackman\u2013Harris window, which is a type of window function used in signal processing to reduce spectral leakage. The window can be generated in two variants: symmetric and periodic. The symmetric form is typically used in FIR filter design, while the periodic form is used for FFT-based spectral analysis.\n\n### Approach\n1. **Special Cases Handling**: \n   - If the window length is less than or equal to zero, return an empty list.\n   - If the window length is exactly 1, return a list containing a single element, 1.0.\n\n2. **Window Generation**:\n   - For the symmetric variant, generate the window of the specified length using the formula provided.\n   - For the periodic variant, generate a window of length \\( L + 1 \\) (where \\( L \\) is the desired window length) and then discard the last element to ensure the window tiles seamlessly for FFT analysis.\n\n3. **Formula Application**:\n   - Use the given coefficients \\( a_0, a_1, a_2, a_3 \\) to compute each point of the window using the cosine terms as specified.\n\n4. **Rounding**:\n   - Each computed value is rounded to four decimal places before being added to the result list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n    \n    if symmetric:\n        n_count = window_len\n        N = n_count - 1\n    else:\n        n_count = window_len + 1\n        N = n_count - 1\n    \n    window = []\n    for n in range(n_count):\n        term1 = a1 * np.cos(2 * np.pi * n / N)\n        term2 = a2 * np.cos(4 * np.pi * n / N)\n        term3 = a3 * np.cos(6 * np.pi * n / N)\n        bh = a0 - term1 + term2 - term3\n        window.append(bh)\n    \n    if not symmetric:\n        window = window[:window_len]\n    \n    # Round each value to four decimal places\n    window = [round(value, 4) for value in window]\n    \n    return window\n```\n\n### Explanation\n- **Special Cases Handling**: The function immediately returns an empty list for non-positive window lengths and a list containing 1.0 for a window length of 1.\n- **Window Generation**: Depending on whether the symmetric or periodic variant is requested, the function computes the window values using the specified formula. For the periodic variant, an extra point is generated and then discarded to ensure the window can be seamlessly tiled.\n- **Formula Application**: Each point of the window is computed using the given coefficients and cosine terms, ensuring the correct application of the Blackman\u2013Harris formula.\n- **Rounding**: Each computed value is rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the generated window function is accurate and suitable for both FIR filter design and FFT-based spectral analysis, depending on the specified variant."}
{"task_id": 329, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the center frequencies of each DFT (Discrete Fourier Transform) bin. The function should handle two modes: returning only non-negative frequencies and returning all frequencies as per NumPy's `fftfreq` function. \n\n### Approach\n1. **Check Validity**: First, we check if the number of DFT coefficients (N) and the sampling frequency (fs) are positive. If either is non-positive, we return an empty list.\n2. **Compute Frequencies**: Using NumPy's `fftfreq` function, we compute the frequencies for the given number of points (N) with the sample spacing (d) set to 1/fs. This ensures the frequencies are in Hz.\n3. **Handle Positive-Only Mode**: If the function is in positive-only mode, we take only the non-negative frequencies, which are the first (N//2 + 1) elements of the computed frequencies.\n4. **Round and Convert**: Each frequency value is rounded to 4 decimal places and converted to a Python list using NumPy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz. Default is 44 kHz.\n    positive_only : bool, optional\n        When True, return only the non-negative frequency bins; when False, return all N bins. Default is True.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals. An empty list is returned when N or fs is non-positive.\n    \"\"\"\n    if N <= 0 or fs <= 0:\n        return []\n    freqs = np.fft.fftfreq(N, d=1 / fs)\n    if positive_only:\n        freqs = freqs[: (N // 2 + 1)]\n    rounded = np.round(freqs, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n- **Validity Check**: The function starts by checking if N or fs is non-positive, returning an empty list if true.\n- **Frequency Calculation**: Using `np.fft.fftfreq`, we compute the frequencies, ensuring they are correctly scaled by the sampling frequency.\n- **Positive-Only Mode**: For non-negative frequencies, we slice the computed frequencies to include only the first half plus one element.\n- **Rounding and Conversion**: Each frequency is rounded to four decimal places and converted to a Python list for the final output.\n\nThis approach efficiently handles both modes and ensures the results are accurate and formatted as required."}
{"task_id": 331, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the out-of-bag mean squared error (OOB-MSE) for a Random Forest regressor. The OOB-MSE is an unbiased performance estimate obtained by using predictions from trees where the sample was not part of the training data (out-of-bag).\n\n### Approach\n1. **Convert Inputs to Arrays**: Convert the input lists to numpy arrays for easier manipulation.\n2. **Iterate Over Each Sample**: For each sample, determine which trees were out-of-bag using the provided mask.\n3. **Compute Average Prediction**: For each sample, if there are any out-of-bag trees, compute the average prediction from those trees.\n4. **Calculate Squared Error**: For each sample with out-of-bag predictions, compute the squared error between the average prediction and the true value.\n5. **Compute Mean Squared Error**: Average the squared errors across all samples that have at least one out-of-bag prediction. If no samples have out-of-bag predictions, return -1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    predictions = np.asarray(predictions)\n    oob_mask = np.asarray(oob_mask).astype(bool)\n    \n    n_samples = len(y_true)\n    squared_errors = []\n    \n    for i in range(n_samples):\n        mask = oob_mask[i]\n        if np.any(mask):\n            preds = predictions[i, mask]\n            avg_pred = np.mean(preds)\n            se = (avg_pred - y_true[i]) ** 2\n            squared_errors.append(se)\n    \n    if not squared_errors:\n        return -1\n    else:\n        mse = np.mean(squared_errors)\n        return round(mse, 4)\n```\n\n### Explanation\n1. **Input Conversion**: The function starts by converting the input lists into numpy arrays for easier manipulation. The `oob_mask` is converted to a boolean array to correctly identify out-of-bag samples.\n2. **Sample Iteration**: For each sample, the function checks if there are any out-of-bag trees using the boolean mask.\n3. **Average Prediction Calculation**: If a sample has out-of-bag predictions, the function computes the average of these predictions.\n4. **Squared Error Calculation**: The squared error between the average prediction and the true value is computed for each sample with out-of-bag predictions.\n5. **Mean Squared Error Calculation**: The mean of all squared errors is computed. If no samples have out-of-bag predictions, the function returns -1. Otherwise, it returns the mean squared error rounded to four decimal places.\n\nThis approach ensures that we efficiently compute the OOB-MSE by leveraging numpy operations for array manipulation and averaging, providing an accurate and unbiased performance estimate for the Random Forest regressor."}
{"task_id": 332, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to construct a Frequent-Pattern Tree (FP-Tree) from a set of transactions, given a minimum support threshold. The FP-Tree is represented as a nested dictionary where each node contains the support count and its children nodes.\n\n### Approach\nThe approach to construct the FP-Tree involves the following steps:\n\n1. **Compute Item Frequencies**: Calculate the frequency of each item across all transactions. This helps in identifying which items meet the minimum support threshold.\n2. **Filter Frequent Items**: Remove items whose frequency is below the given minimum support threshold.\n3. **Determine Global Ordering**: Sort the remaining frequent items first by their frequency in descending order and then alphabetically to break ties.\n4. **Process Transactions**: For each transaction, remove duplicates, filter out non-frequent items, and reorder the items according to the global ordering.\n5. **Construct the FP-Tree**: Traverse each processed transaction to build the tree. For each item in the transaction, update the tree by creating new nodes if necessary and incrementing the support count of each node along the path.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of transactions an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node contains two keys:\n        \"support\" and \"children\" (the latter mapping item \u2192 child-node).\n    \"\"\"\n    # Step 1: Compute the frequency of each item\n    freq = Counter()\n    for transaction in transactions:\n        unique_items = set(transaction)\n        for item in unique_items:\n            freq[item] += 1\n\n    # Step 2: Filter items based on min_support\n    frequent_items = {item: count for item, count in freq.items() if count >= min_support}\n    if not frequent_items:\n        return {'support': 0, 'children': {}}\n\n    # Step 3: Create the global ordering of items\n    global_order = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n    order_map = {item: idx for idx, item in enumerate(global_order)}\n\n    # Step 4: Build the FP-Tree\n    root = {'support': 0, 'children': {}}\n    for transaction in transactions:\n        # Process each transaction to get the ordered list of items\n        unique_filtered = [item for item in set(transaction) if item in frequent_items]\n        if not unique_filtered:\n            continue  # Skip if no frequent items left after filtering\n        ordered_transaction = sorted(unique_filtered, key=lambda x: order_map[x])\n\n        # Traverse the tree and update nodes\n        current_node = root\n        for item in ordered_transaction:\n            if item in current_node['children']:\n                current_node = current_node['children'][item]\n                current_node['support'] += 1\n            else:\n                new_node = {'support': 1, 'children': {}}\n                current_node['children'][item] = new_node\n                current_node = new_node\n\n    return root\n```\n\n### Explanation\n1. **Compute Item Frequencies**: We use a `Counter` to count how many times each item appears in the transactions. Each transaction is processed to count unique items, ensuring each item is counted once per transaction.\n2. **Filter Frequent Items**: Items that do not meet the minimum support threshold are discarded. This step ensures that only relevant items are considered for tree construction.\n3. **Global Ordering**: Items are sorted by their frequency in descending order. Alphabetical sorting is used to break ties, ensuring a consistent order for items with the same frequency.\n4. **Process Transactions**: Each transaction is processed to remove duplicates and filter out non-frequent items. The remaining items are reordered according to the global ordering.\n5. **Construct the FP-Tree**: Starting from the root, each item in the processed transaction is used to traverse or create nodes in the tree. The support count of each node along the path is incremented by 1 for each transaction.\n\nThis approach efficiently constructs the FP-Tree by focusing on frequent items and their ordering, ensuring that the tree accurately represents the frequent patterns in the transactions."}
{"task_id": 336, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the kernel matrix using the linear kernel function. The linear kernel function measures the similarity between two input vectors and is defined as \\( k(x, y) = x \\cdot y + c_0 \\), where \\( x \\cdot y \\) is the dot product of vectors \\( x \\) and \\( y \\), and \\( c_0 \\) is an optional constant.\n\n### Approach\n1. **Input Handling**: The function receives two collections of input vectors \\( X \\) and \\( Y \\). If \\( Y \\) is not provided, it defaults to \\( X \\). Both \\( X \\) and \\( Y \\) can be either Python lists or NumPy arrays.\n2. **Validation Checks**: \n   - If \\( X \\) is empty, return an empty list.\n   - Ensure that the number of columns in \\( X \\) and \\( Y \\) are the same. If not, return -1.\n3. **Kernel Matrix Computation**: \n   - Convert \\( X \\) and \\( Y \\) to NumPy arrays for efficient matrix operations.\n   - Compute the kernel matrix using matrix multiplication to get the dot products, then add the constant \\( c_0 \\) to each element.\n4. **Rounding and Conversion**: Round each element of the kernel matrix to 4 decimal places and convert the result back to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_kernel(X: list[list[int | float]] | np.ndarray, Y: list[list[int | float]] | np.ndarray | None = None, c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\"\"\"\n    # Convert X and Y to NumPy arrays\n    X_np = np.asarray(X)\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.asarray(Y)\n    \n    # Check if X is empty\n    if X_np.shape[0] == 0:\n        return []\n    \n    # Check if the number of columns match\n    if X_np.shape[1] != Y_np.shape[1]:\n        return -1\n    \n    # Compute the kernel matrix\n    K = X_np @ Y_np.T  # Matrix multiplication for dot products\n    K += c0\n    \n    # Round to 4 decimal places\n    K_rounded = np.round(K, 4)\n    \n    # Convert to list of lists\n    K_list = K_rounded.tolist()\n    \n    return K_list\n```\n\n### Explanation\n1. **Input Conversion**: The function starts by converting the input lists or arrays into NumPy arrays for efficient computation.\n2. **Empty Check**: If \\( X \\) is empty, the function immediately returns an empty list.\n3. **Dimension Check**: The function checks if the number of columns in \\( X \\) and \\( Y \\) are the same. If not, it returns -1.\n4. **Matrix Multiplication**: The kernel matrix is computed using matrix multiplication, which efficiently calculates the dot product of each pair of vectors from \\( X \\) and \\( Y \\).\n5. **Adjustment and Rounding**: The constant \\( c_0 \\) is added to each element of the kernel matrix, and each element is rounded to 4 decimal places.\n6. **Result Conversion**: The resulting NumPy array is converted back to a list of lists for the final output.\n\nThis approach ensures that the function handles various input types efficiently and computes the kernel matrix correctly while adhering to the problem constraints."}
{"task_id": 340, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single optimization step of the Adam (Adaptive Moment Estimation) algorithm. The Adam optimizer is a popular stochastic gradient descent algorithm that combines the advantages of two other extensions of stochastic gradient descent: AdaGrad and RMSProp. The algorithm maintains two moving averages of the gradient, allowing it to adapt the learning rate for each parameter individually.\n\n### Approach\nThe approach to implement the Adam optimizer's single step involves the following key steps:\n\n1. **Compute the First Moment Estimate (m):** This is done using the formula \\( m_t = \\beta_1 \\cdot m + (1 - \\beta_1) \\cdot g \\), where \\( m \\) is the first moment estimate from the previous step, \\( g \\) is the current gradient, and \\( \\beta_1 \\) is the exponential decay rate for the first moment.\n\n2. **Compute the Second Moment Estimate (v):** This is done using the formula \\( v_t = \\beta_2 \\cdot v + (1 - \\beta_2) \\cdot g^2 \\), where \\( v \\) is the second moment estimate from the previous step, \\( g \\) is the current gradient, and \\( \\beta_2 \\) is the exponential decay rate for the second moment.\n\n3. **Bias Correction for First Moment (m_hat):** The first moment estimate is bias-corrected using \\( m_hat = \\frac{m_t}{1 - \\beta_1^t} \\), where \\( t \\) is the current time step.\n\n4. **Bias Correction for Second Moment (v_hat):** The second moment estimate is bias-corrected using \\( v_hat = \\frac{v_t}{1 - \\beta_2^t} \\).\n\n5. **Update the Parameter (\u03b8):** The parameter is updated using the formula \\( \u03b8' = \u03b8 - \\alpha \\cdot \\frac{m_hat}{\\sqrt{v_hat} + \\epsilon} \\), where \\( \\alpha \\) is the learning rate and \\( \\epsilon \\) is a small constant to avoid division by zero.\n\n6. **Handle Zero Gradient Case:** If the gradient is exactly zero, the parameter remains unchanged, but the moment estimates are still updated.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_parameter, new_moment, new_variance) **as\n        python lists**, each rounded to 8 decimal places.\n    \"\"\"\n    # Compute the new first and second moment estimates\n    m_new = beta1 * m + (1 - beta1) * grad\n    v_new = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Check if the gradient is exactly zero\n    is_zero_grad = np.all(grad == 0)\n    \n    if not is_zero_grad:\n        # Compute bias-corrected first and second moments\n        denominator_beta1 = 1 - (beta1 ** t)\n        denominator_beta2 = 1 - (beta2 ** t)\n        m_hat = m_new / denominator_beta1\n        v_hat = v_new / denominator_beta2\n        \n        # Compute the update step\n        sqrt_v_hat = np.sqrt(v_hat)\n        denominator = sqrt_v_hat + eps\n        update = (lr * m_hat) / denominator\n        updated_param = param - update\n    else:\n        # If gradient is zero, parameter remains unchanged\n        updated_param = param.copy()\n    \n    # Round each array to 8 decimal places and convert to list\n    updated_param_list = [round(x, 8) for x in updated_param.tolist()]\n    m_new_list = [round(x, 8) for x in m_new.tolist()]\n    v_new_list = [round(x, 8) for x in v_new.tolist()]\n    \n    return (updated_param_list, m_new_list, v_new_list)\n```\n\n### Explanation\nThe provided code implements the Adam optimization step as described. It first computes the new moment estimates (m_new and v_new) using the given formulas. It then checks if the gradient is zero to decide whether to update the parameter. If the gradient is non-zero, it computes the bias-corrected moments and updates the parameter. Finally, it rounds the updated parameter and the new moment estimates to 8 decimal places and returns them as lists. This approach ensures that the parameter is optimized efficiently while handling edge cases such as zero gradients correctly."}
{"task_id": 343, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the core numerical step of the Cross-Entropy Method (CEM) used in Reinforcement Learning to update policy parameters. The goal is to retain the top fraction of parameter samples based on their rewards, compute the mean and variance of these retained samples, and return these values as lists rounded to four decimal places.\n\n### Approach\n1. **Determine the Number of Samples to Retain**: Calculate the number of top samples to retain using the given fraction (`retain_prcnt`). This is done by computing `K = int(retain_prcnt * N)`, where `N` is the total number of samples.\n2. **Sort Samples by Rewards**: Sort the parameter samples in descending order based on their corresponding rewards. This helps in selecting the top `K` samples.\n3. **Select Top Samples**: Extract the top `K` samples from the sorted list.\n4. **Compute Mean and Variance**: Calculate the mean and per-dimension variance of the retained samples. The mean is computed by averaging each dimension across the retained samples, and the variance is computed as the average of the squared deviations from the mean for each dimension.\n5. **Round Results**: Convert the computed mean and variance to lists and round each value to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    N = theta_samples.shape[0]\n    K = int(retain_prcnt * N)\n    \n    # Get indices sorted by descending rewards\n    sorted_indices = np.argsort(-rewards)\n    \n    # Select top K indices\n    top_indices = sorted_indices[:K]\n    \n    # Get the retained samples\n    retained = theta_samples[top_indices]\n    \n    # Compute mean and variance\n    mean = np.mean(retained, axis=0)\n    var = np.var(retained, axis=0)\n    \n    # Round to four decimal places\n    rounded_mean = [round(x, 4) for x in mean.tolist()]\n    rounded_var = [round(x, 4) for x in var.tolist()]\n    \n    return (rounded_mean, rounded_var)\n```\n\n### Explanation\n- **Sorting and Selecting Samples**: The samples are sorted based on their rewards in descending order to ensure we retain the top-performing samples. This is done using `np.argsort(-rewards)`, which gives the indices of the sorted rewards.\n- **Mean and Variance Calculation**: The mean is calculated by taking the average of each dimension across the retained samples. The variance is computed as the average of the squared differences from the mean for each dimension, ensuring we use the population variance (divided by `K`).\n- **Rounding**: The results are rounded to four decimal places to meet the problem's requirements, ensuring precision and readability.\n\nThis approach efficiently narrows down the parameter samples to the top performers, computes the necessary statistics, and returns them in the required format."}
{"task_id": 353, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Shannon entropy of a given sequence of class labels. Shannon entropy is a measure of the impurity or disorder in a dataset and is often used in decision-tree learning algorithms to determine the best splits.\n\n### Approach\n1. **Problem Analysis**: The Shannon entropy of a discrete distribution is calculated using the formula \\( H = -\\sum_i p_i \\log_2 p_i \\), where \\( p_i \\) is the probability of each class label. The entropy is zero if all labels are identical or if the input is empty.\n2. **Counting Frequencies**: We use the `Counter` class from the `collections` module to count the occurrences of each unique label in the input sequence.\n3. **Calculating Probabilities**: For each unique label, we compute its probability by dividing its count by the total number of labels.\n4. **Entropy Calculation**: Using the probabilities, we compute the entropy by summing \\( p_i \\log_2 p_i \\) for each label and then multiplying the result by -1.\n5. **Edge Cases**: Handle cases where the input is empty or all labels are identical by returning an entropy of 0.0.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    counts = Counter(labels)\n    total = sum(counts.values())\n    if total == 0:\n        return 0.0\n    ent = 0.0\n    for count in counts.values():\n        p = count / total\n        ent += p * math.log2(p)\n    ent *= -1\n    return round(ent, 5)\n```\n\n### Explanation\n1. **Counting Frequencies**: The `Counter` class efficiently counts the occurrences of each label in the input sequence.\n2. **Total Calculation**: The total number of labels is computed by summing the counts of all unique labels.\n3. **Probability and Entropy Calculation**: For each unique label, its probability is calculated, and the entropy is updated by adding the product of the probability and its logarithm base 2.\n4. **Edge Handling**: If the input is empty, the function immediately returns 0.0. If all labels are identical, the calculated entropy will naturally be 0.0 due to the properties of the logarithm function.\n\nThis approach ensures that the Shannon entropy is computed efficiently and correctly handles all specified edge cases."}
{"task_id": 356, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that extracts the correct prediction from a `Leaf` instance based on whether the tree is used for classification or regression. The function should return the predicted class index for classification and the raw scalar value for regression.\n\n### Approach\n1. **Problem Analysis**:\n   - For classification trees, each leaf node stores an array of class-membership probabilities. The predicted class is the index of the highest probability, with ties resolved in favor of the smaller index.\n   - For regression trees, each leaf node stores the mean of the target values that fell into that region, which is a scalar value.\n\n2. **Intuition**:\n   - When dealing with classification, we need to find the index of the maximum probability in the array. This can be efficiently done using the `max` function to find the highest probability and then using the `index` method to find the first occurrence of this maximum value.\n   - For regression, the value stored in the leaf is directly returned as it represents the mean value.\n\n3. **Algorithm Selection**:\n   - Use the `max` function to find the highest probability in the array for classification.\n   - Use the `index` method to find the position of this maximum value, ensuring the smallest index is chosen in case of ties.\n\n### Solution Code\n```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        probs = leaf.value\n        max_prob = max(probs)\n        return probs.index(max_prob)\n    else:\n        return leaf.value\n```\n\n### Explanation\n- **Classification Case**: The function retrieves the array of probabilities from the leaf. It finds the maximum probability using `max` and then determines the index of this maximum value using `index`, ensuring the smallest index is returned in case of ties.\n- **Regression Case**: The function directly returns the scalar value stored in the leaf, which represents the mean of the target values.\n\nThis approach efficiently handles both classification and regression scenarios, ensuring the correct prediction is returned based on the type of tree."}
{"task_id": 362, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert an arbitrary 2-D list or NumPy array of non-negative numbers into a row-stochastic matrix. A row-stochastic matrix is one where each row sums to 1. The function should handle various edge cases, such as rows summing to zero, and return the normalized matrix rounded to four decimal places.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input can be a list of lists or a NumPy array. We first convert it into a NumPy array of float type to facilitate efficient numerical operations.\n2. **Compute Row Sums**: Calculate the sum of each row. If any row sum is zero, return -1 as normalization is impossible.\n3. **Normalize Each Row**: Divide each element in a row by the sum of that row to make the row sum to 1.\n4. **Round Values**: Round each element of the resulting matrix to four decimal places.\n5. **Convert to List of Lists**: Convert the final NumPy array back to a native Python list of lists for the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    data_array = np.array(data, dtype=float)\n    row_sums = np.sum(data_array, axis=1)\n    \n    if np.any(row_sums == 0):\n        return -1\n    \n    normalized = data_array / row_sums[:, None]\n    rounded = np.round(normalized, 4)\n    \n    return rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input data is converted to a NumPy array with float type to handle both lists and NumPy arrays uniformly.\n2. **Row Sum Calculation**: Using NumPy's `sum` function along axis=1, we compute the sum of each row. If any row sum is zero, the function immediately returns -1.\n3. **Normalization**: Each element in a row is divided by the row's sum. This is efficiently done using NumPy's broadcasting.\n4. **Rounding**: The normalized values are rounded to four decimal places to meet the problem's requirements.\n5. **Conversion to List**: The final NumPy array is converted back to a list of lists using the `tolist()` method to provide the result in the required format.\n\nThis approach ensures that the function handles various input types and edge cases efficiently, providing the correct row-stochastic matrix or indicating when normalization is impossible."}
{"task_id": 363, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine whether the action and observation spaces of a given environment are continuous. A space is considered continuous if it is an instance of `Box`. For composite spaces (`Tuple` or `Dict`), the space is continuous only if all its sub-spaces are `Box`.\n\n### Approach\n1. **Identify Space Type**: For each space (action and observation), check if it is a composite space (indicated by the flags `tuple_action` and `tuple_obs`).\n2. **Check Continuity for Composite Spaces**: If the space is composite (either `Tuple` or `Dict`), iterate through all its sub-spaces. If all sub-spaces are `Box`, the space is continuous.\n3. **Check Continuity for Non-Composite Spaces**: If the space is not composite, simply check if it is an instance of `Box`.\n\n### Solution Code\n```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    # Check action space continuity\n    if tuple_action:\n        action_space = env.action_space\n        if isinstance(action_space, Tuple):\n            sub_spaces = action_space.spaces\n        else:  # it's a Dict\n            sub_spaces = action_space.spaces.values()\n        cont_action = all(isinstance(s, Box) for s in sub_spaces)\n    else:\n        cont_action = isinstance(env.action_space, Box)\n    \n    # Check observation space continuity\n    if tuple_obs:\n        obs_space = env.observation_space\n        if isinstance(obs_space, Tuple):\n            sub_spaces = obs_space.spaces\n        else:  # it's a Dict\n            sub_spaces = obs_space.spaces.values()\n        cont_obs = all(isinstance(s, Box) for s in sub_spaces)\n    else:\n        cont_obs = isinstance(env.observation_space, Box)\n    \n    return (cont_action, cont_obs)\n```\n\n### Explanation\n- **Composite Spaces Check**: When the space is a `Tuple` or `Dict`, we extract all sub-spaces and check if each is a `Box`. If any sub-space is not a `Box`, the entire space is not continuous.\n- **Non-Composite Spaces Check**: If the space is not composite, we directly check if it is a `Box`.\n\nThis approach ensures that we correctly determine the continuity of both action and observation spaces based on their structure and the given flags."}
{"task_id": 369, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate the outputs of multiple weak learners (usually decision trees) in a Gradient Boosting model. The aggregation process involves summing the scaled outputs of these trees and then processing the result based on whether the task is regression or classification.\n\n### Approach\n1. **Initialization**: Start with a running prediction initialized to zero. The shape of this running prediction depends on whether the task is regression or classification.\n2. **Summing Updates**: Compute the sum of all the updates provided by the individual trees. This sum is then scaled by the learning rate.\n3. **Running Prediction**: The running prediction is obtained by taking the negative of the scaled sum of updates.\n4. **Regression Handling**: For regression tasks, the running prediction is directly returned after rounding to four decimal places.\n5. **Classification Handling**: For classification tasks, the running prediction is treated as logits. These logits are converted to probabilities using the softmax function, and the class with the highest probability is selected using the argmax function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        # If there are no updates, running prediction is zero\n        shape = updates[0].shape if updates else (0,)\n        running_pred = np.zeros(shape, dtype=np.float64)\n    else:\n        # Compute the sum of all updates\n        sum_updates = np.sum(updates, axis=0)\n        # Compute the running prediction\n        running_pred = -learning_rate * sum_updates\n\n    if regression:\n        # For regression, round to 4 decimal places\n        return np.round(running_pred, 4)\n    else:\n        # For classification, apply softmax and get argmax\n        # Compute max logits for numerical stability\n        max_logits = np.max(running_pred, axis=1, keepdims=True)\n        # Compute exponentials\n        exp_logits = np.exp(running_pred - max_logits)\n        # Compute sum of exponents for each row\n        sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n        # Compute probabilities\n        probabilities = exp_logits / sum_exp\n        # Get predicted class indices\n        predicted_classes = np.argmax(probabilities, axis=1)\n        return predicted_classes\n```\n\n### Explanation\n1. **Summing Updates**: The updates from all trees are summed up. This is done efficiently using NumPy's `sum` function along the appropriate axis.\n2. **Scaling by Learning Rate**: The summed updates are scaled by the learning rate, which controls the contribution of each tree's output.\n3. **Running Prediction**: The running prediction is initialized to zero and updated by subtracting the scaled sum of updates.\n4. **Regression Handling**: The running prediction is directly returned after rounding to four decimal places for regression tasks.\n5. **Classification Handling**: The running prediction is treated as logits. The softmax function is applied to convert these logits into probabilities, and the argmax function is used to determine the predicted class labels.\n\nThis approach ensures that the function is fully vectorized and efficient, leveraging NumPy operations to handle large datasets without explicit Python loops."}
{"task_id": 373, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Gini impurity of a given sequence of integer class labels. The Gini impurity is a measure used in decision-tree learning to quantify how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset.\n\n### Approach\n1. **Check for Empty Input**: If the input sequence is empty, return 0.0 as the Gini impurity by convention.\n2. **Convert Input to NumPy Array**: Convert the input, whether it is a list or a NumPy array, into a NumPy array for efficient computation.\n3. **Calculate Class Counts**: Use NumPy's `unique` function to get the unique classes and their counts in the input sequence.\n4. **Compute Total Samples**: Determine the total number of samples, which is the length of the input sequence.\n5. **Calculate Sum of Squared Probabilities**: For each unique class, compute the probability of the class occurring, square it, and sum these squared probabilities.\n6. **Compute Gini Impurity**: Subtract the sum of squared probabilities from 1 to get the Gini impurity.\n7. **Round the Result**: Return the Gini impurity rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gini(y: list[int] | np.ndarray) -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    if len(y) == 0:\n        return 0.0\n    \n    y_array = np.asarray(y)\n    unique_classes, counts = np.unique(y_array, return_counts=True)\n    N = y_array.size\n    sum_squares = np.sum((counts / N) ** 2)\n    gini_value = 1 - sum_squares\n    return round(gini_value, 4)\n```\n\n### Explanation\n- **Empty Input Handling**: The function immediately returns 0.0 if the input sequence is empty.\n- **Efficient Counting**: Using NumPy's `unique` function with `return_counts=True` efficiently counts the occurrences of each class.\n- **Probability Calculation**: The probability of each class is calculated as the count of the class divided by the total number of samples. Squaring these probabilities and summing them gives the sum of squared probabilities.\n- **Gini Impurity Calculation**: Subtracting the sum of squared probabilities from 1 yields the Gini impurity, which is then rounded to four decimal places for the final result.\n\nThis approach ensures that the function is both efficient and easy to understand, leveraging NumPy for optimized computations."}
{"task_id": 374, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to train a multiple linear regression model using batch gradient descent. The goal is to minimize the mean-squared error (MSE) by adjusting the model parameters (weights and bias) iteratively.\n\n### Approach\n1. **Input Consistency Check**: Ensure the number of samples in the feature matrix `X` and the target vector `y` are the same. If not, return `-1`.\n2. **Reshape Handling**: If the feature matrix `X` is a 1D array, reshape it to a 2D array for consistency in matrix operations.\n3. **Initialization**: Initialize the weight vector `theta` with zeros and the bias `b` with zero.\n4. **Gradient Descent Iterations**: For each iteration, compute the predicted values, error, and gradients. Update the weights and bias using the computed gradients and the learning rate.\n5. **Rounding**: After completing the iterations, round the weights and bias to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\n\n    Your task is to implement this function following the specification\n    provided in the problem statement.\n    \"\"\"\n    # Check if the number of samples in X and y are consistent\n    if X.shape[0] != y.shape[0]:\n        return -1\n    \n    # Reshape X to 2D if it's 1D\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    \n    m = X.shape[0]\n    n = X.shape[1]\n    \n    # Initialize parameters\n    theta = np.zeros(n)\n    b = 0.0\n    \n    for _ in range(n_iterations):\n        # Compute predictions\n        y_hat = np.dot(X, theta) + b\n        \n        # Calculate the error\n        error = y_hat - y\n        \n        # Compute gradients\n        dw = (2.0 / m) * np.dot(X.T, error)\n        db = (2.0 / m) * np.sum(error)\n        \n        # Update parameters\n        theta = theta - learning_rate * dw\n        b = b - learning_rate * db\n    \n    # Round the parameters to four decimal places\n    theta = np.round(theta, 4)\n    b = round(b, 4)\n    \n    return (theta.tolist(), b)\n```\n\n### Explanation\n1. **Input Consistency Check**: The function first checks if the number of samples in `X` and `y` are the same. If not, it returns `-1`.\n2. **Reshape Handling**: If `X` is a 1D array, it is reshaped to a 2D array to facilitate matrix operations.\n3. **Initialization**: The weight vector `theta` is initialized to zeros, and the bias `b` is initialized to zero.\n4. **Gradient Descent Iterations**: For each iteration, the predicted values `y_hat` are computed. The error between predicted and actual values is used to compute the gradients for weights (`dw`) and bias (`db`). These gradients are then used to update the parameters `theta` and `b` using the specified learning rate.\n5. **Rounding**: After completing the iterations, the weights and bias are rounded to four decimal places to meet the problem requirements.\n\nThis approach ensures that the model parameters are optimized to minimize the mean-squared error using batch gradient descent, providing an efficient solution for multiple linear regression."}
{"task_id": 376, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given 2-D numeric array is symmetric with respect to its main diagonal. A matrix is considered symmetric if it is square and each element at position (i, j) is equal to the element at position (j, i) for all valid indices (i, j). Additionally, due to potential floating-point rounding errors, we consider two numbers equal if their absolute difference does not exceed 1 \u00d7 10\u207b\u2078.\n\n### Approach\n1. **Check if the Matrix is Square**: First, we verify if the matrix is square by ensuring that the number of rows is equal to the number of columns. This is done by checking the length of each row against the total number of rows.\n2. **Convert to NumPy Array**: If the matrix is square, we convert it into a NumPy array for easier manipulation and comparison.\n3. **Transpose and Compare**: We then compute the transpose of the matrix and use NumPy's `allclose` function to check if the original matrix and its transpose are equal within the specified tolerance (1 \u00d7 10\u207b\u2078).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    n = len(X)\n    for row in X:\n        if len(row) != n:\n            return False\n    arr = np.array(X)\n    return np.allclose(arr, arr.T, atol=1e-8)\n```\n\n### Explanation\n1. **Square Matrix Check**: The function first checks if the matrix is square by comparing the length of each row with the total number of rows. If any row does not match the expected length, the function returns `False`.\n2. **NumPy Conversion**: The matrix is converted into a NumPy array to leverage efficient array operations.\n3. **Transpose Comparison**: Using NumPy's `allclose` function, the original matrix is compared with its transpose. This function returns `True` if all elements are within the specified tolerance, indicating the matrix is symmetric.\n\nThis approach ensures that we efficiently and accurately determine the symmetry of the matrix, handling both integer and floating-point values appropriately."}
{"task_id": 377, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the negative gradient of the logistic loss function used in Gradient Boosting for binary classification. The negative gradient is used to update the model predictions in each boosting iteration.\n\n### Approach\n1. **Input Validation**: First, we check if all elements in the label array `y` are either 0 or 1. If any element is outside this range, we return -1.\n2. **Label Transformation**: Convert the binary labels from {0, 1} to {\u22121, 1} using the transformation `y' = 2y - 1`.\n3. **Negative Gradient Calculation**: For each sample, compute the negative gradient using the formula `g_i = y'_i / (1 + exp(y'_i * f_i))`, where `y'_i` is the transformed label and `f_i` is the current prediction score.\n4. **Rounding**: Round each computed gradient value to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"Compute the negative gradient of the logistic loss for binary classification.\n    \n    Args:\n    y (list): Binary class labels (0 or 1).\n    f (list): Current prediction scores.\n    \n    Returns:\n    list: A list of negative gradients rounded to four decimal places.\n          Returns -1 if any label in y is not 0 or 1.\n    \"\"\"\n    # Check if all elements in y are 0 or 1\n    if not all(yi in (0, 1) for yi in y):\n        return -1\n    \n    # Convert to numpy arrays for efficient computation\n    y_np = np.array(y, dtype=int)\n    f_np = np.array(f, dtype=float)\n    \n    # Compute y_prime\n    y_prime = 2 * y_np - 1\n    \n    # Compute the denominator\n    denominator = 1 + np.exp(y_prime * f_np)\n    \n    # Compute the negative gradient\n    g = y_prime / denominator\n    \n    # Round to four decimal places and convert back to list\n    g_rounded = np.round(g, 4)\n    \n    return g_rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first ensures that all labels are valid (0 or 1). If any label is invalid, it returns -1 immediately.\n2. **Label Transformation**: The labels are transformed from {0, 1} to {\u22121, 1} to be used in the logistic loss function.\n3. **Gradient Calculation**: Using vectorized operations with NumPy, the function efficiently computes the negative gradient for each sample. This involves element-wise multiplication and exponentiation, followed by division.\n4. **Rounding**: The computed gradients are rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the function is both efficient and correct, handling edge cases and large inputs gracefully."}
{"task_id": 394, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the RMSprop optimization algorithm's update step. RMSprop is an adaptive learning rate optimization algorithm used for training neural networks. It maintains a running average of the squared gradients to scale the learning rate for each parameter, which helps in stabilizing the training process.\n\n### Approach\nThe RMSprop update step involves two main computations:\n1. Updating the running average of the squared gradients.\n2. Adjusting the parameter vector using the current gradient and the updated running average.\n\nThe steps are as follows:\n1. **Initialize the running average**: If the running average `E_g` is not provided (i.e., it is `None`), initialize it with a zero array of the same shape as the gradient.\n2. **Compute the new running average**: This is done using the formula \\( E_g^{(t)} = \\rho E_g^{(t-1)} + (1 - \\rho) g^2 \\), where \\( \\rho \\) is the decay rate and \\( g \\) is the current gradient.\n3. **Update the parameter vector**: The parameter vector is updated using the formula \\( w^{(t)} = w^{(t-1)} - \\frac{\\alpha g}{\\sqrt{E_g^{(t)} + \\varepsilon}} \\), where \\( \\alpha \\) is the learning rate and \\( \\varepsilon \\) is a small constant for numerical stability.\n4. **Round and convert results**: Both the updated parameter vector and the new running average are rounded to four decimal places and converted to regular Python lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n    \n    # Compute the new running average of squared gradients\n    Eg_new = rho * Eg + (1 - rho) * (grad ** 2)\n    \n    # Compute the denominator for the weight update\n    denominator = np.sqrt(Eg_new + 1e-8)\n    \n    # Update the parameter vector\n    w_new = w - (learning_rate * grad) / denominator\n    \n    # Round the results to four decimal places\n    w_rounded = np.round(w_new, 4)\n    Eg_rounded = np.round(Eg_new, 4)\n    \n    # Convert numpy arrays to regular Python lists\n    w_list = w_rounded.tolist()\n    Eg_list = Eg_rounded.tolist()\n    \n    return (w_list, Eg_list)\n```\n\n### Explanation\n1. **Initialization**: The function first checks if the running average `E_g` is `None`. If it is, `E_g` is initialized to a zero array of the same shape as the gradient.\n2. **Running Average Update**: The new running average `Eg_new` is computed using the decay rate `rho` and the squared gradient values.\n3. **Parameter Update**: The parameter vector `w` is updated by subtracting the product of the learning rate and the gradient, scaled by the inverse square root of the new running average plus a small constant for stability.\n4. **Rounding and Conversion**: Both the updated parameter vector and the new running average are rounded to four decimal places and converted to regular Python lists before being returned.\n\nThis approach ensures that the RMSprop update step is correctly implemented, handling both initialisation and subsequent updates efficiently."}
{"task_id": 398, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Exponential Linear Unit (ELU) activation function and its gradient. The ELU function is widely used in deep learning models because it helps the network converge faster and reduces the vanishing-gradient problem. The function and its gradient are computed element-wise for each input value.\n\n### Approach\n1. **Problem Analysis**: The ELU function is defined differently for positive and negative inputs. For positive inputs, it is simply the input value. For negative inputs, it is a scaled exponential function. The gradient of ELU is 1 for positive inputs and a scaled exponential function for negative inputs.\n2. **Intuition**: The ELU function helps in avoiding the vanishing gradient problem by ensuring that the gradient for negative values is not zero but a small positive value. This is achieved by using an exponential function scaled by a hyper-parameter alpha.\n3. **Algorithm Selection**: We use NumPy for efficient element-wise operations on arrays. This allows us to handle both lists and NumPy arrays as input seamlessly.\n4. **Complexity Analysis**: The operations are performed element-wise, making the time complexity O(n), where n is the number of elements in the input array.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    x_arr = np.asarray(x)\n    mask = x_arr >= 0\n    if not derivative:\n        res = np.where(mask, x_arr, alpha * (np.exp(x_arr) - 1))\n    else:\n        res = np.where(mask, 1.0, alpha * np.exp(x_arr))\n    rounded = np.round(res, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `x` is converted to a NumPy array to facilitate efficient element-wise operations.\n2. **Mask Creation**: A boolean mask is created to identify positive values (including zero) in the input array.\n3. **ELU Activation Calculation**: For non-derivative cases, the function computes the ELU activation using the mask. Positive values are kept as they are, while negative values are transformed using the scaled exponential function.\n4. **Gradient Calculation**: For derivative cases, the function computes the gradient. Positive values result in a gradient of 1, while negative values use the scaled exponential function.\n5. **Rounding and Conversion**: The results are rounded to four decimal places and converted back to a Python list for the final output.\n\nThis approach ensures that the function is both efficient and easy to understand, leveraging NumPy's capabilities for element-wise operations."}
{"task_id": 419, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Bayesian linear regression with a conjugate Gaussian prior and known observation variance. The goal is to compute the Maximum A Posteriori (MAP) predictive mean for new data points.\n\n### Approach\n1. **Add Intercept Column**: If `fit_intercept` is True, add a column of ones to both the training data `X` and the new data `X_new` to account for the intercept term in the regression model.\n2. **Process Prior Parameters**: The prior distribution for the parameter vector `\u03b2` is Gaussian with mean `\u03bc` and covariance matrix `V`. Handle different forms of `V` (scalar, 1D array, or 2D matrix) and construct the appropriate matrix. If `V` is None, use the identity matrix.\n3. **Compute Posterior Distribution**: Using the training data, compute the posterior distribution parameters. The posterior covariance matrix is given by the inverse of the sum of the inverse prior covariance matrix and the matrix product of the transpose of `X` with itself. The posterior mean is computed using the inverse of this sum matrix multiplied by the sum of the product of the inverse prior covariance matrix with the prior mean and the product of the transpose of `X` with the target vector `y`.\n4. **Predictions**: For each new data point in `X_new`, compute the predictive mean using the posterior mean and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    The function must return the MAP predictive means for every row in\n    `X_new`.  See the task description for full details.\n    \"\"\"\n    # Process X and X_new by adding intercept if needed\n    if fit_intercept:\n        X_proc = np.hstack((np.ones((X.shape[0], 1)), X))\n        X_new_proc = np.hstack((np.ones((X_new.shape[0], 1)), X_new))\n    else:\n        X_proc = X\n        X_new_proc = X_new\n\n    n_features = X_proc.shape[1]\n\n    # Process V to construct the covariance matrix\n    if V is None:\n        V = np.eye(n_features)\n    elif isinstance(V, (float, int)):\n        V = V * np.eye(n_features)\n    elif isinstance(V, np.ndarray):\n        if V.ndim == 1:\n            V = np.diag(V)\n        else:\n            if V.shape != (n_features, n_features):\n                raise ValueError(\"V must be a square matrix of size n_features x n_features\")\n    else:\n        raise ValueError(\"V must be None, float, or ndarray\")\n\n    V_inv = np.linalg.inv(V)\n\n    # Process mu to ensure it's a vector of appropriate length\n    if isinstance(mu, (float, int)):\n        mu = np.full(n_features, mu)\n    elif isinstance(mu, np.ndarray):\n        if mu.shape[0] != n_features:\n            raise ValueError(\"mu must be a scalar or vector of length n_features\")\n    else:\n        raise ValueError(\"mu must be a scalar or ndarray\")\n\n    # Compute terms for the posterior mean\n    term1 = V_inv @ mu\n    term2 = X_proc.T @ y\n    numerator = term1 + term2\n\n    # Compute the posterior covariance and mean\n    sum_matrix = V_inv + X_proc.T @ X_proc\n    Sigma_p = np.linalg.inv(sum_matrix)\n    mu_p = Sigma_p @ numerator\n\n    # Compute predictions for X_new\n    predictions = X_new_proc @ mu_p\n\n    # Round to 4 decimal places and convert to list\n    rounded_predictions = [round(pred, 4) for pred in predictions]\n    return rounded_predictions\n```\n\n### Explanation\n1. **Adding Intercept**: The code first checks if an intercept should be added to both the training and new data matrices. This is done by appending a column of ones to the left of each matrix.\n2. **Constructing Covariance Matrix**: The prior covariance matrix `V` is processed to handle different input types (scalar, 1D array, or 2D matrix). If `V` is None, it defaults to an identity matrix.\n3. **Posterior Distribution**: The posterior covariance matrix is computed as the inverse of the sum of the inverse prior covariance matrix and the matrix product of the transpose of `X` with itself. The posterior mean is then calculated using this covariance matrix and the combined terms from the prior and data.\n4. **Predictions**: For each new data point, the predictive mean is computed by taking the dot product of the new data matrix with the posterior mean vector. Each prediction is rounded to four decimal places and returned as a list.\n\nThis approach ensures that we correctly handle different forms of prior parameters and compute the MAP estimates efficiently."}
{"task_id": 423, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Root Mean Squared Logarithmic Error (RMSLE) between two lists of non-negative numbers. RMSLE is a regression evaluation metric that measures the ratio between predicted and actual values, penalizing the ratio rather than the absolute difference.\n\n### Approach\n1. **Input Validation**: \n   - Ensure both input lists (`actual` and `predicted`) have the same length.\n   - Ensure both lists are non-empty.\n   - Ensure all elements in both lists are non-negative.\n\n2. **Compute RMSLE**:\n   - Convert the input lists into numpy arrays for efficient computation.\n   - Compute the logarithm of each element in both arrays after adding 1 to avoid taking the logarithm of zero.\n   - Calculate the differences between the logarithms of the predicted and actual values.\n   - Square these differences and compute their mean.\n   - Take the square root of the mean to get the RMSLE.\n   - Round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Calculates the Root Mean Squared Logarithmic Error (RMSLE) between two lists of numbers.\n    \n    Args:\n        actual (list[float]): The ground truth values.\n        predicted (list[float]): The predicted values.\n    \n    Returns:\n        float: The RMSLE value rounded to four decimal places, or -1 if the inputs are invalid.\n    \"\"\"\n    # Check if the lengths are equal\n    if len(actual) != len(predicted):\n        return -1\n    \n    n = len(actual)\n    # Check if the lists are empty\n    if n == 0:\n        return -1\n    \n    # Check for non-negative values\n    for a, p in zip(actual, predicted):\n        if a < 0 or p < 0:\n            return -1\n    \n    # Convert to numpy arrays\n    actual_np = np.array(actual, dtype=float)\n    predicted_np = np.array(predicted, dtype=float)\n    \n    # Compute logarithms\n    log_actual = np.log(1 + actual_np)\n    log_predicted = np.log(1 + predicted_np)\n    \n    # Calculate differences and square them\n    diffs = log_predicted - log_actual\n    squared_diffs = diffs ** 2\n    \n    # Compute the mean of squared differences\n    mean_sq = np.mean(squared_diffs)\n    \n    # Compute RMSLE\n    rmsle = np.sqrt(mean_sq)\n    \n    # Round to four decimal places\n    return round(rmsle, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input lists are valid. This includes checking if they have the same length, are non-empty, and contain only non-negative numbers. If any of these conditions are not met, the function returns -1.\n\n2. **Logarithmic Transformation**: The function converts the input lists into numpy arrays and computes the logarithm of each element after adding 1. This transformation helps in penalizing the ratio of predicted to actual values rather than their absolute differences.\n\n3. **Difference Calculation**: The differences between the logarithms of the predicted and actual values are computed, squared, and then averaged.\n\n4. **RMSLE Calculation**: The square root of the mean of the squared differences is taken to get the RMSLE, which is then rounded to four decimal places for the final result.\n\nThis approach ensures that the function efficiently computes the RMSLE while handling edge cases and invalid inputs appropriately."}
{"task_id": 428, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a Hann window (also known as the Hanning window) of a specified length. The Hann window is widely used in digital signal processing tasks such as spectral analysis and short-time Fourier transforms because its endpoints smoothly reach zero, reducing spectral leakage.\n\n### Approach\nThe approach to generate the Hann window involves the following steps:\n\n1. **Handle Special Cases**: \n   - If the window length is less than or equal to 0, return an empty list.\n   - If the window length is 1, return a list containing a single element, 1.0, regardless of the symmetric parameter.\n\n2. **Determine the Computed Length**:\n   - If the symmetric parameter is True, the computed length is the same as the window length.\n   - If the symmetric parameter is False, the computed length is the window length plus 1. This is because a periodic window is created by building a symmetric window of length window_len + 1 and then dropping the last entry.\n\n3. **Generate the Window**:\n   - Use the formula for the Hann window: \\( \\text{hann}(n) = 0.5 - 0.5 \\times \\cos\\left(\\frac{2\\pi n}{\\text{computed_length} - 1}\\right) \\) for each \\( n \\) from 0 to computed_length - 1.\n\n4. **Adjust for Periodic Window**:\n   - If the symmetric parameter is False, drop the last element of the computed window to ensure the result has the specified window length.\n\n5. **Round and Convert**:\n   - Round each element of the window to four decimal places and convert the result from a NumPy array to a regular Python list.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    computed_length = window_len if symmetric else window_len + 1\n    n = np.arange(computed_length)\n    window = 0.5 - 0.5 * np.cos(2 * np.pi * n / (computed_length - 1))\n    \n    if not symmetric:\n        window = window[:-1]\n    \n    window = np.round(window, 4)\n    return window.tolist()\n```\n\n### Explanation\nThe function `hann` generates a Hann window based on the specified length and symmetry. It handles special cases for non-positive lengths and a length of 1 directly. For other cases, it computes the window using NumPy for efficiency, adjusts the window length if necessary for periodicity, and ensures the result is rounded and converted to a list as required. This approach ensures that the generated window meets the requirements for both symmetric and periodic use cases in digital signal processing."}
{"task_id": 433, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the fan-in and fan-out for a given weight tensor shape. These values are crucial in neural network initialization schemes like Xavier/Glorot initialization. The approach varies depending on whether the tensor is a 2-D convolutional kernel or another type of tensor.\n\n### Approach\n1. **Identify the Tensor Type**: \n   - If the tensor shape has exactly 4 dimensions, it is treated as a 2-D convolutional kernel with the layout `(out_channels, in_channels, kernel_height, kernel_width)`.\n   - For all other cases (including 2-D, 3-D, and higher-dimensional tensors), the first two dimensions are taken directly as fan-in and fan-out.\n\n2. **Compute Fan-in and Fan-out for 4-D Tensors**:\n   - Calculate the receptive field size as the product of the kernel's height and width.\n   - Fan-in is computed as the product of the number of input channels and the receptive field size.\n   - Fan-out is computed as the product of the number of output channels and the receptive field size.\n\n3. **Compute Fan-in and Fan-out for Other Tensors**:\n   - Directly use the first two dimensions of the shape as fan-in and fan-out.\n\n### Solution Code\n```python\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    if len(shape) == 4:\n        in_channels = shape[1]\n        out_channels = shape[0]\n        kernel_h = shape[2]\n        kernel_w = shape[3]\n        receptive_field = kernel_h * kernel_w\n        fan_in = in_channels * receptive_field\n        fan_out = out_channels * receptive_field\n    else:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    return (float(fan_in), float(fan_out))\n```\n\n### Explanation\n- **4-D Convolutional Kernel**: For a 4-D tensor, the function calculates the receptive field size by multiplying the kernel's height and width. Fan-in is then the product of the input channels and the receptive field size, while fan-out is the product of the output channels and the receptive field size.\n- **Other Tensors**: For tensors with dimensions other than 4, the function simply takes the first two dimensions as fan-in and fan-out, ensuring the result is returned as floats.\n\nThis approach efficiently handles both convolutional and non-convolutional tensors, providing the necessary fan-in and fan-out values for proper neural network initialization."}
{"task_id": 435, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a small-scale version of the Gradient Boosting Regressor using ordinary least-squares (OLS) linear regression as the weak learner and squared error as the loss function. The goal is to predict target values based on the given training data.\n\n### Approach\n1. **Convert Input Data**: Convert the input lists into NumPy arrays for efficient numerical computations.\n2. **Handle Special Cases**: If the number of estimators is non-positive or the learning rate is zero, return a vector filled with the mean of the target values.\n3. **Initialize Prediction**: Start with the initial prediction as the mean of the target values.\n4. **Iterative Boosting**: For each estimator, compute the residuals (difference between actual and predicted values), fit an OLS model to these residuals, and update the prediction using the learning rate.\n5. **Return Result**: After all iterations, round the final predictions to four decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]], y: list[float], n_estimators: int = 10, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    X_np = np.array(X, dtype=np.float64)\n    y_np = np.array(y, dtype=np.float64)\n    \n    if n_estimators <= 0 or learning_rate == 0:\n        y_mean = y_np.mean()\n        return [round(y_mean, 4)] * len(y_np)\n    \n    m = X_np.shape[0]\n    y_hat = np.full(m, y_np.mean())\n    \n    for _ in range(n_estimators):\n        residuals = y_np - y_hat\n        X_extended = np.hstack((np.ones((m, 1)), X_np))\n        coefficients, _, _, _ = np.linalg.lstsq(X_extended, residuals, rcond=None)\n        h = np.dot(X_extended, coefficients)\n        y_hat += learning_rate * h\n    \n    rounded = [round(float(val), 4) for val in y_hat]\n    return rounded\n```\n\n### Explanation\n1. **Data Conversion**: The input lists `X` and `y` are converted to NumPy arrays for efficient computation.\n2. **Special Cases Handling**: If the number of estimators is zero or negative, or the learning rate is zero, the function immediately returns a list where each element is the mean of the target values, rounded to four decimal places.\n3. **Initial Prediction**: The initial prediction is set to the mean of the target values.\n4. **Boosting Loop**: For each estimator, the residuals are computed. An OLS model is fit to these residuals using an extended design matrix (including an intercept term). The model's predictions are then used to update the overall prediction, scaled by the learning rate.\n5. **Result Preparation**: After completing the iterations, the final predictions are rounded to four decimal places and returned as a list.\n\nThis approach efficiently combines multiple weak learners (OLS regressions) to produce a strong predictive model, leveraging the gradient boosting technique to iteratively reduce the error."}
{"task_id": 437, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the logistic sigmoid function and its first and second derivatives. The function should handle scalar, list, and NumPy array inputs and return the results in the appropriate format, rounded to four decimal places.\n\n### Approach\n1. **Check Order Validity**: The function first checks if the provided order is 0, 1, or 2. If not, it returns -1.\n2. **Determine Input Type**: The function checks if the input is a scalar (int or float) or another type (list or NumPy array).\n3. **Convert Input to NumPy Array**: The input is converted to a NumPy array to handle both scalar and non-scalar inputs uniformly.\n4. **Compute Sigmoid Function**: The logistic sigmoid function is computed for the input values.\n5. **Compute Derivatives**: Depending on the order, the function computes the first or second derivative using the precomputed sigmoid values.\n6. **Round Results**: The results are rounded to four decimal places.\n7. **Return Appropriate Type**: If the input was a scalar, the result is returned as a float. Otherwise, the result is returned as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in [0, 1, 2]:\n        return -1\n    \n    is_scalar = isinstance(x, (int, float))\n    arr = np.asarray(x)\n    \n    sigma = 1 / (1 + np.exp(-arr))\n    \n    if order == 0:\n        res = sigma\n    elif order == 1:\n        res = sigma * (1 - sigma)\n    else:  # order == 2\n        res = sigma * (1 - sigma) * (1 - 2 * sigma)\n    \n    rounded = np.round(res, 4)\n    \n    if is_scalar:\n        return float(rounded.item())\n    else:\n        return rounded.tolist()\n```\n\n### Explanation\n- **Order Check**: The function immediately returns -1 if the order is not 0, 1, or 2.\n- **Input Handling**: The input is converted to a NumPy array to facilitate element-wise operations, regardless of whether the input is a scalar, list, or NumPy array.\n- **Sigmoid Computation**: The logistic sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n- **Derivative Computation**: The first derivative is computed as \\( \\sigma(x)(1 - \\sigma(x)) \\) and the second derivative as \\( \\sigma(x)(1 - \\sigma(x))(1 - 2\\sigma(x)) \\).\n- **Rounding and Return**: The results are rounded to four decimal places. If the input was a scalar, the result is returned as a float; otherwise, it is returned as a list, preserving the original shape.\n\nThis approach ensures that the function efficiently handles different input types and correctly computes the required values while maintaining the expected output format."}
{"task_id": 438, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that converts a vector of class labels into a one-hot encoded matrix. This matrix will have a 1 in the column corresponding to the class label for each row, and 0s elsewhere. The function should handle both cases where the number of classes is provided and where it needs to be inferred automatically.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input can be a list or a NumPy array, so we first convert it to a NumPy array for easier manipulation.\n2. **Handle Empty Input**: If the input is empty, return an empty list immediately.\n3. **Determine Number of Classes**: If the number of classes (`num_classes`) is not provided, infer it as the maximum value in the input plus one. If `num_classes` is provided, check if it is smaller than the maximum value plus one. If it is, return -1 to indicate an error.\n4. **Create One-Hot Encoded Matrix**: If the number of classes is valid, create a matrix of zeros with dimensions `(number of samples, num_classes)`. For each sample, set the corresponding column to 1 based on the class label.\n5. **Convert to List of Lists**: Convert the resulting NumPy array to a list of lists before returning it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    y = np.asarray(y)\n    n_samples = len(y)\n    if n_samples == 0:\n        return []\n    \n    y_max = y.max()\n    \n    if num_classes is None:\n        num_classes = y_max + 1\n    else:\n        if num_classes < y_max + 1:\n            return -1\n    \n    result = np.zeros((n_samples, num_classes), dtype=int)\n    for i in range(n_samples):\n        result[i, y[i]] = 1\n    \n    return result.tolist()\n```\n\n### Explanation\n- **Conversion to NumPy Array**: This allows us to use NumPy functions for efficient computation.\n- **Handling Empty Input**: Directly return an empty list if there are no samples to process.\n- **Inferring Number of Classes**: If `num_classes` is not provided, it is determined by the maximum value in the input plus one to ensure all classes are covered.\n- **Error Handling**: If the provided `num_classes` is too small to cover all class labels, return -1.\n- **Matrix Construction**: A matrix of zeros is created, and each row is updated to have a 1 in the column corresponding to the class label of the sample.\n- **Conversion to List of Lists**: The final NumPy array is converted to a list of lists for the required output format.\n\nThis approach ensures that the function efficiently handles both provided and inferred class counts, correctly constructs the one-hot encoded matrix, and returns the appropriate result or error indicator."}
{"task_id": 440, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate the probability predictions from multiple estimators (such as trees in a Random Forest) to produce an averaged probability distribution for each sample and determine the predicted class label based on these averaged probabilities.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input is a three-level nested list. Converting this to a NumPy array allows us to leverage vectorized operations for efficient computation.\n2. **Compute Average Probabilities**: For each sample, compute the average probability across all estimators. This is done using NumPy's `mean` function along the axis corresponding to the estimators.\n3. **Round Probabilities**: Round each averaged probability to four decimal places to meet the problem's requirements.\n4. **Determine Predicted Class Labels**: For each sample, find the class with the highest probability after rounding. In case of ties, the smallest index is chosen, which is the default behavior of NumPy's `argmax` function.\n5. **Convert Results to Lists**: Convert the NumPy arrays back to regular Python lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    # Convert the predictions list to a NumPy array for easier manipulation\n    pred_array = np.array(predictions)\n    \n    # Compute the average probability across all estimators for each sample and class\n    avg_probs = np.mean(pred_array, axis=0)\n    \n    # Round the averaged probabilities to four decimal places\n    rounded_avg = np.round(avg_probs, 4)\n    \n    # Determine the predicted class for each sample based on the rounded probabilities\n    class_labels = np.argmax(rounded_avg, axis=1)\n    \n    # Convert the results back to Python lists\n    averaged_probs_list = rounded_avg.tolist()\n    class_labels_list = class_labels.tolist()\n    \n    return (averaged_probs_list, class_labels_list)\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input list is converted to a NumPy array to facilitate efficient computation of means and other operations.\n2. **Averaging Probabilities**: Using `np.mean` along the first axis (estimators), we compute the average probability for each class across all estimators for every sample.\n3. **Rounding**: The averaged probabilities are rounded to four decimal places to ensure precision and meet the problem's requirements.\n4. **Class Prediction**: Using `np.argmax`, we determine the class with the highest probability for each sample. This function naturally handles ties by returning the smallest index.\n5. **Conversion to Lists**: The results are converted back to Python lists to match the expected output format.\n\nThis approach efficiently handles the aggregation and prediction tasks using vectorized operations, ensuring both accuracy and performance."}
{"task_id": 453, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the training targets for a Deep Q-Network (DQN) based on the given inputs. The targets are derived from the current and next state Q-values, actions taken, rewards received, and whether the next state is terminal. The function must handle input validation and return the updated Q-values as a list of lists, rounded to four decimal places.\n\n### Approach\n1. **Input Validation**: Check if the discount factor `gamma` is within the valid range [0, 1]. Ensure that the lengths of `actions`, `rewards`, and `dones` match the batch size of `Q` and `Q_next`. Also, verify that `Q` and `Q_next` have the same shape.\n2. **Target Calculation**: For each sample in the batch, compute the target Q-value. If the next state is terminal, the target is just the reward. Otherwise, the target is the reward plus the discounted maximum Q-value of the next state.\n3. **Update Q-Values**: Create a copy of the current Q-values and update the specific action's Q-value with the computed target for each sample.\n4. **Rounding and Conversion**: Round the updated Q-values to four decimal places and convert the result to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # Check if gamma is within the valid range\n    if gamma < 0 or gamma > 1:\n        return -1\n    \n    # Get the batch size from Q\n    batch_size = Q.shape[0]\n    \n    # Check if all input lists have the same length as the batch size\n    if (len(actions) != batch_size or\n        len(rewards) != batch_size or\n        len(dones) != batch_size):\n        return -1\n    \n    # Check if Q and Q_next have the same shape\n    if Q.shape != Q_next.shape:\n        return -1\n    \n    # Copy the original Q values to create the target matrix\n    y = Q.copy()\n    \n    # Iterate through each sample in the batch\n    for i in range(batch_size):\n        action = actions[i]\n        reward = rewards[i]\n        done = dones[i]\n        \n        if done:\n            target = reward\n        else:\n            # Compute the maximum Q_next value for the next state\n            max_next_q = np.max(Q_next[i])\n            target = reward + gamma * max_next_q\n        \n        # Update the specific action's Q-value\n        y[i, action] = target\n    \n    # Round the values to four decimal places\n    y = np.round(y, 4)\n    \n    # Convert the numpy array to a list of lists\n    return y.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if `gamma` is within the valid range and if all input lists have consistent lengths with the batch size of `Q` and `Q_next`. If any check fails, it returns -1.\n2. **Target Calculation**: For each sample, the target Q-value is computed based on whether the next state is terminal. If terminal, the target is the reward; otherwise, it includes the discounted maximum Q-value of the next state.\n3. **Update Q-Values**: The function updates the specific action's Q-value in the copied Q matrix with the computed target.\n4. **Rounding and Conversion**: The updated Q-values are rounded to four decimal places and converted to a list of lists for the final output.\n\nThis approach ensures that the function correctly computes the training targets for DQN while handling edge cases and input validation efficiently."}
{"task_id": 458, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a one-dimensional array-like object of non-negative integer labels into a two-dimensional one-hot encoded matrix. This is a common requirement in machine learning workflows where neural network libraries expect class labels in a one-hot (categorical) form.\n\n### Approach\n1. **Check for Empty Input**: If the input list of labels is empty, return an empty list immediately.\n2. **Check for Negative Labels**: If any label is negative, return -1 as this is an invalid input.\n3. **Determine Number of Classes**: If the number of classes (`num_classes`) is not provided, infer it as the maximum label value plus one. If `num_classes` is provided, ensure it is not smaller than the maximum label value plus one.\n4. **Validate Labels Against Classes**: Ensure that all labels are within the valid range of 0 to `num_classes - 1`. If any label is out of this range, return -1.\n5. **Generate One-Hot Encoded Matrix**: For each label, create a row in the resulting matrix where the position corresponding to the label is set to 1, and all other positions are set to 0.\n\n### Solution Code\n```python\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    # Check if labels is empty\n    if not labels:\n        return []\n    \n    # Check for negative labels\n    for label in labels:\n        if label < 0:\n            return -1\n    \n    max_label = max(labels)\n    \n    if num_classes is not None:\n        if num_classes < (max_label + 1):\n            return -1\n        # Check if any label is beyond num_classes - 1\n        for label in labels:\n            if label >= num_classes:\n                return -1\n    else:\n        num_classes = max_label + 1\n    \n    # Generate the one-hot encoded matrix\n    result = []\n    for label in labels:\n        row = [0] * num_classes\n        row[label] = 1\n        result.append(row)\n    \n    return result\n```\n\n### Explanation\n1. **Empty Input Handling**: The function immediately returns an empty list if the input labels are empty.\n2. **Negative Label Check**: The function iterates through each label to check for negative values, returning -1 if any are found.\n3. **Infer Number of Classes**: If `num_classes` is not provided, it is inferred as the maximum label value plus one. If provided, it must be at least one more than the maximum label value.\n4. **Label Validation**: Each label is checked to ensure it falls within the valid range of 0 to `num_classes - 1`.\n5. **Matrix Generation**: For each label, a row is created with a 1 at the position corresponding to the label and 0s elsewhere, resulting in the one-hot encoded matrix.\n\nThis approach ensures that the function handles various edge cases and invalid inputs gracefully while efficiently generating the required one-hot encoded matrix."}
{"task_id": 471, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement one step of the on-policy TD(0) Expected\u2013SARSA algorithm for a tabular setting. The goal is to update the Q-table based on the immediate reward, the next state, and the expected future value calculated using the \u03b5-soft policy.\n\n### Approach\n1. **Check for Terminal State**: If the next state is `None`, the expected future value is zero, so the target is just the immediate reward.\n2. **Compute Expected Future Value**: For a non-terminal next state, calculate the expected Q-value using the \u03b5-soft policy. This involves:\n   - Identifying the greedy action (the action with the highest Q-value, breaking ties by choosing the first occurrence).\n   - Calculating the probability for each action under the \u03b5-soft policy.\n   - Summing the product of each action's Q-value and its probability to get the expected value.\n3. **Calculate Target**: The target is the sum of the immediate reward and the discounted expected future value.\n4. **Update Q-Table**: Adjust the Q-value for the current state-action pair using the target and learning rate.\n5. **Round Values**: Ensure all values in the Q-table are rounded to four decimal places before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    # Create a deep copy of the Q-table to avoid modifying the original\n    q = [row.copy() for row in q_table]\n    num_actions = len(q[0]) if q else 0\n\n    if next_state is not None:\n        q_next = q[next_state]\n        max_val = max(q_next)\n        # Find the first action with the maximum Q-value\n        greedy_action = next((a for a, val in enumerate(q_next) if val == max_val), None)\n        expected_value = 0.0\n        for a in range(num_actions):\n            if a == greedy_action:\n                prob = (1.0 - epsilon) + (epsilon / num_actions)\n            else:\n                prob = epsilon / num_actions\n            expected_value += prob * q_next[a]\n        target = reward + gamma * expected_value\n    else:\n        target = reward\n\n    # Update the specific state-action pair\n    current_q = q[state][action]\n    new_q = current_q + lr * (target - current_q)\n    q[state][action] = new_q\n\n    # Round all values to four decimal places\n    for s in range(len(q)):\n        for a in range(len(q[s])):\n            q[s][a] = round(q[s][a], 4)\n\n    return q\n```\n\n### Explanation\n1. **Deep Copy of Q-Table**: To avoid modifying the original Q-table, a deep copy is created.\n2. **Terminal State Check**: If the next state is `None`, the target is simply the immediate reward.\n3. **Greedy Action Identification**: The action with the highest Q-value in the next state is identified, with ties broken by selecting the first occurrence.\n4. **Probability Calculation**: Each action's probability under the \u03b5-soft policy is calculated, with higher probability for the greedy action.\n5. **Expected Value Calculation**: The expected future value is computed as the sum of each action's Q-value multiplied by its probability.\n6. **Target Calculation**: The target is computed using the immediate reward and the expected future value.\n7. **Q-Table Update**: The Q-value for the current state-action pair is updated using the target and learning rate.\n8. **Rounding**: All values in the Q-table are rounded to four decimal places to ensure precision.\n\nThis approach ensures that the Q-table is updated correctly according to the Expected\u2013SARSA algorithm, considering both terminal and non-terminal transitions."}
{"task_id": 474, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a 2-D toy data set suitable for a binary classification problem. The data set should be perfectly separable, with each class arranged in rectangular clusters. The function should be reusable, accept various parameters, and ensure reproducibility with an optional random seed.\n\n### Approach\n1. **Problem Analysis**: The task is to generate a 2-D data set where each class is arranged in a rectangular cluster. The first class (label -1) is roughly in the square [1,9]\u00d7[1,9], while the second class (label +1) is shifted upward by a specified interval. The function should handle multiple classes and generate both training and validation sets.\n\n2. **Point Generation Rule**: For each class index `i`:\n   - The x-coordinate is uniformly sampled from the range [(\u230ai/2\u230b + 0.1) * 10, (\u230ai/2\u230b + 0.9) * 10].\n   - The y-coordinate is uniformly sampled from the range [((i mod 2) * interval + 0.1) * 10, ((i mod 2) * interval + 0.9) * 10].\n   - The label is determined by the formula (i - 0.5) * 2.\n\n3. **Reusability and Reproducibility**: The function should accept parameters for the number of classes, samples per class for training and validation, interval, and an optional random seed for reproducibility.\n\n4. **Implementation Steps**:\n   - Set the random seed if provided.\n   - For each class, generate the specified number of training and validation samples.\n   - Collect all generated data and concatenate them into the final arrays.\n   - Round all coordinates to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    X_train_list = []\n    Y_train_list = []\n    X_val_list = []\n    Y_val_list = []\n    \n    for i in range(m):\n        # Calculate x range\n        x_low = ((i // 2) + 0.1) * 10\n        x_high = ((i // 2) + 0.9) * 10\n        \n        # Calculate y range\n        y_low = ((i % 2) * interval + 0.1) * 10\n        y_high = ((i % 2) * interval + 0.9) * 10\n        \n        # Determine the label\n        label = (i - 0.5) * 2\n        \n        # Generate training data for this class\n        x_train = np.random.uniform(x_low, x_high, n_train)\n        y_train = np.random.uniform(y_low, y_high, n_train)\n        X_train_samples = np.column_stack((x_train, y_train))\n        Y_train_samples = np.full(n_train, label)\n        \n        # Generate validation data for this class\n        x_val = np.random.uniform(x_low, x_high, n_val)\n        y_val = np.random.uniform(y_low, y_high, n_val)\n        X_val_samples = np.column_stack((x_val, y_val))\n        Y_val_samples = np.full(n_val, label)\n        \n        # Append to the lists\n        X_train_list.append(X_train_samples)\n        Y_train_list.append(Y_train_samples)\n        X_val_list.append(X_val_samples)\n        Y_val_list.append(Y_val_samples)\n    \n    # Concatenate all the samples\n    X_train = np.concatenate(X_train_list, axis=0)\n    Y_train = np.concatenate(Y_train_list, axis=0)\n    X_val = np.concatenate(X_val_list, axis=0)\n    Y_val = np.concatenate(Y_val_list, axis=0)\n    \n    # Round to 4 decimal places\n    X_train = np.round(X_train, 4)\n    X_val = np.round(X_val, 4)\n    \n    return (X_train, X_val, Y_train, Y_val)\n```\n\n### Explanation\n- **Random Seed Handling**: The function sets the random seed if provided to ensure reproducibility of the generated data.\n- **Class-wise Data Generation**: For each class, the function calculates the x and y ranges based on the class index and interval. It then generates the specified number of training and validation samples within these ranges.\n- **Data Concatenation**: After generating data for all classes, the function concatenates the samples into the final training and validation arrays.\n- **Rounding**: All coordinates are rounded to four decimal places to meet the problem requirements.\n\nThis approach ensures that the generated data set is perfectly separable, with each class arranged in distinct rectangular clusters, making it suitable for binary classification tasks."}
{"task_id": 475, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the core mathematics of the Adam optimizer, which is a popular stochastic gradient descent algorithm. The Adam optimizer uses two moving averages of the gradients to update the parameters, making it more efficient and adaptive. The task is to perform a single Adam update step given the current parameters, gradients, and previous moments.\n\n### Approach\nThe Adam optimizer maintains two moving averages of the gradients: the first moment (mean) and the second moment (uncentred variance). These moments are updated iteratively and then bias-corrected to produce more accurate parameter updates. The steps involved in the Adam update are as follows:\n\n1. **Initialize Moments**: If the previous first moment (`m_prev`) or second moment (`v_prev`) is `None`, initialize them to zero arrays of the same shape as the gradient.\n2. **Update Moments**: Compute the new first moment (`m_t`) and second moment (`v_t`) using the given formulas.\n3. **Bias Correction**: Apply bias correction to the moments to get the bias-corrected moments (`m_hat` and `v_hat`).\n4. **Parameter Update**: Use the bias-corrected moments to update the parameters, ensuring numerical stability by adding a small constant to the denominator.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    # Handle cases where m_prev or v_prev is None by initializing to zero arrays\n    if m_prev is None:\n        m_prev = np.zeros_like(grad)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad)\n    \n    # Compute the first and second moments\n    m_t = b1 * m_prev + (1 - b1) * grad\n    v_t = b2 * v_prev + (1 - b2) * (grad ** 2)\n    \n    # Compute the bias correction denominators\n    denom_m = 1.0 - (b1 ** t)\n    denom_v = 1.0 - (b2 ** t)\n    \n    # Compute bias-corrected moments\n    m_hat = m_t / denom_m\n    v_hat = v_t / denom_v\n    \n    # Compute the denominator for the parameter update\n    sqrt_v_hat = np.sqrt(v_hat)\n    denominator = sqrt_v_hat + eps\n    \n    # Update the parameters\n    w_new = w - learning_rate * (m_hat / denominator)\n    \n    # Return the updated parameters and moments\n    return (w_new, m_t, v_t)\n```\n\n### Explanation\n1. **Initialization**: The function starts by checking if the previous moments (`m_prev` and `v_prev`) are `None`. If they are, it initializes them to zero arrays of the same shape as the gradient.\n2. **Moment Updates**: The first moment (`m_t`) is updated using the formula `m_t = \u03b2\u2081 * m_prev + (1 - \u03b2\u2081) * grad`. Similarly, the second moment (`v_t`) is updated using `v_t = \u03b2\u2082 * v_prev + (1 - \u03b2\u2082) * grad\u00b2`.\n3. **Bias Correction**: The moments are bias-corrected by dividing them by `(1 - \u03b2\u2081\u1d57)` and `(1 - \u03b2\u2082\u1d57)` to get `m_hat` and `v_hat`.\n4. **Parameter Update**: The parameters are updated using the bias-corrected moments. The update rule is `w_new = w - \u03b1 * m_hat / (\u221av_hat + \u03b5)`, where `\u03b1` is the learning rate and `\u03b5` is a small constant for numerical stability.\n\nThis approach ensures that the parameters are updated efficiently and adaptively, leveraging the moving averages of gradients to escape local minima and handle varying learning rates."}
{"task_id": 479, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the \u03b5-greedy algorithm for the stationary multi-armed bandit problem. The goal is to simulate one run of the \u03b5-greedy strategy and return the final estimates of the expected reward for every arm.\n\n### Approach\nThe \u03b5-greedy algorithm is a popular strategy for balancing exploration and exploitation in the multi-armed bandit problem. The algorithm works as follows:\n\n1. **Initialization**: Each arm is initialized with a prior expected value (ev_prior) and a counter set to zero to keep track of the number of times each arm has been pulled.\n\n2. **Exploration vs. Exploitation**: At each time step, with probability \u03b5, the algorithm chooses an arm uniformly at random (exploration). Otherwise, it selects the arm with the highest estimated expected value (exploitation). In case of ties, the arm with the smallest index is chosen.\n\n3. **Reward and Update**: After selecting an arm, the corresponding reward is received. The estimate for the selected arm is then updated using the incremental sample mean formula.\n\n4. **Return Estimates**: After processing all time steps, the final estimates of the expected rewards for each arm are returned, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where ``rewards[t][i]`` is the reward for arm *i* at time-step\n        *t*.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    if not rewards:\n        return []\n    \n    N = len(rewards[0])\n    estimates = [ev_prior] * N\n    counts = [0] * N\n    \n    for t in range(len(rewards)):\n        if np.random.rand() < epsilon:\n            arm = np.random.randint(N)\n        else:\n            max_val = max(estimates)\n            candidates = [i for i, val in enumerate(estimates) if val == max_val]\n            arm = min(candidates)\n        \n        r = rewards[t][arm]\n        counts[arm] += 1\n        estimates[arm] += (r - estimates[arm]) / counts[arm]\n    \n    return [round(v, 4) for v in estimates]\n```\n\n### Explanation\n1. **Initialization**: The number of arms (N) is determined from the length of the first row of the rewards matrix. Each arm's estimated value is initialized to ev_prior, and the count of pulls for each arm is set to zero.\n\n2. **Exploration vs. Exploitation**: For each time step, a random number is generated. If this number is less than \u03b5, an arm is chosen randomly. Otherwise, the arm with the highest estimated value is selected, with ties broken by choosing the smallest index.\n\n3. **Reward and Update**: The reward for the selected arm at the current time step is retrieved. The count for the selected arm is incremented, and its estimated value is updated using the incremental sample mean formula.\n\n4. **Return Estimates**: After processing all time steps, the estimated values for each arm are rounded to four decimal places and returned as a list.\n\nThis approach ensures that the algorithm balances exploration and exploitation effectively, providing a robust solution to the multi-armed bandit problem."}
{"task_id": 482, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that converts a vector of integer class labels into a 2-D array where each row is a one-hot representation of the corresponding label. This is a common technique used in machine learning to transform categorical data into a numerical format that can be processed by algorithms.\n\n### Approach\nThe function `to_categorical` takes a vector of integer labels and an optional parameter `num_classes`. The approach involves the following steps:\n\n1. **Convert Input to NumPy Array**: The input vector `y` is converted into a NumPy array for efficient numerical operations.\n2. **Handle Empty Input**: If the input vector `y` is empty, the function returns an empty NumPy array with appropriate dimensions based on whether `num_classes` is provided or not.\n3. **Determine Number of Classes**: If `num_classes` is not provided, it is inferred as the maximum value in `y` plus one. If `num_classes` is provided, the function checks that all labels in `y` are valid (i.e., non-negative and less than `num_classes`).\n4. **Create One-Hot Encoded Array**: Using NumPy's `eye` function, an identity matrix of size `num_classes` is created. The rows of this matrix corresponding to the labels in `y` are selected to form the one-hot encoded result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    y = np.asarray(y)\n    \n    if y.size == 0:\n        if num_classes is None:\n            return np.zeros((0, 0), dtype=int)\n        else:\n            return np.zeros((0, num_classes), dtype=int)\n    \n    if num_classes is None:\n        if (y < 0).any():\n            raise ValueError(\"Negative labels found when num_classes is None.\")\n        num_classes = max(y) + 1\n    else:\n        if (y < 0).any() or (y >= num_classes).any():\n            raise ValueError(\"Invalid label\")\n    \n    result = np.eye(num_classes, dtype=int)[y]\n    return result\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `y` is converted to a NumPy array to facilitate efficient array operations.\n2. **Handling Empty Input**: If `y` is empty, the function returns an empty array with shape `(0, 0)` if `num_classes` is `None`, otherwise `(0, num_classes)`.\n3. **Inferring Number of Classes**: When `num_classes` is not provided, it is calculated as `max(y) + 1`. If any label is negative, a `ValueError` is raised.\n4. **Validation with Provided Classes**: If `num_classes` is provided, the function ensures all labels are within the valid range `[0, num_classes - 1]`.\n5. **One-Hot Encoding**: Using `np.eye`, an identity matrix is created, and the rows corresponding to the labels in `y` are selected to form the one-hot encoded result.\n\nThis approach efficiently handles the conversion of integer labels to their one-hot encoded form, ensuring correctness and optimal performance using NumPy operations."}
{"task_id": 490, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a one-dimensional vector into a square diagonal matrix where the elements of the vector are placed on the principal diagonal, and all off-diagonal positions are filled with zeros. The input can be a list, tuple, or a one-dimensional NumPy array, and the output should be a list of lists.\n\n### Approach\n1. **Check for Empty Input**: If the input vector is empty, return an empty list immediately.\n2. **Convert Input to NumPy Array**: Use NumPy's `asarray` function to convert the input into a NumPy array if it isn't already. This handles lists, tuples, and NumPy arrays uniformly.\n3. **Create Diagonal Matrix**: Utilize NumPy's `diag` function to create a diagonal matrix from the input vector. This function efficiently places the elements of the vector on the principal diagonal and fills the rest with zeros.\n4. **Convert to List of Lists**: Convert the resulting NumPy matrix into a list of lists using the `tolist()` method for the final output.\n\nThis approach ensures that the solution is efficient and handles various input types gracefully.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    if not x:\n        return []\n    x_np = np.asarray(x)\n    matrix = np.diag(x_np)\n    return matrix.tolist()\n```\n\n### Explanation\n- **Empty Input Handling**: The function first checks if the input is empty and returns an empty list if true.\n- **Conversion to NumPy Array**: The input is converted to a NumPy array to leverage NumPy's efficient matrix operations.\n- **Diagonal Matrix Creation**: Using `np.diag`, the function constructs a square matrix where the input vector forms the diagonal.\n- **Conversion to List of Lists**: The resulting NumPy matrix is converted to a list of lists for the final output, ensuring compatibility with standard Python data structures.\n\nThis method efficiently handles the conversion and ensures the resulting matrix meets the problem requirements."}
{"task_id": 493, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the mean-squared error (MSE) of a given 1-D numeric sample. The MSE is a measure of the average squared difference between each element in the sample and the sample mean. If the input sequence is empty, the function should return -1.\n\n### Approach\n1. **Check for Empty Input**: First, we check if the input is empty. If it is, we return -1 immediately.\n2. **Convert to NumPy Array**: Convert the input to a NumPy array to leverage efficient vectorized operations.\n3. **Compute the Mean**: Calculate the mean of the sample.\n4. **Compute Squared Differences**: For each element in the sample, compute the squared difference from the mean.\n5. **Compute MSE**: Calculate the average of these squared differences to get the MSE.\n6. **Round the Result**: Round the resulting MSE to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    y_array = np.asarray(y)\n    n = y_array.size\n    if n == 0:\n        return -1\n    mean_y = y_array.mean()\n    squared_diffs = (y_array - mean_y) ** 2\n    mse_value = squared_diffs.mean()\n    return round(mse_value, 4)\n```\n\n### Explanation\n1. **Check for Empty Input**: The function first converts the input to a NumPy array and checks if its size is zero. If it is, the function returns -1.\n2. **Compute the Mean**: Using NumPy's `mean()` function, we compute the mean of the sample.\n3. **Compute Squared Differences**: We subtract the mean from each element and square the result. This is done efficiently using vectorized operations.\n4. **Compute MSE**: The mean of the squared differences is computed, which gives the MSE.\n5. **Round the Result**: The result is rounded to four decimal places using Python's `round()` function to meet the problem's requirements.\n\nThis approach ensures that the function handles both lists and NumPy arrays efficiently and returns the correct MSE rounded to four decimal places."}
{"task_id": 496, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that applies an affine activation to an input vector and returns the activation values along with their first and second derivatives. The function should handle both lists and NumPy arrays, and it should treat scalar inputs as single-element vectors.\n\n### Approach\n1. **Input Handling**: Convert the input `x` into a NumPy array to handle both lists and scalars uniformly. If `x` is a scalar, it is reshaped into a 1-element array.\n2. **Affine Activation Calculation**: Compute the affine activation values using the formula \\( y = \\text{slope} \\cdot x + \\text{intercept} \\).\n3. **Derivative Calculations**: The first derivative of the affine function is the slope, and the second derivative is zero. These are computed element-wise for the entire input vector.\n4. **Rounding**: Each element of the activation values and their derivatives is rounded to four decimal places.\n5. **Conversion to Lists**: Convert the NumPy arrays back to lists before returning them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    x = np.asarray(x)\n    if x.ndim == 0:\n        x = x.reshape(1)\n    y = slope * x + intercept\n    grad = np.full_like(x, slope)\n    grad2 = np.zeros_like(x)\n    y_rounded = np.round(y, 4)\n    grad_rounded = np.round(grad, 4)\n    grad2_rounded = np.round(grad2, 4)\n    y_list = y_rounded.tolist()\n    grad_list = grad_rounded.tolist()\n    grad2_list = grad2_rounded.tolist()\n    return (y_list, grad_list, grad2_list)\n```\n\n### Explanation\n1. **Input Conversion**: The input `x` is converted to a NumPy array to handle both lists and scalars. If `x` is a scalar, it is reshaped into a 1-element array to ensure consistent processing.\n2. **Affine Activation**: The affine activation is computed using the formula \\( y = \\text{slope} \\cdot x + \\text{intercept} \\). This operation is element-wise, making it efficient for vectors.\n3. **Derivatives**: The first derivative of the affine function is simply the slope, and the second derivative is zero. These are computed for each element in the input vector.\n4. **Rounding**: Each element of the activation values and their derivatives is rounded to four decimal places to meet the problem's requirements.\n5. **Return Values**: The results are converted from NumPy arrays back to lists and returned as a tuple of three lists.\n\nThis approach ensures that the function handles various input types efficiently and returns the required results in the specified format."}
{"task_id": 509, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Chebyshev distance between two real-valued vectors. The Chebyshev distance is defined as the maximum absolute difference between corresponding elements of the two vectors. The function must handle both lists and NumPy arrays as input and return the distance rounded to four decimal places.\n\n### Approach\n1. **Check Vector Lengths**: First, verify if the lengths of the two input vectors are the same. If they are not, return -1.\n2. **Compute Absolute Differences**: For each pair of corresponding elements in the two vectors, compute the absolute difference.\n3. **Find Maximum Difference**: Track the maximum absolute difference encountered during the iteration.\n4. **Round the Result**: Round the maximum difference to four decimal places and return it as a float.\n\nThis approach ensures that we handle both lists and NumPy arrays efficiently without converting them to a specific type, leveraging Python's built-in functions for simplicity and clarity.\n\n### Solution Code\n```python\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    max_diff = 0.0\n    for a, b in zip(x, y):\n        diff = abs(a - b)\n        if diff > max_diff:\n            max_diff = diff\n    return round(max_diff, 4)\n```\n\n### Explanation\n- **Vector Length Check**: The function starts by checking if the lengths of the two vectors are the same. If not, it returns -1 immediately.\n- **Iterate and Compute Differences**: Using a loop, the function iterates through each pair of elements from the two vectors, computes the absolute difference, and keeps track of the maximum difference found.\n- **Rounding**: After finding the maximum difference, the result is rounded to four decimal places using Python's `round` function and returned as a float.\n\nThis approach ensures that the function is efficient, handles various input types, and meets the problem requirements accurately."}
{"task_id": 510, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Variational Auto-Encoder (VAE) loss function, which consists of two main components: the reconstruction loss and the Kullback-Leibler (KL) divergence. The goal is to compute the average loss for a mini-batch of data, ensuring numerical stability and correct handling of edge cases.\n\n### Approach\n1. **Reconstruction Loss**: This is computed using the binary cross-entropy between the ground truth input `y` and the reconstructed probabilities `y_pred`. To avoid numerical instability when taking logarithms, we clip the values of `y_pred` to a small range around 0 and 1.\n2. **KL Divergence**: This measures the difference between the approximate posterior distribution and the unit Gaussian prior. It is computed using the mean and log-variance of the latent variables, which are provided as inputs.\n3. **Combining Losses**: The total loss is the sum of the reconstruction loss and the KL divergence, averaged over the mini-batch.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    eps = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, eps, 1 - eps)\n    \n    # Compute reconstruction loss (binary cross-entropy)\n    recon_loss = y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped)\n    recon_loss = -recon_loss  # Because BCE is the negative of this sum\n    total_recon = np.sum(recon_loss)\n    \n    # Compute KL divergence\n    mu_squared = t_mean ** 2\n    var = np.exp(t_log_var)\n    kl_terms = 1 + t_log_var - mu_squared - var\n    total_kl = np.sum(kl_terms) * (-0.5)\n    \n    # Total loss is the average over the mini-batch\n    batch_size = y.shape[0]\n    total_loss = (total_recon + total_kl) / batch_size\n    \n    # Round to six decimal places\n    return round(total_loss, 6)\n```\n\n### Explanation\n1. **Clipping Values**: The reconstructed probabilities `y_pred` are clipped to ensure they lie within the interval (\u03b5, 1-\u03b5) to avoid taking the logarithm of 0 or 1, which would cause numerical instability.\n2. **Reconstruction Loss Calculation**: For each element in the mini-batch, the binary cross-entropy loss is computed. This is done by summing the losses for each feature in each sample and then taking the negative of the sum.\n3. **KL Divergence Calculation**: The KL divergence is computed using the provided mean and log-variance of the latent variables. The formula used is derived from the analytical form of the KL divergence between two normal distributions.\n4. **Combining Losses**: The total loss is the sum of the reconstruction loss and the KL divergence, divided by the batch size to get the average loss per sample.\n5. **Rounding**: The final result is rounded to six decimal places to meet the problem's requirements.\n\nThis approach ensures that the VAE loss is computed efficiently and correctly, handling potential numerical issues and providing the required precision in the result."}
{"task_id": 513, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the predictions for a Factorization Machine (FM) regression model. The FM model combines linear regression with pairwise feature interactions, making it capable of capturing both linear and non-linear relationships in the data.\n\n### Approach\nThe FM prediction formula is given by:\n\u0177 = w\u2080 + \u03a3\u2c7c w\u2c7c x\u2c7c + \u00bd \u03a3_{f=1}^{k} [ (\u03a3\u2c7c V_{j,f} x\u2c7c)\u00b2 \u2212 \u03a3\u2c7c V_{j,f}\u00b2 x\u2c7c\u00b2 ]\n\nWhere:\n- w\u2080 is the global bias term.\n- **w** is the vector of linear weights.\n- **V** is the matrix of latent factors that model pairwise interactions.\n- k is the number of latent factors.\n\nThe approach involves the following steps:\n1. **Linear Part Calculation**: Compute the linear part of the prediction using the formula w\u2080 + \u03a3\u2c7c w\u2c7c x\u2c7c. This is done using matrix multiplication.\n2. **Interaction Part Calculation**: Compute the interaction part using the latent factors. For each factor, compute the sum of the products of the features and their corresponding latent factors, square this sum, and then subtract the sum of the squared products. This is done efficiently using matrix operations to avoid loops.\n3. **Summing Contributions**: Sum the contributions from all latent factors and multiply by 0.5.\n4. **Combining Results**: Add the linear and interaction parts to get the final prediction.\n5. **Rounding**: Round the final predictions to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    # Convert input lists to numpy arrays\n    X_np = np.array(X, dtype=float)\n    w_np = np.array(w, dtype=float)\n    V_np = np.array(V, dtype=float)\n    \n    # Compute the linear part of the prediction\n    linear_part = w0 + np.dot(X_np, w_np)\n    \n    # Compute the interaction part\n    # Calculate s = X * V, which gives the sum for each factor\n    s = np.dot(X_np, V_np)\n    s_squared = s ** 2\n    \n    # Calculate the sum of V^2 element-wise multiplied by X^2\n    X_squared = X_np ** 2\n    V_squared = V_np ** 2\n    sum_v2x2 = np.dot(X_squared, V_squared)\n    \n    # Compute the term for each factor and sum them\n    term = s_squared - sum_v2x2\n    interaction = 0.5 * np.sum(term, axis=1)\n    \n    # Combine linear and interaction parts\n    predictions = linear_part + interaction\n    \n    # Round to 4 decimal places\n    rounded_predictions = np.round(predictions, 4)\n    \n    # Convert numpy array to list of floats\n    return rounded_predictions.tolist()\n```\n\n### Explanation\n1. **Linear Part Calculation**: The linear part is computed using matrix multiplication between the feature matrix **X** and the weight vector **w**, with the addition of the bias term w\u2080.\n2. **Interaction Part Calculation**: For each latent factor, the sum of the products of the features and their corresponding latent factors is computed. This sum is squared, and the sum of the squared products is subtracted. This is done efficiently using matrix operations to handle all factors and samples simultaneously.\n3. **Summing Contributions**: The contributions from all latent factors are summed and scaled by 0.5.\n4. **Combining Results**: The linear and interaction parts are combined to form the final prediction.\n5. **Rounding**: The final predictions are rounded to four decimal places to meet the problem requirements.\n\nThis approach ensures that the function is efficient and handles arbitrary numbers of samples, features, and latent factors using numpy for numerical computations."}
{"task_id": 517, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the sigmoid activation function in a way that is numerically stable and works for various input types, including scalars, lists, tuples, and NumPy arrays. The sigmoid function is defined as \\( g(z) = \\frac{1}{1 + e^{-z}} \\), but we need to handle very large values of \\( z \\) to avoid overflow.\n\n### Approach\n1. **Numerical Stability**: The sigmoid function can be computed in a numerically stable way by using different expressions for positive and negative values of \\( z \\). For positive \\( z \\), we use \\( \\frac{1}{1 + e^{-z}} \\), and for negative \\( z \\), we use \\( \\frac{e^z}{1 + e^z} \\). This avoids overflow for large values of \\( |z| \\).\n2. **Handling Different Input Types**: The function should accept scalars, lists, tuples, and NumPy arrays. If the input is a scalar, the result is returned as a float. For other types, the result is returned as a list.\n3. **Vectorized Computation**: For array-like inputs, we use NumPy to compute the sigmoid element-wise efficiently. This involves creating a mask to separate positive and negative values and applying the appropriate formula to each.\n4. **Rounding**: The results are rounded to four decimal places before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    if isinstance(z, (int, float, np.number)):\n        # Handle scalar input\n        if z >= 0:\n            s = 1.0 / (1.0 + np.exp(-z))\n        else:\n            s = np.exp(z) / (1.0 + np.exp(z))\n        s = round(s, 4)\n        return float(s)\n    else:\n        # Handle array-like input\n        arr = np.asarray(z)\n        mask = (arr >= 0)\n        sig = np.zeros_like(arr)\n        # Compute for positive or zero values\n        sig[mask] = 1.0 / (1.0 + np.exp(-arr[mask]))\n        # Compute for negative values\n        sig[~mask] = np.exp(arr[~mask]) / (1.0 + np.exp(arr[~mask]))\n        # Round to four decimal places\n        rounded = np.round(sig, 4)\n        # Convert to Python list and return\n        return rounded.tolist()\n```\n\n### Explanation\n1. **Scalar Handling**: If the input is a scalar, we directly compute the sigmoid using the appropriate formula based on whether the value is positive or negative. The result is then rounded to four decimal places and returned as a float.\n2. **Array Handling**: For array-like inputs, we convert the input to a NumPy array. We then create a mask to identify positive values and compute the sigmoid for these values. For negative values, we use the alternative formula to avoid overflow. The results are rounded and converted back to a Python list before returning.\n3. **Numerical Stability**: By using different formulas for positive and negative values, we ensure that the function does not overflow for very large values of \\( |z| \\).\n\nThis approach ensures that the sigmoid function is computed efficiently and accurately for all valid input types, providing the correct results rounded to four decimal places."}
{"task_id": 518, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a decorator that automatically converts one-hot encoded NumPy arrays into their corresponding integer label representation before passing them to a metric function. This conversion is necessary because many machine learning metrics expect labels in a one-dimensional integer format rather than a one-hot encoded matrix format.\n\n### Approach\nThe approach involves creating a decorator function `unhot` that wraps a given metric function. The wrapper function within the decorator processes the input arrays to convert any one-hot encoded arrays into integer labels. The steps are as follows:\n\n1. **Check Array Dimensions**: For each input array (actual and predicted), check if it is a two-dimensional array with a second dimension greater than one. This indicates that the array is one-hot encoded.\n2. **Convert One-Hot Encoded Arrays**: If an array is identified as one-hot encoded, convert it to a one-dimensional array of integer labels using `argmax` along the second dimension.\n3. **Call the Metric Function**: After processing both arrays, call the original metric function with the processed arrays and return the result.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n    def wrapper(actual, predicted):\n        # Process the actual array\n        processed_actual = actual\n        if processed_actual.ndim == 2 and processed_actual.shape[1] > 1:\n            processed_actual = processed_actual.argmax(axis=1)\n        \n        # Process the predicted array\n        processed_predicted = predicted\n        if processed_predicted.ndim == 2 and processed_predicted.shape[1] > 1:\n            processed_predicted = processed_predicted.argmax(axis=1)\n        \n        # Call the original function with processed arrays\n        return function(processed_actual, processed_predicted)\n    \n    return wrapper\n```\n\n### Explanation\n- **Decorator Function**: The `unhot` function is a decorator that takes a metric function as input and returns a wrapped version of this function.\n- **Wrapper Function**: Inside the decorator, the `wrapper` function processes the input arrays. It checks if each array is two-dimensional and has a second dimension greater than one. If so, it converts the array using `argmax` along the second dimension to get the integer labels.\n- **Processing and Conversion**: Each input array is processed independently. If an array is one-hot encoded, it is converted to a one-dimensional array of integer labels. If not, it remains unchanged.\n- **Function Call**: The processed arrays are then passed to the original metric function, which computes the desired result using the integer labels.\n\nThis approach ensures that any one-hot encoded arrays are automatically converted to the appropriate format before being used by the metric function, making the workflow more efficient and less error-prone."}
{"task_id": 520, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the derivative of the sigmoid function given its output. The sigmoid function is defined as \\( g(z) = \\frac{1}{1 + e^{-z}} \\), and its derivative is \\( g'(z) = g(z) \\cdot (1 - g(z)) \\). The task is to compute this derivative for scalar, 1-D, or 2-D inputs, which can be Python scalars, lists, or NumPy arrays, and return the result in the same structure as the input, rounded to four decimal places.\n\n### Approach\n1. **Scalar Input**: If the input is a scalar (a single number), compute the gradient using the formula \\( x \\cdot (1 - x) \\), round the result to four decimal places, and return it as a float.\n2. **List Input**: If the input is a list, recursively process each element. If an element is a list, continue recursion; otherwise, compute the gradient for the scalar value, round it, and collect the results in a new list maintaining the original structure.\n3. **NumPy Array Input**: If the input is a NumPy array, compute the gradient for all elements using vectorized operations, round each element to four decimal places, and return the resulting NumPy array.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(z, list):\n        return [sigmoid_grad(x) for x in z]\n    elif isinstance(z, np.ndarray):\n        grad = z * (1 - z)\n        rounded = np.round(grad, 4)\n        return rounded\n    else:\n        # Scalar case\n        grad = z * (1 - z)\n        rounded = np.round(grad, 4)\n        return float(rounded)\n```\n\n### Explanation\n- **Scalar Handling**: For a scalar input, the function directly computes the gradient, rounds it, and returns it as a float.\n- **List Handling**: For a list input, the function uses recursion to process each element. If an element is a list, it processes each sub-element recursively, ensuring the structure is maintained.\n- **NumPy Array Handling**: For a NumPy array, the function leverages vectorized operations to compute the gradient for all elements efficiently, rounds each result, and returns the array.\n\nThis approach ensures that the function handles different input types appropriately, maintaining the input structure and providing the correct gradient values rounded to four decimal places."}
{"task_id": 528, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a grid of predictions for a 2-D linear classifier. The classifier is defined by a weight vector and a bias term. The goal is to create a dense grid that spans the bounding box of the given data and classify each grid point using the linear classifier.\n\n### Approach\n1. **Extract Features**: Extract the first and second features from the given dataset to determine the range for each feature.\n2. **Create Grid Points**: Using the minimum and maximum values of each feature, create two 1-D arrays of evenly spaced points. These arrays will be used to form the grid.\n3. **Meshgrid Creation**: Convert the 1-D arrays into a 2-D grid using the meshgrid function. This grid will cover the entire range of the input data.\n4. **Classification**: For each point in the grid, compute the classification using the linear classifier's decision rule. The decision rule is based on the sign of the weighted sum of the features plus the bias.\n5. **Convert to List**: Convert the resulting numpy array of classifications into a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    # Extract the x1 and x2 values from the dataset\n    x1 = [point[0] for point in X]\n    x2 = [point[1] for point in X]\n    \n    # Determine the range for each feature\n    min_x1, max_x1 = min(x1), max(x1)\n    min_x2, max_x2 = min(x2), max(x2)\n    \n    # Create the 1-D arrays for the grid\n    x1_plot = np.linspace(min_x1, max_x1, grid_n)\n    x2_plot = np.linspace(min_x2, max_x2, grid_n)\n    \n    # Generate the meshgrid\n    x1_mesh, x2_mesh = np.meshgrid(x1_plot, x2_plot)\n    \n    # Compute the decision function for each grid point\n    z = W[0] * x1_mesh + W[1] * x2_mesh + b\n    \n    # Determine the labels based on the decision function\n    labels = np.where(z >= 0, 1, -1)\n    \n    # Convert the numpy array to a list of lists\n    labels_list = labels.tolist()\n    \n    return labels_list\n```\n\n### Explanation\n1. **Extract Features**: The code extracts the first and second features from the dataset to determine the range for each feature. This helps in creating a grid that spans the entire range of the input data.\n2. **Create Grid Points**: Using `np.linspace`, the code creates two 1-D arrays, `x1_plot` and `x2_plot`, which are evenly spaced and cover the range of each feature.\n3. **Meshgrid Creation**: The `np.meshgrid` function is used to create a 2-D grid from the 1-D arrays. This grid represents all possible points within the bounding box of the input data.\n4. **Classification**: For each point in the grid, the code computes the weighted sum of the features plus the bias. The result determines the class label (1 if the result is non-negative, -1 otherwise).\n5. **Convert to List**: The resulting numpy array of labels is converted into a list of lists to meet the output requirements.\n\nThis approach ensures that the grid is correctly generated and each point is classified accurately based on the linear classifier's decision rule."}
{"task_id": 539, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the binary cross-entropy cost for a binary classification neural network. The binary cross-entropy cost measures the difference between the predicted probabilities and the actual labels, which helps in training the model to make better predictions.\n\n### Approach\nThe binary cross-entropy cost is defined as:\n\n\\[ J = -\\frac{1}{m} \\cdot \\sum [ y \\cdot \\ln(a) + (1 - y) \\cdot \\ln(1 - a) ] \\]\n\nwhere:\n- \\( m \\) is the number of training examples.\n- \\( a \\) are the predicted probabilities from the model.\n- \\( y \\) are the true labels (0 or 1).\n\nThe steps to compute this cost are as follows:\n1. **Clip the probabilities**: To avoid numerical overflow when taking logarithms, clip the predicted probabilities \\( a \\) to a range \\([1 \\times 10^{-15}, 1 - 1 \\times 10^{-15}]\\).\n2. **Compute the logarithms**: Calculate the natural logarithm of the clipped probabilities and the natural logarithm of 1 minus the clipped probabilities.\n3. **Calculate the terms**: For each example, compute the term \\( y \\cdot \\ln(a) + (1 - y) \\cdot \\ln(1 - a) \\).\n4. **Sum the terms**: Sum all the computed terms across all examples.\n5. **Compute the cost**: Multiply the sum by \\(-1/m\\) to get the final cost.\n6. **Return the result**: Convert the result to a Python float and round it to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Computes the binary cross-entropy cost.\"\"\"\n    m = AL.size\n    # Clip the probabilities to avoid numerical instability\n    AL_clipped = np.clip(AL, 1e-15, 1 - 1e-15)\n    # Compute the terms for the cost function\n    sum_terms = Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped)\n    # Sum all terms and compute the cost\n    sum_total = np.sum(sum_terms)\n    J = -sum_total / m\n    # Convert to Python float and round to 4 decimal places\n    return round(float(J), 4)\n```\n\n### Explanation\n- **Clipping Probabilities**: The probabilities are clipped to ensure they are within a valid range for taking logarithms, preventing numerical overflow.\n- **Logarithm Calculation**: The natural logarithm is used to measure the difference between predicted and actual values.\n- **Summing Terms**: Each term in the sum represents the loss for an individual example, which is then averaged over all examples.\n- **Cost Calculation**: The final cost is computed by taking the negative average of the summed terms, providing a measure of how well the model's predictions match the actual labels.\n\nThis approach ensures that the binary cross-entropy cost is computed efficiently and accurately, handling potential numerical issues and providing a clear measure of model performance."}
{"task_id": 555, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm from scratch. DBSCAN is a clustering algorithm that groups points based on density, marking points in low-density regions as noise. The algorithm identifies core points and expands clusters through a breadth-first search (BFS).\n\n### Approach\n1. **Compute Pairwise Distances**: For each point, calculate the Euclidean distance to every other point to determine its neighborhood within a given radius (\u03b5).\n2. **Build Neighborhoods**: For each point, create a list of indices of points that lie within the \u03b5 radius.\n3. **Identify Core Points**: A point is a core point if the number of points in its neighborhood (including itself) is at least the specified minimum (min_samples).\n4. **Breadth-First Search (BFS) for Clustering**: Starting from each unlabeled core point, use BFS to expand the cluster. Points are labeled as part of the current cluster if they are reachable through core points.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample. Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    n_samples = data.shape[0]\n    neighborhoods = [[] for _ in range(n_samples)]\n    \n    # Compute all pairwise distances and build neighborhoods\n    for i in range(n_samples):\n        for j in range(n_samples):\n            if i != j:\n                dist = np.linalg.norm(data[i] - data[j])\n                if dist <= eps:\n                    neighborhoods[i].append(j)\n    \n    # Determine core points\n    core = [len(neighbors) >= min_samples for neighbors in neighborhoods]\n    \n    # Initialize labels\n    labels = [-1] * n_samples\n    cluster_id = 0\n    \n    # Process each point to find and expand clusters\n    for i in range(n_samples):\n        if labels[i] == -1 and core[i]:\n            cluster_id += 1\n            labels[i] = cluster_id\n            queue = deque()\n            queue.append(i)\n            \n            while queue:\n                current = queue.popleft()\n                for neighbor in neighborhoods[current]:\n                    if labels[neighbor] == -1:\n                        labels[neighbor] = cluster_id\n                        if core[neighbor]:\n                            queue.append(neighbor)\n    \n    return labels\n```\n\n### Explanation\n1. **Neighborhood Calculation**: For each point, compute the Euclidean distance to all other points. If the distance is within \u03b5, add the point to the neighborhood list.\n2. **Core Points Identification**: A point is a core point if its neighborhood contains at least min_samples points.\n3. **Cluster Expansion**: Starting from each unlabeled core point, use BFS to explore all reachable points. Points are labeled as part of the current cluster. Core points are added to the queue to continue expanding the cluster, while non-core points are simply labeled without further expansion.\n\nThis approach ensures that all points are processed efficiently, and clusters are formed based on density, with noise points correctly identified."}
{"task_id": 563, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the dual-form Perceptron learning algorithm. This algorithm is different from the classical perceptron as it updates coefficients (alphas) for each training example rather than directly updating the weight vector. The weight vector is then recovered from these coefficients once the algorithm converges.\n\n### Approach\n1. **Initialization**: Start with an array `alpha` of zeros and a bias term `b` set to zero.\n2. **Gram Matrix**: Compute the Gram matrix `G` where each element `G[i, j]` is the dot product of the i-th and j-th training examples. This matrix helps in efficiently computing the inner products in the feature space.\n3. **Training Loop**: Iterate over the training examples, checking if each example is correctly classified. If an example is misclassified, update the corresponding coefficient `alpha` and the bias term `b`.\n4. **Convergence Check**: Stop the training if a complete pass over the training set results in no updates or if the maximum number of iterations is reached.\n5. **Weight Recovery**: After training, compute the weight vector `w` using the final values of `alpha` and the training examples.\n6. **Rounding**: Round the components of the weight vector and the bias term to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    N = len(y_data)\n    alpha = np.zeros(N)\n    b = 0.0\n    G = X_data @ X_data.T  # Compute the Gram matrix\n\n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            # Compute g using the current alpha and y\n            g = np.dot(alpha * y_data, G[:, i])\n            # Check the condition for updating\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n        if not updated:\n            break  # Converged\n\n    # Compute the weight vector w\n    w = np.sum((alpha * y_data)[:, None] * X_data, axis=0)\n    # Round the results\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = round(b, 4)\n    return (w_rounded, b_rounded)\n```\n\n### Explanation\n1. **Initialization**: The coefficients `alpha` are initialized to zero, and the bias term `b` is also initialized to zero.\n2. **Gram Matrix**: The Gram matrix `G` is computed to store the inner products of all pairs of training examples, which allows efficient computation of the margin for each example.\n3. **Training Loop**: For each training example, the margin is computed using the current coefficients and the Gram matrix. If the example is misclassified, the corresponding coefficient and bias are updated.\n4. **Convergence Check**: The loop stops early if no updates occur during a complete pass over the training set, indicating convergence.\n5. **Weight Recovery**: The weight vector `w` is computed as a weighted sum of the training examples, where the weights are given by the coefficients `alpha` and the class labels.\n6. **Rounding**: The final weight vector and bias term are rounded to four decimal places to meet the problem requirements.\n\nThis approach efficiently trains a perceptron in the dual form, leveraging the Gram matrix for computations in the feature space and ensuring convergence or stopping after a maximum number of iterations."}
{"task_id": 564, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to predict the outcome for a given sample by traversing a decision tree. The tree can be either a classification tree or a regression tree, and each type has specific traversal rules. The goal is to implement a function that correctly navigates the tree based on these rules and returns the prediction from the appropriate leaf node.\n\n### Approach\nThe function `predict_sample` is designed to handle both classification and regression trees. The approach involves recursively traversing the tree starting from the root node and making decisions based on the type of tree and the current node's properties.\n\n1. **Base Case**: If the current node is a leaf (i.e., it has a `y` attribute), return the value of `y`.\n2. **Classification Tree**:\n   - For each internal node, determine the feature to inspect using the node's `label`.\n   - Check each child node to find the one whose `x` value matches the feature value of the sample.\n   - If a matching child is found, recursively traverse to that child.\n   - If no match is found, default to the second child (index 1).\n3. **Regression Tree**:\n   - For each internal node, determine the feature to inspect using the node's `label`.\n   - Compare the feature value against the node's split point `s`.\n   - Traverse to the first child if the feature value is less than or equal to `s`, otherwise traverse to the second child.\n\n### Solution Code\n```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    if hasattr(root, 'y'):\n        return root.y\n    \n    if task_type == 'classification':\n        feature_index = root.label\n        feature_value = features[feature_index]\n        for child in root.child:\n            if child.x == feature_value:\n                return predict_sample(child, features, task_type)\n        # Default to the second child\n        return predict_sample(root.child[1], features, task_type)\n    else:  # regression\n        feature_index = root.label\n        feature_value = features[feature_index]\n        if feature_value <= root.s:\n            next_node = root.child[0]\n        else:\n            next_node = root.child[1]\n        return predict_sample(next_node, features, task_type)\n```\n\n### Explanation\n- **Base Case Handling**: The function first checks if the current node is a leaf by looking for the `y` attribute. If found, it returns the value immediately.\n- **Classification Tree Traversal**: For internal nodes in a classification tree, the function iterates through each child to find a match for the feature value. If no match is found, it defaults to the second child.\n- **Regression Tree Traversal**: For internal nodes in a regression tree, the function compares the feature value against the split point `s` and selects the appropriate child to traverse next.\n\nThis approach ensures that the tree is traversed correctly according to the given rules, and the prediction from the appropriate leaf node is returned efficiently."}
{"task_id": 569, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to analyze a collection of text documents and compute two main results: a list of all distinct words with their document-frequency ratios and a set containing the most common words based on a given cut-off value.\n\n### Approach\n1. **Extract Unique Words**: For each document, extract the unique words to avoid counting duplicates within the same document.\n2. **Count Document Frequencies**: Use a counter to keep track of how many documents each word appears in.\n3. **Compute Ratios**: Calculate the document-frequency ratio for each word by dividing the count of documents it appears in by the total number of documents. Round each ratio to four decimal places.\n4. **Sort Words**: Sort the words based on their document-frequency ratios in descending order. If two words have the same ratio, they can appear in any order.\n5. **Determine Top Words**: Based on the cut-off value, determine the set of top words. If the cut-off is `None`, include all words. If it's zero, return an empty set. Otherwise, include the top `cut_off` words.\n\n### Solution Code\n```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    word_counts = Counter()\n    for document in data:\n        _, words = document\n        unique_words = set(words)\n        for word in unique_words:\n            word_counts[word] += 1\n\n    total_documents = len(data)\n    word_ratios = []\n    if total_documents != 0:\n        for word, count in word_counts.items():\n            ratio = count / total_documents\n            rounded_ratio = round(ratio, 4)\n            word_ratios.append((word, rounded_ratio))\n\n    # Sort by ratio descending, then word ascending\n    sorted_word_ratios = sorted(word_ratios, key=lambda x: (-x[1], x[0]))\n\n    # Determine the set of top words\n    if cut_off is None:\n        top_words = {word for word, ratio in sorted_word_ratios}\n    elif cut_off == 0:\n        top_words = set()\n    else:\n        top_words = set()\n        for word, ratio in sorted_word_ratios[:cut_off]:\n            top_words.add(word)\n\n    return (sorted_word_ratios, top_words)\n```\n\n### Explanation\n1. **Extracting Unique Words**: Each document's words are converted to a set to ensure each word is counted only once per document.\n2. **Counting Frequencies**: A counter is used to track how many documents each word appears in.\n3. **Computing Ratios**: The ratio of documents each word appears in is calculated and rounded to four decimal places.\n4. **Sorting**: Words are sorted by their ratios in descending order. If two words have the same ratio, they are sorted alphabetically.\n5. **Top Words Set**: The set of top words is determined based on the cut-off value, ensuring the correct number of words are included.\n\nThis approach efficiently processes the documents, computes the necessary statistics, and returns the results in the required format."}
{"task_id": 571, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the weight vector and bias term of a linear Support Vector Machine (SVM) using the given formulas derived from the Sequential Minimal Optimization (SMO) algorithm. The weight vector and bias term are computed using the Lagrange multipliers, training data, and class labels.\n\n### Approach\n1. **Compute the Weight Vector (w):** The weight vector is calculated using the formula \\( \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_i y_i \\mathbf{x}_i \\). This can be efficiently computed using matrix operations as \\( \\mathbf{w} = X^\\top (\\boldsymbol{\\alpha} \\odot \\mathbf{y}) \\), where \\( \\boldsymbol{\\alpha} \\odot \\mathbf{y} \\) is the element-wise product of the Lagrange multipliers and the class labels.\n2. **Compute the Bias Term (b):** The bias term is calculated as the average of the residuals, where each residual is the difference between the true class label and the predicted value using the weight vector. The formula for the bias term is \\( b = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\mathbf{w}^\\top \\mathbf{x}_i) \\).\n3. **Rounding:** Both the weight vector and the bias term are rounded to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\"\"\"\n    # Calculate the weight vector w\n    alpha_y = alpha * y\n    w = np.dot(X.T, alpha_y)\n    \n    # Calculate the bias term b\n    residuals = y - np.dot(X, w)\n    b = residuals.mean()\n    \n    # Round the results to four decimal places\n    w_rounded = np.round(w, 4)\n    b_rounded = round(b, 4)\n    \n    # Convert the weight vector to a list\n    w_list = w_rounded.tolist()\n    \n    return (w_list, b_rounded)\n```\n\n### Explanation\n1. **Weight Vector Calculation:** The weight vector \\( \\mathbf{w} \\) is computed by taking the transpose of the training matrix \\( X \\) and multiplying it by the element-wise product of the Lagrange multipliers \\( \\alpha \\) and the class labels \\( y \\). This operation efficiently sums up the contributions of each training sample weighted by their respective Lagrange multipliers and class labels.\n2. **Bias Term Calculation:** The bias term \\( b \\) is computed by first calculating the residuals, which are the differences between the true class labels and the predictions made using the weight vector. The average of these residuals gives the bias term.\n3. **Rounding:** Both the weight vector and the bias term are rounded to four decimal places to meet the problem's requirements. The weight vector is converted to a list for the final output.\n\nThis approach ensures that we accurately compute the parameters of the linear SVM using the given formulas and return them in the required format."}
{"task_id": 574, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a synthetic 2-D dataset that can be used for quick experiments or visualizations. The dataset consists of training and validation points for multiple classes, where each class's points are sampled from a uniform distribution within a specific square region.\n\n### Approach\n1. **Problem Analysis**: The task requires generating points for multiple classes, each with its own square region. Each class has a specified number of training and validation points. The points are sampled from a uniform distribution within a square region defined for each class.\n\n2. **Intuition**: For each class, the square region is determined by a base value calculated as `10 * floor(c / 2)`, where `c` is the class label. Points are then uniformly sampled from this region. The use of a seed ensures reproducibility of the generated data.\n\n3. **Algorithm Selection**: We use NumPy's random functions to generate the points efficiently. The random seed is set to ensure deterministic results when the seed is provided. Points are generated for each class and then concatenated to form the final training and validation datasets.\n\n4. **Complexity Analysis**: The algorithm efficiently generates points using vectorized operations, making it suitable for large datasets. The complexity is linear with respect to the number of points generated.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    X_train = []\n    y_train = []\n    X_val = []\n    y_val = []\n    \n    for c in range(m):\n        base = 10 * (c // 2)\n        \n        # Training points\n        x_train = np.random.uniform(base + 1, base + 9, n_train)\n        y_train_c = np.random.uniform(base + 1, base + 9, n_train)\n        points_train = np.column_stack((x_train, y_train_c))\n        X_train.append(points_train)\n        y_train.append(np.full(n_train, c))\n        \n        # Validation points\n        x_val = np.random.uniform(base + 1, base + 9, n_val)\n        y_val_c = np.random.uniform(base + 1, base + 9, n_val)\n        points_val = np.column_stack((x_val, y_val_c))\n        X_val.append(points_val)\n        y_val.append(np.full(n_val, c))\n    \n    # Concatenate all points\n    X_train = np.concatenate(X_train, axis=0)\n    y_train = np.concatenate(y_train, axis=0)\n    X_val = np.concatenate(X_val, axis=0)\n    y_val = np.concatenate(y_val, axis=0)\n    \n    # Round to 4 decimal places\n    X_train = np.round(X_train, 4)\n    X_val = np.round(X_val, 4)\n    \n    return (X_train, X_val, y_train, y_val)\n```\n\n### Explanation\n1. **Seed Setting**: The function sets the random seed for both the standard random module and NumPy to ensure reproducibility of the generated data when a seed is provided.\n\n2. **Loop Through Classes**: For each class, the base value is calculated, and points are generated within the specified square region using NumPy's uniform distribution function.\n\n3. **Data Collection**: Points for each class are collected into lists, which are then concatenated into NumPy arrays for the final output.\n\n4. **Rounding**: The coordinates of the points are rounded to 4 decimal places to ensure compact and deterministic output.\n\nThis approach efficiently generates the required synthetic dataset, ensuring reproducibility and correctness."}
{"task_id": 581, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Elastic-Net regularized linear regression using batch gradient descent. The goal is to fit a linear model that predicts target values based on given features, while incorporating both L1 and L2 regularization to prevent overfitting.\n\n### Approach\n1. **Problem Analysis**: Elastic-Net regression combines the benefits of both L1 and L2 regularization. The loss function includes penalties for both the absolute values of the weights (L1) and the squared values of the weights (L2). The gradients for the weights and bias are computed iteratively to update the model parameters.\n\n2. **Initialization**: Start with all weights and the bias term initialized to zero.\n\n3. **Gradient Computation**: For each iteration, compute the predictions, residuals, and gradients. The gradients for the weights include terms for the residuals, L1 penalty, and L2 penalty. The gradient for the bias is computed from the sum of the residuals.\n\n4. **Parameter Update**: Update the weights and bias using the computed gradients and the learning rate. This is done iteratively for the specified number of iterations.\n\n5. **Edge Cases**: If the learning rate is zero or the number of iterations is zero, return the initial parameters immediately.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    m = X.shape[0]\n    n = X.shape[1]\n    \n    if learning_rate == 0 or iterations == 0:\n        return [0.0 for _ in range(n)], 0.0\n    \n    w = np.zeros(n, dtype=np.float64)\n    b = 0.0\n    \n    for _ in range(iterations):\n        y_hat = np.dot(X, w) + b\n        r = y - y_hat\n        \n        # Compute sign of w, with 0 treated as -1\n        sign_w = np.sign(w)\n        sign_w[sign_w == 0] = -1.0\n        \n        # Compute gradient for each weight\n        gradient_w = np.zeros(n, dtype=np.float64)\n        for j in range(n):\n            xj = X[:, j]\n            term1 = -2 * np.dot(xj, r)\n            term2 = l1_penalty * sign_w[j]\n            term3 = 2 * l2_penalty * w[j]\n            gradient_w[j] = (term1 + term2 + term3) / m\n        \n        # Compute gradient for bias\n        gradient_b = (-2 * np.sum(r)) / m\n        \n        # Update parameters\n        w -= learning_rate * gradient_w\n        b -= learning_rate * gradient_b\n    \n    # Round the results\n    weights = [round(wj, 4) for wj in w]\n    bias = round(b, 4)\n    \n    return weights, bias\n```\n\n### Explanation\n1. **Initialization**: The weights `w` are initialized to a zero vector, and the bias `b` is initialized to zero.\n\n2. **Predictions and Residuals**: For each iteration, compute the predicted values `y_hat` using the current weights and bias. Calculate the residuals `r` as the difference between the actual target values `y` and the predicted values `y_hat`.\n\n3. **Gradients Calculation**: \n   - For each weight, compute the gradient considering the residuals, L1 penalty, and L2 penalty. The sign of each weight is determined, treating zero as -1 to handle the L1 penalty correctly.\n   - For the bias, compute the gradient from the sum of the residuals.\n\n4. **Parameter Update**: Update the weights and bias using the computed gradients scaled by the learning rate.\n\n5. **Rounding**: After completing the specified number of iterations, round the weights and bias to four decimal places for the final result.\n\nThis approach ensures that the model is trained using batch gradient descent with Elastic-Net regularization, providing a robust solution that balances both L1 and L2 penalties."}
