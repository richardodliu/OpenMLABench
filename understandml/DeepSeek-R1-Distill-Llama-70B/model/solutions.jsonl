{"task_id": 2, "completion_id": 0, "solution": "import numpy as np\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n\n    def identity_link(x):\n        return x\n\n    def identity_inv_link(x):\n        return x\n\n    def identity_derivative(x):\n        return np.ones_like(x)\n\n    def log_link(x):\n        return np.log(x)\n\n    def log_inv_link(x):\n        return np.exp(x)\n\n    def log_derivative(x):\n        return 1 / x\n\n    def logit_link(x):\n        return np.log(x / (1 - x))\n\n    def logit_inv_link(x):\n        return 1 / (1 + np.exp(-x))\n\n    def logit_derivative(x):\n        return 1 / (x * (1 - x))\n    links = {'identity': {'link': identity_link, 'inv_link': identity_inv_link, 'derivative': identity_derivative}, 'log': {'link': log_link, 'inv_link': log_inv_link, 'derivative': log_derivative}, 'logit': {'link': logit_link, 'inv_link': logit_inv_link, 'derivative': logit_derivative}}\n    return links"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_svm_predict(X_train: np.ndarray, y_train: np.ndarray, alpha: np.ndarray, b: float, gamma: float, X_test: np.ndarray) -> list[int]:\n    X_test_reshaped = X_test[:, None, :]\n    X_train_reshaped = X_train[None, :, :]\n    diff = X_test_reshaped - X_train_reshaped\n    sq_dist = np.sum(diff ** 2, axis=2)\n    kernel = np.exp(-gamma * sq_dist)\n    weights = alpha * y_train\n    sum_terms = np.sum(weights * kernel, axis=1)\n    g = sum_terms + b\n    sign_g = np.sign(g)\n    sign_g = np.where(sign_g == 0, 1, sign_g).astype(int)\n    return sign_g.tolist()"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef random_binary_tensor(shape: tuple[int, ...], sparsity: float=0.5, seed: int | None=None):\n    \"\"\"Generates a NumPy tensor filled with 0.0s and 1.0s based on the given shape and sparsity.\n    \n    Args:\n        shape (tuple[int, ...]): The shape of the output tensor.\n        sparsity (float, optional): Probability of an element being 1.0. Defaults to 0.5.\n        seed (int | None, optional): Seed for the random number generator. Defaults to None.\n    \n    Returns:\n        numpy.ndarray | int: A tensor with values 0.0 and 1.0 if sparsity is valid; otherwise, returns -1.\n    \"\"\"\n    if not 0.0 <= sparsity <= 1.0:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_arr = np.random.rand(*shape)\n    threshold = 1.0 - sparsity\n    binary_tensor = (random_arr >= threshold).astype(np.float64)\n    return binary_tensor"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    epsilon = np.finfo(float).eps\n    logouri = np.log(y_pred + epsilon)\n    product = y * logouri\n    summed = np.sum(product)\n    L = -summed\n    return round(L, 4)"}
{"task_id": 86, "completion_id": 0, "solution": "from collections import Counter\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions:\n        return []\n    transposed = zip(*predictions)\n    result = []\n    for votes in transposed:\n        counts = Counter(votes)\n        max_count = max(counts.values()) if counts else 0\n        candidates = [label for (label, cnt) in counts.items() if cnt == max_count]\n        selected_label = min(candidates)\n        result.append(selected_label)\n    return result"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z). This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        result = np.logaddexp(0.0, z)\n        rounded_result = round(result, 4)\n        return float(rounded_result)\n    else:\n        z_np = np.asarray(z)\n        computed = np.logaddexp(0.0, z_np)\n        rounded = np.round(computed, 4)\n        return rounded.tolist()"}
{"task_id": 113, "completion_id": 0, "solution": "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        output_str = ''.join(output_chars)\n        print(f'input:  {example}')\n        print(f'output: {output_str}')\n        predictions.append(output_str)\n    return predictions"}
{"task_id": 155, "completion_id": 0, "solution": "import numpy as np\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    state = np.asarray(state)\n    h1_linear = np.dot(weights['W1'], state) + weights['b1']\n    h1_activation = np.maximum(h1_linear, 0)\n    h2_linear = np.dot(weights['W2'], h1_activation) + weights['b2']\n    h2_activation = np.maximum(h2_linear, 0)\n    action_linear = np.dot(weights['W3'], h2_activation) + weights['b3']\n    action = np.tanh(action_linear)\n    action_rounded = np.round(action, 4)\n    return action_rounded.tolist()"}
{"task_id": 218, "completion_id": 0, "solution": "def blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n\n    def calculate_score(hand):\n        sum_cards = sum(hand)\n        num_aces = hand.count(1)\n        if num_aces >= 1 and sum_cards + 10 <= 21:\n            return sum_cards + 10\n        else:\n            return sum_cards\n    player_score = calculate_score(player)\n    dealer_score = calculate_score(dealer)\n\n    def is_natural(hand):\n        return len(hand) == 2 and 1 in hand and (10 in hand)\n    player_natural = is_natural(player)\n    dealer_natural = is_natural(dealer)\n    if player_score > 21:\n        return -1.0\n    elif dealer_score > 21:\n        return 1.0\n    elif player_score > dealer_score:\n        if player_natural:\n            return 1.5\n        else:\n            return 1.0\n    elif dealer_score > player_score:\n        return -1.0\n    else:\n        return 0.0"}
{"task_id": 221, "completion_id": 0, "solution": "import numpy as np\ndef squared_log_error(actual, predicted):\n    \"\"\"Helper function to compute element-wise squared log errors.\"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    log_actual = np.log(1 + actual)\n    log_predicted = np.log(1 + predicted)\n    errors = log_actual - log_predicted\n    squared_errors = errors ** 2\n    return squared_errors\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    if actual.shape != predicted.shape:\n        return -1\n    if np.any(actual < 0) or np.any(predicted < 0):\n        return -1\n    if actual.size == 0:\n        return -1\n    squared_errors = squared_log_error(actual, predicted)\n    msle = np.mean(squared_errors)\n    msle_rounded = round(msle, 4)\n    return msle_rounded"}
{"task_id": 222, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    if isinstance(z, (int, float, np.number)):\n        if z >= 0:\n            sig = 1.0 / (1.0 + math.exp(-z))\n        else:\n            sig = math.exp(z) / (1.0 + math.exp(z))\n        return round(sig, 4)\n    else:\n        z_array = np.asarray(z, dtype=np.float64)\n        mask = z_array >= 0\n        sig = np.zeros_like(z_array)\n        sig[mask] = 1.0 / (1.0 + np.exp(-z_array[mask]))\n        sig[~mask] = np.exp(z_array[~mask]) / (1.0 + np.exp(z_array[~mask]))\n        rounded = np.round(sig, 4)\n        return rounded.tolist()"}
{"task_id": 224, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(element, a) for element in z]\n    elif z >= 0:\n        return z\n    else:\n        return a * z"}
{"task_id": 243, "completion_id": 0, "solution": "import numpy as np\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    sum_state = sum(state)\n    a = max(0, sum_state)\n    action_probs = [1 / 3, 1 / 3, 1 / 3]\n    state_value = 64 * a\n    action_probs = [round(prob, 4) for prob in action_probs]\n    state_value = round(state_value, 4)\n    return (action_probs, state_value)"}
{"task_id": 253, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w = np.asarray(w)\n    L1 = np.sum(np.abs(w))\n    L2 = 0.5 * np.sum(w ** 2)\n    penalty = alpha * (l1_ratio * L1 + (1 - l1_ratio) * L2)\n    if not gradient:\n        return round(penalty, 4)\n    else:\n        grad = alpha * (l1_ratio * np.sign(w) + (1 - l1_ratio) * w)\n        grad = np.round(grad, 4)\n        return grad.tolist()"}
{"task_id": 256, "completion_id": 0, "solution": "import numpy as np\nfrom copy import deepcopy\ndef grad_check_RNN(model, loss_func, param_name: str, n_t: int, X, epsilon: float=1e-07):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\"\"\"\n    if param_name in ['X', 'y']:\n        return None\n    if param_name in ['Ba', 'Bx']:\n        param_key = param_name.lower()\n    else:\n        param_key = param_name\n    theta = model.parameters[param_key]\n    grads = np.zeros_like(theta, dtype=np.float64)\n    for idx in np.ndenumerate(theta):\n        original_value = theta[idx]\n        model_plus = deepcopy(model)\n        model_plus.flush_gradients()\n        plus_params = model_plus.parameters[param_key]\n        plus_params[idx] += epsilon\n        predictions_plus = []\n        for t in range(n_t):\n            x_t = X[:, :, t]\n            pred = model_plus.forward(x_t)\n            predictions_plus.append(pred)\n        loss_plus = loss_func(predictions_plus)\n        model_minus = deepcopy(model)\n        model_minus.flush_gradients()\n        minus_params = model_minus.parameters[param_key]\n        minus_params[idx] -= epsilon\n        predictions_minus = []\n        for t in range(n_t):\n            x_t = X[:, :, t]\n            pred = model_minus.forward(x_t)\n            predictions_minus.append(pred)\n        loss_minus = loss_func(predictions_minus)\n        grad = (loss_plus - loss_minus) / (2 * epsilon)\n        grads[idx] = grad\n    return grads.T"}
{"task_id": 261, "completion_id": 0, "solution": "import numpy as np\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    if len(shape) == 2:\n        fan_in = shape[1]\n        fan_out = shape[0]\n    else:\n        in_channels = shape[1]\n        out_channels = shape[0]\n        receptive_field = 1\n        for d in shape[2:]:\n            receptive_field *= d\n        fan_in = in_channels * receptive_field\n        fan_out = out_channels * receptive_field\n    s = np.sqrt(2 / (fan_in + fan_out))\n    rng = np.random.default_rng()\n    return rng.normal(0, s, shape)"}
{"task_id": 266, "completion_id": 0, "solution": "from typing import Any, List, Tuple\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples `(u, v)` describing undirected edges that connect\n        vertices `u` and `v`.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list `G` where `G[i]` contains all vertices adjacent to `V[i]`.\n        \u2022 The outer list has the same length and order as `V`.\n        \u2022 Inner lists contain no duplicates and are ordered by their\n          appearance order in `V`.\n    \"\"\"\n    index_map = {v: i for (i, v) in enumerate(V)}\n    adj_sets = [set() for _ in range(len(V))]\n    for (u, v) in E:\n        adj_sets[index_map[u]].add(v)\n        adj_sets[index_map[v]].add(u)\n    G = []\n    for i in range(len(V)):\n        neighbors = sorted(adj_sets[i], key=lambda x: index_map[x])\n        G.append(neighbors)\n    return G"}
{"task_id": 286, "completion_id": 0, "solution": "import random\nfrom typing import Callable, Tuple, List\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    initializer = globals().get(name)\n    if initializer is None or not callable(initializer):\n        raise ValueError('Invalid initialization function.')\n    return initializer"}
{"task_id": 290, "completion_id": 0, "solution": "import numpy as np\nfrom dataclasses import dataclass\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object\n    right: object\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-08)\n    elif isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-08):\n            return False\n        return compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    else:\n        return False"}
{"task_id": 292, "completion_id": 0, "solution": "import numpy as np\ndef single_point_crossover(parent1: list[list[int | float]], parent2: list[list[int | float]], cutoff: int) -> tuple[list[list[float]], list[list[float]]] | int:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    p1 = np.array(parent1)\n    p2 = np.array(parent2)\n    if p1.shape != p2.shape:\n        return -1\n    num_cols = p1.shape[1]\n    child1 = np.concatenate((p1[:, :cutoff], p2[:, cutoff:]), axis=1)\n    child2 = np.concatenate((p2[:, :cutoff], p1[:, cutoff:]), axis=1)\n    child1_list = child1.tolist()\n    child2_list = child2.tolist()\n    return (child1_list, child2_list)"}
{"task_id": 296, "completion_id": 0, "solution": "import numpy as np\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    x = np.asarray(x)\n    mask = (x == 0) | (x == 1)\n    return mask.all()"}
{"task_id": 303, "completion_id": 0, "solution": "def err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str='') -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\"\"\"\n    prev_ix = max(ix - 1, 0)\n    prev_tuple = params[prev_ix]\n    curr_tuple = params[ix]\n    (prev_mine, prev_label) = prev_tuple\n    (curr_mine, curr_label) = curr_tuple\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    top_line = '-' * 25 + ' DEBUG ' + '-' * 25\n    end_line = '-' * 23 + ' END DEBUG ' + '-' * 23\n    lines = [top_line, f'Mine (prev) [{prev_label}]:', prev_mine, f'Theirs (prev) [{prev_label}]:', prev_gold, f'Mine [{curr_label}]:', curr_mine, f'Theirs [{curr_label}]:', curr_gold + warn_str, end_line]\n    return '\\n'.join(lines)"}
{"task_id": 312, "completion_id": 0, "solution": "import numpy as np\ndef blackman_harris(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n    if symmetric:\n        n_count = window_len\n        N = n_count - 1\n    else:\n        n_count = window_len + 1\n        N = n_count - 1\n    window = []\n    for n in range(n_count):\n        term1 = a1 * np.cos(2 * np.pi * n / N)\n        term2 = a2 * np.cos(4 * np.pi * n / N)\n        term3 = a3 * np.cos(6 * np.pi * n / N)\n        bh = a0 - term1 + term2 - term3\n        window.append(bh)\n    if not symmetric:\n        window = window[:window_len]\n    window = [round(value, 4) for value in window]\n    return window"}
{"task_id": 329, "completion_id": 0, "solution": "import numpy as np\ndef dft_bins(N: int, fs: int=44000, positive_only: bool=True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz. Default is 44 kHz.\n    positive_only : bool, optional\n        When True, return only the non-negative frequency bins; when False, return all N bins. Default is True.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals. An empty list is returned when N or fs is non-positive.\n    \"\"\"\n    if N <= 0 or fs <= 0:\n        return []\n    freqs = np.fft.fftfreq(N, d=1 / fs)\n    if positive_only:\n        freqs = freqs[:N // 2 + 1]\n    rounded = np.round(freqs, 4)\n    return rounded.tolist()"}
{"task_id": 331, "completion_id": 0, "solution": "import numpy as np\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    predictions = np.asarray(predictions)\n    oob_mask = np.asarray(oob_mask).astype(bool)\n    n_samples = len(y_true)\n    squared_errors = []\n    for i in range(n_samples):\n        mask = oob_mask[i]\n        if np.any(mask):\n            preds = predictions[i, mask]\n            avg_pred = np.mean(preds)\n            se = (avg_pred - y_true[i]) ** 2\n            squared_errors.append(se)\n    if not squared_errors:\n        return -1\n    else:\n        mse = np.mean(squared_errors)\n        return round(mse, 4)"}
{"task_id": 332, "completion_id": 0, "solution": "from collections import Counter\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of transactions an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node contains two keys:\n        \"support\" and \"children\" (the latter mapping item \u2192 child-node).\n    \"\"\"\n    freq = Counter()\n    for transaction in transactions:\n        unique_items = set(transaction)\n        for item in unique_items:\n            freq[item] += 1\n    frequent_items = {item: count for (item, count) in freq.items() if count >= min_support}\n    if not frequent_items:\n        return {'support': 0, 'children': {}}\n    global_order = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n    order_map = {item: idx for (idx, item) in enumerate(global_order)}\n    root = {'support': 0, 'children': {}}\n    for transaction in transactions:\n        unique_filtered = [item for item in set(transaction) if item in frequent_items]\n        if not unique_filtered:\n            continue\n        ordered_transaction = sorted(unique_filtered, key=lambda x: order_map[x])\n        current_node = root\n        for item in ordered_transaction:\n            if item in current_node['children']:\n                current_node = current_node['children'][item]\n                current_node['support'] += 1\n            else:\n                new_node = {'support': 1, 'children': {}}\n                current_node['children'][item] = new_node\n                current_node = new_node\n    return root"}
{"task_id": 336, "completion_id": 0, "solution": "import numpy as np\ndef linear_kernel(X: list[list[int | float]] | np.ndarray, Y: list[list[int | float]] | np.ndarray | None=None, c0: int | float=0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\"\"\"\n    X_np = np.asarray(X)\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.asarray(Y)\n    if X_np.shape[0] == 0:\n        return []\n    if X_np.shape[1] != Y_np.shape[1]:\n        return -1\n    K = X_np @ Y_np.T\n    K += c0\n    K_rounded = np.round(K, 4)\n    K_list = K_rounded.tolist()\n    return K_list"}
{"task_id": 340, "completion_id": 0, "solution": "import numpy as np\ndef adam_update(param: np.ndarray, grad: np.ndarray, t: int, m: np.ndarray, v: np.ndarray, lr: float=0.001, beta1: float=0.9, beta2: float=0.999, eps: float=1e-08) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_parameter, new_moment, new_variance) **as\n        python lists**, each rounded to 8 decimal places.\n    \"\"\"\n    m_new = beta1 * m + (1 - beta1) * grad\n    v_new = beta2 * v + (1 - beta2) * grad ** 2\n    is_zero_grad = np.all(grad == 0)\n    if not is_zero_grad:\n        denominator_beta1 = 1 - beta1 ** t\n        denominator_beta2 = 1 - beta2 ** t\n        m_hat = m_new / denominator_beta1\n        v_hat = v_new / denominator_beta2\n        sqrt_v_hat = np.sqrt(v_hat)\n        denominator = sqrt_v_hat + eps\n        update = lr * m_hat / denominator\n        updated_param = param - update\n    else:\n        updated_param = param.copy()\n    updated_param_list = [round(x, 8) for x in updated_param.tolist()]\n    m_new_list = [round(x, 8) for x in m_new.tolist()]\n    v_new_list = [round(x, 8) for x in v_new.tolist()]\n    return (updated_param_list, m_new_list, v_new_list)"}
{"task_id": 343, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_update(theta_samples: np.ndarray, rewards: list[float] | np.ndarray, retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    N = theta_samples.shape[0]\n    K = int(retain_prcnt * N)\n    sorted_indices = np.argsort(-rewards)\n    top_indices = sorted_indices[:K]\n    retained = theta_samples[top_indices]\n    mean = np.mean(retained, axis=0)\n    var = np.var(retained, axis=0)\n    rounded_mean = [round(x, 4) for x in mean.tolist()]\n    rounded_var = [round(x, 4) for x in var.tolist()]\n    return (rounded_mean, rounded_var)"}
{"task_id": 353, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    counts = Counter(labels)\n    total = sum(counts.values())\n    if total == 0:\n        return 0.0\n    ent = 0.0\n    for count in counts.values():\n        p = count / total\n        ent += p * math.log2(p)\n    ent *= -1\n    return round(ent, 5)"}
{"task_id": 356, "completion_id": 0, "solution": "def leaf_predict(leaf: 'Leaf', classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        probs = leaf.value\n        max_prob = max(probs)\n        return probs.index(max_prob)\n    else:\n        return leaf.value"}
{"task_id": 362, "completion_id": 0, "solution": "import numpy as np\ndef row_stochastic_matrix(data: list[list[int | float]] | 'np.ndarray') -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    data_array = np.array(data, dtype=float)\n    row_sums = np.sum(data_array, axis=1)\n    if np.any(row_sums == 0):\n        return -1\n    normalized = data_array / row_sums[:, None]\n    rounded = np.round(normalized, 4)\n    return rounded.tolist()"}
{"task_id": 363, "completion_id": 0, "solution": "from typing import Any, Dict, Iterable, Tuple as PyTuple\nclass Space:\n    pass\nclass Box(Space):\n\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\nclass Tuple(Space):\n\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    if tuple_action:\n        action_space = env.action_space\n        if isinstance(action_space, Tuple):\n            sub_spaces = action_space.spaces\n        else:\n            sub_spaces = action_space.spaces.values()\n        cont_action = all((isinstance(s, Box) for s in sub_spaces))\n    else:\n        cont_action = isinstance(env.action_space, Box)\n    if tuple_obs:\n        obs_space = env.observation_space\n        if isinstance(obs_space, Tuple):\n            sub_spaces = obs_space.spaces\n        else:\n            sub_spaces = obs_space.spaces.values()\n        cont_obs = all((isinstance(s, Box) for s in sub_spaces))\n    else:\n        cont_obs = isinstance(env.observation_space, Box)\n    return (cont_action, cont_obs)"}
{"task_id": 369, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        shape = updates[0].shape if updates else (0,)\n        running_pred = np.zeros(shape, dtype=np.float64)\n    else:\n        sum_updates = np.sum(updates, axis=0)\n        running_pred = -learning_rate * sum_updates\n    if regression:\n        return np.round(running_pred, 4)\n    else:\n        max_logits = np.max(running_pred, axis=1, keepdims=True)\n        exp_logits = np.exp(running_pred - max_logits)\n        sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n        probabilities = exp_logits / sum_exp\n        predicted_classes = np.argmax(probabilities, axis=1)\n        return predicted_classes"}
{"task_id": 373, "completion_id": 0, "solution": "import numpy as np\ndef gini(y: list[int] | np.ndarray) -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    if len(y) == 0:\n        return 0.0\n    y_array = np.asarray(y)\n    (unique_classes, counts) = np.unique(y_array, return_counts=True)\n    N = y_array.size\n    sum_squares = np.sum((counts / N) ** 2)\n    gini_value = 1 - sum_squares\n    return round(gini_value, 4)"}
{"task_id": 374, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float=0.01, n_iterations: int=10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\n\n    Your task is to implement this function following the specification\n    provided in the problem statement.\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        return -1\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    m = X.shape[0]\n    n = X.shape[1]\n    theta = np.zeros(n)\n    b = 0.0\n    for _ in range(n_iterations):\n        y_hat = np.dot(X, theta) + b\n        error = y_hat - y\n        dw = 2.0 / m * np.dot(X.T, error)\n        db = 2.0 / m * np.sum(error)\n        theta = theta - learning_rate * dw\n        b = b - learning_rate * db\n    theta = np.round(theta, 4)\n    b = round(b, 4)\n    return (theta.tolist(), b)"}
{"task_id": 376, "completion_id": 0, "solution": "import numpy as np\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    n = len(X)\n    for row in X:\n        if len(row) != n:\n            return False\n    arr = np.array(X)\n    return np.allclose(arr, arr.T, atol=1e-08)"}
{"task_id": 377, "completion_id": 0, "solution": "import numpy as np\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"Compute the negative gradient of the logistic loss for binary classification.\n    \n    Args:\n    y (list): Binary class labels (0 or 1).\n    f (list): Current prediction scores.\n    \n    Returns:\n    list: A list of negative gradients rounded to four decimal places.\n          Returns -1 if any label in y is not 0 or 1.\n    \"\"\"\n    if not all((yi in (0, 1) for yi in y)):\n        return -1\n    y_np = np.array(y, dtype=int)\n    f_np = np.array(f, dtype=float)\n    y_prime = 2 * y_np - 1\n    denominator = 1 + np.exp(y_prime * f_np)\n    g = y_prime / denominator\n    g_rounded = np.round(g, 4)\n    return g_rounded.tolist()"}
{"task_id": 394, "completion_id": 0, "solution": "import numpy as np\ndef rmsprop_update(w: np.ndarray, grad: np.ndarray, Eg: np.ndarray | None=None, learning_rate: float=0.01, rho: float=0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n    Eg_new = rho * Eg + (1 - rho) * grad ** 2\n    denominator = np.sqrt(Eg_new + 1e-08)\n    w_new = w - learning_rate * grad / denominator\n    w_rounded = np.round(w_new, 4)\n    Eg_rounded = np.round(Eg_new, 4)\n    w_list = w_rounded.tolist()\n    Eg_list = Eg_rounded.tolist()\n    return (w_list, Eg_list)"}
{"task_id": 398, "completion_id": 0, "solution": "import numpy as np\ndef elu(x, alpha: float=0.1, derivative: bool=False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    x_arr = np.asarray(x)\n    mask = x_arr >= 0\n    if not derivative:\n        res = np.where(mask, x_arr, alpha * (np.exp(x_arr) - 1))\n    else:\n        res = np.where(mask, 1.0, alpha * np.exp(x_arr))\n    rounded = np.round(res, 4)\n    return rounded.tolist()"}
{"task_id": 419, "completion_id": 0, "solution": "import numpy as np\ndef bayesian_linear_regression_pred(X: np.ndarray, y: np.ndarray, X_new: np.ndarray, mu: float | np.ndarray=0.0, sigma: float=1.0, V: float | np.ndarray | None=None, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    The function must return the MAP predictive means for every row in\n    `X_new`.  See the task description for full details.\n    \"\"\"\n    if fit_intercept:\n        X_proc = np.hstack((np.ones((X.shape[0], 1)), X))\n        X_new_proc = np.hstack((np.ones((X_new.shape[0], 1)), X_new))\n    else:\n        X_proc = X\n        X_new_proc = X_new\n    n_features = X_proc.shape[1]\n    if V is None:\n        V = np.eye(n_features)\n    elif isinstance(V, (float, int)):\n        V = V * np.eye(n_features)\n    elif isinstance(V, np.ndarray):\n        if V.ndim == 1:\n            V = np.diag(V)\n        elif V.shape != (n_features, n_features):\n            raise ValueError('V must be a square matrix of size n_features x n_features')\n    else:\n        raise ValueError('V must be None, float, or ndarray')\n    V_inv = np.linalg.inv(V)\n    if isinstance(mu, (float, int)):\n        mu = np.full(n_features, mu)\n    elif isinstance(mu, np.ndarray):\n        if mu.shape[0] != n_features:\n            raise ValueError('mu must be a scalar or vector of length n_features')\n    else:\n        raise ValueError('mu must be a scalar or ndarray')\n    term1 = V_inv @ mu\n    term2 = X_proc.T @ y\n    numerator = term1 + term2\n    sum_matrix = V_inv + X_proc.T @ X_proc\n    Sigma_p = np.linalg.inv(sum_matrix)\n    mu_p = Sigma_p @ numerator\n    predictions = X_new_proc @ mu_p\n    rounded_predictions = [round(pred, 4) for pred in predictions]\n    return rounded_predictions"}
{"task_id": 423, "completion_id": 0, "solution": "import numpy as np\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Calculates the Root Mean Squared Logarithmic Error (RMSLE) between two lists of numbers.\n    \n    Args:\n        actual (list[float]): The ground truth values.\n        predicted (list[float]): The predicted values.\n    \n    Returns:\n        float: The RMSLE value rounded to four decimal places, or -1 if the inputs are invalid.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1\n    n = len(actual)\n    if n == 0:\n        return -1\n    for (a, p) in zip(actual, predicted):\n        if a < 0 or p < 0:\n            return -1\n    actual_np = np.array(actual, dtype=float)\n    predicted_np = np.array(predicted, dtype=float)\n    log_actual = np.log(1 + actual_np)\n    log_predicted = np.log(1 + predicted_np)\n    diffs = log_predicted - log_actual\n    squared_diffs = diffs ** 2\n    mean_sq = np.mean(squared_diffs)\n    rmsle = np.sqrt(mean_sq)\n    return round(rmsle, 4)"}
{"task_id": 428, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef hann(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    computed_length = window_len if symmetric else window_len + 1\n    n = np.arange(computed_length)\n    window = 0.5 - 0.5 * np.cos(2 * np.pi * n / (computed_length - 1))\n    if not symmetric:\n        window = window[:-1]\n    window = np.round(window, 4)\n    return window.tolist()"}
{"task_id": 433, "completion_id": 0, "solution": "def glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    if len(shape) == 4:\n        in_channels = shape[1]\n        out_channels = shape[0]\n        kernel_h = shape[2]\n        kernel_w = shape[3]\n        receptive_field = kernel_h * kernel_w\n        fan_in = in_channels * receptive_field\n        fan_out = out_channels * receptive_field\n    else:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    return (float(fan_in), float(fan_out))"}
{"task_id": 435, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_regressor(X: list[list[float]], y: list[float], n_estimators: int=10, learning_rate: float=0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    X_np = np.array(X, dtype=np.float64)\n    y_np = np.array(y, dtype=np.float64)\n    if n_estimators <= 0 or learning_rate == 0:\n        y_mean = y_np.mean()\n        return [round(y_mean, 4)] * len(y_np)\n    m = X_np.shape[0]\n    y_hat = np.full(m, y_np.mean())\n    for _ in range(n_estimators):\n        residuals = y_np - y_hat\n        X_extended = np.hstack((np.ones((m, 1)), X_np))\n        (coefficients, _, _, _) = np.linalg.lstsq(X_extended, residuals, rcond=None)\n        h = np.dot(X_extended, coefficients)\n        y_hat += learning_rate * h\n    rounded = [round(float(val), 4) for val in y_hat]\n    return rounded"}
{"task_id": 437, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x, order: int=0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in [0, 1, 2]:\n        return -1\n    is_scalar = isinstance(x, (int, float))\n    arr = np.asarray(x)\n    sigma = 1 / (1 + np.exp(-arr))\n    if order == 0:\n        res = sigma\n    elif order == 1:\n        res = sigma * (1 - sigma)\n    else:\n        res = sigma * (1 - sigma) * (1 - 2 * sigma)\n    rounded = np.round(res, 4)\n    if is_scalar:\n        return float(rounded.item())\n    else:\n        return rounded.tolist()"}
{"task_id": 438, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None=None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    y = np.asarray(y)\n    n_samples = len(y)\n    if n_samples == 0:\n        return []\n    y_max = y.max()\n    if num_classes is None:\n        num_classes = y_max + 1\n    elif num_classes < y_max + 1:\n        return -1\n    result = np.zeros((n_samples, num_classes), dtype=int)\n    for i in range(n_samples):\n        result[i, y[i]] = 1\n    return result.tolist()"}
{"task_id": 440, "completion_id": 0, "solution": "import numpy as np\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    pred_array = np.array(predictions)\n    avg_probs = np.mean(pred_array, axis=0)\n    rounded_avg = np.round(avg_probs, 4)\n    class_labels = np.argmax(rounded_avg, axis=1)\n    averaged_probs_list = rounded_avg.tolist()\n    class_labels_list = class_labels.tolist()\n    return (averaged_probs_list, class_labels_list)"}
{"task_id": 453, "completion_id": 0, "solution": "import numpy as np\ndef update_q_values(Q: 'np.ndarray', Q_next: 'np.ndarray', actions: list[int], rewards: list[float], dones: list[bool], gamma: float) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    if gamma < 0 or gamma > 1:\n        return -1\n    batch_size = Q.shape[0]\n    if len(actions) != batch_size or len(rewards) != batch_size or len(dones) != batch_size:\n        return -1\n    if Q.shape != Q_next.shape:\n        return -1\n    y = Q.copy()\n    for i in range(batch_size):\n        action = actions[i]\n        reward = rewards[i]\n        done = dones[i]\n        if done:\n            target = reward\n        else:\n            max_next_q = np.max(Q_next[i])\n            target = reward + gamma * max_next_q\n        y[i, action] = target\n    y = np.round(y, 4)\n    return y.tolist()"}
{"task_id": 458, "completion_id": 0, "solution": "def to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None=None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    if not labels:\n        return []\n    for label in labels:\n        if label < 0:\n            return -1\n    max_label = max(labels)\n    if num_classes is not None:\n        if num_classes < max_label + 1:\n            return -1\n        for label in labels:\n            if label >= num_classes:\n                return -1\n    else:\n        num_classes = max_label + 1\n    result = []\n    for label in labels:\n        row = [0] * num_classes\n        row[label] = 1\n        result.append(row)\n    return result"}
{"task_id": 471, "completion_id": 0, "solution": "import numpy as np\ndef expected_sarsa_update(q_table: list[list[float]], state: int, action: int, reward: float, next_state: int | None, epsilon: float, lr: float, gamma: float) -> list[list[float]]:\n    q = [row.copy() for row in q_table]\n    num_actions = len(q[0]) if q else 0\n    if next_state is not None:\n        q_next = q[next_state]\n        max_val = max(q_next)\n        greedy_action = next((a for (a, val) in enumerate(q_next) if val == max_val), None)\n        expected_value = 0.0\n        for a in range(num_actions):\n            if a == greedy_action:\n                prob = 1.0 - epsilon + epsilon / num_actions\n            else:\n                prob = epsilon / num_actions\n            expected_value += prob * q_next[a]\n        target = reward + gamma * expected_value\n    else:\n        target = reward\n    current_q = q[state][action]\n    new_q = current_q + lr * (target - current_q)\n    q[state][action] = new_q\n    for s in range(len(q)):\n        for a in range(len(q[s])):\n            q[s][a] = round(q[s][a], 4)\n    return q"}
{"task_id": 474, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef generate_data(m: int, n_train: int, n_val: int, interval: float, seed: int | None=None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    X_train_list = []\n    Y_train_list = []\n    X_val_list = []\n    Y_val_list = []\n    for i in range(m):\n        x_low = (i // 2 + 0.1) * 10\n        x_high = (i // 2 + 0.9) * 10\n        y_low = (i % 2 * interval + 0.1) * 10\n        y_high = (i % 2 * interval + 0.9) * 10\n        label = (i - 0.5) * 2\n        x_train = np.random.uniform(x_low, x_high, n_train)\n        y_train = np.random.uniform(y_low, y_high, n_train)\n        X_train_samples = np.column_stack((x_train, y_train))\n        Y_train_samples = np.full(n_train, label)\n        x_val = np.random.uniform(x_low, x_high, n_val)\n        y_val = np.random.uniform(y_low, y_high, n_val)\n        X_val_samples = np.column_stack((x_val, y_val))\n        Y_val_samples = np.full(n_val, label)\n        X_train_list.append(X_train_samples)\n        Y_train_list.append(Y_train_samples)\n        X_val_list.append(X_val_samples)\n        Y_val_list.append(Y_val_samples)\n    X_train = np.concatenate(X_train_list, axis=0)\n    Y_train = np.concatenate(Y_train_list, axis=0)\n    X_val = np.concatenate(X_val_list, axis=0)\n    Y_val = np.concatenate(Y_val_list, axis=0)\n    X_train = np.round(X_train, 4)\n    X_val = np.round(X_val, 4)\n    return (X_train, X_val, Y_train, Y_val)"}
{"task_id": 475, "completion_id": 0, "solution": "import numpy as np\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-08):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    if m_prev is None:\n        m_prev = np.zeros_like(grad)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad)\n    m_t = b1 * m_prev + (1 - b1) * grad\n    v_t = b2 * v_prev + (1 - b2) * grad ** 2\n    denom_m = 1.0 - b1 ** t\n    denom_v = 1.0 - b2 ** t\n    m_hat = m_t / denom_m\n    v_hat = v_t / denom_v\n    sqrt_v_hat = np.sqrt(v_hat)\n    denominator = sqrt_v_hat + eps\n    w_new = w - learning_rate * (m_hat / denominator)\n    return (w_new, m_t, v_t)"}
{"task_id": 479, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Optional\ndef epsilon_greedy_bandit(rewards: List[List[float]], epsilon: float=0.05, ev_prior: float=0.5, seed: Optional[int]=None) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where ``rewards[t][i]`` is the reward for arm *i* at time-step\n        *t*.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    if not rewards:\n        return []\n    N = len(rewards[0])\n    estimates = [ev_prior] * N\n    counts = [0] * N\n    for t in range(len(rewards)):\n        if np.random.rand() < epsilon:\n            arm = np.random.randint(N)\n        else:\n            max_val = max(estimates)\n            candidates = [i for (i, val) in enumerate(estimates) if val == max_val]\n            arm = min(candidates)\n        r = rewards[t][arm]\n        counts[arm] += 1\n        estimates[arm] += (r - estimates[arm]) / counts[arm]\n    return [round(v, 4) for v in estimates]"}
{"task_id": 482, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    y = np.asarray(y)\n    if y.size == 0:\n        if num_classes is None:\n            return np.zeros((0, 0), dtype=int)\n        else:\n            return np.zeros((0, num_classes), dtype=int)\n    if num_classes is None:\n        if (y < 0).any():\n            raise ValueError('Negative labels found when num_classes is None.')\n        num_classes = max(y) + 1\n    elif (y < 0).any() or (y >= num_classes).any():\n        raise ValueError('Invalid label')\n    result = np.eye(num_classes, dtype=int)[y]\n    return result"}
{"task_id": 490, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    if not x:\n        return []\n    x_np = np.asarray(x)\n    matrix = np.diag(x_np)\n    return matrix.tolist()"}
{"task_id": 493, "completion_id": 0, "solution": "import numpy as np\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    y_array = np.asarray(y)\n    n = y_array.size\n    if n == 0:\n        return -1\n    mean_y = y_array.mean()\n    squared_diffs = (y_array - mean_y) ** 2\n    mse_value = squared_diffs.mean()\n    return round(mse_value, 4)"}
{"task_id": 496, "completion_id": 0, "solution": "import numpy as np\ndef apply_affine(x, slope: float=1.0, intercept: float=0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    x = np.asarray(x)\n    if x.ndim == 0:\n        x = x.reshape(1)\n    y = slope * x + intercept\n    grad = np.full_like(x, slope)\n    grad2 = np.zeros_like(x)\n    y_rounded = np.round(y, 4)\n    grad_rounded = np.round(grad, 4)\n    grad2_rounded = np.round(grad2, 4)\n    y_list = y_rounded.tolist()\n    grad_list = grad_rounded.tolist()\n    grad2_list = grad2_rounded.tolist()\n    return (y_list, grad_list, grad2_list)"}
{"task_id": 509, "completion_id": 0, "solution": "def chebyshev(x: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray') -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    max_diff = 0.0\n    for (a, b) in zip(x, y):\n        diff = abs(a - b)\n        if diff > max_diff:\n            max_diff = diff\n    return round(max_diff, 4)"}
{"task_id": 510, "completion_id": 0, "solution": "import numpy as np\ndef vae_loss(y: np.ndarray, y_pred: np.ndarray, t_mean: np.ndarray, t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    eps = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, eps, 1 - eps)\n    recon_loss = y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped)\n    recon_loss = -recon_loss\n    total_recon = np.sum(recon_loss)\n    mu_squared = t_mean ** 2\n    var = np.exp(t_log_var)\n    kl_terms = 1 + t_log_var - mu_squared - var\n    total_kl = np.sum(kl_terms) * -0.5\n    batch_size = y.shape[0]\n    total_loss = (total_recon + total_kl) / batch_size\n    return round(total_loss, 6)"}
{"task_id": 513, "completion_id": 0, "solution": "import numpy as np\ndef fm_predict(X: list[list[int | float]], w0: float, w: list[float], V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    X_np = np.array(X, dtype=float)\n    w_np = np.array(w, dtype=float)\n    V_np = np.array(V, dtype=float)\n    linear_part = w0 + np.dot(X_np, w_np)\n    s = np.dot(X_np, V_np)\n    s_squared = s ** 2\n    X_squared = X_np ** 2\n    V_squared = V_np ** 2\n    sum_v2x2 = np.dot(X_squared, V_squared)\n    term = s_squared - sum_v2x2\n    interaction = 0.5 * np.sum(term, axis=1)\n    predictions = linear_part + interaction\n    rounded_predictions = np.round(predictions, 4)\n    return rounded_predictions.tolist()"}
{"task_id": 517, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    if isinstance(z, (int, float, np.number)):\n        if z >= 0:\n            s = 1.0 / (1.0 + np.exp(-z))\n        else:\n            s = np.exp(z) / (1.0 + np.exp(z))\n        s = round(s, 4)\n        return float(s)\n    else:\n        arr = np.asarray(z)\n        mask = arr >= 0\n        sig = np.zeros_like(arr)\n        sig[mask] = 1.0 / (1.0 + np.exp(-arr[mask]))\n        sig[~mask] = np.exp(arr[~mask]) / (1.0 + np.exp(arr[~mask]))\n        rounded = np.round(sig, 4)\n        return rounded.tolist()"}
{"task_id": 518, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Callable\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n\n    def wrapper(actual, predicted):\n        processed_actual = actual\n        if processed_actual.ndim == 2 and processed_actual.shape[1] > 1:\n            processed_actual = processed_actual.argmax(axis=1)\n        processed_predicted = predicted\n        if processed_predicted.ndim == 2 and processed_predicted.shape[1] > 1:\n            processed_predicted = processed_predicted.argmax(axis=1)\n        return function(processed_actual, processed_predicted)\n    return wrapper"}
{"task_id": 520, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(z, list):\n        return [sigmoid_grad(x) for x in z]\n    elif isinstance(z, np.ndarray):\n        grad = z * (1 - z)\n        rounded = np.round(grad, 4)\n        return rounded\n    else:\n        grad = z * (1 - z)\n        rounded = np.round(grad, 4)\n        return float(rounded)"}
{"task_id": 528, "completion_id": 0, "solution": "import numpy as np\ndef decision_boundary_grid(X: list[list[int | float]], W: list[int | float], b: float, grid_n: int=100) -> list[list[int]]:\n    x1 = [point[0] for point in X]\n    x2 = [point[1] for point in X]\n    (min_x1, max_x1) = (min(x1), max(x1))\n    (min_x2, max_x2) = (min(x2), max(x2))\n    x1_plot = np.linspace(min_x1, max_x1, grid_n)\n    x2_plot = np.linspace(min_x2, max_x2, grid_n)\n    (x1_mesh, x2_mesh) = np.meshgrid(x1_plot, x2_plot)\n    z = W[0] * x1_mesh + W[1] * x2_mesh + b\n    labels = np.where(z >= 0, 1, -1)\n    labels_list = labels.tolist()\n    return labels_list"}
{"task_id": 539, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Computes the binary cross-entropy cost.\"\"\"\n    m = AL.size\n    AL_clipped = np.clip(AL, 1e-15, 1 - 1e-15)\n    sum_terms = Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped)\n    sum_total = np.sum(sum_terms)\n    J = -sum_total / m\n    return round(float(J), 4)"}
{"task_id": 555, "completion_id": 0, "solution": "import numpy as np\nfrom collections import deque\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample. Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    n_samples = data.shape[0]\n    neighborhoods = [[] for _ in range(n_samples)]\n    for i in range(n_samples):\n        for j in range(n_samples):\n            if i != j:\n                dist = np.linalg.norm(data[i] - data[j])\n                if dist <= eps:\n                    neighborhoods[i].append(j)\n    core = [len(neighbors) >= min_samples for neighbors in neighborhoods]\n    labels = [-1] * n_samples\n    cluster_id = 0\n    for i in range(n_samples):\n        if labels[i] == -1 and core[i]:\n            cluster_id += 1\n            labels[i] = cluster_id\n            queue = deque()\n            queue.append(i)\n            while queue:\n                current = queue.popleft()\n                for neighbor in neighborhoods[current]:\n                    if labels[neighbor] == -1:\n                        labels[neighbor] = cluster_id\n                        if core[neighbor]:\n                            queue.append(neighbor)\n    return labels"}
{"task_id": 563, "completion_id": 0, "solution": "import numpy as np\ndef perceptron_dual_train(X_data: np.ndarray, y_data: np.ndarray, eta: float=1.0, max_iter: int=1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    N = len(y_data)\n    alpha = np.zeros(N)\n    b = 0.0\n    G = X_data @ X_data.T\n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            g = np.dot(alpha * y_data, G[:, i])\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n        if not updated:\n            break\n    w = np.sum((alpha * y_data)[:, None] * X_data, axis=0)\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = round(b, 4)\n    return (w_rounded, b_rounded)"}
{"task_id": 564, "completion_id": 0, "solution": "def predict_sample(root: 'Node', features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    if hasattr(root, 'y'):\n        return root.y\n    if task_type == 'classification':\n        feature_index = root.label\n        feature_value = features[feature_index]\n        for child in root.child:\n            if child.x == feature_value:\n                return predict_sample(child, features, task_type)\n        return predict_sample(root.child[1], features, task_type)\n    else:\n        feature_index = root.label\n        feature_value = features[feature_index]\n        if feature_value <= root.s:\n            next_node = root.child[0]\n        else:\n            next_node = root.child[1]\n        return predict_sample(next_node, features, task_type)"}
{"task_id": 569, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\ndef statistic_key_word(data: List[Tuple[Any, Iterable[str]]], cut_off: int | None=None) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    word_counts = Counter()\n    for document in data:\n        (_, words) = document\n        unique_words = set(words)\n        for word in unique_words:\n            word_counts[word] += 1\n    total_documents = len(data)\n    word_ratios = []\n    if total_documents != 0:\n        for (word, count) in word_counts.items():\n            ratio = count / total_documents\n            rounded_ratio = round(ratio, 4)\n            word_ratios.append((word, rounded_ratio))\n    sorted_word_ratios = sorted(word_ratios, key=lambda x: (-x[1], x[0]))\n    if cut_off is None:\n        top_words = {word for (word, ratio) in sorted_word_ratios}\n    elif cut_off == 0:\n        top_words = set()\n    else:\n        top_words = set()\n        for (word, ratio) in sorted_word_ratios[:cut_off]:\n            top_words.add(word)\n    return (sorted_word_ratios, top_words)"}
{"task_id": 571, "completion_id": 0, "solution": "import numpy as np\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\"\"\"\n    alpha_y = alpha * y\n    w = np.dot(X.T, alpha_y)\n    residuals = y - np.dot(X, w)\n    b = residuals.mean()\n    w_rounded = np.round(w, 4)\n    b_rounded = round(b, 4)\n    w_list = w_rounded.tolist()\n    return (w_list, b_rounded)"}
{"task_id": 574, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef generate_data(m: int, n_train: int, n_val: int, seed: int | None=None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    X_train = []\n    y_train = []\n    X_val = []\n    y_val = []\n    for c in range(m):\n        base = 10 * (c // 2)\n        x_train = np.random.uniform(base + 1, base + 9, n_train)\n        y_train_c = np.random.uniform(base + 1, base + 9, n_train)\n        points_train = np.column_stack((x_train, y_train_c))\n        X_train.append(points_train)\n        y_train.append(np.full(n_train, c))\n        x_val = np.random.uniform(base + 1, base + 9, n_val)\n        y_val_c = np.random.uniform(base + 1, base + 9, n_val)\n        points_val = np.column_stack((x_val, y_val_c))\n        X_val.append(points_val)\n        y_val.append(np.full(n_val, c))\n    X_train = np.concatenate(X_train, axis=0)\n    y_train = np.concatenate(y_train, axis=0)\n    X_val = np.concatenate(X_val, axis=0)\n    y_val = np.concatenate(y_val, axis=0)\n    X_train = np.round(X_train, 4)\n    X_val = np.round(X_val, 4)\n    return (X_train, X_val, y_train, y_val)"}
{"task_id": 581, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regression(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int, l1_penalty: float, l2_penalty: float) -> tuple[list[float], float]:\n    m = X.shape[0]\n    n = X.shape[1]\n    if learning_rate == 0 or iterations == 0:\n        return ([0.0 for _ in range(n)], 0.0)\n    w = np.zeros(n, dtype=np.float64)\n    b = 0.0\n    for _ in range(iterations):\n        y_hat = np.dot(X, w) + b\n        r = y - y_hat\n        sign_w = np.sign(w)\n        sign_w[sign_w == 0] = -1.0\n        gradient_w = np.zeros(n, dtype=np.float64)\n        for j in range(n):\n            xj = X[:, j]\n            term1 = -2 * np.dot(xj, r)\n            term2 = l1_penalty * sign_w[j]\n            term3 = 2 * l2_penalty * w[j]\n            gradient_w[j] = (term1 + term2 + term3) / m\n        gradient_b = -2 * np.sum(r) / m\n        w -= learning_rate * gradient_w\n        b -= learning_rate * gradient_b\n    weights = [round(wj, 4) for wj in w]\n    bias = round(b, 4)\n    return (weights, bias)"}
