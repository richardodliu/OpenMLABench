{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum([x * y for x, y in zip(row, b)])\n        result.append(dot_product)\n    return result\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    return list(map(list, zip(*a)))\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    try:\n        reshaped = np.reshape(a, new_shape).tolist()\n        return reshaped\n    except ValueError:\n        return []\n\n\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        return [sum(col) / len(col) for col in zip(*matrix)]\n    else:\n        raise ValueError(\"Invalid mode. Mode must be 'row' or 'column'\")\n\n\n```"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    result = [[element * scalar for element in row] for row in matrix]\n    return result\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport math\n\n\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    a, b, c, d = matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1]\n    discriminant = (a - d) ** 2 + 4 * b * c\n    root1 = ((a - d) + math.sqrt(discriminant)) / 2\n    root2 = ((a - d) - math.sqrt(discriminant)) / 2\n    eigenvalues = [root1, root2]\n    eigenvalues.sort(reverse=True)\n    return eigenvalues\n\n\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[\n    list[int | float]]:\n    T = np.array(T)\n    S = np.array(S)\n    A = np.array(A)\n\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n\n    T_inv = np.linalg.inv(T)\n    result = np.dot(T_inv, np.dot(A, S))\n    return np.round(result, 4).tolist()\n\n\n```"}
{"task_id": 8, "completion_id": 0, "completion": "```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    a, b = matrix[0]\n    c, d = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    return [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n\n\n```"}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a:list[list[int|float]], b:list[list[int|float]])-> list[list[int|float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    num_features = len(vectors[0])\n    num_observations = len(vectors)\n\n    mean_vectors = [np.mean([vector[i] for vector in vectors]) for i in range(num_features)]\n\n    covariance_matrix = [[0] * num_features for _ in range(num_features)]\n\n    for i in range(num_features):\n        for j in range(num_features):\n            covariance_matrix[i][j] = np.mean([(vectors[k][i] - mean_vectors[i]) * (vectors[k][j] - mean_vectors[j]) for k in range(num_observations)])\n\n    return covariance_matrix\n\n\n# Test cases\ntest_vectors = [\n    [1, 2],\n    [3, 4],\n    [5, 6]\n]\nexpected_result = [\n    [2.6666666666666665, 2.6666666666666665],\n    [2.6666666666666665, 2.6666666666666665]\n]\nresult = calculate_covariance_matrix(test_vectors)\nassert np.allclose(result, expected_result), f\"Expected {expected_result}, but got {result}\"\n\n\n```"}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    D = np.diag(np.diag(A))\n    LU = A - D\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x = (b - np.dot(LU, x)) / np.diag(D)\n        x = np.round(x, 4)\n    return x.reshape(-1, 1).tolist()\n\n\n```"}
{"task_id": 12, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Calculate the covariance matrix\n    C = np.dot(A.T, A)\n\n    # Calculate the eigenvalues and eigenvectors of C\n    eigenvalues, eigenvectors = np.linalg.eig(C)\n\n    # Sort eigenvalues in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Calculate singular values\n    singular_values = np.sqrt(eigenvalues)\n\n    # Calculate the right singular vectors\n    V = eigenvectors\n\n    # Calculate the left singular vectors\n    U = np.dot(A, V) / singular_values\n\n    # Normalize the left singular vectors\n    for i in range(U.shape[1]):\n        U[:, i] = U[:, i] / np.linalg.norm(U[:, i])\n\n    # Round the results to the nearest 4th decimal\n    U = np.round(U, 4)\n    singular_values = np.round(singular_values, 4)\n    V = np.round(V, 4)\n\n    return U, singular_values, V\n\n\n```"}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"Input must be a 4x4 matrix\")\n\n    if len(matrix) == 1:\n        return matrix[0][0]\n\n    det = 0\n    for col in range(4):\n        sign = (-1) ** col\n        sub_matrix = [row[:col] + row[col + 1:] for row in matrix[1:]]\n        sub_det = determinant_4x4(sub_matrix)\n        det += sign * matrix[0][col] * sub_det\n\n    return det\n\n\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y).reshape(-1, 1)\n    X_transpose = X.T\n    left_side = np.dot(X_transpose, X)\n    inverse_left_side = np.linalg.inv(left_side)\n    right_side = np.dot(X_transpose, y)\n    coefficients = np.dot(inverse_left_side, right_side)\n    return [round(coeff[0], 4) for coeff in coefficients]\n\n\n```"}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m, n = X.shape\n    theta = np.zeros(n)\n\n    for _ in range(iterations):\n        prediction = np.dot(X, theta)\n        error = prediction - y\n        gradient = np.dot(X.T, error) / m\n        theta = theta - alpha * gradient\n\n    return theta.round(4).tolist()\n\n\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    standardized_list = standardized_data.tolist()\n\n    # Min - Max Normalization\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_vals) / (max_vals - min_vals)\n    min_max_normalized_list = min_max_normalized_data.tolist()\n\n    for sublist in standardized_list:\n        for i in range(len(sublist)):\n            sublist[i] = round(sublist[i], 4)\n\n    for sublist in min_max_normalized_list:\n        for i in range(len(sublist)):\n            sublist[i] = round(sublist[i], 4)\n\n    return standardized_list, min_max_normalized_list\n\n\n```"}
{"task_id": 17, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]],\n                       max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n\n        if np.allclose(centroids, new_centroids):\n            break\n\n        centroids = new_centroids\n\n    rounded_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return rounded_centroids\n\n\n```"}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    n_samples = X.shape[0]\n    fold_size = n_samples // k\n    indices = np.arange(n_samples)\n\n    if shuffle:\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n\n    train_test_indices = []\n    for i in range(k):\n        start = i * fold_size\n        end = (i + 1) * fold_size if i < k - 1 else n_samples\n        test_indices = indices[start:end]\n        train_indices = np.concatenate([indices[:start], indices[end:]])\n        train_test_indices.append((train_indices, test_indices))\n\n    return train_test_indices\n\n\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Standardize the dataset\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n\n    # Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n\n    # Find the eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n\n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Select the top k eigenvectors (principal components)\n    principal_components = sorted_eigenvectors[:, :k].tolist()\n\n    # Round the results to the nearest 4th decimal\n    principal_components = [[round(num, 4) for num in sublist] for sublist in principal_components]\n\n    return principal_components\n\n\n```"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    if not examples:\n        return most_common_value([example[target_attr] for example in examples])\n    elif all_same_class(examples, target_attr):\n        return examples[0][target_attr]\n    elif not attributes:\n        return most_common_value([example[target_attr] for example in examples])\n    else:\n        best_attr = choose_best_attribute(examples, attributes, target_attr)\n        tree = {best_attr: {}}\n        for value in unique_values(examples, best_attr):\n            subtree_examples = [example for example in examples if example[best_attr] == value]\n            subtree_attributes = [attr for attr in attributes if attr != best_attr]\n            subtree = learn_decision_tree(subtree_examples, subtree_attributes, target_attr)\n            tree[best_attr][value] = subtree\n        return tree\n\n\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    class_counts = Counter([example[target_attr] for example in examples])\n    total_count = len(examples)\n    entropy_value = 0\n    for count in class_counts.values():\n        p = count / total_count\n        entropy_value -= p * math.log2(p)\n    return entropy_value\n\n\ndef information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n    parent_entropy = entropy(examples, target_attr)\n    total_count = len(examples)\n    weighted_entropy = 0\n    for value in unique_values(examples, attribute):\n        subset = [example for example in examples if example[attribute] == value]\n        subset_count = len(subset)\n        weighted_entropy += (subset_count / total_count) * entropy(subset, target_attr)\n    return parent_entropy - weighted_entropy\n\n\ndef choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n    best_gain = 0\n    best_attr = None\n    for attr in attributes:\n        gain = information_gain(examples, attr, target_attr)\n        if gain > best_gain:\n            best_gain = gain\n            best_attr = attr\n    return best_attr\n\n\ndef unique_values(examples: list[dict], attribute: str) -> list:\n    return list(set([example[attribute] for example in examples]))\n\n\ndef all_same_class(examples: list[dict], target_attr: str) -> bool:\n    first_class = examples[0][target_attr]\n    return all(example[target_attr] == first_class for example in examples)\n\n\ndef most_common_value(class_list: list) -> str:\n    counter = Counter(class_list)\n    return counter.most_common(1)[0][0]\n\n\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100,\n                       sigma=1.0):\n    n_samples, n_features = data.shape\n    alphas = np.zeros(n_samples)\n    b = 0\n\n    if kernel == 'linear':\n        kernel_matrix = np.dot(data, data.T)\n    elif kernel == 'rbf':\n        kernel_matrix = np.exp(-np.linalg.norm(data[:, np.newaxis] - data[np.newaxis, :], axis=2) ** 2 / (2 * sigma ** 2))\n    else:\n        raise ValueError(\"Unsupported kernel\")\n\n    for _ in range(iterations):\n        for i in range(n_samples):\n            margin = labels[i] * (np.dot(alphas * labels, kernel_matrix[i]) + b)\n            if margin < 1:\n                alphas[i] = (1 - lambda_val * 1 / n_samples) * alphas[i] + lambda_val * labels[i]\n                b = b + labels[i]\n            else:\n                alphas[i] = (1 - lambda_val * 1 / n_samples) * alphas[i]\n\n    return alphas.reshape(-1, 1).tolist(), round(b, 4)\n\n\n```"}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\n\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)\n\n\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\n\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values\n\n\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (\n        list[float], float):\n    z = [sum([features[i][j] * weights[j] for j in range(len(weights))]) + bias for i in range(len(features))]\n    sigmoid_output = [1 / (1 + math.exp(-z_i)) for z_i in z]\n    mse = np.mean((np.array(sigmoid_output) - np.array(labels)) ** 2)\n    return np.round(sigmoid_output, 4).tolist(), round(mse, 4)\n\n\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float,\n                 learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n\n    for _ in range(epochs):\n        predictions = 1 / (1 + np.exp(-(np.dot(features, weights) + bias)))\n        error = predictions - labels\n        mse = np.mean(error ** 2)\n        mse_values.append(round(mse, 4))\n\n        d_weights = np.dot(features.T, error * predictions * (1 - predictions))\n        d_bias = np.sum(error * predictions * (1 - predictions))\n\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n\n    return weights.tolist(), round(bias, 4), mse_values\n\n\n```"}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n\n```\n\nHere's how you can use it:\n\n```python\na = Value(2.0)\nb = Value(3.0)\nc = a + b\nd = c * 4\ne = d.relu()\ne.backward()\nprint(f\"Gradient of e with respect to a: {a.grad}\")\nprint(f\"Gradient of e with respect to b: {b.grad}\")\n\n\n```\n\nIn this code:\n\n1. The `__add__` method implements the addition operation. It creates a new `Value` object for the result. The backward pass for addition accumulates the gradients from the output to the inputs.\n2. The `__mul__` method implements the multiplication operation. Similar to addition, it creates a new `Value` object for the result and defines the backward pass to correctly compute the gradients.\n3. The `relu` method implements the ReLU activation function. It creates a new `Value` object for the result and defines the backward pass to compute the gradients.\n4. The `backward` method performs a topological sort of the computational graph and then computes the gradients by calling the `_backward` functions of each node in reverse topological order."}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B = np.array(B)\n    C = np.array(C)\n    P = np.linalg.solve(B, C)\n    P = np.round(P, 4)\n    return P.tolist()\n\n\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Compute A^T A\n    AtA = np.dot(A.T, A)\n\n    # Perform eigendecomposition on A^T A\n    eigenvalues, eigenvectors = np.linalg.eigh(AtA)\n\n    # Construct the singular values as the square roots of the eigenvalues\n    singular_values = np.sqrt(eigenvalues)\n\n    # Form the diagonal matrix S\n    S = np.diag(singular_values)\n\n    # Use the eigenvectors of A^T A as the columns of V\n    V = eigenvectors\n\n    # Compute U by normalizing A v_i for each non-zero singular value sigma_i\n    U = []\n    for i in range(len(singular_values)):\n        if singular_values[i] != 0:\n            v = V[:, i]\n            Av = np.dot(A, v)\n            u = Av / np.linalg.norm(Av)\n            U.append(u)\n    U = np.array(U).T\n\n    # Pad U with zeros if necessary to make it a 2x2 matrix\n    if U.shape[1] < 2:\n        num_cols_to_pad = 2 - U.shape[1]\n        padding = np.zeros((2, num_cols_to_pad))\n        U = np.hstack((U, padding))\n\n    # Round all results to the nearest 4th decimal\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n\n    return U.tolist(), S.tolist(), V.tolist()\n\n\n```"}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    combined = np.hstack((X, y.reshape(-1, 1)))\n    np.random.shuffle(combined)\n    shuffled_X = combined[:, :-1]\n    shuffled_y = combined[:, -1]\n    return [shuffled_X.tolist(), shuffled_y.tolist()]\n\n\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef batch_iterator(X, y=None, batch_size=64):\n    num_samples = X.shape[0]\n    for i in range(0, num_samples, batch_size):\n        end_i = min(i + batch_size, num_samples)\n        batch_X = X[i:end_i]\n        if y is not None:\n            batch_y = y[i:end_i]\n            yield [batch_X.tolist(), batch_y.tolist()]\n        else:\n            yield batch_X.tolist()\n\n\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef divide_on_feature(X, feature_i, threshold):\n    condition = X[:, feature_i] >= threshold\n    subset1 = X[condition].tolist()\n    subset2 = X[~condition].tolist()\n    return subset1, subset2\n\n\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\n\ndef polynomial_features(X, degree):\n    num_samples, num_features = X.shape\n    feature_combinations = []\n    for r in range(1, degree + 1):\n        combos = combinations_with_replacement(range(num_features), r)\n        feature_combinations.extend(combos)\n\n    new_features = []\n    for sample in X:\n        sample_features = []\n        for combo in feature_combinations:\n            feat = 1\n            for idx in combo:\n                feat *= sample[idx]\n            sample_features.append(feat)\n        new_features.append(sample_features)\n\n    new_features = np.array(new_features)\n    return new_features.tolist()\n\n\n```"}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    num_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(num_samples, num_samples)\n        else:\n            indices = np.random.choice(num_samples, num_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [subset[0].tolist() for subset in subsets], [subset[1].tolist() for subset in subsets]\n\n\n```\n\nIn the above code:\n\n1. `np.random.seed(seed)` sets the random seed for reproducibility.\n2. We loop `n_subsets` times.\n3. If `replacements` is `True`, we use `np.random.choice(num_samples, num_samples)` to sample indices with replacement. If `replacements` is `False`, we use `np.random.choice(num_samples, num_samples, replace=False)` to sample indices without replacement.\n4. We then create subsets of `X` and `y` using the sampled indices and append them to the `subsets` list.\n5. Finally, we return the subsets as a list of tuples, where each tuple contains the subset of `X` and the subset of `y`. The `tolist()` method is used to convert the numpy arrays to Python lists as required by the output constraints. "}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((len(x), n_col))\n    one_hot[np.arange(len(x)), x] = 1\n    return one_hot.reshape(-1, n_col).tolist()\n\n\n```"}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    correct_predictions = np.sum(y_true == y_pred)\n    accuracy = correct_predictions / len(y_true)\n    return round(accuracy, 4)\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        corr_matrix = np.corrcoef(X)\n    else:\n        corr_matrix = np.corrcoef(X, Y)\n    corr_matrix = np.round(corr_matrix, 4)\n    return corr_matrix.tolist()\n\n\n```"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                for polarity in [-1, 1]:\n                    predictions = np.where(X[:, feature] * polarity < threshold * polarity, -1, 1)\n                    error = np.sum(sample_weights * (predictions != y))\n                    if error < best_error:\n                        best_error = error\n                        best_feature = feature\n                        best_threshold = threshold\n                        best_polarity = polarity\n\n        alpha = 0.5 * math.log((1 - best_error) / best_error)\n        predictions = np.where(X[:, best_feature] * best_polarity < best_threshold * best_polarity, -1, 1)\n        sample_weights = sample_weights * np.exp(-alpha * y * predictions)\n        sample_weights = sample_weights / np.sum(sample_weights)\n\n        classifier = {\n            'feature': best_feature,\n            'threshold': best_threshold,\n            'polarity': best_polarity,\n            'alpha': alpha\n        }\n        classifiers.append(classifier)\n\n    return classifiers\n\n\n```"}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores)\n    exp_scores = np.exp(scores - np.max(scores))\n    softmax_scores = exp_scores / np.sum(exp_scores)\n    log_softmax_scores = np.log(softmax_scores)\n    return np.round(log_softmax_scores, 4).tolist()\n\n\n```"}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport copy\nimport math\n\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_optimizer = copy.deepcopy(self.W)\n        self.w0_optimizer = copy.deepcopy(self.w0)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        dW = np.dot(self.layer_input.T, accum_grad)\n        dw0 = np.sum(accum_grad, axis=0, keepdims=True)\n\n        if self.trainable:\n            self.W_optimizer = self.W - dW\n            self.w0_optimizer = self.w0 - dw0\n            self.W = self.W_optimizer\n            self.w0 = self.w0_optimizer\n\n        dX = np.dot(accum_grad, self.W.T)\n        return dX\n\n    def output_shape(self):\n        return (self.n_units,).tolist()\n\n\n```"}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    output_height = (input_height - kernel_height + 2 * padding) // stride + 1\n    output_width = (input_width - kernel_width + 2 * padding) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n\n    padded_input = np.pad(input_matrix, padding, 'constant')\n\n    for out_y in range(output_height):\n        for out_x in range(output_width):\n            y_start = out_y * stride\n            y_end = y_start + kernel_height\n            x_start = out_x * stride\n            x_end = x_start + kernel_width\n            patch = padded_input[y_start:y_end, x_start:x_end]\n            output_matrix[out_y, out_x] = np.sum(patch * kernel)\n\n    return output_matrix.round(4).tolist()\n\n\n```"}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    if z > 0:\n        return z\n    else:\n        return 0\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    mse = np.mean((np.dot(X, w) - y_true) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    loss = mse + reg_term\n    return round(loss, 4)\n\n\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float|int:\n    if z < 0:\n        return alpha * z\n    return z\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef precision(y_true, y_pred):\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    precision_score = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0\n    return precision_score\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    m = len(y)\n    for _ in range(n_iterations):\n        if method == 'batch':\n            predictions = np.dot(X, weights)\n            gradient = (1 / m) * np.dot(X.T, (predictions - y))\n        elif method =='stochastic':\n            i = np.random.randint(m)\n            predictions = np.dot(X[i], weights)\n            gradient = X[i] * (predictions - y[i])\n        elif method =='mini-batch':\n            start = np.random.randint(0, m - batch_size + 1)\n            end = start + batch_size\n            X_batch = X[start:end]\n            y_batch = y[start:end]\n            predictions = np.dot(X_batch, weights)\n            gradient = (1 / batch_size) * np.dot(X_batch.T, (predictions - y_batch))\n        else:\n            raise ValueError(\"Invalid method. Choose 'batch','stochastic', or'mini-batch'.\")\n        weights = weights - learning_rate * gradient\n    return weights.reshape(-1).tolist()\n\n\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef rref(matrix):\n    m = np.array(matrix, dtype=float)\n    rows, cols = m.shape\n\n    # Pivot rows\n    pivot_row = 0\n    for col in range(cols):\n        if pivot_row >= rows:\n            break\n\n        # Find the first non-zero element in the current column below the pivot row\n        non_zero_row = pivot_row\n        while non_zero_row < rows and m[non_zero_row, col] == 0:\n            non_zero_row += 1\n\n        if non_zero_row == rows:\n            continue\n\n        # Swap the pivot row with the non-zero row\n        m[[pivot_row, non_zero_row]] = m[[non_zero_row, pivot_row]]\n\n        # Make the leading coefficient 1\n        pivot = m[pivot_row, col]\n        m[pivot_row] = m[pivot_row] / pivot\n\n        # Eliminate other elements in the pivot column\n        for i in range(rows):\n            if i != pivot_row:\n                factor = m[i, col]\n                m[i] = m[i] - factor * m[pivot_row]\n\n        pivot_row += 1\n\n    # Back substitution to get zeros above the pivots\n    for i in range(rows - 1, 0, -1):\n        for j in range(i):\n            factor = m[j, i]\n            m[j] = m[j] - factor * m[i]\n\n    return m.tolist()\n\n\n```"}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    x = x0\n\n    for i in range(num_iterations):\n        grad_value = grad(x)\n        m = beta1 * m + (1 - beta1) * grad_value\n        v = beta2 * v + (1 - beta2) * (grad_value ** 2)\n        m_hat = m / (1 - beta1 ** (i + 1))\n        v_hat = v / (1 - beta2 ** (i + 1))\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    return x.reshape(-1).tolist()\n\n\n```"}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01,\n                                       max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    n, p = X.shape\n    w = np.zeros(p)\n    b = 0\n\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        dw = (1 / n) * np.dot(X.T, (y_pred - y)) + (alpha / n) * np.sign(w)\n        db = (1 / n) * np.sum(y_pred - y)\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        if np.linalg.norm(dw) < tol and np.abs(db) < tol:\n            break\n\n    return w.tolist(), round(b, 4)\n\n\n```"}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef OSA(source: str, target: str) -> int:\n    m, n = len(source), len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n\n    for i in range(m + 1):\n        for j in range(n + 1):\n            if i == 0:\n                dp[i][j] = j\n            elif j == 0:\n                dp[i][j] = i\n            elif source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(\n                    dp[i - 1][j],  # deletion\n                    dp[i][j - 1],  # insertion\n                    dp[i - 1][j - 1]  # substitution\n                )\n                if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                    dp[i][j] = min(dp[i][j], 1 + dp[i - 2][j - 2])  # transposition\n\n    return dp[m][n]\n\n\n```"}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef recall(y_true, y_pred):\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    if true_positives + false_negatives == 0:\n        return 0.0\n    recall_value = true_positives / (true_positives + false_negatives)\n    return round(recall_value, 3)\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n\n    return output.tolist()\n\n\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    hidden_state = np.array(initial_hidden_state)\n    for input_vector in input_sequence:\n        input_vector = np.array(input_vector)\n        pre_activation = np.dot(Wx, input_vector) + np.dot(Wh, hidden_state) + b\n        hidden_state = np.tanh(pre_activation)\n    return np.round(hidden_state, 4).tolist()\n\n\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef translate_object(points, tx, ty):\n    points = np.array(points)\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n    homogeneous_points = np.hstack((points, np.ones((points.shape[0], 1))))\n    translated_points = np.dot(translation_matrix, homogeneous_points.T).T\n    result = translated_points[:, :2]\n    return result.tolist()\n\n\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    return 0.5 * (term1 + term2 - 1)\n\n\n```"}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    if x_ini is None:\n        x = np.zeros(len(b))\n    else:\n        x = x_ini.copy()\n\n    for _ in range(n):\n        for i in range(len(A)):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n\n    return np.round(x, 4).tolist()\n\n\n```"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gaussian_elimination(A, b):\n    n = len(b)\n    # Combine A and b into an augmented matrix\n    Ab = np.hstack((A, b.reshape(-1, 1)))\n\n    # Gaussian Elimination with partial pivoting\n    for i in range(n):\n        # Find the pivot row\n        max_row = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n\n        # Elimination\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, :] -= factor * Ab[i, :]\n\n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:-1], x[i + 1:])) / Ab[i, i]\n\n    return x.round(4).tolist()\n\n\n```"}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        time_steps = x.shape[0]\n        hidden_states = []\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n\n        for t in range(time_steps):\n            input_with_hidden = np.concatenate((x[t], hidden_state), axis=0).reshape(1, -1).T\n\n            forget_gate = sigmoid(np.dot(self.Wf, input_with_hidden) + self.bf)\n            input_gate = sigmoid(np.dot(self.Wi, input_with_hidden) + self.bi)\n            candidate_cell_state = np.tanh(np.dot(self.Wc, input_with_hidden) + self.bc)\n            output_gate = sigmoid(np.dot(self.Wo, input_with_hidden) + self.bo)\n\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell_state\n            hidden_state = output_gate * np.tanh(cell_state)\n\n            hidden_states.append(hidden_state)\n\n        return [state.tolist() for state in hidden_states], hidden_state.tolist(), cell_state.tolist()\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\n```\n\nYou can use the following way to test the code:\n\n```python\n# Example usage\ninput_size = 3\nhidden_size = 4\nlstm = LSTM(input_size, hidden_size)\n\nx = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))\n\nhidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\n\nprint(\"Hidden states at each time step:\", hidden_states)\nprint(\"Final hidden state:\", final_hidden_state)\nprint(\"Final cell state:\", final_cell_state)\n\n\n```\n\nIn this code, the `LSTM` class has an `__init__` method to initialize the weights and biases. The `forward` method processes the input sequence step by step, computing the forget gate, input gate, candidate cell state, and output gate at each time step to update the hidden state and cell state. The `sigmoid` function is used to compute the sigmoid activation. The results are returned as lists after reshaping using `tolist()`."}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"Corpus cannot be empty\")\n\n    num_docs = len(corpus)\n\n    # Compute term frequency (TF) for each document\n    tf = []\n    for doc in corpus:\n        doc_tf = {}\n        total_words = len(doc)\n        for word in doc:\n            doc_tf[word] = doc_tf.get(word, 0) + 1\n        for word in doc_tf:\n            doc_tf[word] = doc_tf[word] / total_words\n        tf.append(doc_tf)\n\n    # Compute document frequency (DF) for each term\n    df = {}\n    for doc in corpus:\n        unique_words = set(doc)\n        for word in unique_words:\n            df[word] = df.get(word, 0) + 1\n\n    # Compute inverse document frequency (IDF) for each term\n    idf = {}\n    for word in df:\n        idf[word] = np.log((num_docs + 1) / (df[word] + 1)) + 1\n\n    # Compute TF-IDF scores for the query\n    query_tf = {}\n    total_query_words = len(query)\n    for word in query:\n        query_tf[word] = query_tf.get(word, 0) + 1\n    for word in query_tf:\n        query_tf[word] = query_tf[word] / total_query_words\n\n    tf_idf_scores = []\n    for doc_tf in tf:\n        doc_tf_idf = []\n        for word in query:\n            tf_score = doc_tf.get(word, 0)\n            idf_score = idf.get(word, 0)\n            doc_tf_idf.append(tf_score * idf_score)\n        tf_idf_scores.append(doc_tf_idf)\n\n    rounded_scores = np.round(tf_idf_scores, 5).tolist()\n    return rounded_scores\n\n\n```"}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n    f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score_value, 3)\n\n\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def rnn_forward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the output, the last inputs and the hidden states.\n        \"\"\"\n        T = len(input_sequence)\n        outputs = []\n        hiddens = [np.zeros((hidden_size, 1))]\n\n        for t in range(T):\n            x = input_sequence[t].reshape(-1, 1)\n            h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, hiddens[t]) + b_h)\n            y = np.dot(W_hy, h) + b_y\n            outputs.append(y)\n            hiddens.append(h)\n\n        return np.array(outputs), input_sequence[-1].reshape(-1, 1), np.array(hiddens[1:])\n\n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs,\n                     last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(W_xh)\n        dW_hh = np.zeros_like(W_hh)\n        dW_hy = np.zeros_like(W_hy)\n        db_h = np.zeros_like(b_h)\n        db_y = np.zeros_like(b_y)\n\n        loss = 0\n        for t in reversed(range(T)):\n            y = outputs[t]\n            error = y - expected_output[t].reshape(-1, 1)\n            loss += 0.5 * np.sum(error ** 2)\n\n            dW_hy += np.dot(error, last_hiddens[t].T)\n            db_y += error\n\n            dh = np.dot(W_hy.T, error) + db_h\n            dh_raw = (1 - last_hiddens[t] ** 2) * dh\n\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, last_hiddens[t - 1].T)\n            db_h += dh_raw\n\n        dW_xh /= T\n        dW_hh /= T\n        dW_hy /= T\n        db_h /= T\n        db_y /= T\n\n        W_xh -= learning_rate * dW_xh\n        W_hh -= learning_rate * dW_hh\n        W_hy -= learning_rate * dW_hy\n        b_h -= learning_rate * db_h\n        b_y -= learning_rate * db_y\n\n        return loss\n\n\n# Example usage:\ninput_size = 2\nhidden_size = 3\noutput_size = 1\nrnn = SimpleRNN(input_size, hidden_size, output_size)\n\ninput_sequence = np.array([[1, 0], [0, 1], [1, 1]])\nexpected_output = np.array([[0], [1], [0]])\n\noutputs, last_input, last_hiddens = rnn.rnn_forward(rnn.W_xh, rnn.W_hh, rnn.W_hy, rnn.b_h, rnn.b_y, hidden_size,\n                                                   input_sequence)\nloss = rnn.rnn_backward(rnn.W_xh, rnn.W_hh, rnn.W_hy, rnn.b_h, rnn.b_y, hidden_size, input_sequence, expected_output,\n                        outputs, last_input, last_hiddens, 0.1)\n\nprint(\"Loss:\", loss)\n\n\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array = None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - A.dot(x)\n    p = r.copy()\n    rsold = r.dot(r)\n\n    for _ in range(n):\n        Ap = A.dot(p)\n        alpha = rsold / (p.dot(Ap))\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = r.dot(r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + (rsnew / rsold) * p\n        rsold = rsnew\n\n    return np.round(x, 8).tolist()\n\n\n```"}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    unique, counts = np.unique(y, return_counts=True)\n    total = len(y)\n    impurity = 1\n    for count in counts:\n        p = count / total\n        impurity -= p ** 2\n    return round(impurity, 3)\n\n\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_ptr = [0]\n\n    for row in dense_matrix:\n        for col, val in enumerate(row):\n            if val != 0:\n                values.append(val)\n                col_indices.append(col)\n        row_ptr.append(len(values))\n\n    return values, col_indices, row_ptr\n```"}
{"task_id": 66, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = np.dot(v, L)\n    norm_squared = np.dot(L, L)\n    projection_coefficient = dot_product / norm_squared\n    projection_vector = [projection_coefficient * L[i] for i in range(len(L))]\n    return [round(num, 3) for num in projection_vector]\n\n\n```"}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n\n    values = []\n    row_indices = []\n    column_pointer = [0]\n\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer.append(len(values))\n\n    return values, row_indices, column_pointer\n\n\n```"}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef matrix_image(A):\n    _, _, v = np.linalg.svd(A)\n    rank = np.linalg.matrix_rank(A)\n    basis_vectors = A[:, np.argsort(np.abs(np.diag(v)))[::-1][:rank]]\n    return np.round(basis_vectors, 8).tolist()\n\n\n```"}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    ss_tot = np.sum((y_true - y_mean) ** 2)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - (ss_res / ss_tot)\n    return round(r2, 3)\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    if not img:\n        return -1\n    height = len(img)\n    width = len(img[0]) if height > 0 else 0\n    total_pixels = height * width\n    brightness_sum = 0\n\n    for row in img:\n        if len(row) != width:\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            brightness_sum += pixel\n\n    if total_pixels == 0:\n        return -1\n    average_brightness = brightness_sum / total_pixels\n    return round(average_brightness, 2)\n\n\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError(\"Both y_true and y_pred must be numpy arrays\")\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Arrays y_true and y_pred must have the same shape\")\n    if len(y_true) == 0:\n        raise ValueError(\"Arrays cannot be empty\")\n\n    squared_errors = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    rmse_value = np.sqrt(mean_squared_error)\n    return round(rmse_value, 3)\n\n\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0 if intersection == 0 else 0.0\n    return intersection / union\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    intersection = np.sum(y_true & y_pred)\n    union = np.sum(y_true) + np.sum(y_pred)\n    if union == 0:\n        return 1.0\n    dice = 2 * intersection / union\n    return round(dice, 3)\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef create_row_hv(row, dim, random_seeds):\n    composite_hv = []\n    for feature, value in row.items():\n        seed = random_seeds[feature]\n        np.random.seed(seed)\n        feature_hv = np.random.choice([-1, 1], dim)\n        value_seed = hash(value)\n        np.random.seed(value_seed)\n        value_hv = np.random.choice([-1, 1], dim)\n        bound_hv = np.logical_xor(feature_hv, value_hv).astype(int)\n        composite_hv.append(bound_hv)\n    final_hv = np.sum(composite_hv, axis=0)\n    final_hv[final_hv == 0] = -1\n    return final_hv.tolist()\n\n\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\n\ndef confusion_matrix(data):\n    true_positive = 0\n    true_negative = 0\n    false_positive = 0\n    false_negative = 0\n\n    for pair in data:\n        y_true, y_pred = pair\n        if y_true == 1 and y_pred == 1:\n            true_positive += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negative += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positive += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negative += 1\n\n    return [[true_negative, false_positive], [false_negative, true_positive]]\n\n\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    assert all([x in [0, 1] for x in actual]), \"All elements in actual must be 0 or 1\"\n    assert all([x in [0, 1] for x in predicted]), \"All elements in predicted must be 0 or 1\"\n    assert len(actual) == len(predicted), \"actual and predicted must have the same length\"\n\n    true_positive = 0\n    false_positive = 0\n    false_negative = 0\n    true_negative = 0\n\n    for act, pred in zip(actual, predicted):\n        if act == 1 and pred == 1:\n            true_positive += 1\n        elif act == 0 and pred == 1:\n            false_positive += 1\n        elif act == 1 and pred == 0:\n            false_negative += 1\n        elif act == 0 and pred == 0:\n            true_negative += 1\n\n    confusion_matrix = [[true_negative, false_positive], [false_negative, true_positive]]\n\n    accuracy = (true_positive + true_negative) / len(actual)\n    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0\n    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n    specificity = true_negative / (true_negative + false_positive)\n    negative_predictive_value = true_negative / (true_negative + false_negative)\n\n    return (\n        confusion_matrix,\n        round(accuracy, 3),\n        round(f1_score, 3),\n        round(specificity, 3),\n        round(negative_predictive_value, 3)\n    )\n\n\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef descriptive_statistics(data):\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = np.mode(data) if hasattr(np, 'mode') else None\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentiles = np.percentile(data, [25, 50, 75])\n    interquartile_range = percentiles[2] - percentiles[0]\n\n    result = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode[0] if mode is not None else None,\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentiles[0], 4),\n        '50th_percentile': round(percentiles[1], 4),\n        '75th_percentile': round(percentiles[2], 4),\n        'interquartile_range': round(interquartile_range, 4)\n    }\n\n    return result\n\n\n```"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * (p ** k) * ((1 - p) ** (n - k))\n    return round(probability, 5)\n\n\n```"}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -((x - mean) ** 2) / (2 * std_dev ** 2)\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\n\n\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    return round((math.exp(-lam) * (lam ** k)) / math.factorial(k), 5)\n\n\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast\n\n\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    return np.dot(vec1, vec2)\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    num_data_points = len(data)\n    result = []\n    for point in data:\n        point_transformed = []\n        for d in range(1, degree + 1):\n            for exponent in np.ndindex((d,)):\n                term = 1\n                for i, exp in enumerate(exponent):\n                    term *= point ** (exp + 1)\n                point_transformed.append(term)\n        result.append(point_transformed)\n    for sublist in result:\n        for i, value in enumerate(sublist):\n            sublist[i] = round(value, 8)\n    return result\n\n\n```"}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    encoding = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            encoding[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))\n            encoding[pos, i + 1] = np.cos(pos / (10000 ** (2 * i / d_model)))\n    return encoding.tolist()\n\n\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\n\n\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - (learning_rate * m_hat) / (np.sqrt(v_hat) + epsilon)\n    return [round(updated_parameter, 5).tolist(), round(m, 5).tolist(), round(v, 5).tolist()]\n\n\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n\n    for _ in range(n_tokens_to_generate):\n        input_ids = np.array(token_ids[-hparams[\"n_ctx\"]:])\n        input_embeds = params[\"wte\"][input_ids] + params[\"wpe\"][:len(input_ids)]\n\n        # Multi - head Attention (simplified, no full implementation here)\n        attn_output = input_embeds\n\n        # Feed - Forward Network (simplified, no full implementation here)\n        ff_output = attn_output\n\n        # Layer Normalization (simplified, no full implementation here)\n        norm_output = ff_output\n\n        last_token_embed = norm_output[-1]\n        logits = np.dot(last_token_embed, params[\"wte\"].T)\n        next_token_id = np.argmax(logits)\n        token_ids.append(next_token_id)\n\n    generated_text = encoder.decode(token_ids)\n    return generated_text\n\n\n```"}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n\n    # Convert crystal_values to a 2D array for matrix operations\n    crystal_values = np.array(crystal_values).reshape(n, dimension)\n\n    # Compute the attention scores\n    attention_scores = np.dot(crystal_values, crystal_values.T)\n\n    # Apply softmax to the attention scores\n    attention_weights = softmax(attention_scores)\n\n    # Compute the final weighted pattern for each crystal\n    final_patterns = np.dot(attention_weights, crystal_values)\n\n    # Flatten the final patterns and round to 4 decimal places\n    return [round(val, 4) for val in final_patterns.flatten()]\n\n\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    n = len(corpus)\n    avdl = sum(len(doc) for doc in corpus) / n\n    scores = []\n\n    for doc in corpus:\n        doc_term_count = Counter(doc)\n        score = 0\n        for term in query:\n            tf = doc_term_count[term]\n            idf = np.log((n - np.count_nonzero([term in d for d in corpus]) + 0.5) / (np.count_nonzero([term in d for d in corpus]) + 0.5))\n            numerator = (k1 + 1) * tf\n            denominator = tf + k1 * (1 - b + b * len(doc) / avdl)\n            score += idf * (numerator / denominator)\n        scores.append(round(score, 3))\n\n    return scores\n\n\n```"}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n\n    for true, pred in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            true_positives += 1\n        elif true == 0 and pred == 1:\n            false_positives += 1\n        elif true == 1 and pred == 0:\n            false_negatives += 1\n\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return round(f1_score, 3)\n\n\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nPI = 3.14159\n\n\ndef power_grid_forecast(consumption_data):\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n\n    x = list(range(1, len(detrended_data) + 1))\n    y = detrended_data\n\n    n = len(x)\n    sum_x = sum(x)\n    sum_y = sum(y)\n    sum_xy = sum([x[i] * y[i] for i in range(n)])\n    sum_x_squared = sum([x[i] ** 2 for i in range(n)])\n\n    slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x ** 2)\n    intercept = (sum_y - slope * sum_x) / n\n\n    day_15_base_consumption = intercept + slope * 15\n\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base_consumption + day_15_fluctuation\n\n    final_consumption = math.ceil(day_15_consumption * 1.05)\n\n    return final_consumption\n\n\n```"}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    error = np.abs(y_true - y_pred)\n    mae_value = np.mean(error)\n    return round(mae_value, 3)\n\n\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return output\n\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    batch_size, seq_len, d_model = X.shape\n    head_dim = d_model // n_heads\n\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n    Q = np.reshape(Q, (batch_size, seq_len, n_heads, head_dim))\n    K = np.reshape(K, (batch_size, seq_len, n_heads, head_dim))\n    V = np.reshape(V, (batch_size, seq_len, n_heads, head_dim))\n\n    Q = np.transpose(Q, (0, 2, 1, 3))\n    K = np.transpose(K, (0, 2, 1, 3))\n    V = np.transpose(V, (0, 2, 1, 3))\n\n    head_outputs = []\n    for i in range(n_heads):\n        head_Q = Q[:, i, :, :]\n        head_K = K[:, i, :, :]\n        head_V = V[:, i, :, :]\n        head_output = self_attention(head_Q, head_K, head_V)\n        head_outputs.append(head_output)\n\n    concatenated = np.concatenate(head_outputs, axis=-1)\n    concatenated = np.transpose(concatenated, (0, 2, 1, 3))\n    concatenated = np.reshape(concatenated, (batch_size, seq_len, d_model))\n\n    return concatenated.tolist()\n\n\n```"}
{"task_id": 95, "completion_id": 0, "completion": "```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n = len(x)\n    a = sum(x)\n    b = sum(y)\n    c = n - a\n    d = n - b\n    e = sum([x[i] * y[i] for i in range(n)])\n    f = sum([x[i] * (1 - y[i]) for i in range(n)])\n    g = sum([(1 - x[i]) * y[i] for i in range(n)])\n    h = sum([(1 - x[i]) * (1 - y[i]) for i in range(n)])\n\n    numerator = e * h - f * g\n    denominator = ((e + f) * (e + g) * (h + f) * (h + g)) ** 0.5\n\n    phi = numerator / denominator\n    return round(phi, 4)\n\n\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0\n    elif x > 2.5:\n        return 1\n    else:\n        return (x + 2.5) / 5\n```"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)\n\n\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x < 0:\n        return alpha * x\n    return x\n\n\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x < -30:\n        return 0\n    elif x > 30:\n        return x\n    else:\n        return math.log(1 + math.exp(x))\n\n\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\n\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n\n    unclipped_objective = np.sum(rhos * A)\n    clipped_objective = np.sum(clipped_rhos * A)\n\n    kl_divergence = np.sum(pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref)))\n\n    objective = np.min([unclipped_objective, clipped_objective]) - beta * kl_divergence\n\n    return round(objective, 6)\n\n\n```"}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * sigmoid(x)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\n```\n\nTo use this function, you can call `swish(some_value)` where `some_value` is the input for which you want to compute the Swish activation value. For example:\n\n```python\ninput_value = 2.5\nresult = swish(input_value)\nrounded_result = round(result, 4)\nprint(rounded_result)\n```\n\nThis will print the Swish activation value for the input `2.5` rounded to the nearest 4th decimal.\n\nIf you want to use it in a more general way for arrays or lists of values, you can do something like this:\n\n```python\ninput_array = np.array([1.0, 2.0, 3.0])\nresult_array = np.vectorize(swish)(input_array)\nrounded_result_array = np.round(result_array, 4)\nprint(rounded_result_array)\n```\n\nThis will compute the Swish activation values for each element in the `input_array` and print the rounded results. "}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\n\n\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\u0097 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    sigmoid_values = 1 / (1 + np.exp(-z))\n    predictions = np.where(sigmoid_values >= 0.5, 1, 0)\n    return predictions.tolist()\n\n\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    num_samples, num_features = X.shape\n    num_classes = len(np.unique(y))\n\n    B = np.zeros((num_features, num_classes))\n    losses = []\n\n    for _ in range(iterations):\n        z = np.dot(X, B)\n        exp_scores = np.exp(z)\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n        correct_logprobs = -np.log(probs[range(num_samples), y])\n        loss = np.sum(correct_logprobs) / num_samples\n        losses.append(round(loss, 4))\n\n        dscores = probs\n        dscores[range(num_samples), y] -= 1\n        dscores /= num_samples\n\n        dB = np.dot(X.T, dscores)\n\n        B -= learning_rate * dB\n\n    B = B.tolist()\n    losses = [round(loss, 4) for loss in losses]\n\n    return B, losses\n\n\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    n, p = X.shape\n    coefficients = np.zeros(p)\n    loss_values = []\n\n    for _ in range(iterations):\n        z = np.dot(X, coefficients)\n        y_pred = 1 / (1 + np.exp(-z))\n        gradient = np.dot(X.T, (y_pred - y)) / n\n        coefficients -= learning_rate * gradient\n\n        # Calculate Binary Cross Entropy loss\n        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        loss_values.append(round(loss, 4))\n\n    return coefficients.tolist(), loss_values\n\n\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n    scores = scores.masked_fill(mask == 0, -np.inf)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.matmul(attention_weights, V)\n    return output.tolist()\n\n\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport math\n\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    unique_colors = set(apples)\n    color_count = {color: apples.count(color) for color in unique_colors}\n    total_apples = len(apples)\n    entropy = 0\n    for count in color_count.values():\n        p = count / total_apples\n        entropy -= p * math.log2(p)\n    return round(entropy, 4)\n\n\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    norm_X = (X - mean) / np.sqrt(var + epsilon)\n    out = gamma * norm_X + beta\n    return np.round(out, 5).tolist()\n\n\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    ref_words = reference.split()\n    cand_words = candidate.split()\n\n    ref_word_count = Counter(ref_words)\n    cand_word_count = Counter(cand_words)\n\n    unigram_matches = 0\n    for word in cand_words:\n        if word in ref_word_count:\n            unigram_matches += min(cand_word_count[word], ref_word_count[word])\n\n    precision = unigram_matches / len(cand_words) if cand_words else 0\n    recall = unigram_matches / len(ref_words) if ref_words else 0\n\n    f_mean = (precision * recall) / (alpha * precision + (1 - alpha) * recall) if (precision + recall) > 0 else 0\n\n    word_order_fragmentation = 0\n    for i in range(len(cand_words)):\n        if cand_words[i] in ref_words:\n            ref_index = ref_words.index(cand_words[i])\n            if i > ref_index:\n                word_order_fragmentation += 1\n\n    word_order_fragmentation = word_order_fragmentation / len(cand_words) if cand_words else 0\n\n    meteor_score = (1 - gamma) * f_mean + gamma * (1 - word_order_fragmentation)\n\n    return round(meteor_score, 3)\n\n\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)\n\n\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    min_val = min(x)\n    max_val = max(x)\n    normalized = [(i - min_val) / (max_val - min_val) for i in x]\n    return [round(num, 4) for num in normalized]\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    # First layer of matrix multiplication\n    y1 = np.dot(x, w1)\n    # Apply ReLU activation\n    y1 = np.maximum(0, y1)\n    # Second layer of matrix multiplication\n    y2 = np.dot(y1, w2)\n    # Add the original input via shortcut connection\n    y = y2 + x\n    # Apply final ReLU activation\n    y = np.maximum(0, y)\n    return np.round(y, 4).tolist()\n\n\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef global_avg_pool(x: np.ndarray):\n    height, width, channels = x.shape\n    result = np.zeros(channels)\n    for c in range(channels):\n        result[c] = np.mean(x[:, :, c])\n    return result\n\n\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    batch_size, num_channels, height, width = X.shape\n\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n\n    out = gamma * X_hat + beta\n\n    return np.round(out, 4).tolist()\n\n\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    if n == 0:\n        return 0\n    else:\n        derivative = c * n * (x ** (n - 1))\n        return round(derivative, 4)\n\n\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    vectors = np.array(vectors)\n    basis = []\n    for v in vectors:\n        v = np.array(v)\n        for u in basis:\n            v = v - np.dot(v, u) * u\n        if np.linalg.norm(v) > tol:\n            v = v / np.linalg.norm(v)\n            basis.append(v)\n    orthonormal_vectors = [vector.tolist() for vector in basis]\n    rounded_vectors = [[round(num, 4) for num in vector] for vector in orthonormal_vectors]\n    return rounded_vectors\n\n\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef cross_product(a, b):\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()\n\n\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef cramers_rule(A, b):\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = A.shape[0]\n    x = []\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        xi = det_Ai / det_A\n        x.append(xi)\n    return np.round(x, 4).tolist()\n\n\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or not q:\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    root_product = np.sqrt(p * q)\n    distance = -np.log(np.sum(root_product))\n    return round(distance, 4)\n\n\n```"}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for x, y in zip(a, b)]\n    return result\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradients = np.zeros_like(theta)\n\n    for episode in episodes:\n        G = 0\n        for state, action, reward in reversed(episode):\n            G = reward + G\n            log_prob = np.log(softmax(theta[state, :]))[action]\n            gradients[state, :] += log_prob * G\n\n    avg_gradients = gradients / len(episodes)\n    return avg_gradients.reshape(-1).tolist()\n\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\n\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\nimport math\n\n\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    # FLOPs for dense layer\n    flops_dense = d_in * d_out\n\n    # FLOPs for MoE layer\n    flops_moe = k_active * d_in * d_out\n\n    # Computational cost savings percentage\n    savings_percentage = ((flops_dense - flops_moe) / flops_dense) * 100\n\n    return round(savings_percentage, 1)\n\n\n```"}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Calculate the logits\n    logits = np.matmul(X, W_g) + np.multiply(W_noise, N)\n\n    # Apply softmax to get probabilities\n    exp_logits = np.exp(logits)\n    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n\n    # Get the top-k indices for each row\n    top_k_indices = np.argsort(probs, axis=1)[:, -k:]\n\n    # Create a mask for the top-k indices\n    mask = np.zeros_like(probs, dtype=bool)\n    for i, indices in enumerate(top_k_indices):\n        mask[i, indices] = True\n\n    # Set probabilities outside the top-k to zero\n    probs[~mask] = 0\n\n    # Normalize the probabilities again to ensure they sum to 1\n    probs = probs / np.sum(probs, axis=1, keepdims=True)\n\n    return probs.round(4).tolist()\n\n\n```"}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    batch_size, seq_len, input_dim = x.shape\n    gates = np.dot(x, Wg)\n    gating_probs = np.exp(gates) / np.sum(np.exp(gates), axis=1, keepdims=True)\n\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n\n    expert_outputs = []\n    for i in range(batch_size):\n        for j in range(seq_len):\n            expert_output = np.zeros((input_dim,))\n            for k in range(top_k):\n                expert_index = top_k_indices[i, j, k]\n                expert_output += top_k_probs[i, j, k] * np.dot(x[i, j], We[expert_index])\n            expert_outputs.append(expert_output)\n\n    output = np.array(expert_outputs).reshape(batch_size, seq_len, input_dim)\n    return output.tolist()\n\n\n```"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    B, C, H, W = X.shape\n    assert C % num_groups == 0, \"Number of channels must be divisible by the number of groups\"\n\n    X = X.reshape(B, num_groups, C // num_groups, H, W)\n\n    mean = np.mean(X, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X, axis=(2, 3, 4), keepdims=True)\n\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n\n    X_norm = X_norm.reshape(B, C, H, W)\n\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n\n    out = gamma * X_norm + beta\n\n    return out.tolist()\n\n\n```"}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    x = start_x\n    for _ in range(max_iters):\n        gradient = 4 * x ** 3 - 9 * x ** 2\n        x = x - learning_rate * gradient\n        if abs(gradient) < tolerance:\n            break\n    return round(x, 4)\n\n\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    # Calculate the dynamic tanh transformation\n    result = gamma * (np.tanh(alpha * x) + beta)\n\n    # Reshape the result to a 1D list and round to 4 decimal places\n    return np.round(result.reshape(-1), 4).tolist()\n\n\n```"}
