{"task_id": 0, "completion_id": 0, "solution": "import numpy as np\ndef ridge_regression(data: np.ndarray, alpha: float=0.0) -> list[float]:\n    \"\"\"\n    Closed-form Ridge (L2-regularised) linear-regression solver.\n    \n    Parameters\n    ----------\n    data  : np.ndarray\n        2-D array where the last column is the target y and the remaining\n        columns are the features X.\n    alpha : float, default 0.0\n        Regularisation strength (\u03b1 \u2265 0).\n        \n    Returns\n    -------\n    list[float]\n        Weight vector (including the bias/intercept as the first element)\n        rounded to 4 decimal places.  \n        If (X\u1d40X + \u03b1I) is numerically singular the function returns -1.\n    \"\"\"\n    (X, y) = (data[:, :-1], data[:, -1])\n    n_samples = X.shape[0]\n    X_bias = np.hstack((np.ones((n_samples, 1)), X))\n    A = X_bias.T @ X_bias + alpha * np.eye(X_bias.shape[1])\n    try:\n        det = np.linalg.det(A)\n    except np.linalg.LinAlgError:\n        return -1\n    if np.isclose(det, 0.0, atol=1e-12):\n        return -1\n    w = np.linalg.inv(A) @ X_bias.T @ y\n    return np.round(w, 4).tolist()"}
{"task_id": 1, "completion_id": 0, "solution": "import numpy as np\ndef perceptron_dual(X_train: np.ndarray, y_train: np.ndarray, eta: float=1.0, n_iter: int=10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron learning algorithm.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples, shape (n_samples, n_features).\n    y_train : np.ndarray\n        Labels (+1 or -1), length n_samples.\n    eta : float, optional\n        Learning rate, default 1.0.\n    n_iter : int, optional\n        Maximum number of parameter updates, default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        Weight vector (as a list) and bias, both rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=float).ravel()\n    if not set(np.unique(y)).issubset({-1.0, 1.0}):\n        raise ValueError('y_train must contain only +1 and -1.')\n    (n_samples, n_features) = X.shape\n    alpha = np.zeros(n_samples, dtype=float)\n    b = 0.0\n    updates = 0\n    gram = X @ X.T\n    while updates < n_iter:\n        no_mistake_in_pass = True\n        for i in range(n_samples):\n            activation = np.dot(alpha * y, gram[:, i])\n            margin = y[i] * (activation + b)\n            if margin <= 0.0:\n                alpha[i] += eta\n                b += eta * y[i]\n                updates += 1\n                no_mistake_in_pass = False\n                break\n        if no_mistake_in_pass:\n            break\n    w = np.dot(alpha * y, X)\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return (w_rounded, b_rounded)"}
{"task_id": 2, "completion_id": 0, "solution": "import numpy as np\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A dictionary with three entries  (\"identity\", \"log\", \"logit\").\n        Each entry itself is a dictionary with three callables\n\n            \u2022 \"link\"      :  g(\u03bc)\n            \u2022 \"inv_link\"  :  g\u207b\u00b9(\u03b7)\n            \u2022 \"derivative\":  g\u2032(\u03bc)\n\n        All functions work element\u2013wise on numpy scalars, 1-D and 2-D\n        numpy arrays.\n    \"\"\"\n\n    def _id_link(mu):\n        return mu\n\n    def _id_inv_link(eta):\n        return eta\n\n    def _id_derivative(mu):\n        return np.ones_like(mu, dtype=float)\n\n    def _log_link(mu):\n        return np.log(mu)\n\n    def _log_inv_link(eta):\n        return np.exp(eta)\n\n    def _log_derivative(mu):\n        return 1.0 / mu\n\n    def _logit_link(mu):\n        return np.log(mu / (1.0 - mu))\n\n    def _logit_inv_link(eta):\n        eta = np.asarray(eta)\n        return np.where(eta >= 0, 1.0 / (1.0 + np.exp(-eta)), np.exp(eta) / (1.0 + np.exp(eta)))\n\n    def _logit_derivative(mu):\n        return 1.0 / (mu * (1.0 - mu))\n    return {'identity': {'link': _id_link, 'inv_link': _id_inv_link, 'derivative': _id_derivative}, 'log': {'link': _log_link, 'inv_link': _log_inv_link, 'derivative': _log_derivative}, 'logit': {'link': _logit_link, 'inv_link': _logit_inv_link, 'derivative': _logit_derivative}}"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Return, for every sample, the depth of the leaf it lands in inside a\n    single random isolation tree grown as described in the task statement.\n    \"\"\"\n    np.random.seed(0)\n    if data.size == 0:\n        return []\n    (n_samples, n_features) = data.shape\n    depths = [-1] * n_samples\n\n    def grow(indices: list[int], depth: int) -> None:\n        \"\"\"Recursively grow the isolation tree while filling `depths`.\"\"\"\n        if len(indices) < 3:\n            for idx in indices:\n                depths[idx] = depth\n            return\n        f = np.random.randint(0, n_features)\n        vals = data[indices, f]\n        (down, up) = (vals.min(), vals.max())\n        if down == up:\n            for idx in indices:\n                depths[idx] = depth\n            return\n        v = np.random.uniform(down, up)\n        left = [idx for idx in indices if data[idx, f] <= v]\n        right = [idx for idx in indices if data[idx, f] > v]\n        if len(left) == 0 or len(right) == 0:\n            for idx in indices:\n                depths[idx] = depth\n            return\n        grow(left, depth + 1)\n        grow(right, depth + 1)\n    grow(list(range(n_samples)), depth=0)\n    return depths"}
{"task_id": 4, "completion_id": 0, "solution": "import numpy as np\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Return the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A nested Python list where element [i][j] is the squared Euclidean\n        distance between samples i and j, rounded to 4 decimal places.\n    \"\"\"\n    if X.shape[0] == 0:\n        return []\n    sq_norms = np.sum(X ** 2, axis=1, keepdims=True)\n    D = sq_norms + sq_norms.T - 2 * X @ X.T\n    D = np.maximum(D, 0.0)\n    return np.round(D, 4).tolist()"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef xgboost_predict(predictions: list, learning_rate: float) -> list[int]:\n    \"\"\"Aggregate XGBoost tree outputs and produce final class predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        Each element is a 2-D array-like object of shape (n_samples, n_classes)\n        containing the raw outputs produced by one tree.\n    learning_rate : float\n        The learning-rate (eta) used by the booster.\n\n    Returns\n    -------\n    list[int]\n        The predicted class label for every sample.\n    \"\"\"\n    total = np.asarray(predictions[0], dtype=float)\n    for tree_out in predictions[1:]:\n        total += np.asarray(tree_out, dtype=float)\n    raw_scores = -learning_rate * total\n    row_max = np.max(raw_scores, axis=1, keepdims=True)\n    exp_scores = np.exp(raw_scores - row_max)\n    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    return np.argmax(probs, axis=1).tolist()"}
{"task_id": 7, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable, Iterable, FrozenSet\ndef _sorted_tuple(it: Iterable[Hashable]) -> Tuple[Hashable, ...]:\n    \"\"\"\n    Return the items of *it* as an ascending tuple.\n\n    When item types are not mutually comparable we fall back to comparing their\n    textual representation, which is at least deterministic.\n    \"\"\"\n    try:\n        return tuple(sorted(it))\n    except TypeError:\n        return tuple(sorted(it, key=repr))\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent item-sets together with their absolute support.\n\n    Args\n    ----\n    transactions : list[list[hashable]]\n        Transactional data base \u2013 duplicates inside one transaction are ignored.\n    minsup       : float\n        Minimum support given as a fraction in the (0, 1] interval.\n\n    Returns\n    -------\n    dict[tuple, int]\n        Maps every frequent item-set (tuple with ascending items) to its support.\n        The empty dict is returned when no item-set fulfils the threshold.\n    \"\"\"\n    if not 0 < minsup <= 1:\n        raise ValueError('minsup must be in the interval (0, 1].')\n    transactions = [set(t) for t in transactions]\n    n_transactions = len(transactions)\n    if n_transactions == 0:\n        return {}\n    min_count = max(1, math.ceil(minsup * n_transactions))\n    result: Dict[Tuple[Hashable, ...], int] = {}\n    item_counter = Counter()\n    for t in transactions:\n        item_counter.update(t)\n    L_k: Dict[FrozenSet[Hashable], int] = {frozenset([item]): supp for (item, supp) in item_counter.items() if supp >= min_count}\n    for (fs, supp) in L_k.items():\n        result[_sorted_tuple(fs)] = supp\n    k = 2\n    while L_k:\n        L_prev = list(L_k.keys())\n        candidates: set[FrozenSet[Hashable]] = set()\n        for i in range(len(L_prev)):\n            for j in range(i + 1, len(L_prev)):\n                union = L_prev[i] | L_prev[j]\n                if len(union) != k:\n                    continue\n                if all((union - frozenset([item]) in L_k for item in union)):\n                    candidates.add(union)\n        if not candidates:\n            break\n        cand_counter = Counter()\n        for t in transactions:\n            for cand in candidates:\n                if cand.issubset(t):\n                    cand_counter[cand] += 1\n        L_k = {cand: supp for (cand, supp) in cand_counter.items() if supp >= min_count}\n        for (fs, supp) in L_k.items():\n            result[_sorted_tuple(fs)] = supp\n        k += 1\n    return result"}
{"task_id": 8, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef logistic_regression_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, learning_rate: float=0.1, n_iterations: int=4000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model via batch GD and predict labels.\n\n    Args:\n        X_train: Training feature matrix of shape (n_samples, n_features).\n        y_train: Binary training labels of shape (n_samples,).\n        X_test:  Feature matrix to classify, shape (m_samples, n_features).\n        learning_rate: Step size used by gradient descent.\n        n_iterations:  Number of gradient-descent iterations.\n\n    Returns:\n        Python list with the predicted class (0 or 1) for every row of X_test.\n    \"\"\"\n\n    def _sigmoid(z: np.ndarray) -> np.ndarray:\n        z_clipped = np.clip(z, -709, 709)\n        return 1.0 / (1.0 + np.exp(-z_clipped))\n    X_train = np.asarray(X_train, dtype=np.float64)\n    y_train = np.asarray(y_train, dtype=np.float64).flatten()\n    (n_samples, n_features) = X_train.shape\n    w = np.zeros(n_features, dtype=np.float64)\n    b = 0.0\n    for _ in range(n_iterations):\n        z = X_train @ w + b\n        p = _sigmoid(z)\n        error = p - y_train\n        grad_w = X_train.T @ error / n_samples\n        grad_b = error.mean()\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n    z_test = X_test @ w + b\n    probs = _sigmoid(z_test)\n    preds = np.round(probs).astype(int)\n    return preds.tolist()"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\ndef smooth(prev, cur, weight):\n    \"\"\"Apply first-order exponential smoothing.\n\n    Parameters\n    ----------\n    prev : float | list | tuple | numpy.ndarray\n        Smoothed value from the previous time step (X\u0303_{t-1}).\n    cur : float | list | tuple | numpy.ndarray\n        Current raw observation (X_t).\n    weight : float | list | tuple | numpy.ndarray\n        Smoothing factor \u03b5, 0 \u2264 weight \u2264 1.  Scalars or one\u2013dimensional\n        vectors are supported and broadcast against each other.\n\n    Returns\n    -------\n    float or list[float]\n        Smoothed value(s) rounded to four decimal places.  A scalar is returned\n        if all inputs are scalars; otherwise a list is returned.\n    \"\"\"\n\n    def _is_scalar(x):\n        return np.isscalar(x) or (isinstance(x, np.ndarray) and x.ndim == 0)\n    prev_arr = np.asarray(prev, dtype=float)\n    cur_arr = np.asarray(cur, dtype=float)\n    weight_arr = np.asarray(weight, dtype=float)\n    if np.any(weight_arr < 0) or np.any(weight_arr > 1):\n        raise ValueError('weight must lie in the interval [0, 1].')\n    result = weight_arr * prev_arr + (1.0 - weight_arr) * cur_arr\n    result = np.round(result, 4)\n    if _is_scalar(prev) and _is_scalar(cur) and _is_scalar(weight):\n        return float(result)\n    else:\n        return result.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data: np.ndarray, k: int, epsilon: float=0.001, max_iter: int=2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array (n_samples \u00d7 n_features).\n    k : int\n        Number of desired clusters (1 \u2264 k \u2264 n_samples).\n    epsilon : float, default 1e-3\n        Stop if the largest centre movement is smaller than *epsilon*.\n    max_iter : int, default 2000\n        Hard iteration cap.\n\n    Returns\n    -------\n    (centres, labels)\n        centres : list[list[float]]\n            k cluster centres rounded to 4 decimals.\n        labels : list[int]\n            Cluster index (0 \u2026 k-1) assigned to every sample.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('data must be a 2-D array')\n    (n_samples, n_features) = data.shape\n    if not 1 <= k <= n_samples:\n        raise ValueError('k must satisfy 1 \u2264 k \u2264 number of samples')\n    data = data.astype(float, copy=False)\n    centres = data[:k].copy()\n    for _ in range(max_iter):\n        dists = np.sum((data[:, None, :] - centres[None, :, :]) ** 2, axis=2)\n        labels = np.argmin(dists, axis=1)\n        new_centres = centres.copy()\n        for j in range(k):\n            mask = labels == j\n            if mask.any():\n                new_centres[j] = data[mask].mean(axis=0)\n            else:\n                rnd_idx = np.random.randint(0, n_samples)\n                new_centres[j] = data[rnd_idx]\n        shifts = np.linalg.norm(new_centres - centres, axis=1)\n        if np.max(shifts) < epsilon:\n            centres = new_centres\n            break\n        centres = new_centres\n    centres_rounded = np.round(centres, 4).tolist()\n    return (centres_rounded, labels.tolist())"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef power_spectrum(frames: np.ndarray, scale: bool=False) -> list[list[float]]:\n    \"\"\"Compute the power spectrum for every frame of a real-valued signal.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray of shape (M, N)\n        Collection of M frames, each containing N real-valued samples.\n    scale : bool, optional (default=False)\n        If True, each spectrum is divided by ``N//2 + 1``.\n\n    Returns\n    -------\n    list[list[float]]\n        Power spectrum (positive frequencies only) for every frame,\n        rounded to four decimal places and converted to a regular Python list.\n    \"\"\"\n    N = frames.shape[1]\n    fft_vals = np.fft.rfft(frames, axis=1)\n    power = np.abs(fft_vals) ** 2\n    if scale:\n        power /= N // 2 + 1\n    return np.round(power, 4).tolist()"}
{"task_id": 13, "completion_id": 0, "solution": "import numpy as np\nfrom collections import defaultdict\ndef knn_recommend(data: np.ndarray, user_ind: int, k: int, criterion: str='cosine') -> list[int]:\n    \"\"\"Item\u2013based k-NN collaborative\u2013filtering recommender.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Rating matrix of shape (n_user, n_item); 0 = *not rated*.\n    user_ind : int\n        Index of the active user.\n    k : int\n        Maximal number of items to recommend.\n    criterion : {'cosine', 'pearson'}, default 'cosine'\n        Similarity measure to use.\n\n    Returns\n    -------\n    list[int]\n        Indices of at most *k* unseen items ordered by decreasing\n        predicted attractiveness.\n    \"\"\"\n    if criterion not in ('cosine', 'pearson'):\n        raise ValueError(\"criterion must be either 'cosine' or 'pearson'\")\n    (n_user, n_item) = data.shape\n    if not 0 <= user_ind < n_user:\n        raise IndexError('user_ind out of bounds')\n    S = np.zeros((n_item, n_item), dtype=np.float64)\n    for i in range(n_item - 1):\n        col_i = data[:, i]\n        r_i_mask = col_i > 0\n        for j in range(i + 1, n_item):\n            col_j = data[:, j]\n            common = r_i_mask & (col_j > 0)\n            if not np.any(common):\n                continue\n            v1 = col_i[common].astype(np.float64)\n            v2 = col_j[common].astype(np.float64)\n            if criterion == 'cosine':\n                if np.std(v1, ddof=1) > 0.001:\n                    v1c = v1 - v1.mean()\n                else:\n                    v1c = v1\n                if np.std(v2, ddof=1) > 0.001:\n                    v2c = v2 - v2.mean()\n                else:\n                    v2c = v2\n                (n1, n2) = (np.linalg.norm(v1c), np.linalg.norm(v2c))\n                sim = 0.0 if n1 == 0.0 or n2 == 0.0 else float(v1c @ v2c / (n1 * n2))\n            elif len(v1) < 2 or np.std(v1, ddof=1) < 0.001 or np.std(v2, ddof=1) < 0.001:\n                sim = 0.0\n            else:\n                sim = float(np.corrcoef(v1, v2, ddof=1)[0, 1])\n                if np.isnan(sim):\n                    sim = 0.0\n            S[i, j] = S[j, i] = sim\n    user_row = data[user_ind]\n    rated_mask = user_row > 0\n    rated_idx = np.where(rated_mask)[0]\n    if len(rated_idx) == n_item:\n        return []\n    scores = {}\n    for t in range(n_item):\n        if rated_mask[t]:\n            continue\n        sims_to_rated = S[t, rated_idx]\n        ratings = user_row[rated_idx]\n        denom = np.sum(np.abs(sims_to_rated))\n        if denom == 0.0:\n            score = 0.0\n        else:\n            score = float(ratings @ sims_to_rated / denom)\n        scores[t] = score\n    sorted_items = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n    return [idx for (idx, _) in sorted_items[:k]]"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef logistic_loss_metrics(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, list[float], list[float]]:\n    \"\"\"Compute binary logistic loss, its gradient and Hessian diagonal.\n\n    Args:\n        y (np.ndarray): Binary ground-truth labels (0 or 1) with shape (n,).\n        y_pred (np.ndarray): Logits (pre-sigmoid scores) with shape (n,).\n\n    Returns:\n        tuple: (mean_loss, gradient, hessian_diagonal) with every value\n               rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    p = 1.0 / (1.0 + np.exp(-y_pred))\n    eps = 1e-15\n    p_clipped = np.clip(p, eps, 1.0 - eps)\n    losses = -(y * np.log(p_clipped) + (1.0 - y) * np.log(1.0 - p_clipped))\n    mean_loss = round(float(np.mean(losses)), 4)\n    grad = p - y\n    grad_rounded = [round(g, 4) for g in grad]\n    hess = p * (1.0 - p)\n    hess_rounded = [round(h, 4) for h in hess]\n    return (mean_loss, grad_rounded, hess_rounded)"}
{"task_id": 16, "completion_id": 0, "solution": "from typing import Any, Iterable, List, Tuple\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    if N < 1:\n        raise ValueError('N must be an integer \u2265 1')\n    seq_len = len(sequence)\n    if N > seq_len:\n        return []\n    return [tuple(sequence[i:i + N]) for i in range(seq_len - N + 1)]"}
{"task_id": 17, "completion_id": 0, "solution": "def build_tree(items: list[str], counts: list[int], parents: list[int]) -> dict:\n    \"\"\"Construct a rooted tree from parallel *items*, *counts* and *parents* lists.\n\n    Parameters\n    ----------\n    items : list[str]\n        The label (item) stored in each node.\n    counts : list[int]\n        The frequency (count) stored in each node.\n    parents : list[int]\n        The parent index of each node; \u20131 marks the single root.\n\n    Returns\n    -------\n    dict\n        The root node represented as a nested dictionary with keys\n        'item', 'count' and 'children'.  If the inputs are empty, returns\n        an empty dictionary.\n    \"\"\"\n    n = len(items)\n    if n == 0:\n        return {}\n    nodes = [{'item': items[i], 'count': counts[i], 'children': []} for i in range(n)]\n    root_idx = None\n    for (i, parent_idx) in enumerate(parents):\n        if parent_idx == -1:\n            root_idx = i\n        else:\n            nodes[parent_idx]['children'].append(nodes[i])\n    return nodes[root_idx]"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape.  If an int is provided, a 1-D array of that\n        length is created.  If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs\n        Supported keyword argument\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is Python float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n    \"\"\"\n    dtype = kwargs.pop('dtype', float)\n    if kwargs:\n        unexpected = ', '.join(kwargs.keys())\n        raise TypeError(f'zero() got unexpected keyword argument(s): {unexpected}')\n    return np.zeros(shape, dtype=dtype)"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef best_split(X: np.ndarray, g: np.ndarray, h: np.ndarray, gamma: float, lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split maximises the reduction in the regularised loss used by\n    gradient-boosting decision trees.  If no legal split gives a positive\n    gain the function returns ``None``.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    if n_samples < 4:\n        return None\n\n    def leaf_loss(G: float, H: float) -> float:\n        return -0.5 * (G * G) / (H + lam) + gamma\n    G_total = g.sum()\n    H_total = h.sum()\n    parent_loss = leaf_loss(G_total, H_total)\n    best_gain = 0.0\n    best_feature = None\n    best_threshold = None\n    for feat_idx in range(n_features):\n        order = np.argsort(X[:, feat_idx], kind='mergesort')\n        x_sorted = X[order, feat_idx]\n        g_sorted = g[order]\n        h_sorted = h[order]\n        g_cumsum = np.cumsum(g_sorted)\n        h_cumsum = np.cumsum(h_sorted)\n        for i in range(1, n_samples - 2):\n            if x_sorted[i] == x_sorted[i + 1]:\n                continue\n            G_left = g_cumsum[i]\n            H_left = h_cumsum[i]\n            G_right = G_total - G_left\n            H_right = H_total - H_left\n            if n_samples - i - 1 < 2:\n                break\n            loss_left = leaf_loss(G_left, H_left)\n            loss_right = leaf_loss(G_right, H_right)\n            gain = parent_loss - loss_left - loss_right\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = feat_idx\n                best_threshold = x_sorted[i]\n    if best_feature is None:\n        return None\n    return (best_feature, float(best_threshold))"}
{"task_id": 20, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    is_scalar = np.isscalar(x)\n    x_arr = np.asarray(x, dtype=float)\n    sigmoid_arr = 1.0 / (1.0 + np.exp(-x_arr))\n    gradient_arr = sigmoid_arr * (1.0 - sigmoid_arr)\n    sigmoid_arr = np.round(sigmoid_arr, 4)\n    gradient_arr = np.round(gradient_arr, 4)\n    if is_scalar:\n        return (float(sigmoid_arr), float(gradient_arr))\n    else:\n        return (sigmoid_arr.tolist(), gradient_arr.tolist())"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\nimport math\nfrom collections import defaultdict\ndef adaboost_1d_predict(x_train: list[float], y_train: list[int], x_test: list[float], epsilon: float=0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble of decision stumps and returns test predictions.\"\"\"\n\n    def stump_predict(x: np.ndarray, d: int, theta: float) -> np.ndarray:\n        \"\"\"Return \u00b11 predictions of the stump (d , \u03b8) on vector x.\"\"\"\n        if d == 0:\n            return np.where(x <= theta, 1, -1)\n        else:\n            return np.where(x > theta, 1, -1)\n    x_train = np.asarray(x_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    x_test = np.asarray(x_test, dtype=float)\n    n = len(x_train)\n    if n == 0:\n        return [1] * len(x_test)\n    order = np.argsort(x_train)\n    x_sorted = x_train[order]\n    thresholds = [x_sorted[0] - 1.0]\n    for i in range(n - 1):\n        if x_sorted[i] != x_sorted[i + 1]:\n            thresholds.append(0.5 * (x_sorted[i] + x_sorted[i + 1]))\n    thresholds.append(x_sorted[-1] + 1.0)\n    stumps = [(d, th) for th in thresholds for d in (0, 1)]\n    w = np.full(n, 1.0 / n)\n    (alphas, chosen_stumps) = ([], [])\n    while True:\n        best_err = float('inf')\n        best_pred = None\n        best_stump = None\n        for (d, th) in stumps:\n            pred = stump_predict(x_train, d, th)\n            err = w[pred != y_train].sum()\n            if err < best_err - 1e-12:\n                (best_err, best_stump, best_pred) = (err, (d, th), pred)\n        if best_err >= 0.5 - 1e-12:\n            break\n        err = max(best_err, 1e-12)\n        alpha = 0.5 * math.log((1.0 - err) / err)\n        chosen_stumps.append(best_stump)\n        alphas.append(alpha)\n        w *= np.exp(-alpha * y_train * best_pred)\n        w = w / w.sum()\n        F = np.zeros(n)\n        for (a, (d, th)) in zip(alphas, chosen_stumps):\n            F += a * stump_predict(x_train, d, th)\n        train_pred = np.where(F >= 0.0, 1, -1)\n        train_error = (train_pred != y_train).mean()\n        if train_error <= epsilon:\n            break\n    if not alphas:\n        return [1] * len(x_test)\n    F_test = np.zeros(len(x_test))\n    for (a, (d, th)) in zip(alphas, chosen_stumps):\n        F_test += a * stump_predict(x_test, d, th)\n    y_pred = np.where(F_test >= 0.0, 1, -1)\n    return y_pred.tolist()"}
{"task_id": 22, "completion_id": 0, "solution": "import numpy as np\ndef random_tensor(shape: tuple[int, ...], standardize: bool=False) -> np.ndarray:\n    \"\"\"Generate a random real-valued tensor and optionally standardize it.\n\n    Each element is produced as\n        offset + u\n    where\n        offset ~ DiscreteUniform{-300, \u2026, 299}\n        u      ~ ContinuousUniform[0, 1)\n\n    Args:\n        shape: Desired shape of the output tensor.\n        standardize: If True, every column (axis-0 slice) is shifted to mean 0\n            and scaled to unit variance.  A tiny constant \u03b5 is added to the\n            denominator to avoid division by zero.\n\n    Returns:\n        A NumPy ndarray containing the generated data.\n    \"\"\"\n    rng = np.random.default_rng()\n    offset = rng.integers(low=-300, high=300, size=shape)\n    u = rng.random(size=shape)\n    data = offset + u\n    if not standardize:\n        return data\n    eps = np.finfo(float).eps\n    if data.ndim == 0:\n        return np.array(0.0, dtype=float)\n    if data.ndim == 1:\n        mean = data.mean()\n        std = data.std()\n        return (data - mean) / (std + eps)\n    mean = data.mean(axis=0, keepdims=True)\n    std = data.std(axis=0, keepdims=True)\n    return (data - mean) / (std + eps)"}
{"task_id": 24, "completion_id": 0, "solution": "import numpy as np\ndef nesterov_update(w, velocity, grad_func, learning_rate=0.001, momentum=0.9):\n    \"\"\"Perform one Nesterov Accelerated Gradient (NAG) update.\"\"\"\n    w = np.asarray(w, dtype=float)\n    if len(velocity) == 0:\n        v = np.zeros_like(w, dtype=float)\n    else:\n        v = np.asarray(velocity, dtype=float)\n    w_tilde = w - momentum * v\n    g = np.asarray(grad_func(w_tilde), dtype=float)\n    g = np.clip(g, -1.0, 1.0)\n    v_new = momentum * v + learning_rate * g\n    w_new = w - v_new\n    w_new = np.round(w_new, 4).tolist()\n    v_new = np.round(v_new, 4).tolist()\n    return (w_new, v_new)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef gaussian_svm_predict(X_train: np.ndarray, y_train: np.ndarray, alpha: np.ndarray, b: float, gamma: float, X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    y_train = y_train.ravel()\n    alpha = alpha.ravel()\n    coef = alpha * y_train\n    train_sq = np.sum(X_train ** 2, axis=1)\n    test_sq = np.sum(X_test ** 2, axis=1)\n    dists_sq = test_sq[:, None] + train_sq[None, :] - 2.0 * X_test @ X_train.T\n    K = np.exp(-gamma * dists_sq)\n    g = K @ coef + b\n    pred = np.sign(g)\n    pred[pred == 0] = 1\n    return pred.astype(int).tolist()"}
{"task_id": 26, "completion_id": 0, "solution": "import numpy as np\ndef relu(x: list | int | float) -> tuple[list, list]:\n    \"\"\"Compute the element-wise ReLU activation and its gradient mask.\n\n    Parameters\n    ----------\n    x : list or nested list (or scalar)\n        Input data of arbitrary (rectangular) shape.\n\n    Returns\n    -------\n    tuple[list, list]\n        `activation, gradient`, both as (nested) Python lists that keep the\n        exact same shape as the input.\n    \"\"\"\n    arr = np.asarray(x, dtype=float)\n    activation = np.maximum(0, arr)\n    gradient = np.where(arr >= 0, 1, 0)\n    return (activation.tolist(), gradient.tolist())"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        (X_hat, mse) where\n            \u2022 X_hat \u2013 rank-k reconstruction of X (each entry rounded to 4 decimals)\n            \u2022 mse    \u2013 mean-squared reconstruction error (rounded to 4 decimals)\n        If k is not in [1, min(m, n)] the function returns -1.\n    \"\"\"\n    if not isinstance(k, int):\n        return -1\n    try:\n        X_arr = np.asarray(X, dtype=float)\n    except Exception:\n        return -1\n    if X_arr.ndim != 2:\n        return -1\n    (m, n) = X_arr.shape\n    if k < 1 or k > min(m, n):\n        return -1\n    (U, S, VT) = np.linalg.svd(X_arr, full_matrices=False)\n    U_k = U[:, :k]\n    S_k = S[:k]\n    VT_k = VT[:k, :]\n    X_hat = U_k * S_k @ VT_k\n    mse = float(np.round(np.mean((X_arr - X_hat) ** 2), 4))\n    X_hat_rounded = np.round(X_hat, 4).tolist()\n    return (X_hat_rounded, mse)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef predict_boosting_tree(x_train: list[float] | np.ndarray, y_train: list[float] | np.ndarray, x_query: float, epsilon: float=0.01) -> float:\n    \"\"\"1-D gradient boosting with decision stumps and prediction for *x_query*.\n    \"\"\"\n    x = np.asarray(x_train, dtype=float).ravel()\n    y = np.asarray(y_train, dtype=float).ravel()\n    if x.size == 0:\n        return float('nan')\n    if np.all(x == x[0]) or x.size == 1:\n        return round(float(y.mean()), 4)\n    order = np.argsort(x)\n    x_sorted = x[order]\n    y_sorted = y[order]\n    distinct_mask = x_sorted[1:] != x_sorted[:-1]\n    mids = (x_sorted[:-1] + x_sorted[1:]) / 2.0\n    candidate_s = mids[distinct_mask]\n    if candidate_s.size == 0:\n        return round(float(y.mean()), 4)\n    residuals = y_sorted.copy()\n    stumps = []\n    rss = np.sum(residuals ** 2)\n    max_iter = 1000\n    iteration = 0\n    while rss > epsilon and iteration < max_iter:\n        best_rss = np.inf\n        best_params = None\n        for s in candidate_s:\n            mask_left = x_sorted <= s\n            mask_right = ~mask_left\n            if not mask_left.any() or not mask_right.any():\n                continue\n            c1 = residuals[mask_left].mean()\n            c2 = residuals[mask_right].mean()\n            new_residuals = residuals - np.where(mask_left, c1, c2)\n            rss_candidate = np.sum(new_residuals ** 2)\n            if rss_candidate < best_rss:\n                best_rss = rss_candidate\n                best_params = (s, c1, c2, mask_left)\n        if best_params is None or best_rss >= rss - 1e-12:\n            break\n        (s_best, c1_best, c2_best, mask_left_best) = best_params\n        stumps.append((s_best, c1_best, c2_best))\n        residuals = residuals - np.where(mask_left_best, c1_best, c2_best)\n        rss = best_rss\n        iteration += 1\n    prediction = 0.0\n    for (s, c1, c2) in stumps:\n        prediction += c1 if x_query <= s else c2\n    return round(float(prediction), 4)"}
{"task_id": 32, "completion_id": 0, "solution": "def check_data(a, b):\n    \"\"\"Validate and convert the two inputs so that both are numeric.\n\n    Accepts ints, floats, or numeric strings.  Any string is converted\n    with `float()`; already-numeric values are left unchanged.\n    Returns a tuple (a, b) in the original order.\n    \"\"\"\n\n    def _convert(x):\n        if isinstance(x, str):\n            return float(x)\n        if isinstance(x, (int, float)):\n            return x\n        return float(x)\n    return (_convert(a), _convert(b))\ndef validate_input(function):\n\n    def wrapper(a, b):\n        (a, b) = check_data(a, b)\n        return function(a, b)\n    return wrapper\n@validate_input\ndef sum_numbers(a, b):\n    \"\"\"Return the arithmetic sum of *a* and *b* after validation.\n\n    If the result is mathematically an integer, return it as an `int`;\n    otherwise return a `float`.\n    \"\"\"\n    result = a + b\n    if isinstance(result, float) and result.is_integer():\n        return int(result)\n    return result"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef random_binary_tensor(shape: tuple[int, ...], sparsity: float=0.5, seed: int | None=None):\n    \"\"\"Create a tensor whose entries are 0.0 or 1.0.\n    \n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired shape of the returned NumPy array.\n    sparsity : float, default=0.5\n        Probability that any individual entry is 1.0.  Must satisfy\n        0.0 \u2264 sparsity \u2264 1.0.  Values outside this range cause the\n        function to return -1.\n    seed : int | None, default=None\n        If provided, ``np.random.seed(seed)`` is called first so the\n        output is reproducible.\n    \n    Returns\n    -------\n    numpy.ndarray | int\n        \u2022 A floating-dtype NumPy array containing only 0.0 and 1.0, if\n          `sparsity` is legal.  \n        \u2022 The integer -1 when `sparsity` is outside the closed interval\n          [0, 1].\n    \"\"\"\n    if not 0.0 <= sparsity <= 1.0:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_values = np.random.rand(*shape)\n    binary_tensor = (random_values < sparsity).astype(float)\n    return binary_tensor"}
{"task_id": 35, "completion_id": 0, "solution": "from typing import Any\nfrom typing import Any\nclass node:\n    \"\"\"A minimal tree node for decision-tree-like structures.\"\"\"\n\n    def __init__(self, fea: int=-1, res: Any | None=None, child: dict | None=None) -> None:\n        self.fea = fea\n        self.res = res\n        self.child = child or {}\ndef classify(root: node, sample: list[Any]) -> Any:\n    \"\"\"Classify *sample* by walking the decision tree rooted at *root*.\n\n    If at any point the traversal cannot continue (missing feature or\n    missing branch), the function returns -1.\n    \"\"\"\n    if root is None:\n        return -1\n    current = root\n    while current.fea != -1:\n        idx = current.fea\n        if idx < 0 or idx >= len(sample):\n            return -1\n        value = sample[idx]\n        if value not in current.child:\n            return -1\n        current = current.child[value]\n    return current.res"}
{"task_id": 36, "completion_id": 0, "solution": "from typing import List, Optional\nclass Node:\n    \"\"\"Binary decision-tree node (provided).\n\n    Non-leaf node: rule  = (feature_index, threshold)\n    Leaf node:     rule  = (None, prediction_value)\n    \"\"\"\n\n    def __init__(self, left: Optional['Node'], right: Optional['Node'], rule: tuple):\n        self.left = left\n        self.right = right\n        self.feature = rule[0]\n        self.threshold = rule[1]\ndef predict_tree(root: Optional[Node], X: List[List[float]]) -> List[float]:\n    \"\"\"\n    Run a binary decision tree on a batch of samples.\n\n    Parameters\n    ----------\n    root : Optional[Node]\n        Root node of the decision tree (may be None for an empty tree).\n    X : List[List[float]]\n        Batch of input samples; one sample = list/sequence of feature values.\n\n    Returns\n    -------\n    List[float]\n        Predictions for each sample, in the same order as `X`.\n    \"\"\"\n    if root is None:\n        return []\n    preds: List[float] = []\n    for sample in X:\n        node = root\n        while True:\n            is_leaf = node.left is None and node.right is None\n            if is_leaf:\n                preds.append(node.threshold)\n                break\n            feat_idx = node.feature\n            if sample[feat_idx] <= node.threshold:\n                node = node.left\n            else:\n                node = node.right\n    return preds"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import cycle\ndef build_kd_tree(data: np.ndarray) -> dict | None:\n    \"\"\"Build a kd-tree from *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features+1).  The first\n        n_features columns contain the point coordinates, the last\n        column an arbitrary label/value.\n\n    Returns\n    -------\n    dict | None\n        Nested dictionary that represents the kd-tree or *None* when\n        *data* is empty.\n    \"\"\"\n    if data.size == 0:\n        return None\n    m: int = data.shape[1] - 1\n    dim_generator = cycle(range(m))\n\n    def _build(subset: np.ndarray) -> dict | None:\n        \"\"\"Recursive helper that uses the *global* ``dim_generator``.\"\"\"\n        if subset.size == 0:\n            return None\n        d = next(dim_generator)\n        order = subset[:, d].argsort(kind='mergesort')\n        sorted_subset = subset[order]\n        n = sorted_subset.shape[0]\n        median_idx = n // 2\n        median_row = sorted_subset[median_idx]\n        left_subset = sorted_subset[:median_idx]\n        right_subset = sorted_subset[median_idx + 1:]\n        node = {'point': median_row[:m].tolist(), 'label': median_row[m], 'left': _build(left_subset), 'right': _build(right_subset)}\n        return node\n    return _build(data)"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\ndef linucb_select_arm(context: np.ndarray, A: list[list[list[float]]], b: list[list[float]], alpha: float) -> int:\n    \"\"\"Select an arm according to the LinUCB rule.\"\"\"\n    n_arms = context.shape[1]\n    scores = np.empty(n_arms, dtype=float)\n    for a in range(n_arms):\n        A_a = np.asarray(A[a], dtype=float)\n        b_a = np.asarray(b[a], dtype=float)\n        c_a = context[:, a]\n        theta_hat = np.linalg.solve(A_a, b_a)\n        A_inv_c = np.linalg.solve(A_a, c_a)\n        exploit = theta_hat @ c_a\n        explore = alpha * np.sqrt(c_a @ A_inv_c)\n        scores[a] = exploit + explore\n    return int(np.argmax(scores))"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median\n          (upper median for even *n*);\n        \u2022 left  is a list of row indices with strictly smaller values;\n        \u2022 right is a list of row indices with strictly larger  values.\n        The two lists are sorted increasingly.\n    \"\"\"\n    col = data[:, d]\n    n = col.shape[0]\n    k = n // 2\n    part_idx = np.argpartition(col, k)\n    pivot = int(part_idx[k])\n    pivot_value = col[pivot]\n    left = []\n    right = []\n    for idx in range(n):\n        if idx == pivot:\n            continue\n        v = col[idx]\n        if v < pivot_value:\n            left.append(idx)\n        elif v > pivot_value:\n            right.append(idx)\n    return (pivot, left, right)"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool=False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into equally sized, possibly overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length N.\n    frame_width : int\n        Number of samples per frame.\n    stride : int\n        Hop size (number of samples between the starts of successive frames).\n    writeable : bool, default=False\n        If False the returned view is marked read-only.\n        If True the view is writeable only when the source array is writeable.\n\n    Returns\n    -------\n    np.ndarray\n        View on *x* with shape (n_frames, frame_width), where\n            n_frames = (len(x) - frame_width) // stride + 1\n    \"\"\"\n    assert x.ndim == 1, 'x must be one-dimensional'\n    assert frame_width > 0, 'frame_width must be a positive integer'\n    assert stride > 0, 'stride must be a positive integer'\n    assert len(x) >= frame_width, 'frame_width larger than signal length'\n    n_frames = (len(x) - frame_width) // stride + 1\n    shape = (n_frames, frame_width)\n    item_stride = x.strides[0]\n    strides = (item_stride * stride, item_stride)\n    view_writeable = x.flags.writeable if writeable else False\n    frames = as_strided(x, shape=shape, strides=strides, writeable=view_writeable)\n    return frames"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef knn_predict(X_data: list[list[float]], y_data: list, X: list[list[float]], k: int=5) -> list:\n    \"\"\"Predict labels for the query samples *X* using the k-Nearest Neighbours rule.\n\n    Parameters\n    ----------\n    X_data : list[list[float]]\n        Training data \u2013 each inner list is a feature vector.\n    y_data : list\n        Labels corresponding to *X_data*.\n    X : list[list[float]]\n        Query samples to classify.\n    k : int, optional\n        Number of neighbours to look at (default is 5).\n\n    Returns\n    -------\n    list\n        Predicted labels for each query sample in *X*.\n    \"\"\"\n    if not X_data:\n        raise ValueError('Training set is empty.')\n    if k <= 0:\n        raise ValueError('k must be a positive integer.')\n    X_train = np.asarray(X_data, dtype=float)\n    X_query = np.asarray(X, dtype=float)\n    n_train = X_train.shape[0]\n    k_eff = min(k, n_train)\n    predictions: list = []\n    for x in X_query:\n        diff = X_train - x\n        dists2 = np.einsum('ij,ij->i', diff, diff)\n        knn_idx = np.argpartition(dists2, k_eff - 1)[:k_eff]\n        neighbour_labels = [y_data[i] for i in knn_idx]\n        counts = Counter(neighbour_labels)\n        max_count = max(counts.values())\n        best_labels = [lbl for (lbl, cnt) in counts.items() if cnt == max_count]\n        predictions.append(min(best_labels))\n    return predictions"}
{"task_id": 42, "completion_id": 0, "solution": "import numpy as np\ndef rmsle(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute the Root Mean Squared Logarithmic Error (RMSLE).\n\n    Parameters\n    ----------\n    actual : list[float]\n        Non-negative ground-truth values.\n    predicted : list[float]\n        Non-negative values predicted by a model.\n\n    Returns\n    -------\n    float\n        The RMSLE rounded to 4 decimal places, or -1 if the input is invalid.\n    \"\"\"\n    if not isinstance(actual, (list, tuple, np.ndarray)) or not isinstance(predicted, (list, tuple, np.ndarray)):\n        return -1\n    if len(actual) == 0 or len(actual) != len(predicted):\n        return -1\n    a = np.asarray(actual, dtype=float)\n    p = np.asarray(predicted, dtype=float)\n    if np.any(a < 0) or np.any(p < 0):\n        return -1\n    log_diff_sq = (np.log1p(p) - np.log1p(a)) ** 2\n    rmsle_val = np.sqrt(np.mean(log_diff_sq))\n    return round(float(rmsle_val), 4)"}
{"task_id": 44, "completion_id": 0, "solution": "import numpy as np\ndef tanh_activation(x, derivative: bool=False) -> list[float]:\n    \"\"\"Compute tanh or its derivative element-wise.\n\n    Args:\n        x: A 1-D list or NumPy array of numbers.\n        derivative: If True, return the gradient of tanh; otherwise return tanh.\n\n    Returns:\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    z = np.asarray(x, dtype=float).ravel()\n    tanh_z = 2.0 / (1.0 + np.exp(-2.0 * z)) - 1.0\n    out = 1.0 - tanh_z ** 2 if derivative else tanh_z\n    return np.round(out, 4).tolist()"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef autocorrelate_1d(x: list | np.ndarray) -> list:\n    \"\"\"Compute the non-negative-lag autocorrelation of a 1-D real signal.\n\n    Args:\n        x: A one-dimensional sequence (Python list or NumPy array) of length N.\n\n    Returns:\n        A Python list [a0, a1, \u2026, a_{N-1}] where\n            a_k = \u03a3_{n=0}^{N-k-1} x_{n+k} * x_n\n        If N == 0 the empty list is returned.\n    \"\"\"\n    N = len(x)\n    if N == 0:\n        return []\n    x_arr = np.asarray(x)\n    result: list = []\n    for k in range(N):\n        s = 0\n        for n in range(N - k):\n            s += x_arr[n + k] * x_arr[n]\n        result.append(s)\n    return result"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef is_stochastic(X: list[list[float]] | 'np.ndarray') -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\"\"\"\n    try:\n        A = np.asarray(X, dtype=float)\n    except Exception:\n        return False\n    if A.ndim != 2 or A.size == 0:\n        return False\n    if (~np.isfinite(A)).any():\n        return False\n    if ((A < 0) | (A > 1)).any():\n        return False\n    if not np.allclose(A.sum(axis=1), 1.0, atol=1e-08):\n        return False\n    return True"}
{"task_id": 49, "completion_id": 0, "solution": "from typing import List, Union\nNumber = Union[int, float]\ndef k_smallest(nums: List[Number], k: int) -> List[Number]:\n    \"\"\"Return the k smallest numbers contained in *nums*.\n\n    Scan *nums* once while maintaining a max-heap (size \u2264 k) implemented\n    with a plain Python list.  At the end, the heap contains exactly the\n    k smallest elements (in no particular order) \u2013 we then sort and\n    return it.\n\n    If *k* is not strictly between 0 and len(nums) (inclusive), return -1.\n\n    The standard ``heapq`` module is **not** used: the helpers\n    ``_move_up`` and ``_move_down`` restore the heap property after\n    insertions and replacements.\n    \"\"\"\n    n = len(nums)\n    if k <= 0 or k > n:\n        return -1\n\n    def _move_up(heap: List[Number], idx: int) -> None:\n        \"\"\"Bubble the element at *idx* up until the max-heap property holds.\"\"\"\n        while idx > 0:\n            parent = (idx - 1) // 2\n            if heap[idx] > heap[parent]:\n                (heap[idx], heap[parent]) = (heap[parent], heap[idx])\n                idx = parent\n            else:\n                break\n\n    def _move_down(heap: List[Number], idx: int) -> None:\n        \"\"\"Push the element at *idx* down until the max-heap property holds.\"\"\"\n        size = len(heap)\n        while True:\n            left = 2 * idx + 1\n            right = left + 1\n            largest = idx\n            if left < size and heap[left] > heap[largest]:\n                largest = left\n            if right < size and heap[right] > heap[largest]:\n                largest = right\n            if largest != idx:\n                (heap[idx], heap[largest]) = (heap[largest], heap[idx])\n                idx = largest\n            else:\n                break\n    heap: List[Number] = []\n    for x in nums:\n        if len(heap) < k:\n            heap.append(x)\n            _move_up(heap, len(heap) - 1)\n        elif x < heap[0]:\n            heap[0] = x\n            _move_down(heap, 0)\n    heap.sort()\n    return heap"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef newton(X: np.ndarray, y: np.ndarray, epsilon: float=1e-06, max_iter: int=1000) -> list[list[float]]:\n    \"\"\"Newton\u2019s method for 2\u2013D linear least\u2013squares.\n\n    Parameters\n    ----------\n    X : (n, 2) array_like\n        Design matrix.\n    y : (n,) or (n, 1) array_like\n        Target vector.\n    epsilon : float, optional\n        Stopping threshold for \u2016\u2207\u2016\u2082.\n    max_iter : int, optional\n        Maximum number of Newton steps.\n\n    Returns\n    -------\n    list[list[float]]\n        2\u00d71 list with each coefficient rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).reshape(-1)\n    n = X.shape[0]\n    w = np.ones(2, dtype=float)\n    H = 2.0 / n * X.T @ X\n    H_inv = np.linalg.pinv(H)\n    for _ in range(max_iter):\n        grad = 2.0 / n * X.T @ (X @ w - y)\n        if np.linalg.norm(grad) < epsilon:\n            break\n        w -= H_inv @ grad\n    return [[round(float(w[0]), 4)], [round(float(w[1]), 4)]]"}
{"task_id": 54, "completion_id": 0, "solution": "import string\ndef tokenize_whitespace(line: str, lowercase: bool=True, filter_stopwords: bool=True, filter_punctuation: bool=True, **kwargs) -> list[str]:\n    \"\"\"Tokenize *line* by whitespace with optional lower-casing, punctuation\n    stripping and stop-word removal.\n\n    Args:\n        line: Input text string.\n        lowercase: If True, convert text to lower-case before tokenisation.\n        filter_stopwords: If True, remove common English stop-words.\n        filter_punctuation: If True, strip ASCII punctuation from each token.\n        **kwargs: Placeholder for future options.\n\n    Returns:\n        List of processed tokens in their original order.\n    \"\"\"\n    if lowercase:\n        line = line.lower()\n    raw_tokens = line.split()\n    processed: list[str] = []\n    punct_table = string.punctuation\n    for token in raw_tokens:\n        if filter_punctuation:\n            token = token.strip(punct_table)\n        if not token:\n            continue\n        if filter_stopwords and token.lower() in _STOP_WORDS:\n            continue\n        processed.append(token)\n    return processed"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef rms_prop(X: np.ndarray, y: np.ndarray, epsilon: float=0.0001, max_iter: int=10000, eta: float=0.01, rho: float=0.9, batch_size: int=32, eps_station: float=1e-08) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    (n, d) = X.shape\n    w = np.zeros(d)\n    s = np.zeros(d)\n    batch_size = n if n < batch_size else batch_size\n    for it in range(max_iter):\n        start = it * batch_size % n\n        idx = np.arange(start, start + batch_size) % n\n        X_b = X[idx]\n        y_b = y[idx]\n        err = X_b @ w - y_b\n        g = X_b.T @ err / len(y_b)\n        s = rho * s + (1.0 - rho) * g ** 2\n        w -= eta * g / (np.sqrt(s) + eps_station)\n        full_grad = X.T @ (X @ w - y) / n\n        if np.linalg.norm(full_grad) < epsilon:\n            break\n    return np.round(w, 4).tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef softplus(x, deriv: bool=False):\n    \"\"\"\n    Numerically\u2013stable SoftPlus activation (and its gradient).\n\n    Parameters\n    ----------\n    x     : scalar, list or np.ndarray\n        Input value(s).\n    deriv : bool, default False\n        If True return the gradient (sigmoid), otherwise SoftPlus.\n\n    Returns\n    -------\n    float or list\n        Rounded result(s) \u2013 float when the input was a scalar,\n        otherwise a Python list.\n    \"\"\"\n    is_scalar = np.isscalar(x)\n    x = np.asarray(x, dtype=np.float64)\n    if deriv:\n        pos_mask = x >= 0\n        neg_mask = ~pos_mask\n        out = np.empty_like(x)\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n        exp_x = np.exp(x[neg_mask])\n        out[neg_mask] = exp_x / (1.0 + exp_x)\n    else:\n        out = np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n    out = np.round(out, 4)\n    if is_scalar:\n        return float(out)\n    else:\n        return out.tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef _count_ngrams(tokens: list[str], n: int) -> Counter:\n    \"\"\"\n    Helper: return a Counter of all length-n n-grams found in *tokens*.\n    Unigrams are counted as the string itself, higher orders as tuples.\n    \"\"\"\n    if n == 1:\n        return Counter(tokens)\n    counts = Counter()\n    for i in range(len(tokens) - n + 1):\n        counts[tuple(tokens[i:i + n])] += 1\n    return counts\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Unsmooth\\xaded MLE N-gram log-probability of *sequence* given *corpus*.\"\"\"\n    if N < 1:\n        raise ValueError('N must be >= 1')\n    if len(sequence) < N:\n        return 0.0\n    if N == 1:\n        unigram_counts = _count_ngrams(corpus, 1)\n        corpus_len = len(corpus)\n    else:\n        ngram_counts = _count_ngrams(corpus, N)\n        prefix_counts = _count_ngrams(corpus, N - 1)\n    total_log_prob = 0.0\n    for i in range(len(sequence) - N + 1):\n        if N == 1:\n            word = sequence[i]\n            numerator = unigram_counts.get(word, 0)\n            denominator = corpus_len\n        else:\n            ngram = tuple(sequence[i:i + N])\n            prefix = ngram[:-1]\n            numerator = ngram_counts.get(ngram, 0)\n            denominator = prefix_counts.get(prefix, 0)\n        if numerator == 0 or denominator == 0:\n            return float('-inf')\n        prob = numerator / denominator\n        total_log_prob += float(np.log(prob))\n    return round(total_log_prob, 4)"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_impurity(y, criterion):\n    \"\"\"Compute a node's impurity.\n\n    Args:\n        y (numpy.ndarray): 1-D array with the target values.\n        criterion (str): One of {\"entropy\", \"gini\", \"mse\"}.\n\n    Returns:\n        float: Impurity value rounded to 4 decimal places.\n    \"\"\"\n    if y.size == 0:\n        return 0.0\n    criterion = criterion.lower()\n    if criterion in ('entropy', 'gini'):\n        (_, counts) = np.unique(y, return_counts=True)\n        p = counts / counts.sum()\n        if criterion == 'entropy':\n            impurity = -np.sum(p * np.log2(p))\n        else:\n            impurity = 1.0 - np.sum(p ** 2)\n    elif criterion == 'mse':\n        mean_y = y.mean()\n        impurity = np.mean((y - mean_y) ** 2)\n    else:\n        raise ValueError(f'Unsupported criterion: {criterion!r}')\n    return round(float(impurity), 4)"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef grad(X, y, W):\n    \"\"\"Return the gradient of the MSE loss for linear regression.\n\n    Parameters\n    ----------\n    X : list[list[float]] | np.ndarray\n        The design matrix with shape (n_samples, n_features).\n    y : list[float] | list[list[float]] | np.ndarray\n        The target vector with shape (n_samples,) or (n_samples, 1).\n    W : list[float] | list[list[float]] | np.ndarray\n        The weight vector with shape (n_features,) or (n_features, 1).\n\n    Returns\n    -------\n    list[list[float]]\n        The gradient vector of shape (n_features, 1), rounded to 4 decimal\n        places, or -1 if the input shapes are incompatible.\n    \"\"\"\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    W_arr = np.asarray(W, dtype=float)\n    if X_arr.ndim != 2:\n        return -1\n    (n_samples, n_features) = X_arr.shape\n    if y_arr.ndim == 1:\n        y_arr = y_arr.reshape(-1, 1)\n    if y_arr.ndim != 2 or y_arr.shape[1] != 1 or y_arr.shape[0] != n_samples:\n        return -1\n    if W_arr.ndim == 1:\n        W_arr = W_arr.reshape(-1, 1)\n    if W_arr.ndim != 2 or W_arr.shape[1] != 1 or W_arr.shape[0] != n_features:\n        return -1\n    residual = X_arr @ W_arr - y_arr\n    gradient = X_arr.T @ residual / n_samples\n    gradient_rounded = np.round(gradient, 4).tolist()\n    return gradient_rounded"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\ndef lasso_regression(X: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray', degree: int, reg_factor: float, n_iterations: int=1000, tol: float=1e-06) -> list[float]:\n    \"\"\"Univariate Lasso regression with polynomial features (coordinate descent).\"\"\"\n    X = np.asarray(X, dtype=float).ravel()\n    y = np.asarray(y, dtype=float).ravel()\n    m = X.size\n    if m == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y.size != m:\n        raise ValueError('X and y must have the same length.')\n    if degree < 0:\n        raise ValueError('degree must be non-negative.')\n    if reg_factor < 0:\n        raise ValueError('reg_factor must be non-negative.')\n    X_poly = np.column_stack([np.ones_like(X)] + [X ** k for k in range(1, degree + 1)])\n    d_plus_1 = degree + 1\n    w = np.zeros(d_plus_1)\n\n    def soft_threshold(rho: float, lam: float) -> float:\n        if rho > lam:\n            return rho - lam\n        if rho < -lam:\n            return rho + lam\n        return 0.0\n    for _ in range(n_iterations):\n        w_old = w.copy()\n        w[0] = np.mean(y - X_poly[:, 1:] @ w[1:])\n        y_minus_pred = None\n        for j in range(1, d_plus_1):\n            x_j = X_poly[:, j]\n            if y_minus_pred is None:\n                y_minus_pred = y - X_poly @ w\n            rho = x_j @ (y_minus_pred + w[j] * x_j)\n            if reg_factor == 0.0:\n                w[j] = rho / (x_j @ x_j)\n            else:\n                w[j] = soft_threshold(rho, reg_factor) / (x_j @ x_j)\n            y_minus_pred = y_minus_pred - (w[j] - w_old[j]) * x_j\n        if np.max(np.abs(w - w_old)) < tol:\n            break\n    return [round(float(coef), 4) for coef in w]"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"\n    Compute the backward-probability vector \u03b2_t for a discrete Hidden Markov\n    Model.\n\n    Parameters\n    ----------\n    A   : transition-probability matrix, shape (N, N)\n    B   : emission-probability matrix, shape (N, M)\n    obs : list with the indices of the observed symbols; length = T\n    t   : time index for which \u03b2_t is required  (0 \u2264 t < T)\n\n    Returns\n    -------\n    list[float]\n        The N backward probabilities \u03b2_t(i) (i = 0 \u2026 N-1), each rounded\n        to 4 decimal places.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    T = len(obs)\n    N = A.shape[0]\n    if not 0 <= t < T:\n        raise ValueError('t must satisfy 0 \u2264 t < len(obs)')\n    beta_next = np.ones(N)\n    for k in range(T - 2, t - 1, -1):\n        emis_beta = B[:, obs[k + 1]] * beta_next\n        beta_curr = A.dot(emis_beta)\n        beta_next = beta_curr\n    return np.round(beta_next, 4).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef dft(frame: np.ndarray, positive_only: bool=True) -> list:\n    \"\"\"Compute the (naive) Discrete Fourier Transform of a 1-D signal.\n\n    Args:\n        frame: 1-D NumPy array (real or complex) containing the samples.\n        positive_only: If True, return only the non-negative-frequency\n            terms (first N//2+1 coefficients).\n\n    Returns\n    -------\n        list[complex]: DFT spectrum rounded to 4 decimals (real and imag\n        parts separately).  Length is N//2+1 when `positive_only` is True,\n        otherwise N.\n    \"\"\"\n    x = np.asarray(frame, dtype=complex).ravel()\n    N = x.size\n    if N == 0:\n        return []\n    coeffs = np.empty(N, dtype=complex)\n    n = np.arange(N)\n    for k in range(N):\n        coeffs[k] = np.sum(x * np.exp(-2j * np.pi * k * n / N))\n    if positive_only:\n        coeffs = coeffs[:N // 2 + 1]\n    rounded = [complex(round(c.real, 4), round(c.imag, 4)) for c in coeffs]\n    return rounded"}
{"task_id": 65, "completion_id": 0, "solution": "import numpy as np\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Args:\n        A: transition matrix (NxN)\n        B: emission   matrix (NxM)\n        pi: initial state distribution (N)\n        obs: observation index sequence (length T)\n\n    Returns:\n        Probability P(O | \u03bb) rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or (not pi) or (not obs):\n        return 0.0\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    pi = np.asarray(pi, dtype=float)\n    obs = np.asarray(obs, dtype=int)\n    N = len(pi)\n    T = len(obs)\n    beta = np.ones(N, dtype=float)\n    for t in range(T - 2, -1, -1):\n        factor = B[:, obs[t + 1]] * beta\n        beta = A.dot(factor)\n    prob = float(np.dot(pi * B[:, obs[0]], beta))\n    return round(prob, 6)"}
{"task_id": 67, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import accumulate\nTOL = 1e-08\ndef _row_valid(row: list[float]) -> bool:\n    \"\"\"\n    True  -> row is a valid probability vector\n    False -> row is invalid\n    \"\"\"\n    if any((p < 0 for p in row)):\n        return False\n    return abs(sum(row) - 1.0) <= TOL\ndef gen_hmm_data(S: list[float], A: list[list[float]], B: list[list[float]], n_sample: int, seed: int | None=None) -> tuple[list[int], list[int]] | int:\n    \"\"\"\n    Generate a sequence of hidden states and observations from a Hidden Markov\n    Model defined by (S, A, B).\n\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Returns (states, observations) or -1 when *any* probability distribution\n    is invalid (i.e. a row does not sum to 1 within 1 \u00d7 10\u207b\u2078 or contains a\n    negative entry).\n    \"\"\"\n    if n_sample < 1:\n        raise ValueError('n_sample must be \u2265 1')\n    N = len(S)\n    if len(A) != N or len(B) != N:\n        return -1\n    if not _row_valid(S):\n        return -1\n    for row in A:\n        if len(row) != N or not _row_valid(row):\n            return -1\n    M = len(B[0])\n    for row in B:\n        if len(row) != M or not _row_valid(row):\n            return -1\n    if seed is not None:\n        np.random.seed(seed)\n    states: list[int] = []\n    observations: list[int] = []\n    curr_state = int(np.random.choice(N, p=S))\n    states.append(curr_state)\n    obs = int(np.random.choice(M, p=B[curr_state]))\n    observations.append(obs)\n    for _ in range(1, n_sample):\n        curr_state = int(np.random.choice(N, p=A[curr_state]))\n        states.append(curr_state)\n        obs = int(np.random.choice(M, p=B[curr_state]))\n        observations.append(obs)\n    return (states, observations)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef sgd_momentum_update(w: np.ndarray, grad: np.ndarray, learning_rate: float=0.01, momentum: float=0.0, prev_update: np.ndarray | None=None) -> tuple[list, list]:\n    \"\"\"Performs one SGD optimisation step with momentum.\n\n    Args:\n        w: Current parameters (NumPy array).\n        grad: Gradient of the loss with respect to *w*.\n        learning_rate: Learning rate controlling the update magnitude.\n        momentum: Momentum factor in the interval [0, 1].\n        prev_update: Previous momentum update / velocity. If ``None`` a zero\n            tensor of the same shape as *w* is used.\n\n    Returns:\n        A tuple ``(updated_w, new_update)`` where each element is converted to\n        a Python ``list`` and rounded to 4 decimal places.\n    \"\"\"\n    if prev_update is None:\n        prev_update = np.zeros_like(w)\n    new_update = momentum * prev_update + (1.0 - momentum) * grad\n    updated_w = w - learning_rate * new_update\n    updated_w_list = np.round(updated_w, 4).tolist()\n    new_update_list = np.round(new_update, 4).tolist()\n    return (updated_w_list, new_update_list)"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef forward_algorithm(S: list[float], A: list[list[float]], B: list[list[float]], observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    n = len(S)\n    if n == 0 or not observations:\n        return -1\n    if len(A) != n or any((len(row) != n for row in A)):\n        return -1\n    if len(B) != n or len(B[0]) == 0:\n        return -1\n    m = len(B[0])\n    if any((len(row) != m for row in B)):\n        return -1\n    for o in observations:\n        if o < 0 or o >= m:\n            return -1\n    S = np.asarray(S, dtype=float)\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    alpha = S * B[:, observations[0]]\n    for ot in observations[1:]:\n        alpha = B[:, ot] * (alpha @ A)\n    likelihood = float(alpha.sum())\n    return round(likelihood, 4)"}
{"task_id": 70, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Optional, Dict, List\nclass _FPNode:\n    \"\"\"A node in an FP-tree.\"\"\"\n    __slots__ = ('item', 'count', 'parent', 'children', 'link')\n\n    def __init__(self, item: Optional[str], parent: Optional['._FPNode']):\n        self.item: Optional[str] = item\n        self.count: int = 0\n        self.parent: Optional['_FPNode'] = parent\n        self.children: Dict[str, _FPNode] = {}\n        self.link: Optional['_FPNode'] = None\ndef _build_fp_tree(transactions: List[List[str]], min_support: int) -> tuple[Optional[_FPNode], Optional[dict]]:\n    \"\"\"\n    Build an FP-tree and the accompanying header table from *transactions*.\n    Returns (root, header_table) or (None, None) if the tree would be empty.\n    The header table maps an item -> [support, head-of-node-link].\n    \"\"\"\n    freq = Counter()\n    for t in transactions:\n        freq.update(t)\n    freq = {item: c for (item, c) in freq.items() if c >= min_support}\n    if not freq:\n        return (None, None)\n    header: dict[str, list] = {i: [c, None] for (i, c) in freq.items()}\n    root = _FPNode(None, None)\n    for transaction in transactions:\n        filtered = [i for i in transaction if i in freq]\n        if not filtered:\n            continue\n        ordered = sorted(filtered, key=lambda x: (-freq[x], x))\n        current = root\n        for item in ordered:\n            if item in current.children:\n                child = current.children[item]\n                child.count += 1\n            else:\n                child = _FPNode(item, current)\n                child.count = 1\n                current.children[item] = child\n                head = header[item][1]\n                if head is None:\n                    header[item][1] = child\n                else:\n                    while head.link is not None:\n                        head = head.link\n                    head.link = child\n            current = child\n    return (root, header)\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings). `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result is deterministic:\n      \u2022 inside each item-set items are sorted alphabetically;\n      \u2022 the outer list is sorted by `(len(itemset), itemset)`.\n    \"\"\"\n    if not transactions or min_support < 1:\n        return []\n    (root, header) = _build_fp_tree(transactions, min_support)\n    if header is None:\n        return []\n    frequent_itemsets: list[list[str]] = []\n    _mine_tree(header, [], frequent_itemsets, min_support)\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    return frequent_itemsets"}
{"task_id": 71, "completion_id": 0, "solution": "def label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count the occurrence of each label in a tabular data set.\n\n    Each sample in *data* is a list whose last element represents the\n    class label.  The function returns a dictionary mapping every\n    unique label to the number of times it appears in the data set.\n\n    Args:\n        data: List of samples (list of lists).  The last element of each\n              inner list is treated as the label.\n\n    Returns:\n        A dictionary where keys are unique labels and values are their\n        integer counts.  If *data* is empty an empty dictionary is\n        returned.\n    \"\"\"\n    counts: dict = {}\n    for sample in data:\n        if not sample:\n            raise ValueError('Each sample must contain at least one element (the label).')\n        label = sample[-1]\n        counts[label] = counts.get(label, 0) + 1\n    return counts"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef variance_reduction(y: np.ndarray, y_left: np.ndarray, y_right: np.ndarray) -> float:\n    \"\"\"\n    Compute the variance reduction obtained by splitting the parent\u2010node targets ``y`` into\n    the two child nodes ``y_left`` and ``y_right`` in a regression tree.\n\n    Variance reduction (VR) is defined as\n        VR = Var(y) - (n_left / n) * Var(y_left) - (n_right / n) * Var(y_right)\n\n    where Var(\u00b7) is the (population) variance computed feature-wise and then summed over\n    all features to yield a single scalar. ``n``, ``n_left`` and ``n_right`` denote the\n    number of samples in the parent and the two children, respectively.\n\n    The function returns the VR rounded to 4 decimal places.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        Target values in the parent node, shape (n_samples, n_targets) or (n_samples,).\n    y_left : np.ndarray\n        Target values that would go to the left child, non-empty.\n    y_right : np.ndarray\n        Target values that would go to the right child, non-empty.\n\n    Returns\n    -------\n    float\n        The variance reduction produced by the split, rounded to 4 decimals.\n    \"\"\"\n\n    def total_variance(arr: np.ndarray) -> float:\n        \"\"\"\n        Population variance summed over all target dimensions.\n        \"\"\"\n        return np.var(arr, axis=0).sum()\n    n = y.shape[0]\n    n_left = y_left.shape[0]\n    n_right = y_right.shape[0]\n    var_parent = total_variance(y)\n    var_left = total_variance(y_left)\n    var_right = total_variance(y_right)\n    vr = var_parent - n_left / n * var_left - n_right / n * var_right\n    return round(float(vr), 4)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef magnitude_spectrum(frames: 'np.ndarray') -> 'list[list[float]]':\n    \"\"\"Compute the positive-frequency magnitude spectrum for each frame.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray\n        Either a 2-D array of shape (M, N) where each row is a frame, or a\n        1-D array that is treated as a single frame of length N.\n\n    Returns\n    -------\n    list[list[float]]\n        The positive-frequency magnitude spectrum of every frame\n        (length N//2+1), rounded to four decimals.\n    \"\"\"\n    frames = np.asarray(frames)\n    if frames.ndim == 1:\n        frames = frames[np.newaxis, :]\n    elif frames.ndim != 2:\n        raise ValueError('Input must be a 1-D or 2-D NumPy array.')\n    spectrum = np.abs(np.fft.rfft(frames, axis=-1))\n    spectrum = np.round(spectrum, 4)\n    return spectrum.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"k-Nearest Neighbour classifier (Euclidean metric).\n\n    Returns a list with the predicted class for every row in `X_test`.\n    If *k* is not a positive integer or *k* > n_train the function\n    returns the scalar -1 (as required by the specification).\n    \"\"\"\n    if not isinstance(k, int) or k <= 0 or k > len(X_train):\n        return -1\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    preds: list[int] = []\n    for x in X_test:\n        diff = X_train - x\n        dists = np.einsum('ij,ij->i', diff, diff)\n        if k == 1:\n            nn_idx = [int(np.argmin(dists))]\n        else:\n            nn_idx = np.argpartition(dists, k - 1)[:k]\n        votes: dict[int, int] = {}\n        for lbl in y_train[nn_idx]:\n            votes[int(lbl)] = votes.get(int(lbl), 0) + 1\n        max_vote = max(votes.values())\n        winner = min((lbl for (lbl, cnt) in votes.items() if cnt == max_vote))\n        preds.append(winner)\n    return preds"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities (same shape).\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    eps = np.finfo(float).eps\n    loss = -np.sum(y * np.log(y_pred + eps))\n    return float(np.round(loss, 4))"}
{"task_id": 77, "completion_id": 0, "solution": "import numpy as np\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]):\n    \"\"\"Forward propagation for an L-layer fully-connected neural network\n    that uses ReLU in all hidden layers and a sigmoid in the output layer.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing weights W1 \u2026 WL and biases b1 \u2026 bL.\n\n    Returns\n    -------\n    tuple[list[list[float]], list]\n        \u2022 AL_list  \u2013 the final activation rounded to 4 decimals\n                     and converted to a regular Python list.\n        \u2022 caches   \u2013 list with one cache per layer (kept for a backward pass).\n    \"\"\"\n\n    def linear_forward(A_prev, W, b):\n        Z = W @ A_prev + b\n        return (Z, (A_prev, W, b))\n\n    def relu(Z):\n        A = np.maximum(0, Z)\n        return (A, Z)\n\n    def sigmoid(Z):\n        A = 1.0 / (1.0 + np.exp(-Z))\n        return (A, Z)\n    caches = []\n    A = X\n    L = len(parameters) // 2\n    for l in range(1, L):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        (Z, lin_cache) = linear_forward(A, W, b)\n        (A, act_cache) = relu(Z)\n        caches.append((lin_cache, act_cache))\n    WL = parameters[f'W{L}']\n    bL = parameters[f'b{L}']\n    (ZL, lin_cache) = linear_forward(A, WL, bL)\n    (AL, act_cache) = sigmoid(ZL)\n    caches.append((lin_cache, act_cache))\n    AL_list = np.round(AL, 4).tolist()\n    return (AL_list, caches)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\ndef adamax_step(params: list[float], grads: list[float], m: list[float], u: list[float], t: int, learning_rate: float=0.002, beta1: float=0.9, beta2: float=0.999, epsilon: float=1e-08) -> tuple[list[float], list[float], list[float], int]:\n    \"\"\"Perform one Adamax update step and return the updated state.\n\n    Args:\n        params:  Current parameter vector.\n        grads:   Current gradient vector (same length as `params`).\n        m:       First-moment estimates.\n        u:       Exponentially weighted infinity-norms.\n        t:       Current time-step counter (starts at 1).\n        learning_rate: Learning rate \u03b7 (default 0.002).\n        beta1:   Exponential decay rate for the first moment \u03b2\u2081.\n        beta2:   Exponential decay rate for the infinity-norm \u03b2\u2082.\n        epsilon: Small constant to avoid division by zero.\n\n    Returns:\n        (new_params, new_m, new_u, new_t) with every float rounded to 6 decimals.\n    \"\"\"\n    params_arr = np.asarray(params, dtype=np.float64)\n    grads_arr = np.asarray(grads, dtype=np.float64)\n    m_arr = np.asarray(m, dtype=np.float64)\n    u_arr = np.asarray(u, dtype=np.float64)\n    m_t = beta1 * m_arr + (1.0 - beta1) * grads_arr\n    u_t = np.maximum(beta2 * u_arr, np.abs(grads_arr))\n    step_size = learning_rate / (1.0 - beta1 ** t)\n    step = step_size * (m_t / (u_t + epsilon))\n    new_params = params_arr - step\n    new_t = t + 1\n    round_params = np.round(new_params, 6).tolist()\n    round_m = np.round(m_t, 6).tolist()\n    round_u = np.round(u_t, 6).tolist()\n    return (round_params, round_m, round_u, new_t)"}
{"task_id": 80, "completion_id": 0, "solution": "import numpy as np\ndef adadelta_update(w: list[float] | np.ndarray, grad_w: list[float] | np.ndarray, rho: float=0.95, eps: float=1e-06) -> list[float]:\n    \"\"\"Perform ONE Adadelta update assuming both running averages are zero.\"\"\"\n    w = np.asarray(w, dtype=np.float64)\n    grad_w = np.asarray(grad_w, dtype=np.float64)\n    one_minus_rho = 1.0 - rho\n    E_grad = one_minus_rho * grad_w ** 2\n    RMS_grad = np.sqrt(E_grad + eps)\n    RMS_delta = np.sqrt(eps)\n    adaptive_lr = RMS_delta / RMS_grad\n    delta_w = adaptive_lr * grad_w\n    E_delta = one_minus_rho * delta_w ** 2\n    w_new = w - delta_w\n    return np.round(w_new, 4).tolist()"}
{"task_id": 81, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n    \n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n    \n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    eps = 1e-15\n    A2 = np.clip(A2, eps, 1.0 - eps)\n    m = Y.size\n    cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n    return float(np.round(cost, 6))"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef epsilon_soft(Q: list[float] | 'np.ndarray', epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft action\u2013selection probabilities.\n\n    Parameters\n    ----------\n    Q : 1-D sequence (list or np.ndarray)\n        Q-values for every available action in the current state.\n    epsilon : float\n        Exploration parameter 0 \u2264 \u03b5 \u2264 1.\n\n    Returns\n    -------\n    list[float]\n        A list with the probability of choosing every action under the \u03b5-soft\n        policy.  All probabilities are rounded to 4 decimal places and sum to\n        1 (within 1\u00d710\u207b\u00b9\u00b2).\n    \"\"\"\n    q = np.asarray(Q, dtype=float).ravel()\n    if q.ndim != 1:\n        raise ValueError('Q must be a one-dimensional sequence of numbers.')\n    if not 0.0 <= epsilon <= 1.0:\n        raise ValueError('epsilon must satisfy 0 \u2264 \u03b5 \u2264 1.')\n    n = q.size\n    if n == 0:\n        raise ValueError('Q must contain at least one action.')\n    greedy_idx = int(np.argmax(q))\n    base_prob = epsilon / n\n    probs = np.full(n, base_prob, dtype=float)\n    probs[greedy_idx] += 1.0 - epsilon\n    ticks = probs * 10000.0\n    int_part = np.floor(ticks).astype(int)\n    remainder = ticks - int_part\n    deficit = 10000 - int_part.sum()\n    if deficit > 0:\n        order = np.argsort(-remainder)\n        int_part[order[:deficit]] += 1\n    elif deficit < 0:\n        order = np.argsort(remainder)\n        int_part[order[:-deficit]] -= 1\n    final_probs = int_part / 10000.0\n    if abs(final_probs.sum() - 1.0) > 1e-12:\n        raise RuntimeError('Probabilities do not sum to one within tolerance.')\n    return final_probs.round(4).tolist()"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef mse_line_search(y: np.ndarray, y_pred: np.ndarray, h_pred: np.ndarray) -> float:\n    \"\"\"Calculate the optimal step size for adding a new learner when optimising\n    the mean-squared-error (MSE) loss.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        True target values.\n    y_pred : np.ndarray\n        Current model predictions.\n    h_pred : np.ndarray\n        Predictions of the new base learner.\n\n    Returns\n    -------\n    float\n        Optimal step size \u03b3 rounded to four decimal places.  If all h_pred are\n        zero (perfect residual fit) the function returns 1.0.\n    \"\"\"\n    residual = y - y_pred\n    numerator = np.dot(residual, h_pred)\n    denominator = np.dot(h_pred, h_pred)\n    if np.isclose(denominator, 0.0):\n        gamma = 1.0\n    else:\n        gamma = numerator / denominator\n    return float(round(gamma, 4))"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef softmax_regression_step(X: np.ndarray, Y: np.ndarray, W: np.ndarray, lr: float) -> list[list[float]]:\n    \"\"\"Perform one gradient\u2013descent update step for multi-class Softmax regression.\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        Input samples of shape (N, D).\n    Y : np.ndarray\n        One-hot encoded labels of shape (N, C).\n    W : np.ndarray\n        Current weight matrix of shape (D, C).\n    lr : float\n        Learning-rate (if 0 \u2192 no update).\n    \n    Returns\n    -------\n    list[list[float]]\n        Updated weight matrix rounded to 4 decimals.\n    \"\"\"\n    if lr == 0 or np.all(X == 0):\n        return np.round(W, 4).tolist()\n    N = X.shape[0]\n    Z = X @ W\n    Z_shift = Z - np.max(Z, axis=1, keepdims=True)\n    exp_Z = np.exp(Z_shift)\n    Y_hat = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n    grad = X.T @ (Y_hat - Y) / N\n    W_new = W - lr * grad\n    return np.round(W_new, 4).tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List, Union\nLabel = Union[int, float, str]\ndef aggregate_random_forest_votes(predictions: List[List[Label]]) -> List[Label]:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions or not predictions[0]:\n        return []\n    n_samples = len(predictions[0])\n    majority_votes: List[Label] = []\n    for idx in range(n_samples):\n        votes = [tree_preds[idx] for tree_preds in predictions]\n        vote_counts = Counter(votes)\n        max_count = max(vote_counts.values())\n        tied_labels = [label for (label, cnt) in vote_counts.items() if cnt == max_count]\n        majority_votes.append(min(tied_labels))\n    return majority_votes"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) that contains the input data,\n           stored column-wise (each column is one example).\n        Y: 2-D NumPy array of shape (n_y, m) that contains the labels,\n           stored column-wise.\n\n    Returns:\n        A tuple (n_x, n_h, n_y) where\n            n_x \u2013 number of input features  (rows of X),\n            n_h \u2013 size of the hidden layer (fixed to 10),\n            n_y \u2013 number of output units    (rows of Y).\n    \"\"\"\n    n_x = X.shape[0]\n    n_h = 10\n    n_y = Y.shape[0]\n    return (n_x, n_h, n_y)"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    is_scalar = np.isscalar(z) or (isinstance(z, np.ndarray) and z.shape == ())\n    z_arr = np.asarray(z, dtype=float)\n    soft = np.logaddexp(0.0, z_arr)\n    soft = np.round(soft, 4)\n    if is_scalar:\n        return float(soft)\n    else:\n        return soft.tolist()"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef linear_activation_forward(A_prev: np.ndarray, W: np.ndarray, b: np.ndarray, activation: str):\n    \"\"\"Forward propagation for a single neural-network layer.\n\n    Parameters\n    ----------\n    A_prev : np.ndarray\n        Activations from the previous layer, shape (n_{l-1}, m).\n    W : np.ndarray\n        Weight matrix for the current layer, shape (n_l, n_{l-1}).\n    b : np.ndarray\n        Bias vector for the current layer, shape (n_l, 1).\n    activation : str\n        Activation to use: \"relu\" or \"sigmoid\".\n\n    Returns\n    -------\n    tuple\n        (A_as_list, (linear_cache, activation_cache))\n        where A_as_list is rounded to 4 decimals and converted to a list.\n    \"\"\"\n    Z = W @ A_prev + b\n    linear_cache = (A_prev, W, b)\n    if activation.lower() == 'relu':\n        A = np.maximum(0, Z)\n    elif activation.lower() == 'sigmoid':\n        A = 1.0 / (1.0 + np.exp(-Z))\n    else:\n        raise ValueError('activation must be either \"relu\" or \"sigmoid\"')\n    assert A.shape == (W.shape[0], A_prev.shape[1]), f'Output shape {A.shape} does not match expected {(W.shape[0], A_prev.shape[1])}'\n    activation_cache = Z\n    A_out = A.round(4).tolist()\n    cache = (linear_cache, activation_cache)\n    return (A_out, cache)"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Any\ndef _get_field(obj: Any, name: str):\n    \"\"\"Return attribute or (key) item `name` from `obj`, else raise KeyError.\"\"\"\n    if hasattr(obj, name):\n        return getattr(obj, name)\n    try:\n        return obj[name]\n    except Exception as exc:\n        raise KeyError(f'{name!r} not found in supplied object.') from exc\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Mean-squared error between a policy\u2019s value estimates and the truth.\"\"\"\n    try:\n        arm_evs = _get_field(bandit, 'arm_evs')\n    except KeyError:\n        raise ValueError('`bandit` must supply an `arm_evs` field.')\n    try:\n        ev_estimates = _get_field(policy, 'ev_estimates')\n    except KeyError:\n        return np.nan\n    if not ev_estimates:\n        return np.nan\n    errors = []\n    for arm_idx in sorted(ev_estimates):\n        if 0 <= arm_idx < len(arm_evs):\n            diff = ev_estimates[arm_idx] - arm_evs[arm_idx]\n            errors.append(diff * diff)\n    if not errors:\n        return np.nan\n    mse_val = float(np.mean(errors))\n    return round(mse_val, 4)"}
{"task_id": 91, "completion_id": 0, "solution": "import numpy as np\ndef relu_backward(dA: list[list[int | float]], activation_cache: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"Backward pass of the ReLU activation function.\n\n    Parameters\n    ----------\n    dA : list[list[int | float]]\n        Upstream gradient from the next layer.\n    activation_cache : list[list[int | float]]\n        Cached pre-activation values (Z) from the forward pass.\n\n    Returns\n    -------\n    list[list[int | float]]\n        Gradient with respect to Z (same shape as dA).\n        Returns -1 if dA and activation_cache do not share the same shape.\n    \"\"\"\n    dA_arr = np.asarray(dA)\n    Z_arr = np.asarray(activation_cache)\n    if dA_arr.shape != Z_arr.shape:\n        return -1\n    dZ_arr = dA_arr * (Z_arr > 0)\n    return dZ_arr.tolist()"}
{"task_id": 92, "completion_id": 0, "solution": "import numbers\ndef is_number(a) -> bool:\n    \"\"\"Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n    \"\"\"\n    return isinstance(a, numbers.Number) and (not isinstance(a, bool))"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU).\n\n    Args\n    ----\n    Z : array-like\n        A NumPy array, Python scalar or (nested) list of numbers representing\n        the pre-activation values.\n\n    Returns\n    -------\n    tuple[list, list]\n        A 2-tuple ``(A, cache)`` where\n            \u2022 A     \u2013 ReLU(Z)  (all negatives set to 0),\n            \u2022 cache \u2013 the original input Z,\n        both converted to plain Python (sub)lists via ``tolist()`` so the\n        result is JSON-serialisable.\n    \"\"\"\n    Z_arr = np.asarray(Z)\n    A_arr = np.maximum(0, Z_arr)\n    return (A_arr.tolist(), Z_arr.tolist())"}
{"task_id": 94, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Hashable, Sequence, TypeVar\nLabel = TypeVar('Label', bound=Hashable)\ndef knn_majority_vote(neighbors_targets: Sequence[Label]) -> Label:\n    \"\"\"Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A non-empty sequence containing the class labels of\n            the k nearest neighbours.\n\n    Returns:\n        The label that appears most frequently.  In case of a tie, the smallest\n        label according to Python\u2019s default ordering is returned.\n\n    Raises:\n        ValueError: If *neighbors_targets* is empty.\n    \"\"\"\n    if not neighbors_targets:\n        raise ValueError('neighbors_targets must contain at least one label.')\n    counts = Counter(neighbors_targets)\n    top_freq = max(counts.values())\n    top_labels = (label for (label, freq) in counts.items() if freq == top_freq)\n    return min(top_labels)"}
{"task_id": 96, "completion_id": 0, "solution": "import math\nfrom collections import Counter, defaultdict\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float=1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\"\"\"\n    if N <= 0:\n        raise ValueError('N must be a positive integer.')\n    seq_tokens = sequence.split()\n    if len(seq_tokens) < N:\n        return 0.0\n    vocab = set()\n    ngram_counts: dict[int, Counter] = {n: Counter() for n in range(1, N + 1)}\n    for sent in corpus:\n        tokens = sent.split()\n        vocab.update(tokens)\n        for n in range(1, N + 1):\n            if len(tokens) < n:\n                continue\n            for i in range(len(tokens) - n + 1):\n                ngram = tuple(tokens[i:i + n])\n                ngram_counts[n][ngram] += 1\n    vocab.add('<UNK>')\n    V = len(vocab)\n    total_tokens = sum(ngram_counts[1].values())\n    seq_tokens = [tok if tok in vocab else '<UNK>' for tok in seq_tokens]\n    log_prob = 0.0\n    for i in range(len(seq_tokens) - N + 1):\n        ngram = tuple(seq_tokens[i:i + N])\n        context = ngram[:-1]\n        target = ngram[-1]\n        count_ngram = ngram_counts[N].get(ngram, 0)\n        if N == 1:\n            context_count = total_tokens\n        else:\n            context_count = ngram_counts[N - 1].get(context, 0)\n        prob = (count_ngram + K) / (context_count + K * V)\n        log_prob += math.log(prob)\n    return round(log_prob, 4)"}
{"task_id": 97, "completion_id": 0, "solution": "import numpy as np\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input data that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        0.0\\u2006/\\u20061.0 with the same layout as *Z* (float if *Z* is a scalar).\n    \"\"\"\n    if np.isscalar(Z) or (isinstance(Z, np.ndarray) and Z.ndim == 0):\n        return 1.0 if float(Z) > 0.0 else 0.0\n    arr = np.asarray(Z)\n    derivative = (arr > 0).astype(float)\n    return derivative.tolist()"}
{"task_id": 99, "completion_id": 0, "solution": "import numpy as np\ndef relu_(Z):\n    \"\"\"Apply the element-wise Rectified Linear Unit (ReLU) activation.\n\n    The returned object keeps the *same container type* as the input:\n    scalar \u2192 scalar, list \u2192 list / list-of-lists, NumPy array \u2192 NumPy array.\n\n    Args\n    ----\n    Z : int | float | list | numpy.ndarray\n        Scalar, 1-D or 2-D data on which ReLU is to be applied.\n\n    Returns\n    -------\n    Same type as *Z* with all negative entries replaced by 0.\n    \"\"\"\n    if isinstance(Z, np.ndarray):\n        return np.maximum(Z, 0)\n    if isinstance(Z, (int, float, np.number)):\n        return max(Z, 0)\n    if isinstance(Z, list):\n        return np.maximum(np.asarray(Z), 0).tolist()\n    raise TypeError('relu_ only accepts int, float, list, or numpy.ndarray as input.')"}
{"task_id": 100, "completion_id": 0, "solution": "import numpy as np\ndef classification_error(actual, predicted):\n    \"\"\"Compute the proportion of incorrect predictions.\n\n    Parameters\n    ----------\n    actual : list | tuple | numpy.ndarray\n        Ground-truth labels.\n    predicted : list | tuple | numpy.ndarray\n        Labels predicted by a classifier.\n\n    Returns\n    -------\n    float | int\n        Classification error rate rounded to four decimals,\n        or \u22121 when the input lengths are not identical.\n    \"\"\"\n    actual_arr = np.asarray(actual).ravel()\n    predicted_arr = np.asarray(predicted).ravel()\n    if actual_arr.size != predicted_arr.size:\n        return -1\n    total = actual_arr.size\n    if total == 0:\n        return 0.0\n    mismatches = np.count_nonzero(actual_arr != predicted_arr)\n    error_rate = round(mismatches / total, 4)\n    return error_rate"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef softmax(x: np.ndarray, axis: int=1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Axis along which to apply the softmax (negative values allowed).\n\n    Returns:\n        A (nested) Python list containing the soft-maxed probabilities,\n        each rounded to four decimal places.\n\n    Raises:\n        ValueError: If ``x`` is 1-D.\n    \"\"\"\n    if x.ndim == 1:\n        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n    max_along_axis = np.max(x, axis=axis, keepdims=True)\n    shifted = x - max_along_axis\n    exp_shifted = np.exp(shifted)\n    sum_exp = np.sum(exp_shifted, axis=axis, keepdims=True)\n    softmax_vals = exp_shifted / sum_exp\n    softmax_vals = np.round(softmax_vals, 4)\n    return softmax_vals.tolist()"}
{"task_id": 102, "completion_id": 0, "solution": "import numpy as np\ndef polynomial_regression_predict(x: list[float], y: list[float], degree: int, x_pred: list[float]) -> list[float]:\n    \"\"\"Fit a polynomial regression model and return predictions.\n\n    Parameters\n    ----------\n    x : list[float]\n        Training input values.\n    y : list[float]\n        Training target values.\n    degree : int\n        Degree of the polynomial to be fitted (must be >= 0).\n    x_pred : list[float]\n        Values at which the fitted model should be evaluated.\n\n    Returns\n    -------\n    list[float] | int\n        Predicted values for *x_pred* rounded to 4 decimal places, or -1 if\n        the model cannot be fitted (e.g. wrong input, not enough data).\n    \"\"\"\n    if not isinstance(degree, int) or degree < 0:\n        return -1\n    if len(x) != len(y) or len(x) == 0:\n        return -1\n    if len(x) < degree + 1:\n        return -1\n    X = np.vander(np.asarray(x, dtype=float), N=degree + 1, increasing=True)\n    y_vec = np.asarray(y, dtype=float)\n    try:\n        (w, *_) = np.linalg.lstsq(X, y_vec, rcond=None)\n    except Exception:\n        return -1\n    if len(x_pred) == 0:\n        return []\n    X_pred = np.vander(np.asarray(x_pred, dtype=float), N=degree + 1, increasing=True)\n    y_pred = X_pred @ w\n    return [round(float(val), 4) for val in y_pred]"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef _entropy(labels: np.ndarray) -> float:\n    \"\"\"Base-2 entropy of a 1-D label array.\"\"\"\n    n = labels.size\n    if n == 0:\n        return 0.0\n    (_, counts) = np.unique(labels, return_counts=True)\n    probs = counts / n\n    return -np.sum(probs * np.log2(probs))\ndef information_gain(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Computes the information gain of a proposed split.\n\n    Args:\n        y: 1-D NumPy array containing the class labels of the parent node.\n        splits: List of 1-D NumPy arrays. Each array contains the labels that\n                fall into one child node produced by the split.\n\n    Returns:\n        Information gain rounded to 4 decimal places.\n    \"\"\"\n    parent_entropy = _entropy(y)\n    n_parent = y.size\n    weighted_child_entropy = 0.0\n    for child in splits:\n        weight = child.size / n_parent if n_parent else 0.0\n        weighted_child_entropy += weight * _entropy(child)\n    ig = parent_entropy - weighted_child_entropy\n    ig = max(ig, 0.0)\n    return round(float(ig), 4)"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_pdf(x, mean, sigma):\n    \"\"\"Compute the Gaussian probability density for each element in *x*.\n\n    The function returns a list of floats rounded to 5 decimal places. If *sigma*\n    is not strictly positive the function must return -1.\n\n    Args:\n        x (list | tuple | np.ndarray | float | int): Values at which to evaluate the PDF.\n        mean (float | int): Distribution mean (\u03bc).\n        sigma (float | int): Distribution standard deviation (\u03c3).\n\n    Returns:\n        list[float] | int: The PDF values or -1 when \u03c3 \u2264 0.\n    \"\"\"\n    if sigma <= 0:\n        return -1\n    arr = np.asarray(x, dtype=float)\n    if arr.size == 0:\n        return []\n    coeff = 1.0 / (np.sqrt(2.0 * np.pi) * sigma)\n    exponent = -(arr - mean) ** 2 / (2.0 * sigma ** 2)\n    pdf_vals = coeff * np.exp(exponent)\n    pdf_vals = np.round(pdf_vals, 5)\n    if pdf_vals.ndim == 0:\n        return [float(pdf_vals)]\n    return pdf_vals.ravel().tolist()"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef adam_step(theta: np.ndarray, grad: np.ndarray, m_prev: np.ndarray, v_prev: np.ndarray, t: int, alpha: float=0.01, beta1: float=0.9, beta2: float=0.99, epsilon: float=1e-09) -> tuple[list, list, list, int]:\n    \"\"\"Perform one iteration of the Adam optimisation algorithm.\n\n    Returns\n    -------\n    tuple[list, list, list, int]\n        (updated parameters, new first-moment, new second-moment, next time-step).\n        All numerical arrays are rounded to 6 decimal places and converted to\n        native Python lists.\n    \"\"\"\n    m_t = beta1 * m_prev + (1.0 - beta1) * grad\n    v_t = beta2 * v_prev + (1.0 - beta2) * grad ** 2\n    m_hat = m_t / (1.0 - beta1 ** t)\n    v_hat = v_t / (1.0 - beta2 ** t)\n    theta_new = theta - alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n    theta_out = np.round(theta_new, 6).tolist()\n    m_out = np.round(m_t, 6).tolist()\n    v_out = np.round(v_t, 6).tolist()\n    return (theta_out, m_out, v_out, t + 1)"}
{"task_id": 108, "completion_id": 0, "solution": "import numpy as np\ndef als_factorization(X: np.ndarray, K: int, alpha: float=1.0, max_iter: int=200, tol: float=0.0001) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=np.float64)\n    (N, M) = X.shape\n    rng = np.random.RandomState(0)\n    W = rng.randn(N, K)\n    H = rng.randn(K, M)\n    I_K = np.eye(K, dtype=np.float64)\n\n    def _solve(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n        \"\"\"Solve AX = B for X with fallback to pseudo-inverse if singular.\"\"\"\n        try:\n            return np.linalg.solve(A, B)\n        except np.linalg.LinAlgError:\n            return np.linalg.pinv(A) @ B\n    (best_W, best_H) = (W.copy(), H.copy())\n    best_loss = np.inf\n    for _ in range(max_iter):\n        HHT = H @ H.T + alpha * I_K\n        W = _solve(HHT, H @ X.T).T\n        WTW = W.T @ W + alpha * I_K\n        H = _solve(WTW, W.T @ X)\n        diff = X - W @ H\n        loss = np.sum(diff ** 2) + alpha * (np.sum(W ** 2) + np.sum(H ** 2))\n        if loss < best_loss:\n            best_loss = loss\n            (best_W, best_H) = (W.copy(), H.copy())\n        if loss <= tol:\n            break\n    X_hat = (best_W @ best_H).round(4).tolist()\n    return X_hat"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef _euclidean_squared(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"Squared Euclidean distance between every row of `a` and `b` (broadcast).\"\"\"\n    return ((a[:, None, :] - b[None, :, :]) ** 2).sum(axis=2)\ndef _kmeans_pp_init(X: np.ndarray, K: int) -> np.ndarray:\n    \"\"\"Return K initial centroids picked with the K-Means++ heuristic.\"\"\"\n    m = X.shape[0]\n    centroids = [X[np.random.randint(m)]]\n    for _ in range(1, K):\n        d2 = _euclidean_squared(X, np.array(centroids)).min(axis=1)\n        probs = d2 / d2.sum()\n        r = random.random()\n        cumulative = np.cumsum(probs)\n        idx = np.searchsorted(cumulative, r)\n        centroids.append(X[idx])\n    return np.array(centroids)\ndef kmeans(X: np.ndarray, K: int, max_iter: int=100, random_state: int | None=None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\"\"\"\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n    X = np.asarray(X, dtype=float)\n    (m, n) = X.shape\n    if K <= 0 or K > m:\n        raise ValueError('K must be a positive integer \u2264 number of samples.')\n    centroids = _kmeans_pp_init(X, K)\n    labels = np.full(m, -1, dtype=int)\n    for _ in range(max_iter):\n        distances = _euclidean_squared(X, centroids)\n        new_labels = distances.argmin(axis=1)\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n        for k in range(K):\n            mask = labels == k\n            if mask.any():\n                centroids[k] = X[mask].mean(axis=0)\n    centroids_rounded = [[round(float(v), 4) for v in centroid] for centroid in centroids]\n    centroids_sorted = sorted(centroids_rounded, key=lambda c: (c[0], c))\n    return centroids_sorted"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\nTIME_STEPS = 20\nPAD_TOKEN = 0\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"\n    Encode a raw string into a fixed-length list of integer ids.\n\n    \u2022 Each character is looked up in `vocabulary`.\n    \u2022 Unknown characters map to PAD_TOKEN (0).\n    \u2022 The returned list is exactly `time_steps` long:\n        \u2013 shorter input  \u2192 pad with 0 on the right\n        \u2013 longer  input  \u2192 truncate on the right\n    \"\"\"\n    encoded = [vocabulary.get(ch, PAD_TOKEN) for ch in text[:time_steps]]\n    if len(encoded) < time_steps:\n        encoded.extend([PAD_TOKEN] * (time_steps - len(encoded)))\n    return encoded\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"\n    Decode a sequence of integer ids back into a string.\n\n    \u2022 Works with Python lists, tuples or NumPy arrays.\n    \u2022 Padding / unknown tokens (id 0) are ignored.\n    \"\"\"\n    indices = np.ravel(indices)\n    chars = []\n    for idx in indices:\n        if idx == PAD_TOKEN:\n            continue\n        char = inverse_vocab.get(idx)\n        if char is not None:\n            chars.append(char)\n    return ''.join(chars)\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"\n    End-to-end helper:\n    1. encode text\n    2. feed the single example to `model.predict`\n    3. take arg-max over the last axis\n    4. decode the predicted ids back to text\n    \"\"\"\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    batch = np.array(encoded, dtype=np.int64)[np.newaxis, :]\n    logits = model.predict(batch)\n    pred_ids = np.argmax(logits, axis=-1)\n    decoded = int_to_string(pred_ids[0], inv_output_vocabulary)\n    return decoded"}
{"task_id": 112, "completion_id": 0, "solution": "import re\nfrom collections import Counter\nclass Token:\n\n    def __init__(self, word):\n        self.count = 0\n        self.word = word\n\n    def __repr__(self):\n        return \"Token(word='{}', count={})\".format(self.word, self.count)\ndef tokenize_and_count(text: str) -> list[Token]:\n    \"\"\"Convert *text* into a list of Token objects with their frequencies.\"\"\"\n    words = re.findall('[A-Za-z]+', text)\n    if not words:\n        return []\n    words = [w.lower() for w in words]\n    freq = Counter(words)\n    sorted_words = sorted(freq.items(), key=lambda item: (-item[1], item[0]))\n    tokens: list[Token] = []\n    for (word, count) in sorted_words:\n        tok = Token(word)\n        tok.count = count\n        tokens.append(tok)\n    return tokens"}
{"task_id": 113, "completion_id": 0, "solution": "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str] or None\n        A collection of input strings.  If *None*, the function uses the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        try:\n            examples = EXAMPLES\n        except NameError:\n            raise ValueError('No `examples` provided and the global constant `EXAMPLES` is not defined.')\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        predicted_str = ''.join(output_chars)\n        print(f'input:  {example}')\n        print(f'output: {predicted_str}')\n        predictions.append(predicted_str)\n    return predictions"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef selu(x: np.ndarray, derivative: bool=False) -> list:\n    \"\"\"Scaled Exponential Linear Unit (SELU).\n\n    Applies SELU activation or its derivative element-wise to *x*.\n\n    Args:\n        x: A NumPy ndarray (or array-like) containing real values.\n        derivative: If False (default) returns SELU(x);\n                    if True  returns d(SELU)/dx.\n\n    Returns:\n        A (nested) Python list with the same shape as *x*, with every value\n        rounded to six decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772\n    lam = 1.0507009873554805\n    x = np.asarray(x, dtype=float)\n    if derivative:\n        out = np.where(x >= 0, lam, lam * alpha * np.exp(x))\n    else:\n        out = np.where(x >= 0, lam * x, lam * alpha * (np.exp(x) - 1))\n    return np.round(out, 6).tolist()"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Binary cross-entropy loss and gradient for logistic regression.\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (m, n)\n    y : np.ndarray\n        Binary targets of shape (m,) or (m, 1)\n    w : np.ndarray\n        Weight vector of shape (n,) or (n, 1)\n    \n    Returns\n    -------\n    tuple\n        (loss, gradient) where\n          \u2022 loss is a float rounded to 4 decimals\n          \u2022 gradient is a nested list (shape (n, 1)) whose\n            elements are rounded to 4 decimals\n    \"\"\"\n    y = y.reshape(-1)\n    w = w.reshape(-1)\n    m = X.shape[0]\n    z = X @ w\n    p = 1 / (1 + np.exp(-z))\n    eps = 1e-20\n    p = np.clip(p, eps, 1 - eps)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    loss = float(np.round(loss, 4))\n    grad = X.T @ (p - y) / m\n    grad = grad.reshape(-1, 1)\n    grad = np.round(grad, 4).tolist()\n    return (loss, grad)"}
{"task_id": 116, "completion_id": 0, "solution": "import numpy as np\ndef mse_criterion(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Reduction in mean\u2013squared error obtained by a split.\n\n    Args:\n        y: 1-D array of original target values.\n        splits: List with one 1-D array per child node after the split.\n\n    Returns:\n        Parent MSE minus weighted child MSE, rounded to 4 decimals.\n    \"\"\"\n    if y.size == 0 or not splits:\n        return 0.0\n    y_mean = y.mean()\n    mse_parent = np.mean((y - y_mean) ** 2)\n    N = y.size\n    mse_children = 0.0\n    for child in splits:\n        n_child = child.size\n        if n_child == 0:\n            continue\n        child_mean = child.mean()\n        child_mse = np.mean((child - child_mean) ** 2)\n        mse_children += n_child / N * child_mse\n    delta = mse_parent - mse_children\n    return round(float(delta), 4)"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array (m, n) with train features.\n        y_train: 1-D NumPy array (m,) with labels **-1** or **1**.\n        X_test : 2-D NumPy array (k, n) with test features.\n        n_clf  : Number of weak classifiers (\u22651).\n\n    Returns\n        Python list (length k) with predicted labels (-1 or 1).\n    \"\"\"\n\n    def _best_stump(X, y, w):\n        \"\"\"Find the decision stump that minimises weighted error.\"\"\"\n        (m, n_features) = X.shape\n        best_err = np.inf\n        stump = {}\n        best_pred = None\n        for j in range(n_features):\n            Xj = X[:, j]\n            uniq = np.unique(Xj)\n            if uniq.size == 1:\n                thresholds = [uniq[0] - 1e-10, uniq[0] + 1e-10]\n            else:\n                thresholds = list((uniq[:-1] + uniq[1:]) / 2.0)\n                thresholds.append(uniq[0] - 1e-10)\n                thresholds.append(uniq[-1] + 1e-10)\n            for thresh in thresholds:\n                for polarity in (1, -1):\n                    preds = np.ones(m, dtype=int)\n                    if polarity == 1:\n                        preds[Xj < thresh] = -1\n                    else:\n                        preds[Xj < thresh] = 1\n                    err = np.sum(w[preds != y])\n                    if err < best_err:\n                        best_err = err\n                        stump = {'feature': j, 'threshold': thresh, 'polarity': polarity}\n                        best_pred = preds\n        return (stump, best_err, best_pred)\n    if n_clf < 1:\n        n_clf = 1\n    y_train = y_train.astype(int)\n    m = X_train.shape[0]\n    weights = np.full(m, 1 / m, dtype=float)\n    stumps = []\n    eps = 1e-10\n    for _ in range(n_clf):\n        (stump, err, preds) = _best_stump(X_train, y_train, weights)\n        err = np.clip(err, eps, 1 - eps)\n        alpha = 0.5 * np.log((1 - err) / err)\n        weights *= np.exp(-alpha * y_train * preds)\n        weights /= weights.sum()\n        stump['alpha'] = alpha\n        stumps.append(stump)\n    k = X_test.shape[0]\n    agg = np.zeros(k)\n    for stump in stumps:\n        j = stump['feature']\n        thresh = stump['threshold']\n        pol = stump['polarity']\n        pred = np.ones(k, dtype=int)\n        if pol == 1:\n            pred[X_test[:, j] < thresh] = -1\n        else:\n            pred[X_test[:, j] < thresh] = 1\n        agg += stump['alpha'] * pred\n    final_pred = np.sign(agg)\n    final_pred[final_pred == 0] = 1\n    return final_pred.astype(int).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef sgd_update(params: list, grads: list, alpha: float=0.01) -> list:\n    \"\"\"One step of Stochastic Gradient Descent.\n\n    Parameters\n    ----------\n    params : list[np.ndarray]\n        Current model parameters.\n    grads : list[np.ndarray]\n        Corresponding gradients.\n    alpha : float, optional\n        Learning-rate, by default 0.01.\n\n    Returns\n    -------\n    list\n        A list of updated parameters converted to native Python lists, with all\n        values rounded to four decimal places.  If the inputs are invalid\n        (empty or of different lengths) an empty list is returned.\n    \"\"\"\n    if not params or not grads or len(params) != len(grads):\n        return []\n    updated_params = []\n    for (p, g) in zip(params, grads):\n        p_arr = np.asarray(p, dtype=float)\n        g_arr = np.asarray(g, dtype=float)\n        new_p = p_arr - alpha * g_arr\n        updated_params.append(np.round(new_p, 4).tolist())\n    return updated_params"}
{"task_id": 120, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef linear_regression(X: list[list[int | float]], y: list[int | float], n_iterations: int, learning_rate: float) -> list[float]:\n    \"\"\"Learn linear regression weights with batch gradient descent.\n\n    Args:\n        X: Training samples \u2013 list of lists with numerical feature values.\n        y: Target values \u2013 list of numbers, same length as X.\n        n_iterations: Number of gradient-descent steps to perform.\n        learning_rate: Positive learning rate controlling the step size.\n\n    Returns:\n        The learned weight vector as a list (bias first) rounded to 4 decimals.\n        If the input dimensions are incompatible the function returns -1.\n    \"\"\"\n    if len(X) == 0 or len(X) != len(y):\n        return -1\n    X_np = np.asarray(X, dtype=float)\n    if X_np.ndim == 1:\n        X_np = X_np.reshape(-1, 1)\n    m = X_np.shape[0]\n    ones = np.ones((m, 1))\n    X_aug = np.hstack((ones, X_np))\n    y_np = np.asarray(y, dtype=float).reshape(-1, 1)\n    N = X_aug.shape[1]\n    limit = 1 / math.sqrt(N)\n    w = np.random.uniform(-limit, limit, size=(N, 1))\n    lr = float(learning_rate)\n    for _ in range(max(0, int(n_iterations))):\n        preds = X_aug @ w\n        errors = preds - y_np\n        gradient = X_aug.T @ errors / m\n        w -= lr * gradient\n    w_rounded = np.round(w.flatten(), 4)\n    return w_rounded.tolist()"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_weights(X: list[list[int | float]], y: list[int | float]) -> list[float]:\n    \"\"\"Return the ordinary least-squares weight vector for Linear Regression.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        2-D list where each inner list contains the feature values for one sample.\n    y : list[int | float]\n        1-D list with the target value corresponding to each sample.\n\n    Returns\n    -------\n    list[float]\n        Weight vector `[w0, w1, \u2026, wd]` rounded to 4 decimal places, where\n        `w0` is the intercept term.\n    \"\"\"\n    X_np = np.asarray(X, dtype=float)\n    y_np = np.asarray(y, dtype=float).ravel()\n    if X_np.shape[0] != y_np.shape[0]:\n        raise ValueError('X and y must contain the same number of samples.')\n    ones = np.ones((X_np.shape[0], 1), dtype=float)\n    X_aug = np.hstack([ones, X_np])\n    w = np.linalg.pinv(X_aug) @ y_np\n    return np.round(w, 4).tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "import numpy as np\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to one-hot encoded format.\n\n    The distinct categories are detected automatically, sorted, and each\n    element of *y* is converted to a binary vector that has length equal to\n    the number of unique categories.\n\n    Args:\n        y: A one-dimensional NumPy array (or array-like) of integers or\n           strings representing categorical data.\n\n    Returns:\n        A list of lists containing 0/1 integers \u2013 the one-hot encoded matrix.\n    \"\"\"\n    arr = np.asarray(y)\n    if arr.ndim != 1:\n        raise ValueError('Input must be a one-dimensional array or list.')\n    categories = sorted(set(arr.tolist()))\n    n_classes = len(categories)\n    cat2idx = {cat: idx for (idx, cat) in enumerate(categories)}\n    one_hot = [[0] * n_classes for _ in range(len(arr))]\n    for (row_idx, value) in enumerate(arr):\n        col_idx = cat2idx[value]\n        one_hot[row_idx][col_idx] = 1\n    return one_hot"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef polynomial_kernel(X: list[list[int | float]], Y: list[list[int | float]] | None=None, d: int=3, gamma: float | None=None, c0: float=1) -> list[list[float]]:\n    \"\"\"Compute the degree-d polynomial kernel between all rows of *X* and *Y*.\n    \n    Args\n    ----\n    X : list of list of numbers\n        First data matrix with shape (N, C).\n    Y : list of list of numbers or None, optional\n        Second data matrix with shape (M, C).  If *None* defaults to *X*.\n    d : int, default=3\n        Degree of the polynomial.\n    gamma : float or None, default=None\n        Scale factor.  Uses 1/C when *None*.\n    c0 : float, default=1\n        Bias (independent) term.\n    \n    Returns\n    -------\n    list of list of floats\n        The (N\u00d7M) Gram matrix rounded to 4 decimals.\n    \"\"\"\n    X_arr = np.asarray(X, dtype=np.float64)\n    Y_arr = X_arr if Y is None else np.asarray(Y, dtype=np.float64)\n    if X_arr.ndim != 2 or Y_arr.ndim != 2:\n        raise ValueError('X and Y must be 2-D matrices.')\n    if X_arr.shape[1] != Y_arr.shape[1]:\n        raise ValueError('X and Y must have the same number of columns/features.')\n    (_, C) = X_arr.shape\n    if gamma is None:\n        gamma = 1.0 / C\n    dot_products = X_arr @ Y_arr.T\n    base = gamma * dot_products + c0\n    K = np.power(base, d)\n    K_rounded = np.round(K, 4)\n    return K_rounded.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "def confusion_matrix(y_true: list, y_pred: list) -> list:\n    \"\"\"Build a confusion matrix for the given true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : list\n        Ground-truth class labels.\n    y_pred : list\n        Predicted class labels. Must have the same length as `y_true`.\n\n    Returns\n    -------\n    list\n        2-D list representing the confusion matrix.  The element at row *i*\n        and column *j* is the number of instances whose true class equals the\n        *i-th* sorted unique label and whose predicted class equals the\n        *j-th* sorted unique label.\n\n        If the input lengths differ the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    if len(y_true) == 0:\n        return []\n    classes = sorted(set(y_true).union(y_pred))\n    c2idx = {cls: idx for (idx, cls) in enumerate(classes)}\n    C = len(classes)\n    M = [[0 for _ in range(C)] for _ in range(C)]\n    for (t, p) in zip(y_true, y_pred):\n        i = c2idx[t]\n        j = c2idx[p]\n        M[i][j] += 1\n    return M"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Fisher LDA and project the data.\"\"\"\n    if n_components == 0:\n        return []\n    (n_samples, n_features) = X.shape\n    classes = np.unique(y)\n    n_classes = classes.size\n    global_mean = X.mean(axis=0)\n    S_W = np.zeros((n_features, n_features), dtype=float)\n    S_B = np.zeros_like(S_W)\n    for cls in classes:\n        X_c = X[y == cls]\n        n_c = X_c.shape[0]\n        mean_c = X_c.mean(axis=0)\n        diff = X_c - mean_c\n        S_W += diff.T @ diff\n        mean_diff = (mean_c - global_mean).reshape(-1, 1)\n        S_B += n_c * (mean_diff @ mean_diff.T)\n    A = np.linalg.pinv(S_W) @ S_B\n    (eig_vals, eig_vecs) = np.linalg.eigh(A)\n    sorted_idx = np.argsort(eig_vals)[::-1]\n    eig_vecs = eig_vecs[:, sorted_idx]\n    eig_vecs = eig_vecs[:, :n_components]\n    for i in range(eig_vecs.shape[1]):\n        vec = eig_vecs[:, i]\n        for v in vec:\n            if np.abs(v) > 1e-12:\n                if v < 0:\n                    eig_vecs[:, i] = -vec\n                break\n    projected = X @ eig_vecs\n    projected = np.round(projected, 4)\n    return projected.tolist()"}
{"task_id": 129, "completion_id": 0, "solution": "from itertools import islice, cycle\ndef cycle_sequence(sequence: list | tuple, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    if not sequence or samples <= 0:\n        return []\n    return list(islice(cycle(sequence), samples))"}
{"task_id": 130, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Any, List, Tuple\ndef outcome_probs(transitions: List[Tuple[Any, Any, Any, Any]], state: Any, action: Any) -> List[List[float]]:\n    \"\"\"Return empirical outcome probabilities for a given (state, action).\n\n    transitions: list of (state, action, reward, next_state) tuples.\n    state:       queried state.\n    action:      queried action.\n\n    The function returns a list of [next_state, probability] pairs sorted by\n    next_state.  Probabilities are rounded to 4 decimal places.  If the\n    pair never occurred, an empty list is returned.\n    \"\"\"\n    relevant_next_states = [next_state for (s, a, _r, next_state) in transitions if s == state and a == action]\n    if not relevant_next_states:\n        return []\n    counts = Counter(relevant_next_states)\n    total = len(relevant_next_states)\n    result = [[ns, round(cnt / total, 4)] for (ns, cnt) in sorted(counts.items(), key=lambda item: item[0])]\n    return result"}
{"task_id": 131, "completion_id": 0, "solution": "def get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` contains every available sequence so\n    that several parts of a program can work on different sequences at the\n    same time.\n\n    If *uid* or *i* is invalid the function must return *None* instead of\n    raising an exception.\n\n    Args:\n        uid: Integer identifier of the desired sequence.\n        i:   Position inside the selected sequence (supports negative indices).\n\n    Returns:\n        The requested element, or None if the access is invalid.\n    \"\"\"\n    try:\n        seq = _SHARED_SEQUENCES[uid]\n    except (IndexError, TypeError):\n        return None\n    seq_len = len(seq)\n    if i < 0:\n        i += seq_len\n    if i < 0 or i >= seq_len:\n        return None\n    return seq[i]"}
{"task_id": 132, "completion_id": 0, "solution": "import numpy as np\ndef linear(z):\n    \"\"\"Linear (identity) activation function.\n\n    Args:\n        z: Scalar, list, tuple or NumPy ndarray.\n\n    Returns:\n        The same value(s) as *z*, obeying the rules:\n            \u2022 Scalars are returned unchanged.\n            \u2022 Non-scalar inputs are returned as a NumPy ndarray.\n    \"\"\"\n    if np.isscalar(z):\n        return z\n    if isinstance(z, np.ndarray):\n        return z\n    if isinstance(z, (list, tuple)):\n        return np.asarray(z)\n    return np.asarray(z)"}
{"task_id": 133, "completion_id": 0, "solution": "import numpy as np\ndef one_hot_targets(X_train: list[list[int]], vocab_length: int) -> list[list[list[int]]]:\n    \"\"\"Convert integer-encoded sequences into a 3-D one-hot representation.\n\n    Parameters\n    ----------\n    X_train : list[list[int]]\n        A batch of sequences where each element is an integer token index.\n    vocab_length : int\n        The size of the vocabulary.\n\n    Returns\n    -------\n    list[list[list[int]]]\n        A nested list with shape (m, time_steps, vocab_length) representing the\n        one-hot encoded targets, or -1 if the input contains invalid indices.\n    \"\"\"\n    if not isinstance(vocab_length, int) or vocab_length <= 0:\n        return -1\n    eye = np.eye(vocab_length, dtype=int)\n    Y_train: list[list[list[int]]] = []\n    for sequence in X_train:\n        y_seq: list[list[int]] = []\n        for token in sequence:\n            if not isinstance(token, (int, np.integer)) or token < 0 or token >= vocab_length:\n                return -1\n            y_seq.append(eye[token].tolist())\n        Y_train.append(y_seq)\n    return Y_train"}
{"task_id": 134, "completion_id": 0, "solution": "import numpy as np\ndef best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n    \"\"\"Find the arm with the highest expected reward in a Bernoulli bandit.\n\n    Parameters\n    ----------\n    payoff_probs : list[float]\n        Success probabilities for each arm.  Each probability must satisfy\n        0.0 \u2264 p \u2264 1.0.\n\n    Returns\n    -------\n    tuple\n        (max_expected_reward, best_arm_index).  If the input list is empty or\n        contains an invalid entry, returns (-1.0, -1).\n    \"\"\"\n    if not payoff_probs:\n        return (-1.0, -1)\n    best_prob = -1.0\n    best_index = -1\n    for (idx, p) in enumerate(payoff_probs):\n        try:\n            prob = float(p)\n        except (TypeError, ValueError):\n            return (-1.0, -1)\n        if not 0.0 <= prob <= 1.0:\n            return (-1.0, -1)\n        if prob > best_prob:\n            (best_prob, best_index) = (prob, idx)\n    return (best_prob, best_index)"}
{"task_id": 135, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_predict(X_train: list[list[int | float]], y_train: list[int | float], X_test: list[list[int | float]]) -> list[float] | int:\n    \"\"\"\n    Fits an Ordinary Least Squares (OLS) linear-regression model via the normal\n    equation and returns predictions for X_test rounded to 4 decimal places.\n\n    If (X\u1d40X) is singular (non-invertible) the function returns -1.\n    \"\"\"\n    X_tr = np.asarray(X_train, dtype=float)\n    y_tr = np.asarray(y_train, dtype=float).reshape(-1, 1)\n    X_te = np.asarray(X_test, dtype=float)\n    ones_tr = np.ones((X_tr.shape[0], 1))\n    ones_te = np.ones((X_te.shape[0], 1))\n    X_tr_aug = np.hstack((ones_tr, X_tr))\n    X_te_aug = np.hstack((ones_te, X_te))\n    XtX = X_tr_aug.T @ X_tr_aug\n    Xty = X_tr_aug.T @ y_tr\n    if np.linalg.matrix_rank(XtX) < XtX.shape[0]:\n        return -1\n    beta = np.linalg.inv(XtX) @ Xty\n    preds = X_te_aug @ beta\n    preds = np.round(preds.flatten(), 4)\n    return preds.tolist()"}
{"task_id": 136, "completion_id": 0, "solution": "import numpy as np\ndef softmax(z: np.ndarray, axis: int=-1) -> list:\n    \"\"\"Compute the numerically-stable softmax of *z* along *axis*.\n\n    Steps\n    -----\n    1. Shift by the maximum value along *axis* to prevent overflow.\n    2. Exponentiate the shifted values.\n    3. Normalize by the sum of exponentials along the same axis.\n    4. Round to four decimal places.\n    5. Return as a regular Python list.\n\n    Parameters\n    ----------\n    z : np.ndarray\n        Input array of raw scores.\n    axis : int, optional\n        Axis along which softmax is computed (default: -1).\n\n    Returns\n    -------\n    list\n        Softmax probabilities (same shape as *z*), rounded to four decimals.\n    \"\"\"\n    z = np.asarray(z)\n    z_max = np.max(z, axis=axis, keepdims=True)\n    z_shifted = z - z_max\n    exp_z = np.exp(z_shifted)\n    sum_exp = np.sum(exp_z, axis=axis, keepdims=True)\n    softmax_vals = exp_z / sum_exp\n    softmax_vals = np.round(softmax_vals, 4)\n    return softmax_vals.tolist()"}
{"task_id": 138, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    if len(y_true) == 0 or len(y_true) != len(y_pred):\n        return -1\n    matches = sum((1 for (a, b) in zip(y_true, y_pred) if a == b))\n    accuracy = matches / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 139, "completion_id": 0, "solution": "import numpy as np\ndef tanh_activation(z):\n    \"\"\"Compute the hyperbolic tangent (tanh) for every element of the input.\n\n    Args:\n        z (list[int | float] | np.ndarray): A one-dimensional iterable of\n            numeric values.\n\n    Returns:\n        list[float]: Tanh applied element-wise, rounded to four decimals.\n    \"\"\"\n    arr = np.asarray(list(z), dtype=float)\n    if arr.size == 0:\n        return []\n    tanh_vals = 2.0 / (1.0 + np.exp(-2.0 * arr)) - 1.0\n    return np.round(tanh_vals, 4).tolist()"}
{"task_id": 140, "completion_id": 0, "solution": "from collections import deque\nimport math\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Return the minimum number of minutes required for the agent to reach\n    the lower-right corner of a square maze that contains spreading fire.\n\n    The fire spreads first every minute, then the agent moves.  The agent may\n    enter a cell only if that cell is not burning at the moment he arrives.\n\n    Parameters\n    ----------\n    grid : list[str]\n        An n\u00d7n list of strings consisting only of '.', '#', 'F'.\n\n    Returns\n    -------\n    int\n        The minimum time to reach the goal, or \u20111 if it is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 0:\n        return -1\n    INF = math.inf\n    fire_time = [[INF] * n for _ in range(n)]\n    q_fire = deque()\n    for r in range(n):\n        for (c, ch) in enumerate(grid[r]):\n            if ch == 'F':\n                fire_time[r][c] = 0\n                q_fire.append((r, c))\n    dirs = ((1, 0), (-1, 0), (0, 1), (0, -1))\n    while q_fire:\n        (r, c) = q_fire.popleft()\n        for (dr, dc) in dirs:\n            (nr, nc) = (r + dr, c + dc)\n            if 0 <= nr < n and 0 <= nc < n and (grid[nr][nc] != '#'):\n                if fire_time[nr][nc] == INF:\n                    fire_time[nr][nc] = fire_time[r][c] + 1\n                    q_fire.append((nr, nc))\n\n    def burning_at_start(cell_r, cell_c) -> bool:\n        \"\"\"True if that cell is already burning at t = 0.\"\"\"\n        return fire_time[cell_r][cell_c] == 0\n    if grid[0][0] == '#' or grid[n - 1][n - 1] == '#' or burning_at_start(0, 0) or burning_at_start(n - 1, n - 1):\n        return -1\n    if n == 1:\n        return 0\n    q = deque([(0, 0, 0)])\n    best_arrival = [[INF] * n for _ in range(n)]\n    best_arrival[0][0] = 0\n    while q:\n        (r, c, t) = q.popleft()\n        for (dr, dc) in dirs:\n            (nr, nc) = (r + dr, c + dc)\n            if 0 <= nr < n and 0 <= nc < n and (grid[nr][nc] != '#'):\n                next_t = t + 1\n                if next_t >= fire_time[nr][nc]:\n                    continue\n                if next_t < best_arrival[nr][nc]:\n                    best_arrival[nr][nc] = next_t\n                    if nr == n - 1 and nc == n - 1:\n                        return next_t\n                    q.append((nr, nc, next_t))\n    return -1"}
{"task_id": 141, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int, metric: str='euclidean') -> np.ndarray:\n    \"\"\"\n    k\u2013Nearest Neighbours classifier.\n    \n    Parameters\n    ----------\n    X_train : (n_train, n_features) ndarray\n        Training samples.\n    y_train : (n_train,) ndarray\n        Labels of the training samples.\n    X_test  : (n_test,  n_features) ndarray\n        Samples to classify.\n    k       : int\n        Number of neighbours to use (if k > n_train, k == n_train).\n    metric  : {'euclidean', 'manhattan', 'cosine'}, optional\n        Distance metric. Anything else falls back to 'euclidean'.\n    \n    Returns\n    -------\n    preds : (n_test,) ndarray\n        Predicted labels for X_test.\n    \"\"\"\n    if k <= 0:\n        raise ValueError('k must be a positive integer.')\n    metric = metric.lower()\n    n_train = X_train.shape[0]\n    k = min(k, n_train)\n    if metric == 'manhattan':\n        D = np.abs(X_test[:, None, :] - X_train[None, :, :]).sum(axis=2)\n    elif metric == 'cosine':\n        eps = 1e-10\n        t_norm = np.linalg.norm(X_test, axis=1, keepdims=True) + eps\n        r_norm = np.linalg.norm(X_train, axis=1) + eps\n        sim = X_test @ X_train.T / (t_norm * r_norm)\n        D = 1.0 - sim\n    else:\n        diff = X_test[:, None, :] - X_train[None, :, :]\n        D = np.sum(diff ** 2, axis=2)\n    nn_idx = np.argpartition(D, kth=k - 1, axis=1)[:, :k]\n    preds = np.empty(X_test.shape[0], dtype=y_train.dtype)\n    for (i, neighbours) in enumerate(nn_idx):\n        labels = y_train[neighbours]\n        (uniq, counts) = np.unique(labels, return_counts=True)\n        majority = uniq[counts == counts.max()]\n        preds[i] = majority.min()\n    return preds"}
{"task_id": 143, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(x, alpha: float=0.2, derivative: bool=False):\n    \"\"\"Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative slope coefficient. Default is 0.2.\n    derivative : bool, optional\n        If False (default), compute the Leaky ReLU activation.\n        If True, compute the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with the same shape as *x* containing the computed values.\n    \"\"\"\n    x = np.asarray(x)\n    if derivative:\n        out = np.where(x >= 0, 1.0, alpha)\n    else:\n        out = np.where(x >= 0, x, alpha * x)\n    return out"}
{"task_id": 144, "completion_id": 0, "solution": "import numpy as np\ndef mean_absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence containing the true values.\n    predicted : list | tuple | np.ndarray\n        Sequence containing the predicted values. Must have the same length\n        (and shape for multi-dimensional inputs) as ``actual``.\n\n    Returns\n    -------\n    float\n        The MAE rounded to 4 decimal places if inputs have the same shape,\n        otherwise \u20111.\n    \"\"\"\n    try:\n        a = np.asarray(actual, dtype=float)\n        p = np.asarray(predicted, dtype=float)\n    except Exception:\n        return -1\n    if a.shape != p.shape or a.size == 0:\n        return -1\n    mae = np.mean(np.abs(a - p))\n    return float(np.round(mae, 4))"}
{"task_id": 146, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X: np.ndarray, y: np.ndarray, X_test: np.ndarray, k: int=3, metric: str='euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\"\"\"\n    if X.ndim != 2 or X_test.ndim != 2:\n        raise ValueError('X and X_test must be 2-D arrays')\n    if len(X) != len(y):\n        raise ValueError('X and y must contain the same number of samples')\n    if k <= 0:\n        raise ValueError('k must be a positive integer')\n    metric = metric.lower()\n    n_train = X.shape[0]\n    n_test = X_test.shape[0]\n    k = min(k, n_train)\n    if metric == 'euclidean':\n        diff = X_test[:, None, :] - X[None, :, :]\n        dists = np.linalg.norm(diff, axis=2)\n    elif metric == 'manhattan':\n        diff = X_test[:, None, :] - X[None, :, :]\n        dists = np.abs(diff).sum(axis=2)\n    elif metric == 'cosine':\n        eps = 1e-12\n        x_norm = np.linalg.norm(X, axis=1) + eps\n        xt_norm = np.linalg.norm(X_test, axis=1) + eps\n        dot_prod = X_test @ X.T\n        denom = np.outer(xt_norm, x_norm)\n        dists = 1.0 - dot_prod / denom\n    else:\n        raise ValueError(\"metric must be 'euclidean', 'manhattan', or 'cosine'\")\n    predictions = []\n    for i in range(n_test):\n        idx = np.argpartition(dists[i], k - 1)[:k]\n        neigh_y = y[idx]\n        (unique, counts) = np.unique(neigh_y, return_counts=True)\n        max_count = counts.max()\n        winners = unique[counts == max_count]\n        pred = winners.min()\n        predictions.append(pred)\n    return predictions"}
{"task_id": 147, "completion_id": 0, "solution": "import numpy as np\ndef _sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"Numerically\u2013stable sigmoid.\"\"\"\n    z = np.clip(z, -500, 500)\n    return 1.0 / (1.0 + np.exp(-z))\ndef logistic_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, learning_rate: float=0.1, epochs: int=10000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model and predict labels for X_test.\n    See task description for full documentation.\n    \"\"\"\n    if X_train.ndim != 2 or X_test.ndim != 2:\n        raise ValueError('X_train and X_test must be 2-D arrays.')\n    if X_train.shape[1] != X_test.shape[1]:\n        raise ValueError('Train and test must have the same number of features.')\n    if set(np.unique(y_train)) - {0, 1}:\n        raise ValueError('y_train must contain only 0 and 1.')\n    if np.all(y_train == y_train[0]):\n        return [int(y_train[0])] * len(X_test)\n    (n_samples, n_features) = X_train.shape\n    w = np.zeros(n_features, dtype=float)\n    b = 0.0\n    for _ in range(epochs):\n        z = X_train @ w + b\n        p = _sigmoid(z)\n        error = p - y_train\n        grad_w = X_train.T @ error / n_samples\n        grad_b = error.mean()\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n    probs_test = _sigmoid(X_test @ w + b)\n    preds = (probs_test >= 0.5).astype(int)\n    return preds.tolist()"}
{"task_id": 148, "completion_id": 0, "solution": "from typing import List\ndef exp_smoothing(data: List[float], alpha: float) -> List[float] | int:\n    \"\"\"Compute the exponential moving average (EMA) of a numeric sequence.\n\n    Parameters\n    ----------\n    data : List[float]\n        Sequence of numeric observations.\n    alpha : float\n        Smoothing factor (0 \u2264 alpha \u2264 1).\n\n    Returns\n    -------\n    List[float] | int\n        List of smoothed values rounded to 4 decimal places, or -1 for\n        invalid input.\n    \"\"\"\n    if not data or not 0.0 <= alpha <= 1.0:\n        return -1\n    if alpha == 0:\n        return [round(data[0], 4)] * len(data)\n    if alpha == 1:\n        return [round(x, 4) for x in data]\n    smoothed: List[float] = [data[0]]\n    for k in range(1, len(data)):\n        s_k = (1 - alpha) * smoothed[-1] + alpha * data[k]\n        smoothed.append(s_k)\n    return [round(v, 4) for v in smoothed]"}
{"task_id": 150, "completion_id": 0, "solution": "from heapq import heappush, heappop\nfrom typing import List, Tuple, Union\ndef find_shortest_path(grid: List[List[int]]) -> Union[int, List[Tuple[int, int]]]:\n    \"\"\"Return any shortest path from (0,0) to (m-1,n-1) in a 0/1 grid using A*.\n    If such a path does not exist, return -1.\n    \"\"\"\n    if not grid:\n        return -1\n    (m, n) = (len(grid), len(grid[0]))\n    if grid[0][0] == 1 or grid[m - 1][n - 1] == 1:\n        return -1\n\n    def h(r: int, c: int) -> int:\n        return abs(r - (m - 1)) + abs(c - (n - 1))\n    open_heap: List[Tuple[int, int, int, int]] = []\n    heappush(open_heap, (h(0, 0), 0, 0, 0))\n    g_cost = {(0, 0): 0}\n    parent: dict[Tuple[int, int], Tuple[int, int]] = {(0, 0): None}\n    dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    while open_heap:\n        (f, g, r, c) = heappop(open_heap)\n        if (r, c) == (m - 1, n - 1):\n            path: List[Tuple[int, int]] = []\n            cur: Tuple[int, int] | None = (r, c)\n            while cur:\n                path.append(cur)\n                cur = parent[cur]\n            return path[::-1]\n        for (dr, dc) in dirs:\n            (nr, nc) = (r + dr, c + dc)\n            if 0 <= nr < m and 0 <= nc < n and (grid[nr][nc] == 0):\n                new_g = g + 1\n                if new_g < g_cost.get((nr, nc), float('inf')):\n                    g_cost[nr, nc] = new_g\n                    parent[nr, nc] = (r, c)\n                    heappush(open_heap, (new_g + h(nr, nc), new_g, nr, nc))\n    return -1"}
{"task_id": 151, "completion_id": 0, "solution": "import numpy as np\ndef _sigmoid(x: np.ndarray) -> np.ndarray:\n    return 1.0 / (1.0 + np.exp(-x))\ndef _sigmoid_grad(y: np.ndarray) -> np.ndarray:\n    return y * (1.0 - y)\ndef _tanh(x: np.ndarray) -> np.ndarray:\n    return np.tanh(x)\ndef _tanh_grad(y: np.ndarray) -> np.ndarray:\n    return 1.0 - y ** 2\ndef _relu(x: np.ndarray) -> np.ndarray:\n    return np.maximum(0.0, x)\ndef _relu_grad(y: np.ndarray) -> np.ndarray:\n    return (y > 0).astype(y.dtype)\ndef _softmax(x: np.ndarray) -> np.ndarray:\n    z = x - np.max(x, axis=-1, keepdims=True)\n    exp_z = np.exp(z)\n    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\ndef _softmax_grad(_: np.ndarray) -> np.ndarray:\n    return 1.0\n_FORWARD = {'sigmoid': _sigmoid, 'tanh': _tanh, 'relu': _relu, 'softmax': _softmax}\n_BACKWARD = {'sigmoid': _sigmoid_grad, 'tanh': _tanh_grad, 'relu': _relu_grad, 'softmax': _softmax_grad}\ndef activation_forward_backward(X: np.ndarray, activation: str, upstream_grad: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Apply an activation function and its element-wise derivative in one pass.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Raw pre-activation values.\n    activation : str\n        One of: \"sigmoid\", \"tanh\", \"relu\", \"softmax\".\n    upstream_grad : np.ndarray\n        Gradient flowing from the layer above (same shape as X).\n\n    Returns\n    -------\n    (A, grad) : tuple[np.ndarray, np.ndarray]\n        A   \u2013 activation(X)  (rounded to 4 decimals)\n        grad \u2013 upstream_grad * local_derivative  (rounded to 4 decimals)\n    \"\"\"\n    activation = activation.lower()\n    if activation not in _FORWARD:\n        raise ValueError(f\"Unsupported activation '{activation}'. Choose from {list(_FORWARD.keys())}.\")\n    A = _FORWARD[activation](X)\n    local_grad = _BACKWARD[activation](A)\n    grad = upstream_grad * local_grad\n    A = np.round(A, 4)\n    grad = np.round(grad, 4)\n    return (A, grad)"}
{"task_id": 152, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Calculate the proportion of correctly classified samples.\n\n    Parameters\n    ----------\n    y_true : list | numpy.ndarray\n        The true class labels.\n    y_pred : list | numpy.ndarray\n        The predicted class labels.\n\n    Returns\n    -------\n    float | int\n        The accuracy rounded to four decimal places. If the two inputs do\n        not have the same length or are empty, the function returns -1.\n    \"\"\"\n    y_true = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    if y_true.size == 0 or y_pred.size == 0 or y_true.size != y_pred.size:\n        return -1\n    correct = np.sum(y_true == y_pred)\n    accuracy = correct / y_true.size\n    return round(float(accuracy), 4)"}
{"task_id": 154, "completion_id": 0, "solution": "import numpy as np\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float=0.1) -> list[list[float]]:\n    \"\"\"Return the RBF (Gaussian) kernel matrix between two sets of vectors.\n\n    Args:\n        X: Array of shape (n_samples, n_features) or a 1-D array treated as one sample.\n        Y: Array of shape (m_samples, n_features) or a 1-D array treated as one sample.\n        gamma: Positive scalar controlling the kernel width.\n\n    Returns:\n        Nested Python list with the RBF kernel matrix,\n        every entry rounded to six decimal places.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n    if X.ndim == 1:\n        X = X[np.newaxis, :]\n    if Y.ndim == 1:\n        Y = Y[np.newaxis, :]\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError('X and Y must have the same number of features.')\n    if gamma <= 0:\n        raise ValueError('gamma must be a positive value.')\n    X_norm_sq = np.sum(X ** 2, axis=1)[:, np.newaxis]\n    Y_norm_sq = np.sum(Y ** 2, axis=1)[np.newaxis, :]\n    dist_sq = X_norm_sq + Y_norm_sq - 2 * X @ Y.T\n    dist_sq = np.maximum(dist_sq, 0.0)\n    K = np.exp(-gamma * dist_sq)\n    return np.round(K, 6).tolist()"}
{"task_id": 155, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Forward pass of a two-hidden-layer actor network (Linear \u2192 ReLU \u2192\n    Linear \u2192 ReLU \u2192 Linear \u2192 tanh).\n\n    Args:\n        state (list[float] | np.ndarray): 1-D state vector.\n        weights (dict): Dictionary containing the NumPy arrays\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector, each element rounded to four decimals.\n    \"\"\"\n    x = np.asarray(state, dtype=float)\n    (W1, b1) = (weights['W1'], weights['b1'])\n    (W2, b2) = (weights['W2'], weights['b2'])\n    (W3, b3) = (weights['W3'], weights['b3'])\n    z1 = x @ W1 + b1\n    a1 = np.maximum(0, z1)\n    z2 = a1 @ W2 + b2\n    a2 = np.maximum(0, z2)\n    z3 = a2 @ W3 + b3\n    actions = np.tanh(z3)\n    actions_rounded = np.round(actions, 4)\n    if actions_rounded.shape == ():\n        return [float(actions_rounded)]\n    return actions_rounded.tolist()"}
{"task_id": 157, "completion_id": 0, "solution": "def accuracy_score(y_true, y_pred):\n    \"\"\"Return the classification accuracy between *y_true* and *y_pred*.\n\n    The function must:\n      \u2022 return \u20111 if the two containers have different lengths;\n      \u2022 return 0 when both containers are empty;\n      \u2022 otherwise return the proportion of matching elements rounded to\n        4 decimal places.\n\n    Args:\n        y_true (list | tuple): Ground-truth class labels.\n        y_pred (list | tuple): Predicted class labels.\n\n    Returns:\n        float | int: The accuracy or \u20111 according to the rules above.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    if len(y_true) == 0:\n        return 0\n    correct = sum((1 for (yt, yp) in zip(y_true, y_pred) if yt == yp))\n    accuracy = correct / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 158, "completion_id": 0, "solution": "import numpy as np\ndef ridge_regression(X: list[list[int | float]], y: list[int | float], alpha: float=1.0, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Compute Ridge Regression coefficients (closed-form).\n\n    Args:\n        X: Feature matrix as nested list with shape (N, M).\n        y: Target vector of length N.\n        alpha: Non-negative regularisation strength.\n        fit_intercept: If True, adds an unregularised intercept term.\n\n    Returns:\n        List of regression coefficients rounded to four decimals.\n        When fit_intercept is True the first element is the intercept.\n    \"\"\"\n    X_mat = np.asarray(X, dtype=float)\n    y_vec = np.asarray(y, dtype=float)\n    if fit_intercept:\n        ones = np.ones((X_mat.shape[0], 1), dtype=float)\n        X_mat = np.hstack((ones, X_mat))\n    n_features = X_mat.shape[1]\n    A = np.eye(n_features) * alpha\n    if fit_intercept:\n        A[0, 0] = 0.0\n    XtX_plus_A = X_mat.T @ X_mat + A\n    XTy = X_mat.T @ y_vec\n    beta = np.linalg.solve(XtX_plus_A, XTy)\n    return [round(float(b), 4) for b in beta]"}
{"task_id": 159, "completion_id": 0, "solution": "import numpy as np\ndef log_gaussian_pdf(x_i: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> float:\n    \"\"\"Compute log p(x_i | \u03bc, \u03a3) for a multivariate normal distribution.\n\n    Parameters\n    ----------\n    x_i : np.ndarray\n        Sample vector of shape (d,).\n    mu : np.ndarray\n        Mean vector of shape (d,).\n    sigma : np.ndarray\n        Positive\u2013definite covariance matrix of shape (d, d).\n\n    Returns\n    -------\n    float\n        Log probability density rounded to four decimal places.\n    \"\"\"\n    x_i = np.asarray(x_i).ravel()\n    mu = np.asarray(mu).ravel()\n    sigma = np.asarray(sigma)\n    d = x_i.size\n    diff = x_i - mu\n    (sign, logdet) = np.linalg.slogdet(sigma)\n    if sign <= 0:\n        raise ValueError('Covariance matrix must be positive-definite.')\n    alpha = np.linalg.solve(sigma, diff)\n    quad_form = diff @ alpha\n    log_prob = -0.5 * (d * np.log(2 * np.pi) + logdet + quad_form)\n    return round(float(log_prob), 4)"}
{"task_id": 160, "completion_id": 0, "solution": "import numpy as np\ndef _soft_threshold(a: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S(a, \u03c4).\"\"\"\n    if a > tau:\n        return a - tau\n    if a < -tau:\n        return a + tau\n    return 0.0\ndef fit_lasso(X: np.ndarray, y: np.ndarray, lambda_param: float=1.0, max_iters: int=100, fit_intercept: bool=True) -> tuple[list[float], float]:\n    \"\"\"Lasso (L1) linear regression fitted with coordinate descent.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    (m, n) = X.shape\n    intercept_col = np.ones((m, 1)) if fit_intercept else np.zeros((m, 1))\n    XA = np.hstack((intercept_col, X))\n    p = n + 1\n    w = np.zeros(p)\n    col_sq_norms = (XA ** 2).sum(axis=0)\n    for _ in range(max_iters):\n        if fit_intercept:\n            y_hat_wo_b = XA[:, 1:].dot(w[1:])\n            w[0] = (y - y_hat_wo_b).mean()\n        for j in range(1, p):\n            x_j = XA[:, j]\n            r_j = y - XA.dot(w) + w[j] * x_j\n            rho = x_j.dot(r_j)\n            w[j] = _soft_threshold(rho, lambda_param * m) / col_sq_norms[j]\n    weights = np.round(w[1:], 4).tolist()\n    bias = round(float(w[0]), 4)\n    return (weights, bias)"}
{"task_id": 161, "completion_id": 0, "solution": "import numpy as np\ndef factorization_machine_predict(X, w0, w, V):\n    \"\"\"\n    Predict binary labels (+1 / -1) with a second-order Factorization Machine.\n\n    Parameters\n    ----------\n    X  : array_like, shape = (m, n)\n         Design matrix containing m samples with n features each.\n    w0 : float\n         Global bias term.\n    w  : array_like, shape = (n,)\n         Linear weights.\n    V  : array_like, shape = (n, k)\n         Latent factor matrix (k is the embedding dimension).\n\n    Returns\n    -------\n    list\n        Predicted labels (+1 or -1) for the m samples.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    w = np.asarray(w, dtype=float)\n    V = np.asarray(V, dtype=float)\n    linear_term = X @ w\n    XV = X @ V\n    X2V2 = X ** 2 @ V ** 2\n    interaction_term = 0.5 * (XV ** 2 - X2V2).sum(axis=1)\n    scores = w0 + linear_term + interaction_term\n    labels = np.where(scores >= 0, 1, -1)\n    return labels.tolist()"}
{"task_id": 162, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy(Y_hat: np.ndarray, Y: np.ndarray, epsilon: float=1e-20) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute average cross-entropy loss of a batch and its gradient.\n\n    Parameters\n    ----------\n    Y_hat : np.ndarray\n        Soft-max probabilities with shape (batch_size, num_classes).\n    Y : np.ndarray\n        One-hot encoded ground-truth labels with the same shape as Y_hat.\n    epsilon : float, optional\n        Small constant added for numerical stability before taking the log.\n\n    Returns\n    -------\n    tuple[float, list[list[float]]]\n        (loss, gradient) \u2013 both rounded to 4 decimal places.\n    \"\"\"\n    assert Y_hat.shape == Y.shape, 'Y_hat and Y must have the same shape'\n    m = Y_hat.shape[0]\n    log_probs = np.log(Y_hat + epsilon)\n    loss = -np.sum(Y * log_probs) / m\n    grad = (Y_hat - Y) / m\n    loss_rounded = float(np.round(loss, 4))\n    grad_rounded = np.round(grad, 4).tolist()\n    return (loss_rounded, grad_rounded)"}
{"task_id": 163, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regression(X: list[float], y: list[float], degree: int, reg_factor: float, l1_ratio: float, learning_rate: float, n_iterations: int) -> list[float]:\n    \"\"\"Elastic-Net / Ridge (\u03b1=0) for one feature and arbitrary polynomial degree.\"\"\"\n    X = np.asarray(X, dtype=float).reshape(-1)\n    y = np.asarray(y, dtype=float).reshape(-1)\n    n_samples = X.size\n    X_poly = np.vstack([X ** p for p in range(degree + 1)]).T\n    d_plus1 = degree + 1\n    if l1_ratio == 0.0:\n        R = np.eye(d_plus1)\n        R[0, 0] = 0\n        lam = n_samples * reg_factor / 2.0\n        A = X_poly.T @ X_poly + lam * R\n        b = X_poly.T @ y\n        beta = np.linalg.pinv(A) @ b\n    else:\n        beta = np.zeros(d_plus1)\n        for _ in range(n_iterations):\n            y_pred = X_poly @ beta\n            err = y_pred - y\n            grad = 2.0 / n_samples * (X_poly.T @ err)\n            grad_l2 = reg_factor * (1 - l1_ratio) * beta\n            grad_l2[0] = 0.0\n            grad_l1 = reg_factor * l1_ratio * np.sign(beta)\n            grad_l1[0] = 0.0\n            grad += grad_l1 + grad_l2\n            beta -= learning_rate * grad\n    preds = X_poly @ beta\n    return [round(float(v), 4) for v in preds]"}
{"task_id": 164, "completion_id": 0, "solution": "import numpy as np\ndef compute_log_loss(actual, predicted):\n    \"\"\"\n    Compute the binary classification log-loss (cross-entropy loss).\n\n    Parameters\n    ----------\n    actual : sequence of int (0 or 1)\n        Ground-truth binary labels.\n    predicted : sequence of float (in [0, 1])\n        Predicted probabilities for the positive class.\n\n    Returns\n    -------\n    float\n        The average log-loss rounded to six decimal places.\n\n    Raises\n    ------\n    ValueError\n        If *actual* and *predicted* have different lengths\n        or if *actual* contains values other than 0 or 1.\n    \"\"\"\n    EPS = 1e-15\n    y_true = np.asarray(actual, dtype=np.float64)\n    y_pred = np.asarray(predicted, dtype=np.float64)\n    if y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError('`actual` and `predicted` must be of the same length.')\n    if not np.isin(y_true, [0, 1]).all():\n        raise ValueError('`actual` must contain only 0s and 1s.')\n    y_pred_clipped = np.clip(y_pred, EPS, 1.0 - EPS)\n    loss = -np.mean(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n    return round(float(loss), 6)"}
{"task_id": 165, "completion_id": 0, "solution": "import numpy as np\ndef hamming_distance(x: list[int] | 'np.ndarray', y: list[int] | 'np.ndarray') -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length vectors.\n\n    Args:\n        x: 1-D list or NumPy array of integers.\n        y: 1-D list or NumPy array of integers.\n\n    Returns:\n        Float rounded to 4 decimal places representing the distance, or \u20111\n        if the vectors have different lengths.\n    \"\"\"\n    x_arr = np.asarray(x)\n    y_arr = np.asarray(y)\n    if x_arr.ndim != 1 or y_arr.ndim != 1:\n        raise ValueError('Input vectors must be 1-D.')\n    if x_arr.size != y_arr.size:\n        return -1\n    N = x_arr.size\n    if N == 0:\n        return 0.0\n    diff = np.count_nonzero(x_arr != y_arr)\n    distance = diff / N\n    return round(float(distance), 4)"}
{"task_id": 166, "completion_id": 0, "solution": "import numpy as np\ndef perceptron_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_iter: int=1000) -> list[int]:\n    \"\"\"Classic binary Perceptron (learning-rate = 1).\"\"\"\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features, dtype=float)\n    b = 0.0\n    for _ in range(max_iter):\n        errors = 0\n        for (x_i, y_i) in zip(X_train, y_train):\n            margin = y_i * (np.dot(w, x_i) + b)\n            if margin <= 0:\n                w += y_i * x_i\n                b += y_i\n                errors += 1\n        if errors == 0:\n            break\n    scores = X_test @ w + b\n    preds = np.sign(scores)\n    preds[preds == 0] = 1\n    return preds.astype(int).tolist()"}
{"task_id": 167, "completion_id": 0, "solution": "import numpy as np\ndef binary_cross_entropy(y_hat, y, epsilon: float=1e-10):\n    \"\"\"Compute the Binary Cross-Entropy (BCE) loss and its gradient.\n\n    Args:\n        y_hat (np.ndarray): Predicted probabilities, each in [0, 1].\n        y (np.ndarray): Ground-truth binary labels (0 or 1).\n        epsilon (float, optional): Small constant to avoid log(0). Defaults to 1e-10.\n\n    Returns:\n        tuple[float, list[float]]: (loss, gradient) both rounded to 4 decimals.\n    \"\"\"\n    y_hat = np.asarray(y_hat, dtype=float)\n    y = np.asarray(y, dtype=float)\n    y_hat_clipped = np.clip(y_hat, epsilon, 1.0 - epsilon)\n    m = y_hat_clipped.size\n    loss = -np.mean(y * np.log(y_hat_clipped) + (1.0 - y) * np.log(1.0 - y_hat_clipped))\n    grad = (y_hat_clipped - y) / m\n    loss_rounded = float(np.round(loss, 4))\n    grad_rounded = np.round(grad, 4).tolist()\n    return (loss_rounded, grad_rounded)"}
{"task_id": 168, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_naive_bayes(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]]) -> list[int]:\n    \"\"\"\n    Train a Gaussian-Naive-Bayes classifier on (X_train, y_train) and predict the\n    labels of X_test.\n\n    Notes\n    -----\n    \u2022 Feature independence and a Gaussian distribution per feature/class are\n      assumed.\n    \u2022 A small \u03b5 is added to every variance to avoid division-by-zero.\n    \u2022 Only NumPy is used.\n    \"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    X_test = np.asarray(X_test, dtype=float)\n    (n_samples, n_features) = X_train.shape\n    classes = np.unique(y_train)\n    n_classes = classes.size\n    eps = 1e-06\n    means = np.zeros((n_classes, n_features))\n    var = np.zeros_like(means)\n    priors = np.zeros(n_classes)\n    for (idx, c) in enumerate(classes):\n        X_c = X_train[y_train == c]\n        priors[idx] = X_c.shape[0] / n_samples\n        means[idx] = X_c.mean(axis=0)\n        var[idx] = X_c.var(axis=0) + eps\n    log_priors = np.log(priors)\n    log_coef = -0.5 * np.log(2.0 * np.pi * var)\n    predictions = []\n    for x in X_test:\n        log_likelihood = log_coef - 0.5 * (x - means) ** 2 / var\n        log_posterior = log_priors + np.sum(log_likelihood, axis=1)\n        predicted_class = classes[np.argmax(log_posterior)]\n        predictions.append(int(predicted_class))\n    return predictions"}
{"task_id": 169, "completion_id": 0, "solution": "import numpy as np\ndef generate_window(window: str, N: int, coefficients: list[float] | None=None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Window name: \"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate (must be > 0).\n    coefficients : list[float] | None, optional\n        Cosine-series coefficients a\u2080\u2026a_M, required only when\n        window == \"generalized_cosine\".\n\n    Returns\n    -------\n    list[float]\n        List with N floats rounded to 4 decimal places.\n    \"\"\"\n    if not isinstance(N, int) or N <= 0:\n        raise ValueError('N must be a positive integer.')\n    if N == 1:\n        return [1.0]\n    name = window.lower()\n    if name not in {'hamming', 'hann', 'blackman_harris', 'generalized_cosine'}:\n        raise ValueError(f'Unknown window type: {window!r}')\n    n = np.arange(N)\n    denom = N - 1\n    two_pi_term = 2.0 * np.pi * n / denom\n    if name == 'hamming':\n        w = 0.54 - 0.46 * np.cos(two_pi_term)\n    elif name == 'hann':\n        w = 0.5 - 0.5 * np.cos(two_pi_term)\n    elif name == 'blackman_harris':\n        (a0, a1, a2, a3) = (0.35875, 0.48829, 0.14128, 0.01168)\n        w = a0 - a1 * np.cos(two_pi_term) + a2 * np.cos(2 * two_pi_term) - a3 * np.cos(3 * two_pi_term)\n    elif name == 'generalized_cosine':\n        if not coefficients:\n            raise ValueError(\"For 'generalized_cosine' you must supply a non-empty 'coefficients' list.\")\n        w = np.zeros(N, dtype=float)\n        for (k, ak) in enumerate(coefficients):\n            w += ak * np.cos(2.0 * np.pi * k * n / denom)\n    return [round(float(v), 4) for v in w]"}
{"task_id": 171, "completion_id": 0, "solution": "import numpy as np\ndef logistic_regression_train_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], epochs: int=5000, learning_rate: float=0.1, batch_size: int=32) -> list[int]:\n    \"\"\"Binary Logistic-Regression trained with mini-batch Gradient Descent.\"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train)\n    uniq = np.unique(y_train)\n    if len(uniq) == 1:\n        return [int(uniq[0])] * len(X_test)\n    if len(uniq) != 2:\n        raise ValueError('Exactly two distinct labels are required for training.')\n    pos_label = uniq[1]\n    y_bin = (y_train == pos_label).astype(float)\n    ones_train = np.ones((X_train.shape[0], 1))\n    ones_test = np.ones((X_test.shape[0], 1))\n    X_train = np.hstack([ones_train, X_train])\n    X_test_aug = np.hstack([ones_test, X_test])\n    (n_samples, n_features) = X_train.shape\n    w = np.zeros(n_features)\n\n    def sigmoid(z):\n        z = np.clip(z, -500, 500)\n        return 1.0 / (1.0 + np.exp(-z))\n    for _ in range(epochs):\n        permutation = np.random.permutation(n_samples)\n        for start in range(0, n_samples, batch_size):\n            idx = permutation[start:start + batch_size]\n            X_b = X_train[idx]\n            y_b = y_bin[idx]\n            preds = sigmoid(X_b @ w)\n            grad = X_b.T @ (preds - y_b) / len(idx)\n            w -= learning_rate * grad\n    probs = sigmoid(X_test_aug @ w)\n    preds_bin = (probs >= 0.5).astype(int)\n    label_map = {0: int(uniq[0]), 1: int(uniq[1])}\n    preds_original = [label_map[int(p)] for p in preds_bin]\n    return preds_original"}
{"task_id": 172, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef tanh_grad(z):\n    \"\"\"Derivative of the hyper-bolic tangent (tanh) activation.\n\n    The derivative is computed element-wise as 1 \u2212 tanh(z)**2.\n\n    Args:\n        z: A scalar, Python list or NumPy ndarray of floats/ints.\n\n    Returns:\n        float | list[float]: If `z` is scalar a float is returned, otherwise a\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    if np.isscalar(z):\n        grad = 1.0 - np.tanh(z) ** 2\n        return float(np.round(grad, 4))\n    else:\n        arr = np.asarray(z, dtype=float)\n        grad = 1.0 - np.tanh(arr) ** 2\n        grad = np.round(grad, 4)\n        return grad.tolist()"}
{"task_id": 173, "completion_id": 0, "solution": "def alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        The sum of absolute Unicode-code-point (ASCII) differences between\n        corresponding characters.  If the strings differ in length, \u20131 is\n        returned.\n    \"\"\"\n    if len(candidate) != len(target):\n        return -1\n    return sum((abs(ord(c) - ord(t)) for (c, t) in zip(candidate, target)))"}
{"task_id": 174, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef gan_discriminator_metrics(predictions: np.ndarray, labels: np.ndarray) -> tuple[float, float]:\n    \"\"\"Calculate categorical cross-entropy loss & accuracy for a GAN discriminator.\"\"\"\n    preds = predictions.astype(np.float64)\n    lbls = labels.astype(np.float64)\n    eps = 1e-12\n    preds = np.clip(preds, eps, 1.0 - eps)\n    logp = np.log(preds)\n    ce_per_sample = -np.sum(lbls * logp, axis=1)\n    loss = ce_per_sample.mean()\n    pred_classes = np.argmax(preds, 1)\n    true_classes = np.argmax(lbls, 1)\n    accuracy = np.mean(pred_classes == true_classes)\n    return (round(loss, 4), round(accuracy, 4))"}
{"task_id": 175, "completion_id": 0, "solution": "import numpy as np\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n    \"\"\"Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: The penalty value rounded to 4 decimal places.\n    \"\"\"\n    if C < 0:\n        raise ValueError('Regularization strength C must be non-negative.')\n    w = np.asarray(weights, dtype=float).ravel()\n    penalty = float(C) * float(np.dot(w, w))\n    return round(penalty, 4)"}
{"task_id": 176, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: list[list[int | float]], y_train: list[int], X_test: list[list[int | float]], n_estimators: int=10) -> list[int]:\n    \"\"\"AdaBoost (SAMME-R) with decision stumps, implemented from scratch.\n\n    Parameters\n    ----------\n    X_train : list of list of float\n        Training features.\n    y_train : list of int (0/1)\n        Training labels, 0 = negative class, 1 = positive class.\n    X_test  : list of list of float\n        Samples whose labels must be predicted.\n    n_estimators : int, default=10\n        Maximum number of boosting rounds.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (0/1) for every row of *X_test*.\n    \"\"\"\n\n    def train_best_stump(X: np.ndarray, y: np.ndarray, w: np.ndarray):\n        \"\"\"\n        Exhaustively search the decision-stump with the smallest\n        *weighted* classification error.  Tie-breaking is deterministic:\n            1) smallest error\n            2) smallest feature index\n            3) smallest threshold\n            4) polarity  +1 before \u20131\n        \"\"\"\n        (n_samples, n_features) = X.shape\n        best = {'feature': None, 'threshold': None, 'polarity': None, 'pred': None, 'error': np.inf}\n        for j in range(n_features):\n            column = X[:, j]\n            thresholds = np.unique(column)\n            for thr in thresholds:\n                for pol in (1, -1):\n                    if pol == 1:\n                        h = np.where(column < thr, 1, -1)\n                    else:\n                        h = np.where(column >= thr, 1, -1)\n                    err = np.sum(w * (h != y))\n                    if err < best['error'] - 1e-12:\n                        best.update(feature=j, threshold=float(thr), polarity=pol, pred=h, error=err)\n        return best\n\n    def stump_predict(stump, X):\n        \"\"\"Vectorised prediction of a single stump on data matrix X.\"\"\"\n        col = X[:, stump['feature']]\n        if stump['polarity'] == 1:\n            return np.where(col < stump['threshold'], 1, -1)\n        else:\n            return np.where(col >= stump['threshold'], 1, -1)\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=int)\n    y = 2 * y - 1\n    n_samples = X.shape[0]\n    w = np.full(n_samples, 1.0 / n_samples)\n    stumps: list[dict] = []\n    alphas: list[float] = []\n    for _ in range(n_estimators):\n        stump = train_best_stump(X, y, w)\n        error = stump['error']\n        if error <= 1e-12:\n            alpha = 0.5 * np.log((1.0 - 1e-12) / 1e-12)\n            stumps.append(stump)\n            alphas.append(alpha)\n            break\n        alpha = 0.5 * np.log((1.0 - error) / (error + 1e-10))\n        w *= np.exp(-alpha * y * stump['pred'])\n        w /= np.sum(w)\n        stumps.append(stump)\n        alphas.append(alpha)\n    X_te = np.asarray(X_test, dtype=float)\n    agg = np.zeros(X_te.shape[0], dtype=float)\n    for (stump, alpha) in zip(stumps, alphas):\n        agg += alpha * stump_predict(stump, X_te)\n    preds = (agg >= 0).astype(int).tolist()\n    return preds"}
{"task_id": 177, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Union\ndef tanh(z: Union[int, float, np.ndarray, list]) -> Union[float, list]:\n    \"\"\"Compute tanh without using numpy.tanh (4-decimal rounding).\"\"\"\n\n    def _tanh_array(arr: np.ndarray) -> np.ndarray:\n        exp_neg = np.exp(-2 * np.abs(arr))\n        return np.sign(arr) * (1 - exp_neg) / (1 + exp_neg)\n    if np.isscalar(z):\n        z = float(z)\n        result = _tanh_array(np.array(z))\n        return round(float(result), 4)\n    arr = np.asarray(z, dtype=float)\n    result = _tanh_array(arr)\n    return np.round(result, 4).tolist()"}
{"task_id": 178, "completion_id": 0, "solution": "import numpy as np\ndef particle_swarm_optimisation(n_dims: int, num_particles: int, num_iterations: int, seed: int=1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function with Particle Swarm Optimisation.\n\n    Args:\n        n_dims:            number of decision variables  (>0)\n        num_particles:     swarm size                   (>0)\n        num_iterations:    optimisation iterations      (>0)\n        seed:              RNG seed (default 1)\n\n    Returns\n        Best objective value found, rounded to 4 decimals, or \u20131 on bad input.\n    \"\"\"\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n    rng = np.random.default_rng(seed)\n    (w, c1, c2) = (0.5, 1.5, 1.5)\n    (lo, hi) = (-1.0, 1.0)\n    positions = rng.uniform(lo, hi, size=(num_particles, n_dims))\n    velocities = rng.uniform(lo, hi, size=(num_particles, n_dims))\n    pbest_pos = positions.copy()\n    pbest_val = np.sum(pbest_pos ** 2, axis=1)\n    gbest_idx = np.argmin(pbest_val)\n    gbest_pos = pbest_pos[gbest_idx].copy()\n    gbest_val = pbest_val[gbest_idx]\n    for _ in range(num_iterations):\n        r1 = rng.random(size=(num_particles, n_dims))\n        r2 = rng.random(size=(num_particles, n_dims))\n        velocities = w * velocities + c1 * r1 * (pbest_pos - positions) + c2 * r2 * (gbest_pos - positions)\n        positions = np.clip(positions + velocities, lo, hi)\n        curr_val = np.sum(positions ** 2, axis=1)\n        improved = curr_val < pbest_val\n        pbest_pos[improved] = positions[improved]\n        pbest_val[improved] = curr_val[improved]\n        gbest_idx = np.argmin(pbest_val)\n        gbest_pos = pbest_pos[gbest_idx].copy()\n        gbest_val = pbest_val[gbest_idx]\n    return round(float(gbest_val), 4)"}
{"task_id": 179, "completion_id": 0, "solution": "import re\nimport string\ndef tokenize_chars(line: str, lowercase: bool=True, filter_punctuation: bool=True) -> list[str]:\n    \"\"\"Tokenise a text string at the character level.\n\n    Args:\n        line: Input text. If *None*, an empty list is returned.\n        lowercase: When *True*, convert *line* to lower-case beforehand.\n        filter_punctuation: When *True*, remove every character contained in\n            ``string.punctuation`` (the 32 standard ASCII punctuation marks).\n\n    Returns\n    -------\n    list[str]\n        A list of single-character strings after the selected preprocessing\n        steps have been applied.  Runs of \u22652 spaces are collapsed into a single\n        space, and leading/trailing spaces are stripped.  If the final string\n        is empty, an empty list is returned.\n    \"\"\"\n    if line is None:\n        return []\n    if lowercase:\n        line = line.lower()\n    if filter_punctuation:\n        line = line.translate(str.maketrans('', '', string.punctuation))\n    line = re.sub(' {2,}', ' ', line).strip()\n    return list(line) if line else []"}
{"task_id": 180, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef knn_predict(X_train: list[list[float]], y_train: list, X_test: list[list[float]], k: int=5, task: str='classification') -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\"\"\"\n    X_tr = np.asarray(X_train, dtype=float)\n    X_te = np.asarray(X_test, dtype=float)\n    y_tr = np.asarray(y_train, dtype=object)\n    n_train = X_tr.shape[0]\n    if k <= 0 or k > n_train:\n        k = n_train\n    predictions: list = []\n    for x in X_te:\n        dists = np.linalg.norm(X_tr - x, axis=1)\n        if k == n_train:\n            nn_idx = np.arange(n_train)\n        else:\n            nn_idx = np.argpartition(dists, k - 1)[:k]\n        neighbours = y_tr[nn_idx]\n        if task == 'regression':\n            mean_val = float(np.mean(neighbours.astype(float)))\n            predictions.append(round(mean_val, 4))\n        else:\n            counts = Counter(neighbours)\n            max_cnt = max(counts.values())\n            best_label = min((lbl for (lbl, cnt) in counts.items() if cnt == max_cnt))\n            predictions.append(best_label)\n    return predictions"}
{"task_id": 181, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef knn_predict(X_train: list[list[float]], y_train: list, X_test: list[list[float]], k: int=5, problem: str='classify') -> list:\n    \"\"\"Predict targets of *X_test* using k-Nearest Neighbours.\"\"\"\n    if problem not in {'classify', 'regress'}:\n        raise ValueError(\"`problem` must be 'classify' or 'regress'.\")\n    if k < 1:\n        raise ValueError('`k` must be a positive integer.')\n    if len(X_train) == 0:\n        raise ValueError('`X_train` must contain at least one sample.')\n    X_train_np = np.asarray(X_train, dtype=float)\n    X_test_np = np.asarray(X_test, dtype=float)\n    n_train = X_train_np.shape[0]\n    k = min(k, n_train)\n    preds = []\n    for x in X_test_np:\n        diff = X_train_np - x\n        dists = np.einsum('ij,ij->i', diff, diff)\n        idx = np.argpartition(dists, k - 1)[:k]\n        neigh_targets = [y_train[i] for i in idx]\n        if problem == 'classify':\n            cnt = Counter(neigh_targets)\n            max_votes = max(cnt.values())\n            winners = [label for (label, v) in cnt.items() if v == max_votes]\n            preds.append(sorted(winners)[0])\n        else:\n            mean_val = float(np.mean(neigh_targets))\n            preds.append(round(mean_val, 4))\n    return preds"}
{"task_id": 182, "completion_id": 0, "solution": "import numpy as np\ndef polynomial_kernel(X, Y, degree=2):\n    \"\"\"\n    Compute the polynomial-kernel (Gram) matrix K where\n    \n        K[i][j] = ( <X_i , Y_j> ) ** degree,\n    \n    <\u00b7 , \u00b7> being the ordinary dot product.  \n    The result is rounded to 4 decimal places and returned as a\n    plain nested Python list.\n    \n    Parameters\n    ----------\n    X : 2-D list or numpy.ndarray\n        Data matrix of shape (n_samples_X, n_features).\n    Y : 2-D list or numpy.ndarray\n        Data matrix of shape (n_samples_Y, n_features).\n    degree : int, optional (default=2)\n        Positive integer \u2265 1 \u2013 the polynomial degree.\n    \n    Returns\n    -------\n    list\n        Kernel matrix as a nested Python list, rounded to 4 decimals.\n        If any validation check fails the function returns -1.\n    \"\"\"\n    if not isinstance(degree, int) or isinstance(degree, bool) or degree < 1:\n        return -1\n    try:\n        X_arr = np.asarray(X, dtype=float)\n        Y_arr = np.asarray(Y, dtype=float)\n    except Exception:\n        return -1\n    if X_arr.ndim != 2 or Y_arr.ndim != 2:\n        return -1\n    if X_arr.shape[1] != Y_arr.shape[1]:\n        return -1\n    try:\n        kernel = (X_arr @ Y_arr.T) ** degree\n    except Exception:\n        return -1\n    kernel = np.round(kernel, 4)\n    return kernel.tolist()"}
{"task_id": 183, "completion_id": 0, "solution": "import numpy as np\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    weight_shape = tuple(weight_shape)\n    ndim = len(weight_shape)\n    if ndim == 2:\n        fan_in = weight_shape[0]\n    elif ndim == 4:\n        (f_w, f_h, in_ch, _) = weight_shape\n        fan_in = f_w * f_h * in_ch\n    else:\n        raise ValueError(f'he_uniform only supports shapes of length 2 (dense) or 4 (conv); got shape with {ndim} dimensions.')\n    if fan_in <= 0:\n        raise ValueError('fan_in must be positive, got {}'.format(fan_in))\n    limit = np.sqrt(6.0 / fan_in)\n    return np.random.uniform(low=-limit, high=limit, size=weight_shape)"}
{"task_id": 184, "completion_id": 0, "solution": "import numpy as np\ndef decision_tree_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_depth: int | None=None) -> list[int]:\n    \"\"\"CART-style binary decision tree (Gini) completely from scratch.\"\"\"\n\n    class Node:\n        __slots__ = ('is_leaf', 'prediction', 'feature', 'threshold', 'left', 'right')\n\n        def __init__(self, *, is_leaf: bool, prediction: int | None=None, feature: int | None=None, threshold: float | None=None, left: 'Node | None'=None, right: 'Node | None'=None):\n            self.is_leaf = is_leaf\n            self.prediction = prediction\n            self.feature = feature\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n    y_train = y_train.astype(int, copy=False)\n    n_classes = int(y_train.max()) + 1\n\n    def gini(counts: np.ndarray) -> float:\n        \"\"\"Gini impurity from class counts.\"\"\"\n        total = counts.sum()\n        if total == 0:\n            return 0.0\n        probs = counts / total\n        return 1.0 - np.sum(probs ** 2)\n\n    def majority_class(counts: np.ndarray) -> int:\n        \"\"\"Smallest label in case of ties keeps result deterministic.\"\"\"\n        return int(np.argmax(counts))\n\n    def find_best_split(indices: np.ndarray) -> tuple[int, float, float, np.ndarray, np.ndarray] | None:\n        \"\"\"\n        Returns (best_feature, best_threshold, best_gain, left_idx, right_idx)\n        or None if no split brings positive gain.\n        \"\"\"\n        X_node = X_train[indices]\n        y_node = y_train[indices]\n        parent_counts = np.bincount(y_node, minlength=n_classes)\n        parent_gini = gini(parent_counts)\n        best_gain = 0.0\n        best_feature = -1\n        best_threshold = 0.0\n        best_left_idx = best_right_idx = None\n        (n_samples, n_features) = X_node.shape\n        for feat in range(n_features):\n            values = X_node[:, feat]\n            order = np.argsort(values)\n            sorted_y = y_node[order]\n            sorted_vals = values[order]\n            left_counts = np.zeros(n_classes, dtype=int)\n            right_counts = parent_counts.copy()\n            for i in range(1, n_samples):\n                cls = sorted_y[i - 1]\n                left_counts[cls] += 1\n                right_counts[cls] -= 1\n                if sorted_vals[i] == sorted_vals[i - 1]:\n                    continue\n                left_n = i\n                right_n = n_samples - i\n                g_left = gini(left_counts)\n                g_right = gini(right_counts)\n                weighted = (left_n * g_left + right_n * g_right) / n_samples\n                gain = parent_gini - weighted\n                if gain > best_gain + 1e-12:\n                    best_gain = gain\n                    best_feature = feat\n                    best_threshold = (sorted_vals[i] + sorted_vals[i - 1]) / 2.0\n                    mask = X_train[indices, feat] <= best_threshold\n                    best_left_idx = indices[mask]\n                    best_right_idx = indices[~mask]\n        if best_gain <= 0.0 or best_left_idx.size == 0 or best_right_idx.size == 0:\n            return None\n        return (best_feature, best_threshold, best_gain, best_left_idx, best_right_idx)\n\n    def build(indices: np.ndarray, depth: int) -> Node:\n        y_subset = y_train[indices]\n        counts = np.bincount(y_subset, minlength=n_classes)\n        if np.unique(y_subset).size == 1:\n            return Node(is_leaf=True, prediction=int(y_subset[0]))\n        if max_depth is not None and depth >= max_depth:\n            return Node(is_leaf=True, prediction=majority_class(counts))\n        split = find_best_split(indices)\n        if split is None:\n            return Node(is_leaf=True, prediction=majority_class(counts))\n        (feat, thr, _, left_idx, right_idx) = split\n        left_child = build(left_idx, depth + 1)\n        right_child = build(right_idx, depth + 1)\n        return Node(is_leaf=False, feature=feat, threshold=thr, left=left_child, right=right_child)\n    root = build(np.arange(X_train.shape[0]), depth=0)\n\n    def predict_single(x: np.ndarray) -> int:\n        node = root\n        while not node.is_leaf:\n            node = node.left if x[node.feature] <= node.threshold else node.right\n        return node.prediction\n    return [predict_single(row) for row in X_test]"}
{"task_id": 185, "completion_id": 0, "solution": "import numpy as np\ndef dataset_shape(X):\n    \"\"\"Inspect the input data and return *(n_samples, n_features)*.\n\n    Parameters\n    ----------\n    X : array-like\n        Feature data. If *X* is one-dimensional it is treated as a single\n        sample, otherwise the first axis counts the samples.\n\n    Returns\n    -------\n    tuple of two ints\n        *(n_samples, n_features)* extracted from *X* following the same logic\n        used by the `_setup_input` method in the supplied code snippet.\n    \"\"\"\n    X_arr = np.asarray(X)\n    if X_arr.size == 0:\n        raise ValueError('Got an empty matrix.')\n    if X_arr.ndim == 1:\n        n_samples = 1\n        n_features = int(X_arr.shape[0])\n        return (n_samples, n_features)\n    if X_arr.ndim >= 2:\n        n_samples = int(X_arr.shape[0])\n        n_features = int(np.prod(X_arr.shape[1:], dtype=int))\n        return (n_samples, n_features)\n    raise ValueError('Input must be at least 1-dimensional (got a scalar).')"}
{"task_id": 186, "completion_id": 0, "solution": "import numpy as np\ndef baseline_mean_predict(X_train, y_train, X_test):\n    \"\"\"Baseline mean regressor.\n\n    Parameters\n    ----------\n    X_train : array-like, shape (n_samples, n_features)\n        Training data (ignored).\n    y_train : array-like, shape (n_samples,)\n        Target values corresponding to *X_train*.\n    X_test : array-like, shape (m_samples, n_features)\n        Test samples for which predictions are required.\n\n    Returns\n    -------\n    list of float\n        A list with *m_samples* elements where every element equals the mean\n        of *y_train* rounded to 4 decimal places. If *X_test* is empty an\n        empty list is returned.\n    \"\"\"\n    X_test_arr = np.asarray(X_test)\n    y_train_arr = np.asarray(y_train)\n    m_samples = 0 if X_test_arr.ndim == 0 else X_test_arr.shape[0]\n    if m_samples == 0:\n        return []\n    mean_val = round(float(np.mean(y_train_arr)), 4)\n    return [mean_val] * m_samples"}
{"task_id": 188, "completion_id": 0, "solution": "import math\ndef epsilon_decay(max_epsilon: float, min_epsilon: float, decay_rate: float, step: int) -> float:\n    \"\"\"Compute the exponentially decayed \u03b5 (epsilon) for \u03b5-greedy exploration.\n\n    \u03b5(t) = \u03b5_min + (\u03b5_max \u2212 \u03b5_min) * exp(\u2212decay_rate * t)\n\n    Returns the value rounded to 4 decimal places, or \u22121 if any input is invalid.\n    \"\"\"\n    if not isinstance(step, int):\n        return -1\n    valid = 0.0 <= min_epsilon < max_epsilon and decay_rate > 0.0 and (step >= 0)\n    if not valid:\n        return -1\n    decayed = min_epsilon + (max_epsilon - min_epsilon) * math.exp(-decay_rate * step)\n    decayed = max(decayed, min_epsilon)\n    return round(decayed, 4)"}
{"task_id": 189, "completion_id": 0, "solution": "import numpy as np\ndef _get_indices_weights(in_size, out_size):\n    \"\"\"\n    Compute the source indices and interpolation weights needed to resize one\n    dimension from ``in_size`` to ``out_size`` using the classic\n    (i + 0.5) * scale - 0.5  mapping (i \u2011 centre format).\n\n    Returns\n    -------\n    idx0 : ndarray (out_size,)   \u2013  left / top  indices  (\u230acoord\u230b)\n    idx1 : ndarray (out_size,)   \u2013  right / bottom indices (idx0+1, clipped)\n    w    : ndarray (out_size,)   \u2013  weight for idx1   (w = coord \u2212 idx0)\n                                     so that value = (1-w)*v[idx0] + w*v[idx1]\n    \"\"\"\n    scale = in_size / out_size\n    coord = (np.arange(out_size) + 0.5) * scale - 0.5\n    idx0 = np.floor(coord).astype(np.int64)\n    idx1 = idx0 + 1\n    idx0 = np.clip(idx0, 0, in_size - 1)\n    idx1 = np.clip(idx1, 0, in_size - 1)\n    w = coord - idx0\n    w = np.clip(w, 0.0, 1.0)\n    return (idx0, idx1, w)\ndef batch_resample(X, new_dim, mode='bilinear'):\n    \"\"\"Resample a batch of images to a new spatial resolution.\n\n    Parameters\n    ----------\n    X : numpy.ndarray of shape (n_ex, in_rows, in_cols, in_channels)\n        Input batch of images.\n    new_dim : tuple[int, int]\n        Target dimension ``(out_rows, out_cols)``.\n    mode : {\"bilinear\", \"neighbor\"}, default=\"bilinear\"\n        Interpolation method.\n\n    Returns\n    -------\n    numpy.ndarray\n        Resampled batch with shape (n_ex, out_rows, out_cols, in_channels),\n        rounded to 4 decimal places.\n    \"\"\"\n    if mode not in ('bilinear', 'neighbor'):\n        raise NotImplementedError(f'Unrecognized resampling mode: {mode}')\n    if not (isinstance(new_dim, (tuple, list)) and len(new_dim) == 2):\n        raise ValueError('`new_dim` must be a tuple/list like (rows, cols)')\n    (n_ex, in_rows, in_cols, n_ch) = X.shape\n    (out_rows, out_cols) = map(int, new_dim)\n    if mode == 'neighbor':\n        row_scale = in_rows / out_rows\n        col_scale = in_cols / out_cols\n        row_coords = (np.arange(out_rows) + 0.5) * row_scale - 0.5\n        col_coords = (np.arange(out_cols) + 0.5) * col_scale - 0.5\n        row_idx = np.clip(np.round(row_coords).astype(np.int64), 0, in_rows - 1)\n        col_idx = np.clip(np.round(col_coords).astype(np.int64), 0, in_cols - 1)\n        tmp = np.take(X, row_idx, axis=1)\n        out = np.take(tmp, col_idx, axis=2)\n    else:\n        (r0, r1, wy) = _get_indices_weights(in_rows, out_rows)\n        (c0, c1, wx) = _get_indices_weights(in_cols, out_cols)\n        top = np.take(X, r0, axis=1)\n        bottom = np.take(X, r1, axis=1)\n        wy = wy.reshape(1, -1, 1, 1)\n        row_interp = (1.0 - wy) * top + wy * bottom\n        left = np.take(row_interp, c0, axis=2)\n        right = np.take(row_interp, c1, axis=2)\n        wx = wx.reshape(1, 1, -1, 1)\n        out = (1.0 - wx) * left + wx * right\n    return np.round(out, 4)"}
{"task_id": 190, "completion_id": 0, "solution": "import numpy as np\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted\n    Gini impurity for a single binary split.\n\n    Parameters\n    ----------\n    X : list[list[float]] | np.ndarray  (n_samples \u00d7 n_features)\n    y : list[int] | np.ndarray          (n_samples,)\n\n    Returns\n    -------\n    (int, float | None, float)\n        (best_feature_index, best_threshold_value, best_gini_rounded)\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    if X.ndim != 2:\n        raise ValueError('X must be 2-dimensional')\n    if y.ndim != 1 or y.shape[0] != X.shape[0]:\n        raise ValueError('y must be 1-D and have the same length as X')\n    (n_samples, n_features) = X.shape\n    if n_samples == 0 or n_features == 0:\n        return (-1, None, 0.0)\n    (classes, y_enc) = np.unique(y, return_inverse=True)\n    n_classes = classes.size\n\n    def gini(counts, n):\n        if n == 0:\n            return 0.0\n        p = counts / n\n        return 1.0 - np.sum(p * p)\n    total_counts = np.bincount(y_enc, minlength=n_classes)\n    g_whole = gini(total_counts, n_samples)\n    best_gini = np.inf\n    best_feat = -1\n    best_th = None\n    tol = 1e-12\n    for j in range(n_features):\n        col = X[:, j]\n        if np.all(col == col[0]):\n            continue\n        order = np.argsort(col, kind='mergesort')\n        col_sorted = col[order]\n        y_sorted = y_enc[order]\n        left_counts = np.zeros(n_classes, dtype=int)\n        for i in range(n_samples):\n            cls = y_sorted[i]\n            left_counts[cls] += 1\n            is_last_occurrence = i == n_samples - 1 or col_sorted[i] != col_sorted[i + 1]\n            if not is_last_occurrence:\n                continue\n            n_left = i + 1\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:\n                continue\n            right_counts = total_counts - left_counts\n            g_left = gini(left_counts, n_left)\n            g_right = gini(right_counts, n_right)\n            g_split = n_left / n_samples * g_left + n_right / n_samples * g_right\n            if g_split + tol < best_gini:\n                (best_gini, best_feat, best_th) = (g_split, j, col_sorted[i])\n            elif abs(g_split - best_gini) < tol:\n                if j < best_feat:\n                    (best_feat, best_th) = (j, col_sorted[i])\n                elif j == best_feat and col_sorted[i] < best_th:\n                    best_th = col_sorted[i]\n    if best_feat == -1:\n        return (-1, None, round(g_whole, 4))\n    return (best_feat, best_th, round(best_gini, 4))"}
{"task_id": 191, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef metric(name: str):\n    \"\"\"Return a distance function with integrated validation.\"\"\"\n    supported = ('euclidean', 'manhattan', 'chebyshev', 'cosine')\n\n    def d(v1, v2):\n        if name not in supported:\n            return -1\n        if not isinstance(v1, (list, tuple, np.ndarray)):\n            return -1\n        if not isinstance(v2, (list, tuple, np.ndarray)):\n            return -1\n        try:\n            a = np.asarray(v1, dtype=float)\n            b = np.asarray(v2, dtype=float)\n        except Exception:\n            return -1\n        if a.ndim != 1 or b.ndim != 1:\n            return -1\n        if a.size == 0 or b.size == 0:\n            return -1\n        if a.size != b.size:\n            return -1\n        if name == 'euclidean':\n            value = np.linalg.norm(a - b)\n        elif name == 'manhattan':\n            value = np.sum(np.abs(a - b))\n        elif name == 'chebyshev':\n            value = np.max(np.abs(a - b))\n        else:\n            (na, nb) = (np.linalg.norm(a), np.linalg.norm(b))\n            if na == 0 or nb == 0:\n                return -1\n            value = 1 - a @ b / (na * nb)\n        return round(float(value), 4)\n    return d"}
{"task_id": 193, "completion_id": 0, "solution": "import numpy as np\ndef softsign(z):\n    \"\"\"Compute the Softsign activation function.\n\n    Softsign(z) = z / (1 + |z|)\n\n    Parameters\n    ----------\n    z : int, float, list or np.ndarray\n        \u2022 Scalar (int/float/NumPy scalar) \u2013 returns a single float  \n        \u2022 1-D list / NumPy array        \u2013 returns a list of floats\n\n    Returns\n    -------\n    float or list\n        Softsign-transformed value(s), rounded to 4 decimal places.\n    \"\"\"\n    if np.isscalar(z):\n        return round(z / (1 + abs(z)), 4)\n    arr = np.asarray(z, dtype=np.float64)\n    if arr.size == 0:\n        return []\n    result = arr / (1 + np.abs(arr))\n    return [round(float(x), 4) for x in result.tolist()]"}
{"task_id": 194, "completion_id": 0, "solution": "import numpy as np\ndef adagrad_update(weights: list[float], gradients: list[float], G: list[float] | None=None, learning_rate: float=0.01, eps: float=1e-08) -> tuple[list[float], list[float]]:\n    \"\"\"Performs one Adagrad optimisation step.\n\n    Args:\n        weights: Current parameter vector.\n        gradients: Current gradient vector of the loss with respect to *weights*.\n        G: Running sum of squared gradients (None on the very first step).\n        learning_rate: Global learning-rate (\u03b7).\n        eps: Small constant to avoid division by zero.\n\n    Returns:\n        A tuple (new_weights, new_G) where\n            new_weights \u2013 list of updated parameters rounded to 6 decimals,\n            new_G       \u2013 list of updated accumulated squared gradients\n                           rounded to 6 decimals.\n    \"\"\"\n    w = np.asarray(weights, dtype=float)\n    grad = np.asarray(gradients, dtype=float)\n    if w.shape != grad.shape:\n        raise ValueError('`weights` and `gradients` must have the same length.')\n    if G is None:\n        G_prev = np.zeros_like(w)\n    else:\n        G_prev = np.asarray(G, dtype=float)\n        if G_prev.shape != w.shape:\n            raise ValueError('`G` must be the same length as `weights`.')\n    G_new = G_prev + grad ** 2\n    w_new = w - learning_rate * grad / np.sqrt(G_new + eps)\n    w_new = np.round(w_new, 6).tolist()\n    G_new = np.round(G_new, 6).tolist()\n    return (w_new, G_new)"}
{"task_id": 197, "completion_id": 0, "solution": "import math\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\nimport numpy as np\ndef _is_discrete(space: Any) -> bool:\n    \"\"\"Utility: does the sub-space describe a discrete set?\"\"\"\n    return hasattr(space, 'n')\ndef _is_continuous(space: Any) -> bool:\n    \"\"\"Utility: does the sub-space describe a continuous set?\"\"\"\n    return hasattr(space, 'shape')\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"\n    Summarise an RL environment's action space.\n\n    Parameters\n    ----------\n    env : Any\n        Environment exposing an `action_space` attribute \u00e0-la OpenAI-Gym.\n    md_action : bool\n        Ignored by the logic (kept for the required signature) \u2013 the function\n        determines the true structure directly from `env.action_space`.\n    cont_action : bool\n        Ignored by the logic (kept for the required signature) \u2013 the function\n        determines the true structure directly from `env.action_space`.\n\n    Returns\n    -------\n    Tuple containing\n        n_actions_per_dim : list[int | float]\n            Number of distinct actions for every dimension\n            (`np.inf` for continuous ones).\n        action_ids : list[tuple[int, ...]] | None\n            All possible discrete actions as tuples; `None` if at least one\n            dimension is continuous.\n        action_dim : int\n            Number of action dimensions.\n    \"\"\"\n    space = env.action_space\n    n_actions_per_dim: List[Union[int, float]] = []\n    if hasattr(space, 'spaces'):\n        subspaces = space.spaces\n        for sub in subspaces:\n            if _is_discrete(sub):\n                n_actions_per_dim.append(int(sub.n))\n            elif _is_continuous(sub):\n                n_actions_per_dim.append(np.inf)\n            else:\n                n_actions_per_dim.append(np.inf)\n        action_dim = len(subspaces)\n    elif _is_discrete(space):\n        n_actions_per_dim.append(int(space.n))\n        action_dim = 1\n    elif _is_continuous(space):\n        dim = int(space.shape[0]) if len(space.shape) > 0 else 1\n        n_actions_per_dim.extend([np.inf] * dim)\n        action_dim = dim\n    else:\n        n_actions_per_dim.append(np.inf)\n        action_dim = 1\n    contains_continuous = any((np.isinf(x) for x in n_actions_per_dim))\n    if contains_continuous:\n        action_ids = None\n    else:\n        ranges = [range(int(n)) for n in n_actions_per_dim]\n        action_ids = [tuple(p) for p in product(*ranges)]\n    return (n_actions_per_dim, action_ids, action_dim)"}
{"task_id": 198, "completion_id": 0, "solution": "import numpy as np\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        One array per document. Array of document *d* has shape (N_d, T)\n        and contains the current variational parameter \u03d5 of this document.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the index of the *n*-th token\n        in document *d*.\n    V : int\n        Size of the vocabulary.\n\n    Returns\n    -------\n    list[list[float]]\n        (V \u00d7 T) matrix with the updated word\u2013topic probabilities.\n        Each column sums to one (\u2264 1e-4 rounding error) and every\n        number is rounded to four decimals.\n    \"\"\"\n    if not phi:\n        return [[0.0] * 0 for _ in range(V)]\n    T = phi[0].shape[1]\n    counts = np.zeros((V, T), dtype=np.float64)\n    for (doc_phi, doc_words) in zip(phi, corpus):\n        words = np.asarray(doc_words, dtype=np.intp)\n        np.add.at(counts, words, doc_phi)\n    seen_words_mask = counts.sum(axis=1) > 0\n    col_sums = counts.sum(axis=0)\n    beta = np.zeros_like(counts)\n    non_zero_cols = col_sums > 0\n    beta[:, non_zero_cols] = counts[:, non_zero_cols] / col_sums[non_zero_cols]\n    zero_cols = ~non_zero_cols\n    if zero_cols.any():\n        observed_idx = np.where(seen_words_mask)[0]\n        if observed_idx.size == 0:\n            beta[:, zero_cols] = 1.0 / V\n        else:\n            beta[observed_idx[:, None], zero_cols] = 1.0 / observed_idx.size\n    beta = np.round(beta, 4)\n    for t in range(T):\n        diff = 1.0 - beta[:, t].sum()\n        if abs(diff) > 0.0001:\n            col = beta[:, t]\n            idx = int(np.argmax(col))\n            adj = np.round(diff, 4)\n            new_val = np.round(col[idx] + adj, 4)\n            beta[idx, t] = max(new_val, 0.0)\n    return beta.tolist()"}
{"task_id": 199, "completion_id": 0, "solution": "import numpy as np\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    \"\"\"Split the label array `y` into two parts based on a threshold applied to `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        1-D array of feature values.\n    y : np.ndarray\n        1-D array of labels, same length as `X`.\n    value : float\n        Threshold used for the split.\n\n    Returns\n    -------\n    tuple[list[int | float], list[int | float]]\n        (y_left, y_right) where\n        * y_left  contains labels with `X < value`.\n        * y_right contains labels with `X >= value`.\n    \"\"\"\n    if X.ndim != 1 or y.ndim != 1:\n        raise ValueError('X and y must be one-dimensional arrays.')\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same length.')\n    left_mask = X < value\n    right_mask = ~left_mask\n    y_left = y[left_mask].tolist()\n    y_right = y[right_mask].tolist()\n    return (y_left, y_right)"}
{"task_id": 200, "completion_id": 0, "solution": "import numpy as np\nfrom numbers import Number\nfrom collections.abc import Iterable\ndef _abs_sum(x):\n    \"\"\"Recursively compute \u03a3|x\u1d62| for any nested structure.\"\"\"\n    if isinstance(x, Number):\n        return abs(float(x))\n    if isinstance(x, np.ndarray):\n        return float(np.abs(x).sum())\n    if isinstance(x, Iterable) and (not isinstance(x, (str, bytes))):\n        total = 0.0\n        for item in x:\n            total += _abs_sum(item)\n        return total\n    raise TypeError(f'Unsupported element type: {type(x)}')\ndef compute_l1_penalty(weights, C):\n    \"\"\"Compute the L1 regularization penalty.\n\n    Args:\n        weights: Sequence or np.ndarray containing numeric weights.\n        C: Non-negative regularization strength (float).\n\n    Returns:\n        L1 penalty (float) rounded to 4 decimal places.\n    \"\"\"\n    if weights is None:\n        return 0.0\n    try:\n        arr = np.asarray(weights, dtype=float)\n        if arr.size == 0:\n            return 0.0\n        base_sum = float(np.abs(arr).sum())\n    except (ValueError, TypeError):\n        base_sum = _abs_sum(weights)\n    penalty = C * base_sum\n    return round(penalty, 4)"}
{"task_id": 201, "completion_id": 0, "solution": "import numpy as np\ndef max_norm(p, m=2.0, axis=0):\n    \"\"\"Clip the L2-norm of slices of *p* along *axis* so that they do not exceed *m*.\"\"\"\n    p = np.asarray(p, dtype=float)\n    eps = 1e-12\n    if axis is None:\n        norm = np.linalg.norm(p)\n        scale = min(1.0, m / (norm + eps))\n        p_hat = p * scale\n    else:\n        axis = axis if axis >= 0 else p.ndim + axis\n        norms = np.linalg.norm(p, axis=axis, keepdims=True)\n        scales = np.minimum(1.0, m / (norms + eps))\n        p_hat = p * scales\n    return np.round(p_hat, 4).tolist()"}
{"task_id": 202, "completion_id": 0, "solution": "import numpy as np\ndef _total_cost(dist_matrix: np.ndarray, medoid_idx: list[int]) -> float:\n    \"\"\"\n    Return the sum of distances of every sample to its nearest medoid.\n    \"\"\"\n    d_to_medoids = dist_matrix[:, medoid_idx]\n    return np.min(d_to_medoids, axis=1).sum()\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Input data.\n    k : int\n        Number of desired clusters (1 \u2264 k \u2264 n_samples).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels (0 \u2026 k-1) for every sample.\n    \"\"\"\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D NumPy array.')\n    n_samples = X.shape[0]\n    if not 1 <= k <= n_samples:\n        raise ValueError('k must satisfy 1 \u2264 k \u2264 n_samples')\n    diff = X[:, None, :] - X[None, :, :]\n    dist_matrix = np.linalg.norm(diff, axis=-1)\n    medoid_idx: list[int] = list(range(k))\n    current_cost = _total_cost(dist_matrix, medoid_idx)\n    improved = True\n    while improved:\n        best_swap = None\n        best_reduction = 0.0\n        non_medoid_idx = [i for i in range(n_samples) if i not in medoid_idx]\n        for (p_pos, p) in enumerate(medoid_idx):\n            for h in non_medoid_idx:\n                candidate_medoid_idx = medoid_idx.copy()\n                candidate_medoid_idx[p_pos] = h\n                cand_cost = _total_cost(dist_matrix, candidate_medoid_idx)\n                reduction = current_cost - cand_cost\n                if reduction > best_reduction:\n                    best_reduction = reduction\n                    best_swap = (p_pos, h)\n        if best_swap is not None:\n            (p_pos, h) = best_swap\n            medoid_idx[p_pos] = h\n            current_cost -= best_reduction\n        else:\n            improved = False\n    final_dists = dist_matrix[:, medoid_idx]\n    labels = final_dists.argmin(axis=1).tolist()\n    return labels"}
{"task_id": 203, "completion_id": 0, "solution": "import numpy as np\ndef build_alias_table(probs: list[float] | np.ndarray) -> tuple[list[float], list[int]] | int:\n    \"\"\"Build probability and alias tables for Vose\u2019s alias sampling method.\n\n    Parameters\n    ----------\n    probs : list[float] | np.ndarray\n        1-D array\u2010like of positive numbers that must sum to 1.\n\n    Returns\n    -------\n    (prob_table, alias_table) : tuple[list[float], list[int]]\n        Both lists have length N.  Values in `prob_table` are rounded\n        to four decimals.\n        If the input is invalid the function returns -1.\n    \"\"\"\n    probs = np.asarray(probs, dtype=float)\n    if probs.ndim != 1 or probs.size == 0:\n        return -1\n    if np.any(probs < 0):\n        return -1\n    if abs(probs.sum() - 1.0) > 1e-12:\n        return -1\n    N = probs.size\n    scaled_probs = probs * N\n    small = [i for (i, p) in enumerate(scaled_probs) if p < 1.0]\n    large = [i for (i, p) in enumerate(scaled_probs) if p >= 1.0]\n    prob_table = np.zeros(N, dtype=float)\n    alias_table = np.zeros(N, dtype=int)\n    while small and large:\n        l = small.pop()\n        g = large.pop()\n        prob_table[l] = scaled_probs[l]\n        alias_table[l] = g\n        scaled_probs[g] = scaled_probs[g] + scaled_probs[l] - 1.0\n        if scaled_probs[g] < 1.0:\n            small.append(g)\n        else:\n            large.append(g)\n    for idx in large:\n        prob_table[idx] = 1.0\n    for idx in small:\n        prob_table[idx] = 1.0\n    prob_list = [round(float(x), 4) for x in prob_table]\n    alias_list = alias_table.tolist()\n    return (prob_list, alias_list)"}
{"task_id": 204, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef num_missing(data) -> int:\n    \"\"\"Count missing values in a 1-D iterable.\n\n    An element is missing if it is ``None`` or the special floating-point\n    value ``NaN`` (Not-a-Number).\n\n    Parameters\n    ----------\n    data : iterable\n        Any one-dimensional iterable (list, tuple, NumPy array, \u2026).\n\n    Returns\n    -------\n    int\n        Number of missing elements.  Returns 0 for an empty input.\n    \"\"\"\n    try:\n        iterator = iter(data)\n    except TypeError:\n        raise TypeError('`data` must be an iterable')\n    missing = 0\n    for item in iterator:\n        if item is None:\n            missing += 1\n            continue\n        try:\n            if math.isnan(item):\n                missing += 1\n                continue\n        except TypeError:\n            pass\n        try:\n            if np.isnan(item):\n                missing += 1\n        except (TypeError, ValueError):\n            pass\n    return int(missing)"}
{"task_id": 205, "completion_id": 0, "solution": "def get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    unique_ids = []\n    seen = set()\n    for item in registry:\n        if isinstance(item, dict) and 'id' in item:\n            env_id = item['id']\n            if env_id not in seen:\n                unique_ids.append(env_id)\n                seen.add(env_id)\n    return unique_ids"}
{"task_id": 206, "completion_id": 0, "solution": "import numpy as np\ndef absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        A one-dimensional sequence of ground-truth numeric values.\n    predicted : list | tuple | np.ndarray\n        A one-dimensional sequence of predicted numeric values.\n\n    Returns\n    -------\n    float | int\n        The MAE rounded to four decimal places. If the two sequences are not\n        of equal length or are not one-dimensional, return -1.\n    \"\"\"\n    try:\n        actual_arr = np.asarray(actual, dtype=float)\n        predicted_arr = np.asarray(predicted, dtype=float)\n    except (TypeError, ValueError):\n        return -1\n    if actual_arr.ndim != 1 or predicted_arr.ndim != 1 or actual_arr.size != predicted_arr.size or (actual_arr.size == 0):\n        return -1\n    mae = np.mean(np.abs(actual_arr - predicted_arr))\n    return float(np.round(mae, 4))"}
{"task_id": 207, "completion_id": 0, "solution": "def convolution_shape(img_height: int, img_width: int, filter_shape: tuple[int, int], stride: tuple[int, int], padding: tuple[int, int]) -> tuple[int, int] | int:\n    \"\"\"Compute the spatial dimensions of the output produced by a 2-D convolution.\n\n    Args:\n        img_height (int): Height  (H) of the input image.\n        img_width  (int): Width   (W) of the input image.\n        filter_shape (tuple[int, int]): (kernel_height KH, kernel_width KW).\n        stride       (tuple[int, int]): (stride_height SH, stride_width  SW).\n        padding      (tuple[int, int]): (pad_height    PH, pad_width     PW).\n\n    Returns\n    -------\n        (OH, OW): tuple[int, int]   if the configuration is valid.\n        -1:                        otherwise.\n    \"\"\"\n    is_pos_int = lambda x: isinstance(x, int) and x > 0\n    is_nonneg_int = lambda x: isinstance(x, int) and x >= 0\n    if not (is_pos_int(img_height) and is_pos_int(img_width)):\n        return -1\n    if not isinstance(filter_shape, tuple) or len(filter_shape) != 2:\n        return -1\n    (KH, KW) = filter_shape\n    if not (is_pos_int(KH) and is_pos_int(KW)):\n        return -1\n    if not isinstance(stride, tuple) or len(stride) != 2:\n        return -1\n    (SH, SW) = stride\n    if not (is_pos_int(SH) and is_pos_int(SW)):\n        return -1\n    if not isinstance(padding, tuple) or len(padding) != 2:\n        return -1\n    (PH, PW) = padding\n    if not (is_nonneg_int(PH) and is_nonneg_int(PW)):\n        return -1\n    num_H = img_height + 2 * PH - KH\n    num_W = img_width + 2 * PW - KW\n    if num_H < 0 or num_W < 0:\n        return -1\n    if num_H % SH != 0 or num_W % SW != 0:\n        return -1\n    OH = num_H // SH + 1\n    OW = num_W // SW + 1\n    if OH <= 0 or OW <= 0:\n        return -1\n    return (OH, OW)"}
{"task_id": 208, "completion_id": 0, "solution": "import numpy as np\ndef one_hot_encode(labels: list[str], categories: list[str] | None=None) -> list[list[int]]:\n    \"\"\"Convert categorical labels to a one-hot encoded matrix.\n\n    Args:\n        labels: A list (length N) with the categorical labels to be encoded.\n        categories: Optional list (length C) that contains the complete set\n            of allowed categories and fixes their order.  If *None*, the\n            categories are inferred from *labels* preserving the order of\n            their first appearance.\n\n    Returns\n    -------\n    list[list[int]]\n        An (N \u00d7 C) one-hot encoded matrix represented as a list of lists.\n        If *categories* is given and an unknown label is encountered, the\n        function returns **-1**.\n    \"\"\"\n    if categories is None:\n        categories = []\n        seen = set()\n        for lbl in labels:\n            if lbl not in seen:\n                seen.add(lbl)\n                categories.append(lbl)\n    else:\n        categories = list(categories)\n    cat2idx = {cat: idx for (idx, cat) in enumerate(categories)}\n    C = len(categories)\n    N = len(labels)\n    one_hot = np.zeros((N, C), dtype=int)\n    for (row, lbl) in enumerate(labels):\n        idx = cat2idx.get(lbl)\n        if idx is None:\n            return -1\n        one_hot[row, idx] = 1\n    return one_hot.tolist()"}
{"task_id": 209, "completion_id": 0, "solution": "import numpy as np\ndef analyze_bandit(payoffs: list[list[int | float]], payoff_probs: list[list[float]]) -> tuple[list[float], int, float] | int:\n    \"\"\"Validate a multi-armed bandit description and compute the arms\u2019 expected pay-offs.\n\n    The function returns\n        (expected_values, best_arm, best_ev)\n\n    or the integer \u20131 if the input is invalid (see the doc-string in the task\n    description for the exact requirements).\n    \"\"\"\n    tol = 1e-08\n    if not isinstance(payoffs, list) or not isinstance(payoff_probs, list):\n        return -1\n    if len(payoffs) != len(payoff_probs):\n        return -1\n    expected_values = []\n    for (arm_idx, (vals, probs)) in enumerate(zip(payoffs, payoff_probs)):\n        if len(vals) != len(probs) or len(vals) == 0:\n            return -1\n        if not all((isinstance(p, (int, float)) and 0.0 <= p <= 1.0 for p in probs)):\n            return -1\n        if abs(sum(probs) - 1.0) > tol:\n            return -1\n        ev = float(np.dot(vals, probs))\n        expected_values.append(round(ev, 4))\n    max_ev = max(expected_values)\n    best_arm = next((i for (i, ev) in enumerate(expected_values) if ev == max_ev))\n    return (expected_values, best_arm, max_ev)"}
{"task_id": 210, "completion_id": 0, "solution": "import functools\nfrom typing import Callable, Any\ndef doublewrap(decorator_func: Callable[..., Callable[..., Any]]) -> Callable[..., Any]:\n    \"\"\"Make *decorator_func* usable with or without its own parentheses.\n\n    The returned object can therefore be written either as\n\n        @wrapped_deco\n        def func(...): ...\n\n    or\n\n        @wrapped_deco(arg, kw=val)\n        def func(...): ...\n\n    while in both cases *decorator_func* is finally invoked as\n\n        decorator_func(func, *arg, **kw)\n    \"\"\"\n\n    @functools.wraps(decorator_func)\n    def decorator_or_factory(*args, **kwargs):\n        if args and callable(args[0]) and (len(args) == 1) and (not kwargs):\n            target_func = args[0]\n            return decorator_func(target_func)\n\n        @functools.wraps(decorator_func)\n        def real_decorator(func: Callable[..., Any]) -> Callable[..., Any]:\n            return decorator_func(func, *args, **kwargs)\n        return real_decorator\n    return decorator_or_factory"}
{"task_id": 211, "completion_id": 0, "solution": "def accuracy(actual, predicted):\n    \"\"\"Return the classification accuracy between two sequences of labels.\n\n    Parameters\n    ----------\n    actual : Sequence\n        The ground-truth labels.\n    predicted : Sequence\n        The predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy rounded to four decimal places, or -1 if the inputs\n        are empty or of unequal length.\n    \"\"\"\n    actual_list = list(actual)\n    predicted_list = list(predicted)\n    if not actual_list or len(actual_list) != len(predicted_list):\n        return -1\n    mismatches = sum((1 for (a, p) in zip(actual_list, predicted_list) if a != p))\n    classification_error = mismatches / len(actual_list)\n    acc = 1.0 - classification_error\n    return round(acc, 4)"}
{"task_id": 212, "completion_id": 0, "solution": "from collections import deque\ndef extract_realtime_model(graph: dict[str, list[str]]) -> list[str]:\n    \"\"\"Return one shortest \u2013 and lexicographically smallest \u2013 path from the\n    layer called \"input\" to the layer called \"output_realtime\".\n\n    Parameters\n    ----------\n    graph : dict[str, list[str]]\n        Directed graph given as adjacency list (layer -> list[successors]).\n\n    Returns\n    -------\n    list[str]\n        Names of the layers on the desired path or an empty list if no\n        such path exists.\n    \"\"\"\n    start = 'input'\n    goal = 'output_realtime'\n    if start == goal:\n        return [start]\n    queue = deque([start])\n    visited = {start}\n    parent: dict[str, str] = {}\n    while queue:\n        current = queue.popleft()\n        for neighbour in sorted(graph.get(current, [])):\n            if neighbour in visited:\n                continue\n            visited.add(neighbour)\n            parent[neighbour] = current\n            if neighbour == goal:\n                path = [goal]\n                while path[-1] != start:\n                    path.append(parent[path[-1]])\n                return list(reversed(path))\n            queue.append(neighbour)\n    return []"}
{"task_id": 213, "completion_id": 0, "solution": "import numpy as np\ndef he_normal(shape: tuple[int, ...]) -> list:\n    \"\"\"Return weights initialised with the He (Kaiming) Normal strategy.\n    \n    Args:\n        shape: Desired tensor shape.\n        \n    Returns:\n        Nested Python list of weights drawn from \ud835\udca9(0, \u221a(2 / fan_in)).\n    \"\"\"\n    dim = len(shape)\n    if dim == 0:\n        fan_in = fan_out = 1\n    elif dim == 1:\n        fan_in = fan_out = shape[0]\n    elif dim == 2:\n        (fan_in, fan_out) = (shape[0], shape[1])\n    else:\n        (out_channels, in_channels) = (shape[0], shape[1])\n        receptive_field = np.prod(shape[2:])\n        fan_in = in_channels * receptive_field\n        fan_out = out_channels * receptive_field\n    if fan_in == 0:\n        return []\n    rng = np.random.default_rng(seed=42)\n    std = np.sqrt(2.0 / fan_in)\n    data = rng.normal(loc=0.0, scale=std, size=shape)\n    return data.tolist()"}
{"task_id": 214, "completion_id": 0, "solution": "import numpy as np\ndef mean_squared_error(actual, predicted):\n    \"\"\"Calculate the Mean Squared Error (MSE) between two numerical sequences.\n\n    The function returns -1 if the two input sequences do not share the same\n    shape/length. Otherwise, it returns the MSE rounded to 4 decimal places.\n    Both Python lists and NumPy arrays are supported.\n\n    Args:\n        actual (list[int | float] | np.ndarray): Ground-truth values.\n        predicted (list[int | float] | np.ndarray): Predicted values.\n\n    Returns:\n        float | int: The rounded MSE, or -1 when the inputs are incompatible.\n    \"\"\"\n    a = np.asarray(actual, dtype=float)\n    p = np.asarray(predicted, dtype=float)\n    if a.shape != p.shape:\n        return -1\n    mse = np.mean((a - p) ** 2)\n    return float(np.round(mse, 4))"}
{"task_id": 216, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic-Net linear regression trained with batch gradient descent.\"\"\"\n    try:\n        X_tr = np.asarray(x_train, dtype=float)\n        X_te = np.asarray(x_test, dtype=float)\n        y_tr = np.asarray(y_train, dtype=float).ravel()\n    except Exception:\n        return -1\n    if X_tr.ndim == 1:\n        X_tr = X_tr.reshape(-1, 1)\n    if X_te.ndim == 1:\n        X_te = X_te.reshape(-1, 1)\n    if X_tr.shape[1] != X_te.shape[1] or n_iterations < 1:\n        return -1\n\n    def poly_expansion(matrix, deg):\n        if deg <= 1:\n            return matrix.copy()\n        powers = [matrix ** p for p in range(1, deg + 1)]\n        return np.concatenate(powers, axis=1)\n    X_tr_poly = poly_expansion(X_tr, degree)\n    X_te_poly = poly_expansion(X_te, degree)\n    mu = X_tr_poly.mean(axis=0)\n    sigma = X_tr_poly.std(axis=0)\n    sigma[sigma == 0] = 1.0\n    X_tr_std = (X_tr_poly - mu) / sigma\n    X_te_std = (X_te_poly - mu) / sigma\n    X_tr_std = np.hstack((np.ones((X_tr_std.shape[0], 1)), X_tr_std))\n    X_te_std = np.hstack((np.ones((X_te_std.shape[0], 1)), X_te_std))\n    (n_samples, n_features) = X_tr_std.shape\n    w = np.zeros(n_features)\n    l2_factor = reg_factor * (1.0 - l1_ratio)\n    l1_factor = reg_factor * l1_ratio\n    for _ in range(int(n_iterations)):\n        y_hat = X_tr_std @ w\n        error = y_hat - y_tr\n        grad = X_tr_std.T @ error / n_samples\n        grad[1:] += l2_factor * w[1:]\n        grad[1:] += l1_factor * np.sign(w[1:])\n        w -= learning_rate * grad\n    y_pred = X_te_std @ w\n    return np.round(y_pred, 4).tolist()"}
{"task_id": 217, "completion_id": 0, "solution": "import numpy as np\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    Args:\n        actual    : 1-D container (list/tuple/np.ndarray) with elements +1 or \u22121.\n        predicted : 1-D container (list/tuple/np.ndarray) with raw model scores.\n\n    Returns:\n        (grad_list, hess_list, prob_list) \u2013 three ordinary Python lists,\n        each value rounded to exactly 6 decimal places.\n    \"\"\"\n    y = np.asarray(actual, dtype=float)\n    z = np.asarray(predicted, dtype=float)\n    if y.shape != z.shape:\n        raise ValueError('`actual` and `predicted` must have the same length.')\n\n    def _sigmoid(x):\n        out = np.empty_like(x, dtype=float)\n        pos_mask = x >= 0\n        neg_mask = ~pos_mask\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n        exp_x = np.exp(x[neg_mask])\n        out[neg_mask] = exp_x / (1.0 + exp_x)\n        return out\n    prob = _sigmoid(z)\n    grad = y * _sigmoid(-y * z)\n    hess = prob * (1.0 - prob)\n    prob = np.round(prob, 6).tolist()\n    grad = np.round(grad, 6).tolist()\n    hess = np.round(hess, 6).tolist()\n    return (grad, hess, prob)"}
{"task_id": 218, "completion_id": 0, "solution": "def blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n\n    def hand_score(hand: list[int]) -> int:\n        \"\"\"Return the best score (\u226421 if possible) for this hand.\"\"\"\n        s = sum(hand)\n        if 1 in hand and s + 10 <= 21:\n            return s + 10\n        return s\n\n    def is_bust(hand: list[int]) -> bool:\n        \"\"\"True if the hand is bust (>21).\"\"\"\n        return hand_score(hand) > 21\n\n    def is_natural(hand: list[int]) -> bool:\n        \"\"\"True if the hand is a natural blackjack (exactly two cards: Ace + 10).\"\"\"\n        return len(hand) == 2 and 1 in hand and (10 in hand)\n    player_score = hand_score(player)\n    dealer_score = hand_score(dealer)\n    player_bust = player_score > 21\n    dealer_bust = dealer_score > 21\n    player_natural = is_natural(player)\n    if player_bust:\n        return -1.0\n    if dealer_bust:\n        return 1.5 if player_natural else 1.0\n    if player_score > dealer_score:\n        return 1.5 if player_natural else 1.0\n    if player_score < dealer_score:\n        return -1.0\n    return 0.0"}
{"task_id": 220, "completion_id": 0, "solution": "import numpy as np\ndef estimate_lda_params(C_wt, C_dt, alpha, beta):\n    \"\"\"Estimate the word-topic (phi) and document-topic (theta) \n    distributions for an LDA model with symmetric Dirichlet smoothing.\n\n    Parameters\n    ----------\n    C_wt : numpy.ndarray\n        Word\u2013topic count matrix  (V \u00d7 T).\n    C_dt : numpy.ndarray\n        Document\u2013topic count matrix (D \u00d7 T).\n    alpha : float\n        Symmetric Dirichlet prior for theta.\n    beta  : float\n        Symmetric Dirichlet prior for phi.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[list[float]]]\n        (phi, theta) \u2013 both rounded to 4 decimals and given \n        as ordinary Python nested lists.  If a count matrix is\n        empty, the corresponding output is an empty list.\n    \"\"\"\n    C_wt = np.asarray(C_wt)\n    C_dt = np.asarray(C_dt)\n    if C_wt.size == 0:\n        phi = []\n    else:\n        (V, T) = C_wt.shape\n        topic_totals = C_wt.sum(axis=0) + V * beta\n        phi_mat = (C_wt + beta) / topic_totals\n        phi = np.round(phi_mat, 4).tolist()\n    if C_dt.size == 0:\n        theta = []\n    else:\n        (D, T_) = C_dt.shape\n        doc_totals = C_dt.sum(axis=1, keepdims=True) + T_ * alpha\n        theta_mat = (C_dt + alpha) / doc_totals\n        theta = np.round(theta_mat, 4).tolist()\n    return (phi, theta)"}
{"task_id": 221, "completion_id": 0, "solution": "import numpy as np\ndef squared_log_error(actual, predicted):\n    \"\"\"\n    Element-wise squared logarithmic error.\n\n    Parameters\n    ----------\n    actual, predicted : np.ndarray\n        Arrays of the same shape containing non-negative numbers.\n\n    Returns\n    -------\n    np.ndarray\n        (log1p(actual) - log1p(predicted))**2 computed element-wise.\n    \"\"\"\n    return np.square(np.log1p(actual) - np.log1p(predicted))\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        True target values (must be non-negative).\n    predicted : list | tuple | np.ndarray\n        Predicted values (must be non-negative and same shape as `actual`).\n\n    Returns\n    -------\n    float\n        MSLE rounded to 4 decimal places, or -1 if the inputs are invalid.\n    \"\"\"\n    try:\n        actual_arr = np.asarray(actual, dtype=float)\n        pred_arr = np.asarray(predicted, dtype=float)\n    except Exception:\n        return -1\n    if actual_arr.shape != pred_arr.shape:\n        return -1\n    if actual_arr.size == 0:\n        return -1\n    if np.isnan(actual_arr).any() or np.isnan(pred_arr).any():\n        return -1\n    if np.any(actual_arr < 0) or np.any(pred_arr < 0):\n        return -1\n    sq_log_err = squared_log_error(actual_arr, pred_arr)\n    msle = float(np.mean(sq_log_err))\n    return round(msle, 4)"}
{"task_id": 222, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function works for scalar numbers, Python lists and NumPy arrays and\n    remains numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: If `z` is a scalar, the sigmoid value rounded to 4\n        decimals (float).  Otherwise, a Python list with the same nested\n        structure where every element is rounded to 4 decimals.\n    \"\"\"\n    if np.isscalar(z):\n        z = float(z)\n        if z >= 0:\n            val = 1.0 / (1.0 + np.exp(-z))\n        else:\n            exp_z = np.exp(z)\n            val = exp_z / (1.0 + exp_z)\n        return round(val, 4)\n    arr = np.asarray(z, dtype=float)\n    out = np.empty_like(arr)\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    exp_vals = np.exp(arr[neg_mask])\n    out[neg_mask] = exp_vals / (1.0 + exp_vals)\n    return np.round(out, 4).tolist()"}
{"task_id": 224, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky-ReLU activation to every element in *z*.\n\n    Args:\n        z:  A scalar, (nested) list or ``numpy.ndarray`` of numbers.\n        a:  Slope used for the negative part, a float in the half-open\n            interval [0, 1).  Defaults to 0.01.\n\n    Returns\n    -------\n    Same \u201cshape\u201d as *z* but converted to plain Python objects\n    (i.e. lists / scalars).  For array or list inputs the function\n    returns a (possibly nested) list; for a scalar input it returns a\n    scalar.\n    \"\"\"\n    if not 0.0 <= a < 1.0:\n        raise ValueError(\"Parameter 'a' must lie in the interval [0, 1).\")\n    arr = np.asarray(z)\n    activated = np.where(arr >= 0, arr, a * arr)\n    return activated.tolist()"}
{"task_id": 225, "completion_id": 0, "solution": "import numpy as np\ndef hz2mel(hz, formula: str='htk') -> list:\n    \"\"\"Convert a set of frequencies from Hertz to the mel scale.\n\n    Parameters\n    ----------\n    hz : array-like of float\n        One- or multi-dimensional collection of frequencies in Hertz.\n    formula : {\"htk\", \"slaney\"}, optional\n        Conversion formula to use:\n        \u2022 \"htk\"    \u2013 2595 * log10(1 + hz / 700)\n        \u2022 \"slaney\" \u2013 1127 * ln   (1 + hz / 700)\n        Default is \"htk\".\n\n    Returns\n    -------\n    list of float\n        Mel-scaled frequencies with the same shape as *hz*, rounded to four\n        decimal places.\n\n    Raises\n    ------\n    AssertionError\n        If *formula* is not \"htk\" or \"slaney\".\n    \"\"\"\n    assert formula in {'htk', 'slaney'}, \"formula must be either 'htk' or 'slaney'\"\n    hz_arr = np.asarray(hz, dtype=float)\n    if formula == 'htk':\n        mel = 2595.0 * np.log10(1.0 + hz_arr / 700.0)\n    else:\n        mel = 1127.0 * np.log(1.0 + hz_arr / 700.0)\n    return np.round(mel, 4).tolist()"}
{"task_id": 226, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef _stump_predict(column: np.ndarray, threshold: float, polarity: int) -> np.ndarray:\n    \"\"\"\n    Helper \u2013 returns the predictions of one decision stump for a single feature column.\n    \"\"\"\n    if polarity == 1:\n        return np.where(column < threshold, -1, 1)\n    else:\n        return np.where(column < threshold, 1, -1)\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\"\"\"\n    (n_samples, n_features) = X_train.shape\n    w = np.full(n_samples, 1.0 / n_samples)\n    ensemble: list[tuple[int, float, int, float]] = []\n    for _ in range(n_clf):\n        best_feat = 0\n        best_thresh = 0.0\n        best_polarity = 1\n        best_error = float('inf')\n        best_pred_train = None\n        for feat_idx in range(n_features):\n            col = X_train[:, feat_idx]\n            thresholds = np.unique(col)\n            for thr in thresholds:\n                pred = _stump_predict(col, thr, 1)\n                err = np.sum(w[pred != y_train])\n                if err < best_error:\n                    (best_error, best_feat, best_thresh, best_polarity) = (err, feat_idx, thr, 1)\n                    best_pred_train = pred\n                pred = _stump_predict(col, thr, -1)\n                err = np.sum(w[pred != y_train])\n                if err < best_error:\n                    (best_error, best_feat, best_thresh, best_polarity) = (err, feat_idx, thr, -1)\n                    best_pred_train = pred\n        eps = 1e-10\n        alpha = 0.5 * math.log((1 - best_error) / (best_error + eps))\n        w *= np.exp(-alpha * y_train * best_pred_train)\n        w /= np.sum(w)\n        ensemble.append((best_feat, best_thresh, best_polarity, alpha))\n    agg = np.zeros(X_test.shape[0])\n    for (feat_idx, thr, pol, alpha) in ensemble:\n        preds = _stump_predict(X_test[:, feat_idx], thr, pol)\n        agg += alpha * preds\n    final_preds = np.sign(agg)\n    final_preds[final_preds == 0] = 1\n    return final_preds.astype(int).tolist()"}
{"task_id": 227, "completion_id": 0, "solution": "import numpy as np\ndef value_network_forward(state: list[float], W1: list[list[float]], W2: list[list[float]], W3: list[list[float]] | list[float]) -> float:\n    \"\"\"Forward pass of a 2-hidden-layer value network using tanh activations.\"\"\"\n    try:\n        s = np.asarray(state, dtype=float)\n        W1 = np.asarray(W1, dtype=float)\n        W2 = np.asarray(W2, dtype=float)\n        W3 = np.asarray(W3, dtype=float)\n    except Exception:\n        return -1\n    if s.ndim != 1:\n        return -1\n    N = s.shape[0]\n    if W1.ndim != 2 or W1.shape[0] != N:\n        return -1\n    H1 = W1.shape[1]\n    if W2.ndim != 2 or W2.shape[0] != H1:\n        return -1\n    H2 = W2.shape[1]\n    if W3.ndim == 2 and W3.shape != (H2, 1):\n        return -1\n    elif W3.ndim == 1 and W3.shape[0] != H2:\n        return -1\n    elif W3.ndim not in (1, 2):\n        return -1\n    h1 = np.tanh(s @ W1)\n    h2 = np.tanh(h1 @ W2)\n    v = float(h2 @ W3.squeeze())\n    return round(v, 4)"}
{"task_id": 228, "completion_id": 0, "solution": "import numpy as np\ndef pairwise_l2_distances(X: np.ndarray, Y: np.ndarray | None=None) -> list[list[float]]:\n    \"\"\"Compute the pair-wise Euclidean (L2) distances between each row of X and each row of Y.\n\n    If ``Y`` is None the distances inside X itself are computed instead.\n    The distances are rounded to four decimal places and returned as a\n    pure Python nested list.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Array of shape (N, C).\n    Y : np.ndarray | None, optional\n        Array of shape (M, C).  If None, Y is taken to be X.\n\n    Returns\n    -------\n    list[list[float]]\n        The (N \u00d7 M) matrix of pair-wise Euclidean distances.\n    \"\"\"\n    if Y is None:\n        Y = X\n    if X.ndim != 2 or Y.ndim != 2:\n        raise ValueError('X and Y must be 2-D arrays.')\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError('X and Y must have the same number of columns (features).')\n    x2 = np.sum(X ** 2, axis=1, keepdims=True)\n    y2 = np.sum(Y ** 2, axis=1, keepdims=True).T\n    d2 = x2 + y2 - 2.0 * X @ Y.T\n    d2 = np.maximum(d2, 0.0)\n    D = np.sqrt(d2)\n    return np.round(D, 4).tolist()"}
{"task_id": 230, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_naive_bayes(X_train, y_train, X_test):\n    \"\"\"\n    Gaussian Naive Bayes classifier (binary) implemented from scratch.\n\n    Parameters\n    ----------\n    X_train : ndarray, shape (n_samples, n_features)\n    y_train : ndarray, shape (n_samples,)       \u2013 labels 0 / 1\n    X_test  : ndarray, shape (m_samples, n_features)\n\n    Returns\n    -------\n    list[int]  \u2013 predicted labels for X_test\n    \"\"\"\n    eps = 1e-09\n    classes = np.array([0, 1])\n    n_features = X_train.shape[1]\n    means = np.zeros((2, n_features))\n    variances = np.zeros((2, n_features))\n    priors = np.zeros(2)\n    n_samples = len(y_train)\n    for (idx, c) in enumerate(classes):\n        X_c = X_train[y_train == c]\n        if X_c.size == 0:\n            means[idx] = 0.0\n            variances[idx] = 1.0\n            priors[idx] = eps\n            continue\n        means[idx] = X_c.mean(axis=0)\n        variances[idx] = X_c.var(axis=0) + eps\n        priors[idx] = X_c.shape[0] / n_samples\n    priors[priors == 0] = eps\n    diff = X_test[:, None, :] - means[None, :, :]\n    log_likelihood = -0.5 * (np.log(2.0 * np.pi * variances)[None, :, :] + diff ** 2 / variances[None, :, :])\n    log_posteriors = np.sum(log_likelihood, axis=2) + np.log(priors)[None, :]\n    predictions = np.argmax(log_posteriors, axis=1)\n    return predictions.tolist()"}
{"task_id": 231, "completion_id": 0, "solution": "def is_natural(hand: list[int]) -> bool:\n    \"\"\"\n    Determine whether a given two-card blackjack hand is a natural (blackjack).\n\n    A natural hand must satisfy BOTH conditions:\n        1. It contains exactly two cards.\n        2. Those two cards consist of one Ace (encoded as 1) and one ten-valued card\n           (10, J, Q, or K \u2013 all encoded as 10).\n\n    Parameters\n    ----------\n    hand : list[int]\n        A list with the numerical ranks of the cards (Ace \u2192 1, 10/J/Q/K \u2192 10).\n\n    Returns\n    -------\n    bool\n        True  \u2013 if `hand` is exactly [1, 10] in any order.\n        False \u2013 for every other combination/length.\n    \"\"\"\n    return len(hand) == 2 and set(hand) == {1, 10}"}
{"task_id": 232, "completion_id": 0, "solution": "import numpy as np\ndef is_one_hot(x: np.ndarray) -> bool:\n    \"\"\"Check if *x* is a valid one-hot encoded NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A NumPy array to verify.\n\n    Returns\n    -------\n    bool\n        True if *x* is one-hot encoded, otherwise False.\n    \"\"\"\n    if not isinstance(x, np.ndarray) or x.ndim != 2:\n        return False\n    if x.shape[0] == 0 or x.shape[1] == 0:\n        return False\n    if not np.isin(x, (0, 1)).all():\n        return False\n    if not (x.sum(axis=1) == 1).all():\n        return False\n    return True"}
{"task_id": 234, "completion_id": 0, "solution": "import numpy as np\ndef thompson_sampling_beta_binomial(successes: list[int], failures: list[int], prior_alpha: float=1.0, prior_beta: float=1.0, seed: int | None=None) -> int:\n    \"\"\"Single-step Thompson sampling for a Bernoulli multi-armed bandit.\"\"\"\n    if not isinstance(successes, (list, tuple)) or not isinstance(failures, (list, tuple)) or len(successes) == 0 or (len(successes) != len(failures)):\n        return -1\n    try:\n        succ_arr = np.asarray(successes, dtype=float)\n        fail_arr = np.asarray(failures, dtype=float)\n    except Exception:\n        return -1\n    if (succ_arr < 0).any() or (fail_arr < 0).any():\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    alpha = prior_alpha + succ_arr\n    beta = prior_beta + fail_arr\n    samples = np.random.beta(alpha, beta)\n    chosen_arm = int(np.argmax(samples))\n    return chosen_arm"}
{"task_id": 235, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef to_one_hot(labels: np.ndarray, n_classes: int | None=None) -> np.ndarray:\n    \"\"\"Convert integer class labels to one-hot encodings.\n\n    Parameters\n    ----------\n    labels : np.ndarray\n        A 1-D NumPy array containing integer class labels.\n    n_classes : int | None, optional (default=None)\n        Total number of classes.  If None, it will be inferred from the\n        data as ``labels.max() + 1``.\n\n    Returns\n    -------\n    np.ndarray\n        A two-dimensional NumPy array of shape (N, C) where N is the\n        number of labels and C is the number of classes.  Each row is a\n        one-hot vector corresponding to the associated label.\n    \"\"\"\n    if labels.ndim != 1:\n        raise ValueError('`labels` must be a 1-D array.')\n    if not np.issubdtype(labels.dtype, np.integer):\n        raise ValueError('`labels` must contain integer values.')\n    if labels.size == 0:\n        if n_classes is None:\n            raise ValueError('Cannot infer number of classes from an empty array.')\n        return np.zeros((0, n_classes), dtype=float)\n    max_label = int(labels.max())\n    if max_label < 0:\n        raise ValueError('Negative class labels are not allowed.')\n    if n_classes is None:\n        C = max_label + 1\n    else:\n        if n_classes < max_label + 1:\n            raise ValueError('`n_classes` ({}) is smaller than max label + 1 ({}).'.format(n_classes, max_label + 1))\n        C = int(n_classes)\n    N = labels.shape[0]\n    one_hot = np.zeros((N, C), dtype=float)\n    one_hot[np.arange(N), labels] = 1.0\n    return one_hot"}
{"task_id": 236, "completion_id": 0, "solution": "import numpy as np\ndef minibatch(X: np.ndarray, batchsize: int=256, shuffle: bool=True):\n    \"\"\"Create a generator that yields index mini-batches.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (N, *). The first axis holds the samples.\n    batchsize : int, default = 256\n        Desired mini-batch size. The last batch may be smaller.\n    shuffle : bool, default = True\n        Whether to shuffle sample indices before splitting.\n\n    Returns\n    -------\n    tuple\n        (mb_generator, n_batches)\n        mb_generator : generator yielding NumPy integer arrays \u2013 the indices\n            of each mini-batch in the order they are processed.\n        n_batches : int \u2013 total number of mini-batches.\n    \"\"\"\n    if batchsize <= 0:\n        raise ValueError('`batchsize` must be a positive integer.')\n    if X.ndim == 0:\n        raise ValueError('`X` must have at least one dimension (samples).')\n    N = int(X.shape[0])\n    n_batches = (N + batchsize - 1) // batchsize\n    indices = np.arange(N, dtype=np.int64)\n    if shuffle:\n        np.random.shuffle(indices)\n\n    def _generator():\n        for start in range(0, N, batchsize):\n            yield indices[start:start + batchsize]\n    return (_generator(), n_batches)"}
{"task_id": 237, "completion_id": 0, "solution": "from typing import List\ndef score(hand: List[str]) -> int:\n    \"\"\"Return the Blackjack score of *hand* or 0 if the hand is bust.\n\n    Args:\n        hand: A list of card labels ('2'-'10', 'J', 'Q', 'K', 'A').\n\n    Returns:\n        An integer between 0 and 21 (inclusive).  If every possible value of\n        the hand exceeds 21 the function returns 0.\n    \"\"\"\n    total = 0\n    aces = 0\n    for card in hand:\n        if card == 'A':\n            aces += 1\n            total += 11\n        elif card in {'J', 'Q', 'K'}:\n            total += 10\n        else:\n            total += int(card)\n    while total > 21 and aces:\n        total -= 10\n        aces -= 1\n    return total if total <= 21 else 0"}
{"task_id": 239, "completion_id": 0, "solution": "import math\nfrom typing import Tuple\ndef determine_padding(filter_shape: Tuple[int, int], output_shape: str='same') -> Tuple[Tuple[int, int], Tuple[int, int]]:\n    \"\"\"Calculate the amount of zero-padding needed on all borders for a 2-D convolution\n    with stride 1.\n\n    Parameters\n    ----------\n    filter_shape : (int, int)\n        (filter_height, filter_width) of the convolution kernel.\n    output_shape : {\"same\", \"valid\"}, default=\"same\"\n        \"same\"  \u2013 pad so that the output spatial dimensions equal the input.\n        \"valid\" \u2013 perform the convolution without any zero-padding.\n\n    Returns\n    -------\n    ((int, int), (int, int))\n        ((pad_top, pad_bottom), (pad_left, pad_right))\n    \"\"\"\n    if output_shape == 'valid':\n        return ((0, 0), (0, 0))\n    if output_shape == 'same':\n        (fh, fw) = filter_shape\n        total_pad_h = fh - 1\n        total_pad_w = fw - 1\n        pad_top = total_pad_h // 2\n        pad_bottom = total_pad_h - pad_top\n        pad_left = total_pad_w // 2\n        pad_right = total_pad_w - pad_left\n        return ((pad_top, pad_bottom), (pad_left, pad_right))\n    raise ValueError(\"output_shape must be either 'same' or 'valid'\")"}
{"task_id": 240, "completion_id": 0, "solution": "import numpy as np\ndef create_encoding_dicts(obs_ids: list, action_ids: list, obs_encoder: dict | None=None, act_encoder: dict | None=None) -> dict:\n    \"\"\"Generate forward and backward mappings for observations and actions.\n\n    Args:\n        obs_ids (list): All distinct observation identifiers.\n        action_ids (list): All distinct action identifiers.\n        obs_encoder (dict | None, optional): Existing obs\u2192number mapping.\n        act_encoder (dict | None, optional): Existing act\u2192number mapping.\n\n    Returns\n    -------\n        dict\n            A dictionary that contains the keys 'obs2num', 'num2obs',\n            'act2num', and 'num2act'.\n    \"\"\"\n\n    def _build_maps(items: list, given_encoder: dict | None):\n        \"\"\"\n        Helper that creates a forward dictionary (sym\u2192int) and its inverse\n        (int\u2192sym) either from a supplied encoder or by enumerating *items*.\n        \"\"\"\n        if given_encoder is not None:\n            forward = dict(given_encoder)\n        else:\n            forward = {item: idx for (idx, item) in enumerate(items)}\n        inverse = {v: k for (k, v) in forward.items()}\n        return (forward, inverse)\n    (obs2num, num2obs) = _build_maps(obs_ids, obs_encoder) if obs_ids or obs_encoder else ({}, {})\n    (act2num, num2act) = _build_maps(action_ids, act_encoder) if action_ids or act_encoder else ({}, {})\n    for obs in obs2num:\n        assert num2obs[obs2num[obs]] == obs, 'Inconsistent obs dictionaries'\n    for act in act2num:\n        assert num2act[act2num[act]] == act, 'Inconsistent action dictionaries'\n    return {'obs2num': obs2num, 'num2obs': num2obs, 'act2num': act2num, 'num2act': num2act}"}
{"task_id": 241, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal(shape: tuple[int, ...], scale: float=0.5) -> list:\n    \"\"\"Initialise a tensor with an orthogonal matrix (Saxe et al., 2014).\n\n    Args:\n        shape: Desired output shape. Must have length \u2265 2.\n        scale: Multiplicative scaling factor (default 0.5).\n\n    Returns:\n        A (possibly nested) python list whose first two dimensions are\n        orthogonal up to the given scaling factor, or \u20131 if `shape` is 1-D.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n    d0 = shape[0]\n    d_rest = int(np.prod(shape[1:], dtype=int))\n    A = np.random.standard_normal((d0, d_rest))\n    (U, _, Vt) = np.linalg.svd(A, full_matrices=False)\n    Q = U if U.shape == A.shape else Vt\n    tensor = (scale * Q.reshape(shape)).tolist()\n    return tensor"}
{"task_id": 242, "completion_id": 0, "solution": "import math\nfrom typing import List\ndef ucb1_select(ev_estimates: List[float], pull_counts: List[int], current_step: int, C: float=1.0) -> int:\n    \"\"\"Return the arm index that UCB1 would pick next.\"\"\"\n    if len(ev_estimates) != len(pull_counts):\n        raise ValueError('`ev_estimates` and `pull_counts` must have the same length')\n    if len(ev_estimates) == 0:\n        raise ValueError('At least one arm is required')\n    for (idx, pulls) in enumerate(pull_counts):\n        if pulls == 0:\n            return idx\n    log_term = math.log(current_step + 1) * 2.0\n    best_idx = 0\n    best_score = float('-inf')\n    for (idx, (mean, pulls)) in enumerate(zip(ev_estimates, pull_counts)):\n        exploration = C * math.sqrt(log_term / pulls)\n        score = mean + exploration\n        if score > best_score:\n            best_score = score\n            best_idx = idx\n    return best_idx"}
{"task_id": 243, "completion_id": 0, "solution": "import numpy as np\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a tiny fully-connected Actor\u2013Critic network\n    whose weights are all ones and whose biases are all zeros.\n\n    Args:\n        state: List with three floats \u2013 the environment state.\n\n    Returns:\n        (action_probabilities, state_value) where\n            action_probabilities \u2013 list with three floats, each rounded to 4 decimals\n            state_value          \u2013 single float, rounded to 4 decimals\n    \"\"\"\n    s_sum = np.sum(state)\n    z1 = max(0.0, s_sum)\n    z3 = 16.0 * z1\n    action_probs = [round(1 / 3, 4)] * 3\n    state_value = round(64.0 * z1, 4)\n    return (action_probs, state_value)"}
{"task_id": 244, "completion_id": 0, "solution": "import numpy as np\nfrom numbers import Real\ndef random_binary_tensor(shape: tuple, sparsity: float=0.5) -> list:\n    \"\"\"Generate a random binary tensor.\n\n    Parameters\n    ----------\n    shape : tuple\n        Desired tensor shape (tuple of positive integers).\n    sparsity : float, optional\n        Probability that an entry is 1.0 (must lie in the interval [0,\\u20061]).\n\n    Returns\n    -------\n    list\n        Nested Python list containing only 0.0 and 1.0, or -1 on invalid input.\n    \"\"\"\n    if not isinstance(shape, tuple) or len(shape) == 0 or (not all((isinstance(dim, int) and dim > 0 for dim in shape))):\n        return -1\n    if not isinstance(sparsity, Real) or not 0.0 <= sparsity <= 1.0:\n        return -1\n    threshold = 1.0 - sparsity\n    random_draws = np.random.rand(*shape)\n    binary_tensor = (random_draws >= threshold).astype(float)\n    return binary_tensor.tolist()"}
{"task_id": 247, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_bandit_oracle(payoff_dists: list[tuple[float, float]], payoff_probs: list[float]) -> tuple[float, int] | int:\n    \"\"\"Return the highest expected reward (rounded to 4 decimals) and its arm\n    index for a Gaussian bandit, or -1 if the input is invalid.\"\"\"\n    if not isinstance(payoff_dists, list) or not isinstance(payoff_probs, list):\n        return -1\n    K = len(payoff_dists)\n    if K == 0 or len(payoff_probs) != K:\n        return -1\n    try:\n        dists_arr = np.asarray(payoff_dists, dtype=float)\n    except (ValueError, TypeError):\n        return -1\n    if dists_arr.ndim != 2 or dists_arr.shape[1] != 2 or dists_arr.shape[0] != K:\n        return -1\n    mus = dists_arr[:, 0]\n    vars_ = dists_arr[:, 1]\n    try:\n        probs = np.asarray(payoff_probs, dtype=float)\n    except (ValueError, TypeError):\n        return -1\n    if np.any(vars_ <= 0) or np.any(probs < 0) or np.any(probs > 1):\n        return -1\n    exp_vals = mus * probs\n    best_arm = int(np.argmax(exp_vals))\n    best_ev = round(float(exp_vals[best_arm]), 4)\n    return (best_ev, best_arm)"}
{"task_id": 248, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef baseline_prob_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[float]:\n    \"\"\"Return a constant probability equal to the fraction of positive labels.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Unused by this baseline; included only for API completeness.\n    y_train : np.ndarray\n        One-dimensional array of binary labels (0 = negative, 1 = positive).\n    X_test : np.ndarray\n        Feature matrix whose rows require a probability prediction.\n\n    Returns\n    -------\n    list[float]\n        List of identical, 4-decimal-rounded probabilities.\n    \"\"\"\n    if y_train.size == 0:\n        positive_fraction = 0.0\n    else:\n        positive_fraction = y_train.sum() / len(y_train)\n    positive_fraction = round(float(positive_fraction), 4)\n    n_rows_test = len(X_test)\n    if n_rows_test == 0:\n        return []\n    return [positive_fraction] * n_rows_test"}
{"task_id": 249, "completion_id": 0, "solution": "import numpy as np\ndef _dense(x: np.ndarray, w: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A small helper that applies a fully-connected layer to a 1-D input vector.\n    It copes with both common weight layouts:\n        \u2022  (in_dim , out_dim)   so   x @ w\n        \u2022  (out_dim, in_dim )   so   w @ x\n    \"\"\"\n    if x.shape[-1] == w.shape[0]:\n        return x @ w + b\n    if x.shape[-1] == w.shape[1]:\n        return w @ x + b\n    raise ValueError('Weight matrix has incompatible shape.')\ndef _relu(z: np.ndarray) -> np.ndarray:\n    return np.maximum(0.0, z)\ndef _softmax(z: np.ndarray) -> np.ndarray:\n    z = z - np.max(z)\n    e = np.exp(z)\n    return e / e.sum()\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max) \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)            \u2192 state value\n    \"\"\"\n    x = np.asarray(state, dtype=np.float64)\n    W1 = np.asarray(params['W1'], dtype=np.float64)\n    b1 = np.asarray(params['b1'], dtype=np.float64)\n    W2 = np.asarray(params['W2'], dtype=np.float64)\n    b2 = np.asarray(params['b2'], dtype=np.float64)\n    W3 = np.asarray(params['W3'], dtype=np.float64)\n    b3 = np.asarray(params['b3'], dtype=np.float64)\n    Wa = np.asarray(params['Wa'], dtype=np.float64)\n    ba = np.asarray(params['ba'], dtype=np.float64)\n    Wc = np.asarray(params['Wc'], dtype=np.float64)\n    bc = np.asarray(params['bc'], dtype=np.float64)\n    x = _relu(_dense(x, W1, b1))\n    x = _relu(_dense(x, W2, b2))\n    hidden = _relu(_dense(x, W3, b3))\n    logits = _dense(hidden, Wa, ba)\n    probs = _softmax(logits)\n    value = float(_dense(hidden, Wc, bc))\n    probs_rounded = [round(float(p), 4) for p in probs]\n    value_rounded = round(value, 4)\n    return (probs_rounded, value_rounded)"}
{"task_id": 251, "completion_id": 0, "solution": "import numpy as np\ndef _is_scalar(x):\n    \"\"\"Return True if *x* is a Python scalar (int/float or 0-d ndarray).\"\"\"\n    return np.isscalar(x) or (isinstance(x, np.ndarray) and x.ndim == 0)\ndef _round4(x):\n    \"\"\"Round every numeric entry to 4 decimal places and keep structure.\"\"\"\n    if _is_scalar(x):\n        return round(float(x), 4)\n    return np.round(x, 4)\ndef sigmoid(x):\n    \"\"\"Element-wise logistic sigmoid.\"\"\"\n    if _is_scalar(x):\n        return _round4(1 / (1 + np.exp(-x)))\n    x = np.asarray(x, dtype=float).ravel()\n    out = 1 / (1 + np.exp(-x))\n    return _round4(out)\ndef tanh(x):\n    \"\"\"Element-wise hyperbolic tangent.\"\"\"\n    if _is_scalar(x):\n        return _round4(np.tanh(x))\n    x = np.asarray(x, dtype=float).ravel()\n    out = np.tanh(x)\n    return _round4(out)\ndef relu(x):\n    \"\"\"Element-wise Rectified Linear Unit.\"\"\"\n    if _is_scalar(x):\n        return _round4(max(0, x))\n    x = np.asarray(x, dtype=float).ravel()\n    out = np.maximum(0, x)\n    return _round4(out)\ndef leaky_relu(x, alpha: float=0.01):\n    \"\"\"Element-wise Leaky-ReLU with slope *alpha* for negative inputs.\"\"\"\n    if _is_scalar(x):\n        return _round4(x if x >= 0 else alpha * x)\n    x = np.asarray(x, dtype=float).ravel()\n    out = np.where(x >= 0, x, alpha * x)\n    return _round4(out)\ndef softmax(x):\n    \"\"\"Numerically stable softmax for a 1-D input.\"\"\"\n    if _is_scalar(x):\n        return 1.0\n    x = np.asarray(x, dtype=float).ravel()\n    shift = x - np.max(x)\n    exp_shift = np.exp(shift)\n    probs = exp_shift / np.sum(exp_shift)\n    return _round4(probs)\ndef get_activation(name):\n    \"\"\"\n    Return the activation function associated with *name*.\n    \n    Parameters\n    ----------\n    name : str\n        One of {\"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\", \"softmax\"}.\n    \n    Returns\n    -------\n    Callable\n        The corresponding activation function.\n    \n    Raises\n    ------\n    ValueError\n        If *name* is not recognised.\n    \"\"\"\n    lookup = {'sigmoid': sigmoid, 'tanh': tanh, 'relu': relu, 'leaky_relu': leaky_relu, 'softmax': softmax}\n    try:\n        return lookup[name]\n    except KeyError:\n        raise ValueError('Invalid activation function.')"}
{"task_id": 252, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef least_squares_loss(actual: np.ndarray, predicted: np.ndarray) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the gradient and Hessian of the least-squares loss.\n\n    The least-squares loss is defined as 0.5 * ||actual \u2212 predicted||\u00b2.\n\n    Args:\n        actual: 1-D NumPy array containing the true labels/targets.\n        predicted: 1-D NumPy array containing the model predictions.\n\n    Returns:\n        A tuple (grad, hess):\n            grad  \u2013 Python list representing the gradient w.r.t. each prediction.\n            hess  \u2013 Python list representing the diagonal Hessian entries (all ones).\n    \"\"\"\n    grad = (actual - predicted).tolist()\n    hess = np.ones_like(actual, dtype=float).tolist()\n    return (grad, hess)"}
{"task_id": 253, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w        : 1-D weight vector (list or NumPy array).\n        alpha    : Regularisation strength (non-negative float).\n        l1_ratio : Fraction of L1 component (float in [0, 1]).\n        gradient : If True, return gradient, else return penalty value.\n\n    Returns\n        float  \u2013 if *gradient* is False  (penalty value),\n        list   \u2013 if *gradient* is True   (element-wise gradient).\n    \"\"\"\n    if alpha < 0:\n        raise ValueError('alpha must be non-negative.')\n    if not 0.0 <= l1_ratio <= 1.0:\n        raise ValueError('l1_ratio must be in [0, 1].')\n    w = np.asarray(w, dtype=float).ravel()\n    if gradient:\n        grad = alpha * (l1_ratio * np.sign(w) + (1.0 - l1_ratio) * w)\n        return list(np.round(grad, 4))\n    else:\n        penalty = alpha * (l1_ratio * np.linalg.norm(w, 1) + (1.0 - l1_ratio) * 0.5 * np.dot(w, w))\n        return round(float(penalty), 4)"}
{"task_id": 255, "completion_id": 0, "solution": "def usable_ace(hand: list[int]) -> bool:\n    \"\"\"Determine whether a Blackjack hand has a usable ace.\n\n    An ace is *usable* if it can be counted as 11 without the hand busting.\n    Counting one ace as 11 instead of 1 adds 10 to the hand\u2019s total.\n\n    Args:\n        hand: List of integers representing card values (ace == 1).\n\n    Returns:\n        True if the hand has a usable ace, else False.\n    \"\"\"\n    if 1 not in hand:\n        return False\n    total = sum(hand)\n    return total + 10 <= 21"}
{"task_id": 256, "completion_id": 0, "solution": "import numpy as np\nfrom copy import deepcopy\ndef grad_check_RNN(model, loss_func, param_name: str, n_t: int, X, epsilon: float=1e-07):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred\n    finite differences (double precision).\n\n    See the task description for a detailed specification.\n    \"\"\"\n    if param_name in ('X', 'y'):\n        return None\n    real_name = param_name\n    if param_name == 'Ba':\n        real_name = 'ba'\n    elif param_name == 'Bx':\n        real_name = 'bx'\n    if real_name not in model.parameters:\n        raise KeyError(f\"Parameter '{real_name}' not found in model.\")\n    X = np.asarray(X, dtype=np.float64)\n    param: np.ndarray = model.parameters[real_name].astype(np.float64, copy=False)\n    model.parameters[real_name] = param\n    grads = np.zeros_like(param, dtype=np.float64)\n\n    def _reset_state_if_possible():\n        if hasattr(model, 'reset_state') and callable(model.reset_state):\n            model.reset_state()\n\n    def _sequence_loss():\n        model.flush_gradients()\n        _reset_state_if_possible()\n        preds = []\n        for t in range(n_t):\n            preds.append(model.forward(X[:, :, t]))\n        return float(loss_func(preds))\n    it = np.ndindex(param.shape)\n    for idx in it:\n        original_val = param[idx]\n        param[idx] = original_val + epsilon\n        loss_plus = _sequence_loss()\n        param[idx] = original_val - epsilon\n        loss_minus = _sequence_loss()\n        grads[idx] = (loss_plus - loss_minus) / (2.0 * epsilon)\n        param[idx] = original_val\n    return grads.T.astype(np.float64)"}
{"task_id": 257, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\"\"\"\n\n    def stump_predict(X, feature, thresh, polarity):\n        \"\"\"\n        Predict with a single decision stump.\n        polarity =  1 : x < thresh  -> -1     else 1\n        polarity = -1 : x < thresh  ->  1     else -1\n        \"\"\"\n        col = X[:, feature]\n        if polarity == 1:\n            return np.where(col < thresh, -1, 1)\n        else:\n            return np.where(col < thresh, 1, -1)\n    n_clf = max(1, int(n_clf))\n    (m, n_features) = X_train.shape\n    w = np.full(m, 1.0 / m)\n    stumps = []\n    eps = 1e-10\n    for _ in range(n_clf):\n        (best_feature, best_thresh, best_pol, min_err) = (None, None, None, np.inf)\n        for feature in range(n_features):\n            values = X_train[:, feature]\n            thresholds = np.unique(values)\n            for thresh in thresholds:\n                for polarity in (1, -1):\n                    pred = stump_predict(X_train, feature, thresh, polarity)\n                    err = np.sum(w[pred != y_train])\n                    if err < min_err:\n                        (best_feature, best_thresh, best_pol) = (feature, thresh, polarity)\n                        min_err = err\n        if min_err >= 0.5 - eps:\n            break\n        min_err = max(min_err, eps)\n        alpha = 0.5 * np.log((1 - min_err) / min_err)\n        stumps.append((best_feature, best_thresh, best_pol, alpha))\n        predictions = stump_predict(X_train, best_feature, best_thresh, best_pol)\n        w *= np.exp(-alpha * y_train * predictions)\n        w /= w.sum()\n    if not stumps:\n        return [-1] * len(X_test)\n    agg = np.zeros(X_test.shape[0])\n    for (feature, thresh, polarity, alpha) in stumps:\n        agg += alpha * stump_predict(X_test, feature, thresh, polarity)\n    return [1 if x >= 0 else -1 for x in agg]"}
{"task_id": 258, "completion_id": 0, "solution": "import numpy as np\ndef hinge_loss(actual, predicted):\n    \"\"\"Compute the average hinge loss for binary classification.\n\n    Parameters\n    ----------\n    actual : list[int | float] | numpy.ndarray\n        True class labels encoded as -1 or 1.\n    predicted : list[int | float] | numpy.ndarray\n        Real-valued prediction scores.\n\n    Returns\n    -------\n    float | int\n        The mean hinge loss rounded to four decimal places, or -1 if the\n        two inputs have different lengths.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1\n    y_true = np.asarray(actual, dtype=float)\n    y_pred = np.asarray(predicted, dtype=float)\n    per_sample_loss = np.maximum(0, 1 - y_true * y_pred)\n    mean_loss = np.round(per_sample_loss.mean(), 4)\n    return float(mean_loss)"}
{"task_id": 259, "completion_id": 0, "solution": "import random\ndef sample_replay_memory(memory: list, batch_size: int) -> list:\n    \"\"\"Return *batch_size* different transitions sampled from the replay buffer.\n    \n    The sampling is deterministic because it is performed with a `random.Random`\n    instance that is always initialised with seed 42.\n    \n    If *batch_size* is larger than the number of stored transitions, -1\n    is returned instead.\n    \"\"\"\n    if batch_size > len(memory):\n        return -1\n    rng = random.Random(42)\n    return rng.sample(memory, batch_size)"}
{"task_id": 260, "completion_id": 0, "solution": "from itertools import zip_longest\nfrom math import floor\ndef tileswrap(ihtORsize: int, numtilings: int, floats: list[float], wrapwidths: list[int | None], ints: list[int] | None=None, readonly: bool=False) -> list[int]:\n    \"\"\"Wrapping tile-coder that returns the index of the active tile in every\n    one of `numtilings` tilings.\n\n    The code follows the algorithm described in the task text verbatim:\n        1. quantise each float -> q_i\n        2. build coordinates for every tiling, applying wrapping if requested\n        3. hash the integer coordinate list with the fixed polynomial hash\n    \"\"\"\n    if ihtORsize <= 0:\n        raise ValueError('ihtORsize must be a positive integer.')\n    if numtilings <= 0:\n        raise ValueError('numtilings must be a positive integer.')\n    if len(list(zip_longest(floats, wrapwidths))) != len(floats):\n        raise ValueError('wrapwidths must be the same length as floats.')\n    ints = ints or []\n    q_vals = [int(floor(x * numtilings)) for x in floats]\n    active_tiles: list[int] = []\n    for t in range(numtilings):\n        coords: list[int] = [t]\n        b = t\n        for (q, wrap) in zip(q_vals, wrapwidths):\n            offset = b % numtilings\n            c = (q + offset) // numtilings\n            if wrap and wrap > 0:\n                c %= wrap\n            coords.append(c)\n            b += t * 2\n        coords.extend(ints)\n        index = 0\n        for coord in coords:\n            index = (index * 31 + coord) % ihtORsize\n        active_tiles.append(index)\n    return active_tiles"}
{"task_id": 261, "completion_id": 0, "solution": "import numpy as np\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate an array initialised with Glorot/Xavier normal distribution.\n\n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired tensor shape. Must have at least two dimensions so that\n        fan-in and fan-out are well defined.\n\n    Returns\n    -------\n    np.ndarray\n        Array of the requested shape whose elements are drawn independently\n        from \ud835\udca9(0, s\u00b2) with  s = \u221a[2 / (fan_in + fan_out)].\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError('`shape` must have at least two dimensions.')\n    if len(shape) == 2:\n        (fan_in, fan_out) = shape\n    else:\n        receptive_field = int(np.prod(shape[2:], dtype=int))\n        fan_in = shape[1] * receptive_field\n        fan_out = shape[0] * receptive_field\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n    return np.random.normal(loc=0.0, scale=std, size=shape).astype(float)"}
{"task_id": 263, "completion_id": 0, "solution": "import numpy as np\ndef feedforward_predict(x, weights, activations):\n    \"\"\"Runs a forward pass through a fully-connected neural network.\n\n    Parameters\n    ----------\n    x : list[float] | numpy.ndarray\n        Input feature vector (bias term **not** included).\n    weights : list[list[list[float]]]\n        List of weight matrices; each inner list represents one 2-D weight\n        matrix where the first column contains the bias weights.\n    activations : list[str]\n        Activation functions per layer.  Allowed values: \"sigmoid\", \"relu\",\n        \"linear\".\n\n    Returns\n    -------\n    float | list[float]\n        Network output rounded to 4 decimal places.  A scalar is returned for\n        a single-neuron output layer, otherwise a list is returned.\n    \"\"\"\n\n    def apply_activation(z, name):\n        \"\"\"Element-wise activation.\"\"\"\n        if name == 'sigmoid':\n            return 1.0 / (1.0 + np.exp(-z))\n        elif name == 'relu':\n            return np.maximum(0, z)\n        elif name == 'linear':\n            return z\n        else:\n            raise ValueError(f'Unsupported activation: {name}')\n    a = np.asarray(x, dtype=float)\n    for (W_list, act_name) in zip(weights, activations):\n        W = np.asarray(W_list, dtype=float)\n        a_tilde = np.insert(a, 0, 1.0)\n        z = W @ a_tilde\n        a = apply_activation(z, act_name)\n    a = np.round(a, 4)\n    return float(a) if a.size == 1 else a.tolist()"}
{"task_id": 265, "completion_id": 0, "solution": "def king_loss_fn(x: int | float) -> float:\n    \"\"\"Compute King\u2019s piece-wise loss for a scalar value.\n\n    Segments:\n      \u2022 x \u2264 250             : L(x) = \u20130.25 * x + 82.50372665317208\n      \u2022 250 < x \u2264 600       : L(x) = 20.00372665317208\n      \u2022 600 < x \u2264 700       : L(x) = \u20130.2  * x + 140.00372665317207\n      \u2022 x > 700             : L(x) = 0.003726653172066108\n\n    Result is rounded to 4 decimal places.\n\n    Args:\n        x: Scalar input (int or float).\n\n    Returns:\n        Loss value rounded to four decimal places.\n    \"\"\"\n    if x <= 250:\n        value = -0.25 * x + 82.50372665317208\n    elif x <= 600:\n        value = 20.00372665317208\n    elif x <= 700:\n        value = -0.2 * x + 140.00372665317207\n    else:\n        value = 0.003726653172066108\n    return round(value, 4)"}
{"task_id": 266, "completion_id": 0, "solution": "from typing import Any, List, Tuple\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples ``(u, v)`` describing undirected edges that connect\n        vertices ``u`` and ``v``.\n        \u2022 Multiple occurrences of the same edge may be present.  \n        \u2022 Vertices that do not occur in ``V`` are ignored.  \n        \u2022 Self-loops (u == v) are ignored.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list ``G`` where ``G[i]`` contains all vertices adjacent to ``V[i]``.\n        \u2022 The outer list is in the same order as ``V``.  \n        \u2022 Inner lists contain *no duplicates* and are ordered by the vertices\u2019\n          order of appearance in ``V``.\n    \"\"\"\n    index = {vertex: i for (i, vertex) in enumerate(V)}\n    neighbours: List[set] = [set() for _ in V]\n    for (u, v) in E:\n        if u not in index or v not in index:\n            continue\n        if u == v:\n            continue\n        (ui, vi) = (index[u], index[v])\n        neighbours[ui].add(v)\n        neighbours[vi].add(u)\n    G: List[List[Any]] = []\n    for nb_set in neighbours:\n        ordered_nb = sorted(nb_set, key=index.get)\n        G.append(ordered_nb)\n    return G"}
{"task_id": 267, "completion_id": 0, "solution": "import numpy as np\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Best stump (see task description).\n    \"\"\"\n    y = y.ravel().astype(int)\n    w = sample_weights.ravel().astype(float)\n    (n_samples, n_features) = X.shape\n    best_feature = -1\n    best_threshold = 0.0\n    best_polarity = 1\n    best_error = np.inf\n    for j in range(n_features):\n        sorted_idx = np.argsort(X[:, j])\n        f_vals = X[sorted_idx, j]\n        lbls = y[sorted_idx]\n        ws = w[sorted_idx]\n        pos_total = ws[lbls == 1].sum()\n        neg_total = ws[lbls == -1].sum()\n        cum_pos = 0.0\n        cum_neg = 0.0\n        i = 0\n        n = n_samples\n        while i < n:\n            theta = f_vals[i]\n            err_p_1 = cum_neg + (pos_total - cum_pos)\n            err_p_neg1 = cum_pos + (neg_total - cum_neg)\n            if err_p_1 < best_error - 1e-12:\n                best_error = err_p_1\n                best_feature = j\n                best_threshold = float(theta)\n                best_polarity = 1\n            if err_p_neg1 < best_error - 1e-12:\n                best_error = err_p_neg1\n                best_feature = j\n                best_threshold = float(theta)\n                best_polarity = -1\n            while i < n and f_vals[i] == theta:\n                if lbls[i] == 1:\n                    cum_pos += ws[i]\n                else:\n                    cum_neg += ws[i]\n                i += 1\n    return {'feature_index': int(best_feature), 'threshold': round(best_threshold, 4), 'polarity': int(best_polarity), 'weighted_error': round(best_error, 4)}"}
{"task_id": 268, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations, permutations\nfrom itertools import combinations, permutations\nimport numpy as np\ndef random_unweighted_graph(n_vertices: int, edge_prob: float=0.5, directed: bool=False) -> list[list[int]]:\n    \"\"\"Generate an un-weighted Erd\u0151s\u2013R\u00e9nyi G(n,p) random graph.\n\n    Parameters\n    ----------\n    n_vertices : int\n        Number of vertices (labelled 0 \u2026 n_vertices-1).\n    edge_prob  : float, default 0.5\n        Probability p that any admissible edge exists.\n    directed   : bool, default False\n        If True build a directed graph, otherwise an undirected one.\n\n    Returns\n    -------\n    list[list[int]]\n        Adjacency matrix A where A[i][j] == 1  iff  edge i\u2192j exists.\n        The matrix contains only integers 0/1 and has shape\n        (n_vertices, n_vertices).  When n_vertices == 1, [[0]] is\n        returned.\n    \"\"\"\n    if n_vertices == 1:\n        return [[0]]\n    if n_vertices <= 0:\n        raise ValueError('n_vertices must be a positive integer.')\n    if not 0.0 <= edge_prob <= 1.0:\n        raise ValueError('edge_prob must be in the interval [0, 1].')\n    A = np.zeros((n_vertices, n_vertices), dtype=int)\n    rng_rand = np.random.rand\n    if directed:\n        for i in range(n_vertices):\n            for j in range(n_vertices):\n                if i == j:\n                    continue\n                if rng_rand() <= edge_prob:\n                    A[i, j] = 1\n    else:\n        for i in range(n_vertices):\n            for j in range(i + 1, n_vertices):\n                if rng_rand() <= edge_prob:\n                    A[i, j] = A[j, i] = 1\n    return A.tolist()"}
{"task_id": 269, "completion_id": 0, "solution": "def count_trainable_params(state_dimensions: int, action_dimensions: int) -> int:\n    \"\"\"Return the total number of trainable parameters of the DQN network.\n\n    Parameters\n    ----------\n    state_dimensions : int\n        Length of the state (observation) vector.\n    action_dimensions : int\n        Number of possible discrete actions.\n\n    Returns\n    -------\n    int\n        Total count of trainable parameters (weights + biases).\n    \"\"\"\n    layer1_params = state_dimensions * 164 + 164\n    layer2_params = 164 * action_dimensions + action_dimensions\n    return layer1_params + layer2_params"}
{"task_id": 270, "completion_id": 0, "solution": "import numpy as np\ndef oracle_payoff(context: np.ndarray, thetas: np.ndarray) -> tuple[float, int]:\n    \"\"\"Return best (rounded) expected reward and corresponding arm index.\n\n    Parameters\n    ----------\n    context : np.ndarray, shape (D, K)\n        Column k holds the D\u2013dimensional context vector x_{t,k}.\n    thetas  : np.ndarray, shape (D, K)\n        Column k holds the parameter vector \u03b8_k for arm k.\n\n    Returns\n    -------\n    (best_reward, best_arm) : (float, int)\n        best_reward \u2013 maximal expected reward rounded to 4 decimals,\n        best_arm    \u2013 smallest index of an arm that attains best_reward\n                      after rounding.\n    \"\"\"\n    if context.shape != thetas.shape:\n        raise ValueError('context and thetas must have identical shape (D, K).')\n    if context.ndim != 2:\n        raise ValueError('context and thetas must be 2-D arrays (D, K).')\n    rewards = np.sum(context * thetas, axis=0)\n    rounded = np.round(rewards, 4)\n    best_reward = np.max(rounded)\n    best_indices = np.where(rounded == best_reward)[0]\n    best_arm = int(best_indices.min())\n    return (float(best_reward), best_arm)"}
{"task_id": 271, "completion_id": 0, "solution": "import numpy as np\ndef xgboost_split_gain(left_true: list[float], left_pred: list[float], right_true: list[float], right_pred: list[float]) -> float:\n    \"\"\"Compute XGBoost split gain for squared-error loss.\"\"\"\n    l_true = np.asarray(left_true, dtype=float)\n    l_pred = np.asarray(left_pred, dtype=float)\n    r_true = np.asarray(right_true, dtype=float)\n    r_pred = np.asarray(right_pred, dtype=float)\n    H_L = l_true.size\n    H_R = r_true.size\n    if H_L == 0 or H_R == 0:\n        return 0.0\n    G_L = np.sum(l_pred - l_true)\n    G_R = np.sum(r_pred - r_true)\n    G_P = G_L + G_R\n    H_P = H_L + H_R\n    gain = 0.5 * (G_L ** 2 / H_L + G_R ** 2 / H_R - G_P ** 2 / H_P)\n    gain_rounded = round(gain, 4)\n    if gain_rounded == -0.0:\n        gain_rounded = 0.0\n    return gain_rounded"}
{"task_id": 273, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float (rounded to 4 decimals) or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    n = len(feature)\n    if n <= 1:\n        return (None, 0.0)\n\n    def entropy(counter):\n        total = sum(counter.values())\n        if total == 0:\n            return 0.0\n        h = 0.0\n        for cnt in counter.values():\n            p = cnt / total\n            h -= p * math.log2(p)\n        return h\n    pairs = sorted(zip(feature, target), key=lambda x: x[0])\n    sorted_feat = [p[0] for p in pairs]\n    sorted_label = [p[1] for p in pairs]\n    if sorted_feat[0] == sorted_feat[-1]:\n        return (None, 0.0)\n    right_counts = Counter(sorted_label)\n    left_counts = Counter()\n    parent_entropy = entropy(right_counts)\n    best_gain = 0.0\n    best_threshold = None\n    eps = 1e-12\n    for i in range(n - 1):\n        lbl = sorted_label[i]\n        left_counts[lbl] += 1\n        right_counts[lbl] -= 1\n        if right_counts[lbl] == 0:\n            del right_counts[lbl]\n        if sorted_feat[i] != sorted_feat[i + 1]:\n            thr = (sorted_feat[i] + sorted_feat[i + 1]) / 2.0\n            n_left = i + 1\n            n_right = n - n_left\n            gain = parent_entropy - n_left / n * entropy(left_counts) - n_right / n * entropy(right_counts)\n            if gain > best_gain + eps:\n                (best_gain, best_threshold) = (gain, thr)\n            elif abs(gain - best_gain) <= eps and best_gain > 0.0:\n                best_threshold = min(best_threshold, thr)\n    if best_threshold is None:\n        return (None, 0.0)\n    return (round(best_threshold, 4), round(best_gain, 4))"}
{"task_id": 274, "completion_id": 0, "solution": "import numpy as np\ndef small_norm_clip(values):\n    \"\"\"Clip each element of *values* to the closed interval [-5, 5].\n\n    Args:\n        values (list[int | float] | np.ndarray): One-dimensional numeric data.\n\n    Returns:\n        list[float]: A Python list where every element has been limited to the\n            range [-5, 5] while preserving the original order.\n    \"\"\"\n    if isinstance(values, np.ndarray):\n        if values.ndim != 1:\n            raise ValueError('Input array must be one-dimensional.')\n        return np.clip(values, -5, 5).tolist()\n    if isinstance(values, (list, tuple)):\n        return [5 if x > 5 else -5 if x < -5 else x for x in values]\n    raise TypeError('Input must be a list or a one-dimensional NumPy array.')"}
{"task_id": 276, "completion_id": 0, "solution": "import math\nimport string\nfrom collections import Counter\ndef additive_ngram_log_prob(corpus: list[str], sentence: str, N: int, k: float=1.0, unk: bool=True) -> float:\n    \"\"\"Return base-10 log-probability of *sentence* under an additive-smoothed\n    N-gram model that is trained on *corpus*.\"\"\"\n    if N < 1:\n        raise ValueError('N must be a positive integer.')\n    if k <= 0:\n        raise ValueError('k (smoothing parameter) must be > 0.')\n    punct = set(string.punctuation)\n\n    def tokenize(text: str) -> list[str]:\n        \"\"\"Lower-case, white-space split, strip leading/trailing punctuation.\"\"\"\n        out = []\n        for tok in text.lower().split():\n            tok = tok.strip(string.punctuation)\n            if tok:\n                out.append(tok)\n        return out\n    vocab: set[str] = set()\n    sentences_tokens: list[list[str]] = []\n    total_tokens = 0\n    for sent in corpus:\n        toks = tokenize(sent)\n        padded = ['<bol>', *toks, '<eol>']\n        sentences_tokens.append(padded)\n        vocab.update(padded)\n        total_tokens += len(padded)\n    vocab.update({'<bol>', '<eol>'})\n    if unk:\n        vocab.add('<unk>')\n    V = len(vocab)\n    if N == 1:\n        unigram_counts = Counter()\n        for toks in sentences_tokens:\n            unigram_counts.update(toks)\n    else:\n        ngram_counts = Counter()\n        hist_counts = Counter()\n        for toks in sentences_tokens:\n            for i in range(len(toks) - N + 1):\n                ngram = tuple(toks[i:i + N])\n                ngram_counts[ngram] += 1\n                hist_counts[ngram[:-1]] += 1\n    tgt_tokens = tokenize(sentence)\n    if unk:\n        tgt_tokens = [t if t in vocab else '<unk>' for t in tgt_tokens]\n    tgt_tokens = ['<bol>', *tgt_tokens, '<eol>']\n    log_prob = 0.0\n    if N == 1:\n        denom = total_tokens + k * V\n        for w in tgt_tokens:\n            num = unigram_counts.get(w, 0) + k\n            log_prob += math.log10(num / denom)\n    else:\n        for i in range(len(tgt_tokens) - N + 1):\n            ngram = tuple(tgt_tokens[i:i + N])\n            history = ngram[:-1]\n            num = ngram_counts.get(ngram, 0) + k\n            denom = hist_counts.get(history, 0) + k * V\n            log_prob += math.log10(num / denom)\n    return round(log_prob, 4)"}
{"task_id": 277, "completion_id": 0, "solution": "def is_tuple(env: dict) -> tuple:\n    \"\"\"Determine whether the *action* and *observation* spaces contained in\n    ``env`` are composite (tuple or dictionary).\n\n    A *composite* space is defined as a built-in ``tuple`` or ``dict``.\n\n    Args:\n        env (dict): A dictionary that **must** contain the keys\n            ``\"action_space\"`` and ``\"observation_space\"``.\n\n    Returns:\n        tuple: Two booleans ``(tuple_action, tuple_obs)`` indicating whether\n            each space is composite.\n    \"\"\"\n    action_space = env['action_space']\n    observation_space = env['observation_space']\n    tuple_action = isinstance(action_space, (tuple, dict))\n    tuple_obs = isinstance(observation_space, (tuple, dict))\n    return (tuple_action, tuple_obs)"}
{"task_id": 278, "completion_id": 0, "solution": "def evaluate_decision_tree(tree: dict, sample: list[float]):\n    \"\"\"Traverse a binary decision-tree represented by nested dictionaries.\n\n    Args:\n        tree (dict): Root node of the decision tree. Internal nodes contain\n            'feature_i', 'threshold', 'true_branch', 'false_branch'. Leaf\n            nodes contain only 'value'.\n        sample (list[float]): Feature vector that will be classified/regressed.\n\n    Returns:\n        The value stored in the reached leaf (int, float, or str).\n    \"\"\"\n    node = tree\n    while True:\n        if 'value' in node:\n            return node['value']\n        feature_i = node['feature_i']\n        threshold = node['threshold']\n        if feature_i >= len(sample) or feature_i < 0:\n            raise IndexError(f'Feature index {feature_i} is out of bounds for sample of length {len(sample)}')\n        if sample[feature_i] <= threshold:\n            node = node['true_branch']\n        else:\n            node = node['false_branch']"}
{"task_id": 279, "completion_id": 0, "solution": "import numpy as np\ndef mse_loss(y_true, y_pred):\n    \"\"\"Mean Squared Error (MSE) loss, its gradient and accuracy.\n\n    Parameters\n    ----------\n    y_true : list[int] | numpy.ndarray\n        Ground-truth labels (0 or 1).\n    y_pred : list[float] | numpy.ndarray\n        Model predictions (any real numbers).\n\n    Returns\n    -------\n    tuple\n        (mse, gradient_list, accuracy), all rounded to 4 decimals.\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    n = y_true.size\n    diff = y_pred - y_true\n    mse = np.mean(diff ** 2)\n    grad = 2.0 / n * diff\n    pred_class = (y_pred >= 0.5).astype(int)\n    true_class = y_true.astype(int)\n    accuracy = np.mean(pred_class == true_class)\n    mse = round(float(mse), 4)\n    grad_list = [round(float(g), 4) for g in grad.tolist()]\n    accuracy = round(float(accuracy), 4)\n    return (mse, grad_list, accuracy)"}
{"task_id": 280, "completion_id": 0, "solution": "import numpy as np\ndef l2_regularization(w: np.ndarray, alpha: float, return_grad: bool=False):\n    \"\"\"Compute the L2 regularization value or its gradient.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Weight vector / matrix / tensor of any shape.\n    alpha : float\n        Non-negative regularization strength.\n    return_grad : bool, optional (default=False)\n        \u2022 False \u2192 return the scalar regularization value  \n        \u2022 True  \u2192 return the gradient (same shape as *w*)\n\n    Returns\n    -------\n    float or np.ndarray\n        Rounded to 4 decimal places as required.\n    \"\"\"\n    if alpha < 0:\n        raise ValueError('alpha must be non-negative')\n    if return_grad:\n        grad = alpha * w\n        return np.round(grad, 4)\n    else:\n        value = 0.5 * alpha * np.sum(w ** 2)\n        return float(np.round(value, 4))"}
{"task_id": 281, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef L1Regularization(alpha: float):\n    \"\"\"Factory that returns a callable implementing L1 (Lasso) regularisation.\n\n    The resulting object *r* supports\n        value = r(w)          # penalty      (Python float)\n        grad  = r.grad(w)     # gradient     (NumPy array, same shape as w)\n\n    Parameters\n    ----------\n    alpha : float\n        Non-negative strength of the regularisation.  If alpha == 0 both the\n        penalty and its gradient are identically zero.\n    \"\"\"\n    if alpha < 0:\n        raise ValueError('alpha must be non-negative.')\n\n    class _L1:\n        __slots__ = ('alpha',)\n\n        def __init__(self, alpha_):\n            self.alpha = float(alpha_)\n\n        def __call__(self, w):\n            \"\"\"\n            Parameters\n            ----------\n            w : np.ndarray\n                Weight array (any shape)\n\n            Returns\n            -------\n            float\n                J_L1(w) = alpha * ||w||_1\n            \"\"\"\n            if self.alpha == 0.0:\n                return 0.0\n            return float(self.alpha * np.abs(w).sum())\n\n        def grad(self, w):\n            \"\"\"\n            Gradient of the L1 penalty with respect to w.\n\n            Parameters\n            ----------\n            w : np.ndarray\n                Weight array\n\n            Returns\n            -------\n            np.ndarray\n                alpha * sign(w)  (same shape as w, zeros where w == 0)\n            \"\"\"\n            if self.alpha == 0.0:\n                return np.zeros_like(w, dtype=w.dtype)\n            return self.alpha * np.sign(w).astype(w.dtype, copy=False)\n\n        def __repr__(self):\n            return f'L1Regularization(alpha={self.alpha})'\n    return _L1(alpha)"}
{"task_id": 282, "completion_id": 0, "solution": "import math\nfrom typing import Union, List, Tuple\nNumber = Union[int, float]\ndef running_stats(data: List[Number]) -> Tuple[float, float, float]:\n    \"\"\"Compute sample mean, variance and standard deviation in one pass.\n\n    Parameters\n    ----------\n    data : list[int | float]\n        Stream of numeric observations (may be empty).\n\n    Returns\n    -------\n    (mean, variance, std) : tuple[float, float, float]\n        Sample statistics rounded to 4 decimal places.  For an empty\n        input all three numbers are 0.0.\n    \"\"\"\n    if not data:\n        return (0.0, 0.0, 0.0)\n    n = 0\n    mean = 0.0\n    s = 0.0\n    for x in data:\n        n += 1\n        delta = x - mean\n        mean += delta / n\n        s += delta * (x - mean)\n    variance = s / (n - 1) if n > 1 else 0.0\n    std = math.sqrt(variance)\n    return (round(mean, 4), round(variance, 4), round(std, 4))"}
{"task_id": 283, "completion_id": 0, "solution": "import numpy as np\ndef _stable_sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Numerically stable sigmoid.\"\"\"\n    pos_mask = x >= 0\n    neg_mask = ~pos_mask\n    out = np.empty_like(x, dtype=float)\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n    exp_x = np.exp(x[neg_mask])\n    out[neg_mask] = exp_x / (1.0 + exp_x)\n    return out\ndef binary_log_loss_metrics(actual, predicted, regularization=1.0):\n    \"\"\"Compute Newton leaf value and split gain for binary cross-entropy loss.\n\n    Parameters\n    ----------\n    actual : np.ndarray\n        1-D array with true binary labels (0 or 1).\n    predicted : np.ndarray\n        1-D array with raw scores (before sigmoid).\n    regularization : float, default 1.0\n        L2-regularisation term \u03bb.\n\n    Returns\n    -------\n    list[float]\n        [leaf_value, gain] \u2013 each rounded to 4 decimals.\n    \"\"\"\n    y = np.asarray(actual, dtype=float).ravel()\n    f = np.asarray(predicted, dtype=float).ravel()\n    if y.shape != f.shape:\n        raise ValueError('`actual` and `predicted` must have the same shape.')\n    p = _stable_sigmoid(f)\n    g = p - y\n    h = p * (1.0 - p)\n    sum_g = g.sum()\n    sum_h = h.sum()\n    denom = sum_h + regularization\n    leaf_value = sum_g / denom\n    gain = 0.5 * sum_g ** 2 / denom\n    leaf_value = round(leaf_value, 4)\n    gain = round(gain, 4)\n    return [leaf_value, gain]"}
{"task_id": 284, "completion_id": 0, "solution": "def iht_get_index(obj: object, size: int, table: dict, overfull_count: list[int], readonly: bool=False) -> int | None:\n    \"\"\"Map *obj* to a stable integer index using a bounded index hash table.\n\n    Args:\n        obj:               Hashable object that should receive an index.\n        size:              Maximum number of *stored* entries.\n        table:             Mutable dictionary holding the current mapping.\n        overfull_count:    One-element list whose integer counts collisions.\n        readonly:          If True, never modify *table* or *overfull_count*.\n\n    Returns:\n        The integer index for *obj*, or None when *readonly* is True and the\n        object is not yet in *table*.\n    \"\"\"\n    if obj in table:\n        return table[obj]\n    if readonly:\n        return None\n    if len(table) < size:\n        index = len(table)\n        table[obj] = index\n        return index\n    collision_index = hash(obj) % size\n    overfull_count[0] += 1\n    return collision_index"}
{"task_id": 285, "completion_id": 0, "solution": "import numpy as np\ndef linear_kernel(x: np.ndarray, y: np.ndarray) -> list | int:\n    \"\"\"Compute the linear kernel (Gram matrix) between two data sets.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        First input data. Can be 1-D (single sample) or 2-D (multiple samples).\n    y : np.ndarray\n        Second input data. Can be 1-D (single sample) or 2-D (multiple samples).\n\n    Returns\n    -------\n    list | int\n        The linear kernel matrix as a nested Python list.  \n        If the feature dimensions of *x* and *y* differ, returns -1.\n    \"\"\"\n    if x.ndim == 1:\n        x = x.reshape(1, -1)\n    elif x.ndim != 2:\n        raise ValueError('Input x must be 1-D or 2-D.')\n    if y.ndim == 1:\n        y = y.reshape(1, -1)\n    elif y.ndim != 2:\n        raise ValueError('Input y must be 1-D or 2-D.')\n    if x.shape[1] != y.shape[1]:\n        return -1\n    return (x @ y.T).tolist()"}
{"task_id": 286, "completion_id": 0, "solution": "import random\nfrom typing import Callable, Tuple, List\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Retrieve an initializer function by its name.\n\n    Args:\n        name: The name of the initializer to look up (e.g. \"zeros_init\").\n\n    Returns:\n        The corresponding callable weight initializer.\n\n    Raises:\n        ValueError: If `name` is not found or is not callable.\n    \"\"\"\n    obj = globals().get(name)\n    if obj is None or not callable(obj):\n        raise ValueError('Invalid initialization function.')\n    return obj"}
{"task_id": 287, "completion_id": 0, "solution": "import numpy as np\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input tensor of shape (N, C, H, W).\n    pool_shape : (int, int)\n        Height and width of the pooling window  (p_h, p_w).\n    stride : (int, int)\n        Vertical and horizontal stride          (s_h, s_w).\n    accum_grad : np.ndarray\n        Gradient wrt. the pooled output. Shape must match the\n        forward-pass output.\n\n    Returns\n    -------\n    tuple[list, list]\n        (pooled_output, grad_input) \u2013 both rounded to 4 decimals and\n        converted to Python lists.\n    \"\"\"\n    (N, C, H, W) = X.shape\n    (p_h, p_w) = pool_shape\n    (s_h, s_w) = stride\n    out_h = (H - p_h) // s_h + 1\n    out_w = (W - p_w) // s_w + 1\n    pooled = np.zeros((N, C, out_h, out_w), dtype=X.dtype)\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                (h_start, h_end) = (i * s_h, i * s_h + p_h)\n                for j in range(out_w):\n                    (w_start, w_end) = (j * s_w, j * s_w + p_w)\n                    window = X[n, c, h_start:h_end, w_start:w_end]\n                    pooled[n, c, i, j] = window.mean()\n    grad_input = np.zeros_like(X, dtype=X.dtype)\n    scale = 1.0 / (p_h * p_w)\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                (h_start, h_end) = (i * s_h, i * s_h + p_h)\n                for j in range(out_w):\n                    (w_start, w_end) = (j * s_w, j * s_w + p_w)\n                    g = accum_grad[n, c, i, j] * scale\n                    grad_input[n, c, h_start:h_end, w_start:w_end] += g\n    pooled = np.round(pooled, 4).tolist()\n    grad_input = np.round(grad_input, 4).tolist()\n    return (pooled, grad_input)"}
{"task_id": 288, "completion_id": 0, "solution": "import numpy as np\ndef squared_error(actual, predicted):\n    \"\"\"Calculate the squared error or mean squared error.\n\n    If *actual* and *predicted* are scalars, return the squared difference.\n    If they are one-dimensional iterables (same length), return the mean squared\n    error.  Return -1 for length/type mismatches or empty iterables.\n\n    All results are rounded to 4 decimal places.\n    \"\"\"\n    numeric_scalar = (int, float, np.integer, np.floating)\n    is_scalar_a = isinstance(actual, numeric_scalar)\n    is_scalar_p = isinstance(predicted, numeric_scalar)\n    if is_scalar_a and is_scalar_p:\n        return round((actual - predicted) ** 2, 4)\n    if is_scalar_a ^ is_scalar_p:\n        return -1\n    if isinstance(actual, (str, bytes)) or isinstance(predicted, (str, bytes)):\n        return -1\n    try:\n        a_arr = np.asarray(actual, dtype=float)\n        p_arr = np.asarray(predicted, dtype=float)\n    except Exception:\n        return -1\n    if a_arr.ndim != 1 or p_arr.ndim != 1:\n        return -1\n    if a_arr.size == 0 or p_arr.size == 0 or a_arr.size != p_arr.size:\n        return -1\n    mse = np.mean((a_arr - p_arr) ** 2)\n    return round(float(mse), 4)"}
{"task_id": 289, "completion_id": 0, "solution": "def format_entries(entries: list[tuple[str, str]]) -> str:\n    \"\"\"Formats a list of (field, value) tuples.\n\n    Each tuple is converted into a line of the form:\n        field: \"value\"\n    and all lines are joined together by a single newline character. No extra\n    newline is added at the end.\n\n    Args:\n        entries: List of tuples where the first element is the field name and\n                 the second element is the corresponding value. Both should be\n                 strings; however, any value will be converted to its string\n                 representation.\n\n    Returns:\n        A single string containing all formatted lines separated by a newline.\n        Returns an empty string if *entries* is empty.\n    \"\"\"\n    if not entries:\n        return ''\n    formatted_lines = (f'{field}: \"{value}\"' for (field, value) in entries)\n    return '\\n'.join(formatted_lines)"}
{"task_id": 290, "completion_id": 0, "solution": "import numpy as np\nfrom dataclasses import dataclass\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\"\"\"\n    feature: int\n    threshold: float\n    left: object\n    right: object\n_TOL = 1e-08\ndef _values_equal(a, b):\n    \"\"\"Compare two prediction values with the required tolerance.\"\"\"\n    try:\n        return np.allclose(a, b, atol=_TOL, rtol=_TOL)\n    except TypeError:\n        return a == b\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\"\"\"\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return _values_equal(tree_a.value, tree_b.value)\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        return tree_a.feature == tree_b.feature and np.isclose(tree_a.threshold, tree_b.threshold, atol=_TOL, rtol=_TOL) and compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    return False"}
{"task_id": 291, "completion_id": 0, "solution": "import numpy as np\ndef pca_transform(X: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Project *X* onto its first *n_components* principal components.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (m, d).\n    n_components : int\n        Number of principal components to retain (1 \u2264 n_components \u2264 d).\n\n    Returns\n    -------\n    list[list[float]]\n        The transformed data rounded to 4 decimals, or -1 if *n_components*\n        is outside the valid range.\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        return -1\n    if X.ndim != 2:\n        return -1\n    (m, d) = X.shape\n    if not 1 <= n_components <= d:\n        return -1\n    cov_mat = np.cov(X, rowvar=False)\n    (eig_vals, eig_vecs) = np.linalg.eig(cov_mat)\n    eig_vals = eig_vals.real\n    eig_vecs = eig_vecs.real\n    order = np.argsort(eig_vals)[::-1]\n    eig_vecs = eig_vecs[:, order]\n    V = eig_vecs[:, :n_components]\n    first_elements_negative = V[0, :] < 0\n    V[:, first_elements_negative] *= -1\n    transformed = X @ V\n    transformed = np.round(transformed, 4)\n    return transformed.tolist()"}
{"task_id": 292, "completion_id": 0, "solution": "import numpy as np\ndef single_point_crossover(parent1: list[list[int | float]], parent2: list[list[int | float]], cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix (list of lists).\n        parent2: Second parent weight matrix (same shape as *parent1*).\n        cutoff : Column index at which crossover starts (inclusive).\n\n    Returns\n    -------\n        Tuple (child1, child2) with the two children as ordinary Python\n        lists, or -1 if the parent matrices do not have identical shapes.\n    \"\"\"\n    p1 = np.asarray(parent1, dtype=float)\n    p2 = np.asarray(parent2, dtype=float)\n    if p1.ndim != 2 or p2.ndim != 2 or p1.shape != p2.shape:\n        return -1\n    (rows, cols) = p1.shape\n    if not 0 <= cutoff <= cols:\n        raise ValueError(f'cutoff must be in [0, {cols}]')\n    child1 = p1.copy()\n    child2 = p2.copy()\n    child1[:, cutoff:] = p2[:, cutoff:]\n    child2[:, cutoff:] = p1[:, cutoff:]\n    return (child1.tolist(), child2.tolist())"}
{"task_id": 293, "completion_id": 0, "solution": "def map_agent_environment(agent_name: str):\n    \"\"\"Return the environment name and access type that should be used for a given RL agent.\n\n    The mapping is case-insensitive and ignores leading/trailing spaces.  \n    If the agent name is not recognised, the function returns -1.\n\n    Args:\n        agent_name: Name of the RL agent (e.g. \"dqn\", \"A3C\", etc.).\n\n    Returns:\n        Tuple[str, str] if the agent is known, or -1 otherwise.\n    \"\"\"\n    key = agent_name.strip().lower()\n    mapping = {'dqn': ('CartPole-v0', 'unwrapped'), 'ddpg': ('Pendulum-v0', 'env'), 'a3c': ('Pendulum-v0', 'unwrapped'), 'a2c': ('CartPole-v0', 'env'), 'a2c_multi': ('CartPole-v0', 'raw'), 'trpo': ('Pendulum-v0', 'unwrapped')}\n    return mapping.get(key, -1)"}
{"task_id": 294, "completion_id": 0, "solution": "from typing import Any, Dict, List, Tuple\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert the custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph with the keys\n          * 'is_directed' : bool\n          * '_V2I'        : dict  (vertex label -> consecutive index 0..n-1)\n          * '_G'          : list  (for each vertex index a list with tuples\n                                   (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary mapping every vertex label to a list of (neighbour, weight)\n        tuples.  Lists are sorted lexicographically by the neighbour label.\n        For undirected graphs every edge appears once in each direction.\n    \"\"\"\n    if not G or not G.get('_V2I'):\n        return {}\n    directed: bool = bool(G.get('is_directed', False))\n    adj: Dict[Any, List[Tuple[Any, float]]] = {v: [] for v in G['_V2I']}\n    processed_undirected = set()\n    for edge_list in G['_G']:\n        for (u, v, w) in edge_list:\n            if directed:\n                adj[u].append((v, w))\n            else:\n                key = (u, v) if u <= v else (v, u)\n                if key in processed_undirected:\n                    continue\n                processed_undirected.add(key)\n                adj[u].append((v, w))\n                adj[v].append((u, w))\n    for neighbours in adj.values():\n        neighbours.sort(key=lambda t: str(t[0]))\n    return adj"}
{"task_id": 296, "completion_id": 0, "solution": "import numpy as np\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1 (or, for boolean arrays,\n        only ``True``/``False``), otherwise ``False``.\n    \"\"\"\n    arr = np.asarray(x)\n    if arr.dtype == np.bool_:\n        return True\n    return np.isin(arr, (0, 1)).all()"}
{"task_id": 297, "completion_id": 0, "solution": "import numpy as np\ndef min_max_scale(data: list[list[int | float]], feature_range: tuple=(0, 1)) -> list[list[float]]:\n    \"\"\"Scale every feature column of *data* to a given interval.\n\n    Parameters\n    ----------\n    data : list[list[int | float]]\n        Two-dimensional numerical data set where each inner list is a sample.\n    feature_range : tuple, optional\n        Desired range for the transformed data given as ``(lower, upper)``.\n        The default is ``(0, 1)``.\n\n    Returns\n    -------\n    list[list[float]]\n        Scaled data rounded to four decimal places.  \n        Returns ``-1`` if ``feature_range`` is invalid.\n    \"\"\"\n    if not isinstance(feature_range, tuple) or len(feature_range) != 2:\n        return -1\n    (a, b) = feature_range\n    if a >= b:\n        return -1\n    if data == []:\n        return []\n    arr = np.asarray(data, dtype=np.float64)\n    if arr.ndim == 1:\n        arr = arr.reshape(-1, 1)\n    col_min = arr.min(axis=0)\n    col_max = arr.max(axis=0)\n    col_range = col_max - col_min\n    const_mask = col_range == 0\n    col_range[const_mask] = 1.0\n    scaled = (arr - col_min) / col_range\n    scaled = scaled * (b - a) + a\n    if np.any(const_mask):\n        scaled[:, const_mask] = a\n    scaled = np.nan_to_num(scaled, nan=0.0, posinf=0.0, neginf=0.0)\n    scaled = np.round(scaled, 4)\n    return scaled.tolist()"}
{"task_id": 298, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom typing import List, Tuple, Dict\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Notes\n    -----\n    \u2022 White-space is used for tokenisation.\n    \u2022 For N > 1 every sentence is padded with N\u22121 '<bol>' tokens in front\n      and a single '<eol>' token at the end.  Unigrams use no padding.\n    \u2022 If any N-gram (or its (N-1)-gram prefix) required for the query is\n      unseen in *corpus*, or if the query is too short, the function\n      returns float('-inf').\n    \u2022 The final result is rounded to 4 decimal places.\n    \"\"\"\n    if N < 1:\n        raise ValueError('N must be \u2265 1')\n    if not corpus:\n        return float('-inf')\n    if N == 1:\n        unigram_counts: Counter[str] = Counter()\n        total_tokens = 0\n        for sent in corpus:\n            tokens = sent.split()\n            unigram_counts.update(tokens)\n            total_tokens += len(tokens)\n    else:\n        ngram_counts: Counter[Tuple[str, ...]] = Counter()\n        prefix_counts: Counter[Tuple[str, ...]] = Counter()\n        for sent in corpus:\n            tokens = ['<bol>'] * (N - 1) + sent.split() + ['<eol>']\n            for i in range(len(tokens) - N + 1):\n                ngram = tuple(tokens[i:i + N])\n                prefix = tuple(tokens[i:i + N - 1])\n                ngram_counts[ngram] += 1\n                prefix_counts[prefix] += 1\n    if N == 1:\n        q_tokens = query.split()\n        if not q_tokens or total_tokens == 0:\n            return float('-inf')\n    else:\n        q_tokens = ['<bol>'] * (N - 1) + query.split() + ['<eol>']\n        if len(q_tokens) < N:\n            return float('-inf')\n    log_sum = 0.0\n    if N == 1:\n        for tok in q_tokens:\n            cnt = unigram_counts.get(tok, 0)\n            if cnt == 0:\n                return float('-inf')\n            prob = cnt / total_tokens\n            log_sum += math.log(prob)\n    else:\n        for i in range(len(q_tokens) - N + 1):\n            ngram = tuple(q_tokens[i:i + N])\n            prefix = tuple(q_tokens[i:i + N - 1])\n            cnt_ng = ngram_counts.get(ngram, 0)\n            cnt_pre = prefix_counts.get(prefix, 0)\n            if cnt_ng == 0 or cnt_pre == 0:\n                return float('-inf')\n            prob = cnt_ng / cnt_pre\n            log_sum += math.log(prob)\n    return round(log_sum, 4)"}
{"task_id": 299, "completion_id": 0, "solution": "from __future__ import annotations\nimport numpy as np\ndef k_means(X: np.ndarray, k: int, max_iterations: int=500) -> list[int]:\n    \"\"\"K-Means clustering from scratch (deterministic initialisation).\n\n    Args:\n        X: 2-D NumPy array of shape (m, n).\n        k:  Number of clusters (must satisfy 1 \u2264 k \u2264 m).\n        max_iterations: Hard limit on the number of iterations.\n\n    Returns:\n        List[int] of length m; the i-th entry is the cluster index of sample i.\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        raise TypeError('X must be a NumPy array.')\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D array (m samples \u00d7 n features).')\n    (m, n) = X.shape\n    if not 1 <= k <= m:\n        raise ValueError('k must be in the range [1, m].')\n    X = X.astype(float, copy=False)\n    centroids = X[:k].copy()\n    for _ in range(max_iterations):\n        diff = X[:, None, :] - centroids[None, :, :]\n        distances = np.sum(diff ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = centroids.copy()\n        for j in range(k):\n            mask = labels == j\n            if np.any(mask):\n                new_centroids[j] = X[mask].mean(axis=0)\n        if np.allclose(new_centroids, centroids):\n            break\n        centroids = new_centroids\n    return labels.tolist()"}
{"task_id": 300, "completion_id": 0, "solution": "import heapq\nfrom typing import List, Union\nNumber = Union[int, float]\ndef top_k_elements(sequence: List[Number], k: int, order: str='largest') -> List[Number]:\n    \"\"\"Return the k largest or k smallest elements of *sequence*.\n\n    The data stream is inspected exactly once and never keeps more than k\n    elements in memory by exploiting the binary\u2010heap semantics provided by\n    heapq.\n\n    Args:\n        sequence: Iterable of numbers.\n        k: How many extreme values to retain.\n        order: Either \"largest\" (keep the biggest values) or \"smallest\"\n               (keep the smallest values).\n\n    Returns:\n        A list of at most k numbers sorted according to *order*.\n\n    Raises:\n        ValueError: If *order* is neither \"largest\" nor \"smallest\".\n    \"\"\"\n    if k <= 0 or not sequence:\n        return []\n    if order not in {'largest', 'smallest'}:\n        raise ValueError('order must be either \"largest\" or \"smallest\"')\n    n = len(sequence)\n    if k >= n:\n        return sorted(sequence, reverse=order == 'largest')\n    if order == 'largest':\n        heap: List[Number] = []\n        for value in sequence:\n            if len(heap) < k:\n                heapq.heappush(heap, value)\n            elif value > heap[0]:\n                heapq.heapreplace(heap, value)\n        return sorted(heap, reverse=True)\n    else:\n        heap: List[Number] = []\n        for value in sequence:\n            neg_val = -value\n            if len(heap) < k:\n                heapq.heappush(heap, neg_val)\n            elif neg_val > heap[0]:\n                heapq.heapreplace(heap, neg_val)\n        return sorted([-x for x in heap])"}
{"task_id": 301, "completion_id": 0, "solution": "class Node:\n    \"\"\"\n    Simple binary-search-tree node that compares itself by ``val`` only.\n    \"\"\"\n    __slots__ = ('key', 'val', 'left', 'right')\n\n    def __init__(self, key, val):\n        self.key = key\n        self.val = val\n        self.left: 'Node | None' = None\n        self.right: 'Node | None' = None\n\n    def __lt__(self, other):\n        return self.val < other.val\n\n    def __le__(self, other):\n        return self.val <= other.val\n\n    def __gt__(self, other):\n        return self.val > other.val\n\n    def __ge__(self, other):\n        return self.val >= other.val\ndef inorder_keys(pairs):\n    \"\"\"Build a BST from (key, value) *pairs* and return keys from an in-order traversal.\n\n    Args:\n        pairs (list[tuple[Any, int | float]]): List of (key, numeric value) pairs.\n\n    Returns:\n        list: Keys in in-order (ascending by numeric value).  Empty list if *pairs* is empty.\n    \"\"\"\n\n    def insert(root: Node, node: Node) -> None:\n        \"\"\"Iteratively insert *node* into the BST whose root is *root*.\"\"\"\n        cur = root\n        while True:\n            if node < cur:\n                if cur.left is None:\n                    cur.left = node\n                    return\n                cur = cur.left\n            else:\n                if cur.right is None:\n                    cur.right = node\n                    return\n                cur = cur.right\n\n    def inorder(node: Node | None, out: list) -> None:\n        \"\"\"Recursive in-order traversal collecting *key*s.\"\"\"\n        if node is None:\n            return\n        inorder(node.left, out)\n        out.append(node.key)\n        inorder(node.right, out)\n    if not pairs:\n        return []\n    root = Node(*pairs[0])\n    for (key, val) in pairs[1:]:\n        insert(root, Node(key, val))\n    result: list = []\n    inorder(root, result)\n    return result"}
{"task_id": 302, "completion_id": 0, "solution": "import numpy as np\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample (shape = [n_samples, n_features]).\n        k: Desired number of clusters (k \u2264 n_samples).\n\n    Returns:\n        Python list of length n_samples whose entries are integers in [0, k-1].\n    \"\"\"\n    n_samples = X.shape[0]\n    diff = X[:, None, :] - X[None, :, :]\n    dist = np.linalg.norm(diff, axis=2)\n    W = 1.0 / (1.0 + dist)\n    np.fill_diagonal(W, 0.0)\n    degree = W.sum(axis=1)\n    L = np.diag(degree) - W\n    (eig_vals, eig_vecs) = np.linalg.eigh(L)\n    E = eig_vecs[:, :k]\n    centroids = E[:k].copy()\n    labels = np.full(n_samples, -1, dtype=int)\n    max_iter = 100\n    rng = np.random.default_rng()\n    for _ in range(max_iter):\n        dists = np.linalg.norm(E[:, None, :] - centroids[None, :, :], axis=2)\n        new_labels = dists.argmin(axis=1)\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n        for c in range(k):\n            mask = labels == c\n            if np.any(mask):\n                centroids[c] = E[mask].mean(axis=0)\n            else:\n                random_idx = rng.integers(0, n_samples)\n                centroids[c] = E[random_idx]\n    return labels.tolist()"}
{"task_id": 303, "completion_id": 0, "solution": "def err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str='') -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    prev_ix = max(ix - 1, 0)\n    (prev_mine, prev_label) = params[prev_ix]\n    (curr_mine, curr_label) = params[ix]\n    gold_prev = golds.get(prev_label, '')\n    gold_curr = golds.get(curr_label, '')\n    header = '-' * 25 + ' DEBUG ' + '-' * 25\n    footer = '-' * 23 + ' END DEBUG ' + '-' * 23\n    lines = [header, f'Mine (prev) [{prev_label}]:', f'{prev_mine}', '', f'Theirs (prev) [{prev_label}]:', f'{gold_prev}', '', f'Mine [{curr_label}]:', f'{curr_mine}', '', f'Theirs [{curr_label}]:', f'{gold_curr}{warn_str}', footer]\n    return '\\n'.join(lines)"}
{"task_id": 304, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef bayesian_posterior_mean(X: np.ndarray, y: np.ndarray, alpha: float=1.0, beta: float=1.0, mu=0.0, V=None, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Posterior mean (MAP) of the coefficients in Bayesian linear regression.\n\n    Parameters\n    ----------\n    X : (N, M) array_like\n        Design matrix.\n    y : (N,) array_like\n        Targets.\n    alpha, beta : float\n        Hyper\u2013parameters of the inverse-gamma prior on \u03c3\u00b2 (kept for API\n        completeness \u2013 they do not enter the MAP of the weights).\n    mu : float or array_like\n        Prior mean for the weights.  Scalar values are broadcast.\n    V : None, float, 1-D or 2-D array_like\n        Prior scale.  None\u2192I, scalar\u2192s\u00b7I, 1-D\u2192diag(v), 2-D\u2192full matrix.\n    fit_intercept : bool\n        Whether to include a bias column of ones.\n\n    Returns\n    -------\n    list[float]\n        Posterior mean rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    if fit_intercept:\n        X = np.column_stack((np.ones(X.shape[0]), X))\n    (N, M) = X.shape\n    if V is None:\n        V_mat = np.eye(M)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 0:\n            V_mat = float(V_arr) * np.eye(M)\n        elif V_arr.ndim == 1:\n            if V_arr.size != M:\n                raise ValueError('Diagonal length of V does not match #features.')\n            V_mat = np.diag(V_arr)\n        elif V_arr.ndim == 2:\n            if V_arr.shape != (M, M):\n                raise ValueError('Shape of V does not match #features.')\n            V_mat = V_arr\n        else:\n            raise ValueError('Unsupported shape for V.')\n    V_inv = np.linalg.inv(V_mat)\n    if np.isscalar(mu):\n        mu_vec = np.full(M, float(mu))\n    else:\n        mu_vec = np.asarray(mu, dtype=float).ravel()\n        if mu_vec.size != M:\n            raise ValueError('Length of mu does not match #features.')\n    XtX = X.T @ X\n    Sigma_b = np.linalg.inv(V_inv + XtX)\n    rhs = V_inv @ mu_vec + X.T @ y\n    mu_b = Sigma_b @ rhs\n    return np.round(mu_b, 4).tolist()"}
{"task_id": 305, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    \"\"\"Compute the element-wise logistic sigmoid in a numerically stable way.\n\n    The function accepts a scalar, list, tuple or NumPy array and returns a\n    Python list with each sigmoid value rounded to 4 decimal places.\n    \"\"\"\n    arr = np.asarray(x, dtype=float)\n    out = np.empty_like(arr)\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    exp_x = np.exp(arr[neg_mask])\n    out[neg_mask] = exp_x / (1.0 + exp_x)\n    out = np.round(out, 4)\n    return out.ravel().tolist()"}
{"task_id": 307, "completion_id": 0, "solution": "import numpy as np\ndef bootstrap_sample(X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate a bootstrap sample (sampling with replacement) from the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, ...).\n    y : np.ndarray\n        Target vector/array of shape (n_samples, ...).\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        (X_sample, y_sample) \u2013 both have exactly the same shapes as the\n        corresponding inputs.\n\n    Notes\n    -----\n    \u2022 Sampling is performed exclusively with NumPy\u2019s global RNG, so results are\n      reproducible if `np.random.seed(...)` is set beforehand.\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(f'X and y must have the same number of samples along axis 0 (got {X.shape[0]} and {y.shape[0]}).')\n    n_samples = X.shape[0]\n    indices = np.random.randint(0, n_samples, size=n_samples)\n    return (X[indices], y[indices])"}
{"task_id": 308, "completion_id": 0, "solution": "import numpy as np\ndef mel_filterbank(N: int, n_filters: int=20, fs: int=44000, min_freq: int=0, max_freq: int | None=None, normalize: bool=True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    Parameters\n    ----------\n    N : int\n        DFT length (number of frequency-domain samples).\n    n_filters : int, default 20\n        Number of Mel filters to construct.\n    fs : int, default 44_000\n        Sampling frequency of the signal in Hertz.\n    min_freq : int, default 0\n        Lower frequency limit of the filterbank in Hertz.\n    max_freq : int | None, optional\n        Upper frequency limit of the filterbank in Hertz.  If *None* the\n        Nyquist frequency *fs/2* is used.\n    normalize : bool, default True\n        Whether to scale each filter so that its (Mel-domain) area equals 1.\n\n    Returns\n    -------\n    list[list[float]]\n        The filterbank matrix with shape (n_filters, N//2 + 1).  All values are\n        rounded to four decimal places.\n    \"\"\"\n\n    def hz2mel(f_hz: np.ndarray | float) -> np.ndarray | float:\n        return 2595.0 * np.log10(1.0 + np.asarray(f_hz) / 700.0)\n\n    def mel2hz(mel: np.ndarray | float) -> np.ndarray | float:\n        return 700.0 * (10 ** (np.asarray(mel) / 2595.0) - 1.0)\n    if max_freq is None:\n        max_freq = fs / 2\n    if not 0 <= min_freq < max_freq <= fs / 2:\n        raise ValueError('min_freq must be >= 0, and max_freq must not exceed fs/2 with min_freq < max_freq.')\n    (mel_min, mel_max) = (hz2mel(min_freq), hz2mel(max_freq))\n    mel_points = np.linspace(mel_min, mel_max, n_filters + 2)\n    hz_points = mel2hz(mel_points)\n    bin_freqs = np.arange(N // 2 + 1) * (fs / N)\n    F = np.zeros((n_filters, N // 2 + 1), dtype=float)\n    for i in range(n_filters):\n        (f_left, f_center, f_right) = hz_points[i:i + 3]\n        left_mask = np.logical_and(bin_freqs >= f_left, bin_freqs <= f_center)\n        F[i, left_mask] = (bin_freqs[left_mask] - f_left) / (f_center - f_left)\n        right_mask = np.logical_and(bin_freqs >= f_center, bin_freqs <= f_right)\n        F[i, right_mask] = (f_right - bin_freqs[right_mask]) / (f_right - f_center)\n        if normalize:\n            F[i, :] *= 2.0 / (f_right - f_left)\n    F_rounded = np.round(F, 4).tolist()\n    return F_rounded"}
{"task_id": 309, "completion_id": 0, "solution": "import numpy as np\ndef fm_predict(X: np.ndarray, w0: float, w: np.ndarray, v: np.ndarray) -> list[float]:\n    \"\"\"Return Factorization Machine predictions for all samples in *X*.\n    \n    Parameters\n    ----------\n    X  : (m, n) np.ndarray\n        Feature matrix with m samples and n features.\n    w0 : float\n        Global bias term.\n    w  : (n,) np.ndarray\n        Linear weight vector.\n    v  : (n, k) np.ndarray\n        Factor matrix whose columns contain the latent factors.\n    \n    Returns\n    -------\n    list[float]\n        FM predictions for each sample, rounded to four decimal places.\n    \"\"\"\n    linear_term = X @ w\n    xv = X @ v\n    xv_sq = xv ** 2\n    x_sq_v_sq = X ** 2 @ v ** 2\n    interaction_term = 0.5 * np.sum(xv_sq - x_sq_v_sq, axis=1)\n    y_hat = w0 + linear_term + interaction_term\n    return np.round(y_hat, 4).tolist()"}
{"task_id": 310, "completion_id": 0, "solution": "import numpy as np\ndef root_mean_squared_error(actual: list[float] | np.ndarray, predicted: list[float] | np.ndarray) -> float:\n    \"\"\"Compute the Root Mean Squared Error (RMSE) between two numeric sequences.\n\n    The function returns ``-1`` if the two inputs do not have the same\n    non-zero length.\n\n    Args:\n        actual: A list or NumPy array containing the true values.\n        predicted: A list or NumPy array containing the predicted values.\n\n    Returns:\n        A float \u2013 the RMSE rounded to 4 decimal places, or ``-1`` on invalid\n        input.\n    \"\"\"\n    try:\n        n_actual = len(actual)\n        n_pred = len(predicted)\n    except TypeError:\n        return -1\n    if n_actual == 0 or n_pred == 0 or n_actual != n_pred:\n        return -1\n    actual_arr = np.asarray(actual, dtype=float)\n    predicted_arr = np.asarray(predicted, dtype=float)\n    mse = np.mean((actual_arr - predicted_arr) ** 2)\n    rmse = np.sqrt(mse)\n    return round(float(rmse), 4)"}
{"task_id": 311, "completion_id": 0, "solution": "import numpy as np\ndef clip_non_negative(arr):\n    \"\"\"Replace every negative number in *arr* with 0.0.\n\n    Parameters\n    ----------\n    arr : list | numpy.ndarray\n        A list (possibly nested) or NumPy array containing numeric values.\n\n    Returns\n    -------\n    list\n        A Python list of the same shape where all negative values are replaced\n        by 0.0. All elements in the returned list are floats.\n    \"\"\"\n    arr_np = np.array(arr, dtype=float, copy=True)\n    clipped = np.clip(arr_np, a_min=0.0, a_max=None)\n    return clipped.tolist()"}
{"task_id": 312, "completion_id": 0, "solution": "import numpy as np\ndef blackman_harris(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    N = window_len - 1 if symmetric else window_len\n    (a0, a1, a2, a3) = (0.35875, 0.48829, 0.14128, 0.01168)\n    n = np.arange(window_len)\n    w = a0 - a1 * np.cos(2 * np.pi * n / N) + a2 * np.cos(4 * np.pi * n / N) - a3 * np.cos(6 * np.pi * n / N)\n    return [round(float(x), 4) for x in w]"}
{"task_id": 313, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Return E[log X_t] for X ~ Dir(gamma[d, :]), rounded to 4 decimals.\"\"\"\n\n    def digamma(x):\n        \"\"\"\n        Digamma \u03c8(x) for positive real x.\n        Small arguments are stepped up with the recurrence\n            \u03c8(x) = \u03c8(x + 1) \u2212 1/x\n        until x \u2265 6, after which an asymptotic expansion is applied:\n            \u03c8(x) \u2248 ln x \u2212 1/(2x) \u2212 1/(12x\u00b2) + 1/(120x\u2074) \u2212 1/(252x\u2076)\n        The combination is accurate to better than 1e-6 for all x > 0.\n        \"\"\"\n        res = 0.0\n        x = float(x)\n        while x < 6.0:\n            res -= 1.0 / x\n            x += 1.0\n        inv = 1.0 / x\n        inv2 = inv * inv\n        inv4 = inv2 * inv2\n        inv6 = inv4 * inv2\n        res += math.log(x) - 0.5 * inv - inv2 / 12.0 + inv4 / 120.0 - inv6 / 252.0\n        return res\n    g = np.asarray(gamma, dtype=float)\n    row = g[d]\n    alpha_t = row[t]\n    row_sum = float(row.sum())\n    value = digamma(alpha_t) - digamma(row_sum)\n    return round(value, 4)"}
{"task_id": 315, "completion_id": 0, "solution": "import numpy as np\ndef _to_array(x):\n    \"\"\"Convert *x* to a NumPy float array.\"\"\"\n    return np.asarray(x, dtype=float)\ndef _mae(y_true, y_pred):\n    \"\"\"Mean Absolute Error.\"\"\"\n    (y_true, y_pred) = (_to_array(y_true), _to_array(y_pred))\n    return float(np.round(np.mean(np.abs(y_true - y_pred)), 4))\ndef _mse(y_true, y_pred):\n    \"\"\"Mean Squared Error.\"\"\"\n    (y_true, y_pred) = (_to_array(y_true), _to_array(y_pred))\n    return float(np.round(np.mean((y_true - y_pred) ** 2), 4))\ndef _bce(y_true, y_pred):\n    \"\"\"Binary Cross-Entropy / Log-loss.\"\"\"\n    (y_true, y_pred) = (_to_array(y_true), _to_array(y_pred))\n    y_pred = np.clip(y_pred, 1e-15, 1.0 - 1e-15)\n    loss = -np.mean(y_true * np.log(y_pred) + (1.0 - y_true) * np.log(1.0 - y_pred))\n    return float(np.round(loss, 4))\ndef get_loss(name: str):\n    \"\"\"Return the loss function identified by *name* (case-insensitive).\n\n    Recognised aliases (underscores ignored):\n        MAE  \u2013  \"mae\", \"mean_absolute_error\"\n        MSE  \u2013  \"mse\", \"mean_squared_error\"\n        BCE  \u2013  \"cross_entropy\", \"log_loss\", \"ce\"\n\n    Raises\n    ------\n    ValueError\n        If *name* does not correspond to a supported loss.\n    \"\"\"\n    if not isinstance(name, str):\n        raise ValueError('Invalid loss function.')\n    key = name.replace('_', '').lower()\n    registry = {'mae': _mae, 'meanabsoluteerror': _mae, 'mse': _mse, 'meansquarederror': _mse, 'crossentropy': _bce, 'logloss': _bce, 'ce': _bce}\n    try:\n        return registry[key]\n    except KeyError:\n        raise ValueError('Invalid loss function.')"}
{"task_id": 316, "completion_id": 0, "solution": "import numpy as np\ndef euclidean(x, y):\n    \"\"\"Compute the Euclidean (L2) distance between two equal-length vectors.\n\n    Args:\n        x (list | tuple | np.ndarray): First real vector.\n        y (list | tuple | np.ndarray): Second real vector.\n\n    Returns:\n        float | int: L2 distance rounded to four decimals, or -1 if lengths differ.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    x_arr = np.asarray(x, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    dist = np.linalg.norm(x_arr - y_arr)\n    return round(dist, 4)"}
{"task_id": 317, "completion_id": 0, "solution": "from typing import List, Tuple, Dict\ndef err_fmt(params: List[Tuple[str, str]], golds: Dict[str, str], ix: int, warn_str: str='') -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\"\"\"\n    prev_ix = max(ix - 1, 0)\n    (prev_pred, prev_label) = params[prev_ix]\n    (curr_pred, curr_label) = params[ix]\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    header = '-' * 25 + ' DEBUG ' + '-' * 25\n    footer = '-' * 23 + ' END DEBUG ' + '-' * 23\n    parts = [header, f'Mine (prev) [{prev_label}]:', prev_pred, '', f'Theirs (prev) [{prev_label}]:', prev_gold, '', '', f'Mine [{curr_label}]:', curr_pred, '', f'Theirs [{curr_label}]:', f'{curr_gold}{warn_str}', footer]\n    return '\\n'.join(parts)"}
{"task_id": 318, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef _sign(z):\n    \"\"\"Return -1 for z < 0, +1 otherwise (i.e. sign(0)=+1).\"\"\"\n    return np.where(z < 0, -1, 1)\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples with labels \u20131 or +1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (\u20131 or +1) for each sample in X_test.\n    \"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    X_test = np.asarray(X_test, dtype=float)\n    (n_samples, n_features) = X_train.shape\n    w = np.full(n_samples, 1.0 / n_samples)\n    stumps = []\n    for _ in range(n_estimators):\n        best_error = float('inf')\n        best_j = best_t = best_p = None\n        for j in range(n_features):\n            xj = X_train[:, j]\n            idx = np.argsort(xj)\n            x_sorted = xj[idx]\n            y_sorted = y_train[idx]\n            w_sorted = w[idx]\n            pos_mask = y_sorted == 1\n            neg_mask = ~pos_mask\n            pos_cumsum = np.cumsum(w_sorted * pos_mask)\n            neg_cumsum = np.cumsum(w_sorted * neg_mask)\n            pos_total = pos_cumsum[-1]\n            neg_total = neg_cumsum[-1]\n            pos_left = np.concatenate(([0.0], pos_cumsum))\n            neg_left = np.concatenate(([0.0], neg_cumsum))\n            err_p1 = pos_left + (neg_total - neg_left)\n            err_pn1 = neg_left + (pos_total - pos_left)\n            for (polarity, errors) in ((1, err_p1), (-1, err_pn1)):\n                k = int(np.argmin(errors))\n                err = errors[k]\n                if err < best_error - 1e-12:\n                    best_error = err\n                    best_j = j\n                    best_p = polarity\n                    if k == 0:\n                        best_t = x_sorted[0] - 1e-10\n                    elif k == len(x_sorted):\n                        best_t = x_sorted[-1] + 1e-10\n                    else:\n                        best_t = 0.5 * (x_sorted[k - 1] + x_sorted[k])\n        eps = max(best_error, 1e-10)\n        alpha = 0.5 * math.log((1.0 - eps) / eps)\n        preds = best_p * _sign(X_train[:, best_j] - best_t)\n        w *= np.exp(-alpha * y_train * preds)\n        w /= w.sum()\n        stumps.append((best_j, best_t, best_p, alpha))\n    agg = np.zeros(X_test.shape[0])\n    for (j, t, p, alpha) in stumps:\n        agg += alpha * p * _sign(X_test[:, j] - t)\n    y_pred = _sign(agg).astype(int)\n    return y_pred.tolist()"}
{"task_id": 319, "completion_id": 0, "solution": "import numpy as np\ndef stochastic_gradient_descent(X: np.ndarray | list, y: np.ndarray | list, learning_rate: float=0.01, epochs: int=1000) -> list[float]:\n    \"\"\"Learn a linear model with Stochastic Gradient Descent.\n\n    Args:\n        X: Feature matrix (m\u00d7n) \u2013 NumPy array or list-of-lists.\n        y: Target vector (m,) or (m,1) \u2013 NumPy array or list.\n        learning_rate: Learning rate (\u03b1).\n        epochs: Number of epochs (full passes over the dataset).\n\n    Returns:\n        Python list containing the learned parameters [\u03b8\u2080, \u03b8\u2081, \u2026, \u03b8_n],\n        rounded to 4 decimal places.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    (m, n) = X.shape\n    X_b = np.hstack((np.ones((m, 1)), X))\n    theta = np.zeros(n + 1, dtype=float)\n    for _ in range(epochs):\n        for i in range(m):\n            xi = X_b[i]\n            error = theta @ xi - y[i]\n            theta -= learning_rate * error * xi\n    return [round(val, 4) for val in theta.tolist()]"}
{"task_id": 321, "completion_id": 0, "solution": "import numpy as np\nimport string\ndef genetic_algorithm(target: str, population_size: int, mutation_rate: float, iterations: int) -> str:\n    \"\"\"Evolve a population of random strings towards *target* and\n    return the fittest string encountered.\"\"\"\n    np.random.seed(42)\n    chars = ' ' + string.ascii_lowercase + string.ascii_uppercase\n    char2idx = {c: i for (i, c) in enumerate(chars)}\n    idx2char = np.array(list(chars))\n    L = len(target)\n    target_idx = np.fromiter((char2idx[c] for c in target), dtype=np.int16, count=L)\n\n    def random_individual() -> str:\n        return ''.join(np.random.choice(idx2char, size=L))\n\n    def fitness(population):\n        \"\"\"Return fitness np.array of shape (n,) for list/array of strings.\"\"\"\n        pop_idx = np.array([[char2idx[c] for c in s] for s in population], dtype=np.int16)\n        diff = np.abs(pop_idx - target_idx)\n        score = 1.0 / (diff.sum(axis=1) + 1e-06)\n        return score\n    population = [random_individual() for _ in range(population_size)]\n    best_individual = ''\n    best_score = -np.inf\n    for _ in range(iterations):\n        fit = fitness(population)\n        idx_best = int(np.argmax(fit))\n        if fit[idx_best] > best_score:\n            best_score = float(fit[idx_best])\n            best_individual = population[idx_best]\n        if best_individual == target:\n            break\n        probs = fit / fit.sum()\n        parent_indices = np.random.choice(population_size, size=population_size, replace=False, p=probs)\n        parents = [population[i] for i in parent_indices]\n        children = []\n        for i in range(0, population_size - 1, 2):\n            (p1, p2) = (parents[i], parents[i + 1])\n            cx_point = np.random.randint(1, L)\n            c1 = p1[:cx_point] + p2[cx_point:]\n            c2 = p2[:cx_point] + p1[cx_point:]\n\n            def mutate(child):\n                if mutation_rate == 0.0:\n                    return child\n                child_arr = np.frombuffer(child.encode('ascii'), dtype='S1')\n                mask = np.random.rand(L) < mutation_rate\n                if mask.any():\n                    child_arr[mask] = np.random.choice(idx2char, size=mask.sum())\n                return b''.join(child_arr).decode()\n            children.extend((mutate(c1), mutate(c2)))\n        if population_size % 2 == 1:\n            children.append(parents[-1])\n        population = children[:population_size]\n    return best_individual"}
{"task_id": 322, "completion_id": 0, "solution": "import numpy as np\nEPS = 1e-15\ndef binary_crossentropy(actual: np.ndarray, predicted: np.ndarray) -> float:\n    \"\"\"Compute the mean binary cross-entropy loss.\n\n    Parameters\n    ----------\n    actual : np.ndarray\n        Ground-truth labels containing only 0s and 1s.\n    predicted : np.ndarray\n        Predicted probabilities for the positive class. Must have the same\n        shape as `actual`.\n\n    Returns\n    -------\n    float\n        Mean BCE value rounded to four decimal places.\n    \"\"\"\n    if actual.shape != predicted.shape:\n        raise ValueError('`actual` and `predicted` must have the same shape.')\n    clipped_pred = np.clip(predicted, EPS, 1.0 - EPS)\n    loss = -np.mean(actual * np.log(clipped_pred) + (1.0 - actual) * np.log(1.0 - clipped_pred))\n    return float(round(loss, 4))"}
{"task_id": 324, "completion_id": 0, "solution": "import numpy as np\ndef bilinear_interpolate(X: np.ndarray, x: list[float], y: list[float]) -> list[list[float]]:\n    \"\"\"Estimate pixel values at arbitrary (x, y) positions using bilinear interpolation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Image array of shape (rows, cols, channels).\n    x : list[float]\n        Horizontal (column) coordinates.\n    y : list[float]\n        Vertical (row) coordinates.\n\n    Returns\n    -------\n    list[list[float]]\n        Interpolated values where each inner list corresponds to one pixel and\n        contains *channels* numbers rounded to 4 decimal places.  If the input\n        lists `x` and `y` differ in length the function must return an empty\n        list.\n    \"\"\"\n    if len(x) != len(y):\n        return []\n    if len(x) == 0:\n        return []\n    if X.ndim == 2:\n        X = X[:, :, None]\n    (rows, cols, ch) = X.shape\n    Xf = X.astype(float)\n    xs = np.asarray(x, dtype=float)\n    ys = np.asarray(y, dtype=float)\n    xs = np.clip(xs, 0, cols - 1)\n    ys = np.clip(ys, 0, rows - 1)\n    if cols > 1:\n        x0 = np.floor(xs).astype(int)\n        x0 = np.clip(x0, 0, cols - 2)\n        x1 = x0 + 1\n        dx = xs - x0\n    else:\n        x0 = x1 = np.zeros_like(xs, dtype=int)\n        dx = np.zeros_like(xs)\n    if rows > 1:\n        y0 = np.floor(ys).astype(int)\n        y0 = np.clip(y0, 0, rows - 2)\n        y1 = y0 + 1\n        dy = ys - y0\n    else:\n        y0 = y1 = np.zeros_like(ys, dtype=int)\n        dy = np.zeros_like(ys)\n    Ia = Xf[y0, x0]\n    Ib = Xf[y0, x1]\n    Ic = Xf[y1, x0]\n    Id = Xf[y1, x1]\n    wa = (1 - dx) * (1 - dy)\n    wb = dx * (1 - dy)\n    wc = (1 - dx) * dy\n    wd = dx * dy\n    wa = wa[:, None]\n    wb = wb[:, None]\n    wc = wc[:, None]\n    wd = wd[:, None]\n    out = Ia * wa + Ib * wb + Ic * wc + Id * wd\n    return np.round(out, 4).tolist()"}
{"task_id": 326, "completion_id": 0, "solution": "import numpy as np\ndef svm_predict(X: list[list[float]], support_vectors: list[list[float]], lagr_multipliers: list[float], support_vector_labels: list[int], intercept: float, kernel: str='rbf', power: int=3, gamma: float | None=None, coef: float=1.0) -> list[int]:\n    \"\"\"Predict labels for a batch of samples using a pre-trained SVM.\"\"\"\n    X = np.asarray(X, dtype=float)\n    SV = np.asarray(support_vectors, dtype=float)\n    alpha = np.asarray(lagr_multipliers, dtype=float)\n    y_sv = np.asarray(support_vector_labels, dtype=float)\n    if X.ndim != 2 or SV.ndim != 2:\n        raise ValueError('X and support_vectors must be 2-D (samples \u00d7 features).')\n    if SV.shape[0] != alpha.size or alpha.size != y_sv.size:\n        raise ValueError('Lengths of support_vectors, lagr_multipliers and support_vector_labels must match.')\n    (n_query, n_features) = X.shape\n    if gamma is None:\n        gamma = 1.0 / n_features\n    kernel = kernel.lower()\n    if kernel == 'linear':\n        K = X @ SV.T\n    elif kernel == 'polynomial':\n        K = (gamma * (X @ SV.T) + coef) ** power\n    elif kernel == 'rbf':\n        X_sq = np.sum(X ** 2, axis=1, keepdims=True)\n        SV_sq = np.sum(SV ** 2, axis=1)\n        sq_dists = X_sq + SV_sq - 2.0 * (X @ SV.T)\n        K = np.exp(-gamma * sq_dists)\n    else:\n        raise ValueError(\"Unknown kernel '{}'. Choose from 'linear', 'polynomial', 'rbf'.\".format(kernel))\n    alpha_y = alpha * y_sv\n    decision_values = K @ alpha_y + float(intercept)\n    labels = np.where(decision_values >= 0.0, 1, -1)\n    return labels.astype(int).tolist()"}
{"task_id": 328, "completion_id": 0, "solution": "from typing import List\ndef first_capitalized_word(corpus: List[str]) -> List[str]:\n    \"\"\"Return a one-element list with the first word whose first character is\n    uppercase, or an empty list if no such word exists.\n    \"\"\"\n    for word in corpus:\n        if word and word[0].isupper():\n            return [word]\n    return []"}
{"task_id": 329, "completion_id": 0, "solution": "import numpy as np\ndef dft_bins(N: int, fs: int=44000, *, positive_only: bool=True) -> list[float]:\n    \"\"\"Return the centre frequency (Hz) of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        If True, return only the non-negative frequencies (indices\n        0 \u2026 \u230aN/2\u230b).  If False, return all N frequency bins in the same\n        order as `numpy.fft.fftfreq`.  Default is True.\n\n    Returns\n    -------\n    list[float]\n        Frequencies rounded to 4 decimal places.  Empty list if `N`\n        or `fs` is not strictly positive.\n    \"\"\"\n    if N <= 0 or fs <= 0:\n        return []\n    if positive_only:\n        freqs = np.arange(0, N // 2 + 1) * (fs / N)\n    else:\n        freqs = np.fft.fftfreq(N, d=1 / fs)\n    return np.round(freqs, 4).tolist()"}
{"task_id": 330, "completion_id": 0, "solution": "def update_kernel_settings(parameters: dict, hyperparameters: dict, summary_dict: dict) -> tuple[dict, dict]:\n    \"\"\"Return *new* dicts where values found in ``summary_dict`` have been\n    merged into ``parameters`` and/or ``hyperparameters`` without mutating\n    the originals.\n\n    See task description for the exact semantics.\n    \"\"\"\n    new_parameters = parameters.copy()\n    new_hyperparameters = hyperparameters.copy()\n    updates = {}\n    for (key, value) in summary_dict.items():\n        if key == 'parameters' and isinstance(value, dict):\n            updates.update(value)\n        elif key == 'hyperparameters' and isinstance(value, dict):\n            updates.update(value)\n        else:\n            updates[key] = value\n    for (key, value) in updates.items():\n        if key in new_parameters:\n            new_parameters[key] = value\n        elif key in new_hyperparameters:\n            new_hyperparameters[key] = value\n    return (new_parameters, new_hyperparameters)"}
{"task_id": 331, "completion_id": 0, "solution": "import numpy as np\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix whose element (i, j) is True/1 iff sample i\n        was out of bag for tree j, same shape as *predictions*.\n\n    Returns\n    -------\n    float\n        OOB mean squared error rounded to 4 decimal places, or -1 if it cannot\n        be computed.\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=float).ravel()\n    preds = np.asarray(predictions, dtype=float)\n    mask = np.asarray(oob_mask).astype(bool)\n    if preds.shape != mask.shape:\n        raise ValueError('`predictions` and `oob_mask` must have the same shape.')\n    oob_counts = mask.sum(axis=1)\n    valid_idx = oob_counts > 0\n    if not np.any(valid_idx):\n        return -1\n    oob_sums = (preds * mask).sum(axis=1)\n    oob_means = np.zeros_like(y_true)\n    oob_means[valid_idx] = oob_sums[valid_idx] / oob_counts[valid_idx]\n    sq_err = (oob_means[valid_idx] - y_true[valid_idx]) ** 2\n    mse = sq_err.mean()\n    return round(float(mse), 4)"}
{"task_id": 332, "completion_id": 0, "solution": "from collections import Counter\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree represented with plain nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of *transactions* an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node is a mapping:\n        {\"support\": <int>, \"children\": {item -> child-node}}\n    \"\"\"\n    item_counter = Counter()\n    for trx in transactions:\n        for item in set(trx):\n            item_counter[item] += 1\n    freq_items = {item: sup for (item, sup) in item_counter.items() if sup >= min_support}\n    if not freq_items:\n        return {'support': 0, 'children': {}}\n    ordered = sorted(freq_items.items(), key=lambda pair: (-pair[1], pair[0]))\n    order_rank = {item: rank for (rank, (item, _)) in enumerate(ordered)}\n    frequent_set = set(freq_items)\n    root = {'support': 0, 'children': {}}\n    for trx in transactions:\n        filtered = [item for item in set(trx) if item in frequent_set]\n        if not filtered:\n            continue\n        filtered.sort(key=lambda it: order_rank[it])\n        node = root\n        for item in filtered:\n            children = node['children']\n            if item not in children:\n                children[item] = {'support': 0, 'children': {}}\n            node = children[item]\n            node['support'] += 1\n    return root"}
{"task_id": 333, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef logistic_regression(X: np.ndarray, y: np.ndarray, alpha: float=0.1, num_iter: int=10000) -> list[int]:\n    \"\"\"Train a logistic-regression classifier with batch gradient descent and\n    return the predicted labels for the *training* data.\n\n    Parameters\n    ----------\n    X : np.ndarray (m, n)\n        Feature matrix.\n    y : np.ndarray (m,)\n        Binary targets (0 / 1).\n    alpha : float, default=0.1\n        Learning-rate.\n    num_iter : int, default=10_000\n        Number of GD iterations.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for the input samples.\n    \"\"\"\n    eps = 1e-08\n    mu = X.mean(axis=0)\n    sigma = X.std(axis=0) + eps\n    Xn = (X - mu) / sigma\n    (m, n) = Xn.shape\n    X_aug = np.hstack((np.ones((m, 1)), Xn))\n    theta = np.zeros(n + 1)\n    for _ in range(num_iter):\n        z = X_aug @ theta\n        h = 1 / (1 + np.exp(-z))\n        grad = X_aug.T @ (h - y) / m\n        theta -= alpha * grad\n    probs = 1 / (1 + np.exp(-X_aug @ theta))\n    preds = (probs >= 0.5).astype(int)\n    return preds.tolist()"}
{"task_id": 334, "completion_id": 0, "solution": "import math\ndef noam_learning_rate(step: int, model_dim: int=512, scale_factor: float=1.0, warmup_steps: int=4000) -> float:\n    \"\"\"Return the learning-rate given by the Noam schedule.\n\n    lr = scale_factor * model_dim^{-0.5} *\n         min(step^{-0.5}, step * warmup_steps^{-1.5})\n\n    The result is rounded to 9 decimal places.\n    \"\"\"\n    if step < 1:\n        return 0.0\n    model_term = model_dim ** (-0.5)\n    warmup_term = warmup_steps ** (-1.5)\n    decay_part = step ** (-0.5)\n    warmup_part = step * warmup_term\n    lr = scale_factor * model_term * min(decay_part, warmup_part)\n    return round(lr, 9)"}
{"task_id": 335, "completion_id": 0, "solution": "import numpy as np\ndef get_initializer(name: str):\n    \"\"\"Return a weight-initialisation callable associated with *name*.\n\n    The callable has the signature\n        initializer(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray\n\n    Supported strategies (case-insensitive)\n        - \"zeros\"            \u2013 array of zeros          (scale ignored)\n        - \"ones\"             \u2013 ones  * scale\n        - \"normal\"           \u2013 N(0, scale)\n        - \"glorot_uniform\"   \u2013 U[\u2212limit, +limit] with\n                               limit = scale * sqrt(6 / (fan_in + fan_out))\n\n    Any other *name* raises ValueError.\n    \"\"\"\n    name_lc = name.lower()\n    if name_lc == 'zeros':\n\n        def initializer(shape: tuple[int, ...], scale: float=1.0) -> np.ndarray:\n            return np.zeros(shape, dtype=float)\n        return initializer\n    if name_lc == 'ones':\n\n        def initializer(shape: tuple[int, ...], scale: float=1.0) -> np.ndarray:\n            return np.ones(shape, dtype=float) * scale\n        return initializer\n    if name_lc == 'normal':\n\n        def initializer(shape: tuple[int, ...], scale: float=1.0) -> np.ndarray:\n            return np.random.normal(loc=0.0, scale=scale, size=shape)\n        return initializer\n    if name_lc == 'glorot_uniform':\n\n        def initializer(shape: tuple[int, ...], scale: float=1.0) -> np.ndarray:\n            if len(shape) < 2:\n                raise ValueError('glorot_uniform requires at least 2-D shape')\n            (fan_in, fan_out) = (shape[0], shape[1])\n            limit = scale * np.sqrt(6.0 / (fan_in + fan_out))\n            return np.random.uniform(low=-limit, high=limit, size=shape)\n        return initializer\n    raise ValueError(f'Unknown initialisation strategy: {name}')"}
{"task_id": 336, "completion_id": 0, "solution": "import numpy as np\ndef linear_kernel(X: list[list[int | float]] | 'np.ndarray', Y: list[list[int | float]] | 'np.ndarray' | None=None, c0: int | float=0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n    Y : 2-D list or NumPy array with shape (M, C) or None.  If None, Y = X.\n    c0 : additive constant of the kernel (default 0).\n\n    Returns\n    -------\n    list[list[float]] | int\n        Kernel matrix rounded to 4 decimals, or \u20111 for incompatible shapes.\n    \"\"\"\n\n    def _to_2d_float_array(obj):\n        arr = np.asarray(obj, dtype=float)\n        if arr.ndim == 0:\n            arr = arr.reshape(1, 1)\n        elif arr.ndim == 1:\n            arr = arr.reshape(1, -1)\n        return arr\n    if isinstance(X, (list, tuple)) and len(X) == 0:\n        return []\n    X_arr = _to_2d_float_array(X)\n    if Y is None:\n        Y_arr = X_arr\n    elif isinstance(Y, (list, tuple)) and len(Y) == 0:\n        Y_arr = _to_2d_float_array(Y)\n    else:\n        Y_arr = _to_2d_float_array(Y)\n    if X_arr.shape[1] != Y_arr.shape[1]:\n        return -1\n    K = X_arr @ Y_arr.T\n    if c0 != 0:\n        K = K + c0\n    return np.round(K.astype(float), 4).tolist()"}
{"task_id": 337, "completion_id": 0, "solution": "import numpy as np\ndef nn_interpolate_2D(X: 'np.ndarray', x: list[float | int], y: list[float | int]) -> list:\n    \"\"\"Nearest-Neighbour interpolation on a 2-D regular grid.\n\n    Args:\n        X: Image of shape (rows, cols, channels).\n        x: List of x (column) coordinates.\n        y: List of y (row) coordinates.\n\n    Returns:\n        Python list with shape (k, channels) containing the interpolated\n        pixel values, where k == len(x) == len(y).\n    \"\"\"\n    if not x and (not y):\n        return []\n    if len(x) != len(y):\n        raise ValueError('x and y must have the same length')\n    (rows, cols, _) = X.shape\n    x_arr = np.asarray(x, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    x_idx = np.around(x_arr).astype(int)\n    y_idx = np.around(y_arr).astype(int)\n    x_idx = np.clip(x_idx, 0, cols - 1)\n    y_idx = np.clip(y_idx, 0, rows - 1)\n    out = X[y_idx, x_idx]\n    return out.tolist()"}
{"task_id": 338, "completion_id": 0, "solution": "import numpy as np\ndef ridge_regression(X, y, reg_factor):\n    \"\"\"\n    Closed-form Ridge regression.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Design/feature matrix.\n    y : array-like of shape (n_samples,)\n        Target vector.\n    reg_factor : float, >= 0\n        L2 regularisation strength (\u03b1).\n\n    Returns\n    -------\n    list\n        Weight vector [bias, w1, \u2026, wd] rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    n_samples = X.shape[0]\n    X_bar = np.hstack((np.ones((n_samples, 1)), X))\n    d_plus_1 = X_bar.shape[1]\n    A = X_bar.T @ X_bar + reg_factor * np.eye(d_plus_1)\n    b = X_bar.T @ y\n    w = np.linalg.solve(A, b)\n    return np.round(w, 4).tolist()"}
{"task_id": 339, "completion_id": 0, "solution": "from collections import Counter\ndef most_common_label(y):\n    \"\"\"Return the most frequent label in *y*.\n\n    If *y* is empty, the function returns ``None``.\n    In case of a tie, the label whose first appearance comes earliest\n    in *y* is returned.\n    \"\"\"\n    if not y:\n        return None\n    counts = Counter(y)\n    max_freq = max(counts.values())\n    for label in y:\n        if counts[label] == max_freq:\n            return label"}
{"task_id": 340, "completion_id": 0, "solution": "import numpy as np\ndef adam_update(param: np.ndarray, grad: np.ndarray, t: int, m: np.ndarray, v: np.ndarray, lr: float=0.001, beta1: float=0.9, beta2: float=0.999, eps: float=1e-08) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1.\n        beta1: Exponential decay rate for the first moment.\n        beta2: Exponential decay rate for the second moment.\n        eps:   Small constant to avoid division by zero.\n\n    Returns:\n        Tuple of python lists rounded to 8 decimal places:\n        (updated_parameter, new_moment, new_variance)\n    \"\"\"\n    param = np.asarray(param, dtype=np.float64)\n    grad = np.asarray(grad, dtype=np.float64)\n    m = np.asarray(m, dtype=np.float64)\n    v = np.asarray(v, dtype=np.float64)\n    m_t = beta1 * m + (1.0 - beta1) * grad\n    v_t = beta2 * v + (1.0 - beta2) * grad ** 2\n    m_hat = m_t / (1.0 - beta1 ** t)\n    v_hat = v_t / (1.0 - beta2 ** t)\n    if np.all(grad == 0):\n        param_t = param.copy()\n    else:\n        param_t = param - lr * m_hat / (np.sqrt(v_hat) + eps)\n\n    def to_rounded_list(arr: np.ndarray) -> list:\n        return np.round(arr, 8).tolist()\n    return (to_rounded_list(param_t), to_rounded_list(m_t), to_rounded_list(v_t))"}
{"task_id": 342, "completion_id": 0, "solution": "import numpy as np\ndef manhattan(x: list[float | int], y: list[float | int]) -> float:\n    \"\"\"Compute the Manhattan (L1) distance between two real vectors.\n\n    Args:\n        x: List of numbers representing the first vector.\n        y: List of numbers representing the second vector.\n\n    Returns:\n        The L1 distance rounded to 4 decimal places as a float. If the\n        vectors have different lengths, returns -1.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    dist = np.abs(np.asarray(x, dtype=float) - np.asarray(y, dtype=float)).sum()\n    return float(round(dist, 4))"}
{"task_id": 343, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_update(theta_samples: np.ndarray, rewards: list[float] | np.ndarray, retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists with the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    rewards = np.asarray(rewards)\n    n_samples = theta_samples.shape[0]\n    k = max(1, int(retain_prcnt * n_samples))\n    elite_idx = np.argsort(rewards)[::-1][:k]\n    elite_samples = theta_samples[elite_idx]\n    mean = np.mean(elite_samples, axis=0)\n    var = np.var(elite_samples, axis=0)\n    mean_rounded = np.round(mean, 4).tolist()\n    var_rounded = np.round(var, 4).tolist()\n    return (mean_rounded, var_rounded)"}
{"task_id": 344, "completion_id": 0, "solution": "import heapq\nimport itertools\nfrom collections import Counter\nfrom typing import Dict, List\nclass _Node:\n    \"\"\"Internal helper node for the Huffman tree.\"\"\"\n    __slots__ = ('freq', 'token', 'left', 'right')\n\n    def __init__(self, freq: int, token: str='', left: '_Node | None'=None, right: '_Node | None'=None) -> None:\n        self.freq = freq\n        self.token = token\n        self.left = left\n        self.right = right\ndef huffman_encode(tokens: List[str]) -> Dict[str, str]:\n    \"\"\"Generate a Huffman code for the supplied tokens.\n\n    The function returns a dictionary mapping every unique token in *tokens*\n    to its binary, prefix-free Huffman code.\n    \"\"\"\n    if not tokens:\n        return {}\n    counter = Counter(tokens)\n    if len(counter) == 1:\n        return {next(iter(counter)): '0'}\n    heap: list[tuple[int, str, int, _Node]] = []\n    seq = itertools.count()\n    for (tok, freq) in counter.items():\n        node = _Node(freq=freq, token=tok)\n        heapq.heappush(heap, (freq, tok, next(seq), node))\n    while len(heap) > 1:\n        (f1, _, _, n1) = heapq.heappop(heap)\n        (f2, _, _, n2) = heapq.heappop(heap)\n        parent = _Node(freq=f1 + f2, token='', left=n1, right=n2)\n        heapq.heappush(heap, (parent.freq, '', next(seq), parent))\n    root = heap[0][3]\n    codes: Dict[str, str] = {}\n\n    def _walk(node: _Node, prefix: str) -> None:\n        if node.left is None and node.right is None:\n            codes[node.token] = prefix or '0'\n            return\n        _walk(node.left, prefix + '0')\n        _walk(node.right, prefix + '1')\n    _walk(root, '')\n    return codes"}
{"task_id": 345, "completion_id": 0, "solution": "import numpy as np\ndef bayesian_linear_regression_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, mu0: np.ndarray, omega0: np.ndarray) -> list[float]:\n    \"\"\"Predict targets for X_test using Bayesian linear regression.\n\n    The function adds an intercept, computes the posterior mean of the\n    weight vector with a Normal prior (\u03bc\u2080, \u03a9\u2080), and returns the predictions\n    rounded to four decimals.\n    \"\"\"\n    ones_train = np.ones((X_train.shape[0], 1))\n    ones_test = np.ones((X_test.shape[0], 1))\n    X_train_b = np.hstack((ones_train, X_train))\n    X_test_b = np.hstack((ones_test, X_test))\n    XtX = X_train_b.T @ X_train_b\n    posterior_prec = XtX + omega0\n    rhs = X_train_b.T @ y_train.ravel() + omega0 @ mu0\n    mu_n = np.linalg.solve(posterior_prec, rhs)\n    y_pred = X_test_b @ mu_n\n    return np.round(y_pred, 4).tolist()"}
{"task_id": 346, "completion_id": 0, "solution": "import numpy as np\ndef entropy(y):\n    \"\"\"Compute the base-2 Shannon entropy of a vector of integer labels.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        One-dimensional iterable containing non-negative integer class labels.\n\n    Returns\n    -------\n    float\n        The entropy value rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y)\n    n_samples = y.size\n    if n_samples == 0:\n        return 0.0\n    (_, counts) = np.unique(y, return_counts=True)\n    p = counts / n_samples\n    entropy_val = -np.sum(p * np.log2(p))\n    return round(float(entropy_val), 4)"}
{"task_id": 347, "completion_id": 0, "solution": "import numpy as np\ndef wgan_gp_loss(Y_fake: np.ndarray, module: str, Y_real: np.ndarray | None=None, gradInterp: np.ndarray | None=None, lambda_: float=10.0) -> float:\n    \"\"\"Compute the WGAN-GP loss for the critic or the generator.\n\n    Args:\n        Y_fake (np.ndarray): Critic outputs on fake samples (shape: (n_ex, \u2026)).\n        module (str): \"C\" for critic loss, \"G\" for generator loss.\n        Y_real (np.ndarray | None, optional): Critic outputs on real samples.\n            Required when *module* == \"C\".\n        gradInterp (np.ndarray | None, optional): Gradients of the critic\n            outputs on the interpolated samples w.r.t. those samples.\n            Required when *module* == \"C\".\n        lambda_ (float, optional): Gradient-penalty coefficient \u03bb.\n\n    Returns:\n        float: Requested loss rounded to four decimals.\n    \"\"\"\n    module = module.upper()\n    if module not in {'C', 'G'}:\n        raise ValueError('module must be either \"C\" or \"G\".')\n    Y_fake = np.asarray(Y_fake, dtype=np.float32)\n    if module == 'G':\n        loss = -np.mean(Y_fake)\n        return round(float(loss), 4)\n    if Y_real is None or gradInterp is None:\n        raise ValueError('Y_real and gradInterp must be provided for critic loss.')\n    Y_real = np.asarray(Y_real, dtype=np.float32)\n    gradInterp = np.asarray(gradInterp, dtype=np.float32)\n    mean_fake = np.mean(Y_fake)\n    mean_real = np.mean(Y_real)\n    if gradInterp.ndim == 1:\n        grad_norm = np.abs(gradInterp)\n    else:\n        axes = tuple(range(1, gradInterp.ndim))\n        grad_norm = np.sqrt(np.sum(gradInterp ** 2, axis=axes))\n    gp = np.mean((grad_norm - 1.0) ** 2)\n    loss = mean_fake - mean_real + lambda_ * gp\n    return round(float(loss), 4)"}
{"task_id": 348, "completion_id": 0, "solution": "import numpy as np\ndef is_symmetric_positive_definite(X: np.ndarray) -> bool:\n    \"\"\"Check whether a real matrix is symmetric and positive-definite.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        A two-dimensional NumPy array representing the matrix to test.\n\n    Returns\n    -------\n    bool\n        True if *X* is symmetric and positive-definite, otherwise False.\n    \"\"\"\n    if not isinstance(X, np.ndarray) or X.ndim != 2:\n        return False\n    (n_rows, n_cols) = X.shape\n    if n_rows != n_cols:\n        return False\n    if not np.isrealobj(X):\n        return False\n    if not np.allclose(X, X.T, atol=1e-08, rtol=1e-08):\n        return False\n    try:\n        eigvals = np.linalg.eigvalsh(X)\n    except np.linalg.LinAlgError:\n        return False\n    return np.all(eigvals > 1e-08)"}
{"task_id": 350, "completion_id": 0, "solution": "import numpy as np\ndef polynomial_ridge_regression(X: list[float | int], y: list[float | int], degree: int, reg_factor: float) -> list[float]:\n    \"\"\"Fit a polynomial ridge regression model and return its coefficients.\n\n    Parameters\n    ----------\n    X : list of float or int\n        One-dimensional feature vector.\n    y : list of float or int\n        Target values.\n    degree : int\n        Highest polynomial degree (>=0).\n    reg_factor : float\n        Non\u2013negative ridge regularisation factor (\u03bb).\n\n    Returns\n    -------\n    list[float]\n        The (degree+1) learned coefficients, rounded to 4 decimal places.\n    \"\"\"\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    Phi = np.vander(X_arr, N=degree + 1, increasing=True)\n    XtX = Phi.T @ Phi\n    reg_matrix = reg_factor * np.eye(degree + 1)\n    Xt_y = Phi.T @ y_arr\n    w = np.linalg.solve(XtX + reg_matrix, Xt_y)\n    return [round(coef, 4) for coef in w]"}
{"task_id": 351, "completion_id": 0, "solution": "import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\ndef max_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: int) -> list:\n    \"\"\"Perform 2-D max-pooling over a 4-D minibatch of images.\n\n    Args:\n        X: A 4-D NumPy array with shape (N, C, H, W).\n        pool_shape: (p_h, p_w) \u2013 height and width of the pooling window.\n        stride: The number of pixels the window moves vertically and horizontally.\n\n    Returns:\n        Nested Python list with shape (N, C, out_h, out_w) obtained via\n        ``numpy.ndarray.tolist()``.\n    \"\"\"\n    if X.ndim != 4:\n        raise ValueError('Input must be a 4-D tensor of shape (N, C, H, W).')\n    (p_h, p_w) = pool_shape\n    (N, C, H, W) = X.shape\n    out_h = (H - p_h) // stride + 1\n    out_w = (W - p_w) // stride + 1\n    shape_windows = (N, C, out_h, out_w, p_h, p_w)\n    (sN, sC, sH, sW) = X.strides\n    stride_windows = (sN, sC, sH * stride, sW * stride, sH, sW)\n    windows = as_strided(X, shape=shape_windows, strides=stride_windows, writeable=False)\n    pooled = windows.max(axis=(4, 5))\n    return pooled.tolist()"}
{"task_id": 353, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nimport numpy as np\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n                class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    label_list = list(labels)\n    if not label_list:\n        return 0.0\n    total = len(label_list)\n    counts = Counter(label_list)\n    if len(counts) == 1:\n        return 0.0\n    entropy_val = 0.0\n    for count in counts.values():\n        p = count / total\n        if p > 0.0:\n            entropy_val -= p * math.log2(p)\n    return round(entropy_val, 5)"}
{"task_id": 354, "completion_id": 0, "solution": "import numpy as np\ndef _parse_padding(pad, k_r, k_c, dil, stride, in_r, in_c):\n    \"\"\"Returns the 4 individual paddings (top, bottom, left, right).\"\"\"\n    eff_k_r = k_r + (k_r - 1) * dil\n    eff_k_c = k_c + (k_c - 1) * dil\n    if isinstance(pad, int):\n        return (pad, pad, pad, pad)\n    if isinstance(pad, tuple) and len(pad) == 2:\n        (pr, pc) = pad\n        return (pr, pr, pc, pc)\n    if isinstance(pad, tuple) and len(pad) == 4:\n        return pad\n    if pad == 'same':\n        total_pad_r = max(eff_k_r - 1, 0)\n        total_pad_c = max(eff_k_c - 1, 0)\n        pr1 = total_pad_r // 2\n        pr2 = total_pad_r - pr1\n        pc1 = total_pad_c // 2\n        pc2 = total_pad_c - pc1\n        return (pr1, pr2, pc1, pc2)\n    return (0, 0, 0, 0)\ndef conv2D(X: 'np.ndarray', W: 'np.ndarray', stride: int, pad, dilation: int=0):\n    \"\"\"Performs a 2-D convolution (cross-correlation) via im2col + GEMM.\"\"\"\n    (n_ex, in_r, in_c, in_ch) = X.shape\n    (k_r, k_c, in_ch_w, out_ch) = W.shape\n    assert in_ch == in_ch_w, 'Input / kernel channels mismatch'\n    dil = dilation\n    eff_k_r = k_r + (k_r - 1) * dil\n    eff_k_c = k_c + (k_c - 1) * dil\n    (pr1, pr2, pc1, pc2) = _parse_padding(pad, k_r, k_c, dil, stride, in_r, in_c)\n    X_pad = np.pad(X, pad_width=((0, 0), (pr1, pr2), (pc1, pc2), (0, 0)), mode='constant')\n    (pad_r, pad_c) = (in_r + pr1 + pr2, in_c + pc1 + pc2)\n    out_r = (pad_r - eff_k_r) // stride + 1\n    out_c = (pad_c - eff_k_c) // stride + 1\n    cols = np.empty((n_ex, out_r, out_c, k_r, k_c, in_ch), dtype=X.dtype)\n    for kr in range(k_r):\n        row_start = kr * (dil + 1)\n        row_end = row_start + stride * out_r\n        row_slice = slice(row_start, row_end, stride)\n        for kc in range(k_c):\n            col_start = kc * (dil + 1)\n            col_end = col_start + stride * out_c\n            col_slice = slice(col_start, col_end, stride)\n            cols[:, :, :, kr, kc, :] = X_pad[:, row_slice, col_slice, :]\n    cols_2d = cols.reshape(n_ex * out_r * out_c, -1)\n    W_col = W.reshape(-1, out_ch)\n    out_mat = cols_2d @ W_col\n    out = out_mat.reshape(n_ex, out_r, out_c, out_ch)\n    return out.tolist()"}
{"task_id": 355, "completion_id": 0, "solution": "import numpy as np\ndef _deterministic_sign_flip(components: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Flip the sign of each component so that the first non\u2013zero loading is \u2265 0.\n    The operation is done in-place and the resulting array is also returned.\n    \"\"\"\n    for i in range(components.shape[1]):\n        col = components[:, i]\n        for v in col:\n            if abs(v) > 1e-12:\n                if v < 0:\n                    components[:, i] *= -1\n                break\n    return components\ndef pca_transform(data: np.ndarray, n_components: int, solver: str='svd') -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis with a deterministic sign convention.\"\"\"\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array (samples \u00d7 features).')\n    if n_components < 1:\n        raise ValueError('`n_components` must be at least 1.')\n    if solver not in {'svd', 'eigen'}:\n        raise ValueError(\"`solver` must be either 'svd' or 'eigen'.\")\n    (n_samples, n_features) = data.shape\n    n_components = min(n_components, n_features)\n    X = data.astype(float) - data.mean(axis=0, keepdims=True)\n    if solver == 'svd':\n        (_, singular_vals, Vt) = np.linalg.svd(X, full_matrices=False)\n        components = Vt.T[:, :n_components]\n        expl_var = singular_vals ** 2 / (n_samples - 1)\n    else:\n        cov = np.cov(X, rowvar=False, ddof=1)\n        (eig_vals, eig_vecs) = np.linalg.eigh(cov)\n        idx = eig_vals.argsort()[::-1]\n        (eig_vals, eig_vecs) = (eig_vals[idx], eig_vecs[:, idx])\n        components = eig_vecs[:, :n_components]\n        expl_var = eig_vals\n    total_variance = expl_var.sum()\n    explained_variance_ratio = (expl_var / total_variance)[:n_components]\n    components = _deterministic_sign_flip(components)\n    scores = X @ components\n    scores_rounded = np.round(scores, 4).tolist()\n    evr_rounded = np.round(explained_variance_ratio, 4).tolist()\n    return (scores_rounded, evr_rounded)"}
{"task_id": 356, "completion_id": 0, "solution": "def leaf_predict(leaf: 'Leaf', classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                    classification tree; otherwise treat it as regression.\n\n    Returns\n    -------\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        probs = list(leaf.value)\n        best_idx = 0\n        best_val = probs[0]\n        for (idx, val) in enumerate(probs[1:], start=1):\n            if val > best_val:\n                best_val = val\n                best_idx = idx\n        return best_idx\n    return leaf.value"}
{"task_id": 357, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data: np.ndarray, K: int, max_iters: int=100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array of shape (n_samples, n_features).\n    K : int\n        Number of clusters.\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Sorted centroids, rounded to four decimals.\n    \"\"\"\n    data = np.asarray(data, dtype=float)\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array')\n    (n_samples, n_features) = data.shape\n    if K < 1 or K > n_samples:\n        raise ValueError('K must be between 1 and the number of samples')\n    if K == 1:\n        centroid = np.mean(data, axis=0, keepdims=True)\n        centroid = np.round(centroid, 4)\n        return centroid.tolist()\n    centroids = data[:K].astype(float, copy=True)\n    tol = 1e-06\n    for _ in range(max_iters):\n        diff = data[:, None, :] - centroids[None, :, :]\n        dists = np.sum(diff ** 2, axis=2)\n        labels = np.argmin(dists, axis=1)\n        new_centroids = centroids.copy()\n        for k in range(K):\n            members = data[labels == k]\n            if members.size:\n                new_centroids[k] = members.mean(axis=0)\n        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) <= tol):\n            centroids = new_centroids\n            break\n        centroids = new_centroids\n    sort_keys = tuple((centroids[:, i] for i in reversed(range(n_features))))\n    order = np.lexsort(sort_keys)\n    centroids_sorted = np.round(centroids[order], 4)\n    return centroids_sorted.tolist()"}
{"task_id": 358, "completion_id": 0, "solution": "import numpy as np\ndef accuracy(y_true, y_pred):\n    \"\"\"Compute the classification accuracy between two equal-length sequences.\n\n    Args:\n        y_true (list | np.ndarray): Ground-truth class labels.\n        y_pred (list | np.ndarray): Predicted class labels.\n\n    Returns:\n        float | int: Accuracy rounded to 4 decimals, or -1 on length mismatch.\n    \"\"\"\n    if len(y_true) != len(y_pred) or len(y_true) == 0:\n        return -1\n    y_true_arr = np.asarray(y_true)\n    y_pred_arr = np.asarray(y_pred)\n    correct = np.sum(y_true_arr == y_pred_arr)\n    acc = correct / len(y_true_arr)\n    return round(float(acc), 4)"}
{"task_id": 359, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy(y, y_pred, derivative=False):\n    \"\"\"Cross-entropy utility.\n\n    Parameters\n    ----------\n    y : numpy.ndarray\n        One\u2013hot encoded true labels, shape (n_samples, n_classes).\n    y_pred : numpy.ndarray\n        Predicted class probabilities, same shape as *y*.\n    derivative : bool, default False\n        If *True* return the gradient, else return the loss.\n\n    Returns\n    -------\n    float | list[list[float]]\n        Rounded loss (scalar) or gradient (nested list).\n    \"\"\"\n    y = np.asarray(y, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    eps = np.finfo(float).eps\n    if derivative:\n        grad = y_pred - y\n        return np.round(grad, 4).tolist()\n    else:\n        loss = -np.sum(y * np.log(y_pred + eps))\n        return round(loss, 4)"}
{"task_id": 360, "completion_id": 0, "solution": "import numpy as np\ndef dct(frame: np.ndarray | list[float], orthonormal: bool=True) -> list[float]:\n    \"\"\"Compute the 1-D DCT-II of *frame* (na\u00efve implementation).\n\n    Parameters\n    ----------\n    frame : 1-D array-like\n        Real-valued input signal (length N).\n    orthonormal : bool, default=True\n        If True, apply orthonormal scaling (so the transform matrix is unitary).\n\n    Returns\n    -------\n    list[float]\n        DCT-II coefficients rounded to four decimal places.\n    \"\"\"\n    x = np.asarray(frame, dtype=float).ravel()\n    if x.ndim != 1:\n        raise ValueError('Input *frame* must be one-dimensional.')\n    N = x.size\n    if N == 0:\n        return []\n    k = np.arange(N).reshape(-1, 1)\n    n = np.arange(N).reshape(1, -1)\n    cos_kn = np.cos(np.pi * k * (2 * n + 1) / (2 * N))\n    coeffs = 2.0 * cos_kn @ x\n    if orthonormal:\n        coeffs[0] *= 1 / np.sqrt(N)\n        if N > 1:\n            coeffs[1:] *= np.sqrt(2 / N)\n    return np.round(coeffs, 4).tolist()"}
{"task_id": 362, "completion_id": 0, "solution": "import numpy as np\ndef row_stochastic_matrix(data: list[list[int | float]] | 'np.ndarray') -> list[list[float]] | int:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row is divided by its own sum so that every row finally sums to 1.\n    If any row has a sum of exactly 0 normalisation is impossible and \u20131\n    is returned.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The row-normalised matrix (rounded to 4 decimals) or \u20131 when a\n        zero-sum row is encountered.\n    \"\"\"\n    arr = np.asarray(data, dtype=float)\n    if arr.ndim != 2:\n        raise ValueError('Input must be a 2-D structure')\n    row_sums = arr.sum(axis=1)\n    if np.any(row_sums == 0):\n        return -1\n    stochastic = arr / row_sums[:, None]\n    return np.round(stochastic, 4).tolist()"}
{"task_id": 363, "completion_id": 0, "solution": "from typing import Any, Dict, Iterable, Tuple as PyTuple\nclass Space:\n    pass\nclass Box(Space):\n\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\nclass Tuple(Space):\n\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\nclass Dict(Space):\n\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite\n    (`Tuple` or `Dict`) spaces the space is continuous only if **all** its\n    immediate sub-spaces are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: True if the action space is known to be composite.\n        tuple_obs:    True if the observation space is known to be composite.\n\n    Returns:\n        A tuple (cont_action, cont_obs) with the continuity result for the\n        action and observation spaces, respectively.\n    \"\"\"\n\n    def is_simple_box(space: Space) -> bool:\n        \"\"\"Return True only when `space` itself is a `Box`.\"\"\"\n        return isinstance(space, Box)\n\n    def all_subspaces_box(space: Space) -> bool:\n        \"\"\"Return True if every direct sub-space of `space` is a `Box`.\"\"\"\n        if isinstance(space, Tuple):\n            subspaces = space.spaces\n        elif isinstance(space, Dict):\n            subspaces = space.spaces.values()\n        else:\n            return False\n        return all((isinstance(s, Box) for s in subspaces))\n    if tuple_action:\n        cont_action = all_subspaces_box(env.action_space)\n    else:\n        cont_action = is_simple_box(env.action_space)\n    if tuple_obs:\n        cont_obs = all_subspaces_box(env.observation_space)\n    else:\n        cont_obs = is_simple_box(env.observation_space)\n    return (cont_action, cont_obs)"}
{"task_id": 364, "completion_id": 0, "solution": "def sign(x: int | float) -> int:\n    \"\"\"Return the sign of *x*.\n\n    The function returns\n        1  if x > 0\n        0  if x == 0  (handles both 0 and -0.0)\n       -1  if x < 0\n\n    Args:\n        x: A real number (int or float).\n\n    Returns:\n        An integer: -1, 0, or 1 depending on the sign of *x*.\n    \"\"\"\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else:\n        return 0"}
{"task_id": 366, "completion_id": 0, "solution": "import numpy as np\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Compute the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X (np.ndarray): A 2-D array of shape (n_samples, n_features).\n\n    Returns:\n        list[list[float]]: n \u00d7 n matrix where entry (i, j) is the squared\n            \u2113\u2082 distance between samples i and j.  The diagonal entries are 0.0.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D NumPy array')\n    n_samples = X.shape[0]\n    if n_samples == 1:\n        return [[0.0]]\n    sq_norms = np.sum(X * X, axis=1)\n    D = sq_norms[:, None] + sq_norms[None, :] - 2.0 * (X @ X.T)\n    D[D < 0] = 0.0\n    np.fill_diagonal(D, 0.0)\n    return D.tolist()"}
{"task_id": 367, "completion_id": 0, "solution": "import re\nimport ast\nfrom typing import Any, Dict, Optional\nclass KernelBase:\n    \"\"\"Minimal base class for all kernels.  Do NOT modify this class.\"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        self.params: Dict[str, Any] = kwargs\n\n    def set_params(self, state: Dict[str, Any]):\n        \"\"\"(Re)sets parameters from a previously stored summary dict.\"\"\"\n        hp = state.get('hyperparameters', {})\n        self.params.update({k: v for (k, v) in hp.items() if k != 'id'})\n        return self\n\n    def summary(self) -> Dict[str, Any]:\n        \"\"\"Dictionary description that can be fed back to *initialize_kernel*.\"\"\"\n        return {'hyperparameters': {'id': self.__class__.__name__, **self.params}}\nclass LinearKernel(KernelBase):\n\n    def __init__(self):\n        super().__init__()\nclass PolynomialKernel(KernelBase):\n\n    def __init__(self, degree: int=3, coef0: float=1.0):\n        super().__init__(degree=degree, coef0=coef0)\nclass RBFKernel(KernelBase):\n\n    def __init__(self, gamma: float=1.0):\n        super().__init__(gamma=gamma)\ndef _str_to_kwargs(arg_string: str) -> Dict[str, Any]:\n    \"\"\"\n    Convert the inside of the \"(\u2026)\" part of a textual kernel description\n    (\"degree=4, coef0=2\") to a dictionary of correctly-typed Python values.\n    \"\"\"\n    kwargs: Dict[str, Any] = {}\n    if not arg_string:\n        return kwargs\n    for token in [t.strip() for t in arg_string.split(',') if t.strip()]:\n        if '=' not in token:\n            raise ValueError(f\"Malformed argument '{token}'.\")\n        (key, value_txt) = token.split('=', 1)\n        (key, value_txt) = (key.strip(), value_txt.strip())\n        try:\n            value = ast.literal_eval(value_txt)\n        except Exception:\n            value = value_txt.strip('\"\\'')\n        kwargs[key] = value\n    return kwargs\ndef initialize_kernel(param: Optional[Any]=None) -> KernelBase:\n    \"\"\"Create and return a kernel object from various representations.\n\n    Args:\n        param: None (default LinearKernel), an existing KernelBase instance, a\n            string description such as 'RBF(gamma=0.5)', or a dictionary with a\n            'hyperparameters' entry as produced by KernelBase.summary().\n\n    Returns:\n        KernelBase: the newly created or forwarded kernel instance.\n    \"\"\"\n    if param is None:\n        return LinearKernel()\n    if isinstance(param, KernelBase):\n        return param\n    _KERNEL_MAP = {'linear': LinearKernel, 'linearkernel': LinearKernel, 'polynomial': PolynomialKernel, 'polynomialkernel': PolynomialKernel, 'rbf': RBFKernel, 'rbfkernel': RBFKernel}\n    if isinstance(param, str):\n        txt = param.strip()\n        match = re.fullmatch('\\\\s*([A-Za-z_]\\\\w*)\\\\s*(?:\\\\((.*)\\\\))?\\\\s*', txt)\n        if not match:\n            raise ValueError(f\"Cannot parse kernel description '{param!s}'.\")\n        (k_name, arg_string) = (match.group(1), match.group(2))\n        cls = _KERNEL_MAP.get(k_name.lower())\n        if cls is None:\n            raise NotImplementedError(f\"Unknown kernel '{k_name}'.\")\n        kwargs = _str_to_kwargs(arg_string or '')\n        return cls(**kwargs)\n    if isinstance(param, dict):\n        if 'hyperparameters' not in param or not isinstance(param['hyperparameters'], dict):\n            raise ValueError('Dictionary must contain a \"hyperparameters\" key.')\n        hp: Dict[str, Any] = param['hyperparameters']\n        k_id = hp.get('id')\n        if k_id is None:\n            raise ValueError('\"hyperparameters\" dictionary misses required \"id\".')\n        cls = _KERNEL_MAP.get(k_id.lower())\n        if cls is None:\n            raise NotImplementedError(f\"Unknown kernel id '{k_id}'.\")\n        kwargs = {k: v for (k, v) in hp.items() if k != 'id'}\n        return cls(**kwargs)\n    raise TypeError(f\"Unsupported parameter type '{type(param).__name__}' for initialize_kernel.\")"}
{"task_id": 368, "completion_id": 0, "solution": "import numpy as np\ndef minkowski(x: np.ndarray | list, y: np.ndarray | list, p: float) -> float:\n    \"\"\"Compute the Minkowski\u2013p distance between two vectors.\n\n    Args:\n        x (np.ndarray | list): First one-dimensional vector.\n        y (np.ndarray | list): Second one-dimensional vector (same length as x).\n        p (float): Order of the norm (must be > 0).\n\n    Returns:\n        float: The Minkowski distance rounded to 4 decimal places,\n               or -1 when the input validation fails.\n    \"\"\"\n    if p is None or p <= 0:\n        return -1\n    try:\n        x_arr = np.asarray(x, dtype=float)\n        y_arr = np.asarray(y, dtype=float)\n    except Exception:\n        return -1\n    if x_arr.ndim != 1 or y_arr.ndim != 1 or x_arr.shape != y_arr.shape:\n        return -1\n    diff_p = np.abs(x_arr - y_arr) ** p\n    distance = np.sum(diff_p) ** (1.0 / p)\n    return round(float(distance), 4)"}
{"task_id": 369, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        raise ValueError('`updates` must contain at least one tree output.')\n    stacked_updates = np.stack(updates, axis=0)\n    running_pred = -learning_rate * stacked_updates.sum(axis=0)\n    if regression:\n        return np.round(running_pred.astype(float), 4)\n    logits = running_pred\n    logits -= logits.max(axis=1, keepdims=True)\n    exp_logits = np.exp(logits)\n    probas = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n    return probas.argmax(axis=1)"}
{"task_id": 370, "completion_id": 0, "solution": "import numpy as np\ndef multivariate_gaussian(X: np.ndarray, mu: np.ndarray, cov: np.ndarray) -> list[float]:\n    \"\"\"\n    Compute the multivariate normal (Gaussian) density for each sample in `X`.\n\n    Parameters\n    ----------\n    X   : array_like, shape (m, n)\n          Each row is a sample (observation) of dimension n.\n    mu  : array_like, shape (n,)\n          Mean vector of the distribution.\n    cov : array_like, shape (n, n)\n          Positive\u2013definite covariance matrix.\n\n    Returns\n    -------\n    list[float]\n          Densities of each sample rounded to 4 decimal places.\n    \"\"\"\n    X = np.atleast_2d(X)\n    mu = np.ravel(mu)\n    n = mu.size\n    inv_cov = np.linalg.inv(cov)\n    det_cov = np.linalg.det(cov)\n    denom = np.sqrt((2.0 * np.pi) ** n * det_cov)\n    diff = X - mu\n    mahal = np.einsum('ij,jk,ik->i', diff, inv_cov, diff)\n    densities = np.exp(-0.5 * mahal) / denom\n    return [round(d, 4) for d in densities]"}
{"task_id": 371, "completion_id": 0, "solution": "import numpy as np\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int=0) -> list:\n    \"\"\"1-D cross-correlation (a.k.a. \u201cconvolution\u201d in DL frameworks).\n\n    Parameters\n    ----------\n    X : ndarray, shape (batch, signal_length, in_channels)\n        Input signals.\n    W : ndarray, shape (kernel_width, in_channels, out_channels)\n        Convolution kernels (not flipped \u2013 cross-correlation).\n    stride : int\n        Step size of the sliding window.\n    pad : int | tuple(int, int) | \"same\"\n        Zero padding to the left / right of the signal.\n    dilation : int, default 0\n        Number of zeros inserted *between* kernel elements\n        (0 \u21d2 normal, contiguous kernel).\n\n    Returns\n    -------\n    list\n        Convolved output with shape\n        (batch, output_length, out_channels), converted via ``tolist()``.\n    \"\"\"\n    if X.ndim != 3:\n        raise ValueError('X must have shape (batch, length, in_ch).')\n    if W.ndim != 3:\n        raise ValueError('W must have shape (k_width, in_ch, out_ch).')\n    if X.shape[2] != W.shape[1]:\n        raise ValueError('Input-channel dimensions of X and W differ.')\n    if stride <= 0:\n        raise ValueError('stride must be a positive integer.')\n    if dilation < 0:\n        raise ValueError('dilation must be non-negative.')\n    (batch, L_in, C_in) = X.shape\n    (K, _, C_out) = W.shape\n    eff_K = (K - 1) * (dilation + 1) + 1\n    if isinstance(pad, str):\n        pad = pad.lower()\n    if pad == 'same':\n        out_len = int(np.ceil(L_in / stride))\n        total_pad = max(0, (out_len - 1) * stride + eff_K - L_in)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n    elif isinstance(pad, int):\n        p_left = p_right = int(pad)\n    elif isinstance(pad, (tuple, list)) and len(pad) == 2 and all((isinstance(x, int) for x in pad)):\n        (p_left, p_right) = pad\n    else:\n        raise ValueError('pad must be int, 2-tuple or \"same\".')\n    if p_left < 0 or p_right < 0:\n        raise ValueError('Padding values must be non-negative.')\n    X_pad = np.pad(X, ((0, 0), (p_left, p_right), (0, 0)), mode='constant')\n    L_pad = L_in + p_left + p_right\n    out_len = (L_pad - eff_K) // stride + 1\n    if out_len <= 0:\n        return np.empty((batch, 0, C_out)).tolist()\n    start_idx = np.arange(out_len) * stride\n    kernel_offset = np.arange(K) * (dilation + 1)\n    idx = start_idx[:, None] + kernel_offset[None, :]\n    windows = X_pad[:, idx, :]\n    out = np.tensordot(windows, W, axes=([2, 3], [0, 1]))\n    return out.tolist()"}
{"task_id": 373, "completion_id": 0, "solution": "import numpy as np\ndef gini(y: list[int] | 'np.ndarray') -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    y = np.asarray(y).ravel()\n    if y.size == 0:\n        return 0.0\n    (_, counts) = np.unique(y, return_counts=True)\n    probs = counts / counts.sum()\n    impurity = 1.0 - np.sum(probs ** 2)\n    return float(np.round(impurity, 4))"}
{"task_id": 374, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float=0.01, n_iterations: int=10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple-linear\u2010regression model by batch gradient descent.\n\n    Parameters\n    ----------\n    X : np.ndarray (m \u00d7 n)\n        Feature matrix.\n    y : np.ndarray (m,)\n        Target vector.\n    learning_rate : float, default 0.01\n        Step size for gradient descent.\n    n_iterations : int, default 10_000\n        Number of gradient\u2010descent steps.\n\n    Returns\n    -------\n    (weights_list, bias) : tuple[list[float], float]\n        Learned parameters rounded to 4 decimal places.\n        If X and y have inconsistent sample sizes, returns -1.\n    \"\"\"\n    if X.ndim != 2:\n        X = np.atleast_2d(X)\n    y = np.ravel(y)\n    (m, n) = X.shape\n    if m != y.shape[0] or m == 0:\n        return -1\n    theta = np.zeros(n, dtype=float)\n    b = 0.0\n    for _ in range(n_iterations):\n        y_hat = X @ theta + b\n        error = y_hat - y\n        dw = 2 / m * (X.T @ error)\n        db = 2 / m * np.sum(error)\n        theta -= learning_rate * dw\n        b -= learning_rate * db\n    theta_rounded = np.round(theta, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return (theta_rounded, b_rounded)"}
{"task_id": 375, "completion_id": 0, "solution": "import numpy as np\ndef calc_pad_dims_2D(X_shape: tuple, out_dim: tuple, kernel_shape: tuple, stride: int, dilation: int=0) -> tuple:\n    \"\"\"Return the amount of zero-padding needed on each side of a 4-D tensor.\n\n    Parameters\n    ----------\n    X_shape : tuple\n        Shape of the input tensor ``(n_ex, in_rows, in_cols, in_ch)``.\n    out_dim : tuple\n        Desired output height and width ``(out_rows, out_cols)``.\n    kernel_shape : tuple\n        Height and width of the convolution kernel ``(fr, fc)``.\n    stride : int\n        Convolution stride (the same value is used in both directions).\n    dilation : int, optional (default = 0)\n        Dilation factor (number of zeros inserted between kernel elements).\n\n    Returns\n    -------\n    tuple\n        A 4-tuple ``(top, bottom, left, right)``.\n\n    Raises\n    ------\n    ValueError\n        If an argument has a wrong type/size or the requested output\n        dimensions cannot be achieved with non-negative padding.\n    \"\"\"\n\n    def _check_positive_int(name, value, minimum=1):\n        if not isinstance(value, int):\n            raise ValueError(f'{name} must be an integer.')\n        if value < minimum:\n            raise ValueError(f'{name} must be >= {minimum} (got {value}).')\n    if not isinstance(X_shape, (tuple, list)) or len(X_shape) != 4 or (not all((isinstance(x, int) for x in X_shape))):\n        raise ValueError('X_shape has to be a tuple of 4 integers (n_ex, in_rows, in_cols, in_ch).')\n    if not isinstance(out_dim, (tuple, list)) or len(out_dim) != 2 or (not all((isinstance(x, int) for x in out_dim))):\n        raise ValueError('out_dim has to be a tuple (out_rows, out_cols).')\n    if not isinstance(kernel_shape, (tuple, list)) or len(kernel_shape) != 2 or (not all((isinstance(x, int) for x in kernel_shape))):\n        raise ValueError('kernel_shape has to be a tuple (fr, fc).')\n    _check_positive_int('stride', stride, 1)\n    _check_positive_int('dilation', dilation, 0)\n    (_, in_rows, in_cols, _) = X_shape\n    (out_rows, out_cols) = out_dim\n    (fr, fc) = kernel_shape\n    _check_positive_int('in_rows', in_rows, 1)\n    _check_positive_int('in_cols', in_cols, 1)\n    _check_positive_int('out_rows', out_rows, 1)\n    _check_positive_int('out_cols', out_cols, 1)\n    _check_positive_int('fr', fr, 1)\n    _check_positive_int('fc', fc, 1)\n\n    def _pad_1d(in_size: int, out_size: int, k_size: int) -> tuple:\n        \"\"\"\n        Compute (pad_before, pad_after) for a single spatial dimension.\n        \"\"\"\n        eff_k = k_size + (k_size - 1) * dilation\n        total_pad = (out_size - 1) * stride - in_size + eff_k\n        if total_pad < 0:\n            raise ValueError('Requested output size is too small for the given input size, kernel, stride and dilation.')\n        pad_before = total_pad // 2\n        pad_after = total_pad - pad_before\n        return (pad_before, pad_after)\n    (up_pad, down_pad) = _pad_1d(in_rows, out_rows, fr)\n    (left_pad, right_pad) = _pad_1d(in_cols, out_cols, fc)\n    return (up_pad, down_pad, left_pad, right_pad)"}
{"task_id": 376, "completion_id": 0, "solution": "import numpy as np\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if and only if it is square and equal (within\n    numerical tolerance) to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists of numbers.\n\n    Returns:\n        True if *X* is symmetric, otherwise False.\n    \"\"\"\n    if not X:\n        return False\n    n_rows = len(X)\n    for row in X:\n        if len(row) != n_rows:\n            return False\n    A = np.asarray(X, dtype=float)\n    return np.allclose(A, A.T, atol=1e-08, rtol=0.0)"}
{"task_id": 377, "completion_id": 0, "solution": "import numpy as np\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Compute the negative gradient that Gradient Boosting uses when minimising the\n    logistic (binomial-deviance) loss for binary classification.\n    \n    Parameters\n    ----------\n    y : list or 1-D array-like of int (0 or 1)\n        Ground-truth binary labels.\n    f : list or 1-D array-like of float\n        Current prediction scores.\n    \n    Returns\n    -------\n    list\n        Negative gradients for each sample, rounded to 4 decimal places.\n        If `y` contains values other than 0 or 1, returns -1.\n    \"\"\"\n    y_arr = np.asarray(y)\n    f_arr = np.asarray(f, dtype=float)\n    if not np.all(np.isin(y_arr, [0, 1])):\n        return -1\n    y_prime = 2 * y_arr - 1\n    g = y_prime / (1.0 + np.exp(y_prime * f_arr))\n    return np.round(g, 4).tolist()"}
{"task_id": 378, "completion_id": 0, "solution": "def indicator(flag: int) -> int:\n    \"\"\"Return 1 if *flag* is 1, otherwise return 0.\n\n    Args:\n        flag (int): An integer value.\n\n    Returns:\n        int: 1 if flag == 1, else 0.\n    \"\"\"\n    return 1 if flag == 1 else 0"}
{"task_id": 379, "completion_id": 0, "solution": "def sort_priority_nodes(nodes: list[dict]) -> list[str]:\n    \"\"\"Return the `key` values of the given priority-queue nodes in order.\n\n    Order definition:\n        1. Smaller ``priority`` ranks before larger ``priority``.\n        2. If ``priority`` ties, smaller ``entry_id`` ranks before larger\n           ``entry_id``.\n\n    The original input list is left untouched.\n\n    Args:\n        nodes: A list of dictionaries.  Each dictionary must contain the\n               keys ``'key'``, ``'val'``, ``'priority'`` and ``'entry_id'``.\n\n    Returns:\n        A list of keys in their correct priority order.\n    \"\"\"\n    return [n['key'] for n in sorted(nodes, key=lambda n: (n['priority'], n['entry_id']))]"}
{"task_id": 380, "completion_id": 0, "solution": "import numpy as np\n_ALPHA = 1.6732632423543772\n_SCALE = 1.0507009873554805\ndef selu(x, order: int=0):\n    \"\"\"Compute the SELU activation or its first / second derivative.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Works for scalars and any-shaped arrays/lists.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 d(SELU)/dx\n        2 \u2192 d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    float | list\n        Result rounded to 4 decimals.  For array-like inputs the shape is\n        preserved and a Python list is returned; for scalar input a float\n        is returned.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=np.float64)\n    pos_mask = x_arr > 0\n    neg_mask = ~pos_mask\n    if order == 0:\n        out = np.empty_like(x_arr)\n        out[pos_mask] = _SCALE * x_arr[pos_mask]\n        out[neg_mask] = _SCALE * _ALPHA * (np.exp(x_arr[neg_mask]) - 1)\n    elif order == 1:\n        out = np.empty_like(x_arr)\n        out[pos_mask] = _SCALE\n        out[neg_mask] = _SCALE * _ALPHA * np.exp(x_arr[neg_mask])\n    elif order == 2:\n        out = np.empty_like(x_arr)\n        out[pos_mask] = 0.0\n        out[neg_mask] = _SCALE * _ALPHA * np.exp(x_arr[neg_mask])\n    else:\n        raise ValueError('order must be 0, 1 or 2')\n    rounded = np.round(out, 4)\n    return rounded.tolist()"}
{"task_id": 382, "completion_id": 0, "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef gaussian_naive_bayes_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list:\n    \"\"\"Train a Gaussian Naive Bayes model on *X_train*, *y_train* and predict\n    the labels of *X_test*.\n\n    Every feature is assumed to follow an independent Gaussian distribution\n    inside each class.  The implementation follows the exact formulae given\n    in the task statement.  The returned value is a *Python list* whose i-th\n    element is the predicted label for the i-th row of *X_test*.\n    \"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train)\n    X_test = np.asarray(X_test, dtype=float)\n    eps = 0.0001\n    classes = np.unique(y_train)\n    n_classes = classes.shape[0]\n    n_features = X_train.shape[1]\n    priors = np.zeros(n_classes, dtype=float)\n    means = np.zeros((n_classes, n_features))\n    variances = np.zeros((n_classes, n_features))\n    for (idx, c) in enumerate(classes):\n        X_c = X_train[y_train == c]\n        priors[idx] = X_c.shape[0] / X_train.shape[0]\n        means[idx, :] = X_c.mean(axis=0)\n        variances[idx, :] = X_c.var(axis=0)\n    log_priors = np.log(priors)\n    n_test = X_test.shape[0]\n    log_posteriors = np.zeros((n_test, n_classes), dtype=float)\n    for idx in range(n_classes):\n        mu = means[idx]\n        var = variances[idx]\n        log_coef = -0.5 * np.log(2 * np.pi * var + eps)\n        inv_denom = 1.0 / (2 * var + eps)\n        diff2 = (X_test - mu) ** 2\n        log_likelihood = (log_coef - diff2 * inv_denom).sum(axis=1)\n        log_posteriors[:, idx] = log_priors[idx] + log_likelihood\n    best_class_indices = np.argmax(log_posteriors, axis=1)\n    predicted_labels = classes[best_class_indices]\n    return predicted_labels.tolist()"}
{"task_id": 383, "completion_id": 0, "solution": "def find_bounds(y_i: int, y_j: int, alpha_i: float, alpha_j: float, C: float) -> list[float]:\n    \"\"\"Return the feasible interval [L, H] for alpha_j in SMO.\n\n    Parameters\n    ----------\n    y_i : int\n        Label of the i-th example (\u22121 or 1).\n    y_j : int\n        Label of the j-th example (\u22121 or 1).\n    alpha_i : float\n        Current Lagrange multiplier for the i-th example.\n    alpha_j : float\n        Current Lagrange multiplier for the j-th example.\n    C : float\n        Regularisation constant of the SVM.\n\n    Returns\n    -------\n    list[float]\n        A two-element list [L, H] with both values rounded to 4 decimals.\n    \"\"\"\n    if y_i != y_j:\n        L = max(0.0, alpha_j - alpha_i)\n        H = min(C, C - alpha_i + alpha_j)\n    else:\n        L = max(0.0, alpha_i + alpha_j - C)\n        H = min(C, alpha_i + alpha_j)\n    return [round(L, 4), round(H, 4)]"}
{"task_id": 384, "completion_id": 0, "solution": "import numpy as np\ndef get_eigvecs(L: list[list[int | float]] | np.ndarray, k: int):\n    \"\"\"\n    Calculate the eigenvectors that correspond to the k smallest eigenvalues\n    of a Laplacian matrix L.\n\n    Parameters\n    ----------\n    L : list[list[int | float]] | np.ndarray\n        Square Laplacian matrix.\n    k : int\n        Number of eigenvectors wanted.\n\n    Returns\n    -------\n    list[list[float]] | int\n        Matrix (as a list of lists) whose columns are the eigenvectors\n        associated with the k smallest eigenvalues, rounded to 4 decimals.\n        Returns -1 if k is invalid (k > n or k <= 0) or L is not square.\n    \"\"\"\n    L = np.asarray(L, dtype=float)\n    if L.ndim != 2 or L.shape[0] != L.shape[1]:\n        return -1\n    n = L.shape[0]\n    if k <= 0 or k > n:\n        return -1\n    (eigvals, eigvecs) = np.linalg.eigh(L)\n    eigvecs = np.real(eigvecs[:, :k])\n    for j in range(k):\n        v = eigvecs[:, j]\n        norm = np.linalg.norm(v)\n        if norm != 0:\n            v /= norm\n        nz = np.flatnonzero(v)\n        if nz.size > 0 and v[nz[0]] < 0:\n            v *= -1\n        eigvecs[:, j] = v\n    eigvecs = np.round(eigvecs, 4)\n    return eigvecs.tolist()"}
{"task_id": 385, "completion_id": 0, "solution": "import numpy as np\ndef lda_transform(X: list[list[int | float]], y: list[int], k: int) -> list[list[float]]:\n    \"\"\"\n    Project a labelled data set onto its first k Linear Discriminant\n    components and return the projected data rounded to 4 decimals.\n\n    Parameters\n    ----------\n    X : list of list (n_samples, n_features)\n        The input data matrix.\n    y : list (n_samples,)\n        Class labels for every sample.\n    k : int\n        Number of LDA directions that should be retained.\n\n    Returns\n    -------\n    list of list\n        The data projected onto the top-k LDA directions, rounded\n        to 4 decimal places.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    (n_samples, n_features) = X.shape\n    (classes, class_indices) = np.unique(y, return_inverse=True)\n    c = len(classes)\n    max_k = max(c - 1, 0)\n    k = min(k, max_k, n_features)\n    if k == 0:\n        return [[] for _ in range(n_samples)]\n    overall_mean = np.mean(X, axis=0)\n    Sw = np.zeros((n_features, n_features), dtype=float)\n    Sb = np.zeros((n_features, n_features), dtype=float)\n    for cls in classes:\n        cls_mask = y == cls\n        X_c = X[cls_mask]\n        mean_c = np.mean(X_c, axis=0)\n        centered = X_c - mean_c\n        Sw += centered.T @ centered\n        mean_diff = (mean_c - overall_mean).reshape(-1, 1)\n        Sb += X_c.shape[0] * (mean_diff @ mean_diff.T)\n    Sw_inv = np.linalg.pinv(Sw)\n    eig_mat = Sw_inv @ Sb\n    (eigvals, eigvecs) = np.linalg.eig(eig_mat)\n    eigvals = eigvals.real\n    eigvecs = eigvecs.real\n    sorted_idx = np.argsort(eigvals)[::-1]\n    W = eigvecs[:, sorted_idx[:k]]\n    for i in range(W.shape[1]):\n        vec = W[:, i]\n        vec /= np.linalg.norm(vec) + 1e-15\n        if vec[0] < 0:\n            vec = -vec\n        W[:, i] = vec\n    projected = X @ W\n    projected = np.round(projected, 4)\n    return projected.tolist()"}
{"task_id": 386, "completion_id": 0, "solution": "import numpy as np\ndef binary_crossentropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the Binary Cross-Entropy loss.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        1-D array of true labels (0 or 1).\n    y_pred : np.ndarray\n        1-D array of predicted probabilities (between 0 and 1).\n\n    Returns\n    -------\n    float\n        The average BCE loss rounded to 4 decimal places.\n    \"\"\"\n    eps = 1e-15\n    y_pred_clipped = np.clip(y_pred, eps, 1 - eps)\n    loss = -np.mean(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n    return float(np.round(loss, 4))"}
{"task_id": 387, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float=0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n = y.size\n    if n == 0:\n        return []\n    pred = np.full(n, y.mean())\n    for _ in range(n_estimators):\n        residuals = y - pred\n        if np.all(x == x[0]):\n            stump_out = np.full(n, residuals.mean())\n        else:\n            order = np.argsort(x)\n            xs = x[order]\n            rs = residuals[order]\n            csum = np.cumsum(rs)\n            csum_sq = np.cumsum(rs ** 2)\n            total_sum = csum[-1]\n            total_sq = csum_sq[-1]\n            k_left = np.arange(1, n)\n            k_right = n - k_left\n            sum_left = csum[:-1]\n            sum_sq_left = csum_sq[:-1]\n            sum_right = total_sum - sum_left\n            sum_sq_right = total_sq - sum_sq_left\n            mean_left = sum_left / k_left\n            mean_right = sum_right / k_right\n            sse_left = sum_sq_left - k_left * mean_left ** 2\n            sse_right = sum_sq_right - k_right * mean_right ** 2\n            total_sse = sse_left + sse_right\n            valid = xs[:-1] != xs[1:]\n            if not np.any(valid):\n                stump_out = np.full(n, residuals.mean())\n            else:\n                best_pos = np.argmin(np.where(valid, total_sse, np.inf))\n                thr = (xs[best_pos] + xs[best_pos + 1]) / 2.0\n                mu_left = mean_left[best_pos]\n                mu_right = mean_right[best_pos]\n                stump_out = np.where(x <= thr, mu_left, mu_right)\n        pred += learning_rate * stump_out\n    return np.round(pred, 4).tolist()"}
{"task_id": 389, "completion_id": 0, "solution": "import numpy as np\ndef compute_laplacian(W):\n    \"\"\"Compute the (unnormalised) Laplacian matrix of an undirected graph.\n\n    Parameters\n    ----------\n    W : list[list[int | float]] | np.ndarray\n        Square weighted adjacency matrix of the graph.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The Laplacian matrix rounded to 4 decimal places and converted to a\n        Python list of lists. Returns -1 if the input is not a square matrix.\n    \"\"\"\n    try:\n        A = np.asarray(W, dtype=float)\n    except (ValueError, TypeError):\n        return -1\n    if A.ndim != 2 or A.shape[0] != A.shape[1]:\n        return -1\n    degrees = A.sum(axis=1)\n    D = np.diag(degrees)\n    L = D - A\n    L = np.round(L, 4)\n    return L.tolist()"}
{"task_id": 391, "completion_id": 0, "solution": "import numpy as np\ndef nn_interpolate_1D(X: np.ndarray, t: list[float]) -> list[list[float]]:\n    \"\"\"Nearest-neighbour interpolation for a 1-D (multi-channel) signal.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Array of shape (in_length, in_channels) containing the sampled signal.\n        A 1-D array of shape (in_length,) is also accepted (treated as a single\n        channel).\n    t : list[float]\n        Coordinates at which to interpolate.\n\n    Returns\n    -------\n    list[list[float]]\n        Interpolated samples as a plain Python list with shape\n        (len(t), in_channels).\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = X[:, None]\n    in_len = X.shape[0]\n    idx = np.around(np.asarray(t, dtype=float)).astype(int)\n    idx = np.clip(idx, 0, in_len - 1)\n    return X[idx].tolist()"}
{"task_id": 392, "completion_id": 0, "solution": "import numpy as np\ndef preemphasis(x: 'list[float] | np.ndarray', alpha: float) -> list[float]:\n    \"\"\"Apply a first-order pre-emphasis filter to a 1-D signal.\n\n    Args:\n        x: 1-D (or n-D) array-like sequence of numeric samples.\n        alpha: Pre-emphasis coefficient, 0 \u2264 alpha < 1.\n\n    Returns:\n        Filtered signal as a Python list, with every element rounded\n        to four decimal places.\n    \"\"\"\n    if not 0 <= alpha < 1:\n        raise ValueError('`alpha` must satisfy 0 \u2264 alpha < 1.')\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size == 0:\n        return []\n    y = np.empty_like(arr)\n    y[0] = arr[0]\n    if arr.size > 1:\n        y[1:] = arr[1:] - alpha * arr[:-1]\n    return np.round(y, 4).tolist()"}
{"task_id": 394, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef rmsprop_update(w: np.ndarray, grad: np.ndarray, Eg: np.ndarray | None=None, learning_rate: float=0.01, rho: float=0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` is used.\n    learning_rate : float, optional\n        Step size (\u03b1). Default is 0.01.\n    rho : float, optional\n        Decay rate (\u03c1). Default is 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        Tuple ``(w_next, Eg_next)`` \u2013 both rounded to four decimal places and\n        converted to regular Python lists.\n    \"\"\"\n    if Eg is None:\n        Eg = np.zeros_like(grad, dtype=float)\n    epsilon = 1e-08\n    Eg_next = rho * Eg + (1.0 - rho) * grad ** 2\n    w_next = w - learning_rate * grad / np.sqrt(Eg_next + epsilon)\n    w_next_rounded = np.round(w_next, 4).tolist()\n    Eg_next_rounded = np.round(Eg_next, 4).tolist()\n    return (w_next_rounded, Eg_next_rounded)"}
{"task_id": 395, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef print_backward_result(beta: np.ndarray, b_prob: float) -> str:\n    \"\"\"Return a formatted string with a \u03b2-matrix and its probability.\n\n    The string has to look like:\n        **************************************************\n        Beta:\n        <beta as str(beta)>\n        Probability of sequence: <b_prob>\n    \"\"\"\n    header = '*' * 50\n    beta_str = str(beta)\n    prob_str = f'Probability of sequence: {b_prob}'\n    return '\\n'.join([header, 'Beta:', beta_str, prob_str])"}
{"task_id": 396, "completion_id": 0, "solution": "import numpy as np\ndef tanh_with_derivatives(x):\n    \"\"\"Compute tanh and its first two derivatives element-wise.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        1-D collection of real numbers.\n\n    Returns\n    -------\n    list[list[float]]\n        [\n            tanh(x),                     # element-wise tanh\n            1 - tanh(x)**2,              # 1st derivative\n            -2 * tanh(x) * (1 - tanh(x)**2)  # 2nd derivative\n        ]\n    All inner lists are rounded to 6 decimal places.\n    \"\"\"\n    z = np.asarray(x, dtype=float).copy()\n    t = np.tanh(z)\n    dt = 1.0 - t ** 2\n    d2t = -2.0 * t * dt\n    out = [arr.round(6).tolist() for arr in (t, dt, d2t)]\n    return out"}
{"task_id": 397, "completion_id": 0, "solution": "import numpy as np\ndef random_one_hot_matrix(n_examples: int, n_classes: int):\n    \"\"\"Create a random one-hot matrix.\n\n    Parameters\n    ----------\n    n_examples : int\n        Number of rows (examples) to generate.  May be 0.\n    n_classes : int\n        Size of the vocabulary / number of distinct classes (>0).\n\n    Returns\n    -------\n    numpy.ndarray\n        A matrix of shape (n_examples, n_classes) where every row is a one-hot\n        encoded vector chosen uniformly at random from all `n_classes` possible\n        class indices.\n    \"\"\"\n    if n_examples == 0:\n        return np.zeros((0, n_classes), dtype=float)\n    indices = np.random.randint(0, n_classes, size=n_examples)\n    one_hot = np.eye(n_classes, dtype=float)[indices]\n    return one_hot"}
{"task_id": 398, "completion_id": 0, "solution": "import numpy as np\ndef elu(x, alpha: float=0.1, derivative: bool=False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient element-wise.\n\n    Args:\n        x: 1-D list or NumPy array of numeric values.\n        alpha: Positive ELU hyper-parameter (default 0.1).\n        derivative: If True, return the gradient; otherwise return the activation.\n\n    Returns:\n        Python list of floats rounded to 4 decimal places.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=float)\n    pos_mask = x_arr >= 0\n    neg_mask = ~pos_mask\n    if not derivative:\n        out = x_arr.copy()\n        out[neg_mask] = alpha * (np.exp(x_arr[neg_mask]) - 1)\n    else:\n        out = np.ones_like(x_arr)\n        out[neg_mask] = alpha * np.exp(x_arr[neg_mask])\n    return np.round(out, 4).tolist()"}
{"task_id": 399, "completion_id": 0, "solution": "import numpy as np\ndef pairwise_distances(X, Y):\n    \"\"\"Compute the Euclidean distance between every pair of points in X and Y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        A 2-D array of shape (m, d) where each row is a point.\n    Y : numpy.ndarray\n        A 2-D array of shape (n, d) where each row is a point.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The m\u00d7n matrix of distances rounded to four decimals as a list of\n        lists, or \u22121 if the inputs are invalid.\n    \"\"\"\n    if not isinstance(X, np.ndarray) or not isinstance(Y, np.ndarray):\n        return -1\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    XX = np.sum(X * X, axis=1)[:, None]\n    YY = np.sum(Y * Y, axis=1)[None, :]\n    d2 = XX + YY - 2 * X @ Y.T\n    d2 = np.clip(d2, 0.0, None)\n    D = np.sqrt(d2)\n    return np.round(D, 4).tolist()"}
{"task_id": 400, "completion_id": 0, "solution": "import math\ndef gaussian_cdf(x: float, mean: float, variance: float) -> float:\n    \"\"\"Compute \u03a6(x; \u03bc, \u03c3\u00b2) \u2013 the Gaussian CDF evaluated at *x*.\n\n    Parameters\n    ----------\n    x : float\n        The point at which to evaluate the CDF.\n    mean : float\n        The mean (\u03bc) of the Gaussian distribution.\n    variance : float\n        The variance (\u03c3\u00b2) of the Gaussian distribution. Must be non-negative.\n\n    Returns\n    -------\n    float\n        The probability that a Gaussian random variable with the given\n        parameters does not exceed *x*, rounded to six decimal places.\n    \"\"\"\n    if variance < 0:\n        raise ValueError('Variance must be non-negative.')\n    if variance == 0:\n        if x < mean:\n            prob = 0.0\n        elif x > mean:\n            prob = 1.0\n        else:\n            prob = 0.5\n        return round(prob, 6)\n    sigma = math.sqrt(variance)\n    z = (x - mean) / (sigma * math.sqrt(2.0))\n    prob = 0.5 * (1.0 + math.erf(z))\n    return round(prob, 6)"}
{"task_id": 401, "completion_id": 0, "solution": "import numpy as np\ndef to_one_hot(y: np.ndarray, num_classes: int | None=None) -> np.ndarray:\n    \"\"\"Convert a vector of integer class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        \u2022 Shape (N,)  : flat vector of labels  \n        \u2022 Shape (N,1) : column vector of labels  \n        \u2022 Shape (N,C) : already one-hot (every row sums to 1)\n    num_classes : int | None, optional\n        Desired number of classes (width of the output).  If ``None`` it is\n        inferred as ``max(y) + 1`` for label vectors, or the second dimension\n        of ``y`` when it is already one-hot.\n\n    Returns\n    -------\n    np.ndarray\n        Float array of shape (N, C) containing only 0.0 and 1.0.\n    \"\"\"\n    y = np.asarray(y)\n    if y.ndim == 2 and y.shape[1] > 1:\n        is_binary = np.logical_or(y == 0, y == 1).all()\n        sums_to_one = np.allclose(y.sum(axis=1), 1)\n        if is_binary and sums_to_one:\n            return y.astype(float, copy=False)\n    if y.ndim == 2 and y.shape[1] == 1:\n        y = y.ravel()\n    elif y.ndim != 1:\n        raise ValueError('Input must be a 1-D label vector, a (N,1) label matrix, or an already one-hot encoded (N,C) matrix.')\n    if not np.issubdtype(y.dtype, np.integer):\n        raise ValueError('Class labels must be integers.')\n    if np.any(y < 0):\n        raise ValueError('Negative class labels are not supported.')\n    inferred_classes = int(y.max()) + 1\n    C = num_classes if num_classes is not None else inferred_classes\n    if C <= y.max():\n        raise ValueError(f'`num_classes` must be greater than the largest label (y.max()={y.max()!r}, C={C!r}).')\n    N = y.shape[0]\n    one_hot = np.zeros((N, C), dtype=float)\n    one_hot[np.arange(N), y] = 1.0\n    return one_hot"}
{"task_id": 402, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_nb_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, eps: float=1e-06) -> np.ndarray:\n    \"\"\"Predict labels for X_test using a Gaussian Naive Bayes classifier.\n\n    Parameters\n    ----------\n    X_train : np.ndarray of shape (N_train, M)\n        Training feature matrix.\n    y_train : np.ndarray of shape (N_train,)\n        Training labels.\n    X_test : np.ndarray of shape (N_test, M)\n        Test feature matrix whose labels are to be predicted.\n    eps : float, optional (default=1e-6)\n        Small value added to variances to avoid division by zero.\n\n    Returns\n    -------\n    np.ndarray of shape (N_test,)\n        Predicted labels for each row in X_test.\n    \"\"\"\n    (classes, y_inv) = np.unique(y_train, return_inverse=True)\n    n_classes = classes.size\n    n_features = X_train.shape[1]\n    class_counts = np.bincount(y_inv).astype(np.float64)\n    priors = class_counts / class_counts.sum()\n    log_priors = np.log(priors)\n    means = np.empty((n_classes, n_features), dtype=np.float64)\n    vars_ = np.empty_like(means)\n    for (idx, c) in enumerate(classes):\n        X_c = X_train[y_train == c]\n        means[idx] = X_c.mean(axis=0)\n        vars_[idx] = X_c.var(axis=0) + eps\n    log_coeff = -0.5 * (np.log(2.0 * np.pi) + np.log(vars_))\n    class_const = log_coeff.sum(axis=1)\n    diff = X_test[:, None, :] - means[None, :, :]\n    sq_term = -0.5 * diff ** 2 / vars_[None, :, :]\n    log_lik = class_const[None, :] + sq_term.sum(axis=2)\n    log_post = log_priors[None, :] + log_lik\n    y_pred = classes[np.argmax(log_post, axis=1)]\n    return y_pred"}
{"task_id": 405, "completion_id": 0, "solution": "import numpy as np\nimport re\n_FLOAT = '[+-]?(?:\\\\d+(?:\\\\.\\\\d*)?|\\\\.\\\\d+)(?:[eE][+-]?\\\\d+)?'\ndef _parse_leaky(param: str) -> float:\n    \"\"\"\n    Parse   leaky relu(alpha=<float>)\n    \"\"\"\n    pat = f'^leaky\\\\s*relu(?:\\\\(\\\\s*alpha\\\\s*=\\\\s*({_FLOAT})\\\\s*\\\\))?$'\n    m = re.fullmatch(pat, param, flags=re.I)\n    if not m:\n        raise ValueError(f'Unknown activation: {param}')\n    return float(m.group(1)) if m.group(1) is not None else 0.01\ndef _parse_affine(param: str) -> tuple[float, float]:\n    \"\"\"\n    Parse   affine(slope=<float>, intercept=<float>)\n    (order inside the parentheses may be swapped, white-spaces ignored)\n    \"\"\"\n    m = re.fullmatch('affine\\\\((.*)\\\\)', param, flags=re.I)\n    if not m:\n        raise ValueError(f'Unknown activation: {param}')\n    inside = m.group(1)\n    pieces = [p.strip() for p in inside.split(',') if p.strip()]\n    kv = {}\n    for p in pieces:\n        if '=' not in p:\n            raise ValueError(f'Unknown activation: {param}')\n        (k, v) = [s.strip().lower() for s in p.split('=', 1)]\n        if not re.fullmatch(_FLOAT, v, flags=re.I):\n            raise ValueError(f'Unknown activation: {param}')\n        kv[k] = float(v)\n    if 'slope' not in kv or 'intercept' not in kv:\n        raise ValueError(f'Unknown activation: {param}')\n    return (kv['slope'], kv['intercept'])\ndef apply_activation(x, param=None):\n    \"\"\"Apply a chosen activation to the input vector.\n\n    Args:\n        x (list[float] | np.ndarray): Input data. Anything that can be\n            converted to a NumPy array of floats is accepted.\n        param (str | None, optional): Description of the activation. See the\n            task specification for the accepted values. If *None* the\n            identity function is applied.\n\n    Returns:\n        list[float]: Result of applying the activation element-wise, rounded\n            to six decimals and converted to a standard Python *list*.\n    \"\"\"\n    arr = np.asarray(x, dtype=float)\n    if param is None or (isinstance(param, str) and param.strip().lower() == 'identity'):\n        res = arr\n    elif isinstance(param, str) and param.strip().lower() == 'relu':\n        res = np.maximum(0, arr)\n    elif isinstance(param, str) and param.strip().lower() == 'tanh':\n        res = np.tanh(arr)\n    elif isinstance(param, str) and param.strip().lower() == 'sigmoid':\n        res = 1.0 / (1.0 + np.exp(-arr))\n    elif isinstance(param, str) and param.lower().startswith('leaky'):\n        alpha = _parse_leaky(param.strip().lower())\n        res = np.where(arr > 0, arr, alpha * arr)\n    elif isinstance(param, str) and param.lower().startswith('affine'):\n        (slope, intercept) = _parse_affine(param.strip().lower())\n        res = slope * arr + intercept\n    else:\n        raise ValueError(f'Unknown activation: {param}')\n    res = np.round(res, 6)\n    if res.ndim == 0:\n        return [float(res)]\n    return res.tolist()"}
{"task_id": 406, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X_train: 'np.ndarray', y_train: 'np.ndarray', X_test: 'np.ndarray', k: int=5) -> list[int]:\n    \"\"\"k-Nearest Neighbors classifier.\"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    if X_train.ndim != 2 or X_test.ndim != 2:\n        raise ValueError('X_train and X_test must be 2-D arrays')\n    if X_train.shape[0] != y_train.shape[0]:\n        raise ValueError('X_train and y_train have incompatible shapes')\n    if k <= 0:\n        raise ValueError('k must be a positive integer')\n    n_train = X_train.shape[0]\n    k = min(k, n_train)\n    diff = X_test[:, None, :] - X_train[None, :, :]\n    dists = np.einsum('ijk,ijk->ij', diff, diff)\n    predictions: list[int] = []\n    for row in dists:\n        nn_idx = np.argpartition(row, k - 1)[:k]\n        nearest_labels = y_train[nn_idx]\n        vote_count = np.bincount(nearest_labels)\n        predictions.append(int(vote_count.argmax()))\n    return predictions"}
{"task_id": 410, "completion_id": 0, "solution": "import re\ndef parse_activation(act_str: str):\n    \"\"\"Parse an activation-function description string.\n\n    The string can be one of the following forms:\n        \u2022 \"ReLU\"\n        \u2022 \"Sigmoid\"\n        \u2022 \"Tanh\"\n        \u2022 \"Affine(slope=<number>, intercept=<number>)\"\n        \u2022 \"Leaky ReLU(alpha=<number>)\"\n\n    Args:\n        act_str: Description of the activation function.\n\n    Returns:\n        A dictionary with key \"type\" and, when applicable, its parameters.\n        If the input format is invalid, return -1.\n    \"\"\"\n    number_pat = '[-+]?(?:\\\\d+(?:\\\\.\\\\d*)?|\\\\.\\\\d+)'\n    patterns = [(re.compile('^ReLU$'), lambda _: {'type': 'ReLU'}), (re.compile('^Sigmoid$'), lambda _: {'type': 'Sigmoid'}), (re.compile('^Tanh$'), lambda _: {'type': 'Tanh'}), (re.compile(f'^Affine\\\\(slope=({number_pat}), intercept=({number_pat})\\\\)$'), lambda m: {'type': 'Affine', 'slope': float(m.group(1)), 'intercept': float(m.group(2))}), (re.compile(f'^Leaky ReLU\\\\(alpha=({number_pat})\\\\)$'), lambda m: {'type': 'Leaky ReLU', 'alpha': float(m.group(1))})]\n    for (pat, builder) in patterns:\n        m = pat.match(act_str)\n        if m:\n            return builder(m)\n    return -1"}
{"task_id": 411, "completion_id": 0, "solution": "import numpy as np\nfrom numbers import Integral, Real\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute simple statistics of action / observation trajectories.\"\"\"\n\n    def is_iterable(x):\n        \"\"\"Return True if *x* is a list/tuple/ndarray with length.\"\"\"\n        return isinstance(x, (list, tuple, np.ndarray)) and (not np.isscalar(x))\n\n    def analyse(collection):\n        \"\"\"\n        Inspect a list of (possibly-vector) values and return\n        tuple_flag  \u2026 at least one element is an iterable\n        multi_dim   \u2026 dimensionality > 1\n        continuous  \u2026 at least one value is a non-integer float\n        n_per_dim   \u2026 list with #unique values per dimension\n        dim         \u2026 dimensionality\n        ids         \u2026 list with sorted unique ids per dimension\n        \"\"\"\n        if len(collection) == 0:\n            raise ValueError('Empty trajectory supplied.')\n        tuple_flag = any((is_iterable(x) for x in collection))\n        if tuple_flag:\n            for sample in collection:\n                if is_iterable(sample):\n                    dim = len(sample)\n                    break\n        else:\n            dim = 1\n        multi_dim = dim > 1\n        uniq_sets = [set() for _ in range(dim)]\n        continuous = False\n        for sample in collection:\n            if tuple_flag:\n                values = list(sample)\n                if len(values) != dim:\n                    raise ValueError('Inconsistent dimensionality in data.')\n            else:\n                values = [sample]\n            for (d, val) in enumerate(values):\n                uniq_sets[d].add(val)\n                if not continuous:\n                    if isinstance(val, (Real, np.floating, np.integer)):\n                        if isinstance(val, (Integral, np.integer)):\n                            continue\n                        if not float(val).is_integer():\n                            continuous = True\n                    else:\n                        continuous = True\n        ids = [sorted(list(s)) for s in uniq_sets]\n        n_per_dim = [len(s) for s in uniq_sets]\n        return (tuple_flag, multi_dim, continuous, n_per_dim, dim, ids)\n    (tuple_a, multi_a, cont_a, n_a, dim_a, ids_a) = analyse(actions)\n    (tuple_o, multi_o, cont_o, n_o, dim_o, ids_o) = analyse(observations)\n    return {'tuple_actions': tuple_a, 'tuple_observations': tuple_o, 'multidim_actions': multi_a, 'multidim_observations': multi_o, 'continuous_actions': cont_a, 'continuous_observations': cont_o, 'n_actions_per_dim': n_a, 'action_dim': dim_a, 'n_obs_per_dim': n_o, 'obs_dim': dim_o, 'action_ids': ids_a, 'obs_ids': ids_o}"}
{"task_id": 412, "completion_id": 0, "solution": "def get_metric(name: str):\n    \"\"\"Return an evaluation metric function by *name*.\n\n    Supported names:\n    - 'accuracy'\n    - 'precision'\n    - 'recall'\n    - 'f1'\n\n    If the name is invalid, the function returns -1.\n    The returned metric functions accept two equal-length iterables\n    (y_true, y_pred) of 0s and 1s and return the metric value\n    rounded to 4 decimal places.\n    \"\"\"\n\n    def _accuracy(y_true, y_pred):\n        total = 0\n        correct = 0\n        for (t, p) in zip(y_true, y_pred):\n            total += 1\n            if t == p:\n                correct += 1\n        if total == 0:\n            return 0.0\n        return round(correct / total, 4)\n\n    def _precision(y_true, y_pred):\n        tp = fp = 0\n        for (t, p) in zip(y_true, y_pred):\n            if p == 1:\n                if t == 1:\n                    tp += 1\n                else:\n                    fp += 1\n        denom = tp + fp\n        if denom == 0:\n            return 0.0\n        return round(tp / denom, 4)\n\n    def _recall(y_true, y_pred):\n        tp = fn = 0\n        for (t, p) in zip(y_true, y_pred):\n            if t == 1:\n                if p == 1:\n                    tp += 1\n                else:\n                    fn += 1\n        denom = tp + fn\n        if denom == 0:\n            return 0.0\n        return round(tp / denom, 4)\n\n    def _f1(y_true, y_pred):\n        tp = fp = fn = 0\n        for (t, p) in zip(y_true, y_pred):\n            if t == 1:\n                if p == 1:\n                    tp += 1\n                else:\n                    fn += 1\n            elif p == 1:\n                fp += 1\n        prec_denom = tp + fp\n        rec_denom = tp + fn\n        precision = tp / prec_denom if prec_denom else 0.0\n        recall = tp / rec_denom if rec_denom else 0.0\n        if precision == 0 and recall == 0:\n            return 0.0\n        f1_value = 2 * precision * recall / (precision + recall)\n        return round(f1_value, 4)\n    metrics = {'accuracy': _accuracy, 'precision': _precision, 'recall': _recall, 'f1': _f1}\n    return metrics.get(name, -1)"}
{"task_id": 413, "completion_id": 0, "solution": "import numpy as np\nclass _TreeNode:\n    \"\"\"A single node in a CART regression tree.\"\"\"\n    __slots__ = ('is_leaf', 'value', 'feature', 'threshold', 'left', 'right')\n\n    def __init__(self, *, value=None, feature=None, threshold=None, left=None, right=None, is_leaf=False):\n        self.is_leaf = is_leaf\n        self.value = value\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\nclass _CARTRegressor:\n    \"\"\"Very small, depth\u2013limited CART regressor (MSE criterion).\"\"\"\n\n    def __init__(self, max_depth: int=3):\n        self.max_depth = max_depth\n        self.root: _TreeNode | None = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        self.root = self._grow(X, y, depth=0)\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        preds = np.empty(X.shape[0], dtype=float)\n        for (i, x) in enumerate(X):\n            preds[i] = self._predict_row(self.root, x)\n        return preds\n\n    def _predict_row(self, node: _TreeNode, x: np.ndarray) -> float:\n        while not node.is_leaf:\n            node = node.left if x[node.feature] <= node.threshold else node.right\n        return node.value\n\n    def _grow(self, X: np.ndarray, y: np.ndarray, depth: int) -> _TreeNode:\n        (n_samples, n_feats) = X.shape\n        if depth >= self.max_depth or n_samples <= 1 or np.all(y == y[0]):\n            return _TreeNode(value=float(y.mean()), is_leaf=True)\n        best_sse = np.var(y) * n_samples\n        (best_feat, best_thr, best_split) = (None, None, None)\n        for feat in range(n_feats):\n            x_col = X[:, feat]\n            sorted_idx = np.argsort(x_col)\n            (x_sorted, y_sorted) = (x_col[sorted_idx], y[sorted_idx])\n            uniq_vals = np.unique(x_sorted)\n            if uniq_vals.size == 1:\n                continue\n            thr_candidates = (uniq_vals[:-1] + uniq_vals[1:]) / 2.0\n            y_cumsum = np.cumsum(y_sorted)\n            y_sq_cumsum = np.cumsum(y_sorted ** 2)\n            for (idx, thr) in enumerate(thr_candidates, start=1):\n                n_left = np.searchsorted(x_sorted, thr, side='right')\n                if n_left == 0 or n_left == n_samples:\n                    continue\n                sum_y_left = y_cumsum[n_left - 1]\n                sum_yy_left = y_sq_cumsum[n_left - 1]\n                n_l = n_left\n                mse_left = sum_yy_left - sum_y_left ** 2 / n_l\n                sum_y_right = y_cumsum[-1] - sum_y_left\n                sum_yy_right = y_sq_cumsum[-1] - sum_yy_left\n                n_r = n_samples - n_l\n                mse_right = sum_yy_right - sum_y_right ** 2 / n_r\n                sse_split = mse_left + mse_right\n                if sse_split < best_sse - 1e-07:\n                    best_sse = sse_split\n                    best_feat = feat\n                    best_thr = thr\n                    best_split = (n_left, sorted_idx)\n        if best_feat is None:\n            return _TreeNode(value=float(y.mean()), is_leaf=True)\n        (split_idx, sorted_idx) = best_split\n        x_sorted_global = X[sorted_idx, best_feat]\n        mask_left = x_sorted_global <= best_thr\n        left_idx = sorted_idx[mask_left]\n        right_idx = sorted_idx[~mask_left]\n        left_child = self._grow(X[left_idx], y[left_idx], depth + 1)\n        right_child = self._grow(X[right_idx], y[right_idx], depth + 1)\n        return _TreeNode(feature=best_feat, threshold=best_thr, left=left_child, right=right_child, value=float(y.mean()), is_leaf=False)\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int=10, learning_rate: float=0.1, max_depth: int=3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor (simplified).\"\"\"\n    init_pred = y_train.mean()\n    y_hat_train = np.full_like(y_train, init_pred, dtype=float)\n    y_hat_test = np.full(X_test.shape[0], init_pred, dtype=float)\n    for _ in range(n_estimators):\n        residuals = y_train - y_hat_train\n        tree = _CARTRegressor(max_depth=max_depth)\n        tree.fit(X_train, residuals)\n        y_hat_train += learning_rate * tree.predict(X_train)\n        y_hat_test += learning_rate * tree.predict(X_test)\n    return np.round(y_hat_test, 4).tolist()"}
{"task_id": 414, "completion_id": 0, "solution": "import numpy as np\ndef exponential_scheduler(step: int, initial_lr: float=0.01, stage_length: int=500, staircase: bool=False, decay: float=0.1) -> float:\n    \"\"\"Return the exponentially decayed learning-rate for a given optimisation step.\n\n    The learning-rate is computed as:\n\n        lr = initial_lr * decay ** curr_stage\n\n    where ``curr_stage`` is either the real-valued ratio ``step / stage_length``\n    (smooth schedule) or its floor value (stair-case schedule).\n\n    Returns the learning-rate rounded to 6 decimal places or \u20131 if any\n    argument is invalid.\n    \"\"\"\n    if step < 0:\n        return -1\n    if not isinstance(stage_length, int) or stage_length <= 0:\n        return -1\n    if decay <= 0:\n        return -1\n    if staircase:\n        curr_stage = step // stage_length\n    else:\n        curr_stage = step / stage_length\n    lr = initial_lr * decay ** curr_stage\n    return round(float(lr), 6)"}
{"task_id": 415, "completion_id": 0, "solution": "import numpy as np\ndef dct2(frame: list[int | float]) -> list[float]:\n    \"\"\"Un-normalised 1-D Discrete Cosine Transform (type II).\n\n    Parameters\n    ----------\n    frame : list[int | float]\n        Real-valued input samples (length N).\n\n    Returns\n    -------\n    list[float]\n        Length-N list with the DCT-II coefficients, each rounded\n        to 4 decimal places.\n    \"\"\"\n    if not frame:\n        return []\n    x = np.asarray(frame, dtype=float)\n    N = x.size\n    n = np.arange(N).reshape(-1, 1)\n    k = np.arange(N).reshape(1, -1)\n    angles = np.pi / N * (n + 0.5) * k\n    cosines = np.cos(angles)\n    coeffs = (x.reshape(-1, 1) * cosines).sum(axis=0)\n    return [round(float(c), 4) for c in coeffs]"}
{"task_id": 416, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Evaluate the multivariate normal (Gaussian) probability-density function for\n    every sample contained in `X`.\n\n    Parameters\n    ----------\n    X : array_like, shape (n_samples, n_features)  or  (n_samples,)\n        Data points at which the PDF will be evaluated.  If `X` is one\u2013dimensional\n        it is treated as having shape (n_samples, 1).\n    mean : array_like, shape (n_features,)\n        Mean vector (\u03bc) of the distribution.\n    cov : array_like, shape (n_features, n_features)\n        Positive-definite covariance matrix (\u03a3) of the distribution.\n\n    Returns\n    -------\n    list\n        A list whose *i-th* element is the PDF value for `X[i]`, rounded to\n        four decimal places.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    mean = np.asarray(mean, dtype=float)\n    cov = np.asarray(cov, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    if mean.shape[0] != n_features:\n        raise ValueError('Mean vector length must equal the number of features.')\n    if cov.shape != (n_features, n_features):\n        raise ValueError('Covariance matrix shape must be (n_features, n_features).')\n    inv_cov = np.linalg.inv(cov)\n    det_cov = np.linalg.det(cov)\n    if det_cov <= 0:\n        raise ValueError('Covariance matrix must be positive-definite (determinant > 0).')\n    coeff = 1.0 / math.sqrt((2 * math.pi) ** n_features * det_cov)\n    diff = X - mean\n    mahal = np.einsum('ij,jk,ik->i', diff, inv_cov, diff)\n    exponent = -0.5 * mahal\n    pdf_vals = coeff * np.exp(exponent)\n    return [round(val, 4) for val in pdf_vals.tolist()]"}
{"task_id": 417, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int=1) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Perform Principal Component Analysis and return the projected data and its reconstruction.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int, default=1\n        Number of principal components to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[list[float]]]\n        * First element  \u2013 the data expressed in the *k*-dimensional PCA space.\n        * Second element \u2013 the data reconstructed back in the original space.\n\n        Both matrices are rounded to 4 decimal places before being converted\n        to ordinary Python lists.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D NumPy array.')\n    (n_samples, n_features) = data.shape\n    mean = data.mean(axis=0, keepdims=True)\n    Xc = data - mean\n    if n_samples > 1:\n        cov = Xc.T @ Xc / (n_samples - 1)\n    else:\n        cov = np.zeros((n_features, n_features), dtype=Xc.dtype)\n    (eig_vals, eig_vecs) = np.linalg.eigh(cov)\n    idx = np.argsort(eig_vals)[::-1]\n    eig_vecs = eig_vecs[:, idx]\n    k = min(max(int(k), 0), n_features)\n    eig_vecs = eig_vecs[:, :k]\n    for j in range(eig_vecs.shape[1]):\n        vec = eig_vecs[:, j]\n        nz = np.flatnonzero(vec)\n        if nz.size and vec[nz[0]] < 0:\n            eig_vecs[:, j] *= -1\n    low_dim = Xc @ eig_vecs\n    recon = low_dim @ eig_vecs.T + mean\n    low_dim_rounded = np.round(low_dim, 4).tolist()\n    recon_rounded = np.round(recon, 4).tolist()\n    return (low_dim_rounded, recon_rounded)"}
{"task_id": 418, "completion_id": 0, "solution": "import numpy as np\ndef calculate_covariance_matrix(X: np.ndarray | list[list[int | float]]) -> list[list[float]]:\n    \"\"\"Compute and return the sample covariance matrix of *X*.\n\n    Args:\n        X: A 2-D NumPy array or a (nested) Python list whose rows correspond\n           to samples and columns correspond to features.\n\n    Returns:\n        The sample covariance matrix as a nested Python list, with every\n        element rounded to four decimal places.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    if X.ndim != 2:\n        raise ValueError('Input data must be 1-D or 2-D (samples \u00d7 features).')\n    (n_samples, n_features) = X.shape\n    if n_samples < 2:\n        return np.zeros((n_features, n_features), dtype=float).tolist()\n    X_centered = X - X.mean(axis=0, keepdims=True)\n    cov_matrix = X_centered.T @ X_centered / (n_samples - 1)\n    return np.round(cov_matrix, 4).tolist()"}
{"task_id": 419, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef bayesian_linear_regression_pred(X: np.ndarray, y: np.ndarray, X_new: np.ndarray, mu: float | np.ndarray=0.0, sigma: float=1.0, V: float | np.ndarray | None=None, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Bayesian linear regression \u2013 MAP predictive mean (rounded to 4 dp).\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    X_new = np.asarray(X_new, dtype=float)\n    if fit_intercept:\n        ones_train = np.ones((X.shape[0], 1))\n        ones_test = np.ones((X_new.shape[0], 1))\n        X = np.hstack((ones_train, X))\n        X_new = np.hstack((ones_test, X_new))\n    (N, D) = X.shape\n    if np.isscalar(mu):\n        mu_vec = np.full(D, mu, dtype=float)\n    else:\n        mu_vec = np.asarray(mu, dtype=float).ravel()\n        if mu_vec.size != D:\n            raise ValueError('`mu` has incompatible length.')\n    if V is None:\n        V_inv = np.eye(D)\n    elif np.isscalar(V):\n        if V == 0:\n            raise ValueError('Scalar `V` must be non-zero.')\n        V_inv = np.eye(D) / float(V)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 1:\n            if np.any(V_arr == 0):\n                raise ValueError('Zero on the diagonal of `V` is not allowed.')\n            V_inv = np.diag(1.0 / V_arr)\n        else:\n            if V_arr.shape != (D, D):\n                raise ValueError('`V` shape incompatible with number of features.')\n            V_inv = np.linalg.inv(V_arr)\n    XtX = X.T @ X\n    A = V_inv + XtX\n    Sigma_p = np.linalg.inv(A)\n    rhs = V_inv @ mu_vec + X.T @ y\n    mu_p = Sigma_p @ rhs\n    y_pred = X_new @ mu_p\n    return [round(float(val), 4) for val in y_pred]"}
{"task_id": 420, "completion_id": 0, "solution": "import numpy as np\ndef get_split_mask(X: np.ndarray, column: int, value: float) -> tuple[list[bool], list[bool]]:\n    \"\"\"Generate Boolean masks that split rows of *X* on a given threshold.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    column : int\n        Index of the column (feature) used for the split.\n    value : float\n        Threshold value for the split.\n\n    Returns\n    -------\n    tuple[list[bool], list[bool]]\n        A pair *(left_mask, right_mask)* where `left_mask[i]` is `True` iff\n        `X[i, column] < value` and `right_mask[i]` is `True` iff\n        `X[i, column] >= value`.\n\n    Notes\n    -----\n    If *column* is outside the valid range, two empty lists are returned.\n    \"\"\"\n    if not isinstance(X, np.ndarray) or X.ndim != 2 or (not 0 <= column < X.shape[1]):\n        return ([], [])\n    left_mask_np = X[:, column] < value\n    right_mask_np = X[:, column] >= value\n    return (left_mask_np.tolist(), right_mask_np.tolist())"}
{"task_id": 421, "completion_id": 0, "solution": "import numpy as np\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density\u2010Based Spatial Clustering (DBSCAN) from scratch.\n\n    Args\n    ----\n    data : np.ndarray, shape = (n_samples, n_features)\n        The data to cluster.\n    eps : float\n        Maximum distance at which two points are considered neighbours.\n    min_samples : int\n        Minimum number of neighbours (including the point itself) that makes a\n        point a *core* point.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample.  Noise is labelled -1, while clusters\n        are numbered 1, 2, \u2026 in the order they are first encountered during the\n        left-to-right scan over *data*.\n    \"\"\"\n    if eps <= 0:\n        raise ValueError('eps must be positive.')\n    if min_samples <= 0:\n        raise ValueError('min_samples must be a positive integer.')\n    n_samples = data.shape[0]\n    labels = np.zeros(n_samples, dtype=int)\n    cluster_id = 0\n    eps_sq = eps * eps\n\n    def region_query(idx: int) -> np.ndarray:\n        \"\"\"Return indices of all points within *eps* of point *idx* (incl. itself).\"\"\"\n        diff = data - data[idx]\n        dists_sq = np.sum(diff * diff, axis=1)\n        return np.flatnonzero(dists_sq <= eps_sq)\n\n    def expand_cluster(seed_idx: int, neighbours: np.ndarray, cid: int) -> None:\n        \"\"\"Flood-fill the cluster *cid* starting from the core point *seed_idx*.\"\"\"\n        labels[seed_idx] = cid\n        search_queue = list(neighbours)\n        i = 0\n        while i < len(search_queue):\n            point_idx = search_queue[i]\n            if labels[point_idx] == -1:\n                labels[point_idx] = cid\n            elif labels[point_idx] == 0:\n                labels[point_idx] = cid\n                neighbour_pts = region_query(point_idx)\n                if neighbour_pts.size >= min_samples:\n                    for n in neighbour_pts:\n                        if labels[n] == 0 or labels[n] == -1:\n                            search_queue.append(n)\n            i += 1\n    for point_idx in range(n_samples):\n        if labels[point_idx] != 0:\n            continue\n        neighbours = region_query(point_idx)\n        if neighbours.size < min_samples:\n            labels[point_idx] = -1\n        else:\n            cluster_id += 1\n            expand_cluster(point_idx, neighbours, cluster_id)\n    return labels.tolist()"}
{"task_id": 422, "completion_id": 0, "solution": "import numpy as np\ndef _as_python(obj):\n    \"\"\"\n    Helper that converts a NumPy scalar/array into regular Python\n    scalars / (nested) lists after rounding to 4 decimals.\n    \"\"\"\n    obj = np.round(obj, 4)\n    if np.isscalar(obj) or obj.shape == ():\n        return float(obj)\n    return obj.tolist()\ndef softplus(x: 'np.ndarray | list | tuple | float | int', order: int=0):\n    \"\"\"SoftPlus activation function and its derivatives.\n\n    Parameters\n    ----------\n    x : float | int | list | tuple | np.ndarray\n        Input data on which SoftPlus or its derivatives will be computed.\n    order : int, optional\n        0  -> SoftPlus(x)\n        1  -> SoftPlus'(x)\n        2  -> SoftPlus''(x)\n        Any other value makes the function return -1.\n\n    Returns\n    -------\n    float | list\n        Result rounded to 4 decimal places. The shape mirrors the input. If the\n        order is invalid, returns -1.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if order == 0:\n        res = np.logaddexp(0.0, x)\n    elif order == 1:\n        res = 1.0 / (1.0 + np.exp(-x))\n    elif order == 2:\n        sig = 1.0 / (1.0 + np.exp(-x))\n        res = sig * (1.0 - sig)\n    else:\n        return -1\n    return _as_python(res)"}
{"task_id": 423, "completion_id": 0, "solution": "import numpy as np\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute Root Mean Squared Logarithmic Error (RMSLE).\n\n    Returns\n    -------\n    float\n        RMSLE rounded to four decimal places, or -1 if validation fails.\n    \"\"\"\n    if not isinstance(actual, (list, tuple, np.ndarray)) or not isinstance(predicted, (list, tuple, np.ndarray)) or len(actual) != len(predicted) or (len(actual) == 0):\n        return -1\n    try:\n        actual_arr = np.asarray(actual, dtype=float)\n        predicted_arr = np.asarray(predicted, dtype=float)\n    except (ValueError, TypeError):\n        return -1\n    if np.any(actual_arr < 0) or np.any(predicted_arr < 0):\n        return -1\n    log_diff = np.log1p(predicted_arr) - np.log1p(actual_arr)\n    rmsle = np.sqrt(np.mean(np.square(log_diff)))\n    return round(float(rmsle), 4)"}
{"task_id": 424, "completion_id": 0, "solution": "import numpy as np\ndef entropy(y: list[int]) -> float:\n    \"\"\"Compute the Shannon entropy of a label sequence.\n\n    Args:\n        y: A non-empty list of non-negative integers representing class labels.\n\n    Returns:\n        The Shannon entropy of the sequence, rounded to 4 decimal places.\n    \"\"\"\n    labels = np.asarray(y)\n    n = labels.size\n    (_, counts) = np.unique(labels, return_counts=True)\n    probs = counts / n\n    h = -np.sum(probs * np.log2(probs))\n    return float(np.round(h, 4))"}
{"task_id": 425, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef f_entropy(x: list[int] | np.ndarray) -> float:\n    \"\"\"\n    Shannon (natural-log) entropy of a 1-D collection of non-negative integers.\n\n    Parameters\n    ----------\n    x : list[int] | np.ndarray\n        Input data. May be a Python list or a NumPy 1-D array containing\n        non-negative integers.\n\n    Returns\n    -------\n    float\n        Shannon entropy  H = -\u03a3 p_i ln(p_i)  rounded to 4 decimal places.\n        If the input contains only one distinct value (or is empty),\n        the function returns 0.0.\n    \"\"\"\n    arr = np.asarray(x, dtype=int).ravel()\n    if arr.size == 0:\n        return 0.0\n    (unique, counts) = np.unique(arr, return_counts=True)\n    if unique.size == 1:\n        return 0.0\n    probs = counts.astype(float) / arr.size\n    entropy = -np.sum(probs * np.log(probs))\n    return round(float(entropy), 4)"}
{"task_id": 426, "completion_id": 0, "solution": "def classification_accuracy(y_true: list, y_pred: list) -> float:\n    \"\"\"Compute the accuracy between the true and predicted class labels.\n\n    Args:\n        y_true (list): Ground-truth class labels.\n        y_pred (list): Predicted class labels produced by a model.\n\n    Returns:\n        float: The accuracy score rounded to 4 decimal places.\n               Returns -1 when the inputs have different lengths or when\n               either of the inputs is empty.\n    \"\"\"\n    if not y_true or not y_pred or len(y_true) != len(y_pred):\n        return -1\n    correct = sum((1 for (yt, yp) in zip(y_true, y_pred) if yt == yp))\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 427, "completion_id": 0, "solution": "import numpy as np\ndef identity_activation(z):\n    \"\"\"Compute identity activation and its derivative.\n\n    Args:\n        z (list | tuple | numpy.ndarray): Input data (any dimensionality).\n\n    Returns:\n        tuple[list, list]: (activation, derivative) \u2013\n            both are regular Python lists mirroring the shape of *z*.\n    \"\"\"\n    z_arr = np.asarray(z)\n    activation = z_arr.copy()\n    derivative = np.ones_like(z_arr)\n    return (activation.tolist(), derivative.tolist())"}
{"task_id": 428, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef hann(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine\u2010sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places. Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    build_len = window_len if symmetric else window_len + 1\n    n = np.arange(build_len, dtype=float)\n    denom = build_len - 1\n    window = 0.5 - 0.5 * np.cos(2.0 * math.pi * n / denom)\n    if not symmetric:\n        window = window[:-1]\n    window = np.round(window, 4)\n    return window.tolist()"}
{"task_id": 429, "completion_id": 0, "solution": "import numpy as np\ndef oracle_payoff(context_probs: np.ndarray, context: np.ndarray) -> tuple[float, int]:\n    \"\"\"Oracle that returns the optimal expected reward and the corresponding arm.\n\n    Parameters\n    ----------\n    context_probs : np.ndarray\n        2-D array of shape (D, K); row i is the success probabilities of all K\n        arms when the active context is i.\n    context : np.ndarray\n        One-hot vector of length D identifying the current context.\n\n    Returns\n    -------\n    tuple[float, int]\n        (optimal_rwd, optimal_arm): the best achievable expected reward\n        (rounded to four decimals) and the index of the arm that achieves it.\n        If *context* is not a valid one-hot vector the function returns (-1, -1).\n    \"\"\"\n    if context.ndim != 1:\n        return (-1, -1)\n    D = context_probs.shape[0]\n    if len(context) != D:\n        return (-1, -1)\n    if np.count_nonzero(context) != 1:\n        return (-1, -1)\n    unique_vals = np.unique(context)\n    if not np.all(np.isin(unique_vals, [0, 1])):\n        return (-1, -1)\n    ctx_idx = int(np.argmax(context))\n    row = context_probs[ctx_idx]\n    optimal_arm = int(np.argmax(row))\n    optimal_rwd = float(np.round(row[optimal_arm], 4))\n    return (optimal_rwd, optimal_arm)"}
{"task_id": 430, "completion_id": 0, "solution": "from typing import List, Dict, Set, Tuple, Optional\ndef topological_sort(vertices: List, edges: List[Tuple]) -> Optional[List]:\n    \"\"\"Return a deterministic topological ordering of a directed graph.\n\n    The function performs a depth-first search (DFS) that\n    1. starts the search in *vertices* order, left-to-right;\n    2. explores every vertex\u2019s outgoing neighbours in the same left-to-right\n       order induced by *vertices*.\n\n    If during the DFS a back-edge (an edge to a vertex that is still on the\n    recursion stack) is encountered, the graph contains a directed cycle and\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    vertices : list\n        List of vertex identifiers (must be hashable).\n    edges : list[tuple]\n        List of directed edges (u, v) with u \u2192 v.\n\n    Returns\n    -------\n    list\n        A list containing the vertices in deterministic topological order, or\n    None\n        if the graph contains at least one directed cycle.\n    \"\"\"\n    position: Dict = {v: i for (i, v) in enumerate(vertices)}\n    adj: Dict = {v: [] for v in vertices}\n    for (u, v) in edges:\n        if u in position and v in position:\n            adj[u].append(v)\n    for u in adj:\n        adj[u].sort(key=position.__getitem__)\n    state: Dict = {v: 0 for v in vertices}\n    topo: List = []\n\n    def dfs(node) -> bool:\n        \"\"\"Return False on cycle detection, True otherwise.\"\"\"\n        state[node] = 1\n        for nei in adj[node]:\n            if state[nei] == 0:\n                if not dfs(nei):\n                    return False\n            elif state[nei] == 1:\n                return False\n        state[node] = 2\n        topo.append(node)\n        return True\n    for v in vertices:\n        if state[v] == 0:\n            if not dfs(v):\n                return None\n    topo.reverse()\n    return topo"}
{"task_id": 431, "completion_id": 0, "solution": "import numpy as np\ndef dist(x1, x2):\n    \"\"\"Compute the Euclidean distance between two 1-D vectors.\n\n    Parameters\n    ----------\n    x1, x2 : list | tuple | np.ndarray\n        1-D sequences/arrays containing numeric (int/float) values.\n\n    Returns\n    -------\n    float\n        L2 distance rounded to 4 decimal places, or -1 if the vectors\n        have different lengths.\n    \"\"\"\n    try:\n        a = np.asarray(x1, dtype=float).ravel()\n        b = np.asarray(x2, dtype=float).ravel()\n    except (TypeError, ValueError):\n        raise ValueError('Both vectors must contain numeric data only.')\n    if a.size != b.size:\n        return -1\n    dist_val = np.linalg.norm(a - b)\n    return float(round(dist_val, 4))"}
{"task_id": 433, "completion_id": 0, "solution": "import numpy as np\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute Xavier/Glorot fan-in and fan-out values for a weight tensor.\n\n    Args:\n        shape: Tensor shape. For 4-D convolutional kernels the expected layout\n               is (out_channels, in_channels, kernel_height, kernel_width).\n\n    Returns:\n        A tuple (fan_in, fan_out) as floats.\n    \"\"\"\n    if len(shape) == 4:\n        (out_c, in_c, k_h, k_w) = shape[:4]\n        receptive_field_size = k_h * k_w\n        fan_in = in_c * receptive_field_size\n        fan_out = out_c * receptive_field_size\n    else:\n        (fan_in, fan_out) = (shape[0], shape[1])\n    return (float(fan_in), float(fan_out))"}
{"task_id": 434, "completion_id": 0, "solution": "import numpy as np\ndef softmax(logits: np.ndarray) -> list[list[float]]:\n    \"\"\"Compute a numerically-stable, row-wise softmax.\n\n    Parameters\n    ----------\n    logits : np.ndarray\n        1-D array of shape (n,) or 2-D array of shape (m, n)\n        containing the raw scores.\n\n    Returns\n    -------\n    list[list[float]]\n        Row-wise softmax probabilities, each rounded to 6 decimals.\n        Even for a 1-D input, a list containing a single list is returned.\n    \"\"\"\n    logits = np.asarray(logits, dtype=np.float64)\n    if logits.ndim == 1:\n        logits = logits[np.newaxis, :]\n    shifted = logits - np.max(logits, axis=1, keepdims=True)\n    exp_vals = np.exp(shifted)\n    probs = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n    return np.round(probs, 6).tolist()"}
{"task_id": 435, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_regressor(X: list[list[float]], y: list[float], n_estimators: int=10, learning_rate: float=0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear-regression weak learners and squared-error loss.\n    All returned numbers are rounded to 4 decimal places and packed into\n    a plain Python list.\n    \"\"\"\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(-1, 1)\n    m = y_arr.shape[0]\n    y_mean = y_arr.mean()\n    if n_estimators <= 0 or learning_rate == 0:\n        return np.round(np.full(m, y_mean), 4).tolist()\n    current_pred = np.full(m, y_mean, dtype=float)\n    ones = np.ones((m, 1), dtype=float)\n    X_design = np.concatenate((ones, X_arr), axis=1)\n    for _ in range(n_estimators):\n        residuals = y_arr - current_pred\n        (beta, _, _, _) = np.linalg.lstsq(X_design, residuals, rcond=None)\n        h_pred = X_design @ beta\n        current_pred += learning_rate * h_pred\n    return np.round(current_pred, 4).tolist()"}
{"task_id": 437, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x, order: int=0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in (0, 1, 2):\n        return -1\n    arr = np.asarray(x, dtype=float)\n    sig = 1.0 / (1.0 + np.exp(-arr))\n    if order == 0:\n        res = sig\n    elif order == 1:\n        res = sig * (1.0 - sig)\n    else:\n        res = sig * (1.0 - sig) * (1.0 - 2.0 * sig)\n    res = np.round(res, 4)\n    if res.shape == ():\n        return float(res)\n    return res.tolist()"}
{"task_id": 438, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None=None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded representation of *y*, or ``-1`` when the given\n        *num_classes* is smaller than required.\n    \"\"\"\n    y_arr = np.asarray(y, dtype=int).ravel()\n    if y_arr.size == 0:\n        inferred_classes = 0 if num_classes is None else num_classes\n        return [] if inferred_classes == 0 else [[] for _ in range(0)]\n    max_label = int(y_arr.max())\n    if num_classes is None:\n        num_classes = max_label + 1\n    elif num_classes < max_label + 1:\n        return -1\n    one_hot = np.zeros((y_arr.shape[0], num_classes), dtype=int)\n    one_hot[np.arange(y_arr.shape[0]), y_arr] = 1\n    return one_hot.tolist()"}
{"task_id": 439, "completion_id": 0, "solution": "import itertools\nimport math\nfrom collections import defaultdict\nfrom typing import Hashable, List, Tuple\ndef apriori_frequent_itemsets(transactions: List[List[Hashable]], min_sup: float) -> List[Tuple]:\n    \"\"\"Find all frequent item-sets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        The data base: one list per transaction, containing *hashable* items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction in (0, 1].\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets ordered first by their length and then\n        lexicographically inside each length block; inside every tuple the items\n        themselves appear in ascending (lexicographic) order.\n    \"\"\"\n    if not 0 < min_sup <= 1:\n        raise ValueError('min_sup must be in the range (0, 1].')\n    n_transactions = len(transactions)\n    if n_transactions == 0:\n        return []\n    transactions = [set(t) for t in transactions]\n\n    def is_frequent(count: int) -> bool:\n        \"\"\"True if empirical support \u2265 min_sup.\"\"\"\n        return count >= min_sup * n_transactions - 1e-12\n    item_counts: defaultdict[Hashable, int] = defaultdict(int)\n    for t in transactions:\n        for item in t:\n            item_counts[item] += 1\n    L1 = [tuple([item]) for (item, cnt) in item_counts.items() if is_frequent(cnt)]\n    L1.sort()\n    frequent_itemsets: list[tuple] = []\n    frequent_itemsets.extend(L1)\n    prev_L = L1\n    k = 2\n    while prev_L:\n        prev_L_len = len(prev_L)\n        prev_L_set = set(prev_L)\n        Ck: set[tuple] = set()\n        for i in range(prev_L_len):\n            for j in range(i + 1, prev_L_len):\n                (a, b) = (prev_L[i], prev_L[j])\n                if a[:-1] != b[:-1]:\n                    break\n                candidate = tuple(sorted(set(a) | set(b)))\n                if len(candidate) != k:\n                    continue\n                if all((tuple(subset) in prev_L_set for subset in itertools.combinations(candidate, k - 1))):\n                    Ck.add(candidate)\n        if not Ck:\n            break\n        candidate_counts = defaultdict(int)\n        cand_sets = {cand: frozenset(cand) for cand in Ck}\n        for t in transactions:\n            for (cand, cand_set) in cand_sets.items():\n                if cand_set.issubset(t):\n                    candidate_counts[cand] += 1\n        Lk = [cand for (cand, cnt) in candidate_counts.items() if is_frequent(cnt)]\n        Lk.sort()\n        if not Lk:\n            break\n        frequent_itemsets.extend(Lk)\n        prev_L = Lk\n        k += 1\n    return frequent_itemsets"}
{"task_id": 440, "completion_id": 0, "solution": "import numpy as np\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class\u2013probability predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        Shape (n_estimators, n_samples, n_classes).  Each innermost list is a\n        valid probability distribution (sums to 1).\n\n    Returns\n    -------\n    tuple\n        (\n          averaged_probs,      # list[list[float]] shape (n_samples, n_classes)\n          predicted_labels     # list[int]          length == n_samples\n        )\n        `averaged_probs` are rounded to 4 decimal places; `predicted_labels`\n        are obtained with arg-max on the rounded probabilities.  Ties are\n        resolved by the smallest class index (numpy\u2019s default behaviour).\n    \"\"\"\n    if not predictions:\n        raise ValueError('`predictions` must contain at least one estimator.')\n    arr = np.asarray(predictions, dtype=float)\n    if arr.ndim != 3:\n        raise ValueError('`predictions` must be 3-dimensional (n_estimators, n_samples, n_classes).')\n    mean_probs = arr.mean(axis=0)\n    mean_probs = np.round(mean_probs, 4)\n    predicted = np.argmax(mean_probs, axis=1)\n    averaged_probs_list = mean_probs.tolist()\n    predicted_labels_list = predicted.tolist()\n    return (averaged_probs_list, predicted_labels_list)"}
{"task_id": 441, "completion_id": 0, "solution": "import numpy as np\ndef hamming(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a 1-D Hamming window.\n    \n    Parameters\n    ----------\n    window_len : int\n        Number of samples that must be returned.\n    symmetric : bool, optional\n        If True  \u2013 return the symmetric window (for FIR design).\n        If False \u2013 return the periodic window (for FFT analysis).\n\n    Returns\n    -------\n    list[float]\n        The requested window as ordinary Python floats rounded\n        to 4 decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    N = window_len if symmetric else window_len + 1\n    n = np.arange(N, dtype=float)\n    w = 0.54 - 0.46 * np.cos(2 * np.pi * n / (N - 1))\n    if not symmetric:\n        w = w[:-1]\n    return np.round(w, 4).tolist()"}
{"task_id": 442, "completion_id": 0, "solution": "import numpy as np\nimport heapq\ndef knn_kdtree(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> list[int]:\n    \"\"\"k-Nearest Neighbours classification accelerated by a (hand-written) KD-Tree.\"\"\"\n    (n_train, n_features) = X_train.shape\n    indices_all = np.arange(n_train)\n\n    def build(node_indices: np.ndarray):\n        \"\"\"\n        Recursively builds a balanced KD-Tree.\n\n        Every inner node stores:\n            (split_dim, split_val, point_index, left_child, right_child)\n        A leaf is represented by None.\n        \"\"\"\n        if node_indices.size == 0:\n            return None\n        pts = X_train[node_indices]\n        split_dim = np.argmax(pts.var(axis=0))\n        order = np.argsort(pts[:, split_dim])\n        ordered_idx = node_indices[order]\n        mid = ordered_idx.size // 2\n        point_idx = ordered_idx[mid]\n        left_child = build(ordered_idx[:mid])\n        right_child = build(ordered_idx[mid + 1:])\n        split_val = X_train[point_idx, split_dim]\n        return (split_dim, split_val, point_idx, left_child, right_child)\n    root = build(indices_all)\n    predictions: list[int] = []\n\n    def knn_search(node, query_point, heap):\n        \"\"\"\n        Depth-first search with back-tracking & pruning.\n\n        'heap' is a max-heap (implemented as a min-heap of negative distances)\n        that stores at most k entries (-dist_sq, index).\n        \"\"\"\n        if node is None:\n            return\n        (split_dim, split_val, p_idx, left, right) = node\n        if query_point[split_dim] <= split_val:\n            (near, far) = (left, right)\n        else:\n            (near, far) = (right, left)\n        knn_search(near, query_point, heap)\n        dist_sq = float(np.sum((query_point - X_train[p_idx]) ** 2))\n        heapq.heappush(heap, (-dist_sq, p_idx))\n        if len(heap) > k:\n            heapq.heappop(heap)\n        worst_sq = -heap[0][0] if len(heap) == k else float('inf')\n        plane_dist_sq = (query_point[split_dim] - split_val) ** 2\n        if plane_dist_sq < worst_sq:\n            knn_search(far, query_point, heap)\n    for q in X_test:\n        heap: list[tuple[float, int]] = []\n        knn_search(root, q, heap)\n        neighbour_indices = [idx for (_, idx) in heap]\n        vote = np.argmax(np.bincount(y_train[neighbour_indices]))\n        predictions.append(int(vote))\n    return predictions"}
{"task_id": 443, "completion_id": 0, "solution": "import numpy as np\nfrom numbers import Number\ndef _is_numeric(value) -> bool:\n    \"\"\"\n    Decide whether *value* should be treated as \u201cnumeric\u201d **for this task**.\n    Booleans are deliberately excluded although they are subclasses of int.\n    \"\"\"\n    return isinstance(value, Number) and (not isinstance(value, bool))\ndef divide_on_feature(X: np.ndarray, feature_i: int, threshold):\n    \"\"\"Split *X* into two NumPy matrices according to feature *feature_i* and *threshold*.\n\n    A row is sent to the *left* subset iff\n        \u2022 threshold is numeric  ->  X[row, feature_i] >= threshold\n        \u2022 threshold is non-numeric -> X[row, feature_i] == threshold\n\n    Args:\n        X: 2-D NumPy array. Each row is a sample, each column a feature.\n        feature_i: Index of the column that is tested.\n        threshold: Value that determines the split (numeric or categorical).\n\n    Returns:\n        (left_subset, right_subset): tuple of NumPy arrays with the same\n        number of columns as *X*.\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D NumPy array')\n    col = X[:, feature_i]\n    if _is_numeric(threshold):\n        mask = col >= threshold\n    else:\n        mask = col == threshold\n    left_subset = X[mask]\n    right_subset = X[~mask]\n    if left_subset.size == 0:\n        left_subset = left_subset.reshape(0, X.shape[1])\n    if right_subset.size == 0:\n        right_subset = right_subset.reshape(0, X.shape[1])\n    return (left_subset, right_subset)"}
{"task_id": 444, "completion_id": 0, "solution": "import numpy as np\ndef rbf_kernel(X: list[list[int | float]], Y: list[list[int | float]] | None=None, sigma: float | list[float] | None=None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function follows the specifications given in the task description.\n    It returns -1 on invalid input, otherwise a nested list containing the\n    kernel matrix rounded to four decimal places.\n    \"\"\"\n\n    def _to_2d_array(mat):\n        try:\n            arr = np.asarray(mat, dtype=float)\n        except Exception:\n            return None\n        if arr.ndim != 2 or arr.shape[1] == 0:\n            return None\n        return arr\n    X_arr = _to_2d_array(X)\n    if X_arr is None:\n        return -1\n    (n_samples, n_features) = X_arr.shape\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        Y_arr = _to_2d_array(Y)\n        if Y_arr is None or Y_arr.shape[1] != n_features:\n            return -1\n    if sigma is None:\n        sigma_scalar = np.sqrt(n_features / 2.0)\n        if sigma_scalar <= 0:\n            return -1\n        sigma_vec = None\n    elif isinstance(sigma, (int, float, np.floating)):\n        sigma_scalar = float(sigma)\n        if sigma_scalar <= 0:\n            return -1\n        sigma_vec = None\n    else:\n        try:\n            sigma_vec = np.asarray(sigma, dtype=float)\n        except Exception:\n            return -1\n        if sigma_vec.ndim != 1 or sigma_vec.shape[0] != n_features:\n            return -1\n        if np.any(sigma_vec <= 0):\n            return -1\n        sigma_scalar = None\n    if sigma_vec is None:\n        diff = X_arr[:, None, :] - Y_arr[None, :, :]\n        dist2 = np.sum(diff ** 2, axis=-1) / sigma_scalar ** 2\n    else:\n        diff = (X_arr[:, None, :] - Y_arr[None, :, :]) / sigma_vec\n        dist2 = np.sum(diff ** 2, axis=-1)\n    K = np.exp(-0.5 * dist2)\n    return np.round(K, 4).tolist()"}
{"task_id": 446, "completion_id": 0, "solution": "import numpy as np\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find the best 1-level split (decision stump) by information gain.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    (n_samples, n_features) = X.shape\n    (classes, y_indices) = np.unique(y, return_inverse=True)\n    n_classes = len(classes)\n\n    def entropy_from_counts(counts: np.ndarray) -> float:\n        \"\"\"Shannon entropy (base 2) from a 1-D array with class counts.\"\"\"\n        total = counts.sum()\n        if total == 0:\n            return 0.0\n        probs = counts[counts > 0] / total\n        return -np.sum(probs * np.log2(probs))\n\n    def majority_label(counts: np.ndarray) -> int:\n        \"\"\"Most frequent label, ties resolved by smaller label value.\"\"\"\n        max_count = counts.max()\n        winner_idx = np.where(counts == max_count)[0][0]\n        return int(classes[winner_idx])\n    total_counts = np.bincount(y_indices, minlength=n_classes)\n    parent_entropy = entropy_from_counts(total_counts)\n    if parent_entropy == 0.0:\n        maj = majority_label(total_counts)\n        return (-1, None, maj, maj)\n    best_gain = -np.inf\n    best_feature = -1\n    best_threshold = None\n    best_left_label = None\n    best_right_label = None\n    eps = 1e-12\n    for j in range(n_features):\n        col = X[:, j]\n        order = np.argsort(col)\n        sorted_vals = col[order]\n        sorted_y_idx = y_indices[order]\n        left_counts = np.zeros(n_classes, dtype=int)\n        for i in range(n_samples - 1):\n            left_counts[sorted_y_idx[i]] += 1\n            if sorted_vals[i] == sorted_vals[i + 1]:\n                continue\n            thr = (sorted_vals[i] + sorted_vals[i + 1]) / 2.0\n            n_left = i + 1\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:\n                continue\n            right_counts = total_counts - left_counts\n            h_left = entropy_from_counts(left_counts)\n            h_right = entropy_from_counts(right_counts)\n            p_left = n_left / n_samples\n            p_right = 1.0 - p_left\n            gain = parent_entropy - p_left * h_left - p_right * h_right\n            better_gain = gain > best_gain + eps\n            same_gain = abs(gain - best_gain) <= eps\n            update = False\n            if better_gain:\n                update = True\n            elif same_gain:\n                if j < best_feature:\n                    update = True\n                elif j == best_feature and (best_threshold is None or thr < best_threshold - eps):\n                    update = True\n            if update:\n                best_gain = gain\n                best_feature = j\n                best_threshold = thr\n                best_left_label = majority_label(left_counts)\n                best_right_label = majority_label(right_counts)\n    if best_gain <= eps:\n        maj = majority_label(total_counts)\n        return (-1, None, maj, maj)\n    return (best_feature, round(best_threshold, 4), best_left_label, best_right_label)"}
{"task_id": 447, "completion_id": 0, "solution": "import numpy as np\ndef one(shape, **kwargs):\n    \"\"\"Create a nested Python list filled with ones.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired shape of the output list. Works exactly as the first argument of\n        ``numpy.ones``.\n    **kwargs : dict\n        Optional keyword arguments forwarded to ``numpy.ones`` (e.g. ``dtype``).\n\n    Returns\n    -------\n    list\n        A (possibly nested) Python list containing only ones that mirrors the\n        provided shape.\n    \"\"\"\n    return np.ones(shape, **kwargs).tolist()"}
{"task_id": 448, "completion_id": 0, "solution": "import numpy as np\ndef mean_baseline_predict(X_train, y_train, X_test):\n    \"\"\"Baseline regressor that predicts the mean of the training targets.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples_train, n_features_train).\n        (Only its length is relevant for this baseline.)\n    y_train : np.ndarray\n        One-dimensional array of training targets of length n_samples_train.\n    X_test : np.ndarray\n        Test feature matrix of shape (n_samples_test, n_features_test).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every row in ``X_test`` rounded to 4 decimal places.\n        If ``y_train`` is empty, an empty list is returned.\n    \"\"\"\n    y_train = np.asarray(y_train)\n    if y_train.size == 0:\n        return []\n    mean_val = np.mean(y_train)\n    preds = np.full(X_test.shape[0], mean_val, dtype=float)\n    preds = np.round(preds, 4)\n    return preds.tolist()"}
{"task_id": 449, "completion_id": 0, "solution": "import numpy as np\ndef gda(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Train a (simplified) Gaussian Discriminant Analysis model and predict labels.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples, shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary training labels (0/1), length n_samples.\n    X_test : np.ndarray\n        Test samples, shape (k_samples, n_features).\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for every row in `X_test`.\n    \"\"\"\n    (X0, X1) = (X_train[y_train == 0], X_train[y_train == 1])\n    Mu0 = X0.mean(axis=0)\n    Mu1 = X1.mean(axis=0)\n    X_sub_Mu = np.vstack([X0 - Mu0, X1 - Mu1])\n    m = X_train.shape[1]\n    Sigma = 1.0 / m * (X_sub_Mu.T @ X_sub_Mu)\n    normal_vec = Mu1 - Mu0\n    norm = np.linalg.norm(normal_vec)\n    w = normal_vec / norm if norm != 0 else np.zeros_like(normal_vec)\n    b = -np.dot(w.T, (Mu0 + Mu1) / 2.0)\n    sign = int(np.dot(w.T, Mu1) + b > 0)\n    scores = X_test @ w + b\n    preds = (scores > 0).astype(int) * sign\n    return preds.astype(int).tolist()"}
{"task_id": 450, "completion_id": 0, "solution": "import numpy as np\ndef majority_vote(preds: np.ndarray) -> list[int]:\n    \"\"\"Aggregate ensemble predictions using majority voting.\n\n    Parameters\n    ----------\n    preds : np.ndarray\n        A 2-D NumPy array of shape (n_samples, n_estimators) whose entries are\n        *non-negative integer* class labels predicted by the individual\n        estimators.\n\n    Returns\n    -------\n    list[int]\n        The chosen class label for every sample.  In case of a tie the smallest\n        label is returned (NumPy\u2019s argmax behaviour).\n    \"\"\"\n    if preds.ndim != 2:\n        raise ValueError('`preds` must be a 2-D array of shape (n_samples, n_estimators).')\n    winners = [int(np.bincount(row).argmax()) for row in preds]\n    return winners"}
{"task_id": 451, "completion_id": 0, "solution": "import math\nimport heapq\nfrom collections import Counter\nfrom typing import Optional, List\nimport numpy as np\ndef knn_kdtree_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> list[int]:\n    \"\"\"Classify *X_test* with a KD-Tree based exact k-NN search (Euclidean).\n\n    When two or more labels are equally frequent among the *k* neighbours,\n    the smallest label is returned.\n\n    If *X_train* is empty or *k* \u2264 0 an empty list is returned.\n    \"\"\"\n\n    class _KDNode:\n        \"\"\"Light-weight KD-Tree node.\"\"\"\n        __slots__ = ('point', 'label', 'axis', 'left', 'right')\n\n        def __init__(self, point: np.ndarray, label: int, axis: int, left: Optional['._KDNode'], right: Optional['._KDNode']):\n            self.point = point\n            self.label = label\n            self.axis = axis\n            self.left = left\n            self.right = right\n\n    def _build(points: np.ndarray, labels: np.ndarray, depth: int=0) -> Optional[_KDNode]:\n        \"\"\"Recursively build a balanced KD-Tree.\"\"\"\n        if points.shape[0] == 0:\n            return None\n        axis = depth % points.shape[1]\n        idx = np.argsort(points[:, axis])\n        mid = len(idx) // 2\n        return _KDNode(point=points[idx[mid]], label=int(labels[idx[mid]]), axis=axis, left=_build(points[idx[:mid]], labels[idx[:mid]], depth + 1), right=_build(points[idx[mid + 1:]], labels[idx[mid + 1:]], depth + 1))\n\n    def _knn_search(node: Optional[_KDNode], query: np.ndarray, heap: List[tuple]):\n        \"\"\"Fill *heap* with the k closest points to *query* found below *node*.\"\"\"\n        if node is None:\n            return\n        diff = query[node.axis] - node.point[node.axis]\n        (near_branch, far_branch) = (node.left, node.right) if diff < 0 else (node.right, node.left)\n        _knn_search(near_branch, query, heap)\n        dist_sq = float(np.sum((node.point - query) ** 2))\n        if len(heap) < k:\n            heapq.heappush(heap, (-dist_sq, node.label))\n        elif dist_sq < -heap[0][0]:\n            heapq.heapreplace(heap, (-dist_sq, node.label))\n        if len(heap) < k or diff * diff < -heap[0][0]:\n            _knn_search(far_branch, query, heap)\n    if k <= 0 or X_train.size == 0:\n        return []\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    X_test = np.asarray(X_test, dtype=float)\n    k = min(k, X_train.shape[0])\n    root = _build(X_train, y_train)\n    predictions: list[int] = []\n    for query in X_test:\n        heap: list[tuple[float, int]] = []\n        _knn_search(root, query, heap)\n        labels = [lbl for (_, lbl) in heap]\n        counts = Counter(labels)\n        max_votes = max(counts.values())\n        chosen = min((lbl for (lbl, cnt) in counts.items() if cnt == max_votes))\n        predictions.append(chosen)\n    return predictions"}
{"task_id": 452, "completion_id": 0, "solution": "import numbers\nimport numpy as np\nfrom typing import List\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split *X* into two subsets according to one feature and a threshold.\n\n    For a numeric *threshold* (int / float) the first subset contains every\n    sample whose value in column *feature_i* is >= threshold, otherwise the\n    first subset contains every sample whose value equals *threshold*.\n\n    The second subset gets all remaining samples.  The original row order is\n    preserved and both subsets are returned as regular Python lists.\n    \"\"\"\n    is_numeric = isinstance(threshold, numbers.Real) and (not isinstance(threshold, bool))\n    (left, right) = ([], [])\n    for row in X:\n        value = row[feature_i]\n        if is_numeric:\n            goes_left = value >= threshold\n        else:\n            goes_left = value == threshold\n        (left if goes_left else right).append(row)\n    left_out = np.asarray(left, dtype=object).tolist() if left else []\n    right_out = np.asarray(right, dtype=object).tolist() if right else []\n    return [left_out, right_out]"}
{"task_id": 453, "completion_id": 0, "solution": "import numpy as np\ndef update_q_values(Q: 'np.ndarray', Q_next: 'np.ndarray', actions: list[int], rewards: list[float], dones: list[bool], gamma: float) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch (see doc-string above).\"\"\"\n    if not 0.0 <= gamma <= 1.0:\n        return -1\n    if not (isinstance(Q, np.ndarray) and isinstance(Q_next, np.ndarray)):\n        return -1\n    if Q.shape != Q_next.shape or Q.ndim != 2:\n        return -1\n    (batch_size, n_actions) = Q.shape\n    if not len(actions) == len(rewards) == len(dones) == batch_size:\n        return -1\n    if any((a < 0 or a >= n_actions for a in actions)):\n        return -1\n    targets = Q.copy()\n    for i in range(batch_size):\n        if dones[i]:\n            target = rewards[i]\n        else:\n            target = rewards[i] + gamma * np.max(Q_next[i])\n        targets[i, actions[i]] = target\n    targets = np.round(targets, 4).tolist()\n    return targets"}
{"task_id": 454, "completion_id": 0, "solution": "import numpy as np\ndef generate_dcgan_labels(batch_size: int) -> tuple[list[list[int]], list[list[int]], list[list[int]]]:\n    \"\"\"Generate the fixed one-hot label arrays used in DCGAN training.\n\n    The function returns (valid_half, fake_half, valid_full), each\n    converted to a plain Python list via ``tolist()``.\n\n    Args:\n        batch_size: A positive, even integer indicating the total number\n            of samples in one training batch.\n\n    Returns:\n        Tuple containing three Python lists with one-hot vectors.\n    \"\"\"\n    half = batch_size // 2\n    valid_half = np.tile([1, 0], (half, 1))\n    fake_half = np.tile([0, 1], (half, 1))\n    valid_full = np.tile([1, 0], (batch_size, 1))\n    return (valid_half.tolist(), fake_half.tolist(), valid_full.tolist())"}
{"task_id": 455, "completion_id": 0, "solution": "import math\nimport heapq\nimport numpy as np\nfrom collections import Counter, namedtuple\n_KDNode = namedtuple('_KDNode', ['point', 'label', 'axis', 'left', 'right'])\ndef _build_kdtree(points, labels, depth=0):\n    \"\"\"Returns root node of a balanced KD-Tree built from the data\"\"\"\n    if len(points) == 0:\n        return None\n    n_features = points.shape[1]\n    axis = depth % n_features\n    median_idx = len(points) // 2\n    idx = np.argpartition(points[:, axis], median_idx)\n    idx_med = idx[median_idx]\n    point = points[idx_med]\n    label = labels[idx_med]\n    left = _build_kdtree(points[idx[:median_idx]], labels[idx[:median_idx]], depth + 1)\n    right = _build_kdtree(points[idx[median_idx + 1:]], labels[idx[median_idx + 1:]], depth + 1)\n    return _KDNode(point, label, axis, left, right)\ndef _kdtree_search(node, query, k, heap):\n    \"\"\"Best-first traversal that keeps the k closest points in *heap*.\n\n    *heap* is a max-heap implemented through a min-heap that stores\n    (-distance, label).  The root of the heap is therefore the farthest\n    neighbour currently kept.\n    \"\"\"\n    if node is None:\n        return\n    dist = float(np.sum((node.point - query) ** 2))\n    if len(heap) < k:\n        heapq.heappush(heap, (-dist, node.label))\n    elif dist < -heap[0][0]:\n        heapq.heapreplace(heap, (-dist, node.label))\n    axis = node.axis\n    diff = query[axis] - node.point[axis]\n    (first, second) = (node.left, node.right) if diff < 0 else (node.right, node.left)\n    _kdtree_search(first, query, k, heap)\n    if len(heap) < k or diff * diff < -heap[0][0]:\n        _kdtree_search(second, query, k, heap)\ndef _vote(labels):\n    \"\"\"Return the majority label; tie is resolved in favour of the\n    numerically smallest label.\"\"\"\n    counts = Counter(labels)\n    max_count = max(counts.values())\n    winners = [lab for (lab, c) in counts.items() if c == max_count]\n    return min(winners)\ndef knn_classifier(X_train, y_train, X_query, k, method='naive'):\n    \"\"\"k-Nearest Neighbours classifier supporting three search strategies.\n\n    Parameters\n    ----------\n    X_train : (n_samples, n_features) array_like\n    y_train : (n_samples,) array_like of int\n    X_query : (n_queries, n_features) array_like\n    k       : int\n    method  : {\"naive\", \"heap\", \"kdtree\"}\n\n    Returns\n    -------\n    list[int] \u2013 predicted labels for the queries\n    \"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    X_query = np.asarray(X_query, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    (n_train, n_feat) = X_train.shape\n    if k <= 0:\n        raise ValueError('k must be >= 1')\n    k_eff = min(k, n_train)\n    preds = []\n    if method == 'naive':\n        for q in X_query:\n            dists = np.sum((X_train - q) ** 2, axis=1)\n            idx = np.argsort(dists)[:k_eff]\n            preds.append(_vote(y_train[idx]))\n    elif method == 'heap':\n        for q in X_query:\n            heap = []\n            for (xi, yi) in zip(X_train, y_train):\n                d = float(np.sum((xi - q) ** 2))\n                if len(heap) < k_eff:\n                    heapq.heappush(heap, (-d, yi))\n                elif d < -heap[0][0]:\n                    heapq.heapreplace(heap, (-d, yi))\n            preds.append(_vote([lab for (_, lab) in heap]))\n    elif method == 'kdtree':\n        root = _build_kdtree(X_train, y_train)\n        for q in X_query:\n            heap = []\n            _kdtree_search(root, q, k_eff, heap)\n            preds.append(_vote([lab for (_, lab) in heap]))\n    else:\n        raise ValueError(\"Unknown method: choose 'naive', 'heap' or 'kdtree'.\")\n    return preds"}
{"task_id": 456, "completion_id": 0, "solution": "import numpy as np\ndef calculate_covariance_matrix(X: np.ndarray, Y: np.ndarray | None=None) -> list[list[float]]:\n    \"\"\"Compute the unbiased sample covariance (or cross-covariance) matrix.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        (n_samples \u00d7 n_features) data matrix.\n    Y : np.ndarray | None, optional\n        (n_samples \u00d7 m_features) data matrix. If None, the covariance\n        of X with itself is returned.\n\n    Returns\n    -------\n    list[list[float]] | int\n        Covariance matrix rounded to 4 decimals, or -1 for invalid input.\n    \"\"\"\n    X = np.asarray(X)\n    n_samples = X.shape[0]\n    if n_samples < 2:\n        return -1\n    if Y is None:\n        Y = X\n    else:\n        Y = np.asarray(Y)\n        if Y.shape[0] != n_samples:\n            return -1\n    Xc = X - X.mean(axis=0, keepdims=True)\n    Yc = Y - Y.mean(axis=0, keepdims=True)\n    cov = Xc.T @ Yc / (n_samples - 1)\n    return np.round(cov, 4).tolist()"}
{"task_id": 457, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_penalty(weights, alpha):\n    \"\"\"Compute the Elastic-Net penalty for a set of weights.\n\n    Parameters\n    ----------\n    weights : list | tuple | np.ndarray\n        1-D collection of numerical weights.\n    alpha : float\n        Mixing parameter. Must satisfy 0 \u2264 alpha \u2264 1.  If outside this\n        range the function returns -1.\n\n    Returns\n    -------\n    float | int\n        Elastic-Net penalty rounded to 4 decimals, or -1 if *alpha* is\n        outside the valid interval.\n    \"\"\"\n    if not 0.0 <= alpha <= 1.0:\n        return -1\n    w = np.asarray(weights, dtype=float)\n    l2_term = 0.5 * alpha * np.sum(w ** 2)\n    l1_term = (1.0 - alpha) * np.sum(np.abs(w))\n    penalty = l2_term + l1_term\n    return float(round(penalty, 4))"}
{"task_id": 458, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None=None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes.  If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists (shape: (len(labels), num_classes)) containing\n        0/1 integers, or -1 when:\n            \u2022 any label is negative, or\n            \u2022 `num_classes` is provided but smaller than `max(labels)+1`.\n    \"\"\"\n    try:\n        labels_list = list(labels)\n    except TypeError:\n        return -1\n    if not labels_list:\n        inferred_classes = 0 if num_classes is None else num_classes\n        return [] if inferred_classes >= 0 else -1\n    max_label = -1\n    for lbl in labels_list:\n        if not isinstance(lbl, (int, np.integer)) or lbl < 0:\n            return -1\n        if lbl > max_label:\n            max_label = int(lbl)\n    if num_classes is None:\n        num_classes = max_label + 1\n    elif num_classes < max_label + 1:\n        return -1\n    one_hot: list[list[int]] = [[1 if class_idx == lbl else 0 for class_idx in range(num_classes)] for lbl in labels_list]\n    return one_hot"}
{"task_id": 459, "completion_id": 0, "solution": "import numpy as np\nimport heapq\ndef knn_validate_heap(X_train: 'np.ndarray', Y_train: 'np.ndarray', X_val: 'np.ndarray', Y_val: 'np.ndarray', k: int) -> tuple[float, list[int]]:\n    \"\"\"Validate a k-NN classifier using a heap-based neighbour selection.\"\"\"\n    predicted = []\n    Y_train = np.asarray(Y_train, dtype=int)\n    Y_val = np.asarray(Y_val, dtype=int)\n    for x in X_val:\n        diff = X_train - x\n        dists = np.einsum('ij,ij->i', diff, diff)\n        heap: list[tuple[float, int]] = []\n        for (idx, dist) in enumerate(dists):\n            nd = -dist\n            if len(heap) < k:\n                heapq.heappush(heap, (nd, idx))\n            elif nd > heap[0][0]:\n                heapq.heapreplace(heap, (nd, idx))\n        neighbour_indices = [idx for (_, idx) in heap]\n        votes = Y_train[neighbour_indices]\n        pred_label = int(np.bincount(votes).argmax())\n        predicted.append(pred_label)\n    accuracy = round((np.array(predicted) == Y_val).mean(), 4)\n    return (accuracy, predicted)"}
{"task_id": 460, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X: np.ndarray, Y: np.ndarray | None=None) -> list | int:\n    \"\"\"Compute the Pearson correlation matrix between the columns of *X* and *Y*.\n\n    If *Y* is omitted the auto-correlation \u03c1(X,X) is returned.\n    If any column of *X* or *Y* has a population standard deviation of zero\n    the correlation is undefined and the function returns -1.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        (n_samples, n_features_X) data matrix.\n    Y : np.ndarray | None, optional\n        (n_samples, n_features_Y) data matrix; must have the same number of\n        rows as *X* when supplied.\n\n    Returns\n    -------\n    list | int\n        Correlation matrix (Python list of lists, rounded to 4 decimals)\n        or -1 if the computation is impossible.\n    \"\"\"\n    if X.ndim != 2:\n        raise ValueError('X must be two-dimensional')\n    n_samples = X.shape[0]\n    if Y is None:\n        Y = X\n    else:\n        if Y.ndim != 2:\n            raise ValueError('Y must be two-dimensional')\n        if Y.shape[0] != n_samples:\n            raise ValueError('X and Y must have the same number of rows')\n    Xc = X - np.mean(X, axis=0, keepdims=True)\n    Yc = Y - np.mean(Y, axis=0, keepdims=True)\n    std_X = np.sqrt(np.mean(Xc ** 2, axis=0))\n    std_Y = np.sqrt(np.mean(Yc ** 2, axis=0))\n    if np.any(std_X == 0) or np.any(std_Y == 0):\n        return -1\n    cov = Xc.T @ Yc / n_samples\n    corr = cov / (std_X[:, None] * std_Y[None, :])\n    corr_rounded = np.round(corr, 4).tolist()\n    return corr_rounded"}
{"task_id": 461, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_regressor(x: list[float], y: list[float], n_estimators: int=200, learning_rate: float=0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\"\"\"\n    x_arr = np.asarray(x, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    if n_estimators <= 0:\n        return np.round(np.full_like(y_arr, y_arr.mean()), 4).tolist()\n    y_pred = np.full_like(y_arr, y_arr.mean(), dtype=float)\n    uniq_x = np.unique(x_arr)\n    for _ in range(n_estimators):\n        residuals = y_arr - y_pred\n        if np.allclose(residuals, 0.0):\n            break\n        best_sse = np.inf\n        best_t = None\n        best_left_mean = best_right_mean = 0.0\n        thresholds = uniq_x[:-1]\n        for t in thresholds:\n            left_mask = x_arr <= t\n            right_mask = ~left_mask\n            if not left_mask.any() or not right_mask.any():\n                continue\n            r_left = residuals[left_mask]\n            r_right = residuals[right_mask]\n            (m_left, m_right) = (r_left.mean(), r_right.mean())\n            sse = np.sum((r_left - m_left) ** 2) + np.sum((r_right - m_right) ** 2)\n            if sse < best_sse:\n                best_sse = sse\n                best_t = t\n                (best_left_mean, best_right_mean) = (m_left, m_right)\n        if best_t is None:\n            stump_pred = np.full_like(residuals, residuals.mean())\n        else:\n            stump_pred = np.where(x_arr <= best_t, best_left_mean, best_right_mean)\n        y_pred += learning_rate * stump_pred\n    return np.round(y_pred, 4).tolist()"}
{"task_id": 462, "completion_id": 0, "solution": "import numpy as np\ndef cepstral_lifter(mfccs: np.ndarray, D: int) -> np.ndarray:\n    \"\"\"Apply a sinusoidal cepstral lifter to a matrix of MFCC coefficients.\n\n    Args:\n        mfccs: NumPy array of shape (G, C) containing MFCC coefficients.\n        D: Non-negative integer lifter parameter.  \n           A value of 0 or 1 returns the input unchanged.\n\n    Returns:\n        NumPy array with the same shape and dtype as *mfccs* containing the\n        liftered coefficients.\n    \"\"\"\n    if D < 0:\n        raise ValueError('D has to be a non-negative integer.')\n    if mfccs.ndim != 2:\n        raise ValueError('mfccs must be a 2-D array of shape (G, C).')\n    if D <= 1:\n        return mfccs.copy()\n    (G, C) = mfccs.shape\n    n = np.arange(C, dtype=mfccs.dtype)\n    lifter = 1.0 + D / 2.0 * np.sin(np.pi * n / D)\n    lifter = lifter.astype(mfccs.dtype, copy=False)\n    return mfccs * lifter"}
{"task_id": 463, "completion_id": 0, "solution": "import math\nfrom typing import List\ndef compute_ball_radius(centroid: List[float], X: List[List[float]]) -> float:\n    \"\"\"Return the Ball-tree node radius, i.e. the largest Euclidean distance\n    between *centroid* and any point in *X*, rounded to 4 decimals.\n\n    Parameters\n    ----------\n    centroid : List[float]\n        The node\u2019s centroid (a single point).\n    X : List[List[float]]\n        Points belonging to the node.\n\n    Returns\n    -------\n    float\n        The radius rounded to 4 decimal places. 0.0 if *X* is empty.\n    \"\"\"\n    if not X:\n        return 0.0\n    max_sq_dist = 0.0\n    for point in X:\n        sq_dist = sum(((c - p) ** 2 for (c, p) in zip(centroid, point)))\n        if sq_dist > max_sq_dist:\n            max_sq_dist = sq_dist\n    radius = math.sqrt(max_sq_dist)\n    return round(radius, 4)"}
{"task_id": 464, "completion_id": 0, "solution": "import numpy as np\ndef knn_validate(X_train: np.ndarray, Y_train: np.ndarray, X_val: np.ndarray, Y_val: np.ndarray, k: int) -> tuple[float, list[int]]:\n    \"\"\"k-Nearest Neighbours (k-NN) validation helper.\"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    X_val = np.asarray(X_val, dtype=float)\n    Y_train = np.asarray(Y_train, dtype=int)\n    Y_val = np.asarray(Y_val, dtype=int)\n    n_val = X_val.shape[0]\n    predictions = []\n    diff = X_val[:, None, :] - X_train[None, :, :]\n    dists = np.sum(diff ** 2, axis=2)\n    for i in range(n_val):\n        nn_idx = np.argpartition(dists[i], k)[:k]\n        nn_labels = Y_train[nn_idx]\n        label_counts = np.bincount(nn_labels)\n        pred_label = int(np.argmax(label_counts))\n        predictions.append(pred_label)\n    correct = np.sum(np.array(predictions) == Y_val)\n    accuracy = round(correct / n_val if n_val else 0.0, 4)\n    return (accuracy, predictions)"}
{"task_id": 465, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef rbf_kernel(gamma: float, **kwargs):\n    \"\"\"Create and return a Gaussian RBF kernel function.\n\n    The returned function takes two 1-D vectors and computes the Gaussian\n    Radial Basis Function (RBF) similarity between them.\n\n    Args:\n        gamma: A non-negative real number that controls the width of the RBF\n               kernel. A larger gamma makes the kernel decay faster.\n        **kwargs: Optional keyword arguments kept for interface compatibility\n                   (not used in the computation).\n\n    Returns:\n        Function ``f(x1, x2)`` such that ``f`` returns the value of\n        ``exp( -gamma * ||x1 - x2||^2 )`` rounded to four decimal places.\n    \"\"\"\n    if gamma < 0:\n        raise ValueError('gamma must be non-negative')\n\n    def f(x1, x2):\n        \"\"\"Gaussian RBF similarity between two 1-D vectors.\"\"\"\n        a = np.asarray(x1, dtype=float).ravel()\n        b = np.asarray(x2, dtype=float).ravel()\n        if a.shape != b.shape:\n            raise ValueError('Input vectors must have the same length')\n        if gamma == 0:\n            return 1.0\n        diff = a - b\n        dist_sq = np.dot(diff, diff)\n        sim = np.exp(-gamma * dist_sq)\n        return round(float(sim), 4)\n    return f"}
{"task_id": 466, "completion_id": 0, "solution": "import string\nimport string\ndef strip_punctuation(line: str) -> str:\n    \"\"\"Remove punctuation characters from a string.\n\n    The function deletes all characters listed in ``string.punctuation`` and\n    trims surrounding whitespace.\n\n    Args:\n        line (str): Text that may contain punctuation symbols.\n\n    Returns:\n        str: Text with punctuation removed and stripped of leading/trailing\n        whitespace.\n    \"\"\"\n    return line.translate(_PUNC_TABLE).strip()"}
{"task_id": 467, "completion_id": 0, "solution": "import numpy as np\ndef regularization(weights: list[float | int], C: float=0.01, norm: str='l2') -> tuple[float, list[float]] | int:\n    \"\"\"Return the regularisation penalty and gradient for a list of weights.\n\n    Parameters\n    ----------\n    weights : list[float | int]\n        Vector of model weights.\n    C : float, default 0.01\n        Regularisation strength.\n    norm : str, {\"l1\", \"l2\"}, default \"l2\"\n        Type of regularisation to apply.\n\n    Returns\n    -------\n    tuple[float, list[float]]\n        (penalty, gradient) \u2014 every value rounded to 4 decimals.\n        If *norm* is unsupported, returns -1.\n    \"\"\"\n    if norm not in {'l1', 'l2'}:\n        return -1\n    w = np.asarray(weights, dtype=float)\n    if norm == 'l1':\n        penalty = C * np.sum(np.abs(w))\n        grad = C * np.sign(w)\n    else:\n        penalty = 0.5 * C * np.sum(w ** 2)\n        grad = C * w\n    penalty = round(float(penalty), 4)\n    grad = [round(float(g), 4) for g in grad]\n    return (penalty, grad)"}
{"task_id": 468, "completion_id": 0, "solution": "import numpy as np\ndef logistic_regression_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, learning_rate: float=0.1, num_iters: int=10000) -> list[int]:\n    \"\"\"Train a binary logistic-regression model with gradient descent and\n    predict labels for the test set.\n\n    Args:\n        X_train: Training feature matrix of shape (m, n).\n        y_train: Training labels \u2013 either shape (m,) or (m, 1) containing 0/1.\n        X_test:  Test feature matrix of shape (k, n).\n        learning_rate: Step size used by gradient descent (default 0.1).\n        num_iters:      Number of optimisation iterations (default 10000).\n\n    Returns:\n        A list of length *k* holding the predicted class (0 or 1) for every\n        test sample, in order.\n    \"\"\"\n    if y_train.ndim == 1:\n        y = y_train.reshape(-1, 1)\n    else:\n        y = y_train.copy()\n    (m, n) = X_train.shape\n    Xb_train = np.hstack((np.ones((m, 1)), X_train))\n    Xb_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n    W = np.zeros((n + 1, 1))\n\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n    for _ in range(num_iters):\n        z = Xb_train @ W\n        h = sigmoid(z)\n        dW = Xb_train.T @ (h - y) / m\n        W -= learning_rate * dW\n    probs = sigmoid(Xb_test @ W)\n    preds = (probs >= 0.5).astype(int)\n    return preds.ravel().tolist()"}
{"task_id": 469, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef linear_kernel(**kwargs) -> callable:\n    \"\"\"Factory returning a linear-kernel (dot-product) callable.\n\n    Any supplied keyword arguments are ignored so that the signature stays\n    compatible with more elaborate kernel factories.\n    \"\"\"\n\n    def f(x1, x2):\n        \"\"\"\n        Parameters\n        ----------\n        x1, x2 : 1-D list-like or np.ndarray\n            Input feature vectors.\n\n        Returns\n        -------\n        float\n            Dot product of the two vectors, or \u20111 if their lengths differ.\n        \"\"\"\n        x1_arr = np.asarray(x1, dtype=float).ravel()\n        x2_arr = np.asarray(x2, dtype=float).ravel()\n        if x1_arr.size != x2_arr.size:\n            return -1\n        return float(np.inner(x1_arr, x2_arr))\n    return f"}
{"task_id": 470, "completion_id": 0, "solution": "import os\ndef get_scriptdir(file_path: str) -> str:\n    \"\"\"Return the absolute directory that contains *file_path*.\n\n    The function normalises *file_path* by resolving any symbolic links\n    and removing ``..`` / ``.`` path segments, then returns only the\n    directory component of this canonical path.\n\n    Args:\n        file_path: Path string pointing to a file.  May be absolute or\n                   relative and may contain symlinks or redundant path\n                   elements.\n\n    Returns:\n        The absolute directory path that contains *file_path*.  The\n        returned path never ends with a path separator (unless it is the\n        root directory).\n    \"\"\"\n    canonical_path = os.path.realpath(file_path)\n    directory = os.path.dirname(canonical_path)\n    directory = os.path.normpath(directory)\n    return directory"}
{"task_id": 471, "completion_id": 0, "solution": "import numpy as np\ndef expected_sarsa_update(q_table: list[list[float]], state: int, action: int, reward: float, next_state: int | None, epsilon: float, lr: float, gamma: float) -> list[list[float]]:\n    \"\"\"One-step tabular Expected-SARSA TD(0) update.\"\"\"\n    n_actions = len(q_table[0])\n    if next_state is None:\n        expected_q = 0.0\n    else:\n        next_row = q_table[next_state]\n        greedy_idx = max(range(n_actions), key=next_row.__getitem__)\n        base_prob = epsilon / n_actions\n        expected_q = 0.0\n        for a in range(n_actions):\n            prob = base_prob\n            if a == greedy_idx:\n                prob += 1.0 - epsilon\n            expected_q += prob * next_row[a]\n    target = reward + gamma * expected_q\n    q_table[state][action] += lr * (target - q_table[state][action])\n    rounded_q = [[round(v, 4) for v in row] for row in q_table]\n    return rounded_q"}
{"task_id": 472, "completion_id": 0, "solution": "import numpy as np\ndef softmax(x: list | list[list]):\n    \"\"\"Return the softmax probabilities and their element-wise gradient.\n\n    Parameters\n    ----------\n    x : list | list[list]\n        A one- or two-dimensional numeric list.\n\n    Returns\n    -------\n    tuple[list, list]\n        (probabilities, gradient) \u2013 both with the same shape as *x*,\n        rounded to 4 decimal places.\n    \"\"\"\n    arr = np.asarray(x, dtype=float)\n    shifted = arr - arr.max(axis=-1, keepdims=True)\n    exp_vals = np.exp(shifted)\n    probs = exp_vals / exp_vals.sum(axis=-1, keepdims=True)\n    grad = probs * (1.0 - probs)\n    probs_out = np.round(probs, 4).tolist()\n    grad_out = np.round(grad, 4).tolist()\n    return (probs_out, grad_out)"}
{"task_id": 474, "completion_id": 0, "solution": "import numpy as np\nimport random\nimport numpy as np\nimport random\ndef generate_data(m: int, n_train: int, n_val: int, interval: float, seed: int | None=None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set consisting of `m` rectangular clusters.\n\n    Each class i (0-based index) is located at\n        x \u2208 [(\u230ai/2\u230b+0.1)\u00b710 , (\u230ai/2\u230b+0.9)\u00b710]\n        y \u2208 [((i mod 2)*interval+0.1)\u00b710 , ((i mod 2)*interval+0.9)\u00b710]\n    and is assigned the label  (i \u2212 0.5)\u00b72  (\u2192 \u22121, +1, 3, 5, \u2026).\n\n    All returned coordinates are rounded to 4 decimals.\n    \"\"\"\n    if m < 2:\n        raise ValueError('`m` (number of classes) must be at least 2.')\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    (X_train_parts, X_val_parts) = ([], [])\n    (Y_train_parts, Y_val_parts) = ([], [])\n    for i in range(m):\n        col = i // 2\n        row = i % 2\n        (x_min, x_max) = ((col + 0.1) * 10, (col + 0.9) * 10)\n        (y_min, y_max) = ((row * interval + 0.1) * 10, (row * interval + 0.9) * 10)\n        x_tr = np.random.uniform(x_min, x_max, n_train)\n        y_tr = np.random.uniform(y_min, y_max, n_train)\n        X_train_parts.append(np.column_stack((x_tr, y_tr)))\n        x_va = np.random.uniform(x_min, x_max, n_val)\n        y_va = np.random.uniform(y_min, y_max, n_val)\n        X_val_parts.append(np.column_stack((x_va, y_va)))\n        label = int((i - 0.5) * 2)\n        Y_train_parts.append(np.full(n_train, label, dtype=int))\n        Y_val_parts.append(np.full(n_val, label, dtype=int))\n    X_train = np.round(np.vstack(X_train_parts), 4)\n    X_val = np.round(np.vstack(X_val_parts), 4)\n    Y_train = np.concatenate(Y_train_parts)\n    Y_val = np.concatenate(Y_val_parts)\n    return (X_train, X_val, Y_train, Y_val)"}
{"task_id": 475, "completion_id": 0, "solution": "import numpy as np\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-08):\n    \"\"\"Perform a single Adam optimisation step (see doc-string above).\"\"\"\n    grad_arr = np.asarray(grad, dtype=float)\n    if m_prev is None:\n        m_prev = np.zeros_like(grad_arr, dtype=float)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad_arr, dtype=float)\n    m_new = b1 * m_prev + (1.0 - b1) * grad_arr\n    v_new = b2 * v_prev + (1.0 - b2) * grad_arr ** 2\n    m_hat = m_new / (1.0 - b1 ** t)\n    v_hat = v_new / (1.0 - b2 ** t)\n    w_new = np.asarray(w, dtype=float) - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n    if np.isscalar(w):\n        w_new = w_new.item()\n        m_new = m_new.item()\n        v_new = v_new.item()\n    return (w_new, m_new, v_new)"}
{"task_id": 477, "completion_id": 0, "solution": "import heapq\nfrom typing import List, Tuple\ndef astar(grid: List[List[int]], start: Tuple[int, int], dest: Tuple[int, int]) -> int:\n    \"\"\"A* search on a 2-D grid using 4-directional movement.\n\n    Args:\n        grid: 2-D list with 0 = free cell, 1 = obstacle.\n        start: (row, col) start position.\n        dest:  (row, col) destination position.\n\n    Returns:\n        Length of the shortest path (number of moves) or \u20111 if unreachable.\n    \"\"\"\n    if not grid or not grid[0]:\n        return -1\n    (rows, cols) = (len(grid), len(grid[0]))\n    (sr, sc) = start\n    (dr, dc) = dest\n    if not (0 <= sr < rows and 0 <= sc < cols and (0 <= dr < rows) and (0 <= dc < cols)):\n        return -1\n    if grid[sr][sc] == 1 or grid[dr][dc] == 1:\n        return -1\n    if start == dest:\n        return 0\n\n    def heuristic(r: int, c: int) -> int:\n        \"\"\"Manhattan distance to destination.\"\"\"\n        return abs(r - dr) + abs(c - dc)\n    open_heap: List[Tuple[int, int, int, int]] = []\n    g_cost = {start: 0}\n    heapq.heappush(open_heap, (heuristic(sr, sc), 0, sr, sc))\n    directions = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n    while open_heap:\n        (f, g, r, c) = heapq.heappop(open_heap)\n        if g != g_cost.get((r, c), float('inf')):\n            continue\n        if (r, c) == dest:\n            return g\n        for (drc, dcc) in directions:\n            (nr, nc) = (r + drc, c + dcc)\n            if not (0 <= nr < rows and 0 <= nc < cols):\n                continue\n            if grid[nr][nc] == 1:\n                continue\n            ng = g + 1\n            if ng < g_cost.get((nr, nc), float('inf')):\n                g_cost[nr, nc] = ng\n                heapq.heappush(open_heap, (ng + heuristic(nr, nc), ng, nr, nc))\n    return -1"}
{"task_id": 478, "completion_id": 0, "solution": "import numpy as np\ndef calculate_std_dev(X):\n    \"\"\"Return the population standard deviation for every feature in *X*.\n\n    Args:\n        X (list[list[int | float]] | np.ndarray): 2-D data set where rows are\n            samples and columns are features.\n\n    Returns:\n        list[float]: Standard deviation of each feature (population version),\n            rounded to 4 decimal places.  If *X* is empty, [] is returned.\n    \"\"\"\n    arr = np.asarray(X, dtype=float)\n    if arr.size == 0:\n        return []\n    if arr.ndim == 1:\n        arr = arr.reshape(-1, 1)\n    n_samples = arr.shape[0]\n    mean = np.sum(arr, axis=0) / n_samples\n    sq_diffs = (arr - mean) ** 2\n    variance = np.sum(sq_diffs, axis=0) / n_samples\n    std_dev = np.sqrt(variance)\n    std_rounded = np.round(std_dev, 4)\n    return std_rounded.tolist()"}
{"task_id": 479, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Optional\ndef epsilon_greedy_bandit(rewards: List[List[float]], epsilon: float=0.05, ev_prior: float=0.5, seed: Optional[int]=None) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    if not rewards:\n        return [round(ev_prior, 4)]\n    n_arms = len(rewards[0])\n    values = np.full(n_arms, float(ev_prior))\n    counts = np.zeros(n_arms, dtype=int)\n    for row in rewards:\n        if np.random.random() < epsilon:\n            arm = np.random.randint(n_arms)\n        else:\n            arm = int(np.argmax(values))\n        r = row[arm]\n        counts[arm] += 1\n        values[arm] += (r - values[arm]) / counts[arm]\n    return [round(v, 4) for v in values]"}
{"task_id": 480, "completion_id": 0, "solution": "import numpy as np\ndef mel_spectrogram(x: 'np.ndarray', window_duration: float=0.025, stride_duration: float=0.01, mean_normalize: bool=True, window: str='hamming', n_filters: int=20, center: bool=True, alpha: float=0.95, fs: int=44000):\n    \"\"\"Return the Mel spectrogram of *x* together with the energy of every frame.\"\"\"\n    if x.ndim != 1:\n        raise ValueError('x must be one-dimensional')\n\n    def hz2mel(hz):\n        return 2595.0 * np.log10(1.0 + hz / 700.0)\n\n    def mel2hz(mel):\n        return 700.0 * (10.0 ** (mel / 2595.0) - 1.0)\n    x = np.asarray(x, dtype=np.float64)\n    if alpha != 0.0:\n        x = np.append(x[0], x[1:] - alpha * x[:-1])\n    frame_len = int(round(window_duration * fs))\n    frame_step = int(round(stride_duration * fs))\n    if center:\n        pad = frame_len // 2\n        x = np.pad(x, (pad, pad), mode='constant')\n    num_frames = 1 + int(np.floor((len(x) - frame_len) / frame_step))\n    if num_frames <= 0:\n        raise ValueError('Not enough samples for even one frame')\n    shape = (num_frames, frame_len)\n    strides = (x.strides[0] * frame_step, x.strides[0])\n    frames = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides).copy()\n    win_name = window.lower()\n    if win_name == 'hamming':\n        win = np.hamming(frame_len)\n    elif win_name == 'hann':\n        win = np.hanning(frame_len)\n    elif win_name == 'blackman_harris':\n        n = np.arange(frame_len)\n        (a0, a1, a2, a3) = (0.35875, 0.48829, 0.14128, 0.01168)\n        win = a0 - a1 * np.cos(2 * np.pi * n / (frame_len - 1)) + a2 * np.cos(4 * np.pi * n / (frame_len - 1)) - a3 * np.cos(6 * np.pi * n / (frame_len - 1))\n    else:\n        raise ValueError(\"window must be 'hamming', 'hann' or 'blackman_harris'\")\n    frames *= win\n    N = frame_len\n    fft_frames = np.fft.rfft(frames, n=N)\n    power_spec = np.abs(fft_frames) ** 2 / N\n    energy_per_frame = power_spec.sum(axis=1)\n    (mel_low, mel_high) = (0.0, hz2mel(fs / 2.0))\n    mel_points = np.linspace(mel_low, mel_high, n_filters + 2)\n    hz_points = mel2hz(mel_points)\n    bin_idxs = np.floor(N * hz_points / fs).astype(int)\n    bin_idxs = np.clip(bin_idxs, 0, N // 2)\n    fbank = np.zeros((n_filters, N // 2 + 1))\n    for m in range(1, n_filters + 1):\n        (left, center, right) = (bin_idxs[m - 1], bin_idxs[m], bin_idxs[m + 1])\n        if center - left != 0:\n            up = (np.arange(left, center) - left) / (center - left)\n            fbank[m - 1, left:center] = up\n        if right - center != 0:\n            down = (right - np.arange(center, right)) / (right - center)\n            fbank[m - 1, center:right] = down\n    filter_energies = power_spec @ fbank.T\n    if mean_normalize:\n        filter_energies -= filter_energies.mean(axis=0, keepdims=True)\n    eps = np.finfo(float).eps\n    filter_energies[filter_energies == 0.0] = eps\n    energy_per_frame[energy_per_frame == 0.0] = eps\n    filter_energies = np.round(filter_energies, 4).tolist()\n    energy_per_frame = np.round(energy_per_frame, 4).tolist()\n    return (filter_energies, energy_per_frame)"}
{"task_id": 481, "completion_id": 0, "solution": "import numpy as np\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if data.size == 0:\n        return []\n    n_samples = data.shape[0]\n    eps_sq = eps * eps\n    neighbours: list[np.ndarray] = []\n    for i in range(n_samples):\n        diff = data - data[i]\n        dist_sq = np.einsum('ij,ij->i', diff, diff)\n        neighbours.append(np.where(dist_sq <= eps_sq)[0])\n    UNASSIGNED = -1\n    labels = np.full(n_samples, UNASSIGNED, dtype=int)\n    visited = np.zeros(n_samples, dtype=bool)\n    cluster_id = 0\n    for point_idx in range(n_samples):\n        if visited[point_idx]:\n            continue\n        visited[point_idx] = True\n        neigh = neighbours[point_idx]\n        if neigh.size < min_samples:\n            labels[point_idx] = UNASSIGNED\n            continue\n        labels[point_idx] = cluster_id\n        stack = list(neigh)\n        while stack:\n            current = stack.pop()\n            if not visited[current]:\n                visited[current] = True\n                neigh_current = neighbours[current]\n                if neigh_current.size >= min_samples:\n                    stack.extend(neigh_current.tolist())\n            if labels[current] == UNASSIGNED:\n                labels[current] = cluster_id\n        cluster_id += 1\n    return labels.tolist()"}
{"task_id": 482, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    Parameters\n    ----------\n    y : array_like\n        1-D sequence of non-negative integer labels.\n    num_classes : int or None, optional\n        Number of classes (columns in the output).  If None, it is inferred\n        as max(y) + 1.  Every label must satisfy\n        0 <= label < num_classes, otherwise ValueError is raised.\n\n    Returns\n    -------\n    np.ndarray\n        One-hot encoded matrix of shape (len(y), num_classes) with dtype=int.\n    \"\"\"\n    y_arr = np.asarray(y)\n    if y_arr.ndim == 0:\n        y_arr = y_arr.reshape(1)\n    if y_arr.ndim != 1:\n        raise ValueError('`y` must be a 1-D sequence of integer labels.')\n    try:\n        y_int = y_arr.astype(int, casting='safe', copy=False)\n    except TypeError:\n        raise ValueError('`y` must contain integer values only.')\n    n_samples = y_int.size\n    if n_samples == 0:\n        if num_classes is None:\n            return np.zeros((0, 0), dtype=int)\n        if num_classes <= 0:\n            raise ValueError('`num_classes` must be a positive integer.')\n        return np.zeros((0, num_classes), dtype=int)\n    max_label = int(y_int.max())\n    min_label = int(y_int.min())\n    if num_classes is None:\n        if min_label < 0:\n            raise ValueError('Negative labels are not allowed.')\n        num_classes = max_label + 1\n    else:\n        if num_classes <= 0:\n            raise ValueError('`num_classes` must be a positive integer.')\n        if min_label < 0 or max_label >= num_classes:\n            raise ValueError('All labels must satisfy 0 <= label < num_classes.')\n    one_hot = np.zeros((n_samples, num_classes), dtype=int)\n    rows = np.arange(n_samples)\n    one_hot[rows, y_int] = 1\n    return one_hot"}
{"task_id": 483, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(labels, num_classes=None):\n    \"\"\"Convert integer class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    labels : sequence of int or 1-D np.ndarray\n        The class labels to transform.\n    num_classes : int, optional\n        Desired length of the one-hot vectors; if omitted the function uses\n        ``max(labels) + 1``.\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded representation of ``labels``.\n    \"\"\"\n    labels = np.asarray(labels, dtype=int).ravel()\n    if num_classes is None:\n        num_classes = int(labels.max()) + 1\n    one_hot = [[0] * num_classes for _ in range(len(labels))]\n    for (i, lbl) in enumerate(labels):\n        one_hot[i][int(lbl)] = 1\n    return one_hot"}
{"task_id": 484, "completion_id": 0, "solution": "import numpy as np\ndef lasso_regression_predict(X_train: list[float | int], y_train: list[float | int], X_test: list[float | int], degree: int, reg_factor: float=0.1, n_iterations: int=1000) -> list[float]:\n    \"\"\"Fit a one-dimensional Lasso regression model (coordinate descent)\n    and predict on `X_test`.\"\"\"\n\n    def soft_threshold(rho: float, lam: float) -> float:\n        \"\"\"Soft\u2013thresholding operator S(\u03c1, \u03bb).\"\"\"\n        if rho > lam:\n            return rho - lam\n        if rho < -lam:\n            return rho + lam\n        return 0.0\n    X_train = np.asarray(X_train, dtype=float).ravel()\n    y_train = np.asarray(y_train, dtype=float).ravel()\n    X_test = np.asarray(X_test, dtype=float).ravel()\n    n_samples = X_train.size\n    n_features = degree + 1\n    Phi_train = np.column_stack([X_train ** k for k in range(n_features)])\n    Phi_test = np.column_stack([X_test ** k for k in range(n_features)])\n    z = (Phi_train ** 2).sum(axis=0)\n    w = np.zeros(n_features)\n    y_pred = Phi_train @ w\n    for _ in range(n_iterations):\n        for j in range(n_features):\n            residual = y_train - (y_pred - Phi_train[:, j] * w[j])\n            rho_j = (Phi_train[:, j] * residual).sum()\n            if j == 0:\n                new_w_j = rho_j / z[j] if z[j] != 0 else 0.0\n            else:\n                new_w_j = soft_threshold(rho_j, reg_factor) / z[j] if z[j] != 0 else 0.0\n            if new_w_j != w[j]:\n                y_pred += Phi_train[:, j] * (new_w_j - w[j])\n                w[j] = new_w_j\n    y_test_pred = Phi_test @ w\n    return [float(np.round(val, 4)) for val in y_test_pred]"}
{"task_id": 485, "completion_id": 0, "solution": "import numpy as np\ndef regression_tree_predict(X_train: list[float | int], y_train: list[float | int], X_test: list[float | int], max_depth: int=3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\"\"\"\n\n    def build(x: np.ndarray, y: np.ndarray, depth: int):\n        if depth == max_depth or x.size < 2 or np.all(x == x[0]):\n            return {'leaf': True, 'value': float(y.mean())}\n        order = np.argsort(x)\n        (xs, ys) = (x[order], y[order])\n        n = xs.size\n        csum = np.cumsum(ys)\n        csum2 = np.cumsum(ys ** 2)\n        (total_sum, total_sum2) = (csum[-1], csum2[-1])\n        (best_sse, best_idx) = (np.inf, None)\n        for i in range(n - 1):\n            if xs[i] == xs[i + 1]:\n                continue\n            (n_l, n_r) = (i + 1, n - (i + 1))\n            (sum_l, sum_r) = (csum[i], total_sum - csum[i])\n            (sum2_l, sum2_r) = (csum2[i], total_sum2 - csum2[i])\n            sse_l = sum2_l - sum_l ** 2 / n_l\n            sse_r = sum2_r - sum_r ** 2 / n_r\n            sse = sse_l + sse_r\n            if sse < best_sse:\n                (best_sse, best_idx) = (sse, i)\n        if best_idx is None:\n            return {'leaf': True, 'value': float(y.mean())}\n        thr = (xs[best_idx] + xs[best_idx + 1]) / 2.0\n        left_mask = x <= thr\n        right_mask = ~left_mask\n        return {'leaf': False, 'thr': float(thr), 'left': build(x[left_mask], y[left_mask], depth + 1), 'right': build(x[right_mask], y[right_mask], depth + 1)}\n\n    def predict_one(node, x_val: float):\n        while not node['leaf']:\n            node = node['left'] if x_val <= node['thr'] else node['right']\n        return node['value']\n    if len(X_train) == 0:\n        return []\n    X_arr = np.asarray(X_train, dtype=float)\n    y_arr = np.asarray(y_train, dtype=float)\n    tree = build(X_arr, y_arr, depth=0)\n    return [predict_one(tree, float(x)) for x in X_test]"}
{"task_id": 486, "completion_id": 0, "solution": "import numpy as np\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"Compute the Mean Squared Error (MSE) between two numeric sequences.\n\n    The function returns -1 if the two inputs do not have the same\n    non-zero length, otherwise it returns the MSE rounded to 4 decimals.\n\n    Args:\n        y_true: Ground-truth values (list, tuple or NumPy array).\n        y_pred: Predicted values (list, tuple or NumPy array).\n\n    Returns:\n        float: The MSE rounded to 4 decimal places, or -1 on error.\n    \"\"\"\n    try:\n        a_true = np.asarray(y_true, dtype=float).ravel()\n        a_pred = np.asarray(y_pred, dtype=float).ravel()\n    except (TypeError, ValueError):\n        return -1\n    if a_true.size == 0 or a_true.size != a_pred.size:\n        return -1\n    mse = np.mean((a_true - a_pred) ** 2)\n    return float(round(mse, 4))"}
{"task_id": 487, "completion_id": 0, "solution": "import numpy as np\ndef k_means(X: np.ndarray, k: int, max_iters: int=300, tol: float=0.0001) -> list[int]:\n    \"\"\"Classical K-Means clustering (from scratch, NumPy only).\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Data matrix.\n    k : int\n        Number of clusters (1 \u2264 k \u2264 n_samples).\n    max_iters : int, optional\n        Hard iteration cap (default 300).\n    tol : float, optional\n        Convergence threshold on centroid displacement (default 1e-4).\n\n    Returns\n    -------\n    list[int]\n        Cluster index (0 \u2026 k-1) for every sample, in the original order.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    centroids = X[:k].copy()\n    labels = np.zeros(X.shape[0], dtype=int)\n    for _ in range(max_iters):\n        distances = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = centroids.copy()\n        for j in range(k):\n            members = labels == j\n            if np.any(members):\n                new_centroids[j] = X[members].mean(axis=0)\n        if np.linalg.norm(new_centroids - centroids) < tol:\n            centroids = new_centroids\n            break\n        centroids = new_centroids\n    return labels.tolist()"}
{"task_id": 488, "completion_id": 0, "solution": "import numpy as np\ndef calculate_variance(X):\n    \"\"\"Return the population variance of every feature (column) in X.\n\n    Parameters\n    ----------\n    X : list[list[int | float]] | np.ndarray\n        A 2-D data set where each row is a sample and each column is a feature.\n\n    Returns\n    -------\n    list[float] | int\n        A list containing the variance of each feature rounded to four\n        decimals.  Returns -1 if the input is empty, has <1 sample,\n        or is not strictly two-dimensional.\n    \"\"\"\n    try:\n        arr = np.asarray(X, dtype=float)\n    except Exception:\n        return -1\n    if arr.ndim != 2 or arr.shape[0] < 1:\n        return -1\n    mean = arr.mean(axis=0)\n    var = np.mean((arr - mean) ** 2, axis=0)\n    return np.round(var, 4).tolist()"}
{"task_id": 489, "completion_id": 0, "solution": "import numpy as np\ndef to_nominal(one_hot):\n    \"\"\"Convert a batch of one-hot encoded vectors to nominal class labels.\n\n    Parameters\n    ----------\n    one_hot : list[list[int]] | numpy.ndarray\n        A 2-D structure where each row is expected to be a one-hot encoded\n        vector (all zeros except a single 1).\n\n    Returns\n    -------\n    list[int] | int\n        List with the class index for every sample when the input is valid;\n        otherwise the integer \u20111.\n    \"\"\"\n    arr = np.asarray(one_hot)\n    if arr.ndim != 2:\n        return -1\n    labels = []\n    for row in arr:\n        if not np.all((row == 0) | (row == 1)):\n            return -1\n        ones_idx = np.flatnonzero(row)\n        if ones_idx.size != 1:\n            return -1\n        labels.append(int(ones_idx[0]))\n    return labels"}
{"task_id": 490, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    arr = np.asarray(x)\n    if arr.size == 0:\n        return []\n    arr = arr.ravel()\n    diag_matrix = np.diag(arr).tolist()\n    return diag_matrix"}
{"task_id": 491, "completion_id": 0, "solution": "from collections import Counter, defaultdict\nfrom itertools import combinations\nclass _FPNode:\n    __slots__ = ('item', 'count', 'parent', 'children', 'link')\n\n    def __init__(self, item, parent):\n        self.item = item\n        self.count = 0\n        self.parent = parent\n        self.children = {}\n        self.link = None\ndef _build_fp_tree(trans_iterable, min_sup):\n    \"\"\"\n    Build an FP-tree from `trans_iterable` and return (root, header_table).\n    `trans_iterable` must yield tuples   (items_in_transaction , multiplicity)\n    in which every `items_in_transaction` is already ordered according to a\n    global frequency order and contains *only* items that are frequent in the\n    projected data set.\n    \"\"\"\n    header = {}\n    for (items, cnt) in trans_iterable:\n        for itm in items:\n            header.setdefault(itm, [0, None])[0] += cnt\n    header = {itm: val for (itm, val) in header.items() if val[0] >= min_sup}\n    if not header:\n        return (None, None)\n    order = sorted(header.items(), key=lambda x: (-x[1][0], x[0]))\n    rank = {itm: idx for (idx, (itm, _)) in enumerate(order)}\n    root = _FPNode(None, None)\n    for (items, cnt) in trans_iterable:\n        ordered = [i for i in sorted(items, key=lambda x: rank.get(x, 10 ** 9)) if i in header]\n        if not ordered:\n            continue\n        cur = root\n        for itm in ordered:\n            nxt = cur.children.get(itm)\n            if nxt is None:\n                nxt = _FPNode(itm, cur)\n                cur.children[itm] = nxt\n                if header[itm][1] is None:\n                    header[itm][1] = nxt\n                else:\n                    last = header[itm][1]\n                    while last.link is not None:\n                        last = last.link\n                    last.link = nxt\n            nxt.count += cnt\n            cur = nxt\n    return (root, header)\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\"\"\"\n    freq = Counter()\n    for t in transactions:\n        freq.update(t)\n    freq = {k: v for (k, v) in freq.items() if v >= min_sup}\n    if not freq:\n        return []\n    order = sorted(freq.items(), key=lambda x: (-x[1], x[0]))\n    rank = {itm: idx for (idx, (itm, _)) in enumerate(order)}\n    preprocessed = []\n    for t in transactions:\n        filtered = [i for i in t if i in freq]\n        if not filtered:\n            continue\n        filtered.sort(key=lambda x: rank[x])\n        preprocessed.append((tuple(filtered), 1))\n    (root, header) = _build_fp_tree(preprocessed, min_sup)\n    patterns = set()\n    _mine(root, header, min_sup, [], patterns)\n    result = [tuple(sorted(p)) for p in patterns]\n    result.sort(key=lambda x: (len(x), x))\n    return result"}
{"task_id": 492, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a polynomial feature matrix up to *degree*.\n\n    Parameters\n    ----------\n    X : list of list\n        Input data of shape (n_samples, n_features).\n    degree : int\n        Maximum total degree of the generated polynomial terms.\n        If degree < 0 the function returns -1.\n\n    Returns\n    -------\n    list of list\n        Enriched feature matrix whose columns are all monomials of the\n        original variables having total degree \u2264 *degree*, ordered\n        (i)   by increasing total degree,\n        (ii)  lexicographically within the same degree.\n        The first column is 1 (degree-0 term).\n    \"\"\"\n    if degree < 0:\n        return -1\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim != 2:\n        raise ValueError('X must be a 2-D array-like structure')\n    (n_samples, n_features) = X_arr.shape\n    index_tuples: list[tuple[int, ...]] = [()]\n    for p in range(1, degree + 1):\n        for combo in combinations_with_replacement(range(n_features), p):\n            index_tuples.append(combo)\n    n_terms = len(index_tuples)\n    out = np.empty((n_samples, n_terms), dtype=float)\n    for (j, comb) in enumerate(index_tuples):\n        if not comb:\n            out[:, j] = 1.0\n        else:\n            col = np.ones(n_samples, dtype=float)\n            for idx in comb:\n                col *= X_arr[:, idx]\n            out[:, j] = col\n    return out.tolist()"}
{"task_id": 493, "completion_id": 0, "solution": "import numpy as np\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    If *y* is empty return -1.\n\n    Parameters\n    ----------\n    y : Sequence[int | float] | numpy.ndarray\n        One-dimensional collection of numbers.\n\n    Returns\n    -------\n    float\n        MSE rounded to four decimal places, or -1 when *y* is empty.\n    \"\"\"\n    arr = np.asarray(y, dtype=float).ravel()\n    if arr.size == 0:\n        return -1\n    diff = arr - arr.mean()\n    mse_val = (diff ** 2).mean()\n    return float(round(mse_val, 4))"}
{"task_id": 494, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Union\ndef aggregate_predictions(preds: Union[List[List[Union[int, float]]], np.ndarray], classifier: bool=True) -> List[Union[int, float]]:\n    \"\"\"Aggregate ensemble predictions.\n\n    Parameters\n    ----------\n    preds : list[list[int | float]] | np.ndarray\n        Each row contains the predictions of one base model and each\n        column corresponds to a data sample.\n    classifier : bool, default True\n        True  -> classification  (majority voting)\n        False -> regression      (mean of predictions)\n\n    Returns\n    -------\n    list[int | float]\n        Aggregated predictions.  For regression results are rounded to\n        4 decimal places.\n    \"\"\"\n    arr = np.asarray(preds)\n    if arr.ndim != 2:\n        raise ValueError('`preds` must be a 2-D structure (n_models, n_samples).')\n    if classifier:\n\n        def _majority(col: np.ndarray) -> int:\n            (values, counts) = np.unique(col, return_counts=True)\n            return int(values[np.argmax(counts)])\n        majority_per_sample = np.apply_along_axis(_majority, 0, arr)\n        return majority_per_sample.tolist()\n    else:\n        mean_per_sample = np.round(arr.mean(axis=0), 4)\n        return mean_per_sample.tolist()"}
{"task_id": 495, "completion_id": 0, "solution": "import numpy as np\ndef standardize(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Standardize every column of a 2-D NumPy array.\n\n    Each feature (column) will have mean 0 and variance 1 after the\n    transformation.  Columns with zero variance become all zeros.\n\n    The input array is **not** modified in place.\n\n    Args:\n        X: 2-D NumPy array (shape = n_samples \u00d7 n_features).\n\n    Returns:\n        Standardized dataset as a Python list rounded to 4 decimals.\n    \"\"\"\n    means = X.mean(axis=0)\n    stds = X.std(axis=0)\n    safe_stds = np.where(stds == 0, 1, stds)\n    Z = (X - means) / safe_stds\n    Z[:, stds == 0] = 0.0\n    return np.round(Z, 4).tolist()"}
{"task_id": 496, "completion_id": 0, "solution": "import numpy as np\ndef apply_affine(x, slope: float=1.0, intercept: float=0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list/array or a scalar.\n        slope: Coefficient multiplying x (default 1.0).\n        intercept: Constant term added to the product (default 0.0).\n\n    Returns:\n        (y, grad, grad2): tuple of three lists (all rounded to 4 decimals)\n            y[i]     = slope * x[i] + intercept\n            grad[i]  = slope\n            grad2[i] = 0\n    \"\"\"\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim == 0:\n        arr = arr.reshape(1)\n    y = slope * arr + intercept\n    grad = np.full(arr.shape, slope, dtype=float)\n    grad2 = np.zeros(arr.shape, dtype=float)\n    y = np.round(y, 4).tolist()\n    grad = np.round(grad, 4).tolist()\n    grad2 = np.round(grad2, 4).tolist()\n    return (y, grad, grad2)"}
{"task_id": 497, "completion_id": 0, "solution": "import numpy as np\ndef exponential_activation(x, order: int=0):\n    \"\"\"Compute the exponential activation or one of its first two derivatives.\n\n    Parameters\n    ----------\n    x : list | numpy.ndarray\n        A (possibly nested) list or NumPy array containing real numbers.\n    order : int, optional\n        Indicates what to compute:\n        * 0 \u2013 activation itself (default)\n        * 1 \u2013 first derivative\n        * 2 \u2013 second derivative\n\n    Returns\n    -------\n    list | int\n        Python list mirroring the shape of *x* with every entry replaced by\n        the corresponding result rounded to 6 decimals, or -1 for invalid\n        *order*.\n    \"\"\"\n    if order not in (0, 1, 2):\n        return -1\n    arr = np.asarray(x, dtype=float)\n    result = np.exp(arr)\n    result = np.round(result, 6)\n    return result.tolist()"}
{"task_id": 498, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(x: np.ndarray | list | int | float, alpha: float=0.3, order: int=0) -> list:\n    \"\"\"Compute Leaky ReLU, its first derivative, or its second derivative.\n\n    Args:\n        x: Values to be transformed. Accepts NumPy array, list, or scalar.\n        alpha: Slope used when an element is non-positive (default 0.3).\n        order: 0 \u2013 return the function value;\n               1 \u2013 return the first derivative;\n               2 \u2013 return the second derivative.\n\n    Returns:\n        Python list with the same shape as *x* (scalars wrapped in a\n        one-element list) that contains the requested quantity.\n        Returns an empty list if *order* is not 0, 1, or 2.\n    \"\"\"\n    if order not in (0, 1, 2):\n        return []\n    arr = np.asarray(x, dtype=float)\n    if order == 0:\n        result = np.where(arr > 0, arr, alpha * arr)\n    elif order == 1:\n        result = np.where(arr > 0, 1.0, alpha)\n    else:\n        result = np.zeros_like(arr)\n    if result.ndim == 0:\n        return [result.item()]\n    return result.tolist()"}
{"task_id": 499, "completion_id": 0, "solution": "import numpy as np\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places.  \n        If *k* is outside the valid range, return \u22121.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D NumPy array.')\n    (n_samples, n_features) = data.shape\n    if not 1 <= k <= n_features:\n        return -1\n    data = data.astype(float, copy=False)\n    means = data.mean(axis=0)\n    stds = data.std(axis=0, ddof=0)\n    stds_safe = stds.copy()\n    stds_safe[stds_safe == 0] = 1.0\n    Z = (data - means) / stds_safe\n    if n_samples > 1:\n        cov = Z.T @ Z / (n_samples - 1)\n    else:\n        cov = np.zeros((n_features, n_features))\n    (eigvals, eigvecs) = np.linalg.eigh(cov)\n    order = eigvals.argsort()[::-1]\n    eigvals = eigvals[order]\n    eigvecs = eigvecs[:, order]\n    for i in range(eigvecs.shape[1]):\n        v = eigvecs[:, i]\n        if v[np.argmax(np.abs(v))] < 0:\n            eigvecs[:, i] = -v\n    components = eigvecs[:, :k]\n    projected = Z @ components\n    return np.round(projected, 4).tolist()"}
{"task_id": 500, "completion_id": 0, "solution": "import numpy as np\nfrom math import inf\ndef _fit_best_stump(X: np.ndarray, residuals: np.ndarray):\n    \"\"\"Find the decision stump (feature, threshold, left_val, right_val)\n    that minimises the squared error w.r.t. the provided residuals.\"\"\"\n    (n_samples, n_features) = X.shape\n    (best_feature, best_thr) = (None, None)\n    (best_left_val, best_right_val) = (None, None)\n    best_sse = inf\n    for j in range(n_features):\n        f = X[:, j]\n        order = np.argsort(f)\n        f_sorted = f[order]\n        r_sorted = residuals[order]\n        cumsum_r = np.cumsum(r_sorted)\n        cumsum_r2 = np.cumsum(r_sorted ** 2)\n        total_sum = cumsum_r[-1]\n        total_sum2 = cumsum_r2[-1]\n        for i in range(1, n_samples):\n            if f_sorted[i] == f_sorted[i - 1]:\n                continue\n            n_left = i\n            n_right = n_samples - i\n            sum_left = cumsum_r[i - 1]\n            sum_right = total_sum - sum_left\n            sum2_left = cumsum_r2[i - 1]\n            sum2_right = total_sum2 - sum2_left\n            mean_left = sum_left / n_left\n            mean_right = sum_right / n_right\n            sse_left = sum2_left - sum_left ** 2 / n_left\n            sse_right = sum2_right - sum_right ** 2 / n_right\n            sse = sse_left + sse_right\n            if sse < best_sse:\n                best_sse = sse\n                best_feature = j\n                best_thr = 0.5 * (f_sorted[i] + f_sorted[i - 1])\n                best_left_val = mean_left\n                best_right_val = mean_right\n    return (best_feature, best_thr, best_left_val, best_right_val)\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int=20, learning_rate: float=0.1) -> list[int]:\n    \"\"\"Tiny gradient\u2013boosted binary classifier (decision-stump base learners).\"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train, dtype=float)\n    n_train = X_train.shape[0]\n    c0 = y_train.mean()\n    F_train = np.full(n_train, c0, dtype=float)\n    F_test = np.full(X_test.shape[0], c0, dtype=float)\n    stumps = []\n    for _ in range(n_estimators):\n        residuals = y_train - F_train\n        (feat, thr, left_val, right_val) = _fit_best_stump(X_train, residuals)\n        stumps.append((feat, thr, left_val, right_val))\n        pred_train = np.where(X_train[:, feat] <= thr, left_val, right_val)\n        pred_test = np.where(X_test[:, feat] <= thr, left_val, right_val)\n        F_train += learning_rate * pred_train\n        F_test += learning_rate * pred_test\n    y_pred = (F_test >= 0.5).astype(int).tolist()\n    return y_pred"}
{"task_id": 501, "completion_id": 0, "solution": "def constant_scheduler(initial_lr: float, step: int) -> float:\n    \"\"\"Constant learning-rate scheduler.\n\n    Parameters\n    ----------\n    initial_lr : float\n        The fixed learning rate that should be returned on every call.\n    step : int\n        Current training step (ignored).\n\n    Returns\n    -------\n    float\n        The same value as `initial_lr`.\n    \"\"\"\n    return initial_lr"}
{"task_id": 502, "completion_id": 0, "solution": "import numpy as np\ndef hard_sigmoid(z: np.ndarray, order: int=0) -> list[float]:\n    \"\"\"Compute Hard-Sigmoid activation or its derivatives.\n\n    Parameters\n    ----------\n    z : np.ndarray\n        Input array.\n    order : int, optional\n        0 \u2013 activation (default)\n        1 \u2013 first derivative\n        2 \u2013 second derivative\n\n    Returns\n    -------\n    list[float]\n        Rounded results (4 decimals) as a Python list.\n        Returns an empty list for unsupported *order* values.\n    \"\"\"\n    if order not in (0, 1, 2):\n        return []\n    z = np.asarray(z, dtype=float)\n    if order == 0:\n        out = np.where(z < -2.5, 0.0, np.where(z > 2.5, 1.0, 0.2 * z + 0.5))\n    elif order == 1:\n        out = np.where((-2.5 <= z) & (z <= 2.5), 0.2, 0.0)\n    else:\n        out = np.zeros_like(z)\n    return np.round(out, 4).tolist()"}
{"task_id": 503, "completion_id": 0, "solution": "from copy import deepcopy\ndef selective_update(hparams: dict | None, updates: dict | None) -> dict | int:\n    \"\"\"Return a deep-copied version of *hparams* with selective updates applied.\n\n    Only the keys that already exist in *hparams* are overwritten by *updates*.\n    The original inputs are never modified (purely functional).\n\n    Parameters\n    ----------\n    hparams : dict | None\n        Original hyper-parameter dictionary.  If None, the function returns -1.\n    updates : dict | None\n        Dictionary containing new values.  Keys not present in *hparams* are\n        ignored.  If None, no changes are applied.\n\n    Returns\n    -------\n    dict | int\n        A deep-copied (and possibly updated) dictionary, or \u22121 if *hparams* is\n        None.\n    \"\"\"\n    if hparams is None:\n        return -1\n    result = deepcopy(hparams)\n    if updates is not None:\n        for (key, value) in updates.items():\n            if key in result:\n                result[key] = value\n    return result"}
{"task_id": 505, "completion_id": 0, "solution": "import numpy as np\ndef contrastive_divergence(X: np.ndarray, W: np.ndarray, hbias: np.ndarray, vbias: np.ndarray, learning_rate: float, k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM (probabilities only).\"\"\"\n    if X.size == 0:\n        return []\n    m = X.shape[0]\n\n    def _sigmoid(a: np.ndarray) -> np.ndarray:\n        return 1.0 / (1.0 + np.exp(-a))\n    h_prob = _sigmoid(X @ W + hbias)\n    pos_grad = X.T @ h_prob\n    for _ in range(k):\n        v_prob = _sigmoid(h_prob @ W.T + vbias)\n        h_prob = _sigmoid(v_prob @ W + hbias)\n    neg_grad = v_prob.T @ h_prob\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n    return np.round(W_new, 4).tolist()"}
{"task_id": 508, "completion_id": 0, "solution": "from typing import Any, Dict, List\ndef params_with_name(params: Dict[str, Any], name: str) -> List[Any]:\n    \"\"\"Return all values whose keys contain a given substring.\n\n    The original insertion order of the dictionary is preserved.\n\n    Args:\n        params: Dictionary mapping parameter names (strings) to parameter values (any type).\n        name: Substring to search for inside the dictionary keys.\n\n    Returns:\n        List of values whose corresponding keys contain *name* (order preserved).  \n        Returns an empty list if no key matches.\n    \"\"\"\n    return [value for (key, value) in params.items() if name in key]"}
{"task_id": 509, "completion_id": 0, "solution": "import numpy as np\ndef chebyshev(x: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray') -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=float).ravel()\n    y_arr = np.asarray(y, dtype=float).ravel()\n    if x_arr.size != y_arr.size:\n        return -1.0\n    dist = np.max(np.abs(x_arr - y_arr))\n    return float(round(dist, 4))"}
{"task_id": 510, "completion_id": 0, "solution": "import numpy as np\ndef vae_loss(y: np.ndarray, y_pred: np.ndarray, t_mean: np.ndarray, t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    Returns the mini-batch average of the binary cross-entropy reconstruction\n    loss plus the analytical KL divergence, rounded to 6 decimal places.\n    \"\"\"\n    eps = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, eps, 1.0 - eps)\n    recon_element = -y * np.log(y_pred_clipped) - (1.0 - y) * np.log(1.0 - y_pred_clipped)\n    recon_loss = recon_element.sum(axis=1)\n    kl_element = 1.0 + t_log_var - np.square(t_mean) - np.exp(t_log_var)\n    kl_loss = -0.5 * kl_element.sum(axis=1)\n    total_loss = recon_loss + kl_loss\n    batch_mean_loss = total_loss.mean()\n    return round(float(batch_mean_loss), 6)"}
{"task_id": 511, "completion_id": 0, "solution": "import numpy as np\ndef random_bernoulli_mab(n_arms: int=10) -> list[float]:\n    \"\"\"\n    Build a random Bernoulli multi-armed bandit environment.\n\n    Parameters\n    ----------\n    n_arms : int, default=10\n        Number of arms in the bandit.  If `n_arms` is less than 1, an\n        empty list is returned.\n\n    Returns\n    -------\n    list[float]\n        List of success probabilities (rounded to 4 decimal places)\n        whose pre-rounding values sum to 1.\n    \"\"\"\n    if n_arms < 1:\n        return []\n    samples = np.random.rand(n_arms)\n    probs = samples / samples.sum()\n    return np.round(probs, 4).tolist()"}
{"task_id": 512, "completion_id": 0, "solution": "def pooling_shape(pool_shape: tuple[int, int], image_shape: tuple[int, int, int, int], stride: tuple[int, int]) -> tuple[int, int] | int:\n    \"\"\"Calculate the spatial output dimensions of a 2-D no-padding pooling layer.\n\n    out_h = (h - k_h) / s_h + 1\n    out_w = (w - k_w) / s_w + 1\n\n    Return (out_h, out_w) when both are integers and the kernel fits,\n    otherwise return -1.\n    \"\"\"\n    try:\n        (k_h, k_w) = pool_shape\n        (_, _, h, w) = image_shape\n        (s_h, s_w) = stride\n    except ValueError:\n        return -1\n    if k_h <= 0 or k_w <= 0 or s_h <= 0 or (s_w <= 0):\n        return -1\n    if k_h > h or k_w > w:\n        return -1\n    num_h = h - k_h\n    num_w = w - k_w\n    if num_h % s_h or num_w % s_w:\n        return -1\n    out_h = num_h // s_h + 1\n    out_w = num_w // s_w + 1\n    return (out_h, out_w)"}
{"task_id": 513, "completion_id": 0, "solution": "import numpy as np\ndef fm_predict(X: list[list[int | float]], w0: float, w: list[float], V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix (n_samples \u00d7 n_features) given as a list of lists.\n        w0: Global bias term.\n        w: Linear weights (length = n_features).\n        V: Latent factor matrix (n_features \u00d7 k).\n\n    Returns:\n        List with one prediction per sample, rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    w = np.asarray(w, dtype=float)\n    V = np.asarray(V, dtype=float)\n    linear_term = w0 + X @ w\n    XV = X @ V\n    X2_V2 = X ** 2 @ V ** 2\n    interaction_term = 0.5 * np.sum(XV ** 2 - X2_V2, axis=1)\n    y_hat = linear_term + interaction_term\n    return np.round(y_hat, 4).tolist()"}
{"task_id": 514, "completion_id": 0, "solution": "def switch_phase(current_phase: str, set_train: bool | None=None, set_test: bool | None=None) -> str:\n    \"\"\"Update the training/testing phase.\n    \n    See task description for full specification.\n    \"\"\"\n    if current_phase not in ('training', 'testing'):\n        return -1\n    if set_train is not None and set_test is not None:\n        return -1\n    if set_train is not None and (not isinstance(set_train, bool)):\n        return -1\n    if set_test is not None and (not isinstance(set_test, bool)):\n        return -1\n    if set_train is not None:\n        return 'training' if set_train else 'testing'\n    if set_test is not None:\n        return 'testing' if set_test else 'training'\n    return 'testing' if current_phase == 'training' else 'training'"}
{"task_id": 515, "completion_id": 0, "solution": "from typing import Any, Dict\ndef flatten_parameters(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Return a flattened (dot\u2010separated) copy of a potentially nested\n    parameter dictionary.\n\n    The function is purely functional: the *input dictionary is never\n    mutated*; instead a brand-new flat dictionary is produced.\n\n    Examples\n    --------\n    >>> params = {\n    ...     \"optimizer\": {\n    ...         \"type\": \"SGD\",\n    ...         \"config\": {\"lr\": 0.01, \"momentum\": 0.9}\n    ...     },\n    ...     \"epochs\": 25\n    ... }\n    >>> flatten_parameters(params)\n    {'optimizer.type': 'SGD',\n     'optimizer.config.lr': 0.01,\n     'optimizer.config.momentum': 0.9,\n     'epochs': 25}\n    \"\"\"\n\n    def _recurse(current: Dict[str, Any], prefix: str, out: Dict[str, Any]) -> None:\n        \"\"\"Recursive helper that populates `out` in-place.\"\"\"\n        for (key, value) in current.items():\n            full_key = f'{prefix}.{key}' if prefix else key\n            if isinstance(value, dict):\n                _recurse(value, full_key, out)\n            else:\n                out[full_key] = value\n    flat: Dict[str, Any] = {}\n    _recurse(params, prefix='', out=flat)\n    return flat"}
{"task_id": 516, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef euclidean_distance(a, b):\n    \"\"\"Compute the Euclidean distance between two numeric vectors.\n\n    The vectors may be provided as Python lists or NumPy arrays and may contain\n    integers or floats.  If the vectors have different numbers of total\n    elements, the function must return -1.\n\n    Args:\n        a (list[int | float] | np.ndarray): First vector.\n        b (list[int | float] | np.ndarray): Second vector.\n\n    Returns:\n        float: Euclidean distance rounded to 4 decimal places, or -1 if the\n               vectors are of unequal length.\n    \"\"\"\n    a_arr = np.asarray(a, dtype=float).ravel()\n    b_arr = np.asarray(b, dtype=float).ravel()\n    if a_arr.size != b_arr.size:\n        return -1\n    distance = np.linalg.norm(a_arr - b_arr)\n    return round(float(distance), 4)"}
{"task_id": 517, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Compute a numerically-stable, element-wise sigmoid activation.\n\n    Parameters\n    ----------\n    z : int | float | list | tuple | numpy.ndarray\n        Input value(s).\n\n    Returns\n    -------\n    float | list\n        If `z` is a scalar a Python float is returned, otherwise a (nested)\n        list whose elements are the sigmoid of the corresponding entries in\n        `z`, rounded to four decimal places.\n    \"\"\"\n    arr = np.asarray(z, dtype=np.float64)\n    out = np.empty_like(arr)\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    exp_x = np.exp(arr[neg_mask])\n    out[neg_mask] = exp_x / (1.0 + exp_x)\n    out = np.round(out, 4)\n    if out.ndim == 0:\n        return float(out)\n    else:\n        return out.tolist()"}
{"task_id": 518, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Callable\nfrom functools import wraps\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D NumPy array whose second\n    dimension is larger than one, it is assumed to be one-hot encoded and is\n    replaced by the index of the maximum value along axis 1 before the wrapped\n    *function* is executed.\n    \"\"\"\n\n    def _decode(arr: np.ndarray) -> np.ndarray:\n        \"\"\"Return a 1-D label array if *arr* looks like one-hot, else *arr*.\"\"\"\n        if isinstance(arr, np.ndarray) and arr.ndim == 2 and (arr.shape[1] > 1):\n            return arr.argmax(axis=1)\n        return arr\n\n    @wraps(function)\n    def wrapper(actual: np.ndarray, predicted: np.ndarray, *args, **kwargs):\n        actual_decoded = _decode(actual)\n        predicted_decoded = _decode(predicted)\n        return function(actual_decoded, predicted_decoded, *args, **kwargs)\n    return wrapper"}
{"task_id": 520, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).\n    The derivative is computed element-wise as z * (1 - z) and rounded\n    to 4 decimal places.\n\n    Args:\n        z: float, list, or NumPy ndarray holding sigmoid outputs.\n\n    Returns:\n        float if *z* is a scalar; otherwise a (nested) list with the same\n        shape as *z*, each value rounded to 4 decimals.\n    \"\"\"\n    if z == []:\n        return []\n    arr = np.asarray(z, dtype=float)\n    grad = np.round(arr * (1.0 - arr), 4)\n    return float(grad) if grad.ndim == 0 else grad.tolist()"}
{"task_id": 521, "completion_id": 0, "solution": "import numpy as np\ndef relu_grad(z):\n    \"\"\"Element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    z : scalar, list, or np.ndarray\n        Input data.\n\n    Returns\n    -------\n    int or list\n        0/1 values that match the shape of `z`.\n    \"\"\"\n    if np.isscalar(z) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        return int(z > 0)\n    arr = np.asarray(z)\n    grad = (arr > 0).astype(int)\n    return grad.tolist()"}
{"task_id": 522, "completion_id": 0, "solution": "import numpy as np\ndef standard_normal(weight_shape):\n    \"\"\"Return a tensor of the given shape filled with N(0,1) samples.\n\n    The NumPy random seed is reset to 42 on every call, the generated\n    values are rounded to 4 decimal places and the final result is\n    converted to a (nested) Python list.\n\n    Parameters\n    ----------\n    weight_shape : int | tuple[int, ...] | list[int]\n        Desired tensor shape.\n\n    Returns\n    -------\n    list\n        Nested Python list containing the rounded samples.\n    \"\"\"\n    np.random.seed(42)\n    if isinstance(weight_shape, int):\n        shape = (weight_shape,)\n    else:\n        shape = tuple(weight_shape)\n    arr = np.random.standard_normal(shape)\n    arr = np.round(arr, 4)\n    return arr.tolist()"}
{"task_id": 523, "completion_id": 0, "solution": "import numpy as np\ndef he_normal(weight_shape: tuple[int, ...]) -> list:\n    \"\"\"Initialise a weight tensor using the He normal (Kaiming normal) distribution.\n\n    Args:\n        weight_shape: Shape of the desired weight tensor. For a dense layer this\n            is usually (fan_in, fan_out). For a 2-D convolution it is\n            (kernel_h, kernel_w, in_channels, out_channels).\n\n    Returns:\n        Nested Python lists containing the initialised weights, rounded to four\n        decimal places.\n    \"\"\"\n    if len(weight_shape) == 2:\n        fan_in = weight_shape[0]\n    elif len(weight_shape) == 4:\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    else:\n        fan_in = weight_shape[0]\n    std = (2.0 / fan_in) ** 0.5\n    weights = np.random.randn(*weight_shape) * std\n    rounded = np.round(weights, 4)\n    return rounded.tolist()"}
{"task_id": 525, "completion_id": 0, "solution": "import numpy as np\ndef ridge_regression(X: np.ndarray, y: np.ndarray, lambda_: float=0.0) -> tuple[list[float], float]:\n    \"\"\"Fit a Ridge (L2-regularised) linear regression model in closed form.\n\n    Args:\n        X: (m, n) design matrix.\n        y: (m,) target vector (or (m, 1)).\n        lambda_: Non-negative regularisation strength.\n\n    Returns\n    -------\n        weights: list of length n with the coefficients (rounded to 4 dp).\n        bias:    scalar intercept term (rounded to 4 dp).\n    \"\"\"\n    if lambda_ < 0:\n        raise ValueError('lambda_ must be non-negative')\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D array')\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    (m, n) = X.shape\n    if y.shape[0] != m:\n        raise ValueError('X and y have incompatible lengths')\n    x_mean = X.mean(axis=0)\n    y_mean = y.mean()\n    Xc = X - x_mean\n    yc = y - y_mean\n    A = Xc.T @ Xc\n    A += lambda_ * np.eye(n)\n    w = np.linalg.pinv(A) @ Xc.T @ yc\n    b = y_mean - x_mean @ w\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return (w_rounded, b_rounded)"}
{"task_id": 527, "completion_id": 0, "solution": "import os\nfrom typing import List, Union\ndef extract_archive(file_path: str, path: str='.', archive_format: Union[str, List[str], None]='auto') -> bool:\n    \"\"\"Checks whether *file_path* belongs to an allowed archive format.\n\n    The helper never raises and never performs real extraction.\n\n    Args:\n        file_path: Full path (or name) of the archive file.\n        path:     Destination folder \u2013 ignored in this simplified version.\n        archive_format:   \u2022 'auto' \u2500\u2192 ['tar', 'zip']\n                          \u2022 'tar' / 'zip'  \u2192 only that format\n                          \u2022 list           \u2192 that list\n                          \u2022 None / []      \u2192 no format allowed\n\n    Returns:\n        True  \u2013 extension matches one of the allowed formats\n        False \u2013 otherwise or on any error\n    \"\"\"\n    try:\n        if archive_format is None:\n            allowed_formats: List[str] = []\n        elif isinstance(archive_format, str):\n            if archive_format.lower() == 'auto':\n                allowed_formats = ['tar', 'zip']\n            else:\n                allowed_formats = [archive_format]\n        else:\n            allowed_formats = list(archive_format)\n        if not allowed_formats:\n            return False\n        format_to_suffixes = {'tar': ['.tar', '.tar.gz', '.tgz', '.tar.bz', '.tar.bz2', '.tbz'], 'zip': ['.zip']}\n        fname = os.path.basename(str(file_path)).lower()\n        for fmt in allowed_formats:\n            suffixes = format_to_suffixes.get(str(fmt).lower())\n            if not suffixes:\n                continue\n            for suf in suffixes:\n                if fname.endswith(suf):\n                    return True\n        return False\n    except Exception:\n        return False"}
{"task_id": 528, "completion_id": 0, "solution": "import numpy as np\ndef decision_boundary_grid(X: list[list[int | float]], W: list[int | float], b: float, grid_n: int=100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) receives the label  1 if  w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0\n    and \u22121 otherwise.  The returned list has shape (grid_n \u00d7 grid_n) and\n    contains those labels for a regular grid that spans the bounding box\n    of *X*.\n\n    Args:\n        X: 2-D data set \u2013 list of [x\u2081 , x\u2082] pairs (shape n\u00d72).\n        W: Length-2 sequence [w\u2081 , w\u2082] with the classifier\u2019s weights.\n        b: Bias term of the classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Python list of lists with shape (grid_n \u00d7 grid_n) whose entries\n        are exclusively 1 or \u22121.\n    \"\"\"\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim != 2 or X_arr.shape[1] != 2:\n        raise ValueError('X must be a 2-D array-like with exactly two columns.')\n    if len(W) != 2:\n        raise ValueError('W must contain exactly two elements.')\n    if grid_n <= 0:\n        raise ValueError('grid_n must be a positive integer.')\n    (x1_min, x1_max) = (X_arr[:, 0].min(), X_arr[:, 0].max())\n    (x2_min, x2_max) = (X_arr[:, 1].min(), X_arr[:, 1].max())\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    (x1_grid, x2_grid) = np.meshgrid(x1_plot, x2_plot, indexing='xy')\n    (w1, w2) = (float(W[0]), float(W[1]))\n    decision_values = w1 * x1_grid + w2 * x2_grid + b\n    labels = np.where(decision_values >= 0, 1, -1)\n    return labels.tolist()"}
{"task_id": 529, "completion_id": 0, "solution": "import math\ndef generate_progress_bar(current: int, target: int | None, width: int=30) -> str:\n    \"\"\"Return an ASCII progress bar string similar to Keras' *Progbar*.\n\n    Parameters\n    ----------\n    current : int\n        The index of the most recently completed step.\n    target : int | None\n        The total number of steps, or ``None`` if it is unknown.\n    width : int, default=30\n        The length (in characters) of the bar itself (everything situated\n        between the two square brackets).\n\n    Returns\n    -------\n    str\n        A textual progress bar that fully complies with the specification\n        given in the task description (all spaces, brackets, arrows and dots\n        have to be placed exactly).\n    \"\"\"\n    if target is None:\n        return f'{current:>7d}/Unknown'\n    d = len(str(target))\n    prog_width = min(int(width * current / target), width)\n    bar_chars = []\n    if prog_width > 0:\n        bar_chars.append('=' * (prog_width - 1))\n        if current < target:\n            bar_chars.append('>')\n        else:\n            bar_chars.append('=')\n    bar_chars.append('.' * (width - prog_width))\n    bar = ''.join(bar_chars)\n    return f'{current:>{d}d}/{target} [{bar}]'"}
{"task_id": 530, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculates the mean Binary Cross-Entropy (BCE) loss.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        Ground-truth binary labels (0 or 1).\n    y_pred : np.ndarray\n        Predicted probabilities for the positive class. Must have the same\n        shape as *y_true* and values in (0, 1).\n\n    Returns\n    -------\n    float\n        Mean BCE loss rounded to 4 decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('`y_true` and `y_pred` must have the same shape.')\n    eps = 1e-07\n    y_pred_clipped = np.clip(y_pred, eps, 1 - eps)\n    loss = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n    mean_loss = np.mean(loss)\n    return float(np.round(mean_loss, 4))"}
{"task_id": 531, "completion_id": 0, "solution": "import numpy as np\ndef generalized_cosine(window_len: int, coefs: list[float], symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a generalized cosine window.\n\n    Args\n    ----\n    window_len : int\n        Desired length L of the window (must be > 0).\n    coefs : list[float]\n        Coefficients a_k, k = 0 \u2026 K.\n    symmetric : bool, optional\n        \u2022 True  \u2192 symmetric window (exactly L samples from \u2013\u03c0 to  \u03c0)  \n        \u2022 False \u2192 periodic  window (L + 1 samples from \u2013\u03c0 to  \u03c0, last one\n          discarded).  Default is False.\n\n    Returns\n    -------\n    list[float]\n        The window rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        raise ValueError('window_len must be a positive integer.')\n    if not coefs:\n        raise ValueError('coefs list must contain at least one value.')\n    n = np.arange(window_len)\n    if symmetric:\n        denom = window_len - 1 if window_len > 1 else 1\n    else:\n        denom = window_len\n    theta = -np.pi + 2.0 * np.pi * n / denom\n    k = np.arange(len(coefs)).reshape(-1, 1)\n    a = np.asarray(coefs, dtype=float).reshape(-1, 1)\n    w = np.sum(a * np.cos(k * theta), axis=0)\n    return [round(float(val), 4) for val in w]"}
{"task_id": 532, "completion_id": 0, "solution": "def count_fc_parameters(layers: list[int]) -> int:\n    \"\"\"Calculate total trainable parameters (weights + biases) in a fully-connected network.\n\n    Args:\n        layers: List of integers where each element represents the number of\n            neurons in the respective layer (input layer first).\n\n    Returns:\n        An integer \u2013 total count of trainable parameters. Returns 0 when fewer\n        than two layers are supplied.\n    \"\"\"\n    if len(layers) < 2:\n        return 0\n    total_params = 0\n    for (n_in, n_out) in zip(layers[:-1], layers[1:]):\n        total_params += n_in * n_out\n        total_params += n_out\n    return total_params"}
{"task_id": 533, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_likelihood(x, mean, sigma):\n    \"\"\"Compute the probability density of a normal (Gaussian) distribution.\n    \n    Parameters\n    ----------\n    x : float | list[float] | np.ndarray\n        Point(s) where the density will be evaluated.\n    mean : float\n        The mean (\u00b5) of the distribution.\n    sigma : float\n        The standard deviation (\u03c3) of the distribution (\u03c3 > 0).\n    \n    Returns\n    -------\n    float | list[float]\n        PDF value(s) rounded to four decimal places.  A float is returned\n        when `x` is a scalar, otherwise a list is returned.\n    \"\"\"\n    is_scalar = np.isscalar(x)\n    x_arr = np.asarray(x, dtype=float)\n    coeff = 1.0 / (sigma * np.sqrt(2.0 * np.pi))\n    exponent = -(x_arr - mean) ** 2 / (2.0 * sigma ** 2)\n    pdf_vals = coeff * np.exp(exponent)\n    pdf_vals = np.round(pdf_vals, 4)\n    if is_scalar:\n        return float(pdf_vals)\n    else:\n        return pdf_vals.tolist()"}
{"task_id": 534, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_(Z):\n    \"\"\"Compute the logistic sigmoid activation element-wise.\n\n    Args:\n        Z: A scalar, 1-D list/array, or 2-D list/array of real numbers.\n\n    Returns:\n        float  \u2013 if `Z` is a Python scalar;\n        list   \u2013 for every other kind of input (shape preserved),\n                 with every element rounded to 4 decimals.\n    \"\"\"\n    if np.isscalar(Z) or (isinstance(Z, np.ndarray) and Z.ndim == 0):\n        s = 1.0 / (1.0 + np.exp(-float(Z)))\n        return round(float(s), 4)\n    arr = np.asarray(Z, dtype=float)\n    sig = 1.0 / (1.0 + np.exp(-arr))\n    sig = np.round(sig, 4)\n    return sig.tolist()"}
{"task_id": 536, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression(X_train, y_train, X_test, method='normal', alpha=0.01, num_iter=1000):\n    \"\"\"Linear regression by Normal Equation or batch Gradient Descent.\"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train, dtype=float).ravel()\n    (m, n) = X_train.shape\n    X_train = np.hstack((np.ones((m, 1)), X_train))\n    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n    if method.lower() == 'normal':\n        XtX_inv = np.linalg.inv(X_train.T @ X_train)\n        theta = XtX_inv @ X_train.T @ y_train\n    else:\n        theta = np.zeros(n + 1)\n        for _ in range(num_iter):\n            error = X_train @ theta - y_train\n            grad = X_train.T @ error / m\n            theta -= alpha * grad\n    predictions = X_test @ theta\n    return np.round(predictions, 4).tolist()"}
{"task_id": 537, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier (from scratch).\"\"\"\n    eps = 1e-09\n    classes = np.array([0, 1])\n    mask0 = y_train == 0\n    mask1 = y_train == 1\n    mu0 = X_train[mask0].mean(axis=0)\n    mu1 = X_train[mask1].mean(axis=0)\n    std0 = X_train[mask0].std(axis=0, ddof=0) + eps\n    std1 = X_train[mask1].std(axis=0, ddof=0) + eps\n    n_samples = len(y_train)\n    prior0 = mask0.sum() / n_samples\n    prior1 = mask1.sum() / n_samples\n    log_prior = np.log(np.array([prior0, prior1]))\n    log_coeff0 = -0.5 * np.log(2 * np.pi) - np.log(std0)\n    log_coeff1 = -0.5 * np.log(2 * np.pi) - np.log(std1)\n    inv_var0 = 1.0 / (2 * std0 ** 2)\n    inv_var1 = 1.0 / (2 * std1 ** 2)\n    predictions = []\n    for x in X_test:\n        ll0 = (log_coeff0 - (x - mu0) ** 2 * inv_var0).sum()\n        ll1 = (log_coeff1 - (x - mu1) ** 2 * inv_var1).sum()\n        log_post0 = log_prior[0] + ll0\n        log_post1 = log_prior[1] + ll1\n        pred = 0 if log_post0 >= log_post1 else 1\n        predictions.append(pred)\n    return predictions"}
{"task_id": 539, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Computes the binary cross-entropy (log-loss) cost.\n\n    Parameters\n    ----------\n    AL : np.ndarray\n        Model predicted probabilities, shape (m,) or (1, m).\n    Y  : np.ndarray\n        Ground-truth labels (0 or 1), same shape as AL.\n\n    Returns\n    -------\n    float\n        Scalar cost value.\n    \"\"\"\n    if AL.shape != Y.shape:\n        raise ValueError('AL and Y must have the same shape.')\n    AL = AL.ravel()\n    Y = Y.ravel()\n    m = AL.size\n    eps = 1e-15\n    AL_clipped = np.clip(AL, eps, 1.0 - eps)\n    cost = -np.sum(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped)) / m\n    return float(cost)"}
{"task_id": 540, "completion_id": 0, "solution": "import numpy as np\ndef derivative_sigmoid(Z: np.ndarray) -> list:\n    \"\"\"Return the element-wise derivative of the sigmoid function.\n\n    Args:\n        Z: NumPy array (any shape) containing numeric values.\n\n    Returns:\n        A Python list with the same shape as *Z*, containing the\n        derivative of the sigmoid evaluated at each element, rounded to\n        4 decimal places.\n    \"\"\"\n    pos_mask = Z >= 0\n    neg_mask = ~pos_mask\n    sigmoid = np.empty_like(Z, dtype=np.float64)\n    sigmoid[pos_mask] = 1.0 / (1.0 + np.exp(-Z[pos_mask]))\n    exp_z = np.exp(Z[neg_mask])\n    sigmoid[neg_mask] = exp_z / (1.0 + exp_z)\n    derivative = sigmoid * (1.0 - sigmoid)\n    derivative_rounded = np.round(derivative, 4)\n    return derivative_rounded.tolist()"}
{"task_id": 541, "completion_id": 0, "solution": "import numpy as np\ndef return_input_grads(y, y_hat):\n    \"\"\"Compute the gradient of Binary Cross-Entropy loss w.r.t. each logit.\n\n    Parameters\n    ----------\n    y : array-like\n        Ground-truth binary labels (0 or 1). Shape can be scalar, 1-D or 2-D.\n    y_hat : array-like\n        Predicted probabilities (after sigmoid) with the same shape as y.\n\n    Returns\n    -------\n    list\n        Element-wise gradient (y_hat \u2212 y), rounded to 4 decimals and returned\n        as a regular Python list with the original shape.\n    \"\"\"\n    y_arr = np.asarray(y, dtype=float)\n    y_hat_arr = np.asarray(y_hat, dtype=float)\n    if y_arr.shape != y_hat_arr.shape:\n        raise ValueError('y and y_hat must have the same shape.')\n    grads = y_hat_arr - y_arr\n    grads = np.round(grads, 4)\n    return grads.tolist()"}
{"task_id": 542, "completion_id": 0, "solution": "import numpy as np\ndef rmsprop_update(params: list[np.ndarray], grads: list[np.ndarray], s: list[np.ndarray] | None=None, alpha: float=0.01, beta: float=0.9, epsilon: float=1e-08) -> tuple[list[np.ndarray], list[np.ndarray]]:\n    \"\"\"Performs a single RMSProp optimisation step.\n    \n    Parameters\n    ----------\n    params : list[np.ndarray]\n        Current parameter tensors \u03b8.\n    grads : list[np.ndarray]\n        Corresponding gradients g.\n    s : list[np.ndarray] | None, optional\n        Running averages of squared gradients. If None/empty,\n        they are initialised with zeros of the same shape as grads.\n    alpha : float, optional\n        Learning-rate (default 0.01).\n    beta : float, optional\n        Decay factor for the running average (default 0.9).\n    epsilon : float, optional\n        Small constant for numerical stability (default 1e-8).\n    \n    Returns\n    -------\n    tuple[list[np.ndarray], list[np.ndarray]]\n        new_params : list with updated (and 6-dp rounded) parameters.\n        new_s      : list with updated running averages.\n    \"\"\"\n    if not s:\n        s = [np.zeros_like(g, dtype=float) for g in grads]\n    elif len(s) != len(grads):\n        raise ValueError('Length of `s` must match `grads` (or be None/empty).')\n    new_params: list[np.ndarray] = []\n    new_s: list[np.ndarray] = []\n    for (\u03b8, g, s_prev) in zip(params, grads, s):\n        s_next = beta * s_prev + (1.0 - beta) * np.square(g)\n        denom = np.sqrt(s_next) + epsilon\n        \u03b8_next = \u03b8 - alpha * g / denom\n        new_s.append(s_next)\n        new_params.append(np.round(\u03b8_next, 6))\n    return (new_params, new_s)"}
{"task_id": 543, "completion_id": 0, "solution": "from collections import Counter\ndef cal_gini_index(data: list) -> float:\n    \"\"\"Calculate the Gini index of a list of class labels.\n\n    Args:\n        data: A list of hashable values representing class labels.\n\n    Returns:\n        The Gini index rounded to 4 decimal places. If the input list is empty\n        return 0.0.\n    \"\"\"\n    if not data:\n        return 0.0\n    total = len(data)\n    counts = Counter(data)\n    squared_sum = sum(((freq / total) ** 2 for freq in counts.values()))\n    gini = 1.0 - squared_sum\n    return round(gini, 4)"}
{"task_id": 544, "completion_id": 0, "solution": "def split_tree(data: list, fea: int, value: float):\n    \"\"\"Split a data set into two ordered subsets according to a threshold.\n\n    Args:\n        data (list): List of samples. Each sample is a list or tuple of feature values.\n        fea (int): Index of the feature used for the split.\n        value (float): Threshold for the split.\n\n    Returns:\n        tuple[list, list]: Two lists (set1, set2) where set1 contains the samples\n        with feature value >= threshold and set2 contains the rest.\n    \"\"\"\n    (set1, set2) = ([], [])\n    for sample in data:\n        if fea >= len(sample) or fea < -len(sample):\n            raise IndexError(f'Feature index {fea} out of range for sample {sample}')\n        if sample[fea] >= value:\n            set1.append(sample)\n        else:\n            set2.append(sample)\n    return (set1, set2)"}
{"task_id": 545, "completion_id": 0, "solution": "def tree_stats(root: 'node | None') -> tuple[int, int]:\n    \"\"\"Return the number of leaf nodes and the maximum depth of the tree.\n\n    A **leaf** is a node whose ``results`` attribute is *not* ``None``.\n\n    Parameters\n    ----------\n    root : node | None\n        Root of the decision tree.  If *None*, the tree is considered empty.\n\n    Returns\n    -------\n    tuple[int, int]\n        (leaf_count, max_depth)\n    \"\"\"\n\n    def _traverse(cur: 'node | None') -> tuple[int, int]:\n        \"\"\"\n        Recursively compute (leaf_count, max_depth) for the subtree rooted\n        at *cur*.\n        \"\"\"\n        if cur is None:\n            return (0, 0)\n        if cur.results is not None:\n            return (1, 1)\n        (left_leaves, left_depth) = _traverse(cur.left)\n        (right_leaves, right_depth) = _traverse(cur.right)\n        leaf_count = left_leaves + right_leaves\n        max_depth = 1 + max(left_depth, right_depth)\n        return (leaf_count, max_depth)\n    return _traverse(root)"}
{"task_id": 546, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Any, Dict, List\ndef build_tree(data: List[List[Any]]) -> Dict[str, Any]:\n    \"\"\"Build a binary classification tree (CART) using the Gini criterion.\n\n    A sample is a list whose last element is the class label, the preceding\n    elements are feature values.  Categorical and numerical values are both\n    compared by ordinary equality.\n\n    The tree is represented as nested dictionaries:\n        Leaf   -> {\"label\": class_label}\n        Branch -> {\"feature\": feature_index,\n                   \"value\":   split_value,\n                   \"left\":    subtree for samples with feature == value,\n                   \"right\":   subtree for samples with feature != value}\n\n    If *data* is empty the empty dict is returned.\n    \"\"\"\n\n    def gini(subset: List[List[Any]]) -> float:\n        \"\"\"Gini impurity of *subset*.\"\"\"\n        n = len(subset)\n        if n == 0:\n            return 0.0\n        counts = Counter((row[-1] for row in subset))\n        return 1.0 - sum(((cnt / n) ** 2 for cnt in counts.values()))\n\n    def majority_label(subset: List[List[Any]]) -> Any:\n        \"\"\"Most frequent class, ties are broken by first\u2010appearance order.\"\"\"\n        counts: Counter = Counter()\n        first_seen: Dict[Any, int] = {}\n        for (idx, row) in enumerate(subset):\n            label = row[-1]\n            counts[label] += 1\n            if label not in first_seen:\n                first_seen[label] = idx\n        best_count = max(counts.values())\n        candidates = [lbl for (lbl, c) in counts.items() if c == best_count]\n        return min(candidates, key=lambda lbl: first_seen[lbl])\n\n    def sorted_values(values):\n        \"\"\"Return *values* sorted, robust to incomparable mixed types.\"\"\"\n        try:\n            return sorted(values)\n        except TypeError:\n            return sorted(values, key=lambda v: str(v))\n\n    def grow(subset: List[List[Any]]) -> Dict[str, Any]:\n        \"\"\"Recursively build the tree for *subset*.\"\"\"\n        if gini(subset) == 0.0:\n            return {'label': subset[0][-1]}\n        current_impurity = gini(subset)\n        best_gain = 0.0\n        best_feature = None\n        best_value = None\n        best_left = best_right = None\n        n_features = len(subset[0]) - 1\n        for feat_idx in range(n_features):\n            unique_vals = {row[feat_idx] for row in subset}\n            for val in sorted_values(unique_vals):\n                left = [row for row in subset if row[feat_idx] == val]\n                right = [row for row in subset if row[feat_idx] != val]\n                if not left or not right:\n                    continue\n                weighted = len(left) / len(subset) * gini(left) + len(right) / len(subset) * gini(right)\n                gain = current_impurity - weighted\n                if gain > best_gain:\n                    best_gain = gain\n                    (best_feature, best_value) = (feat_idx, val)\n                    (best_left, best_right) = (left, right)\n        if best_gain <= 0.0:\n            return {'label': majority_label(subset)}\n        return {'feature': best_feature, 'value': best_value, 'left': grow(best_left), 'right': grow(best_right)}\n    return grow(data) if data else {}"}
{"task_id": 547, "completion_id": 0, "solution": "import numpy as np\ndef forward_step(A: list[list[float]], B: list[list[float]], S: list[float], obs: list[int], t: int) -> list[float] | int:\n    \"\"\"Compute the forward probability vector \u03b1_t for a Hidden-Markov model.\n\n    All returned numbers are rounded to 4 decimals.  If any input is\n    malformed or t is out of bounds, the function returns -1.\n    \"\"\"\n    try:\n        A = np.asarray(A, dtype=float)\n        B = np.asarray(B, dtype=float)\n        S = np.asarray(S, dtype=float)\n    except Exception:\n        return -1\n    if A.ndim != 2 or A.shape[0] == 0 or A.shape[0] != A.shape[1]:\n        return -1\n    n = A.shape[0]\n    if B.ndim != 2 or B.shape[0] != n or B.shape[1] == 0:\n        return -1\n    m = B.shape[1]\n    if S.ndim != 1 or S.shape[0] != n:\n        return -1\n    if not isinstance(obs, (list, tuple)) or len(obs) == 0:\n        return -1\n    if not all((isinstance(k, int) for k in obs)):\n        return -1\n    if any((k < 0 or k >= m for k in obs)):\n        return -1\n    if not isinstance(t, int) or t < 0 or t >= len(obs):\n        return -1\n    alpha = S * B[:, obs[0]]\n    for step in range(1, t + 1):\n        alpha = alpha @ A\n        alpha = alpha * B[:, obs[step]]\n    return [round(float(x), 4) for x in alpha]"}
{"task_id": 548, "completion_id": 0, "solution": "import numpy as np\ndef compute_xi(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int], t: int) -> list[list[float]]:\n    \"\"\"Return the \u03be (xi) matrix for a single time step in a discrete HMM.\n\n    \u03be_t(i,j) = P(q_t=i , q_{t+1}=j | obs, \u03bb)\n\n    All entries of the returned matrix are rounded to 4 decimals.\n    If t is outside 0 \u2026 len(obs)-2 the function returns -1.\n    \"\"\"\n    T = len(obs)\n    if t < 0 or t >= T - 1:\n        return -1\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    pi = np.asarray(pi, dtype=float)\n    N = A.shape[0]\n    alpha = np.zeros((T, N))\n    alpha[0] = pi * B[:, obs[0]]\n    for k in range(1, T):\n        alpha[k] = alpha[k - 1] @ A * B[:, obs[k]]\n    beta = np.zeros((T, N))\n    beta[T - 1] = np.ones(N)\n    for k in range(T - 2, -1, -1):\n        beta[k] = A @ (B[:, obs[k + 1]] * beta[k + 1])\n    alpha_t = alpha[t]\n    beta_t1 = beta[t + 1]\n    emit_next = B[:, obs[t + 1]] * beta_t1\n    numer = alpha_t[:, None] * A * emit_next[None, :]\n    denom = numer.sum()\n    xi_t = numer / denom\n    return np.round(xi_t, 4).tolist()"}
{"task_id": 549, "completion_id": 0, "solution": "import numpy as np\ndef cal_dis(x, y):\n    \"\"\"Compute the squared Euclidean distance between two vectors.\n\n    Parameters\n    ----------\n    x : list, tuple, or np.ndarray\n        First vector of numbers.\n    y : list, tuple, or np.ndarray\n        Second vector of numbers.\n\n    Returns\n    -------\n    float\n        Squared Euclidean distance rounded to 4 decimal places, or -1 if the\n        vectors are of different lengths.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=float).ravel()\n    y_arr = np.asarray(y, dtype=float).ravel()\n    if x_arr.size != y_arr.size:\n        return -1\n    dist_sq = np.sum((x_arr - y_arr) ** 2)\n    return float(round(dist_sq, 4))"}
{"task_id": 551, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef _gini(labels: np.ndarray) -> float:\n    \"\"\"Gini impurity of a 1-D array of class labels.\"\"\"\n    n = len(labels)\n    if n == 0:\n        return 0.0\n    counts = Counter(labels)\n    return 1.0 - sum(((cnt / n) ** 2 for cnt in counts.values()))\ndef best_gini_split(X: np.ndarray, y: np.ndarray, epsilon: float=0.001) -> tuple[int, object, float]:\n    \"\"\"Find the best equality split (feature == value) w.r.t. Gini impurity.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Shape (n_samples, n_features).  Elements may be numeric or objects.\n    y : np.ndarray\n        Shape (n_samples, ).\n    epsilon : float, optional\n        Required minimum impurity decrease.\n\n    Returns\n    -------\n    tuple\n        (best_feature_index, best_split_value, best_gini_value)\n        or (-1, None, parent_gini) if no valid split is found.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    parent_gini = _gini(y)\n    best_gini = float('inf')\n    best_feature = -1\n    best_value = None\n    for feat_idx in range(n_features):\n        col = X[:, feat_idx]\n        for val in np.unique(col):\n            mask_left = col == val\n            n_left = np.sum(mask_left)\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:\n                continue\n            g_left = _gini(y[mask_left])\n            g_right = _gini(y[~mask_left])\n            weighted_gini = (n_left * g_left + n_right * g_right) / n_samples\n            if weighted_gini < best_gini:\n                best_gini = weighted_gini\n                best_feature = feat_idx\n                best_value = val\n    if best_feature == -1 or parent_gini - best_gini < epsilon:\n        return (-1, None, float(parent_gini))\n    return (best_feature, best_value, float(best_gini))"}
{"task_id": 552, "completion_id": 0, "solution": "import numpy as np\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation uses the forward-backward algorithm with scaling\n    to obtain numerically stable \u03b1, \u03b2 and finally \u03b3\u209c.\n\n    Args:\n        hmm (dict): HMM parameters with keys\n                    'pi' -> 1-D initial state distribution (N,)\n                    'A'  -> state transition matrix      (N,N)\n                    'B'  -> emission matrix              (N,M)\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t,\n                     rounded to four decimals.  If t is out of range an\n                     empty list is returned.\n    \"\"\"\n    if t < 0 or t >= len(observations):\n        return []\n    pi = np.asarray(hmm['pi'], dtype=float)\n    A = np.asarray(hmm['A'], dtype=float)\n    B = np.asarray(hmm['B'], dtype=float)\n    N = pi.shape[0]\n    T = len(observations)\n    obs_idx = observations\n    alpha = np.zeros((T, N), dtype=float)\n    c = np.zeros(T, dtype=float)\n    alpha[0] = pi * B[:, obs_idx[0]]\n    c[0] = 1.0 / alpha[0].sum() if alpha[0].sum() != 0 else 1.0\n    alpha[0] *= c[0]\n    for tt in range(1, T):\n        alpha[tt] = alpha[tt - 1] @ A * B[:, obs_idx[tt]]\n        denom = alpha[tt].sum()\n        c[tt] = 1.0 / denom if denom != 0 else 1.0\n        alpha[tt] *= c[tt]\n    beta = np.zeros((T, N), dtype=float)\n    beta[-1] = 1.0\n    beta[-1] *= c[-1]\n    for tt in range(T - 2, -1, -1):\n        beta[tt] = A @ (B[:, obs_idx[tt + 1]] * beta[tt + 1])\n        beta[tt] *= c[tt]\n    gamma_t = alpha[t] * beta[t]\n    total = gamma_t.sum()\n    if total == 0:\n        return [0.0 for _ in range(N)]\n    gamma_t /= total\n    return [round(float(p), 4) for p in gamma_t]"}
{"task_id": 553, "completion_id": 0, "solution": "import numpy as np\ndef logistic_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, alpha: float=0.01, max_iter: int=1000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model with batch gradient descent and\n    predict class labels for a test set.\"\"\"\n    n_train = X_train.shape[0]\n    n_test = X_test.shape[0]\n    X_train_b = np.hstack((np.ones((n_train, 1)), X_train))\n    X_test_b = np.hstack((np.ones((n_test, 1)), X_test))\n    w = np.zeros(X_train_b.shape[1], dtype=float)\n\n    def _sigmoid(z):\n        positive = z >= 0\n        negative = ~positive\n        out = np.empty_like(z, dtype=float)\n        out[positive] = 1.0 / (1.0 + np.exp(-z[positive]))\n        exp_z = np.exp(z[negative])\n        out[negative] = exp_z / (1.0 + exp_z)\n        return out\n    for _ in range(max_iter):\n        preds = _sigmoid(X_train_b @ w)\n        gradient = X_train_b.T @ (y_train - preds)\n        gradient /= n_train\n        w += alpha * gradient\n    probs = _sigmoid(X_test_b @ w)\n    predictions = (probs >= 0.5).astype(int)\n    return [int(p) for p in predictions]"}
{"task_id": 555, "completion_id": 0, "solution": "import numpy as np\nfrom collections import deque\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN) implemented from scratch.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Array of shape (n_samples, n_features).\n    eps : float\n        Neighbourhood radius.\n    min_samples : int\n        Minimum number of points (inclusive) needed for a core point.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels (1, 2, \u2026) for clustered points; noise is \u22121.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array (n_samples, n_features)')\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n    diff = data[:, None, :] - data[None, :, :]\n    dist_mat = np.linalg.norm(diff, axis=-1)\n    neighbours = [np.where(dist_mat[i] <= eps)[0] for i in range(n_samples)]\n    core_mask = np.array([len(idx) >= min_samples for idx in neighbours], dtype=bool)\n    labels = np.zeros(n_samples, dtype=int)\n    current_label = 0\n    for point in range(n_samples):\n        if not core_mask[point] or labels[point] != 0:\n            continue\n        current_label += 1\n        queue = deque([point])\n        labels[point] = current_label\n        while queue:\n            p = queue.popleft()\n            for nbr in neighbours[p]:\n                if labels[nbr] == 0:\n                    labels[nbr] = current_label\n                    if core_mask[nbr]:\n                        queue.append(nbr)\n    labels[labels == 0] = -1\n    return labels.tolist()"}
{"task_id": 559, "completion_id": 0, "solution": "from typing import List, Optional\nclass Node:\n    \"\"\"Binary-tree node (only leaves hold a value).\"\"\"\n\n    def __init__(self, val: Optional[int]=None, right: 'Optional[Node]'=None, left: 'Optional[Node]'=None):\n        self.val = val\n        self.right = right\n        self.left = left\ndef leaf_values(root: Optional[Node]) -> List[int]:\n    \"\"\"Return all leaf-node values from left to right.\"\"\"\n    if root is None:\n        return []\n    if root.left is None and root.right is None:\n        return [root.val]\n    leaves: List[int] = []\n    leaves.extend(leaf_values(root.left))\n    leaves.extend(leaf_values(root.right))\n    return leaves"}
{"task_id": 560, "completion_id": 0, "solution": "import math\nimport heapq\ndef k_nearest_neighbors(points: list[list[float]], query: list[float], k: int=3) -> list[int]:\n    \"\"\"Find the *k* nearest neighbours of *query* among *points* using a k-d tree.\"\"\"\n    if k <= 0 or not points:\n        return []\n    n_points = len(points)\n    dim = len(points[0])\n    k = min(k, n_points)\n\n    def sq_dist(p, q):\n        \"\"\"Squared Euclidean distance between two points.\"\"\"\n        return sum(((pi - qi) ** 2 for (pi, qi) in zip(p, q)))\n    indices = list(range(n_points))\n\n    def build(idxs, depth=0):\n        if not idxs:\n            return None\n        axis = depth % dim\n        idxs.sort(key=lambda i: points[i][axis])\n        mid = len(idxs) // 2\n        node_idx = idxs[mid]\n        return {'idx': node_idx, 'axis': axis, 'left': build(idxs[:mid], depth + 1), 'right': build(idxs[mid + 1:], depth + 1)}\n    root = build(indices)\n    best = []\n\n    def maybe_add(idx):\n        \"\"\"Insert point *idx* into the candidate heap if it improves the set.\"\"\"\n        d = sq_dist(points[idx], query)\n        item = (-d, -idx)\n        if len(best) < k:\n            heapq.heappush(best, item)\n        elif item > best[0]:\n            heapq.heapreplace(best, item)\n\n    def search(node):\n        if node is None:\n            return\n        idx = node['idx']\n        axis = node['axis']\n        point = points[idx]\n        maybe_add(idx)\n        diff = query[axis] - point[axis]\n        (near, far) = ('left', 'right') if diff < 0 else ('right', 'left')\n        search(node[near])\n        if len(best) < k or diff * diff < -best[0][0]:\n            search(node[far])\n    search(root)\n    result = [(-d, -i) for (d, i) in best]\n    result.sort(key=lambda x: (x[0], x[1]))\n    return [idx for (_, idx) in result]"}
{"task_id": 561, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef gmm_em(data: np.ndarray, k: int, max_iter: int=200, epsilon: float=0.0001) -> list[int]:\n    \"\"\"\n    Simple EM implementation for a full-covariance Gaussian Mixture Model.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array with shape (N, D); N samples, D features.\n    k : int\n        Number of mixture components (clusters).\n    max_iter : int, optional\n        Maximum number of EM iterations (default 200).\n    epsilon : float, optional\n        Convergence tolerance on the maximum absolute change of the mixture\n        weights between two successive iterations (default 1 e-4).\n\n    Returns\n    -------\n    list[int]\n        Length-N list of final cluster labels (0 \u2026 k-1) in deterministic,\n        left-to-right order (ascending first coordinate of the component mean).\n\n    Notes\n    -----\n    \u2022 If k == 1 all samples receive label 0 and the algorithm stops\n      immediately.\n    \u2022 A tiny diagonal regulariser (1 e-6) is added to every covariance\n      estimate to guarantee positive-definiteness and numerical stability.\n    \"\"\"\n    data = np.asarray(data, dtype=float)\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array (N samples, D features).')\n    (n_samples, n_features) = data.shape\n    if k < 1:\n        raise ValueError('`k` must be a positive integer.')\n    if k == 1:\n        return [0] * n_samples\n    _LOG_2PI = n_features * math.log(2.0 * math.pi)\n\n    def _log_gaussian(x: np.ndarray, mean: np.ndarray, cov: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute log N(x | mean, cov) for all rows of `x`.\n        x : (N, D)\n        mean : (D,)\n        cov : (D, D)\n        Returns\n        -------\n        log_prob : (N,)\n        \"\"\"\n        diff = x - mean\n        try:\n            L = np.linalg.cholesky(cov)\n            y = np.linalg.solve(L, diff.T)\n            maha = np.sum(y ** 2, axis=0)\n            log_det = 2.0 * np.sum(np.log(np.diag(L)))\n        except np.linalg.LinAlgError:\n            inv_cov = np.linalg.inv(cov)\n            maha = np.einsum('ij,ij->i', diff, diff @ inv_cov)\n            (sign, log_det) = np.linalg.slogdet(cov)\n            if sign <= 0:\n                log_det = np.log(np.abs(np.linalg.det(cov) + 1e-12))\n        return -0.5 * (_LOG_2PI + log_det + maha)\n    rng = np.random.default_rng()\n    init_idx = rng.choice(n_samples, size=k, replace=False)\n    means = data[init_idx]\n    weights = np.full(k, 1.0 / k)\n    global_cov = np.cov(data.T, bias=True)\n    if global_cov.shape == ():\n        global_cov = global_cov.reshape(1, 1)\n    covs = np.array([global_cov.copy() for _ in range(k)])\n    reg_eps = 1e-06 * np.eye(n_features)\n    for _ in range(max_iter):\n        log_resp = np.empty((n_samples, k))\n        for j in range(k):\n            log_resp[:, j] = np.log(weights[j] + 1e-16) + _log_gaussian(data, means[j], covs[j])\n        log_resp_max = np.max(log_resp, axis=1, keepdims=True)\n        resp = np.exp(log_resp - log_resp_max)\n        resp_sum = resp.sum(axis=1, keepdims=True)\n        resp /= resp_sum\n        nk = resp.sum(axis=0)\n        prev_weights = weights.copy()\n        weights = nk / n_samples\n        means = resp.T @ data / nk[:, None]\n        for j in range(k):\n            diff = data - means[j]\n            cov = (resp[:, j][:, None] * diff).T @ diff / nk[j]\n            covs[j] = cov + reg_eps\n        if np.max(np.abs(weights - prev_weights)) < epsilon:\n            break\n    log_resp = np.empty((n_samples, k))\n    for j in range(k):\n        log_resp[:, j] = np.log(weights[j] + 1e-16) + _log_gaussian(data, means[j], covs[j])\n    labels = np.argmax(log_resp, axis=1)\n    ordering = np.argsort(means[:, 0])\n    remap = np.zeros(k, dtype=int)\n    remap[ordering] = np.arange(k)\n    labels = remap[labels]\n    return labels.tolist()"}
{"task_id": 562, "completion_id": 0, "solution": "import numpy as np\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float=2.0, method: str='unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A Python list containing the cluster label (0-based) for each sample.\n    \"\"\"\n    N = data.shape[0]\n    if n_cluster <= 1 or N == 0:\n        return [0] * N\n    if method not in {'unnormalized', 'normalized'}:\n        raise ValueError(\"`method` must be either 'unnormalized' or 'normalized'\")\n    sq_norms = np.sum(data ** 2, axis=1)\n    sq_dists = sq_norms[:, None] + sq_norms[None, :] - 2.0 * data @ data.T\n    sq_dists = np.maximum(sq_dists, 0.0)\n    W = np.exp(-gamma * sq_dists)\n    np.fill_diagonal(W, 0.0)\n    degree = np.sum(W, axis=1)\n    if method == 'unnormalized':\n        L = np.diag(degree) - W\n    else:\n        d_inv_sqrt = 1.0 / np.sqrt(degree + 1e-12)\n        L = np.eye(N) - d_inv_sqrt[:, None] * W * d_inv_sqrt[None, :]\n    (eigvals, eigvecs) = np.linalg.eigh(L)\n    U = eigvecs[:, :n_cluster]\n    if method == 'normalized':\n        row_norm = np.linalg.norm(U, axis=1, keepdims=True)\n        U = U / (row_norm + 1e-12)\n    embedding = U\n    centroids = embedding[:n_cluster].copy()\n    labels = np.full(N, -1, dtype=int)\n    for _ in range(300):\n        dists = ((embedding[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.array_equal(labels, new_labels):\n            break\n        labels = new_labels\n        for k in range(n_cluster):\n            members = embedding[labels == k]\n            if len(members) > 0:\n                centroids[k] = members.mean(axis=0)\n    unique_old = np.unique(labels)\n    order = sorted(((l, np.where(labels == l)[0][0]) for l in unique_old))\n    relabel_map = {old: new for (new, (old, _)) in enumerate(order)}\n    final_labels = [int(relabel_map[l]) for l in labels]\n    return final_labels"}
{"task_id": 563, "completion_id": 0, "solution": "import numpy as np\ndef perceptron_dual_train(X_data: np.ndarray, y_data: np.ndarray, eta: float=1.0, max_iter: int=1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array with N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate.\n        max_iter: Maximum number of full passes over the training data.\n\n    Returns:\n        (w, b)  \u2013 w is the learned weight vector as a Python list,\n                  b is the bias term; both rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X_data, dtype=float)\n    y = np.asarray(y_data, dtype=float).flatten()\n    (N, d) = X.shape\n    alpha = np.zeros(N, dtype=float)\n    b = 0.0\n    G = X @ X.T\n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            g = np.sum(alpha * y * G[:, i])\n            if y[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y[i]\n                updated = True\n        if not updated:\n            break\n    w_vec = X.T @ (alpha * y)\n    w_rounded = [round(float(val), 4) for val in w_vec]\n    b_rounded = round(float(b), 4)\n    return (w_rounded, b_rounded)"}
{"task_id": 564, "completion_id": 0, "solution": "def predict_sample(root: 'Node', features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    node = root\n    while True:\n        if hasattr(node, 'y'):\n            return node.y\n        if task_type == 'classification':\n            feat_val = features[node.label]\n            next_node = None\n            for child in node.child:\n                if hasattr(child, 'x') and child.x == feat_val:\n                    next_node = child\n                    break\n            if next_node is None:\n                next_node = node.child[1]\n        else:\n            feat_val = features[node.label]\n            next_node = node.child[0] if feat_val <= node.s else node.child[1]\n        node = next_node"}
{"task_id": 565, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef _entropy(labels: np.ndarray) -> float:\n    \"\"\"\n    Shannon entropy H(labels) in bits.\n    labels are integers starting from 0.\n    \"\"\"\n    if labels.size == 0:\n        return 0.0\n    counts = np.bincount(labels)\n    probs = counts[counts > 0] / labels.size\n    return float(-np.sum(probs * np.log2(probs)))\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        (best_feature_index, max_information_gain_rounded_to_6_decimals)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    base_entropy = _entropy(y)\n    best_gain = -math.inf\n    best_index = 0\n    for idx in range(n_features):\n        column = X[:, idx]\n        (vals, counts) = np.unique(column, return_counts=True)\n        cond_entropy = 0.0\n        for (val, cnt) in zip(vals, counts):\n            subset_labels = y[column == val]\n            cond_entropy += cnt / n_samples * _entropy(subset_labels)\n        info_gain = base_entropy - cond_entropy\n        if info_gain > best_gain + 1e-12:\n            best_gain = info_gain\n            best_index = idx\n        elif abs(info_gain - best_gain) <= 1e-12 and idx < best_index:\n            best_index = idx\n    if abs(best_gain) < 1e-12:\n        best_gain = 0.0\n    return (best_index, round(best_gain, 6))"}
{"task_id": 566, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation (deterministic).\n\n    Args\n    ----\n    data        : 2-D NumPy array (n_samples, n_features)\n    n_trees     : number of random isolation trees\n    sample_size : subsample size used to grow every tree\n    epsilon     : fraction (0 \u2264 \u03b5 < 1) of observations regarded as outliers\n\n    Returns\n    -------\n    A sorted list with the zero-based indices of the detected outliers.\n    \"\"\"\n    np.random.seed(42)\n    (n_samples, n_features) = data.shape\n    if epsilon == 0 or n_samples == 0:\n        return []\n    subsz = min(sample_size, n_samples)\n    height_limit = math.ceil(math.log2(subsz)) if subsz > 1 else 0\n\n    def c_factor(n: int) -> float:\n        \"\"\"Expected path length for an unsuccessful search in a BST.\"\"\"\n        if n <= 1:\n            return 0.0\n        return 2.0 * math.log(n - 1) + 0.5772156649 - 2.0 * (n - 1) / n\n\n    class Node:\n        __slots__ = ('is_leaf', 'size', 'feat', 'thr', 'left', 'right')\n\n        def __init__(self, *, is_leaf: bool, size: int, feat=None, thr=None, left=None, right=None):\n            self.is_leaf = is_leaf\n            self.size = size\n            self.feat = feat\n            self.thr = thr\n            self.left = left\n            self.right = right\n\n    def build_tree(idxs: np.ndarray, depth: int) -> Node:\n        \"\"\"Recursively build one isolation tree.\"\"\"\n        if depth >= height_limit or idxs.size <= 1:\n            return Node(is_leaf=True, size=idxs.size)\n        feat = np.random.randint(n_features)\n        col = data[idxs, feat]\n        (f_min, f_max) = (col.min(), col.max())\n        if f_min == f_max:\n            return Node(is_leaf=True, size=idxs.size)\n        thr = np.random.uniform(f_min, f_max)\n        left_mask = col < thr\n        right_mask = ~left_mask\n        if not left_mask.any() or not right_mask.any():\n            return Node(is_leaf=True, size=idxs.size)\n        left_child = build_tree(idxs[left_mask], depth + 1)\n        right_child = build_tree(idxs[right_mask], depth + 1)\n        return Node(is_leaf=False, size=idxs.size, feat=feat, thr=thr, left=left_child, right=right_child)\n\n    def path_length(x: np.ndarray, node: Node, depth: int=0) -> float:\n        \"\"\"Path length of one sample through one tree.\"\"\"\n        if node.is_leaf:\n            return depth + c_factor(node.size)\n        if x[node.feat] < node.thr:\n            return path_length(x, node.left, depth + 1) if node.left is not None else depth + c_factor(node.size)\n        else:\n            return path_length(x, node.right, depth + 1) if node.right is not None else depth + c_factor(node.size)\n    forest: list[Node] = []\n    for _ in range(n_trees):\n        sample_indices = np.random.choice(n_samples, subsz, replace=False)\n        forest.append(build_tree(sample_indices, depth=0))\n    path_sum = np.zeros(n_samples, dtype=float)\n    for tree in forest:\n        for i in range(n_samples):\n            path_sum[i] += path_length(data[i], tree)\n    h_bar = path_sum / n_trees\n    phi = 0.0\n    if n_samples > 1:\n        phi = 2.0 * math.log(n_samples - 1) - 2.0 * (n_samples - 1) / n_samples\n    else:\n        phi = 1.0\n    scores = np.power(2.0, -h_bar / phi)\n    k = int(math.floor(epsilon * n_samples))\n    if k == 0:\n        return []\n    largest_k_idx = np.argsort(scores)[-k:]\n    return sorted(largest_k_idx.tolist())"}
{"task_id": 567, "completion_id": 0, "solution": "import numpy as np\ndef majority_vote(predictions: list[list[int | float]]) -> list[int]:\n    \"\"\"Ensemble majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float]]\n        A 2-D list where each row contains the predictions made by one tree\n        and each column groups the votes for a single sample.\n\n    Returns\n    -------\n    list[int]\n        The final class label for every sample after majority voting.  In a\n        tie, the smallest label is chosen.\n    \"\"\"\n    votes = np.asarray(predictions)\n    n_samples = votes.shape[1]\n    final_labels: list[int] = []\n    for col in range(n_samples):\n        (labels, counts) = np.unique(votes[:, col], return_counts=True)\n        max_count = counts.max()\n        tied_labels = labels[counts == max_count]\n        chosen_label = int(tied_labels.min())\n        final_labels.append(chosen_label)\n    return final_labels"}
{"task_id": 568, "completion_id": 0, "solution": "import math\nfrom collections import defaultdict\nimport numpy as np\ndef maxent_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, epsilon: float=0.001, n_iter: int=100) -> list[int]:\n    \"\"\"\n    Train a Maximum-Entropy classifier with Generalised Iterative Scaling\n    on (categorical) training data and return the label predictions for\n    X_test.\n    \"\"\"\n\n    def active_sum(x_row, y_label):\n        \"\"\"\n        Sum of the weights that are active for (x_row , y_label).\n        Each (feature_index, feature_value, y_label) that matches x_row\n        contributes its current weight.\n        \"\"\"\n        s = 0.0\n        for (i, val) in enumerate(x_row):\n            idx = feat2id.get((i, val, y_label))\n            if idx is not None:\n                s += weights[idx]\n        return s\n    (N, d) = X_train.shape\n    labels = list(set((int(y) for y in y_train)))\n    C = d\n    feat_counts = defaultdict(int)\n    for (x_row, y) in zip(X_train, y_train):\n        y = int(y)\n        for (i, val) in enumerate(x_row):\n            feat_counts[i, val, y] += 1\n    n_feats = len(feat_counts)\n    feat2id = {}\n    emp_exp = np.zeros(n_feats, dtype=float)\n    for (idx, (feat, cnt)) in enumerate(feat_counts.items()):\n        feat2id[feat] = idx\n        emp_exp[idx] = cnt / N\n    weights = np.zeros(n_feats, dtype=float)\n    tiny = 1e-12\n    for _ in range(n_iter):\n        mod_exp = np.zeros(n_feats, dtype=float)\n        for x_row in X_train:\n            raw = {y: active_sum(x_row, y) for y in labels}\n            max_raw = max(raw.values())\n            exp_scores = {y: math.exp(raw[y] - max_raw) for y in labels}\n            Z = sum(exp_scores.values())\n            probs = {y: exp_scores[y] / Z for y in labels}\n            for (y, p) in probs.items():\n                if p == 0.0:\n                    continue\n                for (i, val) in enumerate(x_row):\n                    idx = feat2id.get((i, val, y))\n                    if idx is not None:\n                        mod_exp[idx] += p\n        mod_exp /= N\n        max_change = 0.0\n        for j in range(n_feats):\n            delta = 1.0 / C * math.log((emp_exp[j] + tiny) / (mod_exp[j] + tiny))\n            weights[j] += delta\n            if abs(delta) > max_change:\n                max_change = abs(delta)\n        if max_change < epsilon:\n            break\n    preds = []\n    for x_row in X_test:\n        (best_label, best_score) = (None, -float('inf'))\n        for y in labels:\n            score = active_sum(x_row, y)\n            if score > best_score:\n                best_score = score\n                best_label = y\n        preds.append(int(best_label))\n    return preds"}
{"task_id": 569, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\ndef statistic_key_word(data: List[Tuple[Any, Iterable[str]]], cut_off: int | None=None) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate per-word document-frequency statistics.\n\n    The function returns\n        1. a list of (word, ratio) pairs where *ratio* is the fraction of\n           documents in which the word occurs (rounded to 4 decimals),\n           sorted from the most to the least frequent word;\n        2. a set that contains only the *cut_off* most frequent words\n           (all words if *cut_off* is None).\n\n    Args:\n        data: list/iterable of documents.  Each document is a tuple\n              ``(label, words)`` where *words* is any iterable of strings.\n        cut_off: maximal number of top words to put into the returned set, or\n                 ``None`` for *all* words.\n\n    Returns:\n        Tuple[list[(word, ratio)], set[str]]\n    \"\"\"\n    n_docs = len(data)\n    if n_docs == 0:\n        return ([], set())\n    doc_counter: Counter[str] = Counter()\n    for (_, words) in data:\n        doc_counter.update(set(words))\n    freq_list = [(word, round(count / n_docs, 4)) for (word, count) in doc_counter.items()]\n    freq_list.sort(key=lambda pair: (-pair[1], pair[0]))\n    if cut_off is None:\n        top_k_set = {word for (word, _) in freq_list}\n    elif cut_off <= 0:\n        top_k_set = set()\n    else:\n        top_k_set = {word for (word, _) in freq_list[:cut_off]}\n    return (freq_list, top_k_set)"}
{"task_id": 570, "completion_id": 0, "solution": "from collections import defaultdict\nfrom typing import List, Tuple, Set\ndef naive_bayes_predict(train_data: List[Tuple[str, List[str]]], keywords: Set[str], test_samples: List[List[str]]) -> List[str]:\n    \"\"\"Unsmooothed Bernoulli Naive Bayes classifier that uses only *keywords*\n    as binary features (present / absent).  No class prior and no smoothing.\n    \"\"\"\n    counts = defaultdict(lambda : defaultdict(int))\n    N_docs = defaultdict(int)\n    class_order: list[str] = []\n    for (label, tokens) in train_data:\n        if label not in N_docs:\n            class_order.append(label)\n        N_docs[label] += 1\n        token_set = set(tokens)\n        for w in keywords:\n            if w in token_set:\n                counts[label][w] += 1\n    probs: dict[str, dict[str, float]] = {}\n    for label in N_docs:\n        Nc = N_docs[label]\n        probs[label] = {}\n        for w in keywords:\n            probs[label][w] = counts[label][w] / Nc\n    predictions: list[str] = []\n    for tokens in test_samples:\n        tok_set = set(tokens)\n        best_label = None\n        best_likelihood = -1.0\n        for label in class_order:\n            likelihood = 1.0\n            for w in keywords:\n                pwc = probs[label][w]\n                if w in tok_set:\n                    likelihood *= pwc\n                else:\n                    likelihood *= 1.0 - pwc\n            if likelihood > best_likelihood:\n                best_likelihood = likelihood\n                best_label = label\n        if best_label is None:\n            best_label = class_order[0]\n        predictions.append(best_label)\n    return predictions"}
{"task_id": 571, "completion_id": 0, "solution": "import numpy as np\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Training data.\n    y : np.ndarray, shape (n_samples,)\n        Class labels (+1 or \u22121).\n    alpha : np.ndarray, shape (n_samples,)\n        Optimised Lagrange multipliers.\n\n    Returns\n    -------\n    (w_list, b) : tuple[list[float], float]\n        w_list : the weight vector rounded to 4 decimals (Python list)\n        b       : the bias term rounded to 4 decimals (float)\n    \"\"\"\n    alpha_y = alpha * y\n    w = X.T @ alpha_y\n    predictions = X @ w\n    b = np.mean(y - predictions)\n    w_list = [round(float(w_i), 4) for w_i in w]\n    b = round(float(b), 4)\n    return (w_list, b)"}
{"task_id": 572, "completion_id": 0, "solution": "import numpy as np\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"\n    Computes the Local-Outlier-Factor (LOF) for every sample in `data`\n    and returns\n        (list_of_LOF_scores_rounded_to_4_decimals ,\n         ascending_list_of_indices_with_LOF>epsilon)\n\n    Parameters\n    ----------\n    data : array-like, shape = (n_samples, n_features) or (n_samples,)\n    k    : int, 1 \u2264 k < n_samples\n    epsilon : float, threshold that decides which samples are flagged\n              as outliers        (default 1.5)\n\n    Notes\n    -----\n    \u2022  NO external ML / scientific libraries are used \u2013 only `numpy`.\n    \u2022  All LOF scores are rounded with `numpy.round(score, 4)`.\n    \"\"\"\n    data = np.asarray(data, dtype=float)\n    if data.ndim == 1:\n        data = data[:, None]\n    n_samples = data.shape[0]\n    if not 1 <= k < n_samples:\n        raise ValueError('k must satisfy 1 \u2264 k < n_samples (got k = %d)' % k)\n    diff = data[:, None, :] - data[None, :, :]\n    dist_mat = np.sqrt(np.sum(diff ** 2, axis=2))\n    k_distances = np.empty(n_samples)\n    neighbours = np.empty((n_samples, k), int)\n    for i in range(n_samples):\n        idx_sorted = np.argsort(dist_mat[i])\n        neigh_idx = idx_sorted[1:k + 1]\n        neighbours[i] = neigh_idx\n        k_distances[i] = dist_mat[i, neigh_idx[-1]]\n    lrd = np.empty(n_samples)\n    for p in range(n_samples):\n        reach_dist_sum = 0.0\n        for q in neighbours[p]:\n            reach_dist = max(k_distances[q], dist_mat[p, q])\n            reach_dist_sum += reach_dist\n        lrd[p] = k / reach_dist_sum\n    lof = np.empty(n_samples)\n    for p in range(n_samples):\n        lrd_sum = lrd[neighbours[p]].sum()\n        lof[p] = lrd_sum / k / lrd[p]\n    lof_rounded = np.round(lof, 4).tolist()\n    outlier_idx = [i for (i, score) in enumerate(lof) if score > epsilon]\n    return (lof_rounded, outlier_idx)"}
{"task_id": 573, "completion_id": 0, "solution": "import numpy as np\ndef perceptron_train(X_train: np.ndarray, y_train: np.ndarray, eta: float=0.1, n_iter: int=1000) -> tuple[list[float], float, list[int]]:\n    \"\"\"Implement the classical Rosenblatt Perceptron learning rule.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training patterns of shape (m, n).\n    y_train : np.ndarray\n        Target labels (+1 or -1) of length m.\n    eta : float, optional\n        Learning rate, by default 0.1.\n    n_iter : int, optional\n        Maximum number of passes over the data, by default 1000.\n\n    Returns\n    -------\n    tuple[list[float], float, list[int]]\n        (learned weight vector as list, bias term, list with #errors per epoch),\n        all real\u2010valued outputs rounded to 4 decimal places.\n    \"\"\"\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=float)\n    (m, n) = X.shape\n    w = np.zeros(n, dtype=float)\n    b = 0.0\n    error_history: list[int] = []\n    for _ in range(n_iter):\n        error_count = 0\n        for (xi, yi) in zip(X, y):\n            if yi * (np.dot(xi, w) + b) <= 0.0:\n                w += eta * yi * xi\n                b += eta * yi\n                error_count += 1\n        error_history.append(int(error_count))\n        if error_count == 0:\n            break\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return (w_rounded, b_rounded, error_history)"}
{"task_id": 574, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef generate_data(m: int, n_train: int, n_val: int, seed: int | None=None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the current global\n        random state is used unchanged.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays with shapes\n        (m\u00b7n_train, 2), (m\u00b7n_val, 2), (m\u00b7n_train,) and (m\u00b7n_val,)\n        respectively.  Every coordinate is rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    (X_tr, X_va) = ([], [])\n    (y_tr, y_va) = ([], [])\n    for c in range(m):\n        base = 10 * (c // 2)\n        for _ in range(n_train):\n            x = random.uniform(base + 1, base + 9)\n            y = random.uniform(base + 1, base + 9)\n            X_tr.append((x, y))\n            y_tr.append(c)\n        for _ in range(n_val):\n            x = random.uniform(base + 1, base + 9)\n            y = random.uniform(base + 1, base + 9)\n            X_va.append((x, y))\n            y_va.append(c)\n    X_train = np.round(np.asarray(X_tr, dtype=float), 4)\n    X_val = np.round(np.asarray(X_va, dtype=float), 4)\n    y_train = np.asarray(y_tr, dtype=int)\n    y_val = np.asarray(y_va, dtype=int)\n    return (X_train, X_val, y_train, y_val)"}
{"task_id": 577, "completion_id": 0, "solution": ""}
{"task_id": 578, "completion_id": 0, "solution": "from typing import List, Tuple\ndef make_pairs(corpus: List) -> List[Tuple]:\n    \"\"\"Return every consecutive pair (bigram) found in *corpus*.\n\n    Args:\n        corpus: A list containing any kind of elements.\n\n    Returns:\n        A list of tuples where each tuple is a pair of consecutive\n        elements from *corpus*.\n    \"\"\"\n    return [(corpus[i], corpus[i + 1]) for i in range(len(corpus) - 1)]"}
{"task_id": 579, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(X: np.ndarray, k: int, max_iters: int=100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Deterministic Lloyd-style K-Means clustering.\n\n    Args:\n        X        : (m, d) array with the input points.\n        k        : number of clusters, 1 \u2264 k \u2264 m.\n        max_iters: maximum amount of Lloyd iterations.\n\n    Returns\n    -------\n        labels  : list with length m, the cluster index of every point.\n        centres : list with k inner lists \u2013 the final centres, each\n                  coordinate rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    if X.ndim != 2:\n        raise ValueError('`X` must be a 2-D array (m, d).')\n    (m, d) = X.shape\n    if not 1 <= k <= m:\n        raise ValueError('`k` must satisfy 1 \u2264 k \u2264 number of points.')\n    if max_iters <= 0:\n        raise ValueError('`max_iters` must be positive.')\n    centres = X[:k].copy()\n    labels = np.full(m, -1, dtype=int)\n    for _ in range(max_iters):\n        diff = X[:, None, :] - centres[None, :, :]\n        dist2 = np.sum(diff ** 2, axis=2)\n        new_labels = np.argmin(dist2, axis=1)\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n        for i in range(k):\n            mask = labels == i\n            if np.any(mask):\n                centres[i] = X[mask].mean(axis=0)\n    centres_rounded = [[round(val, 4) for val in centre] for centre in centres]\n    return (labels.tolist(), centres_rounded)"}
{"task_id": 581, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regression(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int, l1_penalty: float, l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation using batch GD.\"\"\"\n    (m, n) = X.shape\n    if learning_rate == 0 or iterations == 0:\n        return ([0.0] * n, 0.0)\n    w = np.zeros(n, dtype=float)\n    b = 0.0\n    for _ in range(iterations):\n        y_hat = X @ w + b\n        r = y - y_hat\n        sign_w = np.sign(w)\n        sign_w[sign_w == 0] = -1\n        grad_w = -2 * X.T @ r + l1_penalty * sign_w + 2 * l2_penalty * w\n        grad_b = -2 * r.sum()\n        w -= learning_rate * (grad_w / m)\n        b -= learning_rate * (grad_b / m)\n    weights_rounded = [round(float(val), 4) for val in w]\n    bias_rounded = round(float(b), 4)\n    return (weights_rounded, bias_rounded)"}
{"task_id": 582, "completion_id": 0, "solution": "import numpy as np\ndef nearest_neighbor_graph(X):\n    \"\"\"Build the undirected k-nearest-neighbour adjacency matrix.\n\n    Parameters\n    ----------\n    X : Sequence[Sequence[float]]\n        2-D array-like object of shape (n_samples, n_features).\n\n    Returns\n    -------\n    list[list[int]]\n        Adjacency matrix W where W[i][j] is 1 iff an undirected edge exists\n        between sample i and sample j, otherwise 0.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    n = X.shape[0]\n    if n <= 1:\n        return [[0]] if n == 1 else []\n    k = min(int(np.sqrt(n)), 10)\n    if k == 0:\n        k = 1\n    diff = X[:, None, :] - X[None, :, :]\n    dist2 = np.sum(diff ** 2, axis=-1)\n    directed = np.zeros((n, n), dtype=bool)\n    for i in range(n):\n        order = np.argsort(dist2[i])\n        neighbours = [idx for idx in order if idx != i][:k]\n        directed[i, neighbours] = True\n    undirected = (directed | directed.T).astype(int)\n    return undirected.tolist()"}
{"task_id": 584, "completion_id": 0, "solution": "import numpy as np\ndef make_initial_state(state: list[list[int | float]]) -> list[list[list[int | float]]]:\n    \"\"\"Stack a single 2-D Atari frame four times to form the initial state.\n\n    Args:\n        state: 2-D list (height \u00d7 width) representing the grayscale frame.\n\n    Returns:\n        3-D list with shape (height \u00d7 width \u00d7 4) where the same frame is\n        repeated along the last axis.\n    \"\"\"\n    frame = np.asarray(state)\n    stacked_state = np.repeat(frame[..., np.newaxis], 4, axis=-1)\n    return stacked_state.tolist()"}
{"task_id": 585, "completion_id": 0, "solution": "def cmp(a: object, b: object) -> int:\n    \"\"\"Compares two values and returns their ordering.\n\n    The function mimics Python 2's ``cmp``. It returns:\n        \u2022  1  if ``a`` is greater than ``b``\n        \u2022  0  if ``a`` equals ``b``\n        \u2022 -1  if ``a`` is less than ``b``\n\n    Args:\n        a: Any Python object supporting comparison operations.\n        b: Any Python object supporting comparison operations.\n\n    Returns:\n        int: -1, 0, or 1 indicating the comparison result.\n    \"\"\"\n    return (a > b) - (a < b)"}
{"task_id": 586, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef lowess_predict(x: np.ndarray, y: np.ndarray, x_query: float, window: int) -> float:\n    \"\"\"Single-point LOWESS prediction (linear local model).\n\n    Parameters\n    ----------\n    x, y : np.ndarray\n        One-dimensional training data (same length).\n    x_query : float\n        Abscissa at which to predict.\n    window : int\n        Number of nearest neighbours to use (2 \u2264 window \u2264 len(x)).\n\n    Returns\n    -------\n    float\n        LOWESS prediction rounded to 4 decimals.\n    \"\"\"\n    if x.shape[0] != y.shape[0]:\n        raise ValueError('x and y must have the same length.')\n    n = x.shape[0]\n    if not 2 <= window <= n:\n        raise ValueError('window must satisfy 2 \u2264 window \u2264 len(x).')\n\n    def _normalise(arr):\n        (mn, mx) = (float(arr.min()), float(arr.max()))\n        if mx == mn:\n            return (np.zeros_like(arr, dtype=float), mn, mx)\n        return ((arr - mn) / (mx - mn), mn, mx)\n    (x_norm, x_min, x_max) = _normalise(x)\n    (y_norm, y_min, y_max) = _normalise(y)\n    if x_max == x_min:\n        xq_norm = 0.0\n    else:\n        xq_norm = (x_query - x_min) / (x_max - x_min)\n    distances = np.abs(x_norm - xq_norm)\n    neighbour_idx = np.argpartition(distances, window - 1)[:window]\n    neighbour_idx = neighbour_idx[np.argsort(distances[neighbour_idx])]\n    d = distances[neighbour_idx]\n    x_sel = x_norm[neighbour_idx]\n    y_sel = y_norm[neighbour_idx]\n    d_max = d.max()\n    if d_max == 0:\n        w = np.ones_like(d)\n    else:\n        u = d / d_max\n        w = (1 - u ** 3) ** 3\n        w[u > 1] = 0.0\n    w_sum = w.sum()\n    wx = w * x_sel\n    wy = w * y_sel\n    x_bar = wx.sum() / w_sum\n    y_bar = wy.sum() / w_sum\n    b_den = (w * (x_sel - x_bar) ** 2).sum()\n    if b_den == 0:\n        y_hat_norm = y_bar\n    else:\n        b = (w * (x_sel - x_bar) * (y_sel - y_bar)).sum() / b_den\n        a = y_bar - b * x_bar\n        y_hat_norm = a + b * xq_norm\n    if y_max == y_min:\n        y_hat = y_min\n    else:\n        y_hat = y_hat_norm * (y_max - y_min) + y_min\n    return float(np.round(y_hat, 4))"}
{"task_id": 587, "completion_id": 0, "solution": "import random\nfrom typing import Any, List, Tuple\ndef manage_replay_memory(operations: List[Tuple]) -> List[Any]:\n    \"\"\"Execute a sequence of replay-buffer operations.\n\n    The buffer is a plain list that starts out empty.\n    \"\"\"\n    random.seed(0)\n    buffer: List[Any] = []\n    outputs: List[Any] = []\n    for op in operations:\n        cmd = op[0]\n        if cmd == 'append':\n            buffer.append(op[1])\n        elif cmd == 'pop':\n            val = buffer.pop() if buffer else None\n            outputs.append(val)\n        elif cmd == 'sample':\n            batch_size = op[1]\n            if batch_size > len(buffer):\n                outputs.append([])\n            else:\n                outputs.append(random.sample(buffer, batch_size))\n        elif cmd == 'size':\n            outputs.append(len(buffer))\n        else:\n            raise ValueError(f'Unsupported command: {cmd}')\n    return outputs"}
{"task_id": 588, "completion_id": 0, "solution": "from typing import Optional\nclass Node:\n    \"\"\"A simple binary tree node.\"\"\"\n\n    def __init__(self, data=None, split_variable=None, split_variable_value=None, left: Optional['Node']=None, right: Optional['Node']=None, depth: int=0, criterion_value=None):\n        self.data = data\n        self.split_variable = split_variable\n        self.split_variable_value = split_variable_value\n        self.left = left\n        self.right = right\n        self.depth = depth\n        self.criterion_value = criterion_value\ndef tree_max_depth(root: Optional[Node]) -> int:\n    \"\"\"Computes the maximum depth of a binary tree.\n\n    The depth of the root node is defined to be 0.\n    The depth of an empty tree is defined to be -1.\n    \"\"\"\n    if root is None:\n        return -1\n    left_depth = tree_max_depth(root.left)\n    right_depth = tree_max_depth(root.right)\n    return max(left_depth, right_depth) + 1"}
{"task_id": 590, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef minesweeper_step(ground: list[list[float | int]], flags: list[list[bool]]) -> tuple[list[tuple[int, int]], list[tuple[int, int]]]:\n    \"\"\"Return the squares that can be revealed (`clicks`) and the squares that\n    have to be bombs (`new_flags`) after **one** logical pass.\n\n    The deduction rules implemented are the standard Minesweeper rules:\n        \u2022 value == 0  \u2192  every hidden, un-flagged neighbour is safe.\n        \u2022 value == 8  \u2192  every hidden, un-flagged neighbour is a bomb.\n        \u2022 otherwise\n              bombs       = # already flagged neighbours\n              unexplored  = set(hidden & un-flagged neighbours)\n\n              \u2013 if bombs == value\n                    \u21d2 all squares in *unexplored* are safe   (clicks)\n              \u2013 elif value \u2013 bombs == len(unexplored)\n                    \u21d2 all squares in *unexplored* are bombs  (new_flags)\n\n    Only the *current* board state is taken into account; newly deduced\n    clicks/flags are **not** fed back into the analysis within the same call.\n    \"\"\"\n    rows = len(ground)\n    cols = len(ground[0]) if rows else 0\n\n    def neighbours(r: int, c: int):\n        for dr in (-1, 0, 1):\n            for dc in (-1, 0, 1):\n                if dr == 0 and dc == 0:\n                    continue\n                (nr, nc) = (r + dr, c + dc)\n                if 0 <= nr < rows and 0 <= nc < cols:\n                    yield (nr, nc)\n    clicks_set: set[tuple[int, int]] = set()\n    new_flags_set: set[tuple[int, int]] = set()\n    for r in range(rows):\n        for c in range(cols):\n            if np.isnan(ground[r][c]) or flags[r][c]:\n                continue\n            value = int(ground[r][c])\n            unexplored = []\n            bombs = 0\n            for (nr, nc) in neighbours(r, c):\n                if flags[nr][nc]:\n                    bombs += 1\n                elif np.isnan(ground[nr][nc]):\n                    unexplored.append((nr, nc))\n            if value == 0:\n                clicks_set.update(unexplored)\n            elif value == 8:\n                new_flags_set.update(unexplored)\n            elif bombs == value:\n                clicks_set.update(unexplored)\n            elif value - bombs == len(unexplored):\n                new_flags_set.update(unexplored)\n    clicks = sorted(clicks_set)\n    new_flags = sorted(new_flags_set)\n    return (clicks, new_flags)"}
{"task_id": 591, "completion_id": 0, "solution": "import numpy as np\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Forward pass of a fully\u2013connected ReLU network with a linear output layer.\n\n    Args:\n        weights : Three-level nested list holding all weight matrices.\n        x       : Input features (without bias).\n\n    Returns:\n        Network prediction rounded to four decimals.\n        Returns a float for a single output neuron, otherwise a list of floats.\n    \"\"\"\n    h = np.asarray(x, dtype=float)\n    n_layers = len(weights)\n    for (L, layer) in enumerate(weights):\n        W = np.asarray(layer, dtype=float)\n        h_aug = np.concatenate(([1.0], h))\n        z = W @ h_aug\n        h = np.maximum(0, z) if L < n_layers - 1 else z\n    if h.size == 1:\n        return round(float(h[0]), 4)\n    else:\n        return [round(v, 4) for v in h.tolist()]"}
{"task_id": 592, "completion_id": 0, "solution": "import numpy as np\ndef analyze_cell(clicked: np.ndarray, flags: np.ndarray, row: int, col: int) -> tuple[list[tuple[int, int]], int]:\n    \"\"\"Return unopened neighbouring cells and flagged-mine count for a board position.\"\"\"\n    (n_rows, n_cols) = clicked.shape\n    if n_rows == 1 and n_cols == 1:\n        return ([], 0)\n    unopened: list[tuple[int, int]] = []\n    flagged_count = 0\n    (r0, r1) = (max(0, row - 1), min(n_rows - 1, row + 1))\n    (c0, c1) = (max(0, col - 1), min(n_cols - 1, col + 1))\n    for r in range(r0, r1 + 1):\n        for c in range(c0, c1 + 1):\n            if (r, c) == (row, col):\n                continue\n            if not clicked[r, c]:\n                unopened.append((r, c))\n            if flags[r, c]:\n                flagged_count += 1\n    unopened.sort()\n    return (unopened, flagged_count)"}
{"task_id": 593, "completion_id": 0, "solution": "from copy import deepcopy\nfrom typing import List, Tuple, Set, FrozenSet\ndef deduce_mines(constraints: List[List]) -> Tuple[List[str], List[str]]:\n    \"\"\"Simplify a list of Minesweeper constraints.\n\n    Each *constraint* is a pair  [variables, value]  meaning\n    'exactly *value* of the listed *variables* hide a mine'.\n\n    The function applies, in a loop, the following rules\n\n        1. solved constraints          (all-safe / all-mine)\n        2. propagation of knowledge    (delete known vars, adjust value)\n        3. subset rule                 (S1\u2282S2  \u21d2  S2 := S2\\\\S1 ,  v2 := v2\u2212v1)\n        4. duplicate removal\n\n    until no more information can be gained.\n\n    It returns two alphabetically sorted lists:\n        (certainly_safe_variables, certainly_mine_variables)\n    \"\"\"\n\n    def remove_duplicates(cons: List[Tuple[FrozenSet, int]]) -> List[Tuple[Set[str], int]]:\n        \"\"\"Drop identical (set,value) pairs, keep only one copy.\"\"\"\n        unique = set(cons)\n        return [(set(vs), val) for (vs, val) in unique]\n\n    def propagate(cons: List[Tuple[Set[str], int]], safe: Set[str], mines: Set[str]) -> List[Tuple[Set[str], int]]:\n        \"\"\"Delete already classified vars, adjust values for known mines.\"\"\"\n        new_cons: List[Tuple[Set[str], int]] = []\n        for (varset, val) in cons:\n            mine_cnt = len(varset & mines)\n            val -= mine_cnt\n            varset = varset - mines - safe\n            if not varset:\n                continue\n            new_cons.append((varset, val))\n        return new_cons\n\n    def apply_subset_rule(cons: List[Tuple[Set[str], int]]) -> Tuple[List[Tuple[Set[str], int]], bool]:\n        \"\"\"Apply subset rule once.  Return (new_constraints, changed?).\"\"\"\n        changed = False\n        n = len(cons)\n        for i in range(n):\n            (Si, vi) = cons[i]\n            for j in range(n):\n                if i == j:\n                    continue\n                (Sj, vj) = cons[j]\n                if Si < Sj:\n                    cons[j] = (Sj - Si, vj - vi)\n                    changed = True\n                    break\n            if changed:\n                break\n        return (cons, changed)\n    constraints = [(set(vars_), val) for (vars_, val) in deepcopy(constraints)]\n    safe: Set[str] = set()\n    mines: Set[str] = set()\n    changed_globally = True\n    while changed_globally:\n        changed_globally = False\n        newly_safe: Set[str] = set()\n        newly_mine: Set[str] = set()\n        for (varset, val) in constraints:\n            if val == 0:\n                newly_safe |= varset\n            elif val == len(varset):\n                newly_mine |= varset\n        if newly_safe - safe or newly_mine - mines:\n            safe |= newly_safe\n            mines |= newly_mine\n            changed_globally = True\n        constraints = propagate(constraints, safe, mines)\n        subset_changed = True\n        while subset_changed:\n            (constraints, subset_changed) = apply_subset_rule(constraints)\n            if subset_changed:\n                changed_globally = True\n                constraints = remove_duplicates([(set(vs), val) for (vs, val) in constraints])\n        before = len(constraints)\n        constraints = remove_duplicates([(set(vs), val) for (vs, val) in constraints])\n        if len(constraints) != before:\n            changed_globally = True\n    return (sorted(safe), sorted(mines))"}
{"task_id": 594, "completion_id": 0, "solution": "def maze_to_graph(maze: list[list[int]]) -> dict[tuple[int, int], list[tuple[int, int]]]:\n    \"\"\"Convert a binary maze into an adjacency-list graph.\n\n    A 0 in the maze denotes an open cell, while 1 denotes a wall.  \n    Two open cells are adjacent if they share an edge (up, down, left, right).\n\n    Args:\n        maze: Rectangular 2-D list containing only 0s and 1s.\n\n    Returns:\n        A dictionary whose keys are coordinates (row, col) for every open\n        cell, and whose values are *sorted* lists of orthogonally adjacent\n        open-cell coordinates.  If the maze has no open cells the function\n        returns an empty dictionary.\n    \"\"\"\n    if not maze:\n        return {}\n    (rows, cols) = (len(maze), len(maze[0]))\n    graph: dict[tuple[int, int], list[tuple[int, int]]] = {}\n    open_cells = {(r, c) for r in range(rows) for c in range(cols) if maze[r][c] == 0}\n    if not open_cells:\n        return {}\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    for (r, c) in open_cells:\n        neighbours: list[tuple[int, int]] = []\n        for (dr, dc) in directions:\n            (nr, nc) = (r + dr, c + dc)\n            if (nr, nc) in open_cells:\n                neighbours.append((nr, nc))\n        neighbours.sort()\n        graph[r, c] = neighbours\n    return graph"}
{"task_id": 595, "completion_id": 0, "solution": "import numpy as np\ndef kmeans_centroids(X: 'np.ndarray', k: int, max_iters: int=100) -> list[list[float]]:\n    \"\"\"\n    K-Means clustering \u2013 return **centroids only**.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    k : int\n        Number of clusters.\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Sorted centroids (lexicographically), each coordinate rounded to 4 decimals.\n    \"\"\"\n    if k <= 0:\n        raise ValueError('k must be a positive integer')\n    if X.ndim != 2:\n        raise ValueError('X must be 2-dimensional')\n    X = X.astype(float, copy=False)\n    (n_samples, n_features) = X.shape\n    if k > n_samples:\n        raise ValueError('k cannot be larger than number of samples')\n    centroids = X[:k].copy()\n    for _ in range(max_iters):\n        distances = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = centroids.copy()\n        for j in range(k):\n            mask = labels == j\n            if np.any(mask):\n                new_centroids[j] = X[mask].mean(axis=0)\n        shifts = np.linalg.norm(new_centroids - centroids, axis=1)\n        centroids = new_centroids\n        if np.all(shifts < 0.0001):\n            break\n    centroids_list = centroids.tolist()\n    centroids_list.sort()\n    rounded = [[round(val, 4) for val in centre] for centre in centroids_list]\n    return rounded"}
{"task_id": 596, "completion_id": 0, "solution": "import numpy as np\ndef convert_maze_to_graph(maze: np.ndarray) -> dict:\n    \"\"\"Convert a binary maze to an adjacency-list representation.\n\n    Each free cell (value 1) becomes a vertex.  Two vertices share an\n    (undirected, un-weighted) edge iff the corresponding cells are adjacent\n    in the four-neighbourhood (up, down, left, right).\n\n    A dictionary mapping every free cell coordinate (row, col) to a *sorted*\n    list of adjacent free cells is returned.  If the maze contains no free\n    cells, an empty dictionary is returned.\n    \"\"\"\n    if maze.size == 0:\n        return {}\n    (rows, cols) = maze.shape\n    free_cells = np.argwhere(maze == 1)\n    if free_cells.size == 0:\n        return {}\n    adjacency = {tuple(cell): set() for cell in free_cells}\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    for (r, c) in free_cells:\n        for (dr, dc) in directions:\n            (nr, nc) = (r + dr, c + dc)\n            if 0 <= nr < rows and 0 <= nc < cols and (maze[nr, nc] == 1):\n                adjacency[r, c].add((nr, nc))\n                adjacency[nr, nc].add((r, c))\n    return {cell: sorted(neighbours) for (cell, neighbours) in adjacency.items()}"}
{"task_id": 597, "completion_id": 0, "solution": "import numpy as np\ndef best_split_mse(X: list | np.ndarray, y: list | np.ndarray) -> list:\n    \"\"\"Return the best (feature, threshold) split for a regression tree.\n\n    A split is evaluated with the weighted mean\u2013squared error (population\n    variance) of the two resulting groups.  Only thresholds that coincide\n    with an existing feature value are examined.\n\n    Parameters\n    ----------\n    X : list | np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : list | np.ndarray\n        Target vector of shape (n_samples,).\n\n    Returns\n    -------\n    list\n        [best_feature_index, best_threshold, best_mse] where *best_mse*\n        is rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    (n_samples, n_features) = X.shape\n    best_feature = None\n    best_threshold = None\n    best_mse = np.inf\n    eps = 1e-12\n    for j in range(n_features):\n        x_col = X[:, j]\n        order = np.argsort(x_col, kind='mergesort')\n        xs = x_col[order]\n        ys = y[order]\n        csum = np.cumsum(ys, dtype=float)\n        csum_sq = np.cumsum(ys ** 2, dtype=float)\n        total_sum = csum[-1]\n        total_sq_sum = csum_sq[-1]\n        change_pos = np.where(np.diff(xs) != 0)[0]\n        if change_pos.size == 0:\n            continue\n        for idx in change_pos:\n            left_n = idx + 1\n            right_n = n_samples - left_n\n            if right_n == 0:\n                continue\n            sum_left = csum[idx]\n            sq_left = csum_sq[idx]\n            mean_left = sum_left / left_n\n            var_left = sq_left / left_n - mean_left ** 2\n            sum_right = total_sum - sum_left\n            sq_right = total_sq_sum - sq_left\n            mean_right = sum_right / right_n\n            var_right = sq_right / right_n - mean_right ** 2\n            mse = left_n / n_samples * var_left + right_n / n_samples * var_right\n            update = False\n            if mse < best_mse - eps:\n                update = True\n            elif abs(mse - best_mse) <= eps:\n                if j < best_feature:\n                    update = True\n                elif j == best_feature and xs[idx] < best_threshold - eps:\n                    update = True\n            if update:\n                best_feature = j\n                best_threshold = xs[idx]\n                best_mse = mse\n    return [int(best_feature), float(best_threshold), round(float(best_mse), 4)]"}
{"task_id": 598, "completion_id": 0, "solution": "import numpy as np\ndef standardize(data: list[list[int | float]]) -> list[list[float]]:\n    \"\"\"Standardise a numeric data matrix feature-wise (z-scores).\n\n    Each column of *data* is centred to mean 0 and scaled to variance 1\n    (population, i.e. ddof=0).  Constant columns (\u03c3 == 0) are set to 0.\n    Any NaN/Inf produced during the computation are replaced by 0.\n    The result is rounded to 4 decimals and returned as a plain list.\n\n    Args:\n        data: 2-D list (n_samples \u00d7 n_features) of numbers.\n\n    Returns:\n        list[list[float]]: Standardised matrix with the same shape.\n    \"\"\"\n    if not data:\n        return []\n    arr = np.asarray(data, dtype=float)\n    if arr.ndim == 1:\n        arr = arr.reshape(-1, 1)\n    mean = arr.mean(axis=0)\n    std = arr.std(axis=0, ddof=0)\n    std_safe = std.copy()\n    std_safe[std_safe == 0] = 1.0\n    z = (arr - mean) / std_safe\n    z[:, std == 0] = 0.0\n    z = np.nan_to_num(z, nan=0.0, posinf=0.0, neginf=0.0)\n    z = np.round(z, 4)\n    return z.tolist()"}
