{"task_id": 0, "completion_id": 0, "solution": "import numpy as np\ndef ridge_regression(data: np.ndarray, alpha: float=0.0) -> list[float]:\n    \"\"\"\n    Closed-form Ridge (L2-regularised) linear-regression solver.\n    \n    Parameters\n    ----------\n    data  : np.ndarray\n        2-D array where the last column is the target y and the remaining\n        columns are the features X.\n    alpha : float, default 0.0\n        Regularisation strength (\u03b1 \u2265 0).\n        \n    Returns\n    -------\n    list[float]\n        Weight vector (including the bias/intercept as the first element)\n        rounded to 4 decimal places.  \n        If (X\u1d40X + \u03b1I) is numerically singular the function returns -1.\n    \"\"\"\n    (X, y) = (data[:, :-1], data[:, -1])\n    n_samples = X.shape[0]\n    X_bias = np.hstack((np.ones((n_samples, 1)), X))\n    A = X_bias.T @ X_bias + alpha * np.eye(X_bias.shape[1])\n    try:\n        det = np.linalg.det(A)\n    except np.linalg.LinAlgError:\n        return -1\n    if np.isclose(det, 0.0, atol=1e-12):\n        return -1\n    w = np.linalg.inv(A) @ X_bias.T @ y\n    return np.round(w, 4).tolist()"}
{"task_id": 1, "completion_id": 0, "solution": "import numpy as np\ndef perceptron_dual(X_train: np.ndarray, y_train: np.ndarray, eta: float=1.0, n_iter: int=10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron learning algorithm.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples, shape (n_samples, n_features).\n    y_train : np.ndarray\n        Labels (+1 or -1), length n_samples.\n    eta : float, optional\n        Learning rate, default 1.0.\n    n_iter : int, optional\n        Maximum number of parameter updates, default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        Weight vector (as a list) and bias, both rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=float).ravel()\n    if not set(np.unique(y)).issubset({-1.0, 1.0}):\n        raise ValueError('y_train must contain only +1 and -1.')\n    (n_samples, n_features) = X.shape\n    alpha = np.zeros(n_samples, dtype=float)\n    b = 0.0\n    updates = 0\n    gram = X @ X.T\n    while updates < n_iter:\n        no_mistake_in_pass = True\n        for i in range(n_samples):\n            activation = np.dot(alpha * y, gram[:, i])\n            margin = y[i] * (activation + b)\n            if margin <= 0.0:\n                alpha[i] += eta\n                b += eta * y[i]\n                updates += 1\n                no_mistake_in_pass = False\n                break\n        if no_mistake_in_pass:\n            break\n    w = np.dot(alpha * y, X)\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return (w_rounded, b_rounded)"}
{"task_id": 2, "completion_id": 0, "solution": "import numpy as np\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A dictionary with three entries  (\"identity\", \"log\", \"logit\").\n        Each entry itself is a dictionary with three callables\n\n            \u2022 \"link\"      :  g(\u03bc)\n            \u2022 \"inv_link\"  :  g\u207b\u00b9(\u03b7)\n            \u2022 \"derivative\":  g\u2032(\u03bc)\n\n        All functions work element\u2013wise on numpy scalars, 1-D and 2-D\n        numpy arrays.\n    \"\"\"\n\n    def _id_link(mu):\n        return mu\n\n    def _id_inv_link(eta):\n        return eta\n\n    def _id_derivative(mu):\n        return np.ones_like(mu, dtype=float)\n\n    def _log_link(mu):\n        return np.log(mu)\n\n    def _log_inv_link(eta):\n        return np.exp(eta)\n\n    def _log_derivative(mu):\n        return 1.0 / mu\n\n    def _logit_link(mu):\n        return np.log(mu / (1.0 - mu))\n\n    def _logit_inv_link(eta):\n        eta = np.asarray(eta)\n        return np.where(eta >= 0, 1.0 / (1.0 + np.exp(-eta)), np.exp(eta) / (1.0 + np.exp(eta)))\n\n    def _logit_derivative(mu):\n        return 1.0 / (mu * (1.0 - mu))\n    return {'identity': {'link': _id_link, 'inv_link': _id_inv_link, 'derivative': _id_derivative}, 'log': {'link': _log_link, 'inv_link': _log_inv_link, 'derivative': _log_derivative}, 'logit': {'link': _logit_link, 'inv_link': _logit_inv_link, 'derivative': _logit_derivative}}"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Return, for every sample, the depth of the leaf it lands in inside a\n    single random isolation tree grown as described in the task statement.\n    \"\"\"\n    np.random.seed(0)\n    if data.size == 0:\n        return []\n    (n_samples, n_features) = data.shape\n    depths = [-1] * n_samples\n\n    def grow(indices: list[int], depth: int) -> None:\n        \"\"\"Recursively grow the isolation tree while filling `depths`.\"\"\"\n        if len(indices) < 3:\n            for idx in indices:\n                depths[idx] = depth\n            return\n        f = np.random.randint(0, n_features)\n        vals = data[indices, f]\n        (down, up) = (vals.min(), vals.max())\n        if down == up:\n            for idx in indices:\n                depths[idx] = depth\n            return\n        v = np.random.uniform(down, up)\n        left = [idx for idx in indices if data[idx, f] <= v]\n        right = [idx for idx in indices if data[idx, f] > v]\n        if len(left) == 0 or len(right) == 0:\n            for idx in indices:\n                depths[idx] = depth\n            return\n        grow(left, depth + 1)\n        grow(right, depth + 1)\n    grow(list(range(n_samples)), depth=0)\n    return depths"}
{"task_id": 7, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable, Iterable, FrozenSet\ndef _sorted_tuple(it: Iterable[Hashable]) -> Tuple[Hashable, ...]:\n    \"\"\"\n    Return the items of *it* as an ascending tuple.\n\n    When item types are not mutually comparable we fall back to comparing their\n    textual representation, which is at least deterministic.\n    \"\"\"\n    try:\n        return tuple(sorted(it))\n    except TypeError:\n        return tuple(sorted(it, key=repr))\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent item-sets together with their absolute support.\n\n    Args\n    ----\n    transactions : list[list[hashable]]\n        Transactional data base \u2013 duplicates inside one transaction are ignored.\n    minsup       : float\n        Minimum support given as a fraction in the (0, 1] interval.\n\n    Returns\n    -------\n    dict[tuple, int]\n        Maps every frequent item-set (tuple with ascending items) to its support.\n        The empty dict is returned when no item-set fulfils the threshold.\n    \"\"\"\n    if not 0 < minsup <= 1:\n        raise ValueError('minsup must be in the interval (0, 1].')\n    transactions = [set(t) for t in transactions]\n    n_transactions = len(transactions)\n    if n_transactions == 0:\n        return {}\n    min_count = max(1, math.ceil(minsup * n_transactions))\n    result: Dict[Tuple[Hashable, ...], int] = {}\n    item_counter = Counter()\n    for t in transactions:\n        item_counter.update(t)\n    L_k: Dict[FrozenSet[Hashable], int] = {frozenset([item]): supp for (item, supp) in item_counter.items() if supp >= min_count}\n    for (fs, supp) in L_k.items():\n        result[_sorted_tuple(fs)] = supp\n    k = 2\n    while L_k:\n        L_prev = list(L_k.keys())\n        candidates: set[FrozenSet[Hashable]] = set()\n        for i in range(len(L_prev)):\n            for j in range(i + 1, len(L_prev)):\n                union = L_prev[i] | L_prev[j]\n                if len(union) != k:\n                    continue\n                if all((union - frozenset([item]) in L_k for item in union)):\n                    candidates.add(union)\n        if not candidates:\n            break\n        cand_counter = Counter()\n        for t in transactions:\n            for cand in candidates:\n                if cand.issubset(t):\n                    cand_counter[cand] += 1\n        L_k = {cand: supp for (cand, supp) in cand_counter.items() if supp >= min_count}\n        for (fs, supp) in L_k.items():\n            result[_sorted_tuple(fs)] = supp\n        k += 1\n    return result"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data: np.ndarray, k: int, epsilon: float=0.001, max_iter: int=2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array (n_samples \u00d7 n_features).\n    k : int\n        Number of desired clusters (1 \u2264 k \u2264 n_samples).\n    epsilon : float, default 1e-3\n        Stop if the largest centre movement is smaller than *epsilon*.\n    max_iter : int, default 2000\n        Hard iteration cap.\n\n    Returns\n    -------\n    (centres, labels)\n        centres : list[list[float]]\n            k cluster centres rounded to 4 decimals.\n        labels : list[int]\n            Cluster index (0 \u2026 k-1) assigned to every sample.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('data must be a 2-D array')\n    (n_samples, n_features) = data.shape\n    if not 1 <= k <= n_samples:\n        raise ValueError('k must satisfy 1 \u2264 k \u2264 number of samples')\n    data = data.astype(float, copy=False)\n    centres = data[:k].copy()\n    for _ in range(max_iter):\n        dists = np.sum((data[:, None, :] - centres[None, :, :]) ** 2, axis=2)\n        labels = np.argmin(dists, axis=1)\n        new_centres = centres.copy()\n        for j in range(k):\n            mask = labels == j\n            if mask.any():\n                new_centres[j] = data[mask].mean(axis=0)\n            else:\n                rnd_idx = np.random.randint(0, n_samples)\n                new_centres[j] = data[rnd_idx]\n        shifts = np.linalg.norm(new_centres - centres, axis=1)\n        if np.max(shifts) < epsilon:\n            centres = new_centres\n            break\n        centres = new_centres\n    centres_rounded = np.round(centres, 4).tolist()\n    return (centres_rounded, labels.tolist())"}
{"task_id": 13, "completion_id": 0, "solution": "import numpy as np\nfrom collections import defaultdict\ndef knn_recommend(data: np.ndarray, user_ind: int, k: int, criterion: str='cosine') -> list[int]:\n    \"\"\"Item\u2013based k-NN collaborative\u2013filtering recommender.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Rating matrix of shape (n_user, n_item); 0 = *not rated*.\n    user_ind : int\n        Index of the active user.\n    k : int\n        Maximal number of items to recommend.\n    criterion : {'cosine', 'pearson'}, default 'cosine'\n        Similarity measure to use.\n\n    Returns\n    -------\n    list[int]\n        Indices of at most *k* unseen items ordered by decreasing\n        predicted attractiveness.\n    \"\"\"\n    if criterion not in ('cosine', 'pearson'):\n        raise ValueError(\"criterion must be either 'cosine' or 'pearson'\")\n    (n_user, n_item) = data.shape\n    if not 0 <= user_ind < n_user:\n        raise IndexError('user_ind out of bounds')\n    S = np.zeros((n_item, n_item), dtype=np.float64)\n    for i in range(n_item - 1):\n        col_i = data[:, i]\n        r_i_mask = col_i > 0\n        for j in range(i + 1, n_item):\n            col_j = data[:, j]\n            common = r_i_mask & (col_j > 0)\n            if not np.any(common):\n                continue\n            v1 = col_i[common].astype(np.float64)\n            v2 = col_j[common].astype(np.float64)\n            if criterion == 'cosine':\n                if np.std(v1, ddof=1) > 0.001:\n                    v1c = v1 - v1.mean()\n                else:\n                    v1c = v1\n                if np.std(v2, ddof=1) > 0.001:\n                    v2c = v2 - v2.mean()\n                else:\n                    v2c = v2\n                (n1, n2) = (np.linalg.norm(v1c), np.linalg.norm(v2c))\n                sim = 0.0 if n1 == 0.0 or n2 == 0.0 else float(v1c @ v2c / (n1 * n2))\n            elif len(v1) < 2 or np.std(v1, ddof=1) < 0.001 or np.std(v2, ddof=1) < 0.001:\n                sim = 0.0\n            else:\n                sim = float(np.corrcoef(v1, v2, ddof=1)[0, 1])\n                if np.isnan(sim):\n                    sim = 0.0\n            S[i, j] = S[j, i] = sim\n    user_row = data[user_ind]\n    rated_mask = user_row > 0\n    rated_idx = np.where(rated_mask)[0]\n    if len(rated_idx) == n_item:\n        return []\n    scores = {}\n    for t in range(n_item):\n        if rated_mask[t]:\n            continue\n        sims_to_rated = S[t, rated_idx]\n        ratings = user_row[rated_idx]\n        denom = np.sum(np.abs(sims_to_rated))\n        if denom == 0.0:\n            score = 0.0\n        else:\n            score = float(ratings @ sims_to_rated / denom)\n        scores[t] = score\n    sorted_items = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n    return [idx for (idx, _) in sorted_items[:k]]"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef best_split(X: np.ndarray, g: np.ndarray, h: np.ndarray, gamma: float, lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split maximises the reduction in the regularised loss used by\n    gradient-boosting decision trees.  If no legal split gives a positive\n    gain the function returns ``None``.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    if n_samples < 4:\n        return None\n\n    def leaf_loss(G: float, H: float) -> float:\n        return -0.5 * (G * G) / (H + lam) + gamma\n    G_total = g.sum()\n    H_total = h.sum()\n    parent_loss = leaf_loss(G_total, H_total)\n    best_gain = 0.0\n    best_feature = None\n    best_threshold = None\n    for feat_idx in range(n_features):\n        order = np.argsort(X[:, feat_idx], kind='mergesort')\n        x_sorted = X[order, feat_idx]\n        g_sorted = g[order]\n        h_sorted = h[order]\n        g_cumsum = np.cumsum(g_sorted)\n        h_cumsum = np.cumsum(h_sorted)\n        for i in range(1, n_samples - 2):\n            if x_sorted[i] == x_sorted[i + 1]:\n                continue\n            G_left = g_cumsum[i]\n            H_left = h_cumsum[i]\n            G_right = G_total - G_left\n            H_right = H_total - H_left\n            if n_samples - i - 1 < 2:\n                break\n            loss_left = leaf_loss(G_left, H_left)\n            loss_right = leaf_loss(G_right, H_right)\n            gain = parent_loss - loss_left - loss_right\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = feat_idx\n                best_threshold = x_sorted[i]\n    if best_feature is None:\n        return None\n    return (best_feature, float(best_threshold))"}
{"task_id": 20, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    is_scalar = np.isscalar(x)\n    x_arr = np.asarray(x, dtype=float)\n    sigmoid_arr = 1.0 / (1.0 + np.exp(-x_arr))\n    gradient_arr = sigmoid_arr * (1.0 - sigmoid_arr)\n    sigmoid_arr = np.round(sigmoid_arr, 4)\n    gradient_arr = np.round(gradient_arr, 4)\n    if is_scalar:\n        return (float(sigmoid_arr), float(gradient_arr))\n    else:\n        return (sigmoid_arr.tolist(), gradient_arr.tolist())"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\nimport math\nfrom collections import defaultdict\ndef adaboost_1d_predict(x_train: list[float], y_train: list[int], x_test: list[float], epsilon: float=0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble of decision stumps and returns test predictions.\"\"\"\n\n    def stump_predict(x: np.ndarray, d: int, theta: float) -> np.ndarray:\n        \"\"\"Return \u00b11 predictions of the stump (d , \u03b8) on vector x.\"\"\"\n        if d == 0:\n            return np.where(x <= theta, 1, -1)\n        else:\n            return np.where(x > theta, 1, -1)\n    x_train = np.asarray(x_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    x_test = np.asarray(x_test, dtype=float)\n    n = len(x_train)\n    if n == 0:\n        return [1] * len(x_test)\n    order = np.argsort(x_train)\n    x_sorted = x_train[order]\n    thresholds = [x_sorted[0] - 1.0]\n    for i in range(n - 1):\n        if x_sorted[i] != x_sorted[i + 1]:\n            thresholds.append(0.5 * (x_sorted[i] + x_sorted[i + 1]))\n    thresholds.append(x_sorted[-1] + 1.0)\n    stumps = [(d, th) for th in thresholds for d in (0, 1)]\n    w = np.full(n, 1.0 / n)\n    (alphas, chosen_stumps) = ([], [])\n    while True:\n        best_err = float('inf')\n        best_pred = None\n        best_stump = None\n        for (d, th) in stumps:\n            pred = stump_predict(x_train, d, th)\n            err = w[pred != y_train].sum()\n            if err < best_err - 1e-12:\n                (best_err, best_stump, best_pred) = (err, (d, th), pred)\n        if best_err >= 0.5 - 1e-12:\n            break\n        err = max(best_err, 1e-12)\n        alpha = 0.5 * math.log((1.0 - err) / err)\n        chosen_stumps.append(best_stump)\n        alphas.append(alpha)\n        w *= np.exp(-alpha * y_train * best_pred)\n        w = w / w.sum()\n        F = np.zeros(n)\n        for (a, (d, th)) in zip(alphas, chosen_stumps):\n            F += a * stump_predict(x_train, d, th)\n        train_pred = np.where(F >= 0.0, 1, -1)\n        train_error = (train_pred != y_train).mean()\n        if train_error <= epsilon:\n            break\n    if not alphas:\n        return [1] * len(x_test)\n    F_test = np.zeros(len(x_test))\n    for (a, (d, th)) in zip(alphas, chosen_stumps):\n        F_test += a * stump_predict(x_test, d, th)\n    y_pred = np.where(F_test >= 0.0, 1, -1)\n    return y_pred.tolist()"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef gaussian_svm_predict(X_train: np.ndarray, y_train: np.ndarray, alpha: np.ndarray, b: float, gamma: float, X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    y_train = y_train.ravel()\n    alpha = alpha.ravel()\n    coef = alpha * y_train\n    train_sq = np.sum(X_train ** 2, axis=1)\n    test_sq = np.sum(X_test ** 2, axis=1)\n    dists_sq = test_sq[:, None] + train_sq[None, :] - 2.0 * X_test @ X_train.T\n    K = np.exp(-gamma * dists_sq)\n    g = K @ coef + b\n    pred = np.sign(g)\n    pred[pred == 0] = 1\n    return pred.astype(int).tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        (X_hat, mse) where\n            \u2022 X_hat \u2013 rank-k reconstruction of X (each entry rounded to 4 decimals)\n            \u2022 mse    \u2013 mean-squared reconstruction error (rounded to 4 decimals)\n        If k is not in [1, min(m, n)] the function returns -1.\n    \"\"\"\n    if not isinstance(k, int):\n        return -1\n    try:\n        X_arr = np.asarray(X, dtype=float)\n    except Exception:\n        return -1\n    if X_arr.ndim != 2:\n        return -1\n    (m, n) = X_arr.shape\n    if k < 1 or k > min(m, n):\n        return -1\n    (U, S, VT) = np.linalg.svd(X_arr, full_matrices=False)\n    U_k = U[:, :k]\n    S_k = S[:k]\n    VT_k = VT[:k, :]\n    X_hat = U_k * S_k @ VT_k\n    mse = float(np.round(np.mean((X_arr - X_hat) ** 2), 4))\n    X_hat_rounded = np.round(X_hat, 4).tolist()\n    return (X_hat_rounded, mse)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef predict_boosting_tree(x_train: list[float] | np.ndarray, y_train: list[float] | np.ndarray, x_query: float, epsilon: float=0.01) -> float:\n    \"\"\"1-D gradient boosting with decision stumps and prediction for *x_query*.\n    \"\"\"\n    x = np.asarray(x_train, dtype=float).ravel()\n    y = np.asarray(y_train, dtype=float).ravel()\n    if x.size == 0:\n        return float('nan')\n    if np.all(x == x[0]) or x.size == 1:\n        return round(float(y.mean()), 4)\n    order = np.argsort(x)\n    x_sorted = x[order]\n    y_sorted = y[order]\n    distinct_mask = x_sorted[1:] != x_sorted[:-1]\n    mids = (x_sorted[:-1] + x_sorted[1:]) / 2.0\n    candidate_s = mids[distinct_mask]\n    if candidate_s.size == 0:\n        return round(float(y.mean()), 4)\n    residuals = y_sorted.copy()\n    stumps = []\n    rss = np.sum(residuals ** 2)\n    max_iter = 1000\n    iteration = 0\n    while rss > epsilon and iteration < max_iter:\n        best_rss = np.inf\n        best_params = None\n        for s in candidate_s:\n            mask_left = x_sorted <= s\n            mask_right = ~mask_left\n            if not mask_left.any() or not mask_right.any():\n                continue\n            c1 = residuals[mask_left].mean()\n            c2 = residuals[mask_right].mean()\n            new_residuals = residuals - np.where(mask_left, c1, c2)\n            rss_candidate = np.sum(new_residuals ** 2)\n            if rss_candidate < best_rss:\n                best_rss = rss_candidate\n                best_params = (s, c1, c2, mask_left)\n        if best_params is None or best_rss >= rss - 1e-12:\n            break\n        (s_best, c1_best, c2_best, mask_left_best) = best_params\n        stumps.append((s_best, c1_best, c2_best))\n        residuals = residuals - np.where(mask_left_best, c1_best, c2_best)\n        rss = best_rss\n        iteration += 1\n    prediction = 0.0\n    for (s, c1, c2) in stumps:\n        prediction += c1 if x_query <= s else c2\n    return round(float(prediction), 4)"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef random_binary_tensor(shape: tuple[int, ...], sparsity: float=0.5, seed: int | None=None):\n    \"\"\"Create a tensor whose entries are 0.0 or 1.0.\n    \n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired shape of the returned NumPy array.\n    sparsity : float, default=0.5\n        Probability that any individual entry is 1.0.  Must satisfy\n        0.0 \u2264 sparsity \u2264 1.0.  Values outside this range cause the\n        function to return -1.\n    seed : int | None, default=None\n        If provided, ``np.random.seed(seed)`` is called first so the\n        output is reproducible.\n    \n    Returns\n    -------\n    numpy.ndarray | int\n        \u2022 A floating-dtype NumPy array containing only 0.0 and 1.0, if\n          `sparsity` is legal.  \n        \u2022 The integer -1 when `sparsity` is outside the closed interval\n          [0, 1].\n    \"\"\"\n    if not 0.0 <= sparsity <= 1.0:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_values = np.random.rand(*shape)\n    binary_tensor = (random_values < sparsity).astype(float)\n    return binary_tensor"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median\n          (upper median for even *n*);\n        \u2022 left  is a list of row indices with strictly smaller values;\n        \u2022 right is a list of row indices with strictly larger  values.\n        The two lists are sorted increasingly.\n    \"\"\"\n    col = data[:, d]\n    n = col.shape[0]\n    k = n // 2\n    part_idx = np.argpartition(col, k)\n    pivot = int(part_idx[k])\n    pivot_value = col[pivot]\n    left = []\n    right = []\n    for idx in range(n):\n        if idx == pivot:\n            continue\n        v = col[idx]\n        if v < pivot_value:\n            left.append(idx)\n        elif v > pivot_value:\n            right.append(idx)\n    return (pivot, left, right)"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool=False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into equally sized, possibly overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length N.\n    frame_width : int\n        Number of samples per frame.\n    stride : int\n        Hop size (number of samples between the starts of successive frames).\n    writeable : bool, default=False\n        If False the returned view is marked read-only.\n        If True the view is writeable only when the source array is writeable.\n\n    Returns\n    -------\n    np.ndarray\n        View on *x* with shape (n_frames, frame_width), where\n            n_frames = (len(x) - frame_width) // stride + 1\n    \"\"\"\n    assert x.ndim == 1, 'x must be one-dimensional'\n    assert frame_width > 0, 'frame_width must be a positive integer'\n    assert stride > 0, 'stride must be a positive integer'\n    assert len(x) >= frame_width, 'frame_width larger than signal length'\n    n_frames = (len(x) - frame_width) // stride + 1\n    shape = (n_frames, frame_width)\n    item_stride = x.strides[0]\n    strides = (item_stride * stride, item_stride)\n    view_writeable = x.flags.writeable if writeable else False\n    frames = as_strided(x, shape=shape, strides=strides, writeable=view_writeable)\n    return frames"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef is_stochastic(X: list[list[float]] | 'np.ndarray') -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\"\"\"\n    try:\n        A = np.asarray(X, dtype=float)\n    except Exception:\n        return False\n    if A.ndim != 2 or A.size == 0:\n        return False\n    if (~np.isfinite(A)).any():\n        return False\n    if ((A < 0) | (A > 1)).any():\n        return False\n    if not np.allclose(A.sum(axis=1), 1.0, atol=1e-08):\n        return False\n    return True"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef rms_prop(X: np.ndarray, y: np.ndarray, epsilon: float=0.0001, max_iter: int=10000, eta: float=0.01, rho: float=0.9, batch_size: int=32, eps_station: float=1e-08) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    (n, d) = X.shape\n    w = np.zeros(d)\n    s = np.zeros(d)\n    batch_size = n if n < batch_size else batch_size\n    for it in range(max_iter):\n        start = it * batch_size % n\n        idx = np.arange(start, start + batch_size) % n\n        X_b = X[idx]\n        y_b = y[idx]\n        err = X_b @ w - y_b\n        g = X_b.T @ err / len(y_b)\n        s = rho * s + (1.0 - rho) * g ** 2\n        w -= eta * g / (np.sqrt(s) + eps_station)\n        full_grad = X.T @ (X @ w - y) / n\n        if np.linalg.norm(full_grad) < epsilon:\n            break\n    return np.round(w, 4).tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef softplus(x, deriv: bool=False):\n    \"\"\"\n    Numerically\u2013stable SoftPlus activation (and its gradient).\n\n    Parameters\n    ----------\n    x     : scalar, list or np.ndarray\n        Input value(s).\n    deriv : bool, default False\n        If True return the gradient (sigmoid), otherwise SoftPlus.\n\n    Returns\n    -------\n    float or list\n        Rounded result(s) \u2013 float when the input was a scalar,\n        otherwise a Python list.\n    \"\"\"\n    is_scalar = np.isscalar(x)\n    x = np.asarray(x, dtype=np.float64)\n    if deriv:\n        pos_mask = x >= 0\n        neg_mask = ~pos_mask\n        out = np.empty_like(x)\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n        exp_x = np.exp(x[neg_mask])\n        out[neg_mask] = exp_x / (1.0 + exp_x)\n    else:\n        out = np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n    out = np.round(out, 4)\n    if is_scalar:\n        return float(out)\n    else:\n        return out.tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef _count_ngrams(tokens: list[str], n: int) -> Counter:\n    \"\"\"\n    Helper: return a Counter of all length-n n-grams found in *tokens*.\n    Unigrams are counted as the string itself, higher orders as tuples.\n    \"\"\"\n    if n == 1:\n        return Counter(tokens)\n    counts = Counter()\n    for i in range(len(tokens) - n + 1):\n        counts[tuple(tokens[i:i + n])] += 1\n    return counts\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Unsmooth\\xaded MLE N-gram log-probability of *sequence* given *corpus*.\"\"\"\n    if N < 1:\n        raise ValueError('N must be >= 1')\n    if len(sequence) < N:\n        return 0.0\n    if N == 1:\n        unigram_counts = _count_ngrams(corpus, 1)\n        corpus_len = len(corpus)\n    else:\n        ngram_counts = _count_ngrams(corpus, N)\n        prefix_counts = _count_ngrams(corpus, N - 1)\n    total_log_prob = 0.0\n    for i in range(len(sequence) - N + 1):\n        if N == 1:\n            word = sequence[i]\n            numerator = unigram_counts.get(word, 0)\n            denominator = corpus_len\n        else:\n            ngram = tuple(sequence[i:i + N])\n            prefix = ngram[:-1]\n            numerator = ngram_counts.get(ngram, 0)\n            denominator = prefix_counts.get(prefix, 0)\n        if numerator == 0 or denominator == 0:\n            return float('-inf')\n        prob = numerator / denominator\n        total_log_prob += float(np.log(prob))\n    return round(total_log_prob, 4)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\ndef lasso_regression(X: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray', degree: int, reg_factor: float, n_iterations: int=1000, tol: float=1e-06) -> list[float]:\n    \"\"\"Univariate Lasso regression with polynomial features (coordinate descent).\"\"\"\n    X = np.asarray(X, dtype=float).ravel()\n    y = np.asarray(y, dtype=float).ravel()\n    m = X.size\n    if m == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y.size != m:\n        raise ValueError('X and y must have the same length.')\n    if degree < 0:\n        raise ValueError('degree must be non-negative.')\n    if reg_factor < 0:\n        raise ValueError('reg_factor must be non-negative.')\n    X_poly = np.column_stack([np.ones_like(X)] + [X ** k for k in range(1, degree + 1)])\n    d_plus_1 = degree + 1\n    w = np.zeros(d_plus_1)\n\n    def soft_threshold(rho: float, lam: float) -> float:\n        if rho > lam:\n            return rho - lam\n        if rho < -lam:\n            return rho + lam\n        return 0.0\n    for _ in range(n_iterations):\n        w_old = w.copy()\n        w[0] = np.mean(y - X_poly[:, 1:] @ w[1:])\n        y_minus_pred = None\n        for j in range(1, d_plus_1):\n            x_j = X_poly[:, j]\n            if y_minus_pred is None:\n                y_minus_pred = y - X_poly @ w\n            rho = x_j @ (y_minus_pred + w[j] * x_j)\n            if reg_factor == 0.0:\n                w[j] = rho / (x_j @ x_j)\n            else:\n                w[j] = soft_threshold(rho, reg_factor) / (x_j @ x_j)\n            y_minus_pred = y_minus_pred - (w[j] - w_old[j]) * x_j\n        if np.max(np.abs(w - w_old)) < tol:\n            break\n    return [round(float(coef), 4) for coef in w]"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"\n    Compute the backward-probability vector \u03b2_t for a discrete Hidden Markov\n    Model.\n\n    Parameters\n    ----------\n    A   : transition-probability matrix, shape (N, N)\n    B   : emission-probability matrix, shape (N, M)\n    obs : list with the indices of the observed symbols; length = T\n    t   : time index for which \u03b2_t is required  (0 \u2264 t < T)\n\n    Returns\n    -------\n    list[float]\n        The N backward probabilities \u03b2_t(i) (i = 0 \u2026 N-1), each rounded\n        to 4 decimal places.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    T = len(obs)\n    N = A.shape[0]\n    if not 0 <= t < T:\n        raise ValueError('t must satisfy 0 \u2264 t < len(obs)')\n    beta_next = np.ones(N)\n    for k in range(T - 2, t - 1, -1):\n        emis_beta = B[:, obs[k + 1]] * beta_next\n        beta_curr = A.dot(emis_beta)\n        beta_next = beta_curr\n    return np.round(beta_next, 4).tolist()"}
{"task_id": 65, "completion_id": 0, "solution": "import numpy as np\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Args:\n        A: transition matrix (NxN)\n        B: emission   matrix (NxM)\n        pi: initial state distribution (N)\n        obs: observation index sequence (length T)\n\n    Returns:\n        Probability P(O | \u03bb) rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or (not pi) or (not obs):\n        return 0.0\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    pi = np.asarray(pi, dtype=float)\n    obs = np.asarray(obs, dtype=int)\n    N = len(pi)\n    T = len(obs)\n    beta = np.ones(N, dtype=float)\n    for t in range(T - 2, -1, -1):\n        factor = B[:, obs[t + 1]] * beta\n        beta = A.dot(factor)\n    prob = float(np.dot(pi * B[:, obs[0]], beta))\n    return round(prob, 6)"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef forward_algorithm(S: list[float], A: list[list[float]], B: list[list[float]], observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    n = len(S)\n    if n == 0 or not observations:\n        return -1\n    if len(A) != n or any((len(row) != n for row in A)):\n        return -1\n    if len(B) != n or len(B[0]) == 0:\n        return -1\n    m = len(B[0])\n    if any((len(row) != m for row in B)):\n        return -1\n    for o in observations:\n        if o < 0 or o >= m:\n            return -1\n    S = np.asarray(S, dtype=float)\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    alpha = S * B[:, observations[0]]\n    for ot in observations[1:]:\n        alpha = B[:, ot] * (alpha @ A)\n    likelihood = float(alpha.sum())\n    return round(likelihood, 4)"}
{"task_id": 70, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Optional, Dict, List\nclass _FPNode:\n    \"\"\"A node in an FP-tree.\"\"\"\n    __slots__ = ('item', 'count', 'parent', 'children', 'link')\n\n    def __init__(self, item: Optional[str], parent: Optional['._FPNode']):\n        self.item: Optional[str] = item\n        self.count: int = 0\n        self.parent: Optional['_FPNode'] = parent\n        self.children: Dict[str, _FPNode] = {}\n        self.link: Optional['_FPNode'] = None\ndef _build_fp_tree(transactions: List[List[str]], min_support: int) -> tuple[Optional[_FPNode], Optional[dict]]:\n    \"\"\"\n    Build an FP-tree and the accompanying header table from *transactions*.\n    Returns (root, header_table) or (None, None) if the tree would be empty.\n    The header table maps an item -> [support, head-of-node-link].\n    \"\"\"\n    freq = Counter()\n    for t in transactions:\n        freq.update(t)\n    freq = {item: c for (item, c) in freq.items() if c >= min_support}\n    if not freq:\n        return (None, None)\n    header: dict[str, list] = {i: [c, None] for (i, c) in freq.items()}\n    root = _FPNode(None, None)\n    for transaction in transactions:\n        filtered = [i for i in transaction if i in freq]\n        if not filtered:\n            continue\n        ordered = sorted(filtered, key=lambda x: (-freq[x], x))\n        current = root\n        for item in ordered:\n            if item in current.children:\n                child = current.children[item]\n                child.count += 1\n            else:\n                child = _FPNode(item, current)\n                child.count = 1\n                current.children[item] = child\n                head = header[item][1]\n                if head is None:\n                    header[item][1] = child\n                else:\n                    while head.link is not None:\n                        head = head.link\n                    head.link = child\n            current = child\n    return (root, header)\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings). `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result is deterministic:\n      \u2022 inside each item-set items are sorted alphabetically;\n      \u2022 the outer list is sorted by `(len(itemset), itemset)`.\n    \"\"\"\n    if not transactions or min_support < 1:\n        return []\n    (root, header) = _build_fp_tree(transactions, min_support)\n    if header is None:\n        return []\n    frequent_itemsets: list[list[str]] = []\n    _mine_tree(header, [], frequent_itemsets, min_support)\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    return frequent_itemsets"}
{"task_id": 75, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"k-Nearest Neighbour classifier (Euclidean metric).\n\n    Returns a list with the predicted class for every row in `X_test`.\n    If *k* is not a positive integer or *k* > n_train the function\n    returns the scalar -1 (as required by the specification).\n    \"\"\"\n    if not isinstance(k, int) or k <= 0 or k > len(X_train):\n        return -1\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    preds: list[int] = []\n    for x in X_test:\n        diff = X_train - x\n        dists = np.einsum('ij,ij->i', diff, diff)\n        if k == 1:\n            nn_idx = [int(np.argmin(dists))]\n        else:\n            nn_idx = np.argpartition(dists, k - 1)[:k]\n        votes: dict[int, int] = {}\n        for lbl in y_train[nn_idx]:\n            votes[int(lbl)] = votes.get(int(lbl), 0) + 1\n        max_vote = max(votes.values())\n        winner = min((lbl for (lbl, cnt) in votes.items() if cnt == max_vote))\n        preds.append(winner)\n    return preds"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities (same shape).\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    eps = np.finfo(float).eps\n    loss = -np.sum(y * np.log(y_pred + eps))\n    return float(np.round(loss, 4))"}
{"task_id": 77, "completion_id": 0, "solution": "import numpy as np\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]):\n    \"\"\"Forward propagation for an L-layer fully-connected neural network\n    that uses ReLU in all hidden layers and a sigmoid in the output layer.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing weights W1 \u2026 WL and biases b1 \u2026 bL.\n\n    Returns\n    -------\n    tuple[list[list[float]], list]\n        \u2022 AL_list  \u2013 the final activation rounded to 4 decimals\n                     and converted to a regular Python list.\n        \u2022 caches   \u2013 list with one cache per layer (kept for a backward pass).\n    \"\"\"\n\n    def linear_forward(A_prev, W, b):\n        Z = W @ A_prev + b\n        return (Z, (A_prev, W, b))\n\n    def relu(Z):\n        A = np.maximum(0, Z)\n        return (A, Z)\n\n    def sigmoid(Z):\n        A = 1.0 / (1.0 + np.exp(-Z))\n        return (A, Z)\n    caches = []\n    A = X\n    L = len(parameters) // 2\n    for l in range(1, L):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        (Z, lin_cache) = linear_forward(A, W, b)\n        (A, act_cache) = relu(Z)\n        caches.append((lin_cache, act_cache))\n    WL = parameters[f'W{L}']\n    bL = parameters[f'b{L}']\n    (ZL, lin_cache) = linear_forward(A, WL, bL)\n    (AL, act_cache) = sigmoid(ZL)\n    caches.append((lin_cache, act_cache))\n    AL_list = np.round(AL, 4).tolist()\n    return (AL_list, caches)"}
{"task_id": 81, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n    \n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n    \n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    eps = 1e-15\n    A2 = np.clip(A2, eps, 1.0 - eps)\n    m = Y.size\n    cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n    return float(np.round(cost, 6))"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef epsilon_soft(Q: list[float] | 'np.ndarray', epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft action\u2013selection probabilities.\n\n    Parameters\n    ----------\n    Q : 1-D sequence (list or np.ndarray)\n        Q-values for every available action in the current state.\n    epsilon : float\n        Exploration parameter 0 \u2264 \u03b5 \u2264 1.\n\n    Returns\n    -------\n    list[float]\n        A list with the probability of choosing every action under the \u03b5-soft\n        policy.  All probabilities are rounded to 4 decimal places and sum to\n        1 (within 1\u00d710\u207b\u00b9\u00b2).\n    \"\"\"\n    q = np.asarray(Q, dtype=float).ravel()\n    if q.ndim != 1:\n        raise ValueError('Q must be a one-dimensional sequence of numbers.')\n    if not 0.0 <= epsilon <= 1.0:\n        raise ValueError('epsilon must satisfy 0 \u2264 \u03b5 \u2264 1.')\n    n = q.size\n    if n == 0:\n        raise ValueError('Q must contain at least one action.')\n    greedy_idx = int(np.argmax(q))\n    base_prob = epsilon / n\n    probs = np.full(n, base_prob, dtype=float)\n    probs[greedy_idx] += 1.0 - epsilon\n    ticks = probs * 10000.0\n    int_part = np.floor(ticks).astype(int)\n    remainder = ticks - int_part\n    deficit = 10000 - int_part.sum()\n    if deficit > 0:\n        order = np.argsort(-remainder)\n        int_part[order[:deficit]] += 1\n    elif deficit < 0:\n        order = np.argsort(remainder)\n        int_part[order[:-deficit]] -= 1\n    final_probs = int_part / 10000.0\n    if abs(final_probs.sum() - 1.0) > 1e-12:\n        raise RuntimeError('Probabilities do not sum to one within tolerance.')\n    return final_probs.round(4).tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List, Union\nLabel = Union[int, float, str]\ndef aggregate_random_forest_votes(predictions: List[List[Label]]) -> List[Label]:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions or not predictions[0]:\n        return []\n    n_samples = len(predictions[0])\n    majority_votes: List[Label] = []\n    for idx in range(n_samples):\n        votes = [tree_preds[idx] for tree_preds in predictions]\n        vote_counts = Counter(votes)\n        max_count = max(vote_counts.values())\n        tied_labels = [label for (label, cnt) in vote_counts.items() if cnt == max_count]\n        majority_votes.append(min(tied_labels))\n    return majority_votes"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    is_scalar = np.isscalar(z) or (isinstance(z, np.ndarray) and z.shape == ())\n    z_arr = np.asarray(z, dtype=float)\n    soft = np.logaddexp(0.0, z_arr)\n    soft = np.round(soft, 4)\n    if is_scalar:\n        return float(soft)\n    else:\n        return soft.tolist()"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Any\ndef _get_field(obj: Any, name: str):\n    \"\"\"Return attribute or (key) item `name` from `obj`, else raise KeyError.\"\"\"\n    if hasattr(obj, name):\n        return getattr(obj, name)\n    try:\n        return obj[name]\n    except Exception as exc:\n        raise KeyError(f'{name!r} not found in supplied object.') from exc\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Mean-squared error between a policy\u2019s value estimates and the truth.\"\"\"\n    try:\n        arm_evs = _get_field(bandit, 'arm_evs')\n    except KeyError:\n        raise ValueError('`bandit` must supply an `arm_evs` field.')\n    try:\n        ev_estimates = _get_field(policy, 'ev_estimates')\n    except KeyError:\n        return np.nan\n    if not ev_estimates:\n        return np.nan\n    errors = []\n    for arm_idx in sorted(ev_estimates):\n        if 0 <= arm_idx < len(arm_evs):\n            diff = ev_estimates[arm_idx] - arm_evs[arm_idx]\n            errors.append(diff * diff)\n    if not errors:\n        return np.nan\n    mse_val = float(np.mean(errors))\n    return round(mse_val, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "import math\nfrom collections import Counter, defaultdict\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float=1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\"\"\"\n    if N <= 0:\n        raise ValueError('N must be a positive integer.')\n    seq_tokens = sequence.split()\n    if len(seq_tokens) < N:\n        return 0.0\n    vocab = set()\n    ngram_counts: dict[int, Counter] = {n: Counter() for n in range(1, N + 1)}\n    for sent in corpus:\n        tokens = sent.split()\n        vocab.update(tokens)\n        for n in range(1, N + 1):\n            if len(tokens) < n:\n                continue\n            for i in range(len(tokens) - n + 1):\n                ngram = tuple(tokens[i:i + n])\n                ngram_counts[n][ngram] += 1\n    vocab.add('<UNK>')\n    V = len(vocab)\n    total_tokens = sum(ngram_counts[1].values())\n    seq_tokens = [tok if tok in vocab else '<UNK>' for tok in seq_tokens]\n    log_prob = 0.0\n    for i in range(len(seq_tokens) - N + 1):\n        ngram = tuple(seq_tokens[i:i + N])\n        context = ngram[:-1]\n        target = ngram[-1]\n        count_ngram = ngram_counts[N].get(ngram, 0)\n        if N == 1:\n            context_count = total_tokens\n        else:\n            context_count = ngram_counts[N - 1].get(context, 0)\n        prob = (count_ngram + K) / (context_count + K * V)\n        log_prob += math.log(prob)\n    return round(log_prob, 4)"}
{"task_id": 108, "completion_id": 0, "solution": "import numpy as np\ndef als_factorization(X: np.ndarray, K: int, alpha: float=1.0, max_iter: int=200, tol: float=0.0001) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=np.float64)\n    (N, M) = X.shape\n    rng = np.random.RandomState(0)\n    W = rng.randn(N, K)\n    H = rng.randn(K, M)\n    I_K = np.eye(K, dtype=np.float64)\n\n    def _solve(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n        \"\"\"Solve AX = B for X with fallback to pseudo-inverse if singular.\"\"\"\n        try:\n            return np.linalg.solve(A, B)\n        except np.linalg.LinAlgError:\n            return np.linalg.pinv(A) @ B\n    (best_W, best_H) = (W.copy(), H.copy())\n    best_loss = np.inf\n    for _ in range(max_iter):\n        HHT = H @ H.T + alpha * I_K\n        W = _solve(HHT, H @ X.T).T\n        WTW = W.T @ W + alpha * I_K\n        H = _solve(WTW, W.T @ X)\n        diff = X - W @ H\n        loss = np.sum(diff ** 2) + alpha * (np.sum(W ** 2) + np.sum(H ** 2))\n        if loss < best_loss:\n            best_loss = loss\n            (best_W, best_H) = (W.copy(), H.copy())\n        if loss <= tol:\n            break\n    X_hat = (best_W @ best_H).round(4).tolist()\n    return X_hat"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef _euclidean_squared(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"Squared Euclidean distance between every row of `a` and `b` (broadcast).\"\"\"\n    return ((a[:, None, :] - b[None, :, :]) ** 2).sum(axis=2)\ndef _kmeans_pp_init(X: np.ndarray, K: int) -> np.ndarray:\n    \"\"\"Return K initial centroids picked with the K-Means++ heuristic.\"\"\"\n    m = X.shape[0]\n    centroids = [X[np.random.randint(m)]]\n    for _ in range(1, K):\n        d2 = _euclidean_squared(X, np.array(centroids)).min(axis=1)\n        probs = d2 / d2.sum()\n        r = random.random()\n        cumulative = np.cumsum(probs)\n        idx = np.searchsorted(cumulative, r)\n        centroids.append(X[idx])\n    return np.array(centroids)\ndef kmeans(X: np.ndarray, K: int, max_iter: int=100, random_state: int | None=None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\"\"\"\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n    X = np.asarray(X, dtype=float)\n    (m, n) = X.shape\n    if K <= 0 or K > m:\n        raise ValueError('K must be a positive integer \u2264 number of samples.')\n    centroids = _kmeans_pp_init(X, K)\n    labels = np.full(m, -1, dtype=int)\n    for _ in range(max_iter):\n        distances = _euclidean_squared(X, centroids)\n        new_labels = distances.argmin(axis=1)\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n        for k in range(K):\n            mask = labels == k\n            if mask.any():\n                centroids[k] = X[mask].mean(axis=0)\n    centroids_rounded = [[round(float(v), 4) for v in centroid] for centroid in centroids]\n    centroids_sorted = sorted(centroids_rounded, key=lambda c: (c[0], c))\n    return centroids_sorted"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\nTIME_STEPS = 20\nPAD_TOKEN = 0\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"\n    Encode a raw string into a fixed-length list of integer ids.\n\n    \u2022 Each character is looked up in `vocabulary`.\n    \u2022 Unknown characters map to PAD_TOKEN (0).\n    \u2022 The returned list is exactly `time_steps` long:\n        \u2013 shorter input  \u2192 pad with 0 on the right\n        \u2013 longer  input  \u2192 truncate on the right\n    \"\"\"\n    encoded = [vocabulary.get(ch, PAD_TOKEN) for ch in text[:time_steps]]\n    if len(encoded) < time_steps:\n        encoded.extend([PAD_TOKEN] * (time_steps - len(encoded)))\n    return encoded\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"\n    Decode a sequence of integer ids back into a string.\n\n    \u2022 Works with Python lists, tuples or NumPy arrays.\n    \u2022 Padding / unknown tokens (id 0) are ignored.\n    \"\"\"\n    indices = np.ravel(indices)\n    chars = []\n    for idx in indices:\n        if idx == PAD_TOKEN:\n            continue\n        char = inverse_vocab.get(idx)\n        if char is not None:\n            chars.append(char)\n    return ''.join(chars)\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"\n    End-to-end helper:\n    1. encode text\n    2. feed the single example to `model.predict`\n    3. take arg-max over the last axis\n    4. decode the predicted ids back to text\n    \"\"\"\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    batch = np.array(encoded, dtype=np.int64)[np.newaxis, :]\n    logits = model.predict(batch)\n    pred_ids = np.argmax(logits, axis=-1)\n    decoded = int_to_string(pred_ids[0], inv_output_vocabulary)\n    return decoded"}
{"task_id": 113, "completion_id": 0, "solution": "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str] or None\n        A collection of input strings.  If *None*, the function uses the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        try:\n            examples = EXAMPLES\n        except NameError:\n            raise ValueError('No `examples` provided and the global constant `EXAMPLES` is not defined.')\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        predicted_str = ''.join(output_chars)\n        print(f'input:  {example}')\n        print(f'output: {predicted_str}')\n        predictions.append(predicted_str)\n    return predictions"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Binary cross-entropy loss and gradient for logistic regression.\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (m, n)\n    y : np.ndarray\n        Binary targets of shape (m,) or (m, 1)\n    w : np.ndarray\n        Weight vector of shape (n,) or (n, 1)\n    \n    Returns\n    -------\n    tuple\n        (loss, gradient) where\n          \u2022 loss is a float rounded to 4 decimals\n          \u2022 gradient is a nested list (shape (n, 1)) whose\n            elements are rounded to 4 decimals\n    \"\"\"\n    y = y.reshape(-1)\n    w = w.reshape(-1)\n    m = X.shape[0]\n    z = X @ w\n    p = 1 / (1 + np.exp(-z))\n    eps = 1e-20\n    p = np.clip(p, eps, 1 - eps)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    loss = float(np.round(loss, 4))\n    grad = X.T @ (p - y) / m\n    grad = grad.reshape(-1, 1)\n    grad = np.round(grad, 4).tolist()\n    return (loss, grad)"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array (m, n) with train features.\n        y_train: 1-D NumPy array (m,) with labels **-1** or **1**.\n        X_test : 2-D NumPy array (k, n) with test features.\n        n_clf  : Number of weak classifiers (\u22651).\n\n    Returns\n        Python list (length k) with predicted labels (-1 or 1).\n    \"\"\"\n\n    def _best_stump(X, y, w):\n        \"\"\"Find the decision stump that minimises weighted error.\"\"\"\n        (m, n_features) = X.shape\n        best_err = np.inf\n        stump = {}\n        best_pred = None\n        for j in range(n_features):\n            Xj = X[:, j]\n            uniq = np.unique(Xj)\n            if uniq.size == 1:\n                thresholds = [uniq[0] - 1e-10, uniq[0] + 1e-10]\n            else:\n                thresholds = list((uniq[:-1] + uniq[1:]) / 2.0)\n                thresholds.append(uniq[0] - 1e-10)\n                thresholds.append(uniq[-1] + 1e-10)\n            for thresh in thresholds:\n                for polarity in (1, -1):\n                    preds = np.ones(m, dtype=int)\n                    if polarity == 1:\n                        preds[Xj < thresh] = -1\n                    else:\n                        preds[Xj < thresh] = 1\n                    err = np.sum(w[preds != y])\n                    if err < best_err:\n                        best_err = err\n                        stump = {'feature': j, 'threshold': thresh, 'polarity': polarity}\n                        best_pred = preds\n        return (stump, best_err, best_pred)\n    if n_clf < 1:\n        n_clf = 1\n    y_train = y_train.astype(int)\n    m = X_train.shape[0]\n    weights = np.full(m, 1 / m, dtype=float)\n    stumps = []\n    eps = 1e-10\n    for _ in range(n_clf):\n        (stump, err, preds) = _best_stump(X_train, y_train, weights)\n        err = np.clip(err, eps, 1 - eps)\n        alpha = 0.5 * np.log((1 - err) / err)\n        weights *= np.exp(-alpha * y_train * preds)\n        weights /= weights.sum()\n        stump['alpha'] = alpha\n        stumps.append(stump)\n    k = X_test.shape[0]\n    agg = np.zeros(k)\n    for stump in stumps:\n        j = stump['feature']\n        thresh = stump['threshold']\n        pol = stump['polarity']\n        pred = np.ones(k, dtype=int)\n        if pol == 1:\n            pred[X_test[:, j] < thresh] = -1\n        else:\n            pred[X_test[:, j] < thresh] = 1\n        agg += stump['alpha'] * pred\n    final_pred = np.sign(agg)\n    final_pred[final_pred == 0] = 1\n    return final_pred.astype(int).tolist()"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Fisher LDA and project the data.\"\"\"\n    if n_components == 0:\n        return []\n    (n_samples, n_features) = X.shape\n    classes = np.unique(y)\n    n_classes = classes.size\n    global_mean = X.mean(axis=0)\n    S_W = np.zeros((n_features, n_features), dtype=float)\n    S_B = np.zeros_like(S_W)\n    for cls in classes:\n        X_c = X[y == cls]\n        n_c = X_c.shape[0]\n        mean_c = X_c.mean(axis=0)\n        diff = X_c - mean_c\n        S_W += diff.T @ diff\n        mean_diff = (mean_c - global_mean).reshape(-1, 1)\n        S_B += n_c * (mean_diff @ mean_diff.T)\n    A = np.linalg.pinv(S_W) @ S_B\n    (eig_vals, eig_vecs) = np.linalg.eigh(A)\n    sorted_idx = np.argsort(eig_vals)[::-1]\n    eig_vecs = eig_vecs[:, sorted_idx]\n    eig_vecs = eig_vecs[:, :n_components]\n    for i in range(eig_vecs.shape[1]):\n        vec = eig_vecs[:, i]\n        for v in vec:\n            if np.abs(v) > 1e-12:\n                if v < 0:\n                    eig_vecs[:, i] = -vec\n                break\n    projected = X @ eig_vecs\n    projected = np.round(projected, 4)\n    return projected.tolist()"}
{"task_id": 140, "completion_id": 0, "solution": "from collections import deque\nimport math\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Return the minimum number of minutes required for the agent to reach\n    the lower-right corner of a square maze that contains spreading fire.\n\n    The fire spreads first every minute, then the agent moves.  The agent may\n    enter a cell only if that cell is not burning at the moment he arrives.\n\n    Parameters\n    ----------\n    grid : list[str]\n        An n\u00d7n list of strings consisting only of '.', '#', 'F'.\n\n    Returns\n    -------\n    int\n        The minimum time to reach the goal, or \u20111 if it is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 0:\n        return -1\n    INF = math.inf\n    fire_time = [[INF] * n for _ in range(n)]\n    q_fire = deque()\n    for r in range(n):\n        for (c, ch) in enumerate(grid[r]):\n            if ch == 'F':\n                fire_time[r][c] = 0\n                q_fire.append((r, c))\n    dirs = ((1, 0), (-1, 0), (0, 1), (0, -1))\n    while q_fire:\n        (r, c) = q_fire.popleft()\n        for (dr, dc) in dirs:\n            (nr, nc) = (r + dr, c + dc)\n            if 0 <= nr < n and 0 <= nc < n and (grid[nr][nc] != '#'):\n                if fire_time[nr][nc] == INF:\n                    fire_time[nr][nc] = fire_time[r][c] + 1\n                    q_fire.append((nr, nc))\n\n    def burning_at_start(cell_r, cell_c) -> bool:\n        \"\"\"True if that cell is already burning at t = 0.\"\"\"\n        return fire_time[cell_r][cell_c] == 0\n    if grid[0][0] == '#' or grid[n - 1][n - 1] == '#' or burning_at_start(0, 0) or burning_at_start(n - 1, n - 1):\n        return -1\n    if n == 1:\n        return 0\n    q = deque([(0, 0, 0)])\n    best_arrival = [[INF] * n for _ in range(n)]\n    best_arrival[0][0] = 0\n    while q:\n        (r, c, t) = q.popleft()\n        for (dr, dc) in dirs:\n            (nr, nc) = (r + dr, c + dc)\n            if 0 <= nr < n and 0 <= nc < n and (grid[nr][nc] != '#'):\n                next_t = t + 1\n                if next_t >= fire_time[nr][nc]:\n                    continue\n                if next_t < best_arrival[nr][nc]:\n                    best_arrival[nr][nc] = next_t\n                    if nr == n - 1 and nc == n - 1:\n                        return next_t\n                    q.append((nr, nc, next_t))\n    return -1"}
{"task_id": 141, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int, metric: str='euclidean') -> np.ndarray:\n    \"\"\"\n    k\u2013Nearest Neighbours classifier.\n    \n    Parameters\n    ----------\n    X_train : (n_train, n_features) ndarray\n        Training samples.\n    y_train : (n_train,) ndarray\n        Labels of the training samples.\n    X_test  : (n_test,  n_features) ndarray\n        Samples to classify.\n    k       : int\n        Number of neighbours to use (if k > n_train, k == n_train).\n    metric  : {'euclidean', 'manhattan', 'cosine'}, optional\n        Distance metric. Anything else falls back to 'euclidean'.\n    \n    Returns\n    -------\n    preds : (n_test,) ndarray\n        Predicted labels for X_test.\n    \"\"\"\n    if k <= 0:\n        raise ValueError('k must be a positive integer.')\n    metric = metric.lower()\n    n_train = X_train.shape[0]\n    k = min(k, n_train)\n    if metric == 'manhattan':\n        D = np.abs(X_test[:, None, :] - X_train[None, :, :]).sum(axis=2)\n    elif metric == 'cosine':\n        eps = 1e-10\n        t_norm = np.linalg.norm(X_test, axis=1, keepdims=True) + eps\n        r_norm = np.linalg.norm(X_train, axis=1) + eps\n        sim = X_test @ X_train.T / (t_norm * r_norm)\n        D = 1.0 - sim\n    else:\n        diff = X_test[:, None, :] - X_train[None, :, :]\n        D = np.sum(diff ** 2, axis=2)\n    nn_idx = np.argpartition(D, kth=k - 1, axis=1)[:, :k]\n    preds = np.empty(X_test.shape[0], dtype=y_train.dtype)\n    for (i, neighbours) in enumerate(nn_idx):\n        labels = y_train[neighbours]\n        (uniq, counts) = np.unique(labels, return_counts=True)\n        majority = uniq[counts == counts.max()]\n        preds[i] = majority.min()\n    return preds"}
{"task_id": 146, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X: np.ndarray, y: np.ndarray, X_test: np.ndarray, k: int=3, metric: str='euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\"\"\"\n    if X.ndim != 2 or X_test.ndim != 2:\n        raise ValueError('X and X_test must be 2-D arrays')\n    if len(X) != len(y):\n        raise ValueError('X and y must contain the same number of samples')\n    if k <= 0:\n        raise ValueError('k must be a positive integer')\n    metric = metric.lower()\n    n_train = X.shape[0]\n    n_test = X_test.shape[0]\n    k = min(k, n_train)\n    if metric == 'euclidean':\n        diff = X_test[:, None, :] - X[None, :, :]\n        dists = np.linalg.norm(diff, axis=2)\n    elif metric == 'manhattan':\n        diff = X_test[:, None, :] - X[None, :, :]\n        dists = np.abs(diff).sum(axis=2)\n    elif metric == 'cosine':\n        eps = 1e-12\n        x_norm = np.linalg.norm(X, axis=1) + eps\n        xt_norm = np.linalg.norm(X_test, axis=1) + eps\n        dot_prod = X_test @ X.T\n        denom = np.outer(xt_norm, x_norm)\n        dists = 1.0 - dot_prod / denom\n    else:\n        raise ValueError(\"metric must be 'euclidean', 'manhattan', or 'cosine'\")\n    predictions = []\n    for i in range(n_test):\n        idx = np.argpartition(dists[i], k - 1)[:k]\n        neigh_y = y[idx]\n        (unique, counts) = np.unique(neigh_y, return_counts=True)\n        max_count = counts.max()\n        winners = unique[counts == max_count]\n        pred = winners.min()\n        predictions.append(pred)\n    return predictions"}
{"task_id": 155, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Forward pass of a two-hidden-layer actor network (Linear \u2192 ReLU \u2192\n    Linear \u2192 ReLU \u2192 Linear \u2192 tanh).\n\n    Args:\n        state (list[float] | np.ndarray): 1-D state vector.\n        weights (dict): Dictionary containing the NumPy arrays\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector, each element rounded to four decimals.\n    \"\"\"\n    x = np.asarray(state, dtype=float)\n    (W1, b1) = (weights['W1'], weights['b1'])\n    (W2, b2) = (weights['W2'], weights['b2'])\n    (W3, b3) = (weights['W3'], weights['b3'])\n    z1 = x @ W1 + b1\n    a1 = np.maximum(0, z1)\n    z2 = a1 @ W2 + b2\n    a2 = np.maximum(0, z2)\n    z3 = a2 @ W3 + b3\n    actions = np.tanh(z3)\n    actions_rounded = np.round(actions, 4)\n    if actions_rounded.shape == ():\n        return [float(actions_rounded)]\n    return actions_rounded.tolist()"}
{"task_id": 160, "completion_id": 0, "solution": "import numpy as np\ndef _soft_threshold(a: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S(a, \u03c4).\"\"\"\n    if a > tau:\n        return a - tau\n    if a < -tau:\n        return a + tau\n    return 0.0\ndef fit_lasso(X: np.ndarray, y: np.ndarray, lambda_param: float=1.0, max_iters: int=100, fit_intercept: bool=True) -> tuple[list[float], float]:\n    \"\"\"Lasso (L1) linear regression fitted with coordinate descent.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    (m, n) = X.shape\n    intercept_col = np.ones((m, 1)) if fit_intercept else np.zeros((m, 1))\n    XA = np.hstack((intercept_col, X))\n    p = n + 1\n    w = np.zeros(p)\n    col_sq_norms = (XA ** 2).sum(axis=0)\n    for _ in range(max_iters):\n        if fit_intercept:\n            y_hat_wo_b = XA[:, 1:].dot(w[1:])\n            w[0] = (y - y_hat_wo_b).mean()\n        for j in range(1, p):\n            x_j = XA[:, j]\n            r_j = y - XA.dot(w) + w[j] * x_j\n            rho = x_j.dot(r_j)\n            w[j] = _soft_threshold(rho, lambda_param * m) / col_sq_norms[j]\n    weights = np.round(w[1:], 4).tolist()\n    bias = round(float(w[0]), 4)\n    return (weights, bias)"}
{"task_id": 165, "completion_id": 0, "solution": "import numpy as np\ndef hamming_distance(x: list[int] | 'np.ndarray', y: list[int] | 'np.ndarray') -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length vectors.\n\n    Args:\n        x: 1-D list or NumPy array of integers.\n        y: 1-D list or NumPy array of integers.\n\n    Returns:\n        Float rounded to 4 decimal places representing the distance, or \u20111\n        if the vectors have different lengths.\n    \"\"\"\n    x_arr = np.asarray(x)\n    y_arr = np.asarray(y)\n    if x_arr.ndim != 1 or y_arr.ndim != 1:\n        raise ValueError('Input vectors must be 1-D.')\n    if x_arr.size != y_arr.size:\n        return -1\n    N = x_arr.size\n    if N == 0:\n        return 0.0\n    diff = np.count_nonzero(x_arr != y_arr)\n    distance = diff / N\n    return round(float(distance), 4)"}
{"task_id": 169, "completion_id": 0, "solution": "import numpy as np\ndef generate_window(window: str, N: int, coefficients: list[float] | None=None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Window name: \"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate (must be > 0).\n    coefficients : list[float] | None, optional\n        Cosine-series coefficients a\u2080\u2026a_M, required only when\n        window == \"generalized_cosine\".\n\n    Returns\n    -------\n    list[float]\n        List with N floats rounded to 4 decimal places.\n    \"\"\"\n    if not isinstance(N, int) or N <= 0:\n        raise ValueError('N must be a positive integer.')\n    if N == 1:\n        return [1.0]\n    name = window.lower()\n    if name not in {'hamming', 'hann', 'blackman_harris', 'generalized_cosine'}:\n        raise ValueError(f'Unknown window type: {window!r}')\n    n = np.arange(N)\n    denom = N - 1\n    two_pi_term = 2.0 * np.pi * n / denom\n    if name == 'hamming':\n        w = 0.54 - 0.46 * np.cos(two_pi_term)\n    elif name == 'hann':\n        w = 0.5 - 0.5 * np.cos(two_pi_term)\n    elif name == 'blackman_harris':\n        (a0, a1, a2, a3) = (0.35875, 0.48829, 0.14128, 0.01168)\n        w = a0 - a1 * np.cos(two_pi_term) + a2 * np.cos(2 * two_pi_term) - a3 * np.cos(3 * two_pi_term)\n    elif name == 'generalized_cosine':\n        if not coefficients:\n            raise ValueError(\"For 'generalized_cosine' you must supply a non-empty 'coefficients' list.\")\n        w = np.zeros(N, dtype=float)\n        for (k, ak) in enumerate(coefficients):\n            w += ak * np.cos(2.0 * np.pi * k * n / denom)\n    return [round(float(v), 4) for v in w]"}
{"task_id": 171, "completion_id": 0, "solution": "import numpy as np\ndef logistic_regression_train_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], epochs: int=5000, learning_rate: float=0.1, batch_size: int=32) -> list[int]:\n    \"\"\"Binary Logistic-Regression trained with mini-batch Gradient Descent.\"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train)\n    uniq = np.unique(y_train)\n    if len(uniq) == 1:\n        return [int(uniq[0])] * len(X_test)\n    if len(uniq) != 2:\n        raise ValueError('Exactly two distinct labels are required for training.')\n    pos_label = uniq[1]\n    y_bin = (y_train == pos_label).astype(float)\n    ones_train = np.ones((X_train.shape[0], 1))\n    ones_test = np.ones((X_test.shape[0], 1))\n    X_train = np.hstack([ones_train, X_train])\n    X_test_aug = np.hstack([ones_test, X_test])\n    (n_samples, n_features) = X_train.shape\n    w = np.zeros(n_features)\n\n    def sigmoid(z):\n        z = np.clip(z, -500, 500)\n        return 1.0 / (1.0 + np.exp(-z))\n    for _ in range(epochs):\n        permutation = np.random.permutation(n_samples)\n        for start in range(0, n_samples, batch_size):\n            idx = permutation[start:start + batch_size]\n            X_b = X_train[idx]\n            y_b = y_bin[idx]\n            preds = sigmoid(X_b @ w)\n            grad = X_b.T @ (preds - y_b) / len(idx)\n            w -= learning_rate * grad\n    probs = sigmoid(X_test_aug @ w)\n    preds_bin = (probs >= 0.5).astype(int)\n    label_map = {0: int(uniq[0]), 1: int(uniq[1])}\n    preds_original = [label_map[int(p)] for p in preds_bin]\n    return preds_original"}
{"task_id": 176, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: list[list[int | float]], y_train: list[int], X_test: list[list[int | float]], n_estimators: int=10) -> list[int]:\n    \"\"\"AdaBoost (SAMME-R) with decision stumps, implemented from scratch.\n\n    Parameters\n    ----------\n    X_train : list of list of float\n        Training features.\n    y_train : list of int (0/1)\n        Training labels, 0 = negative class, 1 = positive class.\n    X_test  : list of list of float\n        Samples whose labels must be predicted.\n    n_estimators : int, default=10\n        Maximum number of boosting rounds.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (0/1) for every row of *X_test*.\n    \"\"\"\n\n    def train_best_stump(X: np.ndarray, y: np.ndarray, w: np.ndarray):\n        \"\"\"\n        Exhaustively search the decision-stump with the smallest\n        *weighted* classification error.  Tie-breaking is deterministic:\n            1) smallest error\n            2) smallest feature index\n            3) smallest threshold\n            4) polarity  +1 before \u20131\n        \"\"\"\n        (n_samples, n_features) = X.shape\n        best = {'feature': None, 'threshold': None, 'polarity': None, 'pred': None, 'error': np.inf}\n        for j in range(n_features):\n            column = X[:, j]\n            thresholds = np.unique(column)\n            for thr in thresholds:\n                for pol in (1, -1):\n                    if pol == 1:\n                        h = np.where(column < thr, 1, -1)\n                    else:\n                        h = np.where(column >= thr, 1, -1)\n                    err = np.sum(w * (h != y))\n                    if err < best['error'] - 1e-12:\n                        best.update(feature=j, threshold=float(thr), polarity=pol, pred=h, error=err)\n        return best\n\n    def stump_predict(stump, X):\n        \"\"\"Vectorised prediction of a single stump on data matrix X.\"\"\"\n        col = X[:, stump['feature']]\n        if stump['polarity'] == 1:\n            return np.where(col < stump['threshold'], 1, -1)\n        else:\n            return np.where(col >= stump['threshold'], 1, -1)\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=int)\n    y = 2 * y - 1\n    n_samples = X.shape[0]\n    w = np.full(n_samples, 1.0 / n_samples)\n    stumps: list[dict] = []\n    alphas: list[float] = []\n    for _ in range(n_estimators):\n        stump = train_best_stump(X, y, w)\n        error = stump['error']\n        if error <= 1e-12:\n            alpha = 0.5 * np.log((1.0 - 1e-12) / 1e-12)\n            stumps.append(stump)\n            alphas.append(alpha)\n            break\n        alpha = 0.5 * np.log((1.0 - error) / (error + 1e-10))\n        w *= np.exp(-alpha * y * stump['pred'])\n        w /= np.sum(w)\n        stumps.append(stump)\n        alphas.append(alpha)\n    X_te = np.asarray(X_test, dtype=float)\n    agg = np.zeros(X_te.shape[0], dtype=float)\n    for (stump, alpha) in zip(stumps, alphas):\n        agg += alpha * stump_predict(stump, X_te)\n    preds = (agg >= 0).astype(int).tolist()\n    return preds"}
{"task_id": 178, "completion_id": 0, "solution": "import numpy as np\ndef particle_swarm_optimisation(n_dims: int, num_particles: int, num_iterations: int, seed: int=1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function with Particle Swarm Optimisation.\n\n    Args:\n        n_dims:            number of decision variables  (>0)\n        num_particles:     swarm size                   (>0)\n        num_iterations:    optimisation iterations      (>0)\n        seed:              RNG seed (default 1)\n\n    Returns\n        Best objective value found, rounded to 4 decimals, or \u20131 on bad input.\n    \"\"\"\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n    rng = np.random.default_rng(seed)\n    (w, c1, c2) = (0.5, 1.5, 1.5)\n    (lo, hi) = (-1.0, 1.0)\n    positions = rng.uniform(lo, hi, size=(num_particles, n_dims))\n    velocities = rng.uniform(lo, hi, size=(num_particles, n_dims))\n    pbest_pos = positions.copy()\n    pbest_val = np.sum(pbest_pos ** 2, axis=1)\n    gbest_idx = np.argmin(pbest_val)\n    gbest_pos = pbest_pos[gbest_idx].copy()\n    gbest_val = pbest_val[gbest_idx]\n    for _ in range(num_iterations):\n        r1 = rng.random(size=(num_particles, n_dims))\n        r2 = rng.random(size=(num_particles, n_dims))\n        velocities = w * velocities + c1 * r1 * (pbest_pos - positions) + c2 * r2 * (gbest_pos - positions)\n        positions = np.clip(positions + velocities, lo, hi)\n        curr_val = np.sum(positions ** 2, axis=1)\n        improved = curr_val < pbest_val\n        pbest_pos[improved] = positions[improved]\n        pbest_val[improved] = curr_val[improved]\n        gbest_idx = np.argmin(pbest_val)\n        gbest_pos = pbest_pos[gbest_idx].copy()\n        gbest_val = pbest_val[gbest_idx]\n    return round(float(gbest_val), 4)"}
{"task_id": 180, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef knn_predict(X_train: list[list[float]], y_train: list, X_test: list[list[float]], k: int=5, task: str='classification') -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\"\"\"\n    X_tr = np.asarray(X_train, dtype=float)\n    X_te = np.asarray(X_test, dtype=float)\n    y_tr = np.asarray(y_train, dtype=object)\n    n_train = X_tr.shape[0]\n    if k <= 0 or k > n_train:\n        k = n_train\n    predictions: list = []\n    for x in X_te:\n        dists = np.linalg.norm(X_tr - x, axis=1)\n        if k == n_train:\n            nn_idx = np.arange(n_train)\n        else:\n            nn_idx = np.argpartition(dists, k - 1)[:k]\n        neighbours = y_tr[nn_idx]\n        if task == 'regression':\n            mean_val = float(np.mean(neighbours.astype(float)))\n            predictions.append(round(mean_val, 4))\n        else:\n            counts = Counter(neighbours)\n            max_cnt = max(counts.values())\n            best_label = min((lbl for (lbl, cnt) in counts.items() if cnt == max_cnt))\n            predictions.append(best_label)\n    return predictions"}
{"task_id": 184, "completion_id": 0, "solution": "import numpy as np\ndef decision_tree_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_depth: int | None=None) -> list[int]:\n    \"\"\"CART-style binary decision tree (Gini) completely from scratch.\"\"\"\n\n    class Node:\n        __slots__ = ('is_leaf', 'prediction', 'feature', 'threshold', 'left', 'right')\n\n        def __init__(self, *, is_leaf: bool, prediction: int | None=None, feature: int | None=None, threshold: float | None=None, left: 'Node | None'=None, right: 'Node | None'=None):\n            self.is_leaf = is_leaf\n            self.prediction = prediction\n            self.feature = feature\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n    y_train = y_train.astype(int, copy=False)\n    n_classes = int(y_train.max()) + 1\n\n    def gini(counts: np.ndarray) -> float:\n        \"\"\"Gini impurity from class counts.\"\"\"\n        total = counts.sum()\n        if total == 0:\n            return 0.0\n        probs = counts / total\n        return 1.0 - np.sum(probs ** 2)\n\n    def majority_class(counts: np.ndarray) -> int:\n        \"\"\"Smallest label in case of ties keeps result deterministic.\"\"\"\n        return int(np.argmax(counts))\n\n    def find_best_split(indices: np.ndarray) -> tuple[int, float, float, np.ndarray, np.ndarray] | None:\n        \"\"\"\n        Returns (best_feature, best_threshold, best_gain, left_idx, right_idx)\n        or None if no split brings positive gain.\n        \"\"\"\n        X_node = X_train[indices]\n        y_node = y_train[indices]\n        parent_counts = np.bincount(y_node, minlength=n_classes)\n        parent_gini = gini(parent_counts)\n        best_gain = 0.0\n        best_feature = -1\n        best_threshold = 0.0\n        best_left_idx = best_right_idx = None\n        (n_samples, n_features) = X_node.shape\n        for feat in range(n_features):\n            values = X_node[:, feat]\n            order = np.argsort(values)\n            sorted_y = y_node[order]\n            sorted_vals = values[order]\n            left_counts = np.zeros(n_classes, dtype=int)\n            right_counts = parent_counts.copy()\n            for i in range(1, n_samples):\n                cls = sorted_y[i - 1]\n                left_counts[cls] += 1\n                right_counts[cls] -= 1\n                if sorted_vals[i] == sorted_vals[i - 1]:\n                    continue\n                left_n = i\n                right_n = n_samples - i\n                g_left = gini(left_counts)\n                g_right = gini(right_counts)\n                weighted = (left_n * g_left + right_n * g_right) / n_samples\n                gain = parent_gini - weighted\n                if gain > best_gain + 1e-12:\n                    best_gain = gain\n                    best_feature = feat\n                    best_threshold = (sorted_vals[i] + sorted_vals[i - 1]) / 2.0\n                    mask = X_train[indices, feat] <= best_threshold\n                    best_left_idx = indices[mask]\n                    best_right_idx = indices[~mask]\n        if best_gain <= 0.0 or best_left_idx.size == 0 or best_right_idx.size == 0:\n            return None\n        return (best_feature, best_threshold, best_gain, best_left_idx, best_right_idx)\n\n    def build(indices: np.ndarray, depth: int) -> Node:\n        y_subset = y_train[indices]\n        counts = np.bincount(y_subset, minlength=n_classes)\n        if np.unique(y_subset).size == 1:\n            return Node(is_leaf=True, prediction=int(y_subset[0]))\n        if max_depth is not None and depth >= max_depth:\n            return Node(is_leaf=True, prediction=majority_class(counts))\n        split = find_best_split(indices)\n        if split is None:\n            return Node(is_leaf=True, prediction=majority_class(counts))\n        (feat, thr, _, left_idx, right_idx) = split\n        left_child = build(left_idx, depth + 1)\n        right_child = build(right_idx, depth + 1)\n        return Node(is_leaf=False, feature=feat, threshold=thr, left=left_child, right=right_child)\n    root = build(np.arange(X_train.shape[0]), depth=0)\n\n    def predict_single(x: np.ndarray) -> int:\n        node = root\n        while not node.is_leaf:\n            node = node.left if x[node.feature] <= node.threshold else node.right\n        return node.prediction\n    return [predict_single(row) for row in X_test]"}
{"task_id": 190, "completion_id": 0, "solution": "import numpy as np\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted\n    Gini impurity for a single binary split.\n\n    Parameters\n    ----------\n    X : list[list[float]] | np.ndarray  (n_samples \u00d7 n_features)\n    y : list[int] | np.ndarray          (n_samples,)\n\n    Returns\n    -------\n    (int, float | None, float)\n        (best_feature_index, best_threshold_value, best_gini_rounded)\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    if X.ndim != 2:\n        raise ValueError('X must be 2-dimensional')\n    if y.ndim != 1 or y.shape[0] != X.shape[0]:\n        raise ValueError('y must be 1-D and have the same length as X')\n    (n_samples, n_features) = X.shape\n    if n_samples == 0 or n_features == 0:\n        return (-1, None, 0.0)\n    (classes, y_enc) = np.unique(y, return_inverse=True)\n    n_classes = classes.size\n\n    def gini(counts, n):\n        if n == 0:\n            return 0.0\n        p = counts / n\n        return 1.0 - np.sum(p * p)\n    total_counts = np.bincount(y_enc, minlength=n_classes)\n    g_whole = gini(total_counts, n_samples)\n    best_gini = np.inf\n    best_feat = -1\n    best_th = None\n    tol = 1e-12\n    for j in range(n_features):\n        col = X[:, j]\n        if np.all(col == col[0]):\n            continue\n        order = np.argsort(col, kind='mergesort')\n        col_sorted = col[order]\n        y_sorted = y_enc[order]\n        left_counts = np.zeros(n_classes, dtype=int)\n        for i in range(n_samples):\n            cls = y_sorted[i]\n            left_counts[cls] += 1\n            is_last_occurrence = i == n_samples - 1 or col_sorted[i] != col_sorted[i + 1]\n            if not is_last_occurrence:\n                continue\n            n_left = i + 1\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:\n                continue\n            right_counts = total_counts - left_counts\n            g_left = gini(left_counts, n_left)\n            g_right = gini(right_counts, n_right)\n            g_split = n_left / n_samples * g_left + n_right / n_samples * g_right\n            if g_split + tol < best_gini:\n                (best_gini, best_feat, best_th) = (g_split, j, col_sorted[i])\n            elif abs(g_split - best_gini) < tol:\n                if j < best_feat:\n                    (best_feat, best_th) = (j, col_sorted[i])\n                elif j == best_feat and col_sorted[i] < best_th:\n                    best_th = col_sorted[i]\n    if best_feat == -1:\n        return (-1, None, round(g_whole, 4))\n    return (best_feat, best_th, round(best_gini, 4))"}
{"task_id": 191, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef metric(name: str):\n    \"\"\"Return a distance function with integrated validation.\"\"\"\n    supported = ('euclidean', 'manhattan', 'chebyshev', 'cosine')\n\n    def d(v1, v2):\n        if name not in supported:\n            return -1\n        if not isinstance(v1, (list, tuple, np.ndarray)):\n            return -1\n        if not isinstance(v2, (list, tuple, np.ndarray)):\n            return -1\n        try:\n            a = np.asarray(v1, dtype=float)\n            b = np.asarray(v2, dtype=float)\n        except Exception:\n            return -1\n        if a.ndim != 1 or b.ndim != 1:\n            return -1\n        if a.size == 0 or b.size == 0:\n            return -1\n        if a.size != b.size:\n            return -1\n        if name == 'euclidean':\n            value = np.linalg.norm(a - b)\n        elif name == 'manhattan':\n            value = np.sum(np.abs(a - b))\n        elif name == 'chebyshev':\n            value = np.max(np.abs(a - b))\n        else:\n            (na, nb) = (np.linalg.norm(a), np.linalg.norm(b))\n            if na == 0 or nb == 0:\n                return -1\n            value = 1 - a @ b / (na * nb)\n        return round(float(value), 4)\n    return d"}
{"task_id": 197, "completion_id": 0, "solution": "import math\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\nimport numpy as np\ndef _is_discrete(space: Any) -> bool:\n    \"\"\"Utility: does the sub-space describe a discrete set?\"\"\"\n    return hasattr(space, 'n')\ndef _is_continuous(space: Any) -> bool:\n    \"\"\"Utility: does the sub-space describe a continuous set?\"\"\"\n    return hasattr(space, 'shape')\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"\n    Summarise an RL environment's action space.\n\n    Parameters\n    ----------\n    env : Any\n        Environment exposing an `action_space` attribute \u00e0-la OpenAI-Gym.\n    md_action : bool\n        Ignored by the logic (kept for the required signature) \u2013 the function\n        determines the true structure directly from `env.action_space`.\n    cont_action : bool\n        Ignored by the logic (kept for the required signature) \u2013 the function\n        determines the true structure directly from `env.action_space`.\n\n    Returns\n    -------\n    Tuple containing\n        n_actions_per_dim : list[int | float]\n            Number of distinct actions for every dimension\n            (`np.inf` for continuous ones).\n        action_ids : list[tuple[int, ...]] | None\n            All possible discrete actions as tuples; `None` if at least one\n            dimension is continuous.\n        action_dim : int\n            Number of action dimensions.\n    \"\"\"\n    space = env.action_space\n    n_actions_per_dim: List[Union[int, float]] = []\n    if hasattr(space, 'spaces'):\n        subspaces = space.spaces\n        for sub in subspaces:\n            if _is_discrete(sub):\n                n_actions_per_dim.append(int(sub.n))\n            elif _is_continuous(sub):\n                n_actions_per_dim.append(np.inf)\n            else:\n                n_actions_per_dim.append(np.inf)\n        action_dim = len(subspaces)\n    elif _is_discrete(space):\n        n_actions_per_dim.append(int(space.n))\n        action_dim = 1\n    elif _is_continuous(space):\n        dim = int(space.shape[0]) if len(space.shape) > 0 else 1\n        n_actions_per_dim.extend([np.inf] * dim)\n        action_dim = dim\n    else:\n        n_actions_per_dim.append(np.inf)\n        action_dim = 1\n    contains_continuous = any((np.isinf(x) for x in n_actions_per_dim))\n    if contains_continuous:\n        action_ids = None\n    else:\n        ranges = [range(int(n)) for n in n_actions_per_dim]\n        action_ids = [tuple(p) for p in product(*ranges)]\n    return (n_actions_per_dim, action_ids, action_dim)"}
{"task_id": 198, "completion_id": 0, "solution": "import numpy as np\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        One array per document. Array of document *d* has shape (N_d, T)\n        and contains the current variational parameter \u03d5 of this document.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the index of the *n*-th token\n        in document *d*.\n    V : int\n        Size of the vocabulary.\n\n    Returns\n    -------\n    list[list[float]]\n        (V \u00d7 T) matrix with the updated word\u2013topic probabilities.\n        Each column sums to one (\u2264 1e-4 rounding error) and every\n        number is rounded to four decimals.\n    \"\"\"\n    if not phi:\n        return [[0.0] * 0 for _ in range(V)]\n    T = phi[0].shape[1]\n    counts = np.zeros((V, T), dtype=np.float64)\n    for (doc_phi, doc_words) in zip(phi, corpus):\n        words = np.asarray(doc_words, dtype=np.intp)\n        np.add.at(counts, words, doc_phi)\n    seen_words_mask = counts.sum(axis=1) > 0\n    col_sums = counts.sum(axis=0)\n    beta = np.zeros_like(counts)\n    non_zero_cols = col_sums > 0\n    beta[:, non_zero_cols] = counts[:, non_zero_cols] / col_sums[non_zero_cols]\n    zero_cols = ~non_zero_cols\n    if zero_cols.any():\n        observed_idx = np.where(seen_words_mask)[0]\n        if observed_idx.size == 0:\n            beta[:, zero_cols] = 1.0 / V\n        else:\n            beta[observed_idx[:, None], zero_cols] = 1.0 / observed_idx.size\n    beta = np.round(beta, 4)\n    for t in range(T):\n        diff = 1.0 - beta[:, t].sum()\n        if abs(diff) > 0.0001:\n            col = beta[:, t]\n            idx = int(np.argmax(col))\n            adj = np.round(diff, 4)\n            new_val = np.round(col[idx] + adj, 4)\n            beta[idx, t] = max(new_val, 0.0)\n    return beta.tolist()"}
{"task_id": 202, "completion_id": 0, "solution": "import numpy as np\ndef _total_cost(dist_matrix: np.ndarray, medoid_idx: list[int]) -> float:\n    \"\"\"\n    Return the sum of distances of every sample to its nearest medoid.\n    \"\"\"\n    d_to_medoids = dist_matrix[:, medoid_idx]\n    return np.min(d_to_medoids, axis=1).sum()\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Input data.\n    k : int\n        Number of desired clusters (1 \u2264 k \u2264 n_samples).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels (0 \u2026 k-1) for every sample.\n    \"\"\"\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D NumPy array.')\n    n_samples = X.shape[0]\n    if not 1 <= k <= n_samples:\n        raise ValueError('k must satisfy 1 \u2264 k \u2264 n_samples')\n    diff = X[:, None, :] - X[None, :, :]\n    dist_matrix = np.linalg.norm(diff, axis=-1)\n    medoid_idx: list[int] = list(range(k))\n    current_cost = _total_cost(dist_matrix, medoid_idx)\n    improved = True\n    while improved:\n        best_swap = None\n        best_reduction = 0.0\n        non_medoid_idx = [i for i in range(n_samples) if i not in medoid_idx]\n        for (p_pos, p) in enumerate(medoid_idx):\n            for h in non_medoid_idx:\n                candidate_medoid_idx = medoid_idx.copy()\n                candidate_medoid_idx[p_pos] = h\n                cand_cost = _total_cost(dist_matrix, candidate_medoid_idx)\n                reduction = current_cost - cand_cost\n                if reduction > best_reduction:\n                    best_reduction = reduction\n                    best_swap = (p_pos, h)\n        if best_swap is not None:\n            (p_pos, h) = best_swap\n            medoid_idx[p_pos] = h\n            current_cost -= best_reduction\n        else:\n            improved = False\n    final_dists = dist_matrix[:, medoid_idx]\n    labels = final_dists.argmin(axis=1).tolist()\n    return labels"}
{"task_id": 216, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic-Net linear regression trained with batch gradient descent.\"\"\"\n    try:\n        X_tr = np.asarray(x_train, dtype=float)\n        X_te = np.asarray(x_test, dtype=float)\n        y_tr = np.asarray(y_train, dtype=float).ravel()\n    except Exception:\n        return -1\n    if X_tr.ndim == 1:\n        X_tr = X_tr.reshape(-1, 1)\n    if X_te.ndim == 1:\n        X_te = X_te.reshape(-1, 1)\n    if X_tr.shape[1] != X_te.shape[1] or n_iterations < 1:\n        return -1\n\n    def poly_expansion(matrix, deg):\n        if deg <= 1:\n            return matrix.copy()\n        powers = [matrix ** p for p in range(1, deg + 1)]\n        return np.concatenate(powers, axis=1)\n    X_tr_poly = poly_expansion(X_tr, degree)\n    X_te_poly = poly_expansion(X_te, degree)\n    mu = X_tr_poly.mean(axis=0)\n    sigma = X_tr_poly.std(axis=0)\n    sigma[sigma == 0] = 1.0\n    X_tr_std = (X_tr_poly - mu) / sigma\n    X_te_std = (X_te_poly - mu) / sigma\n    X_tr_std = np.hstack((np.ones((X_tr_std.shape[0], 1)), X_tr_std))\n    X_te_std = np.hstack((np.ones((X_te_std.shape[0], 1)), X_te_std))\n    (n_samples, n_features) = X_tr_std.shape\n    w = np.zeros(n_features)\n    l2_factor = reg_factor * (1.0 - l1_ratio)\n    l1_factor = reg_factor * l1_ratio\n    for _ in range(int(n_iterations)):\n        y_hat = X_tr_std @ w\n        error = y_hat - y_tr\n        grad = X_tr_std.T @ error / n_samples\n        grad[1:] += l2_factor * w[1:]\n        grad[1:] += l1_factor * np.sign(w[1:])\n        w -= learning_rate * grad\n    y_pred = X_te_std @ w\n    return np.round(y_pred, 4).tolist()"}
{"task_id": 217, "completion_id": 0, "solution": "import numpy as np\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    Args:\n        actual    : 1-D container (list/tuple/np.ndarray) with elements +1 or \u22121.\n        predicted : 1-D container (list/tuple/np.ndarray) with raw model scores.\n\n    Returns:\n        (grad_list, hess_list, prob_list) \u2013 three ordinary Python lists,\n        each value rounded to exactly 6 decimal places.\n    \"\"\"\n    y = np.asarray(actual, dtype=float)\n    z = np.asarray(predicted, dtype=float)\n    if y.shape != z.shape:\n        raise ValueError('`actual` and `predicted` must have the same length.')\n\n    def _sigmoid(x):\n        out = np.empty_like(x, dtype=float)\n        pos_mask = x >= 0\n        neg_mask = ~pos_mask\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n        exp_x = np.exp(x[neg_mask])\n        out[neg_mask] = exp_x / (1.0 + exp_x)\n        return out\n    prob = _sigmoid(z)\n    grad = y * _sigmoid(-y * z)\n    hess = prob * (1.0 - prob)\n    prob = np.round(prob, 6).tolist()\n    grad = np.round(grad, 6).tolist()\n    hess = np.round(hess, 6).tolist()\n    return (grad, hess, prob)"}
{"task_id": 218, "completion_id": 0, "solution": "def blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n\n    def hand_score(hand: list[int]) -> int:\n        \"\"\"Return the best score (\u226421 if possible) for this hand.\"\"\"\n        s = sum(hand)\n        if 1 in hand and s + 10 <= 21:\n            return s + 10\n        return s\n\n    def is_bust(hand: list[int]) -> bool:\n        \"\"\"True if the hand is bust (>21).\"\"\"\n        return hand_score(hand) > 21\n\n    def is_natural(hand: list[int]) -> bool:\n        \"\"\"True if the hand is a natural blackjack (exactly two cards: Ace + 10).\"\"\"\n        return len(hand) == 2 and 1 in hand and (10 in hand)\n    player_score = hand_score(player)\n    dealer_score = hand_score(dealer)\n    player_bust = player_score > 21\n    dealer_bust = dealer_score > 21\n    player_natural = is_natural(player)\n    if player_bust:\n        return -1.0\n    if dealer_bust:\n        return 1.5 if player_natural else 1.0\n    if player_score > dealer_score:\n        return 1.5 if player_natural else 1.0\n    if player_score < dealer_score:\n        return -1.0\n    return 0.0"}
{"task_id": 221, "completion_id": 0, "solution": "import numpy as np\ndef squared_log_error(actual, predicted):\n    \"\"\"\n    Element-wise squared logarithmic error.\n\n    Parameters\n    ----------\n    actual, predicted : np.ndarray\n        Arrays of the same shape containing non-negative numbers.\n\n    Returns\n    -------\n    np.ndarray\n        (log1p(actual) - log1p(predicted))**2 computed element-wise.\n    \"\"\"\n    return np.square(np.log1p(actual) - np.log1p(predicted))\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        True target values (must be non-negative).\n    predicted : list | tuple | np.ndarray\n        Predicted values (must be non-negative and same shape as `actual`).\n\n    Returns\n    -------\n    float\n        MSLE rounded to 4 decimal places, or -1 if the inputs are invalid.\n    \"\"\"\n    try:\n        actual_arr = np.asarray(actual, dtype=float)\n        pred_arr = np.asarray(predicted, dtype=float)\n    except Exception:\n        return -1\n    if actual_arr.shape != pred_arr.shape:\n        return -1\n    if actual_arr.size == 0:\n        return -1\n    if np.isnan(actual_arr).any() or np.isnan(pred_arr).any():\n        return -1\n    if np.any(actual_arr < 0) or np.any(pred_arr < 0):\n        return -1\n    sq_log_err = squared_log_error(actual_arr, pred_arr)\n    msle = float(np.mean(sq_log_err))\n    return round(msle, 4)"}
{"task_id": 222, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function works for scalar numbers, Python lists and NumPy arrays and\n    remains numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: If `z` is a scalar, the sigmoid value rounded to 4\n        decimals (float).  Otherwise, a Python list with the same nested\n        structure where every element is rounded to 4 decimals.\n    \"\"\"\n    if np.isscalar(z):\n        z = float(z)\n        if z >= 0:\n            val = 1.0 / (1.0 + np.exp(-z))\n        else:\n            exp_z = np.exp(z)\n            val = exp_z / (1.0 + exp_z)\n        return round(val, 4)\n    arr = np.asarray(z, dtype=float)\n    out = np.empty_like(arr)\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    exp_vals = np.exp(arr[neg_mask])\n    out[neg_mask] = exp_vals / (1.0 + exp_vals)\n    return np.round(out, 4).tolist()"}
{"task_id": 224, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky-ReLU activation to every element in *z*.\n\n    Args:\n        z:  A scalar, (nested) list or ``numpy.ndarray`` of numbers.\n        a:  Slope used for the negative part, a float in the half-open\n            interval [0, 1).  Defaults to 0.01.\n\n    Returns\n    -------\n    Same \u201cshape\u201d as *z* but converted to plain Python objects\n    (i.e. lists / scalars).  For array or list inputs the function\n    returns a (possibly nested) list; for a scalar input it returns a\n    scalar.\n    \"\"\"\n    if not 0.0 <= a < 1.0:\n        raise ValueError(\"Parameter 'a' must lie in the interval [0, 1).\")\n    arr = np.asarray(z)\n    activated = np.where(arr >= 0, arr, a * arr)\n    return activated.tolist()"}
{"task_id": 226, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef _stump_predict(column: np.ndarray, threshold: float, polarity: int) -> np.ndarray:\n    \"\"\"\n    Helper \u2013 returns the predictions of one decision stump for a single feature column.\n    \"\"\"\n    if polarity == 1:\n        return np.where(column < threshold, -1, 1)\n    else:\n        return np.where(column < threshold, 1, -1)\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\"\"\"\n    (n_samples, n_features) = X_train.shape\n    w = np.full(n_samples, 1.0 / n_samples)\n    ensemble: list[tuple[int, float, int, float]] = []\n    for _ in range(n_clf):\n        best_feat = 0\n        best_thresh = 0.0\n        best_polarity = 1\n        best_error = float('inf')\n        best_pred_train = None\n        for feat_idx in range(n_features):\n            col = X_train[:, feat_idx]\n            thresholds = np.unique(col)\n            for thr in thresholds:\n                pred = _stump_predict(col, thr, 1)\n                err = np.sum(w[pred != y_train])\n                if err < best_error:\n                    (best_error, best_feat, best_thresh, best_polarity) = (err, feat_idx, thr, 1)\n                    best_pred_train = pred\n                pred = _stump_predict(col, thr, -1)\n                err = np.sum(w[pred != y_train])\n                if err < best_error:\n                    (best_error, best_feat, best_thresh, best_polarity) = (err, feat_idx, thr, -1)\n                    best_pred_train = pred\n        eps = 1e-10\n        alpha = 0.5 * math.log((1 - best_error) / (best_error + eps))\n        w *= np.exp(-alpha * y_train * best_pred_train)\n        w /= np.sum(w)\n        ensemble.append((best_feat, best_thresh, best_polarity, alpha))\n    agg = np.zeros(X_test.shape[0])\n    for (feat_idx, thr, pol, alpha) in ensemble:\n        preds = _stump_predict(X_test[:, feat_idx], thr, pol)\n        agg += alpha * preds\n    final_preds = np.sign(agg)\n    final_preds[final_preds == 0] = 1\n    return final_preds.astype(int).tolist()"}
{"task_id": 241, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal(shape: tuple[int, ...], scale: float=0.5) -> list:\n    \"\"\"Initialise a tensor with an orthogonal matrix (Saxe et al., 2014).\n\n    Args:\n        shape: Desired output shape. Must have length \u2265 2.\n        scale: Multiplicative scaling factor (default 0.5).\n\n    Returns:\n        A (possibly nested) python list whose first two dimensions are\n        orthogonal up to the given scaling factor, or \u20131 if `shape` is 1-D.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n    d0 = shape[0]\n    d_rest = int(np.prod(shape[1:], dtype=int))\n    A = np.random.standard_normal((d0, d_rest))\n    (U, _, Vt) = np.linalg.svd(A, full_matrices=False)\n    Q = U if U.shape == A.shape else Vt\n    tensor = (scale * Q.reshape(shape)).tolist()\n    return tensor"}
{"task_id": 243, "completion_id": 0, "solution": "import numpy as np\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a tiny fully-connected Actor\u2013Critic network\n    whose weights are all ones and whose biases are all zeros.\n\n    Args:\n        state: List with three floats \u2013 the environment state.\n\n    Returns:\n        (action_probabilities, state_value) where\n            action_probabilities \u2013 list with three floats, each rounded to 4 decimals\n            state_value          \u2013 single float, rounded to 4 decimals\n    \"\"\"\n    s_sum = np.sum(state)\n    z1 = max(0.0, s_sum)\n    z3 = 16.0 * z1\n    action_probs = [round(1 / 3, 4)] * 3\n    state_value = round(64.0 * z1, 4)\n    return (action_probs, state_value)"}
{"task_id": 249, "completion_id": 0, "solution": "import numpy as np\ndef _dense(x: np.ndarray, w: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A small helper that applies a fully-connected layer to a 1-D input vector.\n    It copes with both common weight layouts:\n        \u2022  (in_dim , out_dim)   so   x @ w\n        \u2022  (out_dim, in_dim )   so   w @ x\n    \"\"\"\n    if x.shape[-1] == w.shape[0]:\n        return x @ w + b\n    if x.shape[-1] == w.shape[1]:\n        return w @ x + b\n    raise ValueError('Weight matrix has incompatible shape.')\ndef _relu(z: np.ndarray) -> np.ndarray:\n    return np.maximum(0.0, z)\ndef _softmax(z: np.ndarray) -> np.ndarray:\n    z = z - np.max(z)\n    e = np.exp(z)\n    return e / e.sum()\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max) \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)            \u2192 state value\n    \"\"\"\n    x = np.asarray(state, dtype=np.float64)\n    W1 = np.asarray(params['W1'], dtype=np.float64)\n    b1 = np.asarray(params['b1'], dtype=np.float64)\n    W2 = np.asarray(params['W2'], dtype=np.float64)\n    b2 = np.asarray(params['b2'], dtype=np.float64)\n    W3 = np.asarray(params['W3'], dtype=np.float64)\n    b3 = np.asarray(params['b3'], dtype=np.float64)\n    Wa = np.asarray(params['Wa'], dtype=np.float64)\n    ba = np.asarray(params['ba'], dtype=np.float64)\n    Wc = np.asarray(params['Wc'], dtype=np.float64)\n    bc = np.asarray(params['bc'], dtype=np.float64)\n    x = _relu(_dense(x, W1, b1))\n    x = _relu(_dense(x, W2, b2))\n    hidden = _relu(_dense(x, W3, b3))\n    logits = _dense(hidden, Wa, ba)\n    probs = _softmax(logits)\n    value = float(_dense(hidden, Wc, bc))\n    probs_rounded = [round(float(p), 4) for p in probs]\n    value_rounded = round(value, 4)\n    return (probs_rounded, value_rounded)"}
{"task_id": 253, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w        : 1-D weight vector (list or NumPy array).\n        alpha    : Regularisation strength (non-negative float).\n        l1_ratio : Fraction of L1 component (float in [0, 1]).\n        gradient : If True, return gradient, else return penalty value.\n\n    Returns\n        float  \u2013 if *gradient* is False  (penalty value),\n        list   \u2013 if *gradient* is True   (element-wise gradient).\n    \"\"\"\n    if alpha < 0:\n        raise ValueError('alpha must be non-negative.')\n    if not 0.0 <= l1_ratio <= 1.0:\n        raise ValueError('l1_ratio must be in [0, 1].')\n    w = np.asarray(w, dtype=float).ravel()\n    if gradient:\n        grad = alpha * (l1_ratio * np.sign(w) + (1.0 - l1_ratio) * w)\n        return list(np.round(grad, 4))\n    else:\n        penalty = alpha * (l1_ratio * np.linalg.norm(w, 1) + (1.0 - l1_ratio) * 0.5 * np.dot(w, w))\n        return round(float(penalty), 4)"}
{"task_id": 256, "completion_id": 0, "solution": "import numpy as np\nfrom copy import deepcopy\ndef grad_check_RNN(model, loss_func, param_name: str, n_t: int, X, epsilon: float=1e-07):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred\n    finite differences (double precision).\n\n    See the task description for a detailed specification.\n    \"\"\"\n    if param_name in ('X', 'y'):\n        return None\n    real_name = param_name\n    if param_name == 'Ba':\n        real_name = 'ba'\n    elif param_name == 'Bx':\n        real_name = 'bx'\n    if real_name not in model.parameters:\n        raise KeyError(f\"Parameter '{real_name}' not found in model.\")\n    X = np.asarray(X, dtype=np.float64)\n    param: np.ndarray = model.parameters[real_name].astype(np.float64, copy=False)\n    model.parameters[real_name] = param\n    grads = np.zeros_like(param, dtype=np.float64)\n\n    def _reset_state_if_possible():\n        if hasattr(model, 'reset_state') and callable(model.reset_state):\n            model.reset_state()\n\n    def _sequence_loss():\n        model.flush_gradients()\n        _reset_state_if_possible()\n        preds = []\n        for t in range(n_t):\n            preds.append(model.forward(X[:, :, t]))\n        return float(loss_func(preds))\n    it = np.ndindex(param.shape)\n    for idx in it:\n        original_val = param[idx]\n        param[idx] = original_val + epsilon\n        loss_plus = _sequence_loss()\n        param[idx] = original_val - epsilon\n        loss_minus = _sequence_loss()\n        grads[idx] = (loss_plus - loss_minus) / (2.0 * epsilon)\n        param[idx] = original_val\n    return grads.T.astype(np.float64)"}
{"task_id": 257, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\"\"\"\n\n    def stump_predict(X, feature, thresh, polarity):\n        \"\"\"\n        Predict with a single decision stump.\n        polarity =  1 : x < thresh  -> -1     else 1\n        polarity = -1 : x < thresh  ->  1     else -1\n        \"\"\"\n        col = X[:, feature]\n        if polarity == 1:\n            return np.where(col < thresh, -1, 1)\n        else:\n            return np.where(col < thresh, 1, -1)\n    n_clf = max(1, int(n_clf))\n    (m, n_features) = X_train.shape\n    w = np.full(m, 1.0 / m)\n    stumps = []\n    eps = 1e-10\n    for _ in range(n_clf):\n        (best_feature, best_thresh, best_pol, min_err) = (None, None, None, np.inf)\n        for feature in range(n_features):\n            values = X_train[:, feature]\n            thresholds = np.unique(values)\n            for thresh in thresholds:\n                for polarity in (1, -1):\n                    pred = stump_predict(X_train, feature, thresh, polarity)\n                    err = np.sum(w[pred != y_train])\n                    if err < min_err:\n                        (best_feature, best_thresh, best_pol) = (feature, thresh, polarity)\n                        min_err = err\n        if min_err >= 0.5 - eps:\n            break\n        min_err = max(min_err, eps)\n        alpha = 0.5 * np.log((1 - min_err) / min_err)\n        stumps.append((best_feature, best_thresh, best_pol, alpha))\n        predictions = stump_predict(X_train, best_feature, best_thresh, best_pol)\n        w *= np.exp(-alpha * y_train * predictions)\n        w /= w.sum()\n    if not stumps:\n        return [-1] * len(X_test)\n    agg = np.zeros(X_test.shape[0])\n    for (feature, thresh, polarity, alpha) in stumps:\n        agg += alpha * stump_predict(X_test, feature, thresh, polarity)\n    return [1 if x >= 0 else -1 for x in agg]"}
{"task_id": 261, "completion_id": 0, "solution": "import numpy as np\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate an array initialised with Glorot/Xavier normal distribution.\n\n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired tensor shape. Must have at least two dimensions so that\n        fan-in and fan-out are well defined.\n\n    Returns\n    -------\n    np.ndarray\n        Array of the requested shape whose elements are drawn independently\n        from \ud835\udca9(0, s\u00b2) with  s = \u221a[2 / (fan_in + fan_out)].\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError('`shape` must have at least two dimensions.')\n    if len(shape) == 2:\n        (fan_in, fan_out) = shape\n    else:\n        receptive_field = int(np.prod(shape[2:], dtype=int))\n        fan_in = shape[1] * receptive_field\n        fan_out = shape[0] * receptive_field\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n    return np.random.normal(loc=0.0, scale=std, size=shape).astype(float)"}
{"task_id": 266, "completion_id": 0, "solution": "from typing import Any, List, Tuple\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples ``(u, v)`` describing undirected edges that connect\n        vertices ``u`` and ``v``.\n        \u2022 Multiple occurrences of the same edge may be present.  \n        \u2022 Vertices that do not occur in ``V`` are ignored.  \n        \u2022 Self-loops (u == v) are ignored.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list ``G`` where ``G[i]`` contains all vertices adjacent to ``V[i]``.\n        \u2022 The outer list is in the same order as ``V``.  \n        \u2022 Inner lists contain *no duplicates* and are ordered by the vertices\u2019\n          order of appearance in ``V``.\n    \"\"\"\n    index = {vertex: i for (i, vertex) in enumerate(V)}\n    neighbours: List[set] = [set() for _ in V]\n    for (u, v) in E:\n        if u not in index or v not in index:\n            continue\n        if u == v:\n            continue\n        (ui, vi) = (index[u], index[v])\n        neighbours[ui].add(v)\n        neighbours[vi].add(u)\n    G: List[List[Any]] = []\n    for nb_set in neighbours:\n        ordered_nb = sorted(nb_set, key=index.get)\n        G.append(ordered_nb)\n    return G"}
{"task_id": 267, "completion_id": 0, "solution": "import numpy as np\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Best stump (see task description).\n    \"\"\"\n    y = y.ravel().astype(int)\n    w = sample_weights.ravel().astype(float)\n    (n_samples, n_features) = X.shape\n    best_feature = -1\n    best_threshold = 0.0\n    best_polarity = 1\n    best_error = np.inf\n    for j in range(n_features):\n        sorted_idx = np.argsort(X[:, j])\n        f_vals = X[sorted_idx, j]\n        lbls = y[sorted_idx]\n        ws = w[sorted_idx]\n        pos_total = ws[lbls == 1].sum()\n        neg_total = ws[lbls == -1].sum()\n        cum_pos = 0.0\n        cum_neg = 0.0\n        i = 0\n        n = n_samples\n        while i < n:\n            theta = f_vals[i]\n            err_p_1 = cum_neg + (pos_total - cum_pos)\n            err_p_neg1 = cum_pos + (neg_total - cum_neg)\n            if err_p_1 < best_error - 1e-12:\n                best_error = err_p_1\n                best_feature = j\n                best_threshold = float(theta)\n                best_polarity = 1\n            if err_p_neg1 < best_error - 1e-12:\n                best_error = err_p_neg1\n                best_feature = j\n                best_threshold = float(theta)\n                best_polarity = -1\n            while i < n and f_vals[i] == theta:\n                if lbls[i] == 1:\n                    cum_pos += ws[i]\n                else:\n                    cum_neg += ws[i]\n                i += 1\n    return {'feature_index': int(best_feature), 'threshold': round(best_threshold, 4), 'polarity': int(best_polarity), 'weighted_error': round(best_error, 4)}"}
{"task_id": 273, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float (rounded to 4 decimals) or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    n = len(feature)\n    if n <= 1:\n        return (None, 0.0)\n\n    def entropy(counter):\n        total = sum(counter.values())\n        if total == 0:\n            return 0.0\n        h = 0.0\n        for cnt in counter.values():\n            p = cnt / total\n            h -= p * math.log2(p)\n        return h\n    pairs = sorted(zip(feature, target), key=lambda x: x[0])\n    sorted_feat = [p[0] for p in pairs]\n    sorted_label = [p[1] for p in pairs]\n    if sorted_feat[0] == sorted_feat[-1]:\n        return (None, 0.0)\n    right_counts = Counter(sorted_label)\n    left_counts = Counter()\n    parent_entropy = entropy(right_counts)\n    best_gain = 0.0\n    best_threshold = None\n    eps = 1e-12\n    for i in range(n - 1):\n        lbl = sorted_label[i]\n        left_counts[lbl] += 1\n        right_counts[lbl] -= 1\n        if right_counts[lbl] == 0:\n            del right_counts[lbl]\n        if sorted_feat[i] != sorted_feat[i + 1]:\n            thr = (sorted_feat[i] + sorted_feat[i + 1]) / 2.0\n            n_left = i + 1\n            n_right = n - n_left\n            gain = parent_entropy - n_left / n * entropy(left_counts) - n_right / n * entropy(right_counts)\n            if gain > best_gain + eps:\n                (best_gain, best_threshold) = (gain, thr)\n            elif abs(gain - best_gain) <= eps and best_gain > 0.0:\n                best_threshold = min(best_threshold, thr)\n    if best_threshold is None:\n        return (None, 0.0)\n    return (round(best_threshold, 4), round(best_gain, 4))"}
{"task_id": 286, "completion_id": 0, "solution": "import random\nfrom typing import Callable, Tuple, List\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Retrieve an initializer function by its name.\n\n    Args:\n        name: The name of the initializer to look up (e.g. \"zeros_init\").\n\n    Returns:\n        The corresponding callable weight initializer.\n\n    Raises:\n        ValueError: If `name` is not found or is not callable.\n    \"\"\"\n    obj = globals().get(name)\n    if obj is None or not callable(obj):\n        raise ValueError('Invalid initialization function.')\n    return obj"}
{"task_id": 287, "completion_id": 0, "solution": "import numpy as np\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input tensor of shape (N, C, H, W).\n    pool_shape : (int, int)\n        Height and width of the pooling window  (p_h, p_w).\n    stride : (int, int)\n        Vertical and horizontal stride          (s_h, s_w).\n    accum_grad : np.ndarray\n        Gradient wrt. the pooled output. Shape must match the\n        forward-pass output.\n\n    Returns\n    -------\n    tuple[list, list]\n        (pooled_output, grad_input) \u2013 both rounded to 4 decimals and\n        converted to Python lists.\n    \"\"\"\n    (N, C, H, W) = X.shape\n    (p_h, p_w) = pool_shape\n    (s_h, s_w) = stride\n    out_h = (H - p_h) // s_h + 1\n    out_w = (W - p_w) // s_w + 1\n    pooled = np.zeros((N, C, out_h, out_w), dtype=X.dtype)\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                (h_start, h_end) = (i * s_h, i * s_h + p_h)\n                for j in range(out_w):\n                    (w_start, w_end) = (j * s_w, j * s_w + p_w)\n                    window = X[n, c, h_start:h_end, w_start:w_end]\n                    pooled[n, c, i, j] = window.mean()\n    grad_input = np.zeros_like(X, dtype=X.dtype)\n    scale = 1.0 / (p_h * p_w)\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                (h_start, h_end) = (i * s_h, i * s_h + p_h)\n                for j in range(out_w):\n                    (w_start, w_end) = (j * s_w, j * s_w + p_w)\n                    g = accum_grad[n, c, i, j] * scale\n                    grad_input[n, c, h_start:h_end, w_start:w_end] += g\n    pooled = np.round(pooled, 4).tolist()\n    grad_input = np.round(grad_input, 4).tolist()\n    return (pooled, grad_input)"}
{"task_id": 290, "completion_id": 0, "solution": "import numpy as np\nfrom dataclasses import dataclass\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\"\"\"\n    feature: int\n    threshold: float\n    left: object\n    right: object\n_TOL = 1e-08\ndef _values_equal(a, b):\n    \"\"\"Compare two prediction values with the required tolerance.\"\"\"\n    try:\n        return np.allclose(a, b, atol=_TOL, rtol=_TOL)\n    except TypeError:\n        return a == b\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\"\"\"\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return _values_equal(tree_a.value, tree_b.value)\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        return tree_a.feature == tree_b.feature and np.isclose(tree_a.threshold, tree_b.threshold, atol=_TOL, rtol=_TOL) and compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    return False"}
{"task_id": 292, "completion_id": 0, "solution": "import numpy as np\ndef single_point_crossover(parent1: list[list[int | float]], parent2: list[list[int | float]], cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix (list of lists).\n        parent2: Second parent weight matrix (same shape as *parent1*).\n        cutoff : Column index at which crossover starts (inclusive).\n\n    Returns\n    -------\n        Tuple (child1, child2) with the two children as ordinary Python\n        lists, or -1 if the parent matrices do not have identical shapes.\n    \"\"\"\n    p1 = np.asarray(parent1, dtype=float)\n    p2 = np.asarray(parent2, dtype=float)\n    if p1.ndim != 2 or p2.ndim != 2 or p1.shape != p2.shape:\n        return -1\n    (rows, cols) = p1.shape\n    if not 0 <= cutoff <= cols:\n        raise ValueError(f'cutoff must be in [0, {cols}]')\n    child1 = p1.copy()\n    child2 = p2.copy()\n    child1[:, cutoff:] = p2[:, cutoff:]\n    child2[:, cutoff:] = p1[:, cutoff:]\n    return (child1.tolist(), child2.tolist())"}
{"task_id": 294, "completion_id": 0, "solution": "from typing import Any, Dict, List, Tuple\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert the custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph with the keys\n          * 'is_directed' : bool\n          * '_V2I'        : dict  (vertex label -> consecutive index 0..n-1)\n          * '_G'          : list  (for each vertex index a list with tuples\n                                   (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary mapping every vertex label to a list of (neighbour, weight)\n        tuples.  Lists are sorted lexicographically by the neighbour label.\n        For undirected graphs every edge appears once in each direction.\n    \"\"\"\n    if not G or not G.get('_V2I'):\n        return {}\n    directed: bool = bool(G.get('is_directed', False))\n    adj: Dict[Any, List[Tuple[Any, float]]] = {v: [] for v in G['_V2I']}\n    processed_undirected = set()\n    for edge_list in G['_G']:\n        for (u, v, w) in edge_list:\n            if directed:\n                adj[u].append((v, w))\n            else:\n                key = (u, v) if u <= v else (v, u)\n                if key in processed_undirected:\n                    continue\n                processed_undirected.add(key)\n                adj[u].append((v, w))\n                adj[v].append((u, w))\n    for neighbours in adj.values():\n        neighbours.sort(key=lambda t: str(t[0]))\n    return adj"}
{"task_id": 296, "completion_id": 0, "solution": "import numpy as np\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1 (or, for boolean arrays,\n        only ``True``/``False``), otherwise ``False``.\n    \"\"\"\n    arr = np.asarray(x)\n    if arr.dtype == np.bool_:\n        return True\n    return np.isin(arr, (0, 1)).all()"}
{"task_id": 298, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom typing import List, Tuple, Dict\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Notes\n    -----\n    \u2022 White-space is used for tokenisation.\n    \u2022 For N > 1 every sentence is padded with N\u22121 '<bol>' tokens in front\n      and a single '<eol>' token at the end.  Unigrams use no padding.\n    \u2022 If any N-gram (or its (N-1)-gram prefix) required for the query is\n      unseen in *corpus*, or if the query is too short, the function\n      returns float('-inf').\n    \u2022 The final result is rounded to 4 decimal places.\n    \"\"\"\n    if N < 1:\n        raise ValueError('N must be \u2265 1')\n    if not corpus:\n        return float('-inf')\n    if N == 1:\n        unigram_counts: Counter[str] = Counter()\n        total_tokens = 0\n        for sent in corpus:\n            tokens = sent.split()\n            unigram_counts.update(tokens)\n            total_tokens += len(tokens)\n    else:\n        ngram_counts: Counter[Tuple[str, ...]] = Counter()\n        prefix_counts: Counter[Tuple[str, ...]] = Counter()\n        for sent in corpus:\n            tokens = ['<bol>'] * (N - 1) + sent.split() + ['<eol>']\n            for i in range(len(tokens) - N + 1):\n                ngram = tuple(tokens[i:i + N])\n                prefix = tuple(tokens[i:i + N - 1])\n                ngram_counts[ngram] += 1\n                prefix_counts[prefix] += 1\n    if N == 1:\n        q_tokens = query.split()\n        if not q_tokens or total_tokens == 0:\n            return float('-inf')\n    else:\n        q_tokens = ['<bol>'] * (N - 1) + query.split() + ['<eol>']\n        if len(q_tokens) < N:\n            return float('-inf')\n    log_sum = 0.0\n    if N == 1:\n        for tok in q_tokens:\n            cnt = unigram_counts.get(tok, 0)\n            if cnt == 0:\n                return float('-inf')\n            prob = cnt / total_tokens\n            log_sum += math.log(prob)\n    else:\n        for i in range(len(q_tokens) - N + 1):\n            ngram = tuple(q_tokens[i:i + N])\n            prefix = tuple(q_tokens[i:i + N - 1])\n            cnt_ng = ngram_counts.get(ngram, 0)\n            cnt_pre = prefix_counts.get(prefix, 0)\n            if cnt_ng == 0 or cnt_pre == 0:\n                return float('-inf')\n            prob = cnt_ng / cnt_pre\n            log_sum += math.log(prob)\n    return round(log_sum, 4)"}
{"task_id": 302, "completion_id": 0, "solution": "import numpy as np\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample (shape = [n_samples, n_features]).\n        k: Desired number of clusters (k \u2264 n_samples).\n\n    Returns:\n        Python list of length n_samples whose entries are integers in [0, k-1].\n    \"\"\"\n    n_samples = X.shape[0]\n    diff = X[:, None, :] - X[None, :, :]\n    dist = np.linalg.norm(diff, axis=2)\n    W = 1.0 / (1.0 + dist)\n    np.fill_diagonal(W, 0.0)\n    degree = W.sum(axis=1)\n    L = np.diag(degree) - W\n    (eig_vals, eig_vecs) = np.linalg.eigh(L)\n    E = eig_vecs[:, :k]\n    centroids = E[:k].copy()\n    labels = np.full(n_samples, -1, dtype=int)\n    max_iter = 100\n    rng = np.random.default_rng()\n    for _ in range(max_iter):\n        dists = np.linalg.norm(E[:, None, :] - centroids[None, :, :], axis=2)\n        new_labels = dists.argmin(axis=1)\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n        for c in range(k):\n            mask = labels == c\n            if np.any(mask):\n                centroids[c] = E[mask].mean(axis=0)\n            else:\n                random_idx = rng.integers(0, n_samples)\n                centroids[c] = E[random_idx]\n    return labels.tolist()"}
{"task_id": 303, "completion_id": 0, "solution": "def err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str='') -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    prev_ix = max(ix - 1, 0)\n    (prev_mine, prev_label) = params[prev_ix]\n    (curr_mine, curr_label) = params[ix]\n    gold_prev = golds.get(prev_label, '')\n    gold_curr = golds.get(curr_label, '')\n    header = '-' * 25 + ' DEBUG ' + '-' * 25\n    footer = '-' * 23 + ' END DEBUG ' + '-' * 23\n    lines = [header, f'Mine (prev) [{prev_label}]:', f'{prev_mine}', '', f'Theirs (prev) [{prev_label}]:', f'{gold_prev}', '', f'Mine [{curr_label}]:', f'{curr_mine}', '', f'Theirs [{curr_label}]:', f'{gold_curr}{warn_str}', footer]\n    return '\\n'.join(lines)"}
{"task_id": 304, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef bayesian_posterior_mean(X: np.ndarray, y: np.ndarray, alpha: float=1.0, beta: float=1.0, mu=0.0, V=None, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Posterior mean (MAP) of the coefficients in Bayesian linear regression.\n\n    Parameters\n    ----------\n    X : (N, M) array_like\n        Design matrix.\n    y : (N,) array_like\n        Targets.\n    alpha, beta : float\n        Hyper\u2013parameters of the inverse-gamma prior on \u03c3\u00b2 (kept for API\n        completeness \u2013 they do not enter the MAP of the weights).\n    mu : float or array_like\n        Prior mean for the weights.  Scalar values are broadcast.\n    V : None, float, 1-D or 2-D array_like\n        Prior scale.  None\u2192I, scalar\u2192s\u00b7I, 1-D\u2192diag(v), 2-D\u2192full matrix.\n    fit_intercept : bool\n        Whether to include a bias column of ones.\n\n    Returns\n    -------\n    list[float]\n        Posterior mean rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    if fit_intercept:\n        X = np.column_stack((np.ones(X.shape[0]), X))\n    (N, M) = X.shape\n    if V is None:\n        V_mat = np.eye(M)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 0:\n            V_mat = float(V_arr) * np.eye(M)\n        elif V_arr.ndim == 1:\n            if V_arr.size != M:\n                raise ValueError('Diagonal length of V does not match #features.')\n            V_mat = np.diag(V_arr)\n        elif V_arr.ndim == 2:\n            if V_arr.shape != (M, M):\n                raise ValueError('Shape of V does not match #features.')\n            V_mat = V_arr\n        else:\n            raise ValueError('Unsupported shape for V.')\n    V_inv = np.linalg.inv(V_mat)\n    if np.isscalar(mu):\n        mu_vec = np.full(M, float(mu))\n    else:\n        mu_vec = np.asarray(mu, dtype=float).ravel()\n        if mu_vec.size != M:\n            raise ValueError('Length of mu does not match #features.')\n    XtX = X.T @ X\n    Sigma_b = np.linalg.inv(V_inv + XtX)\n    rhs = V_inv @ mu_vec + X.T @ y\n    mu_b = Sigma_b @ rhs\n    return np.round(mu_b, 4).tolist()"}
{"task_id": 308, "completion_id": 0, "solution": "import numpy as np\ndef mel_filterbank(N: int, n_filters: int=20, fs: int=44000, min_freq: int=0, max_freq: int | None=None, normalize: bool=True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    Parameters\n    ----------\n    N : int\n        DFT length (number of frequency-domain samples).\n    n_filters : int, default 20\n        Number of Mel filters to construct.\n    fs : int, default 44_000\n        Sampling frequency of the signal in Hertz.\n    min_freq : int, default 0\n        Lower frequency limit of the filterbank in Hertz.\n    max_freq : int | None, optional\n        Upper frequency limit of the filterbank in Hertz.  If *None* the\n        Nyquist frequency *fs/2* is used.\n    normalize : bool, default True\n        Whether to scale each filter so that its (Mel-domain) area equals 1.\n\n    Returns\n    -------\n    list[list[float]]\n        The filterbank matrix with shape (n_filters, N//2 + 1).  All values are\n        rounded to four decimal places.\n    \"\"\"\n\n    def hz2mel(f_hz: np.ndarray | float) -> np.ndarray | float:\n        return 2595.0 * np.log10(1.0 + np.asarray(f_hz) / 700.0)\n\n    def mel2hz(mel: np.ndarray | float) -> np.ndarray | float:\n        return 700.0 * (10 ** (np.asarray(mel) / 2595.0) - 1.0)\n    if max_freq is None:\n        max_freq = fs / 2\n    if not 0 <= min_freq < max_freq <= fs / 2:\n        raise ValueError('min_freq must be >= 0, and max_freq must not exceed fs/2 with min_freq < max_freq.')\n    (mel_min, mel_max) = (hz2mel(min_freq), hz2mel(max_freq))\n    mel_points = np.linspace(mel_min, mel_max, n_filters + 2)\n    hz_points = mel2hz(mel_points)\n    bin_freqs = np.arange(N // 2 + 1) * (fs / N)\n    F = np.zeros((n_filters, N // 2 + 1), dtype=float)\n    for i in range(n_filters):\n        (f_left, f_center, f_right) = hz_points[i:i + 3]\n        left_mask = np.logical_and(bin_freqs >= f_left, bin_freqs <= f_center)\n        F[i, left_mask] = (bin_freqs[left_mask] - f_left) / (f_center - f_left)\n        right_mask = np.logical_and(bin_freqs >= f_center, bin_freqs <= f_right)\n        F[i, right_mask] = (f_right - bin_freqs[right_mask]) / (f_right - f_center)\n        if normalize:\n            F[i, :] *= 2.0 / (f_right - f_left)\n    F_rounded = np.round(F, 4).tolist()\n    return F_rounded"}
{"task_id": 312, "completion_id": 0, "solution": "import numpy as np\ndef blackman_harris(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    N = window_len - 1 if symmetric else window_len\n    (a0, a1, a2, a3) = (0.35875, 0.48829, 0.14128, 0.01168)\n    n = np.arange(window_len)\n    w = a0 - a1 * np.cos(2 * np.pi * n / N) + a2 * np.cos(4 * np.pi * n / N) - a3 * np.cos(6 * np.pi * n / N)\n    return [round(float(x), 4) for x in w]"}
{"task_id": 313, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Return E[log X_t] for X ~ Dir(gamma[d, :]), rounded to 4 decimals.\"\"\"\n\n    def digamma(x):\n        \"\"\"\n        Digamma \u03c8(x) for positive real x.\n        Small arguments are stepped up with the recurrence\n            \u03c8(x) = \u03c8(x + 1) \u2212 1/x\n        until x \u2265 6, after which an asymptotic expansion is applied:\n            \u03c8(x) \u2248 ln x \u2212 1/(2x) \u2212 1/(12x\u00b2) + 1/(120x\u2074) \u2212 1/(252x\u2076)\n        The combination is accurate to better than 1e-6 for all x > 0.\n        \"\"\"\n        res = 0.0\n        x = float(x)\n        while x < 6.0:\n            res -= 1.0 / x\n            x += 1.0\n        inv = 1.0 / x\n        inv2 = inv * inv\n        inv4 = inv2 * inv2\n        inv6 = inv4 * inv2\n        res += math.log(x) - 0.5 * inv - inv2 / 12.0 + inv4 / 120.0 - inv6 / 252.0\n        return res\n    g = np.asarray(gamma, dtype=float)\n    row = g[d]\n    alpha_t = row[t]\n    row_sum = float(row.sum())\n    value = digamma(alpha_t) - digamma(row_sum)\n    return round(value, 4)"}
{"task_id": 317, "completion_id": 0, "solution": "from typing import List, Tuple, Dict\ndef err_fmt(params: List[Tuple[str, str]], golds: Dict[str, str], ix: int, warn_str: str='') -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\"\"\"\n    prev_ix = max(ix - 1, 0)\n    (prev_pred, prev_label) = params[prev_ix]\n    (curr_pred, curr_label) = params[ix]\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    header = '-' * 25 + ' DEBUG ' + '-' * 25\n    footer = '-' * 23 + ' END DEBUG ' + '-' * 23\n    parts = [header, f'Mine (prev) [{prev_label}]:', prev_pred, '', f'Theirs (prev) [{prev_label}]:', prev_gold, '', '', f'Mine [{curr_label}]:', curr_pred, '', f'Theirs [{curr_label}]:', f'{curr_gold}{warn_str}', footer]\n    return '\\n'.join(parts)"}
{"task_id": 318, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef _sign(z):\n    \"\"\"Return -1 for z < 0, +1 otherwise (i.e. sign(0)=+1).\"\"\"\n    return np.where(z < 0, -1, 1)\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples with labels \u20131 or +1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (\u20131 or +1) for each sample in X_test.\n    \"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    X_test = np.asarray(X_test, dtype=float)\n    (n_samples, n_features) = X_train.shape\n    w = np.full(n_samples, 1.0 / n_samples)\n    stumps = []\n    for _ in range(n_estimators):\n        best_error = float('inf')\n        best_j = best_t = best_p = None\n        for j in range(n_features):\n            xj = X_train[:, j]\n            idx = np.argsort(xj)\n            x_sorted = xj[idx]\n            y_sorted = y_train[idx]\n            w_sorted = w[idx]\n            pos_mask = y_sorted == 1\n            neg_mask = ~pos_mask\n            pos_cumsum = np.cumsum(w_sorted * pos_mask)\n            neg_cumsum = np.cumsum(w_sorted * neg_mask)\n            pos_total = pos_cumsum[-1]\n            neg_total = neg_cumsum[-1]\n            pos_left = np.concatenate(([0.0], pos_cumsum))\n            neg_left = np.concatenate(([0.0], neg_cumsum))\n            err_p1 = pos_left + (neg_total - neg_left)\n            err_pn1 = neg_left + (pos_total - pos_left)\n            for (polarity, errors) in ((1, err_p1), (-1, err_pn1)):\n                k = int(np.argmin(errors))\n                err = errors[k]\n                if err < best_error - 1e-12:\n                    best_error = err\n                    best_j = j\n                    best_p = polarity\n                    if k == 0:\n                        best_t = x_sorted[0] - 1e-10\n                    elif k == len(x_sorted):\n                        best_t = x_sorted[-1] + 1e-10\n                    else:\n                        best_t = 0.5 * (x_sorted[k - 1] + x_sorted[k])\n        eps = max(best_error, 1e-10)\n        alpha = 0.5 * math.log((1.0 - eps) / eps)\n        preds = best_p * _sign(X_train[:, best_j] - best_t)\n        w *= np.exp(-alpha * y_train * preds)\n        w /= w.sum()\n        stumps.append((best_j, best_t, best_p, alpha))\n    agg = np.zeros(X_test.shape[0])\n    for (j, t, p, alpha) in stumps:\n        agg += alpha * p * _sign(X_test[:, j] - t)\n    y_pred = _sign(agg).astype(int)\n    return y_pred.tolist()"}
{"task_id": 329, "completion_id": 0, "solution": "import numpy as np\ndef dft_bins(N: int, fs: int=44000, *, positive_only: bool=True) -> list[float]:\n    \"\"\"Return the centre frequency (Hz) of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        If True, return only the non-negative frequencies (indices\n        0 \u2026 \u230aN/2\u230b).  If False, return all N frequency bins in the same\n        order as `numpy.fft.fftfreq`.  Default is True.\n\n    Returns\n    -------\n    list[float]\n        Frequencies rounded to 4 decimal places.  Empty list if `N`\n        or `fs` is not strictly positive.\n    \"\"\"\n    if N <= 0 or fs <= 0:\n        return []\n    if positive_only:\n        freqs = np.arange(0, N // 2 + 1) * (fs / N)\n    else:\n        freqs = np.fft.fftfreq(N, d=1 / fs)\n    return np.round(freqs, 4).tolist()"}
{"task_id": 331, "completion_id": 0, "solution": "import numpy as np\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix whose element (i, j) is True/1 iff sample i\n        was out of bag for tree j, same shape as *predictions*.\n\n    Returns\n    -------\n    float\n        OOB mean squared error rounded to 4 decimal places, or -1 if it cannot\n        be computed.\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=float).ravel()\n    preds = np.asarray(predictions, dtype=float)\n    mask = np.asarray(oob_mask).astype(bool)\n    if preds.shape != mask.shape:\n        raise ValueError('`predictions` and `oob_mask` must have the same shape.')\n    oob_counts = mask.sum(axis=1)\n    valid_idx = oob_counts > 0\n    if not np.any(valid_idx):\n        return -1\n    oob_sums = (preds * mask).sum(axis=1)\n    oob_means = np.zeros_like(y_true)\n    oob_means[valid_idx] = oob_sums[valid_idx] / oob_counts[valid_idx]\n    sq_err = (oob_means[valid_idx] - y_true[valid_idx]) ** 2\n    mse = sq_err.mean()\n    return round(float(mse), 4)"}
{"task_id": 332, "completion_id": 0, "solution": "from collections import Counter\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree represented with plain nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of *transactions* an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node is a mapping:\n        {\"support\": <int>, \"children\": {item -> child-node}}\n    \"\"\"\n    item_counter = Counter()\n    for trx in transactions:\n        for item in set(trx):\n            item_counter[item] += 1\n    freq_items = {item: sup for (item, sup) in item_counter.items() if sup >= min_support}\n    if not freq_items:\n        return {'support': 0, 'children': {}}\n    ordered = sorted(freq_items.items(), key=lambda pair: (-pair[1], pair[0]))\n    order_rank = {item: rank for (rank, (item, _)) in enumerate(ordered)}\n    frequent_set = set(freq_items)\n    root = {'support': 0, 'children': {}}\n    for trx in transactions:\n        filtered = [item for item in set(trx) if item in frequent_set]\n        if not filtered:\n            continue\n        filtered.sort(key=lambda it: order_rank[it])\n        node = root\n        for item in filtered:\n            children = node['children']\n            if item not in children:\n                children[item] = {'support': 0, 'children': {}}\n            node = children[item]\n            node['support'] += 1\n    return root"}
{"task_id": 336, "completion_id": 0, "solution": "import numpy as np\ndef linear_kernel(X: list[list[int | float]] | 'np.ndarray', Y: list[list[int | float]] | 'np.ndarray' | None=None, c0: int | float=0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n    Y : 2-D list or NumPy array with shape (M, C) or None.  If None, Y = X.\n    c0 : additive constant of the kernel (default 0).\n\n    Returns\n    -------\n    list[list[float]] | int\n        Kernel matrix rounded to 4 decimals, or \u20111 for incompatible shapes.\n    \"\"\"\n\n    def _to_2d_float_array(obj):\n        arr = np.asarray(obj, dtype=float)\n        if arr.ndim == 0:\n            arr = arr.reshape(1, 1)\n        elif arr.ndim == 1:\n            arr = arr.reshape(1, -1)\n        return arr\n    if isinstance(X, (list, tuple)) and len(X) == 0:\n        return []\n    X_arr = _to_2d_float_array(X)\n    if Y is None:\n        Y_arr = X_arr\n    elif isinstance(Y, (list, tuple)) and len(Y) == 0:\n        Y_arr = _to_2d_float_array(Y)\n    else:\n        Y_arr = _to_2d_float_array(Y)\n    if X_arr.shape[1] != Y_arr.shape[1]:\n        return -1\n    K = X_arr @ Y_arr.T\n    if c0 != 0:\n        K = K + c0\n    return np.round(K.astype(float), 4).tolist()"}
{"task_id": 340, "completion_id": 0, "solution": "import numpy as np\ndef adam_update(param: np.ndarray, grad: np.ndarray, t: int, m: np.ndarray, v: np.ndarray, lr: float=0.001, beta1: float=0.9, beta2: float=0.999, eps: float=1e-08) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1.\n        beta1: Exponential decay rate for the first moment.\n        beta2: Exponential decay rate for the second moment.\n        eps:   Small constant to avoid division by zero.\n\n    Returns:\n        Tuple of python lists rounded to 8 decimal places:\n        (updated_parameter, new_moment, new_variance)\n    \"\"\"\n    param = np.asarray(param, dtype=np.float64)\n    grad = np.asarray(grad, dtype=np.float64)\n    m = np.asarray(m, dtype=np.float64)\n    v = np.asarray(v, dtype=np.float64)\n    m_t = beta1 * m + (1.0 - beta1) * grad\n    v_t = beta2 * v + (1.0 - beta2) * grad ** 2\n    m_hat = m_t / (1.0 - beta1 ** t)\n    v_hat = v_t / (1.0 - beta2 ** t)\n    if np.all(grad == 0):\n        param_t = param.copy()\n    else:\n        param_t = param - lr * m_hat / (np.sqrt(v_hat) + eps)\n\n    def to_rounded_list(arr: np.ndarray) -> list:\n        return np.round(arr, 8).tolist()\n    return (to_rounded_list(param_t), to_rounded_list(m_t), to_rounded_list(v_t))"}
{"task_id": 343, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_update(theta_samples: np.ndarray, rewards: list[float] | np.ndarray, retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists with the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    rewards = np.asarray(rewards)\n    n_samples = theta_samples.shape[0]\n    k = max(1, int(retain_prcnt * n_samples))\n    elite_idx = np.argsort(rewards)[::-1][:k]\n    elite_samples = theta_samples[elite_idx]\n    mean = np.mean(elite_samples, axis=0)\n    var = np.var(elite_samples, axis=0)\n    mean_rounded = np.round(mean, 4).tolist()\n    var_rounded = np.round(var, 4).tolist()\n    return (mean_rounded, var_rounded)"}
{"task_id": 353, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nimport numpy as np\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n                class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    label_list = list(labels)\n    if not label_list:\n        return 0.0\n    total = len(label_list)\n    counts = Counter(label_list)\n    if len(counts) == 1:\n        return 0.0\n    entropy_val = 0.0\n    for count in counts.values():\n        p = count / total\n        if p > 0.0:\n            entropy_val -= p * math.log2(p)\n    return round(entropy_val, 5)"}
{"task_id": 354, "completion_id": 0, "solution": "import numpy as np\ndef _parse_padding(pad, k_r, k_c, dil, stride, in_r, in_c):\n    \"\"\"Returns the 4 individual paddings (top, bottom, left, right).\"\"\"\n    eff_k_r = k_r + (k_r - 1) * dil\n    eff_k_c = k_c + (k_c - 1) * dil\n    if isinstance(pad, int):\n        return (pad, pad, pad, pad)\n    if isinstance(pad, tuple) and len(pad) == 2:\n        (pr, pc) = pad\n        return (pr, pr, pc, pc)\n    if isinstance(pad, tuple) and len(pad) == 4:\n        return pad\n    if pad == 'same':\n        total_pad_r = max(eff_k_r - 1, 0)\n        total_pad_c = max(eff_k_c - 1, 0)\n        pr1 = total_pad_r // 2\n        pr2 = total_pad_r - pr1\n        pc1 = total_pad_c // 2\n        pc2 = total_pad_c - pc1\n        return (pr1, pr2, pc1, pc2)\n    return (0, 0, 0, 0)\ndef conv2D(X: 'np.ndarray', W: 'np.ndarray', stride: int, pad, dilation: int=0):\n    \"\"\"Performs a 2-D convolution (cross-correlation) via im2col + GEMM.\"\"\"\n    (n_ex, in_r, in_c, in_ch) = X.shape\n    (k_r, k_c, in_ch_w, out_ch) = W.shape\n    assert in_ch == in_ch_w, 'Input / kernel channels mismatch'\n    dil = dilation\n    eff_k_r = k_r + (k_r - 1) * dil\n    eff_k_c = k_c + (k_c - 1) * dil\n    (pr1, pr2, pc1, pc2) = _parse_padding(pad, k_r, k_c, dil, stride, in_r, in_c)\n    X_pad = np.pad(X, pad_width=((0, 0), (pr1, pr2), (pc1, pc2), (0, 0)), mode='constant')\n    (pad_r, pad_c) = (in_r + pr1 + pr2, in_c + pc1 + pc2)\n    out_r = (pad_r - eff_k_r) // stride + 1\n    out_c = (pad_c - eff_k_c) // stride + 1\n    cols = np.empty((n_ex, out_r, out_c, k_r, k_c, in_ch), dtype=X.dtype)\n    for kr in range(k_r):\n        row_start = kr * (dil + 1)\n        row_end = row_start + stride * out_r\n        row_slice = slice(row_start, row_end, stride)\n        for kc in range(k_c):\n            col_start = kc * (dil + 1)\n            col_end = col_start + stride * out_c\n            col_slice = slice(col_start, col_end, stride)\n            cols[:, :, :, kr, kc, :] = X_pad[:, row_slice, col_slice, :]\n    cols_2d = cols.reshape(n_ex * out_r * out_c, -1)\n    W_col = W.reshape(-1, out_ch)\n    out_mat = cols_2d @ W_col\n    out = out_mat.reshape(n_ex, out_r, out_c, out_ch)\n    return out.tolist()"}
{"task_id": 355, "completion_id": 0, "solution": "import numpy as np\ndef _deterministic_sign_flip(components: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Flip the sign of each component so that the first non\u2013zero loading is \u2265 0.\n    The operation is done in-place and the resulting array is also returned.\n    \"\"\"\n    for i in range(components.shape[1]):\n        col = components[:, i]\n        for v in col:\n            if abs(v) > 1e-12:\n                if v < 0:\n                    components[:, i] *= -1\n                break\n    return components\ndef pca_transform(data: np.ndarray, n_components: int, solver: str='svd') -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis with a deterministic sign convention.\"\"\"\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array (samples \u00d7 features).')\n    if n_components < 1:\n        raise ValueError('`n_components` must be at least 1.')\n    if solver not in {'svd', 'eigen'}:\n        raise ValueError(\"`solver` must be either 'svd' or 'eigen'.\")\n    (n_samples, n_features) = data.shape\n    n_components = min(n_components, n_features)\n    X = data.astype(float) - data.mean(axis=0, keepdims=True)\n    if solver == 'svd':\n        (_, singular_vals, Vt) = np.linalg.svd(X, full_matrices=False)\n        components = Vt.T[:, :n_components]\n        expl_var = singular_vals ** 2 / (n_samples - 1)\n    else:\n        cov = np.cov(X, rowvar=False, ddof=1)\n        (eig_vals, eig_vecs) = np.linalg.eigh(cov)\n        idx = eig_vals.argsort()[::-1]\n        (eig_vals, eig_vecs) = (eig_vals[idx], eig_vecs[:, idx])\n        components = eig_vecs[:, :n_components]\n        expl_var = eig_vals\n    total_variance = expl_var.sum()\n    explained_variance_ratio = (expl_var / total_variance)[:n_components]\n    components = _deterministic_sign_flip(components)\n    scores = X @ components\n    scores_rounded = np.round(scores, 4).tolist()\n    evr_rounded = np.round(explained_variance_ratio, 4).tolist()\n    return (scores_rounded, evr_rounded)"}
{"task_id": 356, "completion_id": 0, "solution": "def leaf_predict(leaf: 'Leaf', classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                    classification tree; otherwise treat it as regression.\n\n    Returns\n    -------\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        probs = list(leaf.value)\n        best_idx = 0\n        best_val = probs[0]\n        for (idx, val) in enumerate(probs[1:], start=1):\n            if val > best_val:\n                best_val = val\n                best_idx = idx\n        return best_idx\n    return leaf.value"}
{"task_id": 357, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data: np.ndarray, K: int, max_iters: int=100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array of shape (n_samples, n_features).\n    K : int\n        Number of clusters.\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Sorted centroids, rounded to four decimals.\n    \"\"\"\n    data = np.asarray(data, dtype=float)\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array')\n    (n_samples, n_features) = data.shape\n    if K < 1 or K > n_samples:\n        raise ValueError('K must be between 1 and the number of samples')\n    if K == 1:\n        centroid = np.mean(data, axis=0, keepdims=True)\n        centroid = np.round(centroid, 4)\n        return centroid.tolist()\n    centroids = data[:K].astype(float, copy=True)\n    tol = 1e-06\n    for _ in range(max_iters):\n        diff = data[:, None, :] - centroids[None, :, :]\n        dists = np.sum(diff ** 2, axis=2)\n        labels = np.argmin(dists, axis=1)\n        new_centroids = centroids.copy()\n        for k in range(K):\n            members = data[labels == k]\n            if members.size:\n                new_centroids[k] = members.mean(axis=0)\n        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) <= tol):\n            centroids = new_centroids\n            break\n        centroids = new_centroids\n    sort_keys = tuple((centroids[:, i] for i in reversed(range(n_features))))\n    order = np.lexsort(sort_keys)\n    centroids_sorted = np.round(centroids[order], 4)\n    return centroids_sorted.tolist()"}
{"task_id": 362, "completion_id": 0, "solution": "import numpy as np\ndef row_stochastic_matrix(data: list[list[int | float]] | 'np.ndarray') -> list[list[float]] | int:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row is divided by its own sum so that every row finally sums to 1.\n    If any row has a sum of exactly 0 normalisation is impossible and \u20131\n    is returned.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The row-normalised matrix (rounded to 4 decimals) or \u20131 when a\n        zero-sum row is encountered.\n    \"\"\"\n    arr = np.asarray(data, dtype=float)\n    if arr.ndim != 2:\n        raise ValueError('Input must be a 2-D structure')\n    row_sums = arr.sum(axis=1)\n    if np.any(row_sums == 0):\n        return -1\n    stochastic = arr / row_sums[:, None]\n    return np.round(stochastic, 4).tolist()"}
{"task_id": 363, "completion_id": 0, "solution": "from typing import Any, Dict, Iterable, Tuple as PyTuple\nclass Space:\n    pass\nclass Box(Space):\n\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\nclass Tuple(Space):\n\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\nclass Dict(Space):\n\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite\n    (`Tuple` or `Dict`) spaces the space is continuous only if **all** its\n    immediate sub-spaces are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: True if the action space is known to be composite.\n        tuple_obs:    True if the observation space is known to be composite.\n\n    Returns:\n        A tuple (cont_action, cont_obs) with the continuity result for the\n        action and observation spaces, respectively.\n    \"\"\"\n\n    def is_simple_box(space: Space) -> bool:\n        \"\"\"Return True only when `space` itself is a `Box`.\"\"\"\n        return isinstance(space, Box)\n\n    def all_subspaces_box(space: Space) -> bool:\n        \"\"\"Return True if every direct sub-space of `space` is a `Box`.\"\"\"\n        if isinstance(space, Tuple):\n            subspaces = space.spaces\n        elif isinstance(space, Dict):\n            subspaces = space.spaces.values()\n        else:\n            return False\n        return all((isinstance(s, Box) for s in subspaces))\n    if tuple_action:\n        cont_action = all_subspaces_box(env.action_space)\n    else:\n        cont_action = is_simple_box(env.action_space)\n    if tuple_obs:\n        cont_obs = all_subspaces_box(env.observation_space)\n    else:\n        cont_obs = is_simple_box(env.observation_space)\n    return (cont_action, cont_obs)"}
{"task_id": 369, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        raise ValueError('`updates` must contain at least one tree output.')\n    stacked_updates = np.stack(updates, axis=0)\n    running_pred = -learning_rate * stacked_updates.sum(axis=0)\n    if regression:\n        return np.round(running_pred.astype(float), 4)\n    logits = running_pred\n    logits -= logits.max(axis=1, keepdims=True)\n    exp_logits = np.exp(logits)\n    probas = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n    return probas.argmax(axis=1)"}
{"task_id": 371, "completion_id": 0, "solution": "import numpy as np\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int=0) -> list:\n    \"\"\"1-D cross-correlation (a.k.a. \u201cconvolution\u201d in DL frameworks).\n\n    Parameters\n    ----------\n    X : ndarray, shape (batch, signal_length, in_channels)\n        Input signals.\n    W : ndarray, shape (kernel_width, in_channels, out_channels)\n        Convolution kernels (not flipped \u2013 cross-correlation).\n    stride : int\n        Step size of the sliding window.\n    pad : int | tuple(int, int) | \"same\"\n        Zero padding to the left / right of the signal.\n    dilation : int, default 0\n        Number of zeros inserted *between* kernel elements\n        (0 \u21d2 normal, contiguous kernel).\n\n    Returns\n    -------\n    list\n        Convolved output with shape\n        (batch, output_length, out_channels), converted via ``tolist()``.\n    \"\"\"\n    if X.ndim != 3:\n        raise ValueError('X must have shape (batch, length, in_ch).')\n    if W.ndim != 3:\n        raise ValueError('W must have shape (k_width, in_ch, out_ch).')\n    if X.shape[2] != W.shape[1]:\n        raise ValueError('Input-channel dimensions of X and W differ.')\n    if stride <= 0:\n        raise ValueError('stride must be a positive integer.')\n    if dilation < 0:\n        raise ValueError('dilation must be non-negative.')\n    (batch, L_in, C_in) = X.shape\n    (K, _, C_out) = W.shape\n    eff_K = (K - 1) * (dilation + 1) + 1\n    if isinstance(pad, str):\n        pad = pad.lower()\n    if pad == 'same':\n        out_len = int(np.ceil(L_in / stride))\n        total_pad = max(0, (out_len - 1) * stride + eff_K - L_in)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n    elif isinstance(pad, int):\n        p_left = p_right = int(pad)\n    elif isinstance(pad, (tuple, list)) and len(pad) == 2 and all((isinstance(x, int) for x in pad)):\n        (p_left, p_right) = pad\n    else:\n        raise ValueError('pad must be int, 2-tuple or \"same\".')\n    if p_left < 0 or p_right < 0:\n        raise ValueError('Padding values must be non-negative.')\n    X_pad = np.pad(X, ((0, 0), (p_left, p_right), (0, 0)), mode='constant')\n    L_pad = L_in + p_left + p_right\n    out_len = (L_pad - eff_K) // stride + 1\n    if out_len <= 0:\n        return np.empty((batch, 0, C_out)).tolist()\n    start_idx = np.arange(out_len) * stride\n    kernel_offset = np.arange(K) * (dilation + 1)\n    idx = start_idx[:, None] + kernel_offset[None, :]\n    windows = X_pad[:, idx, :]\n    out = np.tensordot(windows, W, axes=([2, 3], [0, 1]))\n    return out.tolist()"}
{"task_id": 373, "completion_id": 0, "solution": "import numpy as np\ndef gini(y: list[int] | 'np.ndarray') -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    y = np.asarray(y).ravel()\n    if y.size == 0:\n        return 0.0\n    (_, counts) = np.unique(y, return_counts=True)\n    probs = counts / counts.sum()\n    impurity = 1.0 - np.sum(probs ** 2)\n    return float(np.round(impurity, 4))"}
{"task_id": 374, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float=0.01, n_iterations: int=10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple-linear\u2010regression model by batch gradient descent.\n\n    Parameters\n    ----------\n    X : np.ndarray (m \u00d7 n)\n        Feature matrix.\n    y : np.ndarray (m,)\n        Target vector.\n    learning_rate : float, default 0.01\n        Step size for gradient descent.\n    n_iterations : int, default 10_000\n        Number of gradient\u2010descent steps.\n\n    Returns\n    -------\n    (weights_list, bias) : tuple[list[float], float]\n        Learned parameters rounded to 4 decimal places.\n        If X and y have inconsistent sample sizes, returns -1.\n    \"\"\"\n    if X.ndim != 2:\n        X = np.atleast_2d(X)\n    y = np.ravel(y)\n    (m, n) = X.shape\n    if m != y.shape[0] or m == 0:\n        return -1\n    theta = np.zeros(n, dtype=float)\n    b = 0.0\n    for _ in range(n_iterations):\n        y_hat = X @ theta + b\n        error = y_hat - y\n        dw = 2 / m * (X.T @ error)\n        db = 2 / m * np.sum(error)\n        theta -= learning_rate * dw\n        b -= learning_rate * db\n    theta_rounded = np.round(theta, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return (theta_rounded, b_rounded)"}
{"task_id": 376, "completion_id": 0, "solution": "import numpy as np\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if and only if it is square and equal (within\n    numerical tolerance) to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists of numbers.\n\n    Returns:\n        True if *X* is symmetric, otherwise False.\n    \"\"\"\n    if not X:\n        return False\n    n_rows = len(X)\n    for row in X:\n        if len(row) != n_rows:\n            return False\n    A = np.asarray(X, dtype=float)\n    return np.allclose(A, A.T, atol=1e-08, rtol=0.0)"}
{"task_id": 377, "completion_id": 0, "solution": "import numpy as np\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Compute the negative gradient that Gradient Boosting uses when minimising the\n    logistic (binomial-deviance) loss for binary classification.\n    \n    Parameters\n    ----------\n    y : list or 1-D array-like of int (0 or 1)\n        Ground-truth binary labels.\n    f : list or 1-D array-like of float\n        Current prediction scores.\n    \n    Returns\n    -------\n    list\n        Negative gradients for each sample, rounded to 4 decimal places.\n        If `y` contains values other than 0 or 1, returns -1.\n    \"\"\"\n    y_arr = np.asarray(y)\n    f_arr = np.asarray(f, dtype=float)\n    if not np.all(np.isin(y_arr, [0, 1])):\n        return -1\n    y_prime = 2 * y_arr - 1\n    g = y_prime / (1.0 + np.exp(y_prime * f_arr))\n    return np.round(g, 4).tolist()"}
{"task_id": 380, "completion_id": 0, "solution": "import numpy as np\n_ALPHA = 1.6732632423543772\n_SCALE = 1.0507009873554805\ndef selu(x, order: int=0):\n    \"\"\"Compute the SELU activation or its first / second derivative.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Works for scalars and any-shaped arrays/lists.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 d(SELU)/dx\n        2 \u2192 d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    float | list\n        Result rounded to 4 decimals.  For array-like inputs the shape is\n        preserved and a Python list is returned; for scalar input a float\n        is returned.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=np.float64)\n    pos_mask = x_arr > 0\n    neg_mask = ~pos_mask\n    if order == 0:\n        out = np.empty_like(x_arr)\n        out[pos_mask] = _SCALE * x_arr[pos_mask]\n        out[neg_mask] = _SCALE * _ALPHA * (np.exp(x_arr[neg_mask]) - 1)\n    elif order == 1:\n        out = np.empty_like(x_arr)\n        out[pos_mask] = _SCALE\n        out[neg_mask] = _SCALE * _ALPHA * np.exp(x_arr[neg_mask])\n    elif order == 2:\n        out = np.empty_like(x_arr)\n        out[pos_mask] = 0.0\n        out[neg_mask] = _SCALE * _ALPHA * np.exp(x_arr[neg_mask])\n    else:\n        raise ValueError('order must be 0, 1 or 2')\n    rounded = np.round(out, 4)\n    return rounded.tolist()"}
{"task_id": 387, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float=0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n = y.size\n    if n == 0:\n        return []\n    pred = np.full(n, y.mean())\n    for _ in range(n_estimators):\n        residuals = y - pred\n        if np.all(x == x[0]):\n            stump_out = np.full(n, residuals.mean())\n        else:\n            order = np.argsort(x)\n            xs = x[order]\n            rs = residuals[order]\n            csum = np.cumsum(rs)\n            csum_sq = np.cumsum(rs ** 2)\n            total_sum = csum[-1]\n            total_sq = csum_sq[-1]\n            k_left = np.arange(1, n)\n            k_right = n - k_left\n            sum_left = csum[:-1]\n            sum_sq_left = csum_sq[:-1]\n            sum_right = total_sum - sum_left\n            sum_sq_right = total_sq - sum_sq_left\n            mean_left = sum_left / k_left\n            mean_right = sum_right / k_right\n            sse_left = sum_sq_left - k_left * mean_left ** 2\n            sse_right = sum_sq_right - k_right * mean_right ** 2\n            total_sse = sse_left + sse_right\n            valid = xs[:-1] != xs[1:]\n            if not np.any(valid):\n                stump_out = np.full(n, residuals.mean())\n            else:\n                best_pos = np.argmin(np.where(valid, total_sse, np.inf))\n                thr = (xs[best_pos] + xs[best_pos + 1]) / 2.0\n                mu_left = mean_left[best_pos]\n                mu_right = mean_right[best_pos]\n                stump_out = np.where(x <= thr, mu_left, mu_right)\n        pred += learning_rate * stump_out\n    return np.round(pred, 4).tolist()"}
{"task_id": 394, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef rmsprop_update(w: np.ndarray, grad: np.ndarray, Eg: np.ndarray | None=None, learning_rate: float=0.01, rho: float=0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` is used.\n    learning_rate : float, optional\n        Step size (\u03b1). Default is 0.01.\n    rho : float, optional\n        Decay rate (\u03c1). Default is 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        Tuple ``(w_next, Eg_next)`` \u2013 both rounded to four decimal places and\n        converted to regular Python lists.\n    \"\"\"\n    if Eg is None:\n        Eg = np.zeros_like(grad, dtype=float)\n    epsilon = 1e-08\n    Eg_next = rho * Eg + (1.0 - rho) * grad ** 2\n    w_next = w - learning_rate * grad / np.sqrt(Eg_next + epsilon)\n    w_next_rounded = np.round(w_next, 4).tolist()\n    Eg_next_rounded = np.round(Eg_next, 4).tolist()\n    return (w_next_rounded, Eg_next_rounded)"}
{"task_id": 398, "completion_id": 0, "solution": "import numpy as np\ndef elu(x, alpha: float=0.1, derivative: bool=False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient element-wise.\n\n    Args:\n        x: 1-D list or NumPy array of numeric values.\n        alpha: Positive ELU hyper-parameter (default 0.1).\n        derivative: If True, return the gradient; otherwise return the activation.\n\n    Returns:\n        Python list of floats rounded to 4 decimal places.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=float)\n    pos_mask = x_arr >= 0\n    neg_mask = ~pos_mask\n    if not derivative:\n        out = x_arr.copy()\n        out[neg_mask] = alpha * (np.exp(x_arr[neg_mask]) - 1)\n    else:\n        out = np.ones_like(x_arr)\n        out[neg_mask] = alpha * np.exp(x_arr[neg_mask])\n    return np.round(out, 4).tolist()"}
{"task_id": 411, "completion_id": 0, "solution": "import numpy as np\nfrom numbers import Integral, Real\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute simple statistics of action / observation trajectories.\"\"\"\n\n    def is_iterable(x):\n        \"\"\"Return True if *x* is a list/tuple/ndarray with length.\"\"\"\n        return isinstance(x, (list, tuple, np.ndarray)) and (not np.isscalar(x))\n\n    def analyse(collection):\n        \"\"\"\n        Inspect a list of (possibly-vector) values and return\n        tuple_flag  \u2026 at least one element is an iterable\n        multi_dim   \u2026 dimensionality > 1\n        continuous  \u2026 at least one value is a non-integer float\n        n_per_dim   \u2026 list with #unique values per dimension\n        dim         \u2026 dimensionality\n        ids         \u2026 list with sorted unique ids per dimension\n        \"\"\"\n        if len(collection) == 0:\n            raise ValueError('Empty trajectory supplied.')\n        tuple_flag = any((is_iterable(x) for x in collection))\n        if tuple_flag:\n            for sample in collection:\n                if is_iterable(sample):\n                    dim = len(sample)\n                    break\n        else:\n            dim = 1\n        multi_dim = dim > 1\n        uniq_sets = [set() for _ in range(dim)]\n        continuous = False\n        for sample in collection:\n            if tuple_flag:\n                values = list(sample)\n                if len(values) != dim:\n                    raise ValueError('Inconsistent dimensionality in data.')\n            else:\n                values = [sample]\n            for (d, val) in enumerate(values):\n                uniq_sets[d].add(val)\n                if not continuous:\n                    if isinstance(val, (Real, np.floating, np.integer)):\n                        if isinstance(val, (Integral, np.integer)):\n                            continue\n                        if not float(val).is_integer():\n                            continuous = True\n                    else:\n                        continuous = True\n        ids = [sorted(list(s)) for s in uniq_sets]\n        n_per_dim = [len(s) for s in uniq_sets]\n        return (tuple_flag, multi_dim, continuous, n_per_dim, dim, ids)\n    (tuple_a, multi_a, cont_a, n_a, dim_a, ids_a) = analyse(actions)\n    (tuple_o, multi_o, cont_o, n_o, dim_o, ids_o) = analyse(observations)\n    return {'tuple_actions': tuple_a, 'tuple_observations': tuple_o, 'multidim_actions': multi_a, 'multidim_observations': multi_o, 'continuous_actions': cont_a, 'continuous_observations': cont_o, 'n_actions_per_dim': n_a, 'action_dim': dim_a, 'n_obs_per_dim': n_o, 'obs_dim': dim_o, 'action_ids': ids_a, 'obs_ids': ids_o}"}
{"task_id": 413, "completion_id": 0, "solution": "import numpy as np\nclass _TreeNode:\n    \"\"\"A single node in a CART regression tree.\"\"\"\n    __slots__ = ('is_leaf', 'value', 'feature', 'threshold', 'left', 'right')\n\n    def __init__(self, *, value=None, feature=None, threshold=None, left=None, right=None, is_leaf=False):\n        self.is_leaf = is_leaf\n        self.value = value\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\nclass _CARTRegressor:\n    \"\"\"Very small, depth\u2013limited CART regressor (MSE criterion).\"\"\"\n\n    def __init__(self, max_depth: int=3):\n        self.max_depth = max_depth\n        self.root: _TreeNode | None = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        self.root = self._grow(X, y, depth=0)\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        preds = np.empty(X.shape[0], dtype=float)\n        for (i, x) in enumerate(X):\n            preds[i] = self._predict_row(self.root, x)\n        return preds\n\n    def _predict_row(self, node: _TreeNode, x: np.ndarray) -> float:\n        while not node.is_leaf:\n            node = node.left if x[node.feature] <= node.threshold else node.right\n        return node.value\n\n    def _grow(self, X: np.ndarray, y: np.ndarray, depth: int) -> _TreeNode:\n        (n_samples, n_feats) = X.shape\n        if depth >= self.max_depth or n_samples <= 1 or np.all(y == y[0]):\n            return _TreeNode(value=float(y.mean()), is_leaf=True)\n        best_sse = np.var(y) * n_samples\n        (best_feat, best_thr, best_split) = (None, None, None)\n        for feat in range(n_feats):\n            x_col = X[:, feat]\n            sorted_idx = np.argsort(x_col)\n            (x_sorted, y_sorted) = (x_col[sorted_idx], y[sorted_idx])\n            uniq_vals = np.unique(x_sorted)\n            if uniq_vals.size == 1:\n                continue\n            thr_candidates = (uniq_vals[:-1] + uniq_vals[1:]) / 2.0\n            y_cumsum = np.cumsum(y_sorted)\n            y_sq_cumsum = np.cumsum(y_sorted ** 2)\n            for (idx, thr) in enumerate(thr_candidates, start=1):\n                n_left = np.searchsorted(x_sorted, thr, side='right')\n                if n_left == 0 or n_left == n_samples:\n                    continue\n                sum_y_left = y_cumsum[n_left - 1]\n                sum_yy_left = y_sq_cumsum[n_left - 1]\n                n_l = n_left\n                mse_left = sum_yy_left - sum_y_left ** 2 / n_l\n                sum_y_right = y_cumsum[-1] - sum_y_left\n                sum_yy_right = y_sq_cumsum[-1] - sum_yy_left\n                n_r = n_samples - n_l\n                mse_right = sum_yy_right - sum_y_right ** 2 / n_r\n                sse_split = mse_left + mse_right\n                if sse_split < best_sse - 1e-07:\n                    best_sse = sse_split\n                    best_feat = feat\n                    best_thr = thr\n                    best_split = (n_left, sorted_idx)\n        if best_feat is None:\n            return _TreeNode(value=float(y.mean()), is_leaf=True)\n        (split_idx, sorted_idx) = best_split\n        x_sorted_global = X[sorted_idx, best_feat]\n        mask_left = x_sorted_global <= best_thr\n        left_idx = sorted_idx[mask_left]\n        right_idx = sorted_idx[~mask_left]\n        left_child = self._grow(X[left_idx], y[left_idx], depth + 1)\n        right_child = self._grow(X[right_idx], y[right_idx], depth + 1)\n        return _TreeNode(feature=best_feat, threshold=best_thr, left=left_child, right=right_child, value=float(y.mean()), is_leaf=False)\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int=10, learning_rate: float=0.1, max_depth: int=3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor (simplified).\"\"\"\n    init_pred = y_train.mean()\n    y_hat_train = np.full_like(y_train, init_pred, dtype=float)\n    y_hat_test = np.full(X_test.shape[0], init_pred, dtype=float)\n    for _ in range(n_estimators):\n        residuals = y_train - y_hat_train\n        tree = _CARTRegressor(max_depth=max_depth)\n        tree.fit(X_train, residuals)\n        y_hat_train += learning_rate * tree.predict(X_train)\n        y_hat_test += learning_rate * tree.predict(X_test)\n    return np.round(y_hat_test, 4).tolist()"}
{"task_id": 416, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Evaluate the multivariate normal (Gaussian) probability-density function for\n    every sample contained in `X`.\n\n    Parameters\n    ----------\n    X : array_like, shape (n_samples, n_features)  or  (n_samples,)\n        Data points at which the PDF will be evaluated.  If `X` is one\u2013dimensional\n        it is treated as having shape (n_samples, 1).\n    mean : array_like, shape (n_features,)\n        Mean vector (\u03bc) of the distribution.\n    cov : array_like, shape (n_features, n_features)\n        Positive-definite covariance matrix (\u03a3) of the distribution.\n\n    Returns\n    -------\n    list\n        A list whose *i-th* element is the PDF value for `X[i]`, rounded to\n        four decimal places.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    mean = np.asarray(mean, dtype=float)\n    cov = np.asarray(cov, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    if mean.shape[0] != n_features:\n        raise ValueError('Mean vector length must equal the number of features.')\n    if cov.shape != (n_features, n_features):\n        raise ValueError('Covariance matrix shape must be (n_features, n_features).')\n    inv_cov = np.linalg.inv(cov)\n    det_cov = np.linalg.det(cov)\n    if det_cov <= 0:\n        raise ValueError('Covariance matrix must be positive-definite (determinant > 0).')\n    coeff = 1.0 / math.sqrt((2 * math.pi) ** n_features * det_cov)\n    diff = X - mean\n    mahal = np.einsum('ij,jk,ik->i', diff, inv_cov, diff)\n    exponent = -0.5 * mahal\n    pdf_vals = coeff * np.exp(exponent)\n    return [round(val, 4) for val in pdf_vals.tolist()]"}
{"task_id": 419, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef bayesian_linear_regression_pred(X: np.ndarray, y: np.ndarray, X_new: np.ndarray, mu: float | np.ndarray=0.0, sigma: float=1.0, V: float | np.ndarray | None=None, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Bayesian linear regression \u2013 MAP predictive mean (rounded to 4 dp).\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    X_new = np.asarray(X_new, dtype=float)\n    if fit_intercept:\n        ones_train = np.ones((X.shape[0], 1))\n        ones_test = np.ones((X_new.shape[0], 1))\n        X = np.hstack((ones_train, X))\n        X_new = np.hstack((ones_test, X_new))\n    (N, D) = X.shape\n    if np.isscalar(mu):\n        mu_vec = np.full(D, mu, dtype=float)\n    else:\n        mu_vec = np.asarray(mu, dtype=float).ravel()\n        if mu_vec.size != D:\n            raise ValueError('`mu` has incompatible length.')\n    if V is None:\n        V_inv = np.eye(D)\n    elif np.isscalar(V):\n        if V == 0:\n            raise ValueError('Scalar `V` must be non-zero.')\n        V_inv = np.eye(D) / float(V)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 1:\n            if np.any(V_arr == 0):\n                raise ValueError('Zero on the diagonal of `V` is not allowed.')\n            V_inv = np.diag(1.0 / V_arr)\n        else:\n            if V_arr.shape != (D, D):\n                raise ValueError('`V` shape incompatible with number of features.')\n            V_inv = np.linalg.inv(V_arr)\n    XtX = X.T @ X\n    A = V_inv + XtX\n    Sigma_p = np.linalg.inv(A)\n    rhs = V_inv @ mu_vec + X.T @ y\n    mu_p = Sigma_p @ rhs\n    y_pred = X_new @ mu_p\n    return [round(float(val), 4) for val in y_pred]"}
{"task_id": 423, "completion_id": 0, "solution": "import numpy as np\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute Root Mean Squared Logarithmic Error (RMSLE).\n\n    Returns\n    -------\n    float\n        RMSLE rounded to four decimal places, or -1 if validation fails.\n    \"\"\"\n    if not isinstance(actual, (list, tuple, np.ndarray)) or not isinstance(predicted, (list, tuple, np.ndarray)) or len(actual) != len(predicted) or (len(actual) == 0):\n        return -1\n    try:\n        actual_arr = np.asarray(actual, dtype=float)\n        predicted_arr = np.asarray(predicted, dtype=float)\n    except (ValueError, TypeError):\n        return -1\n    if np.any(actual_arr < 0) or np.any(predicted_arr < 0):\n        return -1\n    log_diff = np.log1p(predicted_arr) - np.log1p(actual_arr)\n    rmsle = np.sqrt(np.mean(np.square(log_diff)))\n    return round(float(rmsle), 4)"}
{"task_id": 428, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef hann(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine\u2010sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places. Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    build_len = window_len if symmetric else window_len + 1\n    n = np.arange(build_len, dtype=float)\n    denom = build_len - 1\n    window = 0.5 - 0.5 * np.cos(2.0 * math.pi * n / denom)\n    if not symmetric:\n        window = window[:-1]\n    window = np.round(window, 4)\n    return window.tolist()"}
{"task_id": 433, "completion_id": 0, "solution": "import numpy as np\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute Xavier/Glorot fan-in and fan-out values for a weight tensor.\n\n    Args:\n        shape: Tensor shape. For 4-D convolutional kernels the expected layout\n               is (out_channels, in_channels, kernel_height, kernel_width).\n\n    Returns:\n        A tuple (fan_in, fan_out) as floats.\n    \"\"\"\n    if len(shape) == 4:\n        (out_c, in_c, k_h, k_w) = shape[:4]\n        receptive_field_size = k_h * k_w\n        fan_in = in_c * receptive_field_size\n        fan_out = out_c * receptive_field_size\n    else:\n        (fan_in, fan_out) = (shape[0], shape[1])\n    return (float(fan_in), float(fan_out))"}
{"task_id": 435, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_regressor(X: list[list[float]], y: list[float], n_estimators: int=10, learning_rate: float=0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear-regression weak learners and squared-error loss.\n    All returned numbers are rounded to 4 decimal places and packed into\n    a plain Python list.\n    \"\"\"\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(-1, 1)\n    m = y_arr.shape[0]\n    y_mean = y_arr.mean()\n    if n_estimators <= 0 or learning_rate == 0:\n        return np.round(np.full(m, y_mean), 4).tolist()\n    current_pred = np.full(m, y_mean, dtype=float)\n    ones = np.ones((m, 1), dtype=float)\n    X_design = np.concatenate((ones, X_arr), axis=1)\n    for _ in range(n_estimators):\n        residuals = y_arr - current_pred\n        (beta, _, _, _) = np.linalg.lstsq(X_design, residuals, rcond=None)\n        h_pred = X_design @ beta\n        current_pred += learning_rate * h_pred\n    return np.round(current_pred, 4).tolist()"}
{"task_id": 437, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x, order: int=0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in (0, 1, 2):\n        return -1\n    arr = np.asarray(x, dtype=float)\n    sig = 1.0 / (1.0 + np.exp(-arr))\n    if order == 0:\n        res = sig\n    elif order == 1:\n        res = sig * (1.0 - sig)\n    else:\n        res = sig * (1.0 - sig) * (1.0 - 2.0 * sig)\n    res = np.round(res, 4)\n    if res.shape == ():\n        return float(res)\n    return res.tolist()"}
{"task_id": 438, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None=None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded representation of *y*, or ``-1`` when the given\n        *num_classes* is smaller than required.\n    \"\"\"\n    y_arr = np.asarray(y, dtype=int).ravel()\n    if y_arr.size == 0:\n        inferred_classes = 0 if num_classes is None else num_classes\n        return [] if inferred_classes == 0 else [[] for _ in range(0)]\n    max_label = int(y_arr.max())\n    if num_classes is None:\n        num_classes = max_label + 1\n    elif num_classes < max_label + 1:\n        return -1\n    one_hot = np.zeros((y_arr.shape[0], num_classes), dtype=int)\n    one_hot[np.arange(y_arr.shape[0]), y_arr] = 1\n    return one_hot.tolist()"}
{"task_id": 439, "completion_id": 0, "solution": "import itertools\nimport math\nfrom collections import defaultdict\nfrom typing import Hashable, List, Tuple\ndef apriori_frequent_itemsets(transactions: List[List[Hashable]], min_sup: float) -> List[Tuple]:\n    \"\"\"Find all frequent item-sets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        The data base: one list per transaction, containing *hashable* items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction in (0, 1].\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets ordered first by their length and then\n        lexicographically inside each length block; inside every tuple the items\n        themselves appear in ascending (lexicographic) order.\n    \"\"\"\n    if not 0 < min_sup <= 1:\n        raise ValueError('min_sup must be in the range (0, 1].')\n    n_transactions = len(transactions)\n    if n_transactions == 0:\n        return []\n    transactions = [set(t) for t in transactions]\n\n    def is_frequent(count: int) -> bool:\n        \"\"\"True if empirical support \u2265 min_sup.\"\"\"\n        return count >= min_sup * n_transactions - 1e-12\n    item_counts: defaultdict[Hashable, int] = defaultdict(int)\n    for t in transactions:\n        for item in t:\n            item_counts[item] += 1\n    L1 = [tuple([item]) for (item, cnt) in item_counts.items() if is_frequent(cnt)]\n    L1.sort()\n    frequent_itemsets: list[tuple] = []\n    frequent_itemsets.extend(L1)\n    prev_L = L1\n    k = 2\n    while prev_L:\n        prev_L_len = len(prev_L)\n        prev_L_set = set(prev_L)\n        Ck: set[tuple] = set()\n        for i in range(prev_L_len):\n            for j in range(i + 1, prev_L_len):\n                (a, b) = (prev_L[i], prev_L[j])\n                if a[:-1] != b[:-1]:\n                    break\n                candidate = tuple(sorted(set(a) | set(b)))\n                if len(candidate) != k:\n                    continue\n                if all((tuple(subset) in prev_L_set for subset in itertools.combinations(candidate, k - 1))):\n                    Ck.add(candidate)\n        if not Ck:\n            break\n        candidate_counts = defaultdict(int)\n        cand_sets = {cand: frozenset(cand) for cand in Ck}\n        for t in transactions:\n            for (cand, cand_set) in cand_sets.items():\n                if cand_set.issubset(t):\n                    candidate_counts[cand] += 1\n        Lk = [cand for (cand, cnt) in candidate_counts.items() if is_frequent(cnt)]\n        Lk.sort()\n        if not Lk:\n            break\n        frequent_itemsets.extend(Lk)\n        prev_L = Lk\n        k += 1\n    return frequent_itemsets"}
{"task_id": 440, "completion_id": 0, "solution": "import numpy as np\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class\u2013probability predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        Shape (n_estimators, n_samples, n_classes).  Each innermost list is a\n        valid probability distribution (sums to 1).\n\n    Returns\n    -------\n    tuple\n        (\n          averaged_probs,      # list[list[float]] shape (n_samples, n_classes)\n          predicted_labels     # list[int]          length == n_samples\n        )\n        `averaged_probs` are rounded to 4 decimal places; `predicted_labels`\n        are obtained with arg-max on the rounded probabilities.  Ties are\n        resolved by the smallest class index (numpy\u2019s default behaviour).\n    \"\"\"\n    if not predictions:\n        raise ValueError('`predictions` must contain at least one estimator.')\n    arr = np.asarray(predictions, dtype=float)\n    if arr.ndim != 3:\n        raise ValueError('`predictions` must be 3-dimensional (n_estimators, n_samples, n_classes).')\n    mean_probs = arr.mean(axis=0)\n    mean_probs = np.round(mean_probs, 4)\n    predicted = np.argmax(mean_probs, axis=1)\n    averaged_probs_list = mean_probs.tolist()\n    predicted_labels_list = predicted.tolist()\n    return (averaged_probs_list, predicted_labels_list)"}
{"task_id": 444, "completion_id": 0, "solution": "import numpy as np\ndef rbf_kernel(X: list[list[int | float]], Y: list[list[int | float]] | None=None, sigma: float | list[float] | None=None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function follows the specifications given in the task description.\n    It returns -1 on invalid input, otherwise a nested list containing the\n    kernel matrix rounded to four decimal places.\n    \"\"\"\n\n    def _to_2d_array(mat):\n        try:\n            arr = np.asarray(mat, dtype=float)\n        except Exception:\n            return None\n        if arr.ndim != 2 or arr.shape[1] == 0:\n            return None\n        return arr\n    X_arr = _to_2d_array(X)\n    if X_arr is None:\n        return -1\n    (n_samples, n_features) = X_arr.shape\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        Y_arr = _to_2d_array(Y)\n        if Y_arr is None or Y_arr.shape[1] != n_features:\n            return -1\n    if sigma is None:\n        sigma_scalar = np.sqrt(n_features / 2.0)\n        if sigma_scalar <= 0:\n            return -1\n        sigma_vec = None\n    elif isinstance(sigma, (int, float, np.floating)):\n        sigma_scalar = float(sigma)\n        if sigma_scalar <= 0:\n            return -1\n        sigma_vec = None\n    else:\n        try:\n            sigma_vec = np.asarray(sigma, dtype=float)\n        except Exception:\n            return -1\n        if sigma_vec.ndim != 1 or sigma_vec.shape[0] != n_features:\n            return -1\n        if np.any(sigma_vec <= 0):\n            return -1\n        sigma_scalar = None\n    if sigma_vec is None:\n        diff = X_arr[:, None, :] - Y_arr[None, :, :]\n        dist2 = np.sum(diff ** 2, axis=-1) / sigma_scalar ** 2\n    else:\n        diff = (X_arr[:, None, :] - Y_arr[None, :, :]) / sigma_vec\n        dist2 = np.sum(diff ** 2, axis=-1)\n    K = np.exp(-0.5 * dist2)\n    return np.round(K, 4).tolist()"}
{"task_id": 446, "completion_id": 0, "solution": "import numpy as np\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find the best 1-level split (decision stump) by information gain.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    (n_samples, n_features) = X.shape\n    (classes, y_indices) = np.unique(y, return_inverse=True)\n    n_classes = len(classes)\n\n    def entropy_from_counts(counts: np.ndarray) -> float:\n        \"\"\"Shannon entropy (base 2) from a 1-D array with class counts.\"\"\"\n        total = counts.sum()\n        if total == 0:\n            return 0.0\n        probs = counts[counts > 0] / total\n        return -np.sum(probs * np.log2(probs))\n\n    def majority_label(counts: np.ndarray) -> int:\n        \"\"\"Most frequent label, ties resolved by smaller label value.\"\"\"\n        max_count = counts.max()\n        winner_idx = np.where(counts == max_count)[0][0]\n        return int(classes[winner_idx])\n    total_counts = np.bincount(y_indices, minlength=n_classes)\n    parent_entropy = entropy_from_counts(total_counts)\n    if parent_entropy == 0.0:\n        maj = majority_label(total_counts)\n        return (-1, None, maj, maj)\n    best_gain = -np.inf\n    best_feature = -1\n    best_threshold = None\n    best_left_label = None\n    best_right_label = None\n    eps = 1e-12\n    for j in range(n_features):\n        col = X[:, j]\n        order = np.argsort(col)\n        sorted_vals = col[order]\n        sorted_y_idx = y_indices[order]\n        left_counts = np.zeros(n_classes, dtype=int)\n        for i in range(n_samples - 1):\n            left_counts[sorted_y_idx[i]] += 1\n            if sorted_vals[i] == sorted_vals[i + 1]:\n                continue\n            thr = (sorted_vals[i] + sorted_vals[i + 1]) / 2.0\n            n_left = i + 1\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:\n                continue\n            right_counts = total_counts - left_counts\n            h_left = entropy_from_counts(left_counts)\n            h_right = entropy_from_counts(right_counts)\n            p_left = n_left / n_samples\n            p_right = 1.0 - p_left\n            gain = parent_entropy - p_left * h_left - p_right * h_right\n            better_gain = gain > best_gain + eps\n            same_gain = abs(gain - best_gain) <= eps\n            update = False\n            if better_gain:\n                update = True\n            elif same_gain:\n                if j < best_feature:\n                    update = True\n                elif j == best_feature and (best_threshold is None or thr < best_threshold - eps):\n                    update = True\n            if update:\n                best_gain = gain\n                best_feature = j\n                best_threshold = thr\n                best_left_label = majority_label(left_counts)\n                best_right_label = majority_label(right_counts)\n    if best_gain <= eps:\n        maj = majority_label(total_counts)\n        return (-1, None, maj, maj)\n    return (best_feature, round(best_threshold, 4), best_left_label, best_right_label)"}
{"task_id": 452, "completion_id": 0, "solution": "import numbers\nimport numpy as np\nfrom typing import List\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split *X* into two subsets according to one feature and a threshold.\n\n    For a numeric *threshold* (int / float) the first subset contains every\n    sample whose value in column *feature_i* is >= threshold, otherwise the\n    first subset contains every sample whose value equals *threshold*.\n\n    The second subset gets all remaining samples.  The original row order is\n    preserved and both subsets are returned as regular Python lists.\n    \"\"\"\n    is_numeric = isinstance(threshold, numbers.Real) and (not isinstance(threshold, bool))\n    (left, right) = ([], [])\n    for row in X:\n        value = row[feature_i]\n        if is_numeric:\n            goes_left = value >= threshold\n        else:\n            goes_left = value == threshold\n        (left if goes_left else right).append(row)\n    left_out = np.asarray(left, dtype=object).tolist() if left else []\n    right_out = np.asarray(right, dtype=object).tolist() if right else []\n    return [left_out, right_out]"}
{"task_id": 453, "completion_id": 0, "solution": "import numpy as np\ndef update_q_values(Q: 'np.ndarray', Q_next: 'np.ndarray', actions: list[int], rewards: list[float], dones: list[bool], gamma: float) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch (see doc-string above).\"\"\"\n    if not 0.0 <= gamma <= 1.0:\n        return -1\n    if not (isinstance(Q, np.ndarray) and isinstance(Q_next, np.ndarray)):\n        return -1\n    if Q.shape != Q_next.shape or Q.ndim != 2:\n        return -1\n    (batch_size, n_actions) = Q.shape\n    if not len(actions) == len(rewards) == len(dones) == batch_size:\n        return -1\n    if any((a < 0 or a >= n_actions for a in actions)):\n        return -1\n    targets = Q.copy()\n    for i in range(batch_size):\n        if dones[i]:\n            target = rewards[i]\n        else:\n            target = rewards[i] + gamma * np.max(Q_next[i])\n        targets[i, actions[i]] = target\n    targets = np.round(targets, 4).tolist()\n    return targets"}
{"task_id": 458, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None=None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes.  If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists (shape: (len(labels), num_classes)) containing\n        0/1 integers, or -1 when:\n            \u2022 any label is negative, or\n            \u2022 `num_classes` is provided but smaller than `max(labels)+1`.\n    \"\"\"\n    try:\n        labels_list = list(labels)\n    except TypeError:\n        return -1\n    if not labels_list:\n        inferred_classes = 0 if num_classes is None else num_classes\n        return [] if inferred_classes >= 0 else -1\n    max_label = -1\n    for lbl in labels_list:\n        if not isinstance(lbl, (int, np.integer)) or lbl < 0:\n            return -1\n        if lbl > max_label:\n            max_label = int(lbl)\n    if num_classes is None:\n        num_classes = max_label + 1\n    elif num_classes < max_label + 1:\n        return -1\n    one_hot: list[list[int]] = [[1 if class_idx == lbl else 0 for class_idx in range(num_classes)] for lbl in labels_list]\n    return one_hot"}
{"task_id": 461, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_regressor(x: list[float], y: list[float], n_estimators: int=200, learning_rate: float=0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\"\"\"\n    x_arr = np.asarray(x, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    if n_estimators <= 0:\n        return np.round(np.full_like(y_arr, y_arr.mean()), 4).tolist()\n    y_pred = np.full_like(y_arr, y_arr.mean(), dtype=float)\n    uniq_x = np.unique(x_arr)\n    for _ in range(n_estimators):\n        residuals = y_arr - y_pred\n        if np.allclose(residuals, 0.0):\n            break\n        best_sse = np.inf\n        best_t = None\n        best_left_mean = best_right_mean = 0.0\n        thresholds = uniq_x[:-1]\n        for t in thresholds:\n            left_mask = x_arr <= t\n            right_mask = ~left_mask\n            if not left_mask.any() or not right_mask.any():\n                continue\n            r_left = residuals[left_mask]\n            r_right = residuals[right_mask]\n            (m_left, m_right) = (r_left.mean(), r_right.mean())\n            sse = np.sum((r_left - m_left) ** 2) + np.sum((r_right - m_right) ** 2)\n            if sse < best_sse:\n                best_sse = sse\n                best_t = t\n                (best_left_mean, best_right_mean) = (m_left, m_right)\n        if best_t is None:\n            stump_pred = np.full_like(residuals, residuals.mean())\n        else:\n            stump_pred = np.where(x_arr <= best_t, best_left_mean, best_right_mean)\n        y_pred += learning_rate * stump_pred\n    return np.round(y_pred, 4).tolist()"}
{"task_id": 471, "completion_id": 0, "solution": "import numpy as np\ndef expected_sarsa_update(q_table: list[list[float]], state: int, action: int, reward: float, next_state: int | None, epsilon: float, lr: float, gamma: float) -> list[list[float]]:\n    \"\"\"One-step tabular Expected-SARSA TD(0) update.\"\"\"\n    n_actions = len(q_table[0])\n    if next_state is None:\n        expected_q = 0.0\n    else:\n        next_row = q_table[next_state]\n        greedy_idx = max(range(n_actions), key=next_row.__getitem__)\n        base_prob = epsilon / n_actions\n        expected_q = 0.0\n        for a in range(n_actions):\n            prob = base_prob\n            if a == greedy_idx:\n                prob += 1.0 - epsilon\n            expected_q += prob * next_row[a]\n    target = reward + gamma * expected_q\n    q_table[state][action] += lr * (target - q_table[state][action])\n    rounded_q = [[round(v, 4) for v in row] for row in q_table]\n    return rounded_q"}
{"task_id": 474, "completion_id": 0, "solution": "import numpy as np\nimport random\nimport numpy as np\nimport random\ndef generate_data(m: int, n_train: int, n_val: int, interval: float, seed: int | None=None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set consisting of `m` rectangular clusters.\n\n    Each class i (0-based index) is located at\n        x \u2208 [(\u230ai/2\u230b+0.1)\u00b710 , (\u230ai/2\u230b+0.9)\u00b710]\n        y \u2208 [((i mod 2)*interval+0.1)\u00b710 , ((i mod 2)*interval+0.9)\u00b710]\n    and is assigned the label  (i \u2212 0.5)\u00b72  (\u2192 \u22121, +1, 3, 5, \u2026).\n\n    All returned coordinates are rounded to 4 decimals.\n    \"\"\"\n    if m < 2:\n        raise ValueError('`m` (number of classes) must be at least 2.')\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    (X_train_parts, X_val_parts) = ([], [])\n    (Y_train_parts, Y_val_parts) = ([], [])\n    for i in range(m):\n        col = i // 2\n        row = i % 2\n        (x_min, x_max) = ((col + 0.1) * 10, (col + 0.9) * 10)\n        (y_min, y_max) = ((row * interval + 0.1) * 10, (row * interval + 0.9) * 10)\n        x_tr = np.random.uniform(x_min, x_max, n_train)\n        y_tr = np.random.uniform(y_min, y_max, n_train)\n        X_train_parts.append(np.column_stack((x_tr, y_tr)))\n        x_va = np.random.uniform(x_min, x_max, n_val)\n        y_va = np.random.uniform(y_min, y_max, n_val)\n        X_val_parts.append(np.column_stack((x_va, y_va)))\n        label = int((i - 0.5) * 2)\n        Y_train_parts.append(np.full(n_train, label, dtype=int))\n        Y_val_parts.append(np.full(n_val, label, dtype=int))\n    X_train = np.round(np.vstack(X_train_parts), 4)\n    X_val = np.round(np.vstack(X_val_parts), 4)\n    Y_train = np.concatenate(Y_train_parts)\n    Y_val = np.concatenate(Y_val_parts)\n    return (X_train, X_val, Y_train, Y_val)"}
{"task_id": 475, "completion_id": 0, "solution": "import numpy as np\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-08):\n    \"\"\"Perform a single Adam optimisation step (see doc-string above).\"\"\"\n    grad_arr = np.asarray(grad, dtype=float)\n    if m_prev is None:\n        m_prev = np.zeros_like(grad_arr, dtype=float)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad_arr, dtype=float)\n    m_new = b1 * m_prev + (1.0 - b1) * grad_arr\n    v_new = b2 * v_prev + (1.0 - b2) * grad_arr ** 2\n    m_hat = m_new / (1.0 - b1 ** t)\n    v_hat = v_new / (1.0 - b2 ** t)\n    w_new = np.asarray(w, dtype=float) - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n    if np.isscalar(w):\n        w_new = w_new.item()\n        m_new = m_new.item()\n        v_new = v_new.item()\n    return (w_new, m_new, v_new)"}
{"task_id": 479, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Optional\ndef epsilon_greedy_bandit(rewards: List[List[float]], epsilon: float=0.05, ev_prior: float=0.5, seed: Optional[int]=None) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    if not rewards:\n        return [round(ev_prior, 4)]\n    n_arms = len(rewards[0])\n    values = np.full(n_arms, float(ev_prior))\n    counts = np.zeros(n_arms, dtype=int)\n    for row in rewards:\n        if np.random.random() < epsilon:\n            arm = np.random.randint(n_arms)\n        else:\n            arm = int(np.argmax(values))\n        r = row[arm]\n        counts[arm] += 1\n        values[arm] += (r - values[arm]) / counts[arm]\n    return [round(v, 4) for v in values]"}
{"task_id": 481, "completion_id": 0, "solution": "import numpy as np\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if data.size == 0:\n        return []\n    n_samples = data.shape[0]\n    eps_sq = eps * eps\n    neighbours: list[np.ndarray] = []\n    for i in range(n_samples):\n        diff = data - data[i]\n        dist_sq = np.einsum('ij,ij->i', diff, diff)\n        neighbours.append(np.where(dist_sq <= eps_sq)[0])\n    UNASSIGNED = -1\n    labels = np.full(n_samples, UNASSIGNED, dtype=int)\n    visited = np.zeros(n_samples, dtype=bool)\n    cluster_id = 0\n    for point_idx in range(n_samples):\n        if visited[point_idx]:\n            continue\n        visited[point_idx] = True\n        neigh = neighbours[point_idx]\n        if neigh.size < min_samples:\n            labels[point_idx] = UNASSIGNED\n            continue\n        labels[point_idx] = cluster_id\n        stack = list(neigh)\n        while stack:\n            current = stack.pop()\n            if not visited[current]:\n                visited[current] = True\n                neigh_current = neighbours[current]\n                if neigh_current.size >= min_samples:\n                    stack.extend(neigh_current.tolist())\n            if labels[current] == UNASSIGNED:\n                labels[current] = cluster_id\n        cluster_id += 1\n    return labels.tolist()"}
{"task_id": 482, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    Parameters\n    ----------\n    y : array_like\n        1-D sequence of non-negative integer labels.\n    num_classes : int or None, optional\n        Number of classes (columns in the output).  If None, it is inferred\n        as max(y) + 1.  Every label must satisfy\n        0 <= label < num_classes, otherwise ValueError is raised.\n\n    Returns\n    -------\n    np.ndarray\n        One-hot encoded matrix of shape (len(y), num_classes) with dtype=int.\n    \"\"\"\n    y_arr = np.asarray(y)\n    if y_arr.ndim == 0:\n        y_arr = y_arr.reshape(1)\n    if y_arr.ndim != 1:\n        raise ValueError('`y` must be a 1-D sequence of integer labels.')\n    try:\n        y_int = y_arr.astype(int, casting='safe', copy=False)\n    except TypeError:\n        raise ValueError('`y` must contain integer values only.')\n    n_samples = y_int.size\n    if n_samples == 0:\n        if num_classes is None:\n            return np.zeros((0, 0), dtype=int)\n        if num_classes <= 0:\n            raise ValueError('`num_classes` must be a positive integer.')\n        return np.zeros((0, num_classes), dtype=int)\n    max_label = int(y_int.max())\n    min_label = int(y_int.min())\n    if num_classes is None:\n        if min_label < 0:\n            raise ValueError('Negative labels are not allowed.')\n        num_classes = max_label + 1\n    else:\n        if num_classes <= 0:\n            raise ValueError('`num_classes` must be a positive integer.')\n        if min_label < 0 or max_label >= num_classes:\n            raise ValueError('All labels must satisfy 0 <= label < num_classes.')\n    one_hot = np.zeros((n_samples, num_classes), dtype=int)\n    rows = np.arange(n_samples)\n    one_hot[rows, y_int] = 1\n    return one_hot"}
{"task_id": 485, "completion_id": 0, "solution": "import numpy as np\ndef regression_tree_predict(X_train: list[float | int], y_train: list[float | int], X_test: list[float | int], max_depth: int=3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\"\"\"\n\n    def build(x: np.ndarray, y: np.ndarray, depth: int):\n        if depth == max_depth or x.size < 2 or np.all(x == x[0]):\n            return {'leaf': True, 'value': float(y.mean())}\n        order = np.argsort(x)\n        (xs, ys) = (x[order], y[order])\n        n = xs.size\n        csum = np.cumsum(ys)\n        csum2 = np.cumsum(ys ** 2)\n        (total_sum, total_sum2) = (csum[-1], csum2[-1])\n        (best_sse, best_idx) = (np.inf, None)\n        for i in range(n - 1):\n            if xs[i] == xs[i + 1]:\n                continue\n            (n_l, n_r) = (i + 1, n - (i + 1))\n            (sum_l, sum_r) = (csum[i], total_sum - csum[i])\n            (sum2_l, sum2_r) = (csum2[i], total_sum2 - csum2[i])\n            sse_l = sum2_l - sum_l ** 2 / n_l\n            sse_r = sum2_r - sum_r ** 2 / n_r\n            sse = sse_l + sse_r\n            if sse < best_sse:\n                (best_sse, best_idx) = (sse, i)\n        if best_idx is None:\n            return {'leaf': True, 'value': float(y.mean())}\n        thr = (xs[best_idx] + xs[best_idx + 1]) / 2.0\n        left_mask = x <= thr\n        right_mask = ~left_mask\n        return {'leaf': False, 'thr': float(thr), 'left': build(x[left_mask], y[left_mask], depth + 1), 'right': build(x[right_mask], y[right_mask], depth + 1)}\n\n    def predict_one(node, x_val: float):\n        while not node['leaf']:\n            node = node['left'] if x_val <= node['thr'] else node['right']\n        return node['value']\n    if len(X_train) == 0:\n        return []\n    X_arr = np.asarray(X_train, dtype=float)\n    y_arr = np.asarray(y_train, dtype=float)\n    tree = build(X_arr, y_arr, depth=0)\n    return [predict_one(tree, float(x)) for x in X_test]"}
{"task_id": 490, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    arr = np.asarray(x)\n    if arr.size == 0:\n        return []\n    arr = arr.ravel()\n    diag_matrix = np.diag(arr).tolist()\n    return diag_matrix"}
{"task_id": 491, "completion_id": 0, "solution": "from collections import Counter, defaultdict\nfrom itertools import combinations\nclass _FPNode:\n    __slots__ = ('item', 'count', 'parent', 'children', 'link')\n\n    def __init__(self, item, parent):\n        self.item = item\n        self.count = 0\n        self.parent = parent\n        self.children = {}\n        self.link = None\ndef _build_fp_tree(trans_iterable, min_sup):\n    \"\"\"\n    Build an FP-tree from `trans_iterable` and return (root, header_table).\n    `trans_iterable` must yield tuples   (items_in_transaction , multiplicity)\n    in which every `items_in_transaction` is already ordered according to a\n    global frequency order and contains *only* items that are frequent in the\n    projected data set.\n    \"\"\"\n    header = {}\n    for (items, cnt) in trans_iterable:\n        for itm in items:\n            header.setdefault(itm, [0, None])[0] += cnt\n    header = {itm: val for (itm, val) in header.items() if val[0] >= min_sup}\n    if not header:\n        return (None, None)\n    order = sorted(header.items(), key=lambda x: (-x[1][0], x[0]))\n    rank = {itm: idx for (idx, (itm, _)) in enumerate(order)}\n    root = _FPNode(None, None)\n    for (items, cnt) in trans_iterable:\n        ordered = [i for i in sorted(items, key=lambda x: rank.get(x, 10 ** 9)) if i in header]\n        if not ordered:\n            continue\n        cur = root\n        for itm in ordered:\n            nxt = cur.children.get(itm)\n            if nxt is None:\n                nxt = _FPNode(itm, cur)\n                cur.children[itm] = nxt\n                if header[itm][1] is None:\n                    header[itm][1] = nxt\n                else:\n                    last = header[itm][1]\n                    while last.link is not None:\n                        last = last.link\n                    last.link = nxt\n            nxt.count += cnt\n            cur = nxt\n    return (root, header)\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\"\"\"\n    freq = Counter()\n    for t in transactions:\n        freq.update(t)\n    freq = {k: v for (k, v) in freq.items() if v >= min_sup}\n    if not freq:\n        return []\n    order = sorted(freq.items(), key=lambda x: (-x[1], x[0]))\n    rank = {itm: idx for (idx, (itm, _)) in enumerate(order)}\n    preprocessed = []\n    for t in transactions:\n        filtered = [i for i in t if i in freq]\n        if not filtered:\n            continue\n        filtered.sort(key=lambda x: rank[x])\n        preprocessed.append((tuple(filtered), 1))\n    (root, header) = _build_fp_tree(preprocessed, min_sup)\n    patterns = set()\n    _mine(root, header, min_sup, [], patterns)\n    result = [tuple(sorted(p)) for p in patterns]\n    result.sort(key=lambda x: (len(x), x))\n    return result"}
{"task_id": 492, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a polynomial feature matrix up to *degree*.\n\n    Parameters\n    ----------\n    X : list of list\n        Input data of shape (n_samples, n_features).\n    degree : int\n        Maximum total degree of the generated polynomial terms.\n        If degree < 0 the function returns -1.\n\n    Returns\n    -------\n    list of list\n        Enriched feature matrix whose columns are all monomials of the\n        original variables having total degree \u2264 *degree*, ordered\n        (i)   by increasing total degree,\n        (ii)  lexicographically within the same degree.\n        The first column is 1 (degree-0 term).\n    \"\"\"\n    if degree < 0:\n        return -1\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim != 2:\n        raise ValueError('X must be a 2-D array-like structure')\n    (n_samples, n_features) = X_arr.shape\n    index_tuples: list[tuple[int, ...]] = [()]\n    for p in range(1, degree + 1):\n        for combo in combinations_with_replacement(range(n_features), p):\n            index_tuples.append(combo)\n    n_terms = len(index_tuples)\n    out = np.empty((n_samples, n_terms), dtype=float)\n    for (j, comb) in enumerate(index_tuples):\n        if not comb:\n            out[:, j] = 1.0\n        else:\n            col = np.ones(n_samples, dtype=float)\n            for idx in comb:\n                col *= X_arr[:, idx]\n            out[:, j] = col\n    return out.tolist()"}
{"task_id": 493, "completion_id": 0, "solution": "import numpy as np\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    If *y* is empty return -1.\n\n    Parameters\n    ----------\n    y : Sequence[int | float] | numpy.ndarray\n        One-dimensional collection of numbers.\n\n    Returns\n    -------\n    float\n        MSE rounded to four decimal places, or -1 when *y* is empty.\n    \"\"\"\n    arr = np.asarray(y, dtype=float).ravel()\n    if arr.size == 0:\n        return -1\n    diff = arr - arr.mean()\n    mse_val = (diff ** 2).mean()\n    return float(round(mse_val, 4))"}
{"task_id": 496, "completion_id": 0, "solution": "import numpy as np\ndef apply_affine(x, slope: float=1.0, intercept: float=0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list/array or a scalar.\n        slope: Coefficient multiplying x (default 1.0).\n        intercept: Constant term added to the product (default 0.0).\n\n    Returns:\n        (y, grad, grad2): tuple of three lists (all rounded to 4 decimals)\n            y[i]     = slope * x[i] + intercept\n            grad[i]  = slope\n            grad2[i] = 0\n    \"\"\"\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim == 0:\n        arr = arr.reshape(1)\n    y = slope * arr + intercept\n    grad = np.full(arr.shape, slope, dtype=float)\n    grad2 = np.zeros(arr.shape, dtype=float)\n    y = np.round(y, 4).tolist()\n    grad = np.round(grad, 4).tolist()\n    grad2 = np.round(grad2, 4).tolist()\n    return (y, grad, grad2)"}
{"task_id": 499, "completion_id": 0, "solution": "import numpy as np\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places.  \n        If *k* is outside the valid range, return \u22121.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D NumPy array.')\n    (n_samples, n_features) = data.shape\n    if not 1 <= k <= n_features:\n        return -1\n    data = data.astype(float, copy=False)\n    means = data.mean(axis=0)\n    stds = data.std(axis=0, ddof=0)\n    stds_safe = stds.copy()\n    stds_safe[stds_safe == 0] = 1.0\n    Z = (data - means) / stds_safe\n    if n_samples > 1:\n        cov = Z.T @ Z / (n_samples - 1)\n    else:\n        cov = np.zeros((n_features, n_features))\n    (eigvals, eigvecs) = np.linalg.eigh(cov)\n    order = eigvals.argsort()[::-1]\n    eigvals = eigvals[order]\n    eigvecs = eigvecs[:, order]\n    for i in range(eigvecs.shape[1]):\n        v = eigvecs[:, i]\n        if v[np.argmax(np.abs(v))] < 0:\n            eigvecs[:, i] = -v\n    components = eigvecs[:, :k]\n    projected = Z @ components\n    return np.round(projected, 4).tolist()"}
{"task_id": 500, "completion_id": 0, "solution": "import numpy as np\nfrom math import inf\ndef _fit_best_stump(X: np.ndarray, residuals: np.ndarray):\n    \"\"\"Find the decision stump (feature, threshold, left_val, right_val)\n    that minimises the squared error w.r.t. the provided residuals.\"\"\"\n    (n_samples, n_features) = X.shape\n    (best_feature, best_thr) = (None, None)\n    (best_left_val, best_right_val) = (None, None)\n    best_sse = inf\n    for j in range(n_features):\n        f = X[:, j]\n        order = np.argsort(f)\n        f_sorted = f[order]\n        r_sorted = residuals[order]\n        cumsum_r = np.cumsum(r_sorted)\n        cumsum_r2 = np.cumsum(r_sorted ** 2)\n        total_sum = cumsum_r[-1]\n        total_sum2 = cumsum_r2[-1]\n        for i in range(1, n_samples):\n            if f_sorted[i] == f_sorted[i - 1]:\n                continue\n            n_left = i\n            n_right = n_samples - i\n            sum_left = cumsum_r[i - 1]\n            sum_right = total_sum - sum_left\n            sum2_left = cumsum_r2[i - 1]\n            sum2_right = total_sum2 - sum2_left\n            mean_left = sum_left / n_left\n            mean_right = sum_right / n_right\n            sse_left = sum2_left - sum_left ** 2 / n_left\n            sse_right = sum2_right - sum_right ** 2 / n_right\n            sse = sse_left + sse_right\n            if sse < best_sse:\n                best_sse = sse\n                best_feature = j\n                best_thr = 0.5 * (f_sorted[i] + f_sorted[i - 1])\n                best_left_val = mean_left\n                best_right_val = mean_right\n    return (best_feature, best_thr, best_left_val, best_right_val)\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int=20, learning_rate: float=0.1) -> list[int]:\n    \"\"\"Tiny gradient\u2013boosted binary classifier (decision-stump base learners).\"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train, dtype=float)\n    n_train = X_train.shape[0]\n    c0 = y_train.mean()\n    F_train = np.full(n_train, c0, dtype=float)\n    F_test = np.full(X_test.shape[0], c0, dtype=float)\n    stumps = []\n    for _ in range(n_estimators):\n        residuals = y_train - F_train\n        (feat, thr, left_val, right_val) = _fit_best_stump(X_train, residuals)\n        stumps.append((feat, thr, left_val, right_val))\n        pred_train = np.where(X_train[:, feat] <= thr, left_val, right_val)\n        pred_test = np.where(X_test[:, feat] <= thr, left_val, right_val)\n        F_train += learning_rate * pred_train\n        F_test += learning_rate * pred_test\n    y_pred = (F_test >= 0.5).astype(int).tolist()\n    return y_pred"}
{"task_id": 505, "completion_id": 0, "solution": "import numpy as np\ndef contrastive_divergence(X: np.ndarray, W: np.ndarray, hbias: np.ndarray, vbias: np.ndarray, learning_rate: float, k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM (probabilities only).\"\"\"\n    if X.size == 0:\n        return []\n    m = X.shape[0]\n\n    def _sigmoid(a: np.ndarray) -> np.ndarray:\n        return 1.0 / (1.0 + np.exp(-a))\n    h_prob = _sigmoid(X @ W + hbias)\n    pos_grad = X.T @ h_prob\n    for _ in range(k):\n        v_prob = _sigmoid(h_prob @ W.T + vbias)\n        h_prob = _sigmoid(v_prob @ W + hbias)\n    neg_grad = v_prob.T @ h_prob\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n    return np.round(W_new, 4).tolist()"}
{"task_id": 509, "completion_id": 0, "solution": "import numpy as np\ndef chebyshev(x: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray') -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=float).ravel()\n    y_arr = np.asarray(y, dtype=float).ravel()\n    if x_arr.size != y_arr.size:\n        return -1.0\n    dist = np.max(np.abs(x_arr - y_arr))\n    return float(round(dist, 4))"}
{"task_id": 510, "completion_id": 0, "solution": "import numpy as np\ndef vae_loss(y: np.ndarray, y_pred: np.ndarray, t_mean: np.ndarray, t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    Returns the mini-batch average of the binary cross-entropy reconstruction\n    loss plus the analytical KL divergence, rounded to 6 decimal places.\n    \"\"\"\n    eps = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, eps, 1.0 - eps)\n    recon_element = -y * np.log(y_pred_clipped) - (1.0 - y) * np.log(1.0 - y_pred_clipped)\n    recon_loss = recon_element.sum(axis=1)\n    kl_element = 1.0 + t_log_var - np.square(t_mean) - np.exp(t_log_var)\n    kl_loss = -0.5 * kl_element.sum(axis=1)\n    total_loss = recon_loss + kl_loss\n    batch_mean_loss = total_loss.mean()\n    return round(float(batch_mean_loss), 6)"}
{"task_id": 513, "completion_id": 0, "solution": "import numpy as np\ndef fm_predict(X: list[list[int | float]], w0: float, w: list[float], V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix (n_samples \u00d7 n_features) given as a list of lists.\n        w0: Global bias term.\n        w: Linear weights (length = n_features).\n        V: Latent factor matrix (n_features \u00d7 k).\n\n    Returns:\n        List with one prediction per sample, rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    w = np.asarray(w, dtype=float)\n    V = np.asarray(V, dtype=float)\n    linear_term = w0 + X @ w\n    XV = X @ V\n    X2_V2 = X ** 2 @ V ** 2\n    interaction_term = 0.5 * np.sum(XV ** 2 - X2_V2, axis=1)\n    y_hat = linear_term + interaction_term\n    return np.round(y_hat, 4).tolist()"}
{"task_id": 517, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Compute a numerically-stable, element-wise sigmoid activation.\n\n    Parameters\n    ----------\n    z : int | float | list | tuple | numpy.ndarray\n        Input value(s).\n\n    Returns\n    -------\n    float | list\n        If `z` is a scalar a Python float is returned, otherwise a (nested)\n        list whose elements are the sigmoid of the corresponding entries in\n        `z`, rounded to four decimal places.\n    \"\"\"\n    arr = np.asarray(z, dtype=np.float64)\n    out = np.empty_like(arr)\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    exp_x = np.exp(arr[neg_mask])\n    out[neg_mask] = exp_x / (1.0 + exp_x)\n    out = np.round(out, 4)\n    if out.ndim == 0:\n        return float(out)\n    else:\n        return out.tolist()"}
{"task_id": 518, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Callable\nfrom functools import wraps\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D NumPy array whose second\n    dimension is larger than one, it is assumed to be one-hot encoded and is\n    replaced by the index of the maximum value along axis 1 before the wrapped\n    *function* is executed.\n    \"\"\"\n\n    def _decode(arr: np.ndarray) -> np.ndarray:\n        \"\"\"Return a 1-D label array if *arr* looks like one-hot, else *arr*.\"\"\"\n        if isinstance(arr, np.ndarray) and arr.ndim == 2 and (arr.shape[1] > 1):\n            return arr.argmax(axis=1)\n        return arr\n\n    @wraps(function)\n    def wrapper(actual: np.ndarray, predicted: np.ndarray, *args, **kwargs):\n        actual_decoded = _decode(actual)\n        predicted_decoded = _decode(predicted)\n        return function(actual_decoded, predicted_decoded, *args, **kwargs)\n    return wrapper"}
{"task_id": 520, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).\n    The derivative is computed element-wise as z * (1 - z) and rounded\n    to 4 decimal places.\n\n    Args:\n        z: float, list, or NumPy ndarray holding sigmoid outputs.\n\n    Returns:\n        float if *z* is a scalar; otherwise a (nested) list with the same\n        shape as *z*, each value rounded to 4 decimals.\n    \"\"\"\n    if z == []:\n        return []\n    arr = np.asarray(z, dtype=float)\n    grad = np.round(arr * (1.0 - arr), 4)\n    return float(grad) if grad.ndim == 0 else grad.tolist()"}
{"task_id": 528, "completion_id": 0, "solution": "import numpy as np\ndef decision_boundary_grid(X: list[list[int | float]], W: list[int | float], b: float, grid_n: int=100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) receives the label  1 if  w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0\n    and \u22121 otherwise.  The returned list has shape (grid_n \u00d7 grid_n) and\n    contains those labels for a regular grid that spans the bounding box\n    of *X*.\n\n    Args:\n        X: 2-D data set \u2013 list of [x\u2081 , x\u2082] pairs (shape n\u00d72).\n        W: Length-2 sequence [w\u2081 , w\u2082] with the classifier\u2019s weights.\n        b: Bias term of the classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Python list of lists with shape (grid_n \u00d7 grid_n) whose entries\n        are exclusively 1 or \u22121.\n    \"\"\"\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim != 2 or X_arr.shape[1] != 2:\n        raise ValueError('X must be a 2-D array-like with exactly two columns.')\n    if len(W) != 2:\n        raise ValueError('W must contain exactly two elements.')\n    if grid_n <= 0:\n        raise ValueError('grid_n must be a positive integer.')\n    (x1_min, x1_max) = (X_arr[:, 0].min(), X_arr[:, 0].max())\n    (x2_min, x2_max) = (X_arr[:, 1].min(), X_arr[:, 1].max())\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    (x1_grid, x2_grid) = np.meshgrid(x1_plot, x2_plot, indexing='xy')\n    (w1, w2) = (float(W[0]), float(W[1]))\n    decision_values = w1 * x1_grid + w2 * x2_grid + b\n    labels = np.where(decision_values >= 0, 1, -1)\n    return labels.tolist()"}
{"task_id": 537, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier (from scratch).\"\"\"\n    eps = 1e-09\n    classes = np.array([0, 1])\n    mask0 = y_train == 0\n    mask1 = y_train == 1\n    mu0 = X_train[mask0].mean(axis=0)\n    mu1 = X_train[mask1].mean(axis=0)\n    std0 = X_train[mask0].std(axis=0, ddof=0) + eps\n    std1 = X_train[mask1].std(axis=0, ddof=0) + eps\n    n_samples = len(y_train)\n    prior0 = mask0.sum() / n_samples\n    prior1 = mask1.sum() / n_samples\n    log_prior = np.log(np.array([prior0, prior1]))\n    log_coeff0 = -0.5 * np.log(2 * np.pi) - np.log(std0)\n    log_coeff1 = -0.5 * np.log(2 * np.pi) - np.log(std1)\n    inv_var0 = 1.0 / (2 * std0 ** 2)\n    inv_var1 = 1.0 / (2 * std1 ** 2)\n    predictions = []\n    for x in X_test:\n        ll0 = (log_coeff0 - (x - mu0) ** 2 * inv_var0).sum()\n        ll1 = (log_coeff1 - (x - mu1) ** 2 * inv_var1).sum()\n        log_post0 = log_prior[0] + ll0\n        log_post1 = log_prior[1] + ll1\n        pred = 0 if log_post0 >= log_post1 else 1\n        predictions.append(pred)\n    return predictions"}
{"task_id": 539, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Computes the binary cross-entropy (log-loss) cost.\n\n    Parameters\n    ----------\n    AL : np.ndarray\n        Model predicted probabilities, shape (m,) or (1, m).\n    Y  : np.ndarray\n        Ground-truth labels (0 or 1), same shape as AL.\n\n    Returns\n    -------\n    float\n        Scalar cost value.\n    \"\"\"\n    if AL.shape != Y.shape:\n        raise ValueError('AL and Y must have the same shape.')\n    AL = AL.ravel()\n    Y = Y.ravel()\n    m = AL.size\n    eps = 1e-15\n    AL_clipped = np.clip(AL, eps, 1.0 - eps)\n    cost = -np.sum(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped)) / m\n    return float(cost)"}
{"task_id": 552, "completion_id": 0, "solution": "import numpy as np\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation uses the forward-backward algorithm with scaling\n    to obtain numerically stable \u03b1, \u03b2 and finally \u03b3\u209c.\n\n    Args:\n        hmm (dict): HMM parameters with keys\n                    'pi' -> 1-D initial state distribution (N,)\n                    'A'  -> state transition matrix      (N,N)\n                    'B'  -> emission matrix              (N,M)\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t,\n                     rounded to four decimals.  If t is out of range an\n                     empty list is returned.\n    \"\"\"\n    if t < 0 or t >= len(observations):\n        return []\n    pi = np.asarray(hmm['pi'], dtype=float)\n    A = np.asarray(hmm['A'], dtype=float)\n    B = np.asarray(hmm['B'], dtype=float)\n    N = pi.shape[0]\n    T = len(observations)\n    obs_idx = observations\n    alpha = np.zeros((T, N), dtype=float)\n    c = np.zeros(T, dtype=float)\n    alpha[0] = pi * B[:, obs_idx[0]]\n    c[0] = 1.0 / alpha[0].sum() if alpha[0].sum() != 0 else 1.0\n    alpha[0] *= c[0]\n    for tt in range(1, T):\n        alpha[tt] = alpha[tt - 1] @ A * B[:, obs_idx[tt]]\n        denom = alpha[tt].sum()\n        c[tt] = 1.0 / denom if denom != 0 else 1.0\n        alpha[tt] *= c[tt]\n    beta = np.zeros((T, N), dtype=float)\n    beta[-1] = 1.0\n    beta[-1] *= c[-1]\n    for tt in range(T - 2, -1, -1):\n        beta[tt] = A @ (B[:, obs_idx[tt + 1]] * beta[tt + 1])\n        beta[tt] *= c[tt]\n    gamma_t = alpha[t] * beta[t]\n    total = gamma_t.sum()\n    if total == 0:\n        return [0.0 for _ in range(N)]\n    gamma_t /= total\n    return [round(float(p), 4) for p in gamma_t]"}
{"task_id": 555, "completion_id": 0, "solution": "import numpy as np\nfrom collections import deque\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN) implemented from scratch.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Array of shape (n_samples, n_features).\n    eps : float\n        Neighbourhood radius.\n    min_samples : int\n        Minimum number of points (inclusive) needed for a core point.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels (1, 2, \u2026) for clustered points; noise is \u22121.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array (n_samples, n_features)')\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n    diff = data[:, None, :] - data[None, :, :]\n    dist_mat = np.linalg.norm(diff, axis=-1)\n    neighbours = [np.where(dist_mat[i] <= eps)[0] for i in range(n_samples)]\n    core_mask = np.array([len(idx) >= min_samples for idx in neighbours], dtype=bool)\n    labels = np.zeros(n_samples, dtype=int)\n    current_label = 0\n    for point in range(n_samples):\n        if not core_mask[point] or labels[point] != 0:\n            continue\n        current_label += 1\n        queue = deque([point])\n        labels[point] = current_label\n        while queue:\n            p = queue.popleft()\n            for nbr in neighbours[p]:\n                if labels[nbr] == 0:\n                    labels[nbr] = current_label\n                    if core_mask[nbr]:\n                        queue.append(nbr)\n    labels[labels == 0] = -1\n    return labels.tolist()"}
{"task_id": 561, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef gmm_em(data: np.ndarray, k: int, max_iter: int=200, epsilon: float=0.0001) -> list[int]:\n    \"\"\"\n    Simple EM implementation for a full-covariance Gaussian Mixture Model.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array with shape (N, D); N samples, D features.\n    k : int\n        Number of mixture components (clusters).\n    max_iter : int, optional\n        Maximum number of EM iterations (default 200).\n    epsilon : float, optional\n        Convergence tolerance on the maximum absolute change of the mixture\n        weights between two successive iterations (default 1 e-4).\n\n    Returns\n    -------\n    list[int]\n        Length-N list of final cluster labels (0 \u2026 k-1) in deterministic,\n        left-to-right order (ascending first coordinate of the component mean).\n\n    Notes\n    -----\n    \u2022 If k == 1 all samples receive label 0 and the algorithm stops\n      immediately.\n    \u2022 A tiny diagonal regulariser (1 e-6) is added to every covariance\n      estimate to guarantee positive-definiteness and numerical stability.\n    \"\"\"\n    data = np.asarray(data, dtype=float)\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array (N samples, D features).')\n    (n_samples, n_features) = data.shape\n    if k < 1:\n        raise ValueError('`k` must be a positive integer.')\n    if k == 1:\n        return [0] * n_samples\n    _LOG_2PI = n_features * math.log(2.0 * math.pi)\n\n    def _log_gaussian(x: np.ndarray, mean: np.ndarray, cov: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute log N(x | mean, cov) for all rows of `x`.\n        x : (N, D)\n        mean : (D,)\n        cov : (D, D)\n        Returns\n        -------\n        log_prob : (N,)\n        \"\"\"\n        diff = x - mean\n        try:\n            L = np.linalg.cholesky(cov)\n            y = np.linalg.solve(L, diff.T)\n            maha = np.sum(y ** 2, axis=0)\n            log_det = 2.0 * np.sum(np.log(np.diag(L)))\n        except np.linalg.LinAlgError:\n            inv_cov = np.linalg.inv(cov)\n            maha = np.einsum('ij,ij->i', diff, diff @ inv_cov)\n            (sign, log_det) = np.linalg.slogdet(cov)\n            if sign <= 0:\n                log_det = np.log(np.abs(np.linalg.det(cov) + 1e-12))\n        return -0.5 * (_LOG_2PI + log_det + maha)\n    rng = np.random.default_rng()\n    init_idx = rng.choice(n_samples, size=k, replace=False)\n    means = data[init_idx]\n    weights = np.full(k, 1.0 / k)\n    global_cov = np.cov(data.T, bias=True)\n    if global_cov.shape == ():\n        global_cov = global_cov.reshape(1, 1)\n    covs = np.array([global_cov.copy() for _ in range(k)])\n    reg_eps = 1e-06 * np.eye(n_features)\n    for _ in range(max_iter):\n        log_resp = np.empty((n_samples, k))\n        for j in range(k):\n            log_resp[:, j] = np.log(weights[j] + 1e-16) + _log_gaussian(data, means[j], covs[j])\n        log_resp_max = np.max(log_resp, axis=1, keepdims=True)\n        resp = np.exp(log_resp - log_resp_max)\n        resp_sum = resp.sum(axis=1, keepdims=True)\n        resp /= resp_sum\n        nk = resp.sum(axis=0)\n        prev_weights = weights.copy()\n        weights = nk / n_samples\n        means = resp.T @ data / nk[:, None]\n        for j in range(k):\n            diff = data - means[j]\n            cov = (resp[:, j][:, None] * diff).T @ diff / nk[j]\n            covs[j] = cov + reg_eps\n        if np.max(np.abs(weights - prev_weights)) < epsilon:\n            break\n    log_resp = np.empty((n_samples, k))\n    for j in range(k):\n        log_resp[:, j] = np.log(weights[j] + 1e-16) + _log_gaussian(data, means[j], covs[j])\n    labels = np.argmax(log_resp, axis=1)\n    ordering = np.argsort(means[:, 0])\n    remap = np.zeros(k, dtype=int)\n    remap[ordering] = np.arange(k)\n    labels = remap[labels]\n    return labels.tolist()"}
{"task_id": 562, "completion_id": 0, "solution": "import numpy as np\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float=2.0, method: str='unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A Python list containing the cluster label (0-based) for each sample.\n    \"\"\"\n    N = data.shape[0]\n    if n_cluster <= 1 or N == 0:\n        return [0] * N\n    if method not in {'unnormalized', 'normalized'}:\n        raise ValueError(\"`method` must be either 'unnormalized' or 'normalized'\")\n    sq_norms = np.sum(data ** 2, axis=1)\n    sq_dists = sq_norms[:, None] + sq_norms[None, :] - 2.0 * data @ data.T\n    sq_dists = np.maximum(sq_dists, 0.0)\n    W = np.exp(-gamma * sq_dists)\n    np.fill_diagonal(W, 0.0)\n    degree = np.sum(W, axis=1)\n    if method == 'unnormalized':\n        L = np.diag(degree) - W\n    else:\n        d_inv_sqrt = 1.0 / np.sqrt(degree + 1e-12)\n        L = np.eye(N) - d_inv_sqrt[:, None] * W * d_inv_sqrt[None, :]\n    (eigvals, eigvecs) = np.linalg.eigh(L)\n    U = eigvecs[:, :n_cluster]\n    if method == 'normalized':\n        row_norm = np.linalg.norm(U, axis=1, keepdims=True)\n        U = U / (row_norm + 1e-12)\n    embedding = U\n    centroids = embedding[:n_cluster].copy()\n    labels = np.full(N, -1, dtype=int)\n    for _ in range(300):\n        dists = ((embedding[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.array_equal(labels, new_labels):\n            break\n        labels = new_labels\n        for k in range(n_cluster):\n            members = embedding[labels == k]\n            if len(members) > 0:\n                centroids[k] = members.mean(axis=0)\n    unique_old = np.unique(labels)\n    order = sorted(((l, np.where(labels == l)[0][0]) for l in unique_old))\n    relabel_map = {old: new for (new, (old, _)) in enumerate(order)}\n    final_labels = [int(relabel_map[l]) for l in labels]\n    return final_labels"}
{"task_id": 563, "completion_id": 0, "solution": "import numpy as np\ndef perceptron_dual_train(X_data: np.ndarray, y_data: np.ndarray, eta: float=1.0, max_iter: int=1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array with N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate.\n        max_iter: Maximum number of full passes over the training data.\n\n    Returns:\n        (w, b)  \u2013 w is the learned weight vector as a Python list,\n                  b is the bias term; both rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X_data, dtype=float)\n    y = np.asarray(y_data, dtype=float).flatten()\n    (N, d) = X.shape\n    alpha = np.zeros(N, dtype=float)\n    b = 0.0\n    G = X @ X.T\n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            g = np.sum(alpha * y * G[:, i])\n            if y[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y[i]\n                updated = True\n        if not updated:\n            break\n    w_vec = X.T @ (alpha * y)\n    w_rounded = [round(float(val), 4) for val in w_vec]\n    b_rounded = round(float(b), 4)\n    return (w_rounded, b_rounded)"}
{"task_id": 564, "completion_id": 0, "solution": "def predict_sample(root: 'Node', features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    node = root\n    while True:\n        if hasattr(node, 'y'):\n            return node.y\n        if task_type == 'classification':\n            feat_val = features[node.label]\n            next_node = None\n            for child in node.child:\n                if hasattr(child, 'x') and child.x == feat_val:\n                    next_node = child\n                    break\n            if next_node is None:\n                next_node = node.child[1]\n        else:\n            feat_val = features[node.label]\n            next_node = node.child[0] if feat_val <= node.s else node.child[1]\n        node = next_node"}
{"task_id": 565, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef _entropy(labels: np.ndarray) -> float:\n    \"\"\"\n    Shannon entropy H(labels) in bits.\n    labels are integers starting from 0.\n    \"\"\"\n    if labels.size == 0:\n        return 0.0\n    counts = np.bincount(labels)\n    probs = counts[counts > 0] / labels.size\n    return float(-np.sum(probs * np.log2(probs)))\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        (best_feature_index, max_information_gain_rounded_to_6_decimals)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    base_entropy = _entropy(y)\n    best_gain = -math.inf\n    best_index = 0\n    for idx in range(n_features):\n        column = X[:, idx]\n        (vals, counts) = np.unique(column, return_counts=True)\n        cond_entropy = 0.0\n        for (val, cnt) in zip(vals, counts):\n            subset_labels = y[column == val]\n            cond_entropy += cnt / n_samples * _entropy(subset_labels)\n        info_gain = base_entropy - cond_entropy\n        if info_gain > best_gain + 1e-12:\n            best_gain = info_gain\n            best_index = idx\n        elif abs(info_gain - best_gain) <= 1e-12 and idx < best_index:\n            best_index = idx\n    if abs(best_gain) < 1e-12:\n        best_gain = 0.0\n    return (best_index, round(best_gain, 6))"}
{"task_id": 566, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation (deterministic).\n\n    Args\n    ----\n    data        : 2-D NumPy array (n_samples, n_features)\n    n_trees     : number of random isolation trees\n    sample_size : subsample size used to grow every tree\n    epsilon     : fraction (0 \u2264 \u03b5 < 1) of observations regarded as outliers\n\n    Returns\n    -------\n    A sorted list with the zero-based indices of the detected outliers.\n    \"\"\"\n    np.random.seed(42)\n    (n_samples, n_features) = data.shape\n    if epsilon == 0 or n_samples == 0:\n        return []\n    subsz = min(sample_size, n_samples)\n    height_limit = math.ceil(math.log2(subsz)) if subsz > 1 else 0\n\n    def c_factor(n: int) -> float:\n        \"\"\"Expected path length for an unsuccessful search in a BST.\"\"\"\n        if n <= 1:\n            return 0.0\n        return 2.0 * math.log(n - 1) + 0.5772156649 - 2.0 * (n - 1) / n\n\n    class Node:\n        __slots__ = ('is_leaf', 'size', 'feat', 'thr', 'left', 'right')\n\n        def __init__(self, *, is_leaf: bool, size: int, feat=None, thr=None, left=None, right=None):\n            self.is_leaf = is_leaf\n            self.size = size\n            self.feat = feat\n            self.thr = thr\n            self.left = left\n            self.right = right\n\n    def build_tree(idxs: np.ndarray, depth: int) -> Node:\n        \"\"\"Recursively build one isolation tree.\"\"\"\n        if depth >= height_limit or idxs.size <= 1:\n            return Node(is_leaf=True, size=idxs.size)\n        feat = np.random.randint(n_features)\n        col = data[idxs, feat]\n        (f_min, f_max) = (col.min(), col.max())\n        if f_min == f_max:\n            return Node(is_leaf=True, size=idxs.size)\n        thr = np.random.uniform(f_min, f_max)\n        left_mask = col < thr\n        right_mask = ~left_mask\n        if not left_mask.any() or not right_mask.any():\n            return Node(is_leaf=True, size=idxs.size)\n        left_child = build_tree(idxs[left_mask], depth + 1)\n        right_child = build_tree(idxs[right_mask], depth + 1)\n        return Node(is_leaf=False, size=idxs.size, feat=feat, thr=thr, left=left_child, right=right_child)\n\n    def path_length(x: np.ndarray, node: Node, depth: int=0) -> float:\n        \"\"\"Path length of one sample through one tree.\"\"\"\n        if node.is_leaf:\n            return depth + c_factor(node.size)\n        if x[node.feat] < node.thr:\n            return path_length(x, node.left, depth + 1) if node.left is not None else depth + c_factor(node.size)\n        else:\n            return path_length(x, node.right, depth + 1) if node.right is not None else depth + c_factor(node.size)\n    forest: list[Node] = []\n    for _ in range(n_trees):\n        sample_indices = np.random.choice(n_samples, subsz, replace=False)\n        forest.append(build_tree(sample_indices, depth=0))\n    path_sum = np.zeros(n_samples, dtype=float)\n    for tree in forest:\n        for i in range(n_samples):\n            path_sum[i] += path_length(data[i], tree)\n    h_bar = path_sum / n_trees\n    phi = 0.0\n    if n_samples > 1:\n        phi = 2.0 * math.log(n_samples - 1) - 2.0 * (n_samples - 1) / n_samples\n    else:\n        phi = 1.0\n    scores = np.power(2.0, -h_bar / phi)\n    k = int(math.floor(epsilon * n_samples))\n    if k == 0:\n        return []\n    largest_k_idx = np.argsort(scores)[-k:]\n    return sorted(largest_k_idx.tolist())"}
{"task_id": 568, "completion_id": 0, "solution": "import math\nfrom collections import defaultdict\nimport numpy as np\ndef maxent_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, epsilon: float=0.001, n_iter: int=100) -> list[int]:\n    \"\"\"\n    Train a Maximum-Entropy classifier with Generalised Iterative Scaling\n    on (categorical) training data and return the label predictions for\n    X_test.\n    \"\"\"\n\n    def active_sum(x_row, y_label):\n        \"\"\"\n        Sum of the weights that are active for (x_row , y_label).\n        Each (feature_index, feature_value, y_label) that matches x_row\n        contributes its current weight.\n        \"\"\"\n        s = 0.0\n        for (i, val) in enumerate(x_row):\n            idx = feat2id.get((i, val, y_label))\n            if idx is not None:\n                s += weights[idx]\n        return s\n    (N, d) = X_train.shape\n    labels = list(set((int(y) for y in y_train)))\n    C = d\n    feat_counts = defaultdict(int)\n    for (x_row, y) in zip(X_train, y_train):\n        y = int(y)\n        for (i, val) in enumerate(x_row):\n            feat_counts[i, val, y] += 1\n    n_feats = len(feat_counts)\n    feat2id = {}\n    emp_exp = np.zeros(n_feats, dtype=float)\n    for (idx, (feat, cnt)) in enumerate(feat_counts.items()):\n        feat2id[feat] = idx\n        emp_exp[idx] = cnt / N\n    weights = np.zeros(n_feats, dtype=float)\n    tiny = 1e-12\n    for _ in range(n_iter):\n        mod_exp = np.zeros(n_feats, dtype=float)\n        for x_row in X_train:\n            raw = {y: active_sum(x_row, y) for y in labels}\n            max_raw = max(raw.values())\n            exp_scores = {y: math.exp(raw[y] - max_raw) for y in labels}\n            Z = sum(exp_scores.values())\n            probs = {y: exp_scores[y] / Z for y in labels}\n            for (y, p) in probs.items():\n                if p == 0.0:\n                    continue\n                for (i, val) in enumerate(x_row):\n                    idx = feat2id.get((i, val, y))\n                    if idx is not None:\n                        mod_exp[idx] += p\n        mod_exp /= N\n        max_change = 0.0\n        for j in range(n_feats):\n            delta = 1.0 / C * math.log((emp_exp[j] + tiny) / (mod_exp[j] + tiny))\n            weights[j] += delta\n            if abs(delta) > max_change:\n                max_change = abs(delta)\n        if max_change < epsilon:\n            break\n    preds = []\n    for x_row in X_test:\n        (best_label, best_score) = (None, -float('inf'))\n        for y in labels:\n            score = active_sum(x_row, y)\n            if score > best_score:\n                best_score = score\n                best_label = y\n        preds.append(int(best_label))\n    return preds"}
{"task_id": 569, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\ndef statistic_key_word(data: List[Tuple[Any, Iterable[str]]], cut_off: int | None=None) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate per-word document-frequency statistics.\n\n    The function returns\n        1. a list of (word, ratio) pairs where *ratio* is the fraction of\n           documents in which the word occurs (rounded to 4 decimals),\n           sorted from the most to the least frequent word;\n        2. a set that contains only the *cut_off* most frequent words\n           (all words if *cut_off* is None).\n\n    Args:\n        data: list/iterable of documents.  Each document is a tuple\n              ``(label, words)`` where *words* is any iterable of strings.\n        cut_off: maximal number of top words to put into the returned set, or\n                 ``None`` for *all* words.\n\n    Returns:\n        Tuple[list[(word, ratio)], set[str]]\n    \"\"\"\n    n_docs = len(data)\n    if n_docs == 0:\n        return ([], set())\n    doc_counter: Counter[str] = Counter()\n    for (_, words) in data:\n        doc_counter.update(set(words))\n    freq_list = [(word, round(count / n_docs, 4)) for (word, count) in doc_counter.items()]\n    freq_list.sort(key=lambda pair: (-pair[1], pair[0]))\n    if cut_off is None:\n        top_k_set = {word for (word, _) in freq_list}\n    elif cut_off <= 0:\n        top_k_set = set()\n    else:\n        top_k_set = {word for (word, _) in freq_list[:cut_off]}\n    return (freq_list, top_k_set)"}
{"task_id": 571, "completion_id": 0, "solution": "import numpy as np\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Training data.\n    y : np.ndarray, shape (n_samples,)\n        Class labels (+1 or \u22121).\n    alpha : np.ndarray, shape (n_samples,)\n        Optimised Lagrange multipliers.\n\n    Returns\n    -------\n    (w_list, b) : tuple[list[float], float]\n        w_list : the weight vector rounded to 4 decimals (Python list)\n        b       : the bias term rounded to 4 decimals (float)\n    \"\"\"\n    alpha_y = alpha * y\n    w = X.T @ alpha_y\n    predictions = X @ w\n    b = np.mean(y - predictions)\n    w_list = [round(float(w_i), 4) for w_i in w]\n    b = round(float(b), 4)\n    return (w_list, b)"}
{"task_id": 572, "completion_id": 0, "solution": "import numpy as np\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"\n    Computes the Local-Outlier-Factor (LOF) for every sample in `data`\n    and returns\n        (list_of_LOF_scores_rounded_to_4_decimals ,\n         ascending_list_of_indices_with_LOF>epsilon)\n\n    Parameters\n    ----------\n    data : array-like, shape = (n_samples, n_features) or (n_samples,)\n    k    : int, 1 \u2264 k < n_samples\n    epsilon : float, threshold that decides which samples are flagged\n              as outliers        (default 1.5)\n\n    Notes\n    -----\n    \u2022  NO external ML / scientific libraries are used \u2013 only `numpy`.\n    \u2022  All LOF scores are rounded with `numpy.round(score, 4)`.\n    \"\"\"\n    data = np.asarray(data, dtype=float)\n    if data.ndim == 1:\n        data = data[:, None]\n    n_samples = data.shape[0]\n    if not 1 <= k < n_samples:\n        raise ValueError('k must satisfy 1 \u2264 k < n_samples (got k = %d)' % k)\n    diff = data[:, None, :] - data[None, :, :]\n    dist_mat = np.sqrt(np.sum(diff ** 2, axis=2))\n    k_distances = np.empty(n_samples)\n    neighbours = np.empty((n_samples, k), int)\n    for i in range(n_samples):\n        idx_sorted = np.argsort(dist_mat[i])\n        neigh_idx = idx_sorted[1:k + 1]\n        neighbours[i] = neigh_idx\n        k_distances[i] = dist_mat[i, neigh_idx[-1]]\n    lrd = np.empty(n_samples)\n    for p in range(n_samples):\n        reach_dist_sum = 0.0\n        for q in neighbours[p]:\n            reach_dist = max(k_distances[q], dist_mat[p, q])\n            reach_dist_sum += reach_dist\n        lrd[p] = k / reach_dist_sum\n    lof = np.empty(n_samples)\n    for p in range(n_samples):\n        lrd_sum = lrd[neighbours[p]].sum()\n        lof[p] = lrd_sum / k / lrd[p]\n    lof_rounded = np.round(lof, 4).tolist()\n    outlier_idx = [i for (i, score) in enumerate(lof) if score > epsilon]\n    return (lof_rounded, outlier_idx)"}
{"task_id": 574, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef generate_data(m: int, n_train: int, n_val: int, seed: int | None=None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the current global\n        random state is used unchanged.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays with shapes\n        (m\u00b7n_train, 2), (m\u00b7n_val, 2), (m\u00b7n_train,) and (m\u00b7n_val,)\n        respectively.  Every coordinate is rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    (X_tr, X_va) = ([], [])\n    (y_tr, y_va) = ([], [])\n    for c in range(m):\n        base = 10 * (c // 2)\n        for _ in range(n_train):\n            x = random.uniform(base + 1, base + 9)\n            y = random.uniform(base + 1, base + 9)\n            X_tr.append((x, y))\n            y_tr.append(c)\n        for _ in range(n_val):\n            x = random.uniform(base + 1, base + 9)\n            y = random.uniform(base + 1, base + 9)\n            X_va.append((x, y))\n            y_va.append(c)\n    X_train = np.round(np.asarray(X_tr, dtype=float), 4)\n    X_val = np.round(np.asarray(X_va, dtype=float), 4)\n    y_train = np.asarray(y_tr, dtype=int)\n    y_val = np.asarray(y_va, dtype=int)\n    return (X_train, X_val, y_train, y_val)"}
{"task_id": 579, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(X: np.ndarray, k: int, max_iters: int=100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Deterministic Lloyd-style K-Means clustering.\n\n    Args:\n        X        : (m, d) array with the input points.\n        k        : number of clusters, 1 \u2264 k \u2264 m.\n        max_iters: maximum amount of Lloyd iterations.\n\n    Returns\n    -------\n        labels  : list with length m, the cluster index of every point.\n        centres : list with k inner lists \u2013 the final centres, each\n                  coordinate rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    if X.ndim != 2:\n        raise ValueError('`X` must be a 2-D array (m, d).')\n    (m, d) = X.shape\n    if not 1 <= k <= m:\n        raise ValueError('`k` must satisfy 1 \u2264 k \u2264 number of points.')\n    if max_iters <= 0:\n        raise ValueError('`max_iters` must be positive.')\n    centres = X[:k].copy()\n    labels = np.full(m, -1, dtype=int)\n    for _ in range(max_iters):\n        diff = X[:, None, :] - centres[None, :, :]\n        dist2 = np.sum(diff ** 2, axis=2)\n        new_labels = np.argmin(dist2, axis=1)\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n        for i in range(k):\n            mask = labels == i\n            if np.any(mask):\n                centres[i] = X[mask].mean(axis=0)\n    centres_rounded = [[round(val, 4) for val in centre] for centre in centres]\n    return (labels.tolist(), centres_rounded)"}
{"task_id": 581, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regression(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int, l1_penalty: float, l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation using batch GD.\"\"\"\n    (m, n) = X.shape\n    if learning_rate == 0 or iterations == 0:\n        return ([0.0] * n, 0.0)\n    w = np.zeros(n, dtype=float)\n    b = 0.0\n    for _ in range(iterations):\n        y_hat = X @ w + b\n        r = y - y_hat\n        sign_w = np.sign(w)\n        sign_w[sign_w == 0] = -1\n        grad_w = -2 * X.T @ r + l1_penalty * sign_w + 2 * l2_penalty * w\n        grad_b = -2 * r.sum()\n        w -= learning_rate * (grad_w / m)\n        b -= learning_rate * (grad_b / m)\n    weights_rounded = [round(float(val), 4) for val in w]\n    bias_rounded = round(float(b), 4)\n    return (weights_rounded, bias_rounded)"}
{"task_id": 591, "completion_id": 0, "solution": "import numpy as np\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Forward pass of a fully\u2013connected ReLU network with a linear output layer.\n\n    Args:\n        weights : Three-level nested list holding all weight matrices.\n        x       : Input features (without bias).\n\n    Returns:\n        Network prediction rounded to four decimals.\n        Returns a float for a single output neuron, otherwise a list of floats.\n    \"\"\"\n    h = np.asarray(x, dtype=float)\n    n_layers = len(weights)\n    for (L, layer) in enumerate(weights):\n        W = np.asarray(layer, dtype=float)\n        h_aug = np.concatenate(([1.0], h))\n        z = W @ h_aug\n        h = np.maximum(0, z) if L < n_layers - 1 else z\n    if h.size == 1:\n        return round(float(h[0]), 4)\n    else:\n        return [round(v, 4) for v in h.tolist()]"}
{"task_id": 595, "completion_id": 0, "solution": "import numpy as np\ndef kmeans_centroids(X: 'np.ndarray', k: int, max_iters: int=100) -> list[list[float]]:\n    \"\"\"\n    K-Means clustering \u2013 return **centroids only**.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    k : int\n        Number of clusters.\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Sorted centroids (lexicographically), each coordinate rounded to 4 decimals.\n    \"\"\"\n    if k <= 0:\n        raise ValueError('k must be a positive integer')\n    if X.ndim != 2:\n        raise ValueError('X must be 2-dimensional')\n    X = X.astype(float, copy=False)\n    (n_samples, n_features) = X.shape\n    if k > n_samples:\n        raise ValueError('k cannot be larger than number of samples')\n    centroids = X[:k].copy()\n    for _ in range(max_iters):\n        distances = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = centroids.copy()\n        for j in range(k):\n            mask = labels == j\n            if np.any(mask):\n                new_centroids[j] = X[mask].mean(axis=0)\n        shifts = np.linalg.norm(new_centroids - centroids, axis=1)\n        centroids = new_centroids\n        if np.all(shifts < 0.0001):\n            break\n    centroids_list = centroids.tolist()\n    centroids_list.sort()\n    rounded = [[round(val, 4) for val in centre] for centre in centroids_list]\n    return rounded"}
