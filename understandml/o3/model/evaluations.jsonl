{"task_id": 0, "completion_id": 0, "passed": false, "result": "failed: name '_np' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef ridge_regression(data: np.ndarray, alpha: float=0.0) -> list[float]:\n    \"\"\"\n    Closed-form Ridge (L2-regularised) linear-regression solver.\n    \n    Parameters\n    ----------\n    data  : np.ndarray\n        2-D array where the last column is the target y and the remaining\n        columns are the features X.\n    alpha : float, default 0.0\n        Regularisation strength (\u03b1 \u2265 0).\n        \n    Returns\n    -------\n    list[float]\n        Weight vector (including the bias/intercept as the first element)\n        rounded to 4 decimal places.  \n        If (X\u1d40X + \u03b1I) is numerically singular the function returns -1.\n    \"\"\"\n    (X, y) = (data[:, :-1], data[:, -1])\n    n_samples = X.shape[0]\n    X_bias = np.hstack((np.ones((n_samples, 1)), X))\n    A = X_bias.T @ X_bias + alpha * np.eye(X_bias.shape[1])\n    try:\n        det = np.linalg.det(A)\n    except np.linalg.LinAlgError:\n        return -1\n    if np.isclose(det, 0.0, atol=1e-12):\n        return -1\n    w = np.linalg.inv(A) @ X_bias.T @ y\n    return np.round(w, 4).tolist()\nassert ridge_regression(_np.array([[1, 2], [2, 3], [3, 5]]), 0) == [1.5, 0.3333], \"Failed on OLS example 1\"\nassert ridge_regression(_np.array([[0, 1], [1, 3], [2, 5]]), 0) == [2.0, 1.0], \"Failed on OLS example 2\"\nassert ridge_regression(_np.array([[1, 2], [2, 4], [3, 6]]), 0) == [2.0, 0.0], \"Failed on perfect line\"\nassert ridge_regression(_np.array([[1, 3], [2, 5], [3, 7]]), 0) == [2.0, 1.0], \"Failed on y = 2x + 1\"\nassert ridge_regression(_np.array([[1, 2], [2, 3], [3, 5]]), 1) == [1.3333, 0.5], \"Failed on ridge alpha=1\"\nassert ridge_regression(_np.array([[0, 1], [1, 3], [2, 5]]), 2) == [1.4615, 0.9231], \"Failed on ridge alpha=2\"\nassert ridge_regression(_np.array([[1, 2], [2, 4]]), 10) == [0.5965, 0.3509], \"Failed on high-alpha small set\"\nassert ridge_regression(_np.array([[2, 4], [4, 8], [6, 12], [8, 16]]), 0) == [2.0, 0.0], \"Failed on multiple perfect line\"\nassert ridge_regression(_np.array([[1, 1], [2, 2], [3, 5], [4, 4]]), 0) == [1.2, 0.0], \"Failed on mixed dataset\"\nassert ridge_regression(_np.array([[-1, -1], [0, 1], [1, 3]]), 0) == [2.0, 1.0], \"Failed on negative x values\""}
{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef perceptron_dual(X_train: np.ndarray, y_train: np.ndarray, eta: float=1.0, n_iter: int=10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron learning algorithm.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples, shape (n_samples, n_features).\n    y_train : np.ndarray\n        Labels (+1 or -1), length n_samples.\n    eta : float, optional\n        Learning rate, default 1.0.\n    n_iter : int, optional\n        Maximum number of parameter updates, default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        Weight vector (as a list) and bias, both rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=float).ravel()\n    if not set(np.unique(y)).issubset({-1.0, 1.0}):\n        raise ValueError('y_train must contain only +1 and -1.')\n    (n_samples, n_features) = X.shape\n    alpha = np.zeros(n_samples, dtype=float)\n    b = 0.0\n    updates = 0\n    gram = X @ X.T\n    while updates < n_iter:\n        no_mistake_in_pass = True\n        for i in range(n_samples):\n            activation = np.dot(alpha * y, gram[:, i])\n            margin = y[i] * (activation + b)\n            if margin <= 0.0:\n                alpha[i] += eta\n                b += eta * y[i]\n                updates += 1\n                no_mistake_in_pass = False\n                break\n        if no_mistake_in_pass:\n            break\n    w = np.dot(alpha * y, X)\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return (w_rounded, b_rounded)\nassert (np.sign(np.dot(np.array([[2,2],[4,4],[4,0],[0,0]]), np.array(perceptron_dual(np.array([[2,2],[4,4],[4,0],[0,0]]), np.array([1,1,-1,-1]))[0])) + perceptron_dual(np.array([[2,2],[4,4],[4,0],[0,0]]), np.array([1,1,-1,-1]))[1]) == np.array([1,1,-1,-1])).all(), \"test case failed: perceptron_dual(simple 2D separable)\"\nassert (np.sign(np.dot(np.array([[0],[1]]), np.array(perceptron_dual(np.array([[0],[1]]), np.array([-1,1]))[0])) + perceptron_dual(np.array([[0],[1]]), np.array([-1,1]))[1]) == np.array([-1,1])).all(), \"test case failed: perceptron_dual(simple 1D separable)\"\nassert (np.sign(np.dot(np.array([[1,1],[2,2],[-1,-1],[-2,-2]]), np.array(perceptron_dual(np.array([[1,1],[2,2],[-1,-1],[-2,-2]]), np.array([1,1,-1,-1]))[0])) + perceptron_dual(np.array([[1,1],[2,2],[-1,-1],[-2,-2]]), np.array([1,1,-1,-1]))[1]) == np.array([1,1,-1,-1])).all(), \"test case failed: perceptron_dual(diagonal separable)\"\nassert (np.sign(np.dot(np.array([[1,0],[0,1],[0,-1],[-1,0]]), np.array(perceptron_dual(np.array([[1,0],[0,1],[0,-1],[-1,0]]), np.array([1,1,-1,-1]))[0])) + perceptron_dual(np.array([[1,0],[0,1],[0,-1],[-1,0]]), np.array([1,1,-1,-1]))[1]) == np.array([1,1,-1,-1])).all(), \"test case failed: perceptron_dual(axis-aligned)\"\nassert (np.sign(np.dot(np.array([[2,1],[2,2],[3,2],[-1,-1],[-2,-1],[-3,-2]]), np.array(perceptron_dual(np.array([[2,1],[2,2],[3,2],[-1,-1],[-2,-1],[-3,-2]]), np.array([1,1,1,-1,-1,-1]))[0])) + perceptron_dual(np.array([[2,1],[2,2],[3,2],[-1,-1],[-2,-1],[-3,-2]]), np.array([1,1,1,-1,-1,-1]))[1]) == np.array([1,1,1,-1,-1,-1])).all(), \"test case failed: perceptron_dual(mixed cluster)\"\nassert (np.sign(np.dot(np.array([[-2],[-1],[1],[2]]), np.array(perceptron_dual(np.array([[-2],[-1],[1],[2]]), np.array([-1,-1,1,1]))[0])) + perceptron_dual(np.array([[-2],[-1],[1],[2]]), np.array([-1,-1,1,1]))[1]) == np.array([-1,-1,1,1])).all(), \"test case failed: perceptron_dual(1D symmetric)\"\nassert (np.sign(np.dot(np.array([[1,5],[2,8],[3,12],[12,1],[10,2],[7,0]]), np.array(perceptron_dual(np.array([[1,5],[2,8],[3,12],[12,1],[10,2],[7,0]]), np.array([1,1,1,-1,-1,-1]))[0])) + perceptron_dual(np.array([[1,5],[2,8],[3,12],[12,1],[10,2],[7,0]]), np.array([1,1,1,-1,-1,-1]))[1]) == np.array([1,1,1,-1,-1,-1])).all(), \"test case failed: perceptron_dual(slanted line)\"\nassert (np.sign(np.dot(np.array([[1,0,0],[0,1,0],[0,0,1],[-1,0,0],[0,-1,0],[0,0,-1]]), np.array(perceptron_dual(np.array([[1,0,0],[0,1,0],[0,0,1],[-1,0,0],[0,-1,0],[0,0,-1]]), np.array([1,1,1,-1,-1,-1]))[0])) + perceptron_dual(np.array([[1,0,0],[0,1,0],[0,0,1],[-1,0,0],[0,-1,0],[0,0,-1]]), np.array([1,1,1,-1,-1,-1]))[1]) == np.array([1,1,1,-1,-1,-1])).all(), \"test case failed: perceptron_dual(3D axes)\"\nassert (np.sign(np.dot(np.array([[3,3],[4,4],[5,5],[-3,-3],[-4,-4],[-5,-5]]), np.array(perceptron_dual(np.array([[3,3],[4,4],[5,5],[-3,-3],[-4,-4],[-5,-5]]), np.array([1,1,1,-1,-1,-1]))[0])) + perceptron_dual(np.array([[3,3],[4,4],[5,5],[-3,-3],[-4,-4],[-5,-5]]), np.array([1,1,1,-1,-1,-1]))[1]) == np.array([1,1,1,-1,-1,-1])).all(), \"test case failed: perceptron_dual(scaled diagonal)\"\nassert (np.sign(np.dot(np.array([[0,2],[1,3],[2,5],[-2,0],[-3,1],[-4,2]]), np.array(perceptron_dual(np.array([[0,2],[1,3],[2,5],[-2,0],[-3,1],[-4,2]]), np.array([1,1,1,-1,-1,-1]))[0])) + perceptron_dual(np.array([[0,2],[1,3],[2,5],[-2,0],[-3,1],[-4,2]]), np.array([1,1,1,-1,-1,-1]))[1]) == np.array([1,1,1,-1,-1,-1])).all(), \"test case failed: perceptron_dual(tilted)\""}
{"task_id": 2, "completion_id": 0, "passed": false, "result": "failed: 'link_prime'", "solution": "import numpy as np\nimport numpy as np\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A dictionary with three entries  (\"identity\", \"log\", \"logit\").\n        Each entry itself is a dictionary with three callables\n\n            \u2022 \"link\"      :  g(\u03bc)\n            \u2022 \"inv_link\"  :  g\u207b\u00b9(\u03b7)\n            \u2022 \"derivative\":  g\u2032(\u03bc)\n\n        All functions work element\u2013wise on numpy scalars, 1-D and 2-D\n        numpy arrays.\n    \"\"\"\n\n    def _id_link(mu):\n        return mu\n\n    def _id_inv_link(eta):\n        return eta\n\n    def _id_derivative(mu):\n        return np.ones_like(mu, dtype=float)\n\n    def _log_link(mu):\n        return np.log(mu)\n\n    def _log_inv_link(eta):\n        return np.exp(eta)\n\n    def _log_derivative(mu):\n        return 1.0 / mu\n\n    def _logit_link(mu):\n        return np.log(mu / (1.0 - mu))\n\n    def _logit_inv_link(eta):\n        eta = np.asarray(eta)\n        return np.where(eta >= 0, 1.0 / (1.0 + np.exp(-eta)), np.exp(eta) / (1.0 + np.exp(eta)))\n\n    def _logit_derivative(mu):\n        return 1.0 / (mu * (1.0 - mu))\n    return {'identity': {'link': _id_link, 'inv_link': _id_inv_link, 'derivative': _id_derivative}, 'log': {'link': _log_link, 'inv_link': _log_inv_link, 'derivative': _log_derivative}, 'logit': {'link': _logit_link, 'inv_link': _logit_inv_link, 'derivative': _logit_derivative}}\nassert np.allclose(glm_links()[\"identity\"][\"link\"](np.array([3.0,-1.0])), np.array([3.0,-1.0])), \"test case failed: identity link\"\nassert np.allclose(glm_links()[\"identity\"][\"inv_link\"](np.array([-0.5,2.6])), np.array([-0.5,2.6])), \"test case failed: identity inverse link\"\nassert np.allclose(glm_links()[\"log\"][\"link\"](np.array([1.0,2.0])), np.log(np.array([1.0,2.0]))), \"test case failed: log link\"\nassert np.allclose(glm_links()[\"log\"][\"inv_link\"](np.array([0.0,1.0])), np.exp(np.array([0.0,1.0]))), \"test case failed: log inverse link\"\nassert np.allclose(glm_links()[\"log\"][\"link_prime\"](np.array([2.0,4.0])), np.array([0.5,0.25])), \"test case failed: log link derivative\"\nassert np.allclose(glm_links()[\"logit\"][\"link\"](np.array([0.2,0.8])), np.array([-1.38629436,1.38629436])), \"test case failed: logit link\"\nassert np.allclose(glm_links()[\"logit\"][\"inv_link\"](np.array([-1.38629436,1.38629436])), np.array([0.2,0.8])), \"test case failed: logit inverse link\"\nassert np.allclose(glm_links()[\"logit\"][\"link_prime\"](np.array([0.2,0.8])), 1.0/(np.array([0.2,0.8])*(1.0-np.array([0.2,0.8])))), \"test case failed: logit link derivative\"\nimport numpy as _np; _rng=_np.random.default_rng(1); _mu=_rng.uniform(0.05,0.95,50); _eta=glm_links()[\"logit\"][\"link\"](_mu); assert _np.allclose(glm_links()[\"logit\"][\"inv_link\"](_eta), _mu), \"test case failed: random roundtrip\"\nmu_test = np.array([0.3,0.6]).reshape(1,2); eta_test = glm_links()[\"logit\"][\"link\"](mu_test); assert eta_test.shape == (1,2) and glm_links()[\"logit\"][\"inv_link\"](eta_test).shape == (1,2), \"test case failed: shape preservation\""}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Return, for every sample, the depth of the leaf it lands in inside a\n    single random isolation tree grown as described in the task statement.\n    \"\"\"\n    np.random.seed(0)\n    if data.size == 0:\n        return []\n    (n_samples, n_features) = data.shape\n    depths = [-1] * n_samples\n\n    def grow(indices: list[int], depth: int) -> None:\n        \"\"\"Recursively grow the isolation tree while filling `depths`.\"\"\"\n        if len(indices) < 3:\n            for idx in indices:\n                depths[idx] = depth\n            return\n        f = np.random.randint(0, n_features)\n        vals = data[indices, f]\n        (down, up) = (vals.min(), vals.max())\n        if down == up:\n            for idx in indices:\n                depths[idx] = depth\n            return\n        v = np.random.uniform(down, up)\n        left = [idx for idx in indices if data[idx, f] <= v]\n        right = [idx for idx in indices if data[idx, f] > v]\n        if len(left) == 0 or len(right) == 0:\n            for idx in indices:\n                depths[idx] = depth\n            return\n        grow(left, depth + 1)\n        grow(right, depth + 1)\n    grow(list(range(n_samples)), depth=0)\n    return depths\nassert isolation_tree_path_length(np.arange(1).reshape(-1, 1)) == [0], \"test case failed: n=1\"\nassert isolation_tree_path_length(np.arange(2).reshape(-1, 1)) == [0, 0], \"test case failed: n=2\"\nassert isolation_tree_path_length(np.arange(3).reshape(-1, 1)) == [1, 1, 1], \"test case failed: n=3\"\nassert isolation_tree_path_length(np.arange(4).reshape(-1, 1)) == [1, 1, 1, 1], \"test case failed: n=4\"\nassert isolation_tree_path_length(np.arange(5).reshape(-1, 1)) == [2, 2, 2, 1, 1], \"test case failed: n=5\"\nassert isolation_tree_path_length(np.arange(6).reshape(-1, 1)) == [2, 2, 2, 2, 2, 2], \"test case failed: n=6\"\nassert isolation_tree_path_length(np.arange(7).reshape(-1, 1)) == [3, 3, 3, 2, 2, 2, 2], \"test case failed: n=7\"\nassert isolation_tree_path_length(np.arange(8).reshape(-1, 1)) == [3, 3, 3, 2, 2, 2, 2, 2], \"test case failed: n=8\"\nassert isolation_tree_path_length(np.arange(9).reshape(-1, 1)) == [3, 3, 3, 2, 2, 2, 2, 2, 2], \"test case failed: n=9\"\nassert isolation_tree_path_length(np.arange(10).reshape(-1, 1)) == [3, 3, 3, 2, 2, 3, 3, 3, 2, 2], \"test case failed: n=10\""}
{"task_id": 7, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nfrom collections import Counter\nfrom itertools import combinations\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable, Iterable, FrozenSet\ndef _sorted_tuple(it: Iterable[Hashable]) -> Tuple[Hashable, ...]:\n    \"\"\"\n    Return the items of *it* as an ascending tuple.\n\n    When item types are not mutually comparable we fall back to comparing their\n    textual representation, which is at least deterministic.\n    \"\"\"\n    try:\n        return tuple(sorted(it))\n    except TypeError:\n        return tuple(sorted(it, key=repr))\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent item-sets together with their absolute support.\n\n    Args\n    ----\n    transactions : list[list[hashable]]\n        Transactional data base \u2013 duplicates inside one transaction are ignored.\n    minsup       : float\n        Minimum support given as a fraction in the (0, 1] interval.\n\n    Returns\n    -------\n    dict[tuple, int]\n        Maps every frequent item-set (tuple with ascending items) to its support.\n        The empty dict is returned when no item-set fulfils the threshold.\n    \"\"\"\n    if not 0 < minsup <= 1:\n        raise ValueError('minsup must be in the interval (0, 1].')\n    transactions = [set(t) for t in transactions]\n    n_transactions = len(transactions)\n    if n_transactions == 0:\n        return {}\n    min_count = max(1, math.ceil(minsup * n_transactions))\n    result: Dict[Tuple[Hashable, ...], int] = {}\n    item_counter = Counter()\n    for t in transactions:\n        item_counter.update(t)\n    L_k: Dict[FrozenSet[Hashable], int] = {frozenset([item]): supp for (item, supp) in item_counter.items() if supp >= min_count}\n    for (fs, supp) in L_k.items():\n        result[_sorted_tuple(fs)] = supp\n    k = 2\n    while L_k:\n        L_prev = list(L_k.keys())\n        candidates: set[FrozenSet[Hashable]] = set()\n        for i in range(len(L_prev)):\n            for j in range(i + 1, len(L_prev)):\n                union = L_prev[i] | L_prev[j]\n                if len(union) != k:\n                    continue\n                if all((union - frozenset([item]) in L_k for item in union)):\n                    candidates.add(union)\n        if not candidates:\n            break\n        cand_counter = Counter()\n        for t in transactions:\n            for cand in candidates:\n                if cand.issubset(t):\n                    cand_counter[cand] += 1\n        L_k = {cand: supp for (cand, supp) in cand_counter.items() if supp >= min_count}\n        for (fs, supp) in L_k.items():\n            result[_sorted_tuple(fs)] = supp\n        k += 1\n    return result\nassert find_frequent_itemsets([['bread', 'milk'], ['bread', 'diaper', 'beer', 'egg'], ['milk', 'diaper', 'beer', 'coke'], ['bread', 'milk', 'diaper', 'beer'], ['bread', 'milk', 'diaper', 'coke']], 0.6) == {('bread',): 4, ('milk',): 4, ('diaper',): 4, ('beer',): 3, ('bread', 'milk'): 3, ('bread', 'diaper'): 3, ('diaper', 'milk'): 3, ('beer', 'diaper'): 3}, \"test case failed: bread/milk data set\"\nassert find_frequent_itemsets([[1, 2, 3], [1, 2], [1, 3], [2, 3], [1, 2, 3]], 0.6) == {(1,): 4, (2,): 4, (3,): 4, (1, 2): 3, (1, 3): 3, (2, 3): 3}, \"test case failed: numeric data set\"\nassert find_frequent_itemsets([['a', 'b'], ['b', 'c'], ['a', 'c']], 1.0) == {}, \"test case failed: no frequent itemset data set\"\nassert find_frequent_itemsets([[1], [1], [1], [2], [3]], 0.6) == {(1,): 3}, \"test case failed: single frequent item data set\"\nassert find_frequent_itemsets([], 0.4) == {}, \"test case failed: empty data set\"\nassert find_frequent_itemsets([['a', 'b', 'c'], ['a', 'b'], ['a', 'c'], ['b', 'c'], ['a', 'b', 'c'], ['a', 'b', 'c', 'd']], 0.5) == {('a',): 5, ('b',): 5, ('c',): 5, ('a', 'b'): 4, ('a', 'c'): 4, ('b', 'c'): 4, ('a', 'b', 'c'): 3}, \"test case failed: mixed letters data set\"\nassert find_frequent_itemsets([[1, 2], [2, 3], [3, 4], [1, 4]], 0.5) == {(1,): 2, (2,): 2, (3,): 2, (4,): 2}, \"test case failed: four-numbers data set\"\nassert find_frequent_itemsets([['x'] for _ in range(100)], 0.95) == {('x',): 100}, \"test case failed: 100 identical transactions data set\"\nassert find_frequent_itemsets([['p', 'q', 'r'], ['p', 'q'], ['p', 'r'], ['q', 'r'], ['p', 'q', 'r', 's']], 0.4) == {('p',): 4, ('q',): 4, ('r',): 4, ('p', 'q'): 3, ('p', 'r'): 3, ('q', 'r'): 3, ('p', 'q', 'r'): 2}, \"test case failed: p/q/r data set\"\nassert find_frequent_itemsets([['a'], ['b']], 1.0) == {}, \"test case failed: high minsup no frequent items\""}
{"task_id": 11, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kmeans(data: np.ndarray, k: int, epsilon: float=0.001, max_iter: int=2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array (n_samples \u00d7 n_features).\n    k : int\n        Number of desired clusters (1 \u2264 k \u2264 n_samples).\n    epsilon : float, default 1e-3\n        Stop if the largest centre movement is smaller than *epsilon*.\n    max_iter : int, default 2000\n        Hard iteration cap.\n\n    Returns\n    -------\n    (centres, labels)\n        centres : list[list[float]]\n            k cluster centres rounded to 4 decimals.\n        labels : list[int]\n            Cluster index (0 \u2026 k-1) assigned to every sample.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('data must be a 2-D array')\n    (n_samples, n_features) = data.shape\n    if not 1 <= k <= n_samples:\n        raise ValueError('k must satisfy 1 \u2264 k \u2264 number of samples')\n    data = data.astype(float, copy=False)\n    centres = data[:k].copy()\n    for _ in range(max_iter):\n        dists = np.sum((data[:, None, :] - centres[None, :, :]) ** 2, axis=2)\n        labels = np.argmin(dists, axis=1)\n        new_centres = centres.copy()\n        for j in range(k):\n            mask = labels == j\n            if mask.any():\n                new_centres[j] = data[mask].mean(axis=0)\n            else:\n                rnd_idx = np.random.randint(0, n_samples)\n                new_centres[j] = data[rnd_idx]\n        shifts = np.linalg.norm(new_centres - centres, axis=1)\n        if np.max(shifts) < epsilon:\n            centres = new_centres\n            break\n        centres = new_centres\n    centres_rounded = np.round(centres, 4).tolist()\n    return (centres_rounded, labels.tolist())\nassert kmeans(np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]]), 2) == ([[5.5, 1.0], [5.5, 4.0]], [0, 1, 0, 0, 1, 0]), \"test case failed: basic two-cluster example\"\nassert kmeans(np.array([[0, 0], [0, 2], [10, 0], [10, 2]]), 2) == ([[5.0, 0.0], [5.0, 2.0]], [0, 1, 0, 1]), \"test case failed: symmetric rectangle\"\nassert kmeans(np.array([[1, 2], [3, 4], [5, 6]]), 1) == ([[3.0, 4.0]], [0, 0, 0]), \"test case failed: single cluster mean\"\nassert kmeans(np.array([[0, 0], [1, 1], [2, 2]]), 3) == ([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]], [0, 1, 2]), \"test case failed: k equals number of samples\"\nassert kmeans(np.array([[0], [10]]), 2) == ([[0.0], [10.0]], [0, 1]), \"test case failed: one-dimensional data\"\nassert kmeans(np.zeros((3, 2)), 1) == ([[0.0, 0.0]], [0, 0, 0]), \"test case failed: all identical samples\"\nassert kmeans(np.array([[0, 0], [10, 10], [20, 20]]), 3) == ([[0.0, 0.0], [10.0, 10.0], [20.0, 20.0]], [0, 1, 2]), \"test case failed: three isolated samples\"\nassert kmeans(np.array([[0, 0], [1, 0], [0, 1], [1, 1]]), 1) == ([[0.5, 0.5]], [0, 0, 0, 0]), \"test case failed: square to single centre\"\nassert kmeans(np.array([[2, 2], [2, 2], [2, 2]]), 1) == ([[2.0, 2.0]], [0, 0, 0]), \"test case failed: duplicate points single centre\"\nassert kmeans(np.array([[0, 0], [1, 1], [0, 0], [1, 1]]), 2) == ([[0.0, 0.0], [1.0, 1.0]], [0, 1, 0, 1]), \"test case failed: duplicates two centres\""}
{"task_id": 13, "completion_id": 0, "passed": false, "result": "failed: name 'D1' is not defined", "solution": "import numpy as np\nfrom collections import defaultdict\nimport numpy as np\nfrom collections import defaultdict\ndef knn_recommend(data: np.ndarray, user_ind: int, k: int, criterion: str='cosine') -> list[int]:\n    \"\"\"Item\u2013based k-NN collaborative\u2013filtering recommender.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Rating matrix of shape (n_user, n_item); 0 = *not rated*.\n    user_ind : int\n        Index of the active user.\n    k : int\n        Maximal number of items to recommend.\n    criterion : {'cosine', 'pearson'}, default 'cosine'\n        Similarity measure to use.\n\n    Returns\n    -------\n    list[int]\n        Indices of at most *k* unseen items ordered by decreasing\n        predicted attractiveness.\n    \"\"\"\n    if criterion not in ('cosine', 'pearson'):\n        raise ValueError(\"criterion must be either 'cosine' or 'pearson'\")\n    (n_user, n_item) = data.shape\n    if not 0 <= user_ind < n_user:\n        raise IndexError('user_ind out of bounds')\n    S = np.zeros((n_item, n_item), dtype=np.float64)\n    for i in range(n_item - 1):\n        col_i = data[:, i]\n        r_i_mask = col_i > 0\n        for j in range(i + 1, n_item):\n            col_j = data[:, j]\n            common = r_i_mask & (col_j > 0)\n            if not np.any(common):\n                continue\n            v1 = col_i[common].astype(np.float64)\n            v2 = col_j[common].astype(np.float64)\n            if criterion == 'cosine':\n                if np.std(v1, ddof=1) > 0.001:\n                    v1c = v1 - v1.mean()\n                else:\n                    v1c = v1\n                if np.std(v2, ddof=1) > 0.001:\n                    v2c = v2 - v2.mean()\n                else:\n                    v2c = v2\n                (n1, n2) = (np.linalg.norm(v1c), np.linalg.norm(v2c))\n                sim = 0.0 if n1 == 0.0 or n2 == 0.0 else float(v1c @ v2c / (n1 * n2))\n            elif len(v1) < 2 or np.std(v1, ddof=1) < 0.001 or np.std(v2, ddof=1) < 0.001:\n                sim = 0.0\n            else:\n                sim = float(np.corrcoef(v1, v2, ddof=1)[0, 1])\n                if np.isnan(sim):\n                    sim = 0.0\n            S[i, j] = S[j, i] = sim\n    user_row = data[user_ind]\n    rated_mask = user_row > 0\n    rated_idx = np.where(rated_mask)[0]\n    if len(rated_idx) == n_item:\n        return []\n    scores = {}\n    for t in range(n_item):\n        if rated_mask[t]:\n            continue\n        sims_to_rated = S[t, rated_idx]\n        ratings = user_row[rated_idx]\n        denom = np.sum(np.abs(sims_to_rated))\n        if denom == 0.0:\n            score = 0.0\n        else:\n            score = float(ratings @ sims_to_rated / denom)\n        scores[t] = score\n    sorted_items = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n    return [idx for (idx, _) in sorted_items[:k]]\nassert knn_recommend(D1, 0, 1) == [1], \"failed: knn_recommend(D1,0,1)\"\nassert knn_recommend(D1, 2, 1) == [0], \"failed: knn_recommend(D1,2,1)\"\nassert knn_recommend(D1, 0, 1, 'pearson') == [1], \"failed: pearson D1 user0\"\nassert knn_recommend(D1, 1, 1) == [1], \"failed: knn_recommend(D1,1,1)\"\nassert knn_recommend(D1, 3, 1) == [0], \"failed: knn_recommend(D1,3,1)\"\nassert knn_recommend(D1, 0, 2) == [1], \"failed: k larger than unrated items\"\nassert knn_recommend(D2, 0, 1) == [0], \"failed: knn_recommend(D2,0,1)\"\nassert knn_recommend(D2, 1, 1) == [1], \"failed: knn_recommend(D2,1,1)\"\nassert knn_recommend(D2, 2, 1) == [2], \"failed: knn_recommend(D2,2,1)\"\nassert knn_recommend(D3, 0, 1) == [0], \"failed: knn_recommend(D3,0,1)\""}
{"task_id": 19, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef best_split(X: np.ndarray, g: np.ndarray, h: np.ndarray, gamma: float, lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split maximises the reduction in the regularised loss used by\n    gradient-boosting decision trees.  If no legal split gives a positive\n    gain the function returns ``None``.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    if n_samples < 4:\n        return None\n\n    def leaf_loss(G: float, H: float) -> float:\n        return -0.5 * (G * G) / (H + lam) + gamma\n    G_total = g.sum()\n    H_total = h.sum()\n    parent_loss = leaf_loss(G_total, H_total)\n    best_gain = 0.0\n    best_feature = None\n    best_threshold = None\n    for feat_idx in range(n_features):\n        order = np.argsort(X[:, feat_idx], kind='mergesort')\n        x_sorted = X[order, feat_idx]\n        g_sorted = g[order]\n        h_sorted = h[order]\n        g_cumsum = np.cumsum(g_sorted)\n        h_cumsum = np.cumsum(h_sorted)\n        for i in range(1, n_samples - 2):\n            if x_sorted[i] == x_sorted[i + 1]:\n                continue\n            G_left = g_cumsum[i]\n            H_left = h_cumsum[i]\n            G_right = G_total - G_left\n            H_right = H_total - H_left\n            if n_samples - i - 1 < 2:\n                break\n            loss_left = leaf_loss(G_left, H_left)\n            loss_right = leaf_loss(G_right, H_right)\n            gain = parent_loss - loss_left - loss_right\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = feat_idx\n                best_threshold = x_sorted[i]\n    if best_feature is None:\n        return None\n    return (best_feature, float(best_threshold))\nassert best_split(np.array([[2],[4],[6],[8]]), np.array([ 1, 1,-1,-1]), np.array([1,1,1,1]), 0.1, 1.0)==(0,4), \"test-case 1 failed\"\nassert best_split(np.array([[1],[2],[3],[4],[5]]), np.array([5,4,3,2,1]), np.ones(5), 0.2, 1.0)==(0,3), \"test-case 2 failed\"\nassert best_split(np.array([[1],[2],[3],[4],[5]]), np.array([-5,-4,-3,-2,-1]), np.ones(5), 0.2, 0.5)==(0,3), \"test-case 3 failed\"\nassert best_split(np.array([[1],[2],[3],[4]]), np.array([1,-1,1,-1]), np.ones(4), 0.8, 1.0) is None, \"test-case 4 failed\"\nassert best_split(np.array([[0,0],[1,1],[2,2],[3,3],[4,4]]), np.array([1,1,1,-1,-1]), np.ones(5), 0.1, 1.0)==(0,2), \"test-case 5 failed\"\nassert best_split(np.array([[1],[2],[3],[4],[5],[6]]), np.array([0,0,0,0,0,0]), np.ones(6), 0.1, 1.0) is None, \"test-case 7 failed\"\nassert best_split(np.array([[10],[20],[30],[40]]), np.array([10,-5,-5,0]), np.ones(4), 0.05, 1.0)==(0,20), \"test-case 8 failed\"\nassert best_split(np.array([[2],[2],[2],[2]]), np.array([1,1,1,1]), np.ones(4), 0.1, 1.0) is None, \"test-case 10 failed\""}
{"task_id": 20, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    is_scalar = np.isscalar(x)\n    x_arr = np.asarray(x, dtype=float)\n    sigmoid_arr = 1.0 / (1.0 + np.exp(-x_arr))\n    gradient_arr = sigmoid_arr * (1.0 - sigmoid_arr)\n    sigmoid_arr = np.round(sigmoid_arr, 4)\n    gradient_arr = np.round(gradient_arr, 4)\n    if is_scalar:\n        return (float(sigmoid_arr), float(gradient_arr))\n    else:\n        return (sigmoid_arr.tolist(), gradient_arr.tolist())\nassert sigmoid_activation(0) == (0.5, 0.25), \"failed on scalar 0\"\nassert sigmoid_activation([-1, 0, 1]) == ([0.2689, 0.5, 0.7311], [0.1966, 0.25, 0.1966]), \"failed on list [-1,0,1]\"\nassert sigmoid_activation(np.array([-3, 3])) == ([0.0474, 0.9526], [0.0452, 0.0452]), \"failed on np.array([-3,3])\"\nassert sigmoid_activation([10]) == ([1.0], [0.0]), \"failed on list [10]\"\nassert sigmoid_activation(10) == (1.0, 0.0), \"failed on scalar 10\"\nassert sigmoid_activation(-1000) == (0.0, 0.0), \"failed on large negative scalar\"\nassert sigmoid_activation([0]) == ([0.5], [0.25]), \"failed on list [0]\""}
{"task_id": 21, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport math\nfrom collections import defaultdict\nimport numpy as np\nimport math\nfrom collections import defaultdict\ndef adaboost_1d_predict(x_train: list[float], y_train: list[int], x_test: list[float], epsilon: float=0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble of decision stumps and returns test predictions.\"\"\"\n\n    def stump_predict(x: np.ndarray, d: int, theta: float) -> np.ndarray:\n        \"\"\"Return \u00b11 predictions of the stump (d , \u03b8) on vector x.\"\"\"\n        if d == 0:\n            return np.where(x <= theta, 1, -1)\n        else:\n            return np.where(x > theta, 1, -1)\n    x_train = np.asarray(x_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    x_test = np.asarray(x_test, dtype=float)\n    n = len(x_train)\n    if n == 0:\n        return [1] * len(x_test)\n    order = np.argsort(x_train)\n    x_sorted = x_train[order]\n    thresholds = [x_sorted[0] - 1.0]\n    for i in range(n - 1):\n        if x_sorted[i] != x_sorted[i + 1]:\n            thresholds.append(0.5 * (x_sorted[i] + x_sorted[i + 1]))\n    thresholds.append(x_sorted[-1] + 1.0)\n    stumps = [(d, th) for th in thresholds for d in (0, 1)]\n    w = np.full(n, 1.0 / n)\n    (alphas, chosen_stumps) = ([], [])\n    while True:\n        best_err = float('inf')\n        best_pred = None\n        best_stump = None\n        for (d, th) in stumps:\n            pred = stump_predict(x_train, d, th)\n            err = w[pred != y_train].sum()\n            if err < best_err - 1e-12:\n                (best_err, best_stump, best_pred) = (err, (d, th), pred)\n        if best_err >= 0.5 - 1e-12:\n            break\n        err = max(best_err, 1e-12)\n        alpha = 0.5 * math.log((1.0 - err) / err)\n        chosen_stumps.append(best_stump)\n        alphas.append(alpha)\n        w *= np.exp(-alpha * y_train * best_pred)\n        w = w / w.sum()\n        F = np.zeros(n)\n        for (a, (d, th)) in zip(alphas, chosen_stumps):\n            F += a * stump_predict(x_train, d, th)\n        train_pred = np.where(F >= 0.0, 1, -1)\n        train_error = (train_pred != y_train).mean()\n        if train_error <= epsilon:\n            break\n    if not alphas:\n        return [1] * len(x_test)\n    F_test = np.zeros(len(x_test))\n    for (a, (d, th)) in zip(alphas, chosen_stumps):\n        F_test += a * stump_predict(x_test, d, th)\n    y_pred = np.where(F_test >= 0.0, 1, -1)\n    return y_pred.tolist()\nassert adaboost_1d_predict([1,2,3,4],[1,1,-1,-1],[1.5,3.5])==[1,-1],\"failed on basic separable set\"\nassert adaboost_1d_predict([1,2,3,4,5],[1,1,1,-1,-1],[1,5])==[1,-1],\"failed on unbalanced set\"\nassert adaboost_1d_predict([0,1,2,3],[1,1,-1,-1],[0.5,2.5])==[1,-1],\"failed on shift threshold\"\nassert adaboost_1d_predict([-3,-2,-1,0],[-1,-1,1,1],[-2.5,-0.5])==[-1,1],\"failed on negative values\"\nassert adaboost_1d_predict([10,20,30,40],[1,1,-1,-1],[15,35])==[1,-1],\"failed on large values\"\nassert adaboost_1d_predict([1,3,5,7],[1,1,-1,-1],[2,6])==[1,-1],\"failed on odd spacing\"\nassert adaboost_1d_predict([2,4,6,8],[1,1,-1,-1],[3,7],epsilon=0)==[1,-1],\"failed with explicit epsilon\"\nassert adaboost_1d_predict([0.1,0.2,0.8,0.9],[1,1,-1,-1],[0.15,0.85])==[1,-1],\"failed on float inputs\"\nassert adaboost_1d_predict([5,6,7,8,9,10],[1,1,1,-1,-1,-1],[5.5,9.5])==[1,-1],\"failed on bigger set\"\nassert adaboost_1d_predict([-5,-4,-3,-2,-1,0],[1,1,1,-1,-1,-1],[-4.5,-0.5])==[1,-1],\"failed on negative range\""}
{"task_id": 25, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef gaussian_svm_predict(X_train: np.ndarray, y_train: np.ndarray, alpha: np.ndarray, b: float, gamma: float, X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    y_train = y_train.ravel()\n    alpha = alpha.ravel()\n    coef = alpha * y_train\n    train_sq = np.sum(X_train ** 2, axis=1)\n    test_sq = np.sum(X_test ** 2, axis=1)\n    dists_sq = test_sq[:, None] + train_sq[None, :] - 2.0 * X_test @ X_train.T\n    K = np.exp(-gamma * dists_sq)\n    g = K @ coef + b\n    pred = np.sign(g)\n    pred[pred == 0] = 1\n    return pred.astype(int).tolist()\nassert gaussian_svm_predict(np.array([[1,2],[2,3]]),np.array([1,-1]),np.array([0.6,0.4]),0.1,0.5,np.array([[1.5,2.5]]))==[1],\"Failed on single test sample.\"\nassert gaussian_svm_predict(np.array([[1,0],[0,1]]),np.array([1,-1]),np.array([0.9,0.9]),0.0,2.0,np.array([[0.9,0.1],[0.1,0.9]]))==[1,-1],\"Failed orthogonal samples.\"\nassert gaussian_svm_predict(np.array([[1,2],[3,4]]),np.array([1,1]),np.array([0.5,0.5]),-0.6,0.3,np.array([[2,3]]))==[-1],\"Bias impact failed.\"\nassert gaussian_svm_predict(np.array([[0,0],[0,1],[1,0],[1,1]]),np.array([1,-1,-1,1]),np.array([0.3,0.3,0.3,0.3]),0.0,1.0,np.array([[0.5,0.5],[1.5,1.5]]))==[1,1],\"Failed on XOR-like points.\"\nassert gaussian_svm_predict(np.array([[2]]),np.array([-1]),np.array([0.9]),0.0,1.0,np.array([[1],[3]]))==[-1,-1],\"Failed 1-D input.\"\nassert gaussian_svm_predict(np.array([[1,2,3],[4,5,6]]),np.array([1,-1]),np.array([0.4,0.6]),0.0,0.2,np.array([[1,2,3],[4,5,6]]))==[1,-1],\"Failed predictions identical to training points.\"\nassert gaussian_svm_predict(np.array([[1,2]]),np.array([1]),np.array([1.0]),-1.5,0.1,np.array([[10,20]]))==[-1],\"Far away point should follow bias.\""}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: name 'a' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        (X_hat, mse) where\n            \u2022 X_hat \u2013 rank-k reconstruction of X (each entry rounded to 4 decimals)\n            \u2022 mse    \u2013 mean-squared reconstruction error (rounded to 4 decimals)\n        If k is not in [1, min(m, n)] the function returns -1.\n    \"\"\"\n    if not isinstance(k, int):\n        return -1\n    try:\n        X_arr = np.asarray(X, dtype=float)\n    except Exception:\n        return -1\n    if X_arr.ndim != 2:\n        return -1\n    (m, n) = X_arr.shape\n    if k < 1 or k > min(m, n):\n        return -1\n    (U, S, VT) = np.linalg.svd(X_arr, full_matrices=False)\n    U_k = U[:, :k]\n    S_k = S[:k]\n    VT_k = VT[:k, :]\n    X_hat = U_k * S_k @ VT_k\n    mse = float(np.round(np.mean((X_arr - X_hat) ** 2), 4))\n    X_hat_rounded = np.round(X_hat, 4).tolist()\n    return (X_hat_rounded, mse)\nassert a([[1, 0], [0, 1]], 1) == ([[1.0, 0.0], [0.0, 0.0]], 0.25), \"test case failed: a([[1,0],[0,1]],1)\"\nassert a([[2, 2], [2, 2]], 1) == ([[2.0, 2.0], [2.0, 2.0]], 0.0), \"test case failed: a([[2,2],[2,2]],1)\"\nassert a([[3, 1], [1, 3]], 1) == ([[2.0, 2.0], [2.0, 2.0]], 1.0), \"test case failed: a([[3,1],[1,3]],1)\"\nassert a([[4, 0], [0, 2]], 1) == ([[4.0, 0.0], [0.0, 0.0]], 1.0), \"test case failed: a([[4,0],[0,2]],1)\"\nassert a([[1, 0, 0], [0, 1, 0], [0, 0, 1]], 2) == ([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]], 0.1111), \"test case failed: a(I3,2)\"\nassert a([[1, 2], [3, 4]], 2) == ([[1.0, 2.0], [3.0, 4.0]], 0.0), \"test case failed: a([[1,2],[3,4]],2)\"\nassert a([[1, 0], [0, 1]], 0) == -1, \"test case failed: a(k=0)\"\nassert a([[1, 0], [0, 1]], 3) == -1, \"test case failed: a(k>min(m,n))\"\nassert a([[1, 2, 3], [2, 4, 6]], 1) == ([[1.0, 2.0, 3.0], [2.0, 4.0, 6.0]], 0.0), \"test case failed: a(rank-1 2x3,1)\"\nassert a([[5], [10], [15], [20]], 1) == ([[5.0], [10.0], [15.0], [20.0]], 0.0), \"test case failed: a(single column,1)\""}
{"task_id": 29, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nimport numpy as np\ndef predict_boosting_tree(x_train: list[float] | np.ndarray, y_train: list[float] | np.ndarray, x_query: float, epsilon: float=0.01) -> float:\n    \"\"\"1-D gradient boosting with decision stumps and prediction for *x_query*.\n    \"\"\"\n    x = np.asarray(x_train, dtype=float).ravel()\n    y = np.asarray(y_train, dtype=float).ravel()\n    if x.size == 0:\n        return float('nan')\n    if np.all(x == x[0]) or x.size == 1:\n        return round(float(y.mean()), 4)\n    order = np.argsort(x)\n    x_sorted = x[order]\n    y_sorted = y[order]\n    distinct_mask = x_sorted[1:] != x_sorted[:-1]\n    mids = (x_sorted[:-1] + x_sorted[1:]) / 2.0\n    candidate_s = mids[distinct_mask]\n    if candidate_s.size == 0:\n        return round(float(y.mean()), 4)\n    residuals = y_sorted.copy()\n    stumps = []\n    rss = np.sum(residuals ** 2)\n    max_iter = 1000\n    iteration = 0\n    while rss > epsilon and iteration < max_iter:\n        best_rss = np.inf\n        best_params = None\n        for s in candidate_s:\n            mask_left = x_sorted <= s\n            mask_right = ~mask_left\n            if not mask_left.any() or not mask_right.any():\n                continue\n            c1 = residuals[mask_left].mean()\n            c2 = residuals[mask_right].mean()\n            new_residuals = residuals - np.where(mask_left, c1, c2)\n            rss_candidate = np.sum(new_residuals ** 2)\n            if rss_candidate < best_rss:\n                best_rss = rss_candidate\n                best_params = (s, c1, c2, mask_left)\n        if best_params is None or best_rss >= rss - 1e-12:\n            break\n        (s_best, c1_best, c2_best, mask_left_best) = best_params\n        stumps.append((s_best, c1_best, c2_best))\n        residuals = residuals - np.where(mask_left_best, c1_best, c2_best)\n        rss = best_rss\n        iteration += 1\n    prediction = 0.0\n    for (s, c1, c2) in stumps:\n        prediction += c1 if x_query <= s else c2\n    return round(float(prediction), 4)\nassert predict_boosting_tree([1,2,3,4],[1.5,1.5,3.5,3.5],3)==3.5, \"failed: piece-wise constant right side\"\nassert predict_boosting_tree([1,2,3,4],[1.5,1.5,3.5,3.5],2)==1.5, \"failed: piece-wise constant left side\"\nassert predict_boosting_tree([1,2,4,6],[2,2,4,4],5)==4.0, \"failed: split at 3.0, right prediction\"\nassert predict_boosting_tree([1,2,4,6],[2,2,4,4],2)==2.0, \"failed: split at 3.0, left prediction\"\nassert predict_boosting_tree([1,3,5,7],[10,10,20,20],6)==20.0, \"failed: right side prediction 20\"\nassert predict_boosting_tree([1,3,5,7],[10,10,20,20],2)==10.0, \"failed: left side prediction 10\"\nassert predict_boosting_tree([1,2,3,4],[0,0,0,0],3)==0.0, \"failed: all zeros\"\nassert predict_boosting_tree([1,1.5,2],[2,2,2],1.2)==2.0, \"failed: identical targets\"\nassert predict_boosting_tree([1,2,3,4,5,6],[1,1,1,2,2,2],4)==2.0, \"failed: two-segment data, right\"\nassert predict_boosting_tree([1,2,3,4,5,6],[1,1,1,2,2,2],2)==1.0, \"failed: two-segment data, left\""}
{"task_id": 34, "completion_id": 0, "passed": false, "result": "failed: test failed: shape (2,3), sparsity 0.3, seed 42", "solution": "import numpy as np\nimport numpy as np\ndef random_binary_tensor(shape: tuple[int, ...], sparsity: float=0.5, seed: int | None=None):\n    \"\"\"Create a tensor whose entries are 0.0 or 1.0.\n    \n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired shape of the returned NumPy array.\n    sparsity : float, default=0.5\n        Probability that any individual entry is 1.0.  Must satisfy\n        0.0 \u2264 sparsity \u2264 1.0.  Values outside this range cause the\n        function to return -1.\n    seed : int | None, default=None\n        If provided, ``np.random.seed(seed)`` is called first so the\n        output is reproducible.\n    \n    Returns\n    -------\n    numpy.ndarray | int\n        \u2022 A floating-dtype NumPy array containing only 0.0 and 1.0, if\n          `sparsity` is legal.  \n        \u2022 The integer -1 when `sparsity` is outside the closed interval\n          [0, 1].\n    \"\"\"\n    if not 0.0 <= sparsity <= 1.0:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_values = np.random.rand(*shape)\n    binary_tensor = (random_values < sparsity).astype(float)\n    return binary_tensor\nassert (random_binary_tensor((2,3),0.3,42)==np.array([[0.,1.,1.],[0.,0.,0.]])).all(), \"test failed: shape (2,3), sparsity 0.3, seed 42\"\nassert (random_binary_tensor((3,),0.8,1)==np.array([1.,1.,0.])).all(), \"test failed: shape (3,), sparsity 0.8, seed 1\"\nassert (random_binary_tensor((2,2),0)==np.zeros((2,2))).all(), \"test failed: all zeros when sparsity 0\"\nassert (random_binary_tensor((2,2),1)==np.ones((2,2))).all(), \"test failed: all ones when sparsity 1\"\nassert random_binary_tensor((1,),-0.1)==-1, \"test failed: invalid sparsity -0.1\"\nassert random_binary_tensor((1,),1.1)==-1, \"test failed: invalid sparsity 1.1\"\narr=random_binary_tensor((1000,),0.4,123); assert abs(arr.mean()-0.4)<0.05, \"test failed: empirical sparsity deviates >5% for seed 123\"\nassert random_binary_tensor((5,4),0.5).shape==(5,4), \"test failed: incorrect shape (5,4)\"\ntensor=random_binary_tensor((2,3,4),0.6,7); assert tensor.dtype.kind=='f' and set(np.unique(tensor)).issubset({0.0,1.0}), \"test failed: dtype or values incorrect for 3-D shape\"\nassert random_binary_tensor((10,),0.25,55).sum()==(random_binary_tensor((10,),0.25,55)).sum(), \"test failed: function must be deterministic with same seed\""}
{"task_id": 39, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median\n          (upper median for even *n*);\n        \u2022 left  is a list of row indices with strictly smaller values;\n        \u2022 right is a list of row indices with strictly larger  values.\n        The two lists are sorted increasingly.\n    \"\"\"\n    col = data[:, d]\n    n = col.shape[0]\n    k = n // 2\n    part_idx = np.argpartition(col, k)\n    pivot = int(part_idx[k])\n    pivot_value = col[pivot]\n    left = []\n    right = []\n    for idx in range(n):\n        if idx == pivot:\n            continue\n        v = col[idx]\n        if v < pivot_value:\n            left.append(idx)\n        elif v > pivot_value:\n            right.append(idx)\n    return (pivot, left, right)\nassert get_split(np.array([[2,3],[1,5],[4,1],[0,2],[3,4]]),0) == (0,[1,3],[2,4]), \"failed on example 1\"\nassert get_split(np.array([[10],[20],[30],[40]]),0) == (2,[0,1],[3]), \"failed on even n, single column\"\nassert get_split(np.array([[1,9],[2,8],[3,7],[4,6],[5,5]]),1) == (2,[3,4],[0,1]), \"failed on different column\"\nassert get_split(np.arange(1,11).reshape(-1,1),0) == (5,[0,1,2,3,4],[6,7,8,9]), \"failed on 10 sequential numbers\"\nassert get_split(np.array([[8],[3],[6],[2],[7],[4]]),0) == (2,[1,3,5],[0,4]), \"failed on shuffled unique values\"\nassert get_split(np.array([[5,1],[4,2],[3,3],[2,4],[1,5]]),1) == (2,[0,1],[3,4]), \"failed on descending column 1\"\nassert get_split(np.array([[10],[20],[30],[40],[50],[60],[70]]),0) == (3,[0,1,2],[4,5,6]), \"failed on 7 items\"\nassert get_split(np.array([[100],[200]]),0) == (1,[0],[]), \"failed on two elements\"\nassert get_split(np.array([[42]]),0) == (0,[],[]), \"failed on single element\"\nassert get_split(np.array([[1,5,9],[2,6,8],[3,7,7],[4,4,6],[5,3,5]]),2) == (2,[3,4],[0,1]), \"failed on 3rd column split\""}
{"task_id": 40, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool=False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into equally sized, possibly overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length N.\n    frame_width : int\n        Number of samples per frame.\n    stride : int\n        Hop size (number of samples between the starts of successive frames).\n    writeable : bool, default=False\n        If False the returned view is marked read-only.\n        If True the view is writeable only when the source array is writeable.\n\n    Returns\n    -------\n    np.ndarray\n        View on *x* with shape (n_frames, frame_width), where\n            n_frames = (len(x) - frame_width) // stride + 1\n    \"\"\"\n    assert x.ndim == 1, 'x must be one-dimensional'\n    assert frame_width > 0, 'frame_width must be a positive integer'\n    assert stride > 0, 'stride must be a positive integer'\n    assert len(x) >= frame_width, 'frame_width larger than signal length'\n    n_frames = (len(x) - frame_width) // stride + 1\n    shape = (n_frames, frame_width)\n    item_stride = x.strides[0]\n    strides = (item_stride * stride, item_stride)\n    view_writeable = x.flags.writeable if writeable else False\n    frames = as_strided(x, shape=shape, strides=strides, writeable=view_writeable)\n    return frames\n# 1\nx = np.arange(6)\nassert np.array_equal(to_frames(x, 5, 1), np.array([[0,1,2,3,4],[1,2,3,4,5]])), 'test case failed: to_frames(np.arange(6),5,1)'\n# 2\nassert np.array_equal(to_frames(np.arange(10),3,2), np.array([[0,1,2],[2,3,4],[4,5,6],[6,7,8]])), 'test case failed: to_frames(np.arange(10),3,2)'\n# 3\nassert np.array_equal(to_frames(np.arange(10),4,4), np.array([[0,1,2,3],[4,5,6,7]])), 'test case failed: to_frames(np.arange(10),4,4)'\n# 4\nassert to_frames(np.arange(8),4,2).flags.writeable is False, 'test case failed: writeable flag when default False'\n# 5\nx = np.arange(8)\nframes = to_frames(x,4,2,writeable=True)\nframes[0,0]=123\nassert x[0]==123, 'test case failed: writeable True reflection'\n# 6\nrand = np.random.randn(17)\nframes = to_frames(rand,5,3)\nassert frames.shape == ((17-5)//3+1,5), 'test case failed: random shape'\n# 7\nassert np.array_equal(to_frames(np.arange(10),10,1), np.arange(10).reshape(1,-1)), 'test case failed: single frame'\n# 8\nassert np.array_equal(to_frames(np.arange(9),4,3), np.array([[0,1,2,3],[3,4,5,6]])), 'test case failed: tail trimming'\n# 9\nx = np.arange(4)\nassert np.array_equal(to_frames(x,1,1), x.reshape(-1,1)), 'test case failed: frame width 1'\n# 10\nx = np.arange(25)\nframes = to_frames(x,5,6)\nassert frames.shape == ((25-5)//6+1,5), 'test case failed: stride > width'"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.GenericAlias' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef is_stochastic(X: list[list[float]] | 'np.ndarray') -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\"\"\"\n    try:\n        A = np.asarray(X, dtype=float)\n    except Exception:\n        return False\n    if A.ndim != 2 or A.size == 0:\n        return False\n    if (~np.isfinite(A)).any():\n        return False\n    if ((A < 0) | (A > 1)).any():\n        return False\n    if not np.allclose(A.sum(axis=1), 1.0, atol=1e-08):\n        return False\n    return True\nassert is_stochastic([[1.0]]) is True, \"Failed on 1\u00d71 identity\"\nassert is_stochastic([[0.5, 0.5], [0.1, 0.9]]) is True, \"Failed on simple 2\u00d72 matrix\"\nassert is_stochastic([[0.2, 0.5], [0.3, 0.3]]) is False, \"Rows do not sum to 1\"\nassert is_stochastic([[0.2, -0.2], [0.4, 0.6]]) is False, \"Negative entry allowed\"\nassert is_stochastic([[1.2, -0.2], [0.4, 0.6]]) is False, \"Entry greater than 1 allowed\"\nassert is_stochastic([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) is True, \"Failed on 3\u00d73 identity\"\nassert is_stochastic([[0.3333, 0.3333, 0.3334]]) is True, \"Tolerance handling failed\"\nassert is_stochastic([[0.2, 0.3, 0.5], [0.1, 0.1, 0.8]]) is True, \"Failed on 2\u00d73 valid matrix\"\nassert is_stochastic([[0.2, 0.3, 0.6], [0.1, 0.1, 0.8]]) is False, \"First row sums to 1.1\"\nassert is_stochastic(np.array([[0.7, 0.2, 0.1]])) is True, \"Failed on NumPy input\""}
{"task_id": 55, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rms_prop(X: np.ndarray, y: np.ndarray, epsilon: float=0.0001, max_iter: int=10000, eta: float=0.01, rho: float=0.9, batch_size: int=32, eps_station: float=1e-08) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    (n, d) = X.shape\n    w = np.zeros(d)\n    s = np.zeros(d)\n    batch_size = n if n < batch_size else batch_size\n    for it in range(max_iter):\n        start = it * batch_size % n\n        idx = np.arange(start, start + batch_size) % n\n        X_b = X[idx]\n        y_b = y[idx]\n        err = X_b @ w - y_b\n        g = X_b.T @ err / len(y_b)\n        s = rho * s + (1.0 - rho) * g ** 2\n        w -= eta * g / (np.sqrt(s) + eps_station)\n        full_grad = X.T @ (X @ w - y) / n\n        if np.linalg.norm(full_grad) < epsilon:\n            break\n    return np.round(w, 4).tolist()\nimport numpy as np\nassert all(abs(a-b)<1e-2 for a,b in zip(rms_prop(np.array([[1,0],[1,1],[1,2],[1,3]],float),np.array([1,3,5,7],float)),[1,2])), \"Failed on y = 1+2x\"\nassert all(abs(a-b)<1e-2 for a,b in zip(rms_prop(np.array([[1,0],[1,1],[1,2],[1,3],[1,4]],float),np.array([3,7,11,15,19],float)),[3,4])), \"Failed on y = 3+4x\"\nassert all(abs(a-b)<1e-2 for a,b in zip(rms_prop(np.array([[1,0],[1,1],[1,2]],float),np.array([5,5,5],float)),[5,0])), \"Failed on constant function\"\nassert all(abs(a-b)<1e-2 for a,b in zip(rms_prop(np.array([[1,0],[1,1],[1,2],[1,3]],float),np.array([10,8,6,4],float)),[10,-2])), \"Failed on y = 10-2x\"\nX8=np.array([[1,-1],[1,0],[1,1],[1,2]],float);y8=np.array([4,5,6,7],float)\nassert all(abs(a-b)<1e-2 for a,b in zip(rms_prop(X8,y8),[5,1])), \"Failed on negative to positive x\"\nX9=np.array([[1,2],[1,4],[1,6],[1,8]],float);y9=np.array([5,9,13,17],float)\nassert all(abs(a-b)<1e-2 for a,b in zip(rms_prop(X9,y9),[1,2])), \"Failed on even x\"\nX10=np.array([[1,3],[1,6],[1,9]],float);y10=np.array([2,4,6],float)\nassert all(abs(a-b)<1e-2 for a,b in zip(rms_prop(X10,y10),[0,0.6667])), \"Failed on small sample 10\""}
{"task_id": 56, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef softplus(x, deriv: bool=False):\n    \"\"\"\n    Numerically\u2013stable SoftPlus activation (and its gradient).\n\n    Parameters\n    ----------\n    x     : scalar, list or np.ndarray\n        Input value(s).\n    deriv : bool, default False\n        If True return the gradient (sigmoid), otherwise SoftPlus.\n\n    Returns\n    -------\n    float or list\n        Rounded result(s) \u2013 float when the input was a scalar,\n        otherwise a Python list.\n    \"\"\"\n    is_scalar = np.isscalar(x)\n    x = np.asarray(x, dtype=np.float64)\n    if deriv:\n        pos_mask = x >= 0\n        neg_mask = ~pos_mask\n        out = np.empty_like(x)\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n        exp_x = np.exp(x[neg_mask])\n        out[neg_mask] = exp_x / (1.0 + exp_x)\n    else:\n        out = np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n    out = np.round(out, 4)\n    if is_scalar:\n        return float(out)\n    else:\n        return out.tolist()\nassert softplus(np.array([-1, 0, 1])) == [0.3133, 0.6931, 1.3133], \"failed: softplus([-1,0,1])\"\nassert softplus(np.array([-1, 0, 1]), True) == [0.2689, 0.5, 0.7311], \"failed: softplus([-1,0,1], deriv=True)\"\nassert softplus([-20]) == [0.0], \"failed: softplus([-20])\"\nassert softplus([20]) == [20.0], \"failed: softplus([20])\"\nassert softplus([-20], True) == [0.0], \"failed: softplus([-20], deriv=True)\"\nassert softplus([20], True) == [1.0], \"failed: softplus([20], deriv=True)\"\nassert softplus([-2, 2], True) == [0.1192, 0.8808], \"failed: softplus([-2,2], deriv=True)\"\nassert softplus(0) == 0.6931, \"failed: softplus(0)\"\nassert softplus(0, True) == 0.5, \"failed: softplus(0, deriv=True)\""}
{"task_id": 58, "completion_id": 0, "passed": false, "result": "failed: failed test 1", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef _count_ngrams(tokens: list[str], n: int) -> Counter:\n    \"\"\"\n    Helper: return a Counter of all length-n n-grams found in *tokens*.\n    Unigrams are counted as the string itself, higher orders as tuples.\n    \"\"\"\n    if n == 1:\n        return Counter(tokens)\n    counts = Counter()\n    for i in range(len(tokens) - n + 1):\n        counts[tuple(tokens[i:i + n])] += 1\n    return counts\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Unsmooth\\xaded MLE N-gram log-probability of *sequence* given *corpus*.\"\"\"\n    if N < 1:\n        raise ValueError('N must be >= 1')\n    if len(sequence) < N:\n        return 0.0\n    if N == 1:\n        unigram_counts = _count_ngrams(corpus, 1)\n        corpus_len = len(corpus)\n    else:\n        ngram_counts = _count_ngrams(corpus, N)\n        prefix_counts = _count_ngrams(corpus, N - 1)\n    total_log_prob = 0.0\n    for i in range(len(sequence) - N + 1):\n        if N == 1:\n            word = sequence[i]\n            numerator = unigram_counts.get(word, 0)\n            denominator = corpus_len\n        else:\n            ngram = tuple(sequence[i:i + N])\n            prefix = ngram[:-1]\n            numerator = ngram_counts.get(ngram, 0)\n            denominator = prefix_counts.get(prefix, 0)\n        if numerator == 0 or denominator == 0:\n            return float('-inf')\n        prob = numerator / denominator\n        total_log_prob += float(np.log(prob))\n    return round(total_log_prob, 4)\nassert unsmoothed_ngram_log_prob([\"the\",\"quick\",\"brown\",\"fox\",\"jumps\",\"over\",\"the\",\"lazy\",\"dog\"],[\"the\",\"lazy\",\"dog\"],2)==-0.6931, \"failed test 1\"\nassert unsmoothed_ngram_log_prob(\"a b a b a\".split(),[\"a\",\"b\"],2)==-0.4055, \"failed test 2\"\nassert unsmoothed_ngram_log_prob(\"a b a b a\".split(),[\"b\",\"a\",\"b\"],2)==-0.4055, \"failed test 3\"\nassert unsmoothed_ngram_log_prob(\"a b a b a\".split(),[\"b\",\"c\"],2)==float('-inf'), \"failed test 4\"\nassert unsmoothed_ngram_log_prob(\"a b a b a\".split(),[\"a\",\"b\"],1)==-1.4271, \"failed test 5\"\nassert unsmoothed_ngram_log_prob(\"a b a b a\".split(),[\"c\"],1)==float('-inf'), \"failed test 6\"\nassert unsmoothed_ngram_log_prob(\"i love machine learning i love deep learning\".split(),[\"i\",\"love\",\"deep\"],3)==-0.6931, \"failed test 7\"\nassert unsmoothed_ngram_log_prob(\"i love machine learning i love deep learning\".split(),[\"love\",\"deep\",\"machine\"],3)==float('-inf'), \"failed test 8\"\nassert unsmoothed_ngram_log_prob(\"a b a b a\".split(),[\"a\"],2)==float('-inf'), \"failed test 9\"\nassert unsmoothed_ngram_log_prob(\"i love machine learning i love deep learning\".split(),[\"machine\",\"learning\",\"i\"],3)==0.0, \"failed test 10\""}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.GenericAlias' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef lasso_regression(X: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray', degree: int, reg_factor: float, n_iterations: int=1000, tol: float=1e-06) -> list[float]:\n    \"\"\"Univariate Lasso regression with polynomial features (coordinate descent).\"\"\"\n    X = np.asarray(X, dtype=float).ravel()\n    y = np.asarray(y, dtype=float).ravel()\n    m = X.size\n    if m == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y.size != m:\n        raise ValueError('X and y must have the same length.')\n    if degree < 0:\n        raise ValueError('degree must be non-negative.')\n    if reg_factor < 0:\n        raise ValueError('reg_factor must be non-negative.')\n    X_poly = np.column_stack([np.ones_like(X)] + [X ** k for k in range(1, degree + 1)])\n    d_plus_1 = degree + 1\n    w = np.zeros(d_plus_1)\n\n    def soft_threshold(rho: float, lam: float) -> float:\n        if rho > lam:\n            return rho - lam\n        if rho < -lam:\n            return rho + lam\n        return 0.0\n    for _ in range(n_iterations):\n        w_old = w.copy()\n        w[0] = np.mean(y - X_poly[:, 1:] @ w[1:])\n        y_minus_pred = None\n        for j in range(1, d_plus_1):\n            x_j = X_poly[:, j]\n            if y_minus_pred is None:\n                y_minus_pred = y - X_poly @ w\n            rho = x_j @ (y_minus_pred + w[j] * x_j)\n            if reg_factor == 0.0:\n                w[j] = rho / (x_j @ x_j)\n            else:\n                w[j] = soft_threshold(rho, reg_factor) / (x_j @ x_j)\n            y_minus_pred = y_minus_pred - (w[j] - w_old[j]) * x_j\n        if np.max(np.abs(w - w_old)) < tol:\n            break\n    return [round(float(coef), 4) for coef in w]\nassert lasso_regression([0,1,2,3],[1,3,5,7],1,0)==[1.0,2.0],\"failed: linear data degree 1\"\nassert lasso_regression([1,2,3],[4,6,8],1,0)==[2.0,2.0],\"failed: linear data intercept 2 slope 2\"\nassert lasso_regression([0,1,2],[0,1,4],2,0)==[0.0,0.0,1.0],\"failed: quadratic x^2\"\nassert lasso_regression([0,1,2],[1,2,5],2,0)==[1.0,0.0,1.0],\"failed: quadratic x^2+1\"\nassert lasso_regression([0,1,2],[2,6,12],2,0)==[2.0,3.0,1.0],\"failed: quadratic 2+3x+x^2\"\nassert lasso_regression([-1,0,1],[2,1,0],1,0)==[1.0,-1.0],\"failed: negative slope\"\nassert lasso_regression([0,5],[7,17],1,0)==[7.0,2.0],\"failed: two-point line\""}
{"task_id": 63, "completion_id": 0, "passed": false, "result": "failed: name 'A1' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"\n    Compute the backward-probability vector \u03b2_t for a discrete Hidden Markov\n    Model.\n\n    Parameters\n    ----------\n    A   : transition-probability matrix, shape (N, N)\n    B   : emission-probability matrix, shape (N, M)\n    obs : list with the indices of the observed symbols; length = T\n    t   : time index for which \u03b2_t is required  (0 \u2264 t < T)\n\n    Returns\n    -------\n    list[float]\n        The N backward probabilities \u03b2_t(i) (i = 0 \u2026 N-1), each rounded\n        to 4 decimal places.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    T = len(obs)\n    N = A.shape[0]\n    if not 0 <= t < T:\n        raise ValueError('t must satisfy 0 \u2264 t < len(obs)')\n    beta_next = np.ones(N)\n    for k in range(T - 2, t - 1, -1):\n        emis_beta = B[:, obs[k + 1]] * beta_next\n        beta_curr = A.dot(emis_beta)\n        beta_next = beta_curr\n    return np.round(beta_next, 4).tolist()\nassert backward_beta(A1, B1, obs1, 0) == [0.2032, 0.2164], \"failed: example t=0\"\nassert backward_beta(A1, B1, obs1, 1) == [0.38, 0.26], \"failed: example t=1\"\nassert backward_beta(A1, B1, obs1, 2) == [1.0, 1.0], \"failed: example t=2 (last index)\"\nassert backward_beta(A2, B2, obs2, 3) == [1.0, 1.0, 1.0], \"failed: 3-state t=3\"\nassert backward_beta(A2, B2, obs2, 2) == [0.49, 0.5, 0.47], \"failed: 3-state t=2\"\nassert backward_beta(A2, B2, obs2, 1) == [0.2488, 0.2434, 0.2552], \"failed: 3-state t=1\"\nassert backward_beta(A2, B2, obs2, 0) == [0.1267, 0.1245, 0.133], \"failed: 3-state t=0\"\nassert backward_beta(A3, B3, obs3, 0) == [1.0, 1.0], \"failed: single observation\"\nassert backward_beta(A4, B4, obs4, 0) == [0.45, 0.54], \"failed: custom 2-state t=0\"\nassert backward_beta(A4, B4, obs4, 1) == [1.0, 1.0], \"failed: custom 2-state t=1\""}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Args:\n        A: transition matrix (NxN)\n        B: emission   matrix (NxM)\n        pi: initial state distribution (N)\n        obs: observation index sequence (length T)\n\n    Returns:\n        Probability P(O | \u03bb) rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or (not pi) or (not obs):\n        return 0.0\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    pi = np.asarray(pi, dtype=float)\n    obs = np.asarray(obs, dtype=int)\n    N = len(pi)\n    T = len(obs)\n    beta = np.ones(N, dtype=float)\n    for t in range(T - 2, -1, -1):\n        factor = B[:, obs[t + 1]] * beta\n        beta = A.dot(factor)\n    prob = float(np.dot(pi * B[:, obs[0]], beta))\n    return round(prob, 6)\nassert backward_prob([[0.7,0.3],[0.4,0.6]], [[0.1,0.4,0.5],[0.6,0.3,0.1]], [0.6,0.4], [0,1,2]) == 0.033612, \"test case failed: example sequence\"\nassert backward_prob([[0.7,0.3],[0.4,0.6]], [[0.1,0.4,0.5],[0.6,0.3,0.1]], [0.6,0.4], [2,1]) == 0.1246, \"test case failed: sequence [2,1]\"\nassert backward_prob([[1.0]], [[0.2,0.8]], [1.0], [0,1,1,0]) == 0.0256, \"test case failed: single-state model 1\"\nassert backward_prob([[1.0]], [[0.5,0.5]], [1.0], [1,1,1]) == 0.125, \"test case failed: single-state model 2\"\nassert backward_prob([[0.7,0.3],[0.4,0.6]], [[0.1,0.4,0.5],[0.6,0.3,0.1]], [0.6,0.4], [1]) == 0.36, \"test case failed: length-1 sequence\"\nassert backward_prob([[0.5,0.5,0.0],[0.2,0.3,0.5],[0.0,0.0,1.0]], [[0.1,0.9],[0.7,0.3],[0.4,0.6]], [1.0,0.0,0.0], [0,1]) == 0.06, \"test case failed: 3-state model\"\nassert backward_prob([[1,0],[0,1]], [[0.6,0.4],[0.2,0.8]], [0.5,0.5], [0,1]) == 0.2, \"test case failed: deterministic transitions\"\nassert backward_prob([[0.5,0.5],[0.5,0.5]], [[1,0],[0,1]], [0.5,0.5], [0,0]) == 0.25, \"test case failed: symmetric transitions\"\nassert backward_prob([[0.5,0.5],[0.5,0.5]], [[1,0],[0,1]], [0.5,0.5], [1]) == 0.5, \"test case failed: length-1 identity emissions\"\nassert backward_prob([[1.0]], [[0.3,0.7]], [1.0], [1,1,0]) == 0.147, \"test case failed: single-state model 3\""}
{"task_id": 69, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef forward_algorithm(S: list[float], A: list[list[float]], B: list[list[float]], observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    n = len(S)\n    if n == 0 or not observations:\n        return -1\n    if len(A) != n or any((len(row) != n for row in A)):\n        return -1\n    if len(B) != n or len(B[0]) == 0:\n        return -1\n    m = len(B[0])\n    if any((len(row) != m for row in B)):\n        return -1\n    for o in observations:\n        if o < 0 or o >= m:\n            return -1\n    S = np.asarray(S, dtype=float)\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    alpha = S * B[:, observations[0]]\n    for ot in observations[1:]:\n        alpha = B[:, ot] * (alpha @ A)\n    likelihood = float(alpha.sum())\n    return round(likelihood, 4)\nassert forward_algorithm([0.6,0.4], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4,0.1],[0.1,0.3,0.6]], [0,1,2]) == 0.0363, \"failed: basic 2-state example\"\nassert forward_algorithm([0.6,0.4], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4,0.1],[0.1,0.3,0.6]], [1]) == 0.36, \"failed: single observation\"\nassert forward_algorithm([1.0,0.0], [[0.5,0.5],[0.2,0.8]], [[0.6,0.4],[0.3,0.7]], [0,1,1]) == 0.2004, \"failed: asymmetric model\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4,0.1],[0.1,0.3,0.6]], [2,0]) == 0.097, \"failed: different start distribution\"\nassert forward_algorithm([1.0], [[1.0]], [[0.2,0.3]], [1,1,0]) == 0.018, \"failed: single-state model\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.5]], [0]) == -1, \"failed: B wrong dimensions\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4],[0.5,0.6]], [-1,0]) == -1, \"failed: negative observation index\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4],[0.5,0.6]], [0,2]) == -1, \"failed: observation index out of range\"\nassert forward_algorithm([], [], [], [0]) == -1, \"failed: empty S\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3]], [[0.5,0.5],[0.5,0.5]], [0]) == -1, \"failed: A not square\""}
{"task_id": 70, "completion_id": 0, "passed": false, "result": "failed: Forward reference must be an expression -- got '._FPNode'", "solution": "from collections import Counter, defaultdict\nfrom collections import Counter\nfrom typing import Optional, Dict, List\nclass _FPNode:\n    \"\"\"A node in an FP-tree.\"\"\"\n    __slots__ = ('item', 'count', 'parent', 'children', 'link')\n\n    def __init__(self, item: Optional[str], parent: Optional['._FPNode']):\n        self.item: Optional[str] = item\n        self.count: int = 0\n        self.parent: Optional['_FPNode'] = parent\n        self.children: Dict[str, _FPNode] = {}\n        self.link: Optional['_FPNode'] = None\ndef _build_fp_tree(transactions: List[List[str]], min_support: int) -> tuple[Optional[_FPNode], Optional[dict]]:\n    \"\"\"\n    Build an FP-tree and the accompanying header table from *transactions*.\n    Returns (root, header_table) or (None, None) if the tree would be empty.\n    The header table maps an item -> [support, head-of-node-link].\n    \"\"\"\n    freq = Counter()\n    for t in transactions:\n        freq.update(t)\n    freq = {item: c for (item, c) in freq.items() if c >= min_support}\n    if not freq:\n        return (None, None)\n    header: dict[str, list] = {i: [c, None] for (i, c) in freq.items()}\n    root = _FPNode(None, None)\n    for transaction in transactions:\n        filtered = [i for i in transaction if i in freq]\n        if not filtered:\n            continue\n        ordered = sorted(filtered, key=lambda x: (-freq[x], x))\n        current = root\n        for item in ordered:\n            if item in current.children:\n                child = current.children[item]\n                child.count += 1\n            else:\n                child = _FPNode(item, current)\n                child.count = 1\n                current.children[item] = child\n                head = header[item][1]\n                if head is None:\n                    header[item][1] = child\n                else:\n                    while head.link is not None:\n                        head = head.link\n                    head.link = child\n            current = child\n    return (root, header)\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings). `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result is deterministic:\n      \u2022 inside each item-set items are sorted alphabetically;\n      \u2022 the outer list is sorted by `(len(itemset), itemset)`.\n    \"\"\"\n    if not transactions or min_support < 1:\n        return []\n    (root, header) = _build_fp_tree(transactions, min_support)\n    if header is None:\n        return []\n    frequent_itemsets: list[list[str]] = []\n    _mine_tree(header, [], frequent_itemsets, min_support)\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    return frequent_itemsets\nassert fp_growth([], 1) == [], \"test case failed: empty dataset\"\nassert fp_growth([[\"x\", \"y\"]], 2) == [], \"test case failed: single transaction, high support\"\nassert fp_growth([[\"x\", \"y\"]], 1) == [[\"x\"], [\"y\"], [\"x\", \"y\"]], \"test case failed: single transaction, min_support=1\"\nassert fp_growth([[\"a\", \"b\", \"c\"], [\"a\", \"c\"], [\"b\", \"c\"]], 4) == [], \"test case failed: support greater than number of transactions\"\nassert fp_growth([[\"a\", \"b\", \"c\"], [\"a\", \"c\"], [\"b\", \"c\"]], 2) == [[\"a\"], [\"b\"], [\"c\"], [\"a\", \"c\"], [\"b\", \"c\"]], \"test case failed: small data set min_support=2\"\nassert fp_growth([[\"a\", \"b\", \"c\"]]*3, 2) == [[\"a\"], [\"b\"], [\"c\"], [\"a\", \"b\"], [\"a\", \"c\"], [\"b\", \"c\"], [\"a\", \"b\", \"c\"]], \"test case failed: identical transactions\"\nassert fp_growth([[\"bread\", \"milk\"], [\"bread\", \"diaper\", \"beer\", \"egg\"], [\"milk\", \"diaper\", \"beer\", \"coke\"], [\"bread\", \"milk\", \"diaper\", \"beer\"], [\"bread\", \"milk\", \"diaper\", \"coke\"]], 2) == [[\"beer\"], [\"bread\"], [\"coke\"], [\"diaper\"], [\"milk\"], [\"beer\", \"bread\"], [\"beer\", \"diaper\"], [\"beer\", \"milk\"], [\"bread\", \"diaper\"], [\"bread\", \"milk\"], [\"coke\", \"diaper\"], [\"coke\", \"milk\"], [\"diaper\", \"milk\"], [\"beer\", \"bread\", \"diaper\"], [\"beer\", \"diaper\", \"milk\"], [\"bread\", \"diaper\", \"milk\"], [\"coke\", \"diaper\", \"milk\"]], \"test case failed: example data min_support=2\"\nassert fp_growth([[\"a\", \"b\"], [\"b\", \"c\"], [\"a\", \"c\"], [\"a\", \"b\", \"c\"]], 2) == [[\"a\"], [\"b\"], [\"c\"], [\"a\", \"b\"], [\"a\", \"c\"], [\"b\", \"c\"]], \"test case failed: mixed transactions\"\nassert fp_growth([[\"d\"]], 1) == [[\"d\"]], \"test case failed: single item single transaction\""}
{"task_id": 75, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"k-Nearest Neighbour classifier (Euclidean metric).\n\n    Returns a list with the predicted class for every row in `X_test`.\n    If *k* is not a positive integer or *k* > n_train the function\n    returns the scalar -1 (as required by the specification).\n    \"\"\"\n    if not isinstance(k, int) or k <= 0 or k > len(X_train):\n        return -1\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    preds: list[int] = []\n    for x in X_test:\n        diff = X_train - x\n        dists = np.einsum('ij,ij->i', diff, diff)\n        if k == 1:\n            nn_idx = [int(np.argmin(dists))]\n        else:\n            nn_idx = np.argpartition(dists, k - 1)[:k]\n        votes: dict[int, int] = {}\n        for lbl in y_train[nn_idx]:\n            votes[int(lbl)] = votes.get(int(lbl), 0) + 1\n        max_vote = max(votes.values())\n        winner = min((lbl for (lbl, cnt) in votes.items() if cnt == max_vote))\n        preds.append(winner)\n    return preds\nassert knn_predict([[1,2],[2,3],[3,4]],[0,1,1],[[2,2]],2)==[0],\"failed on single sample, tie case\"\nassert knn_predict([[0,0],[1,1],[2,2],[3,3]],[1,1,0,0],[[1.1,1.1],[2.9,3.1]],3)==[1,0],\"failed on multi predict\"\nassert knn_predict([[1,0],[0,1],[1,1]],[0,0,1],[[0,0]],1)==[0],\"failed on k=1\"\nassert knn_predict([[1,0],[0,1],[1,1]],[0,0,1],[[0,0]],3)==[0],\"failed on k==n_train\"\nassert knn_predict([[1,0],[0,1],[1,1]],[0,2,2],[[0.9,0.9]],2)==[2],\"failed on majority >1\"\nassert knn_predict([[1,1]], [3], [[2,2],[0,0]], 1)==[3,3],\"failed on single-point training set\"\nassert knn_predict([[1,2],[3,4]],[0,1],[[2,3]],2)==[0],\"failed on tie chooses smaller label\"\nassert knn_predict([[1,2],[3,4]],[0,1],[[2,3]],0)==-1,\"failed on invalid k=0\"\nassert knn_predict([[1,2],[3,4]],[0,1],[[2,3]],3)==-1,\"failed on k>n_train\"\nassert knn_predict([[1,2],[2,1],[0,0],[2,2]],[1,1,0,0],[[1,1]],2)==[1],\"failed on mixed labels\""}
{"task_id": 76, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'type' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities (same shape).\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    eps = np.finfo(float).eps\n    loss = -np.sum(y * np.log(y_pred + eps))\n    return float(np.round(loss, 4))\nassert cross_entropy_loss([[1, 0, 0], [0, 1, 0]], [[0.8, 0.1, 0.1], [0.2, 0.5, 0.3]]) == 0.9163, \"failed: basic 3-class example\"\nassert cross_entropy_loss([[1, 0], [0, 1]], [[0.6, 0.4], [0.3, 0.7]]) == 0.8675, \"failed: 2-class example\"\nassert cross_entropy_loss([[1, 0, 0, 0]], [[0.25, 0.25, 0.25, 0.25]]) == 1.3863, \"failed: uniform prediction\"\nassert cross_entropy_loss([[1, 0], [0, 1], [0, 1]], [[0.9, 0.1], [0.4, 0.6], [0.2, 0.8]]) == 0.8393, \"failed: three samples\"\nassert cross_entropy_loss([[1, 0]], [[1.0, 0.0]]) == 0.0, \"failed: perfect prediction\"\nassert cross_entropy_loss([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[0.7, 0.2, 0.1], [0.3, 0.5, 0.2], [0.1, 0.3, 0.6]]) == 1.5606, \"failed: three-class batch\"\nassert cross_entropy_loss([[0, 1, 0]], [[0.1, 0.7, 0.2]]) == 0.3567, \"failed: single sample\"\nassert cross_entropy_loss([[0, 0, 1], [1, 0, 0]], [[0.05, 0.15, 0.8], [0.9, 0.05, 0.05]]) == 0.3285, \"failed: swapped classes\"\nassert cross_entropy_loss([[0, 1], [1, 0]], [[0.5, 0.5], [0.2, 0.8]]) == 2.3026, \"failed: high loss case\"\nassert cross_entropy_loss([[1], [1]], [[0.99], [0.98]]) == 0.0303, \"failed: single-class edge case\""}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: name 'X1' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]):\n    \"\"\"Forward propagation for an L-layer fully-connected neural network\n    that uses ReLU in all hidden layers and a sigmoid in the output layer.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing weights W1 \u2026 WL and biases b1 \u2026 bL.\n\n    Returns\n    -------\n    tuple[list[list[float]], list]\n        \u2022 AL_list  \u2013 the final activation rounded to 4 decimals\n                     and converted to a regular Python list.\n        \u2022 caches   \u2013 list with one cache per layer (kept for a backward pass).\n    \"\"\"\n\n    def linear_forward(A_prev, W, b):\n        Z = W @ A_prev + b\n        return (Z, (A_prev, W, b))\n\n    def relu(Z):\n        A = np.maximum(0, Z)\n        return (A, Z)\n\n    def sigmoid(Z):\n        A = 1.0 / (1.0 + np.exp(-Z))\n        return (A, Z)\n    caches = []\n    A = X\n    L = len(parameters) // 2\n    for l in range(1, L):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        (Z, lin_cache) = linear_forward(A, W, b)\n        (A, act_cache) = relu(Z)\n        caches.append((lin_cache, act_cache))\n    WL = parameters[f'W{L}']\n    bL = parameters[f'b{L}']\n    (ZL, lin_cache) = linear_forward(A, WL, bL)\n    (AL, act_cache) = sigmoid(ZL)\n    caches.append((lin_cache, act_cache))\n    AL_list = np.round(AL, 4).tolist()\n    return (AL_list, caches)\nassert L_model_forward(X1, params1) == [[0.5374, 0.5671]], \"test case failed: example input\"\nassert L_model_forward(X2, params2) == [[0.5]], \"test case failed: zero weights and bias\"\nassert L_model_forward(X2, params3) == [[0.7311]], \"test case failed: bias = 1\"\nassert L_model_forward(X4, params4) == [[0.5]], \"test case failed: two-layer, Z=0\"\nassert L_model_forward(X5, params5) == [[0.5]], \"test case failed: ReLU zeros\"\nassert L_model_forward(X6, params6) == [[0.8808, 0.5]], \"test case failed: batch of 2 examples\"\nassert L_model_forward(X7, params7) == [[0.3775]], \"test case failed: mixed weights\"\nassert L_model_forward(X8, params8) == [[0.2689]], \"test case failed: negative bias\"\nassert L_model_forward(X9, params9) == [[0.6457]], \"test case failed: realistic 2-layer\"\nassert L_model_forward(X10, params10) == [[0.4502]], \"test case failed: small negative Z\""}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n    \n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n    \n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    eps = 1e-15\n    A2 = np.clip(A2, eps, 1.0 - eps)\n    m = Y.size\n    cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n    return float(np.round(cost, 6))\nassert compute_cost(np.array([[0.8, 0.9, 0.4, 0.2]]), np.array([[1, 1, 0, 0]])) == 0.265618, \"Failed on example 1\"\nassert compute_cost(np.array([[0.999, 0.001]]), np.array([[1, 0]])) == 0.001001, \"Failed on example 2\"\nassert compute_cost(np.array([[0.5, 0.5, 0.5]]), np.array([[1, 0, 1]])) == 0.693147, \"Failed on example 3\"\nassert compute_cost(np.array([[0.2, 0.8, 0.6, 0.4]]), np.array([[0, 1, 1, 0]])) == 0.366985, \"Failed on example 4\"\nassert compute_cost(np.array([[0.999, 0.001, 0.999, 0.001]]), np.array([[1, 0, 1, 0]])) == 0.001001, \"Failed on example 5\"\nassert compute_cost(np.array([[0.1, 0.2, 0.3, 0.9, 0.8]]), np.array([[1, 1, 1, 0, 0]])) == 1.805604, \"Failed on example 6\"\nassert compute_cost(np.array([[0.7]]), np.array([[1]])) == 0.356675, \"Failed on example 7\"\nassert compute_cost(np.array([[0.05]]), np.array([[0]])) == 0.051293, \"Failed on example 8\"\nassert compute_cost(np.array([[0.5, 0.5]]), np.array([[0, 1]])) == 0.693147, \"Failed on example 9\"\nassert compute_cost(np.array([[0.0, 1.0]]), np.array([[0, 1]])) == 0.0, \"Failed on example 10\""}
{"task_id": 82, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.GenericAlias' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef epsilon_soft(Q: list[float] | 'np.ndarray', epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft action\u2013selection probabilities.\n\n    Parameters\n    ----------\n    Q : 1-D sequence (list or np.ndarray)\n        Q-values for every available action in the current state.\n    epsilon : float\n        Exploration parameter 0 \u2264 \u03b5 \u2264 1.\n\n    Returns\n    -------\n    list[float]\n        A list with the probability of choosing every action under the \u03b5-soft\n        policy.  All probabilities are rounded to 4 decimal places and sum to\n        1 (within 1\u00d710\u207b\u00b9\u00b2).\n    \"\"\"\n    q = np.asarray(Q, dtype=float).ravel()\n    if q.ndim != 1:\n        raise ValueError('Q must be a one-dimensional sequence of numbers.')\n    if not 0.0 <= epsilon <= 1.0:\n        raise ValueError('epsilon must satisfy 0 \u2264 \u03b5 \u2264 1.')\n    n = q.size\n    if n == 0:\n        raise ValueError('Q must contain at least one action.')\n    greedy_idx = int(np.argmax(q))\n    base_prob = epsilon / n\n    probs = np.full(n, base_prob, dtype=float)\n    probs[greedy_idx] += 1.0 - epsilon\n    ticks = probs * 10000.0\n    int_part = np.floor(ticks).astype(int)\n    remainder = ticks - int_part\n    deficit = 10000 - int_part.sum()\n    if deficit > 0:\n        order = np.argsort(-remainder)\n        int_part[order[:deficit]] += 1\n    elif deficit < 0:\n        order = np.argsort(remainder)\n        int_part[order[:-deficit]] -= 1\n    final_probs = int_part / 10000.0\n    if abs(final_probs.sum() - 1.0) > 1e-12:\n        raise RuntimeError('Probabilities do not sum to one within tolerance.')\n    return final_probs.round(4).tolist()\nassert epsilon_soft([1.2, 0.3, 1.2, -0.1], 0.1) == [0.925, 0.025, 0.025, 0.025], \"Test-1 failed: tie situation with \u03b5 = 0.1\"\nassert epsilon_soft([5, 6, 7], 0) == [0.0, 0.0, 1.0], \"Test-2 failed: \u03b5 = 0 (pure greedy)\"\nassert epsilon_soft([2, 2, 2], 0.3) == [0.8, 0.1, 0.1], \"Test-3 failed: all equal Q-values\"\nassert epsilon_soft([-1, -5], 1) == [0.5, 0.5], \"Test-4 failed: \u03b5 = 1 (fully uniform)\"\nassert epsilon_soft([0], 0.5) == [1.0], \"Test-5 failed: single action case\"\nassert epsilon_soft([10, 0, -1, 8], 0.2) == [0.85, 0.05, 0.05, 0.05], \"Test-6 failed: general case\"\nassert len(epsilon_soft([1, 2, 3, 4, 5], 0.4)) == 5, \"Test-10 failed: wrong output length\""}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import Counter\nfrom collections import Counter\nfrom typing import List, Union\nLabel = Union[int, float, str]\ndef aggregate_random_forest_votes(predictions: List[List[Label]]) -> List[Label]:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions or not predictions[0]:\n        return []\n    n_samples = len(predictions[0])\n    majority_votes: List[Label] = []\n    for idx in range(n_samples):\n        votes = [tree_preds[idx] for tree_preds in predictions]\n        vote_counts = Counter(votes)\n        max_count = max(vote_counts.values())\n        tied_labels = [label for (label, cnt) in vote_counts.items() if cnt == max_count]\n        majority_votes.append(min(tied_labels))\n    return majority_votes\nassert aggregate_random_forest_votes([[1,0,1,1],[0,0,1,1],[1,1,1,0]]) == [1,0,1,1], \"test case failed: basic majority voting\"\nassert aggregate_random_forest_votes([[1,2],[2,1]]) == [1,1], \"test case failed: numeric tie breaking\"\nassert aggregate_random_forest_votes([[\"cat\",\"dog\",\"cat\"],[\"dog\",\"dog\",\"cat\"]]) == [\"cat\",\"dog\",\"cat\"], \"test case failed: string votes\"\nassert aggregate_random_forest_votes([[42,42,42]]) == [42,42,42], \"test case failed: single tree\"\nassert aggregate_random_forest_votes([[1],[0],[1],[0]]) == [0], \"test case failed: single sample tie\"\nassert aggregate_random_forest_votes([[3,3,2,2],[2,2,3,3],[3,2,3,2]]) == [3,2,3,2], \"test case failed: alternating ties\"\nassert aggregate_random_forest_votes([]) == [], \"test case failed: empty input\"\nassert aggregate_random_forest_votes([[1,1,1],[1,1,1]]) == [1,1,1], \"test case failed: identical votes\"\nassert aggregate_random_forest_votes([[5,4,3,2,1],[1,2,3,4,5],[5,4,3,2,1]]) == [5,4,3,2,1], \"test case failed: diverse votes\"\nassert aggregate_random_forest_votes([[\"a\",\"b\",\"a\",\"c\"],[\"b\",\"b\",\"a\",\"c\"],[\"a\",\"c\",\"a\",\"c\"]]) == [\"a\",\"b\",\"a\",\"c\"], \"test case failed: mixed string votes\""}
{"task_id": 88, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    is_scalar = np.isscalar(z) or (isinstance(z, np.ndarray) and z.shape == ())\n    z_arr = np.asarray(z, dtype=float)\n    soft = np.logaddexp(0.0, z_arr)\n    soft = np.round(soft, 4)\n    if is_scalar:\n        return float(soft)\n    else:\n        return soft.tolist()\nassert softplus(-1000) == 0.0, \"failed: softplus(-1000)\"\nassert softplus(0) == 0.6931, \"failed: softplus(0)\"\nassert softplus(1) == 1.3133, \"failed: softplus(1)\"\nassert softplus(20) == 20.0, \"failed: softplus(20)\"\nassert softplus([-1, 0, 1]) == [0.3133, 0.6931, 1.3133], \"failed: softplus([-1,0,1])\"\nimport numpy as np\nassert softplus(np.array([-2, 5])) == [0.1269, 5.0067], \"failed: softplus(np.array([-2,5]))\"\nassert softplus(np.array([[0]])) == [[0.6931]], \"failed: softplus(np.array([[0]]))\"\nassert softplus(-0.5) == 0.4741, \"failed: softplus(-0.5)\"\nassert softplus([1000, -1000]) == [1000.0, 0.0], \"failed: softplus([1000,-1000])\""}
{"task_id": 90, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import Any\ndef _get_field(obj: Any, name: str):\n    \"\"\"Return attribute or (key) item `name` from `obj`, else raise KeyError.\"\"\"\n    if hasattr(obj, name):\n        return getattr(obj, name)\n    try:\n        return obj[name]\n    except Exception as exc:\n        raise KeyError(f'{name!r} not found in supplied object.') from exc\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Mean-squared error between a policy\u2019s value estimates and the truth.\"\"\"\n    try:\n        arm_evs = _get_field(bandit, 'arm_evs')\n    except KeyError:\n        raise ValueError('`bandit` must supply an `arm_evs` field.')\n    try:\n        ev_estimates = _get_field(policy, 'ev_estimates')\n    except KeyError:\n        return np.nan\n    if not ev_estimates:\n        return np.nan\n    errors = []\n    for arm_idx in sorted(ev_estimates):\n        if 0 <= arm_idx < len(arm_evs):\n            diff = ev_estimates[arm_idx] - arm_evs[arm_idx]\n            errors.append(diff * diff)\n    if not errors:\n        return np.nan\n    mse_val = float(np.mean(errors))\n    return round(mse_val, 4)\nassert mse({\"arm_evs\": [0.5, 0.2, 0.9]}, {\"ev_estimates\": {0: 0.4, 1: 0.25, 2: 0.8}}) == 0.0075, \"test 1 failed\"\nassert mse({\"arm_evs\": [1.0, 0.0]}, {\"ev_estimates\": {1: 0.2, 0: 0.8}}) == 0.04, \"test 2 failed\"\nassert mse({\"arm_evs\": [0.3, 0.6, 0.9, 0.1]}, {\"ev_estimates\": {0: 0.3, 1: 0.6, 2: 0.9, 3: 0.1}}) == 0.0, \"test 3 failed\"\nassert mse({\"arm_evs\": [0.0, -1.0, 1.0]}, {\"ev_estimates\": {0: 0.5, 1: 0.0, 2: 1.5}}) == 0.5, \"test 4 failed\"\nassert np.isnan(mse({\"arm_evs\": [0.1, 0.2]}, {\"ev_estimates\": {}})), \"test 5 failed\"\nassert mse({\"arm_evs\": [2.0]}, {\"ev_estimates\": {0: 3.5}}) == 2.25, \"test 6 failed\"\nassert mse({\"arm_evs\": [0.0, 1.0]}, {\"ev_estimates\": {0: 0.35, 1: 0.35}}) == 0.2725, \"test 9 failed\"\nassert mse({\"arm_evs\": [5.0, 5.0, 5.0]}, {\"ev_estimates\": {0: 4.0, 1: 6.0, 2: 5.0}}) == 0.6667, \"test 10 failed\""}
{"task_id": 96, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nfrom collections import Counter, defaultdict\nimport math\nfrom collections import Counter, defaultdict\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float=1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\"\"\"\n    if N <= 0:\n        raise ValueError('N must be a positive integer.')\n    seq_tokens = sequence.split()\n    if len(seq_tokens) < N:\n        return 0.0\n    vocab = set()\n    ngram_counts: dict[int, Counter] = {n: Counter() for n in range(1, N + 1)}\n    for sent in corpus:\n        tokens = sent.split()\n        vocab.update(tokens)\n        for n in range(1, N + 1):\n            if len(tokens) < n:\n                continue\n            for i in range(len(tokens) - n + 1):\n                ngram = tuple(tokens[i:i + n])\n                ngram_counts[n][ngram] += 1\n    vocab.add('<UNK>')\n    V = len(vocab)\n    total_tokens = sum(ngram_counts[1].values())\n    seq_tokens = [tok if tok in vocab else '<UNK>' for tok in seq_tokens]\n    log_prob = 0.0\n    for i in range(len(seq_tokens) - N + 1):\n        ngram = tuple(seq_tokens[i:i + N])\n        context = ngram[:-1]\n        target = ngram[-1]\n        count_ngram = ngram_counts[N].get(ngram, 0)\n        if N == 1:\n            context_count = total_tokens\n        else:\n            context_count = ngram_counts[N - 1].get(context, 0)\n        prob = (count_ngram + K) / (context_count + K * V)\n        log_prob += math.log(prob)\n    return round(log_prob, 4)\nassert additive_ngram_log_prob([\"hello world\"], \"hello world hello\", 1, 1) == -2.7489, \"failed on unigram repetition\"\nassert additive_ngram_log_prob([\"a b c d\"], \"a b c d\", 3, 1) == -2.1972, \"failed on exact trigram\"\nassert additive_ngram_log_prob([\"the cat\"], \"the mouse\", 2, 1) == -1.3863, \"failed on unseen word in bigram\"\nassert additive_ngram_log_prob([\"cat sat\"], \"mouse cat\", 2, 1) == -1.0986, \"failed on unseen context\"\nassert additive_ngram_log_prob([\"a a b b\"], \"a b\", 1, 0.5) == -1.5769, \"failed on K=0.5 smoothing\"\nassert additive_ngram_log_prob([\"I love NLP\", \"I love AI\"], \"I love ML\", 3, 1) == -1.9459, \"failed on trigram with unknown\"\nassert additive_ngram_log_prob([\"a b c\", \"a b d\"], \"a b d\", 2, 2) == -2.4849, \"failed on K=2 bigram\"\nassert additive_ngram_log_prob([\"hello world\"], \"foo\", 1, 1) == -1.6094, \"failed on completely unknown word\""}
{"task_id": 108, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef als_factorization(X: np.ndarray, K: int, alpha: float=1.0, max_iter: int=200, tol: float=0.0001) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=np.float64)\n    (N, M) = X.shape\n    rng = np.random.RandomState(0)\n    W = rng.randn(N, K)\n    H = rng.randn(K, M)\n    I_K = np.eye(K, dtype=np.float64)\n\n    def _solve(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n        \"\"\"Solve AX = B for X with fallback to pseudo-inverse if singular.\"\"\"\n        try:\n            return np.linalg.solve(A, B)\n        except np.linalg.LinAlgError:\n            return np.linalg.pinv(A) @ B\n    (best_W, best_H) = (W.copy(), H.copy())\n    best_loss = np.inf\n    for _ in range(max_iter):\n        HHT = H @ H.T + alpha * I_K\n        W = _solve(HHT, H @ X.T).T\n        WTW = W.T @ W + alpha * I_K\n        H = _solve(WTW, W.T @ X)\n        diff = X - W @ H\n        loss = np.sum(diff ** 2) + alpha * (np.sum(W ** 2) + np.sum(H ** 2))\n        if loss < best_loss:\n            best_loss = loss\n            (best_W, best_H) = (W.copy(), H.copy())\n        if loss <= tol:\n            break\n    X_hat = (best_W @ best_H).round(4).tolist()\n    return X_hat\nassert np.allclose(als_factorization(np.array([[1.,2.],[3.,4.]]),2,alpha=0.0,max_iter=1000,tol=1e-8),[[1.0,2.0],[3.0,4.0]],atol=1e-3),\"failed on 2\u00d72 full-rank matrix\"\nassert np.allclose(als_factorization(np.array([[1.,2.,3.],[4.,5.,6.]]),2,alpha=0.0),[[1.0,2.0,3.0],[4.0,5.0,6.0]],atol=1e-3),\"failed on 2\u00d73 full-rank matrix\"\nassert np.allclose(als_factorization(np.array([[7.,8.],[2.,1.],[3.,4.]]),2,alpha=0.0),[[7.0,8.0],[2.0,1.0],[3.0,4.0]],atol=1e-3),\"failed on 3\u00d72 matrix\"\nassert np.allclose(als_factorization(np.array([[1.5,2.5],[3.5,4.5]]),2,alpha=0.0),[[1.5,2.5],[3.5,4.5]],atol=1e-3),\"failed on decimal matrix\"\nassert np.allclose(als_factorization(np.array([[10.]]),1,alpha=0.0),[[10.0]],atol=1e-3),\"failed on 1\u00d71 matrix\"\nassert np.allclose(als_factorization(np.array([[2.,2.],[2.,2.]]),1,alpha=0.0),[[2.0,2.0],[2.0,2.0]],atol=1e-3),\"failed on constant matrix\"\nassert np.allclose(als_factorization(np.array([[1.,2.],[2.,4.]]),1,alpha=0.0),[[1.0,2.0],[2.0,4.0]],atol=1e-3),\"failed on rank-1 2\u00d72 matrix\""}
{"task_id": 109, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport random\nimport numpy as np\nimport random\ndef _euclidean_squared(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"Squared Euclidean distance between every row of `a` and `b` (broadcast).\"\"\"\n    return ((a[:, None, :] - b[None, :, :]) ** 2).sum(axis=2)\ndef _kmeans_pp_init(X: np.ndarray, K: int) -> np.ndarray:\n    \"\"\"Return K initial centroids picked with the K-Means++ heuristic.\"\"\"\n    m = X.shape[0]\n    centroids = [X[np.random.randint(m)]]\n    for _ in range(1, K):\n        d2 = _euclidean_squared(X, np.array(centroids)).min(axis=1)\n        probs = d2 / d2.sum()\n        r = random.random()\n        cumulative = np.cumsum(probs)\n        idx = np.searchsorted(cumulative, r)\n        centroids.append(X[idx])\n    return np.array(centroids)\ndef kmeans(X: np.ndarray, K: int, max_iter: int=100, random_state: int | None=None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\"\"\"\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n    X = np.asarray(X, dtype=float)\n    (m, n) = X.shape\n    if K <= 0 or K > m:\n        raise ValueError('K must be a positive integer \u2264 number of samples.')\n    centroids = _kmeans_pp_init(X, K)\n    labels = np.full(m, -1, dtype=int)\n    for _ in range(max_iter):\n        distances = _euclidean_squared(X, centroids)\n        new_labels = distances.argmin(axis=1)\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n        for k in range(K):\n            mask = labels == k\n            if mask.any():\n                centroids[k] = X[mask].mean(axis=0)\n    centroids_rounded = [[round(float(v), 4) for v in centroid] for centroid in centroids]\n    centroids_sorted = sorted(centroids_rounded, key=lambda c: (c[0], c))\n    return centroids_sorted\nassert kmeans(np.array([[1,2],[1,4],[1,0],[10,2],[10,4],[10,0]]),2,random_state=42)==[[1.0,2.0],[10.0,2.0]],\"failed: basic 2-cluster example\"\nassert kmeans(np.array([[-5,0],[-6,-1],[-4,1],[0,5],[1,6],[-1,4],[5,0],[6,1],[4,-1]]),3,random_state=0)==[[-5.0,0.0],[0.0,5.0],[5.0,0.0]],\"failed: three clearly separated clusters\"\nassert kmeans(np.array([[0,0],[0,1],[10,0],[10,1]]),2,random_state=7)==[[0.0,0.5],[10.0,0.5]],\"failed: two vertical stripes\"\nassert kmeans(np.array([[1],[2],[8],[9]]),2,random_state=3)==[[1.5],[8.5]],\"failed: one-dimensional data\"\nassert kmeans(np.array([[1],[2],[3],[4],[5]]),1,random_state=11)==[[3.0]],\"failed: single cluster\"\nassert kmeans(np.array([[0,0,0],[0,1,0],[0,0,1],[10,0,0],[10,1,0],[10,0,1]]),2,random_state=13)==[[0.0,0.3333,0.3333],[10.0,0.3333,0.3333]],\"failed: 3-D example\"\nassert kmeans(np.array([[1,1],[2,2],[3,3]]),3,random_state=19)==[[1.0,1.0],[2.0,2.0],[3.0,3.0]],\"failed: K equals number of points\"\nassert kmeans(np.array([[-1,0],[-2,0],[1,0],[2,0]]),2,random_state=23)==[[-1.5,0.0],[1.5,0.0]],\"failed: symmetric clusters on x-axis\"\nassert kmeans(np.array([[0,0],[0,1],[1,0],[1,1]]),4,random_state=29)==[[0.0,0.0],[0.0,1.0],[1.0,0.0],[1.0,1.0]],\"failed: each point its own cluster\"\nassert kmeans(np.array([[2,2],[2,4],[8,2],[8,4]]),2,random_state=5)==[[2.0,3.0],[8.0,3.0]],\"failed: square split into two rectangles\""}
{"task_id": 111, "completion_id": 0, "passed": false, "result": "failed: name '_identity_predict' is not defined", "solution": "import numpy as np\nimport numpy as np\nTIME_STEPS = 20\nPAD_TOKEN = 0\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"\n    Encode a raw string into a fixed-length list of integer ids.\n\n    \u2022 Each character is looked up in `vocabulary`.\n    \u2022 Unknown characters map to PAD_TOKEN (0).\n    \u2022 The returned list is exactly `time_steps` long:\n        \u2013 shorter input  \u2192 pad with 0 on the right\n        \u2013 longer  input  \u2192 truncate on the right\n    \"\"\"\n    encoded = [vocabulary.get(ch, PAD_TOKEN) for ch in text[:time_steps]]\n    if len(encoded) < time_steps:\n        encoded.extend([PAD_TOKEN] * (time_steps - len(encoded)))\n    return encoded\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"\n    Decode a sequence of integer ids back into a string.\n\n    \u2022 Works with Python lists, tuples or NumPy arrays.\n    \u2022 Padding / unknown tokens (id 0) are ignored.\n    \"\"\"\n    indices = np.ravel(indices)\n    chars = []\n    for idx in indices:\n        if idx == PAD_TOKEN:\n            continue\n        char = inverse_vocab.get(idx)\n        if char is not None:\n            chars.append(char)\n    return ''.join(chars)\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"\n    End-to-end helper:\n    1. encode text\n    2. feed the single example to `model.predict`\n    3. take arg-max over the last axis\n    4. decode the predicted ids back to text\n    \"\"\"\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    batch = np.array(encoded, dtype=np.int64)[np.newaxis, :]\n    logits = model.predict(batch)\n    pred_ids = np.argmax(logits, axis=-1)\n    decoded = int_to_string(pred_ids[0], inv_output_vocabulary)\n    return decoded\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'hello') == 'hello', 'failed on \"hello\"'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'HELLO') == 'hello', 'failed on upper-case input'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, '') == '', 'failed on empty string'\na20 = 'a' * 25\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, a20) == 'a' * 20, 'failed on long input truncation'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'hi!') == 'hi', 'failed on unknown character removal'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'data science') == 'data science', 'failed on string with space'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, '       ') == '       ', 'failed on only spaces'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'abc xyz') == 'abc xyz', 'failed on mixed letters and space'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'abc_def') == 'abcdef', 'failed on underscore removal'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'padding test') == 'padding test', 'failed on general case'"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: name 'run_example' is not defined", "solution": "\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str] or None\n        A collection of input strings.  If *None*, the function uses the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        try:\n            examples = EXAMPLES\n        except NameError:\n            raise ValueError('No `examples` provided and the global constant `EXAMPLES` is not defined.')\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        predicted_str = ''.join(output_chars)\n        print(f'input:  {example}')\n        print(f'output: {predicted_str}')\n        predictions.append(predicted_str)\n    return predictions\nassert run_examples(lambda s: s[::-1], None, None, [\"abc\"]) == [\"cba\"], \"test case failed: single example reversal\"\nassert run_examples(lambda s: s.upper(), None, None, [\"hello\", \"world\"]) == [\"HELLO\", \"WORLD\"], \"test case failed: uppercase mapping\"\nassert run_examples(lambda s: s, None, None, []) == [], \"test case failed: empty example list\"\nassert run_examples(lambda s: s[::-1], None, None, [\"\", \"a\"]) == [\"\", \"a\"], \"test case failed: empty and single char strings\"\nassert run_examples(lambda s: s*2, None, None, [\"cat\"]) == [\"catcat\"], \"test case failed: duplication model\"\nassert run_examples(lambda s: ''.join(sorted(s)), None, None, [\"cba\", \"fed\"] ) == [\"abc\", \"def\"], \"test case failed: sort characters\"\nassert run_examples(lambda s: ''.join(chr(ord(c)+1) for c in s), None, None, [\"abc\"]) == [\"bcd\"], \"test case failed: shift characters\"\nassert run_examples(lambda s: s[::-1].upper(), None, None, [\"Python\", \"AI\"]) == [\"NOHTYP\", \"IA\"], \"test case failed: reverse and uppercase\"\nassert run_examples(lambda s: ''.join('*' for _ in s), None, None, [\"mask\"]) == [\"****\"], \"test case failed: masking model\"\nassert run_examples(lambda s: s.strip(), None, None, [\"  spaced  \"]) == [\"spaced\"], \"test case failed: strip whitespaces\""}
{"task_id": 115, "completion_id": 0, "passed": false, "result": "failed: name '_expected' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Binary cross-entropy loss and gradient for logistic regression.\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (m, n)\n    y : np.ndarray\n        Binary targets of shape (m,) or (m, 1)\n    w : np.ndarray\n        Weight vector of shape (n,) or (n, 1)\n    \n    Returns\n    -------\n    tuple\n        (loss, gradient) where\n          \u2022 loss is a float rounded to 4 decimals\n          \u2022 gradient is a nested list (shape (n, 1)) whose\n            elements are rounded to 4 decimals\n    \"\"\"\n    y = y.reshape(-1)\n    w = w.reshape(-1)\n    m = X.shape[0]\n    z = X @ w\n    p = 1 / (1 + np.exp(-z))\n    eps = 1e-20\n    p = np.clip(p, eps, 1 - eps)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    loss = float(np.round(loss, 4))\n    grad = X.T @ (p - y) / m\n    grad = grad.reshape(-1, 1)\n    grad = np.round(grad, 4).tolist()\n    return (loss, grad)\nassert logistic_loss_and_gradient(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 0, 0, 1]), np.array([[0.5], [0.5]])) == _expected(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 0, 0, 1]), np.array([[0.5], [0.5]])), \"test case failed: #1\"\nassert logistic_loss_and_gradient(np.array([[1, 2], [3, 4]]), np.array([1, 0]), np.array([[0.0], [0.0]])) == _expected(np.array([[1, 2], [3, 4]]), np.array([1, 0]), np.array([[0.0], [0.0]])), \"test case failed: #2\"\nassert logistic_loss_and_gradient(np.array([[1, 1], [2, 2], [3, 3]]), np.array([0, 1, 1]), np.array([[0.1], [-0.2]])) == _expected(np.array([[1, 1], [2, 2], [3, 3]]), np.array([0, 1, 1]), np.array([[0.1], [-0.2]])), \"test case failed: #3\"\nassert logistic_loss_and_gradient(np.array([[2, 3, 4], [1, 0, 1], [0, 1, 1]]), np.array([1, 0, 1]), np.array([[0.2], [0.3], [-0.1]])) == _expected(np.array([[2, 3, 4], [1, 0, 1], [0, 1, 1]]), np.array([1, 0, 1]), np.array([[0.2], [0.3], [-0.1]])), \"test case failed: #4\"\nassert logistic_loss_and_gradient(np.array([[1], [2], [3]]), np.array([0, 0, 1]), np.array([[0.0]])) == _expected(np.array([[1], [2], [3]]), np.array([0, 0, 1]), np.array([[0.0]])), \"test case failed: #5\"\nassert logistic_loss_and_gradient(np.array([[0.5, 1.5], [1.5, 0.5]]), np.array([1, 0]), np.array([[0.2], [0.2]])) == _expected(np.array([[0.5, 1.5], [1.5, 0.5]]), np.array([1, 0]), np.array([[0.2], [0.2]])), \"test case failed: #6\"\nassert logistic_loss_and_gradient(np.array([[10, 10], [-10, -10]]), np.array([1, 0]), np.array([[0.01], [0.01]])) == _expected(np.array([[10, 10], [-10, -10]]), np.array([1, 0]), np.array([[0.01], [0.01]])), \"test case failed: #7\"\nassert logistic_loss_and_gradient(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([0, 1, 0]), np.array([[0.1], [0.2], [0.3]])) == _expected(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([0, 1, 0]), np.array([[0.1], [0.2], [0.3]])), \"test case failed: #8\"\nassert logistic_loss_and_gradient(np.array([[5, 1], [2, 3], [3, 5], [1, 1]]), np.array([1, 0, 1, 0]), np.array([[0.2], [-0.3]])) == _expected(np.array([[5, 1], [2, 3], [3, 5], [1, 1]]), np.array([1, 0, 1, 0]), np.array([[0.2], [-0.3]])), \"test case failed: #9\"\nassert logistic_loss_and_gradient(np.array([[0, 0], [0, 0]]), np.array([0, 0]), np.array([[0], [0]])) == _expected(np.array([[0, 0], [0, 0]]), np.array([0, 0]), np.array([[0], [0]])), \"test case failed: #10\""}
{"task_id": 118, "completion_id": 0, "passed": false, "result": "failed: failed: larger n_clf", "solution": "import numpy as np\nimport numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array (m, n) with train features.\n        y_train: 1-D NumPy array (m,) with labels **-1** or **1**.\n        X_test : 2-D NumPy array (k, n) with test features.\n        n_clf  : Number of weak classifiers (\u22651).\n\n    Returns\n        Python list (length k) with predicted labels (-1 or 1).\n    \"\"\"\n\n    def _best_stump(X, y, w):\n        \"\"\"Find the decision stump that minimises weighted error.\"\"\"\n        (m, n_features) = X.shape\n        best_err = np.inf\n        stump = {}\n        best_pred = None\n        for j in range(n_features):\n            Xj = X[:, j]\n            uniq = np.unique(Xj)\n            if uniq.size == 1:\n                thresholds = [uniq[0] - 1e-10, uniq[0] + 1e-10]\n            else:\n                thresholds = list((uniq[:-1] + uniq[1:]) / 2.0)\n                thresholds.append(uniq[0] - 1e-10)\n                thresholds.append(uniq[-1] + 1e-10)\n            for thresh in thresholds:\n                for polarity in (1, -1):\n                    preds = np.ones(m, dtype=int)\n                    if polarity == 1:\n                        preds[Xj < thresh] = -1\n                    else:\n                        preds[Xj < thresh] = 1\n                    err = np.sum(w[preds != y])\n                    if err < best_err:\n                        best_err = err\n                        stump = {'feature': j, 'threshold': thresh, 'polarity': polarity}\n                        best_pred = preds\n        return (stump, best_err, best_pred)\n    if n_clf < 1:\n        n_clf = 1\n    y_train = y_train.astype(int)\n    m = X_train.shape[0]\n    weights = np.full(m, 1 / m, dtype=float)\n    stumps = []\n    eps = 1e-10\n    for _ in range(n_clf):\n        (stump, err, preds) = _best_stump(X_train, y_train, weights)\n        err = np.clip(err, eps, 1 - eps)\n        alpha = 0.5 * np.log((1 - err) / err)\n        weights *= np.exp(-alpha * y_train * preds)\n        weights /= weights.sum()\n        stump['alpha'] = alpha\n        stumps.append(stump)\n    k = X_test.shape[0]\n    agg = np.zeros(k)\n    for stump in stumps:\n        j = stump['feature']\n        thresh = stump['threshold']\n        pol = stump['polarity']\n        pred = np.ones(k, dtype=int)\n        if pol == 1:\n            pred[X_test[:, j] < thresh] = -1\n        else:\n            pred[X_test[:, j] < thresh] = 1\n        agg += stump['alpha'] * pred\n    final_pred = np.sign(agg)\n    final_pred[final_pred == 0] = 1\n    return final_pred.astype(int).tolist()\nassert adaboost_predict(np.array([[1],[2],[3],[4]]), np.array([-1,-1,1,1]), np.array([[1.5],[3.5]]), 3) == [-1, 1], \"failed: simple threshold\"\nassert adaboost_predict(np.array([[2],[4],[6],[8]]), np.array([-1,-1,1,1]), np.array([[5],[7]]), 5) == [-1, 1], \"failed: larger n_clf\"\nassert adaboost_predict(np.array([[1,2],[2,1],[3,1],[1,3]]), np.array([1,-1,-1,1]), np.array([[2,2]]), 5)[0] in (-1,1), \"failed: prediction in allowed set\"\nassert len(adaboost_predict(np.array([[0],[1]]), np.array([-1,1]), np.array([[0],[1],[0.5]]), 2)) == 3, \"failed: output length\"\nassert adaboost_predict(np.array([[0],[1],[2]]), np.array([-1,1,-1]), np.array([[1.5]]), 3)[0] in (-1,1), \"failed: odd labels\"\nassert set(adaboost_predict(np.array([[0],[1]]), np.array([-1,1]), np.array([[0],[1]]), 2)).issubset({-1,1}), \"failed: output values range\""}
{"task_id": 128, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Fisher LDA and project the data.\"\"\"\n    if n_components == 0:\n        return []\n    (n_samples, n_features) = X.shape\n    classes = np.unique(y)\n    n_classes = classes.size\n    global_mean = X.mean(axis=0)\n    S_W = np.zeros((n_features, n_features), dtype=float)\n    S_B = np.zeros_like(S_W)\n    for cls in classes:\n        X_c = X[y == cls]\n        n_c = X_c.shape[0]\n        mean_c = X_c.mean(axis=0)\n        diff = X_c - mean_c\n        S_W += diff.T @ diff\n        mean_diff = (mean_c - global_mean).reshape(-1, 1)\n        S_B += n_c * (mean_diff @ mean_diff.T)\n    A = np.linalg.pinv(S_W) @ S_B\n    (eig_vals, eig_vecs) = np.linalg.eigh(A)\n    sorted_idx = np.argsort(eig_vals)[::-1]\n    eig_vecs = eig_vecs[:, sorted_idx]\n    eig_vecs = eig_vecs[:, :n_components]\n    for i in range(eig_vecs.shape[1]):\n        vec = eig_vecs[:, i]\n        for v in vec:\n            if np.abs(v) > 1e-12:\n                if v < 0:\n                    eig_vecs[:, i] = -vec\n                break\n    projected = X @ eig_vecs\n    projected = np.round(projected, 4)\n    return projected.tolist()\nassert multi_class_lda(np.array([[1,1],[1,2],[2,1],[2,2],[8,8],[9,8],[8,9],[9,9]]), np.array([0,0,0,0,1,1,1,1]), 1) == [[1.4142],[2.1213],[2.1213],[2.8284],[11.3137],[12.0208],[12.0208],[12.7279]], \"test case failed: multi_class_lda(example 1, 1 component)\"\nassert multi_class_lda(np.array([[1,1],[1,2],[2,1],[2,2],[8,8],[9,8],[8,9],[9,9]]), np.array([0,0,0,0,1,1,1,1]), 2) == [[1.4142,0.0],[2.1213,-0.7071],[2.1213,0.7071],[2.8284,0.0],[11.3137,0.0],[12.0208,0.7071],[12.0208,-0.7071],[12.7279,0.0]], \"test case failed: multi_class_lda(example 1, 2 components)\"\nassert multi_class_lda(np.array([[2,0],[4,0],[0,2],[0,4]]), np.array([0,0,1,1]), 1) == [[1.4142],[2.8284],[-1.4142],[-2.8284]], \"test case failed: axis-separated data\"\nassert multi_class_lda(np.array([[1],[2],[8],[9]]), np.array([0,0,1,1]), 1) == [[1.0],[2.0],[8.0],[9.0]], \"test case failed: one-dimensional data #1\"\nassert multi_class_lda(np.array([[1,0],[2,0],[8,0],[9,0]]), np.array([0,0,1,1]), 1) == [[1.0],[2.0],[8.0],[9.0]], \"test case failed: axis data #2\"\nassert multi_class_lda(np.array([[2],[4],[6],[8],[10],[12]]), np.array([0,0,1,1,2,2]), 1) == [[2.0],[4.0],[6.0],[8.0],[10.0],[12.0]], \"test case failed: one-dimensional data #2\"\nassert multi_class_lda(np.array([[0,1],[0,2],[0,8],[0,9]]), np.array([0,0,1,1]), 1) == [[1.0],[2.0],[8.0],[9.0]], \"test case failed: axis y-data\"\nassert multi_class_lda(np.array([[-2,0],[-4,0],[0,-2],[0,-4]]), np.array([0,0,1,1]), 1) == [[-1.4142],[-2.8284],[1.4142],[2.8284]], \"test case failed: negative axis data\"\nassert multi_class_lda(np.array([[0,-1],[0,-2],[0,3],[0,4]]), np.array([0,0,1,1]), 1) == [[-1.0],[-2.0],[3.0],[4.0]], \"test case failed: axis y negative-positive data\""}
{"task_id": 140, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import deque\nimport math\nfrom collections import deque\nimport math\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Return the minimum number of minutes required for the agent to reach\n    the lower-right corner of a square maze that contains spreading fire.\n\n    The fire spreads first every minute, then the agent moves.  The agent may\n    enter a cell only if that cell is not burning at the moment he arrives.\n\n    Parameters\n    ----------\n    grid : list[str]\n        An n\u00d7n list of strings consisting only of '.', '#', 'F'.\n\n    Returns\n    -------\n    int\n        The minimum time to reach the goal, or \u20111 if it is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 0:\n        return -1\n    INF = math.inf\n    fire_time = [[INF] * n for _ in range(n)]\n    q_fire = deque()\n    for r in range(n):\n        for (c, ch) in enumerate(grid[r]):\n            if ch == 'F':\n                fire_time[r][c] = 0\n                q_fire.append((r, c))\n    dirs = ((1, 0), (-1, 0), (0, 1), (0, -1))\n    while q_fire:\n        (r, c) = q_fire.popleft()\n        for (dr, dc) in dirs:\n            (nr, nc) = (r + dr, c + dc)\n            if 0 <= nr < n and 0 <= nc < n and (grid[nr][nc] != '#'):\n                if fire_time[nr][nc] == INF:\n                    fire_time[nr][nc] = fire_time[r][c] + 1\n                    q_fire.append((nr, nc))\n\n    def burning_at_start(cell_r, cell_c) -> bool:\n        \"\"\"True if that cell is already burning at t = 0.\"\"\"\n        return fire_time[cell_r][cell_c] == 0\n    if grid[0][0] == '#' or grid[n - 1][n - 1] == '#' or burning_at_start(0, 0) or burning_at_start(n - 1, n - 1):\n        return -1\n    if n == 1:\n        return 0\n    q = deque([(0, 0, 0)])\n    best_arrival = [[INF] * n for _ in range(n)]\n    best_arrival[0][0] = 0\n    while q:\n        (r, c, t) = q.popleft()\n        for (dr, dc) in dirs:\n            (nr, nc) = (r + dr, c + dc)\n            if 0 <= nr < n and 0 <= nc < n and (grid[nr][nc] != '#'):\n                next_t = t + 1\n                if next_t >= fire_time[nr][nc]:\n                    continue\n                if next_t < best_arrival[nr][nc]:\n                    best_arrival[nr][nc] = next_t\n                    if nr == n - 1 and nc == n - 1:\n                        return next_t\n                    q.append((nr, nc, next_t))\n    return -1\nassert escape_fire_maze([\".\"]) == 0, \"test case failed: grid=['.']\"\nassert escape_fire_maze([\"F\"]) == -1, \"test case failed: grid=['F']\"\nassert escape_fire_maze([\"..\", \"..\"]) == 2, \"test case failed: grid=['..','..']\"\nassert escape_fire_maze([\"F.\", \"..\"]) == -1, \"test case failed: grid=['F.','..']\"\nassert escape_fire_maze([\"..F\", \"...\", \"...\"]) == -1, \"test case failed: grid=['..F','...','...']\"\nassert escape_fire_maze([\"...\", \".F.\", \"...\"]) == -1, \"test case failed: grid=['...','.F.','...']\"\nassert escape_fire_maze([\"..\", \".F\"]) == -1, \"test case failed: grid=['..','.F']\"\nassert escape_fire_maze([\"...\", \"...\", \"...\"]) == 4, \"test case failed: grid=['...','...','...']\"\nassert escape_fire_maze([\"....\", \"....\", \"....\", \"....\"]) == 6, \"test case failed: grid=['....','....','....','....']\"\nassert escape_fire_maze([\"F..\", \".F.\", \"..F\"]) == -1, \"test case failed: grid=['F..','.F.','..F']\""}
{"task_id": 141, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef knn_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int, metric: str='euclidean') -> np.ndarray:\n    \"\"\"\n    k\u2013Nearest Neighbours classifier.\n    \n    Parameters\n    ----------\n    X_train : (n_train, n_features) ndarray\n        Training samples.\n    y_train : (n_train,) ndarray\n        Labels of the training samples.\n    X_test  : (n_test,  n_features) ndarray\n        Samples to classify.\n    k       : int\n        Number of neighbours to use (if k > n_train, k == n_train).\n    metric  : {'euclidean', 'manhattan', 'cosine'}, optional\n        Distance metric. Anything else falls back to 'euclidean'.\n    \n    Returns\n    -------\n    preds : (n_test,) ndarray\n        Predicted labels for X_test.\n    \"\"\"\n    if k <= 0:\n        raise ValueError('k must be a positive integer.')\n    metric = metric.lower()\n    n_train = X_train.shape[0]\n    k = min(k, n_train)\n    if metric == 'manhattan':\n        D = np.abs(X_test[:, None, :] - X_train[None, :, :]).sum(axis=2)\n    elif metric == 'cosine':\n        eps = 1e-10\n        t_norm = np.linalg.norm(X_test, axis=1, keepdims=True) + eps\n        r_norm = np.linalg.norm(X_train, axis=1) + eps\n        sim = X_test @ X_train.T / (t_norm * r_norm)\n        D = 1.0 - sim\n    else:\n        diff = X_test[:, None, :] - X_train[None, :, :]\n        D = np.sum(diff ** 2, axis=2)\n    nn_idx = np.argpartition(D, kth=k - 1, axis=1)[:, :k]\n    preds = np.empty(X_test.shape[0], dtype=y_train.dtype)\n    for (i, neighbours) in enumerate(nn_idx):\n        labels = y_train[neighbours]\n        (uniq, counts) = np.unique(labels, return_counts=True)\n        majority = uniq[counts == counts.max()]\n        preds[i] = majority.min()\n    return preds\nassert knn_predict(np.array([[0, 0], [1, 1], [0, 1], [1, 0]]), np.array([0, 0, 1, 1]), np.array([[0.9, 0.9]]), 3, 'euclidean').tolist() == [1], 'Test 1 failed.'\nassert knn_predict(np.array([[0, 0], [1, 2], [3, 4]]), np.array([0, 1, 1]), np.array([[0, 1]]), 1, 'manhattan').tolist() == [0], 'Test 2 failed.'\nassert knn_predict(np.array([[1, 0], [0, 1]]), np.array([0, 1]), np.array([[1, 0.1]]), 1, 'cosine').tolist() == [0], 'Test 3 failed.'\nassert knn_predict(np.array([[0, 0], [0, 2]]), np.array([0, 1]), np.array([[0, 1]]), 2, 'euclidean').tolist() == [0], 'Test 4 failed.'\nassert knn_predict(np.array([[1, 1], [2, 2], [3, 3], [8, 8], [9, 9]]), np.array([0, 0, 0, 1, 1]), np.array([[1.5, 1.5], [9, 8.9]]), 3, 'euclidean').tolist() == [0, 1], 'Test 5 failed.'\nassert knn_predict(np.array([[1, 1], [2, 2], [3, 3]]), np.array([0, 0, 0]), np.array([[2.1, 2]]), 1).tolist() == [0], 'Test 6 failed.'\nassert knn_predict(np.array([[0, 0], [0, 1], [1, 1]]), np.array([0, 0, 1]), np.array([[0.1, 0.1]]), 2, 'minkowski').tolist() == [0], 'Test 7 failed.'\nassert knn_predict(np.array([[0], [1], [2]]), np.array([0, 1, 1]), np.array([[1.1]]), 3, 'manhattan').tolist() == [1], 'Test 8 failed.'\nassert knn_predict(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([0, 1, 2]), np.array([[0.9, 0.05, 0.05]]), 1, 'cosine').tolist() == [0], 'Test 9 failed.'\nassert knn_predict(np.array([[-1, -1], [-2, -2], [1, 1], [2, 2]]), np.array([0, 0, 1, 1]), np.array([[-1.5, -1.5]]), 2, 'euclidean').tolist() == [0], 'Test 10 failed.'"}
{"task_id": 146, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef knn_predict(X: np.ndarray, y: np.ndarray, X_test: np.ndarray, k: int=3, metric: str='euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\"\"\"\n    if X.ndim != 2 or X_test.ndim != 2:\n        raise ValueError('X and X_test must be 2-D arrays')\n    if len(X) != len(y):\n        raise ValueError('X and y must contain the same number of samples')\n    if k <= 0:\n        raise ValueError('k must be a positive integer')\n    metric = metric.lower()\n    n_train = X.shape[0]\n    n_test = X_test.shape[0]\n    k = min(k, n_train)\n    if metric == 'euclidean':\n        diff = X_test[:, None, :] - X[None, :, :]\n        dists = np.linalg.norm(diff, axis=2)\n    elif metric == 'manhattan':\n        diff = X_test[:, None, :] - X[None, :, :]\n        dists = np.abs(diff).sum(axis=2)\n    elif metric == 'cosine':\n        eps = 1e-12\n        x_norm = np.linalg.norm(X, axis=1) + eps\n        xt_norm = np.linalg.norm(X_test, axis=1) + eps\n        dot_prod = X_test @ X.T\n        denom = np.outer(xt_norm, x_norm)\n        dists = 1.0 - dot_prod / denom\n    else:\n        raise ValueError(\"metric must be 'euclidean', 'manhattan', or 'cosine'\")\n    predictions = []\n    for i in range(n_test):\n        idx = np.argpartition(dists[i], k - 1)[:k]\n        neigh_y = y[idx]\n        (unique, counts) = np.unique(neigh_y, return_counts=True)\n        max_count = counts.max()\n        winners = unique[counts == max_count]\n        pred = winners.min()\n        predictions.append(pred)\n    return predictions\nassert knn_predict(np.array([[0,0],[0,1],[1,0],[1,1]]), np.array([0,0,0,1]), np.array([[0.9,0.9]]), 3, 'euclidean') == [0], \"Test 1 failed\"\nassert knn_predict(np.array([[0,0],[0,1],[1,0],[1,1]]), np.array([0,0,0,1]), np.array([[0.9,0.9]]), 1, 'euclidean') == [1], \"Test 2 failed\"\nassert knn_predict(np.array([[0,0],[0,1],[1,0],[1,1]]), np.array([0,0,0,1]), np.array([[0.9,0.9]]), 3, 'manhattan') == [0], \"Test 3 failed\"\nassert knn_predict(np.array([[0,0],[0,1],[1,0],[1,1]]), np.array([0,0,0,1]), np.array([[0.9,0.9]]), 3, 'cosine') == [0], \"Test 4 failed\"\nassert knn_predict(np.array([[1],[2],[3],[10]]), np.array([0,0,0,1]), np.array([[2.5]]), 3, 'euclidean') == [0], \"Test 5 failed\"\nassert knn_predict(np.array([[1],[2],[3],[10]]), np.array([0,0,0,1]), np.array([[2.5]]), 1, 'manhattan') == [0], \"Test 6 failed\"\nassert knn_predict(np.array([[0,0],[0,1],[1,0],[1,1]]), np.array([0,0,0,1]), np.array([[0.2,0.2]]), 10, 'euclidean') == [0], \"Test 7 failed\"\nassert knn_predict(np.array([[0],[1],[2],[3]]), np.array([0,0,1,1]), np.array([[1.5]]), 4, 'euclidean') == [0], \"Test 8 failed\"\nassert knn_predict(np.array([[1,0],[0,1],[1,1]]), np.array([0,1,1]), np.array([[1,1]]), 1, 'cosine') == [1], \"Test 9 failed\"\nassert knn_predict(np.array([[0,0],[0,1],[1,0],[1,1]]), np.array([0,0,0,1]), np.array([[0.1,0.1],[0.9,0.9]]), 1, 'euclidean') == [0,1], \"Test 10 failed\""}
{"task_id": 155, "completion_id": 0, "passed": false, "result": "failed: name 'weights_simple' is not defined", "solution": "import numpy as np\nimport numpy as np\nimport numpy as np\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Forward pass of a two-hidden-layer actor network (Linear \u2192 ReLU \u2192\n    Linear \u2192 ReLU \u2192 Linear \u2192 tanh).\n\n    Args:\n        state (list[float] | np.ndarray): 1-D state vector.\n        weights (dict): Dictionary containing the NumPy arrays\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector, each element rounded to four decimals.\n    \"\"\"\n    x = np.asarray(state, dtype=float)\n    (W1, b1) = (weights['W1'], weights['b1'])\n    (W2, b2) = (weights['W2'], weights['b2'])\n    (W3, b3) = (weights['W3'], weights['b3'])\n    z1 = x @ W1 + b1\n    a1 = np.maximum(0, z1)\n    z2 = a1 @ W2 + b2\n    a2 = np.maximum(0, z2)\n    z3 = a2 @ W3 + b3\n    actions = np.tanh(z3)\n    actions_rounded = np.round(actions, 4)\n    if actions_rounded.shape == ():\n        return [float(actions_rounded)]\n    return actions_rounded.tolist()\nassert actor_forward([0.5, -0.5], weights_simple) == [0.4621], \"test case failed: actor_forward([0.5,-0.5],weights_simple)\"\nassert actor_forward([2.0, 3.0], weights_simple) == [0.9999], \"test case failed: actor_forward([2,3],weights_simple)\"\nassert actor_forward([-2.0, 3.0], weights_simple) == [0.9951], \"test case failed: actor_forward([-2,3],weights_simple)\"\nassert actor_forward([1.0, 1.0], weights_simple) == [0.964], \"test case failed: actor_forward([1,1],weights_simple)\"\nassert actor_forward([0.0, 0.0], weights_simple) == [0.0], \"test case failed: actor_forward([0,0],weights_simple)\"\nassert actor_forward([-1.0, -1.0], weights_simple) == [0.0], \"test case failed: actor_forward([-1,-1],weights_simple)\"\nassert actor_forward([100.0, -100.0], weights_simple) == [1.0], \"test case failed: actor_forward([100,-100],weights_simple)\"\nassert actor_forward([-0.1, 0.1], weights_simple) == [0.0997], \"test case failed: actor_forward([-0.1,0.1],weights_simple)\"\nassert actor_forward([0.3, 0.3], weights_simple) == [0.537], \"test case failed: actor_forward([0.3,0.3],weights_simple)\"\nassert actor_forward([-0.3, 0.7], weights_simple) == [0.6044], \"test case failed: actor_forward([-0.3,0.7],weights_simple)\""}
{"task_id": 160, "completion_id": 0, "passed": false, "result": "failed: failed on simple perfect line", "solution": "import numpy as np\nimport numpy as np\ndef _soft_threshold(a: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S(a, \u03c4).\"\"\"\n    if a > tau:\n        return a - tau\n    if a < -tau:\n        return a + tau\n    return 0.0\ndef fit_lasso(X: np.ndarray, y: np.ndarray, lambda_param: float=1.0, max_iters: int=100, fit_intercept: bool=True) -> tuple[list[float], float]:\n    \"\"\"Lasso (L1) linear regression fitted with coordinate descent.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    (m, n) = X.shape\n    intercept_col = np.ones((m, 1)) if fit_intercept else np.zeros((m, 1))\n    XA = np.hstack((intercept_col, X))\n    p = n + 1\n    w = np.zeros(p)\n    col_sq_norms = (XA ** 2).sum(axis=0)\n    for _ in range(max_iters):\n        if fit_intercept:\n            y_hat_wo_b = XA[:, 1:].dot(w[1:])\n            w[0] = (y - y_hat_wo_b).mean()\n        for j in range(1, p):\n            x_j = XA[:, j]\n            r_j = y - XA.dot(w) + w[j] * x_j\n            rho = x_j.dot(r_j)\n            w[j] = _soft_threshold(rho, lambda_param * m) / col_sq_norms[j]\n    weights = np.round(w[1:], 4).tolist()\n    bias = round(float(w[0]), 4)\n    return (weights, bias)\nassert fit_lasso(np.array([[1], [2], [3]]), np.array([2, 4, 6]), 0.0, 60) == ([2.0], 0.0), \"failed on simple perfect line\"\nassert fit_lasso(np.array([[0], [1], [2]]), np.array([1, 3, 5]), 0.0, 60) == ([2.0], 1.0), \"failed on line with intercept\"\nassert fit_lasso(np.array([[3], [3], [3]]), np.array([2, 2, 2]), 0.0, 30, False) == ([0.6667], 0.0), \"no intercept forces through origin\"\nassert fit_lasso(np.eye(3), np.array([1, 2, 3]), 0.0, 50) == ([1.0, 2.0, 3.0], 0.0), \"identity design matrix\"\nassert fit_lasso(np.eye(3), np.array([1, 2, 3]), 5.0, 50) == ([0.0, 0.0, 0.0], 2.0), \"\u03bb eliminates weights\"\nassert fit_lasso(np.array([[1, -1], [-1, 1]]), np.array([0, 0]), 0.0, 40) == ([0.0, 0.0], 0.0), \"all zeros target\""}
{"task_id": 165, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.GenericAlias' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef hamming_distance(x: list[int] | 'np.ndarray', y: list[int] | 'np.ndarray') -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length vectors.\n\n    Args:\n        x: 1-D list or NumPy array of integers.\n        y: 1-D list or NumPy array of integers.\n\n    Returns:\n        Float rounded to 4 decimal places representing the distance, or \u20111\n        if the vectors have different lengths.\n    \"\"\"\n    x_arr = np.asarray(x)\n    y_arr = np.asarray(y)\n    if x_arr.ndim != 1 or y_arr.ndim != 1:\n        raise ValueError('Input vectors must be 1-D.')\n    if x_arr.size != y_arr.size:\n        return -1\n    N = x_arr.size\n    if N == 0:\n        return 0.0\n    diff = np.count_nonzero(x_arr != y_arr)\n    distance = diff / N\n    return round(float(distance), 4)\nassert hamming_distance([0, 1, 0, 1], [0, 1, 0, 1]) == 0.0, \"Failed identical vectors case\"\nassert hamming_distance([0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1]) == 1.0, \"Failed all differ case\"\nassert hamming_distance([1, 0, 1, 1], [1, 1, 0, 1]) == 0.5, \"Failed example case\"\nassert hamming_distance([1, 2, 3], [1, 2]) == -1, \"Failed unequal length case\"\nassert hamming_distance([3, 5, 7, 9, 11], [3, 4, 7, 8, 11]) == 0.4, \"Failed mixed ints case\"\nimport numpy as np\narr1 = np.array([2, 2, 2, 2])\narr2 = np.array([2, 3, 2, 3])\nassert hamming_distance(arr1, arr2) == 0.5, \"Failed NumPy array input case\"\nassert hamming_distance([7], [8]) == 1.0, \"Failed single element different case\"\nassert hamming_distance([-1, -2, -3], [-1, -2, -4]) == 0.3333, \"Failed negative ints case\"\nassert hamming_distance([1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 1, 0, 1, 0, 0, 1]) == 0.5, \"Failed additional binary case\""}
{"task_id": 169, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef generate_window(window: str, N: int, coefficients: list[float] | None=None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Window name: \"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate (must be > 0).\n    coefficients : list[float] | None, optional\n        Cosine-series coefficients a\u2080\u2026a_M, required only when\n        window == \"generalized_cosine\".\n\n    Returns\n    -------\n    list[float]\n        List with N floats rounded to 4 decimal places.\n    \"\"\"\n    if not isinstance(N, int) or N <= 0:\n        raise ValueError('N must be a positive integer.')\n    if N == 1:\n        return [1.0]\n    name = window.lower()\n    if name not in {'hamming', 'hann', 'blackman_harris', 'generalized_cosine'}:\n        raise ValueError(f'Unknown window type: {window!r}')\n    n = np.arange(N)\n    denom = N - 1\n    two_pi_term = 2.0 * np.pi * n / denom\n    if name == 'hamming':\n        w = 0.54 - 0.46 * np.cos(two_pi_term)\n    elif name == 'hann':\n        w = 0.5 - 0.5 * np.cos(two_pi_term)\n    elif name == 'blackman_harris':\n        (a0, a1, a2, a3) = (0.35875, 0.48829, 0.14128, 0.01168)\n        w = a0 - a1 * np.cos(two_pi_term) + a2 * np.cos(2 * two_pi_term) - a3 * np.cos(3 * two_pi_term)\n    elif name == 'generalized_cosine':\n        if not coefficients:\n            raise ValueError(\"For 'generalized_cosine' you must supply a non-empty 'coefficients' list.\")\n        w = np.zeros(N, dtype=float)\n        for (k, ak) in enumerate(coefficients):\n            w += ak * np.cos(2.0 * np.pi * k * n / denom)\n    return [round(float(v), 4) for v in w]\nassert generate_window(\"hamming\", 5) == [0.08, 0.54, 1.0, 0.54, 0.08], \"test case failed: hamming, N=5\"\nassert generate_window(\"hann\", 5) == [0.0, 0.5, 1.0, 0.5, 0.0], \"test case failed: hann, N=5\"\nassert generate_window(\"blackman_harris\", 5) == [0.0001, 0.2175, 1.0, 0.2175, 0.0001], \"test case failed: blackman_harris, N=5\"\nassert generate_window(\"generalized_cosine\", 5, coefficients=[0.5, -0.5]) == [0.0, 0.5, 1.0, 0.5, 0.0], \"test case failed: generalized_cosine, N=5, coeff=[0.5,-0.5]\"\nassert generate_window(\"hamming\", 1) == [1.0], \"test case failed: hamming, N=1\"\nassert generate_window(\"hann\", 1) == [1.0], \"test case failed: hann, N=1\"\nassert generate_window(\"blackman_harris\", 1) == [1.0], \"test case failed: blackman_harris, N=1\"\nassert generate_window(\"hamming\", 6) == [0.08, 0.3979, 0.9121, 0.9121, 0.3979, 0.08], \"test case failed: hamming, N=6\"\nassert generate_window(\"blackman_harris\", 3) == [0.0001, 1.0, 0.0001], \"test case failed: blackman_harris, N=3\"\nassert generate_window(\"generalized_cosine\", 4, coefficients=[1.0]) == [1.0, 1.0, 1.0, 1.0], \"test case failed: generalized_cosine, N=4, coeff=[1.0]\""}
{"task_id": 171, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef logistic_regression_train_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], epochs: int=5000, learning_rate: float=0.1, batch_size: int=32) -> list[int]:\n    \"\"\"Binary Logistic-Regression trained with mini-batch Gradient Descent.\"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train)\n    uniq = np.unique(y_train)\n    if len(uniq) == 1:\n        return [int(uniq[0])] * len(X_test)\n    if len(uniq) != 2:\n        raise ValueError('Exactly two distinct labels are required for training.')\n    pos_label = uniq[1]\n    y_bin = (y_train == pos_label).astype(float)\n    ones_train = np.ones((X_train.shape[0], 1))\n    ones_test = np.ones((X_test.shape[0], 1))\n    X_train = np.hstack([ones_train, X_train])\n    X_test_aug = np.hstack([ones_test, X_test])\n    (n_samples, n_features) = X_train.shape\n    w = np.zeros(n_features)\n\n    def sigmoid(z):\n        z = np.clip(z, -500, 500)\n        return 1.0 / (1.0 + np.exp(-z))\n    for _ in range(epochs):\n        permutation = np.random.permutation(n_samples)\n        for start in range(0, n_samples, batch_size):\n            idx = permutation[start:start + batch_size]\n            X_b = X_train[idx]\n            y_b = y_bin[idx]\n            preds = sigmoid(X_b @ w)\n            grad = X_b.T @ (preds - y_b) / len(idx)\n            w -= learning_rate * grad\n    probs = sigmoid(X_test_aug @ w)\n    preds_bin = (probs >= 0.5).astype(int)\n    label_map = {0: int(uniq[0]), 1: int(uniq[1])}\n    preds_original = [label_map[int(p)] for p in preds_bin]\n    return preds_original\nassert logistic_regression_train_predict([[-3],[-2],[2],[3]],[0,0,1,1],[[-2.5],[2.5]]) == [0,1], \"failed on shifted 1D separation\"\nassert logistic_regression_train_predict([[1],[2],[3]],[0,0,0],[[10],[-10]]) == [0,0], \"failed on constant-zero label case\"\nassert logistic_regression_train_predict([[1],[2],[3]],[1,1,1],[[0],[4]]) == [1,1], \"failed on constant-one label case\"\nassert logistic_regression_train_predict([[1,1],[1,2],[2,3],[3,3]],[0,0,1,1],[[1,1.5],[3,4]]) == [0,1], \"failed on simple 2D separation\"\nassert logistic_regression_train_predict([[0,0,1],[1,1,1],[2,2,3],[3,3,3]],[0,0,1,1],[[0,0,0.5],[3,3,4]]) == [0,1], \"failed on 3D separation\"\nassert logistic_regression_train_predict([[1],[2],[3],[4]],[1,1,0,0],[[1.5],[3.5]]) == [1,0], \"failed on negative slope separation\"\nassert logistic_regression_train_predict([[0],[1],[10],[11]],[0,0,1,1],[[0.5],[10.5]]) == [0,1], \"failed on large gap separation\"\nassert logistic_regression_train_predict([[1,0],[0,1],[1,1],[2,2]],[0,0,1,1],[[0.2,0.2],[1.5,1.5]]) == [0,1], \"failed on mixed 2D separation\""}
{"task_id": 176, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adaboost_predict(X_train: list[list[int | float]], y_train: list[int], X_test: list[list[int | float]], n_estimators: int=10) -> list[int]:\n    \"\"\"AdaBoost (SAMME-R) with decision stumps, implemented from scratch.\n\n    Parameters\n    ----------\n    X_train : list of list of float\n        Training features.\n    y_train : list of int (0/1)\n        Training labels, 0 = negative class, 1 = positive class.\n    X_test  : list of list of float\n        Samples whose labels must be predicted.\n    n_estimators : int, default=10\n        Maximum number of boosting rounds.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (0/1) for every row of *X_test*.\n    \"\"\"\n\n    def train_best_stump(X: np.ndarray, y: np.ndarray, w: np.ndarray):\n        \"\"\"\n        Exhaustively search the decision-stump with the smallest\n        *weighted* classification error.  Tie-breaking is deterministic:\n            1) smallest error\n            2) smallest feature index\n            3) smallest threshold\n            4) polarity  +1 before \u20131\n        \"\"\"\n        (n_samples, n_features) = X.shape\n        best = {'feature': None, 'threshold': None, 'polarity': None, 'pred': None, 'error': np.inf}\n        for j in range(n_features):\n            column = X[:, j]\n            thresholds = np.unique(column)\n            for thr in thresholds:\n                for pol in (1, -1):\n                    if pol == 1:\n                        h = np.where(column < thr, 1, -1)\n                    else:\n                        h = np.where(column >= thr, 1, -1)\n                    err = np.sum(w * (h != y))\n                    if err < best['error'] - 1e-12:\n                        best.update(feature=j, threshold=float(thr), polarity=pol, pred=h, error=err)\n        return best\n\n    def stump_predict(stump, X):\n        \"\"\"Vectorised prediction of a single stump on data matrix X.\"\"\"\n        col = X[:, stump['feature']]\n        if stump['polarity'] == 1:\n            return np.where(col < stump['threshold'], 1, -1)\n        else:\n            return np.where(col >= stump['threshold'], 1, -1)\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=int)\n    y = 2 * y - 1\n    n_samples = X.shape[0]\n    w = np.full(n_samples, 1.0 / n_samples)\n    stumps: list[dict] = []\n    alphas: list[float] = []\n    for _ in range(n_estimators):\n        stump = train_best_stump(X, y, w)\n        error = stump['error']\n        if error <= 1e-12:\n            alpha = 0.5 * np.log((1.0 - 1e-12) / 1e-12)\n            stumps.append(stump)\n            alphas.append(alpha)\n            break\n        alpha = 0.5 * np.log((1.0 - error) / (error + 1e-10))\n        w *= np.exp(-alpha * y * stump['pred'])\n        w /= np.sum(w)\n        stumps.append(stump)\n        alphas.append(alpha)\n    X_te = np.asarray(X_test, dtype=float)\n    agg = np.zeros(X_te.shape[0], dtype=float)\n    for (stump, alpha) in zip(stumps, alphas):\n        agg += alpha * stump_predict(stump, X_te)\n    preds = (agg >= 0).astype(int).tolist()\n    return preds\nassert adaboost_predict([[0,0],[1,0],[0,1],[1,1]],[0,1,1,1],[[0,0],[1,1]],3)==[0,1],\"failed OR data\"\nassert adaboost_predict([[1,2],[2,3],[3,4],[4,5]],[0,0,1,1],[[1.5,2.5],[3.5,4.5]],4)==[0,1],\"failed linear split\"\nassert adaboost_predict([[1],[2],[3],[4]],[0,0,1,1],[[2],[4]],3)==[0,1],\"failed 1-D split\"\nassert adaboost_predict([[1],[2],[3]],[0,1,1],[[1],[3]],3)==[0,1],\"failed small 1-D\"\nassert adaboost_predict([[0,0],[0,1],[1,0],[1,1]],[0,0,0,1],[[0,1],[1,1]],4)==[0,1],\"failed AND-like data\"\nassert adaboost_predict([[0],[2],[4],[6]],[0,0,1,1],[[1],[5]],5)==[0,1],\"failed even/odd split\"\nassert adaboost_predict([[1,1],[1,2],[2,1],[2,2]],[0,0,1,1],[[1,1],[2,2]],3)==[0,1],\"failed grid split\"\nassert adaboost_predict([[2],[3],[10],[12]],[0,0,1,1],[[2.5],[11]],4)==[0,1],\"failed distant clusters\"\nassert adaboost_predict([[0,5],[1,6],[2,7],[3,8]],[0,0,1,1],[[0.5,5.5],[2.5,7.5]],4)==[0,1],\"failed correlated features\""}
{"task_id": 178, "completion_id": 0, "passed": false, "result": "failed: test failed: particle_swarm_optimisation(2,1,1,2)", "solution": "import numpy as np\nimport numpy as np\ndef particle_swarm_optimisation(n_dims: int, num_particles: int, num_iterations: int, seed: int=1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function with Particle Swarm Optimisation.\n\n    Args:\n        n_dims:            number of decision variables  (>0)\n        num_particles:     swarm size                   (>0)\n        num_iterations:    optimisation iterations      (>0)\n        seed:              RNG seed (default 1)\n\n    Returns\n        Best objective value found, rounded to 4 decimals, or \u20131 on bad input.\n    \"\"\"\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n    rng = np.random.default_rng(seed)\n    (w, c1, c2) = (0.5, 1.5, 1.5)\n    (lo, hi) = (-1.0, 1.0)\n    positions = rng.uniform(lo, hi, size=(num_particles, n_dims))\n    velocities = rng.uniform(lo, hi, size=(num_particles, n_dims))\n    pbest_pos = positions.copy()\n    pbest_val = np.sum(pbest_pos ** 2, axis=1)\n    gbest_idx = np.argmin(pbest_val)\n    gbest_pos = pbest_pos[gbest_idx].copy()\n    gbest_val = pbest_val[gbest_idx]\n    for _ in range(num_iterations):\n        r1 = rng.random(size=(num_particles, n_dims))\n        r2 = rng.random(size=(num_particles, n_dims))\n        velocities = w * velocities + c1 * r1 * (pbest_pos - positions) + c2 * r2 * (gbest_pos - positions)\n        positions = np.clip(positions + velocities, lo, hi)\n        curr_val = np.sum(positions ** 2, axis=1)\n        improved = curr_val < pbest_val\n        pbest_pos[improved] = positions[improved]\n        pbest_val[improved] = curr_val[improved]\n        gbest_idx = np.argmin(pbest_val)\n        gbest_pos = pbest_pos[gbest_idx].copy()\n        gbest_val = pbest_val[gbest_idx]\n    return round(float(gbest_val), 4)\nassert particle_swarm_optimisation(2, 30, 100, seed=42) == 0.0, \"test failed: particle_swarm_optimisation(2,30,100,42)\"\nassert particle_swarm_optimisation(5, 50, 200, seed=3) == 0.0, \"test failed: particle_swarm_optimisation(5,50,200,3)\"\nassert particle_swarm_optimisation(10, 60, 300, seed=7) == 0.0, \"test failed: particle_swarm_optimisation(10,60,300,7)\"\nassert particle_swarm_optimisation(3, 10, 150, seed=11) == 0.0, \"test failed: particle_swarm_optimisation(3,10,150,11)\"\nassert particle_swarm_optimisation(4, 80, 250, seed=19) == 0.0, \"test failed: particle_swarm_optimisation(4,80,250,19)\"\nassert particle_swarm_optimisation(6, 40, 120, seed=23) == 0.0, \"test failed: particle_swarm_optimisation(6,40,120,23)\"\nassert particle_swarm_optimisation(1, 20, 90, seed=29) == 0.0, \"test failed: particle_swarm_optimisation(1,20,90,29)\"\nassert particle_swarm_optimisation(8, 70, 300, seed=31) == 0.0, \"test failed: particle_swarm_optimisation(8,70,300,31)\"\nassert particle_swarm_optimisation(2, 1, 1, seed=2) == 0.0, \"test failed: particle_swarm_optimisation(2,1,1,2)\"\nassert particle_swarm_optimisation(-1, 30, 100) == -1, \"test failed: invalid input not handled\""}
{"task_id": 180, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef knn_predict(X_train: list[list[float]], y_train: list, X_test: list[list[float]], k: int=5, task: str='classification') -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\"\"\"\n    X_tr = np.asarray(X_train, dtype=float)\n    X_te = np.asarray(X_test, dtype=float)\n    y_tr = np.asarray(y_train, dtype=object)\n    n_train = X_tr.shape[0]\n    if k <= 0 or k > n_train:\n        k = n_train\n    predictions: list = []\n    for x in X_te:\n        dists = np.linalg.norm(X_tr - x, axis=1)\n        if k == n_train:\n            nn_idx = np.arange(n_train)\n        else:\n            nn_idx = np.argpartition(dists, k - 1)[:k]\n        neighbours = y_tr[nn_idx]\n        if task == 'regression':\n            mean_val = float(np.mean(neighbours.astype(float)))\n            predictions.append(round(mean_val, 4))\n        else:\n            counts = Counter(neighbours)\n            max_cnt = max(counts.values())\n            best_label = min((lbl for (lbl, cnt) in counts.items() if cnt == max_cnt))\n            predictions.append(best_label)\n    return predictions\nassert knn_predict([[0,0],[1,1],[2,2]], [0,0,1], [[1,1]], 2, \"classification\") == [0], \"failed: basic classification\"\nassert knn_predict([[0],[1],[2]], [1.0,1.5,3.0], [[1]], 2, \"regression\") == [1.25], \"failed: basic regression\"\nassert knn_predict([[0,0],[3,3]], [\"A\",\"B\"], [[1,1]], 5, \"classification\") == [\"A\"], \"failed: k larger than samples\"\nassert knn_predict([[0,0],[2,0],[0,2],[2,2]], [1,2,2,3], [[1,1]], 0, \"regression\") == [2.0], \"failed: k == 0 (use all)\"\nassert knn_predict([[0],[1],[2],[3]], [1,2,2,3], [[1.5]], 3, \"classification\") == [2], \"failed: tie vote numeric\"\nassert knn_predict([[0],[10]], [5.5555, 5.5555], [[5]], 1, \"regression\") == [5.5555], \"failed: rounding unchanged\"\nassert knn_predict([[1,2],[2,3],[3,4]], [10,20,30], [[2,3]], 2, \"regression\") == [15.0], \"failed: regression average\"\nassert knn_predict([[0,0],[1,1],[1,-1]], [\"yes\",\"no\",\"no\"], [[1,0]], 2, \"classification\") == [\"no\"], \"failed: majority vote\"\nassert knn_predict([[0,0],[0,0],[1,1]], [1,1,2], [[0,0]], 2, \"classification\") == [1], \"failed: duplicate points\""}
{"task_id": 184, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef decision_tree_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_depth: int | None=None) -> list[int]:\n    \"\"\"CART-style binary decision tree (Gini) completely from scratch.\"\"\"\n\n    class Node:\n        __slots__ = ('is_leaf', 'prediction', 'feature', 'threshold', 'left', 'right')\n\n        def __init__(self, *, is_leaf: bool, prediction: int | None=None, feature: int | None=None, threshold: float | None=None, left: 'Node | None'=None, right: 'Node | None'=None):\n            self.is_leaf = is_leaf\n            self.prediction = prediction\n            self.feature = feature\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n    y_train = y_train.astype(int, copy=False)\n    n_classes = int(y_train.max()) + 1\n\n    def gini(counts: np.ndarray) -> float:\n        \"\"\"Gini impurity from class counts.\"\"\"\n        total = counts.sum()\n        if total == 0:\n            return 0.0\n        probs = counts / total\n        return 1.0 - np.sum(probs ** 2)\n\n    def majority_class(counts: np.ndarray) -> int:\n        \"\"\"Smallest label in case of ties keeps result deterministic.\"\"\"\n        return int(np.argmax(counts))\n\n    def find_best_split(indices: np.ndarray) -> tuple[int, float, float, np.ndarray, np.ndarray] | None:\n        \"\"\"\n        Returns (best_feature, best_threshold, best_gain, left_idx, right_idx)\n        or None if no split brings positive gain.\n        \"\"\"\n        X_node = X_train[indices]\n        y_node = y_train[indices]\n        parent_counts = np.bincount(y_node, minlength=n_classes)\n        parent_gini = gini(parent_counts)\n        best_gain = 0.0\n        best_feature = -1\n        best_threshold = 0.0\n        best_left_idx = best_right_idx = None\n        (n_samples, n_features) = X_node.shape\n        for feat in range(n_features):\n            values = X_node[:, feat]\n            order = np.argsort(values)\n            sorted_y = y_node[order]\n            sorted_vals = values[order]\n            left_counts = np.zeros(n_classes, dtype=int)\n            right_counts = parent_counts.copy()\n            for i in range(1, n_samples):\n                cls = sorted_y[i - 1]\n                left_counts[cls] += 1\n                right_counts[cls] -= 1\n                if sorted_vals[i] == sorted_vals[i - 1]:\n                    continue\n                left_n = i\n                right_n = n_samples - i\n                g_left = gini(left_counts)\n                g_right = gini(right_counts)\n                weighted = (left_n * g_left + right_n * g_right) / n_samples\n                gain = parent_gini - weighted\n                if gain > best_gain + 1e-12:\n                    best_gain = gain\n                    best_feature = feat\n                    best_threshold = (sorted_vals[i] + sorted_vals[i - 1]) / 2.0\n                    mask = X_train[indices, feat] <= best_threshold\n                    best_left_idx = indices[mask]\n                    best_right_idx = indices[~mask]\n        if best_gain <= 0.0 or best_left_idx.size == 0 or best_right_idx.size == 0:\n            return None\n        return (best_feature, best_threshold, best_gain, best_left_idx, best_right_idx)\n\n    def build(indices: np.ndarray, depth: int) -> Node:\n        y_subset = y_train[indices]\n        counts = np.bincount(y_subset, minlength=n_classes)\n        if np.unique(y_subset).size == 1:\n            return Node(is_leaf=True, prediction=int(y_subset[0]))\n        if max_depth is not None and depth >= max_depth:\n            return Node(is_leaf=True, prediction=majority_class(counts))\n        split = find_best_split(indices)\n        if split is None:\n            return Node(is_leaf=True, prediction=majority_class(counts))\n        (feat, thr, _, left_idx, right_idx) = split\n        left_child = build(left_idx, depth + 1)\n        right_child = build(right_idx, depth + 1)\n        return Node(is_leaf=False, feature=feat, threshold=thr, left=left_child, right=right_child)\n    root = build(np.arange(X_train.shape[0]), depth=0)\n\n    def predict_single(x: np.ndarray) -> int:\n        node = root\n        while not node.is_leaf:\n            node = node.left if x[node.feature] <= node.threshold else node.right\n        return node.prediction\n    return [predict_single(row) for row in X_test]\nassert decision_tree_predict(np.array([[1],[2],[3],[4],[5],[6]]), np.array([0,0,0,1,1,1]), np.array([[1.5],[3.5],[5.5]])) == [0,0,1], \"failed: simple 1-D split\"\nassert decision_tree_predict(np.array([[0,0],[1,1],[2,2],[9,9],[10,10],[11,11]]), np.array([0,0,0,1,1,1]), np.array([[1,1],[10,10]])) == [0,1], \"failed: two-dimensional clearly separated\"\nassert decision_tree_predict(np.array([[1],[2],[3],[10],[11],[12],[20],[21],[22]]), np.array([0,0,0,1,1,1,2,2,2]), np.array([[2.5],[11.5],[21]])) == [0,1,2], \"failed: three-class 1-D split\"\nassert decision_tree_predict(np.array([[1],[2],[3]]), np.array([1,1,1]), np.array([[0],[5]])) == [1,1], \"failed: all labels identical\"\nassert decision_tree_predict(np.array([[0.1],[0.2],[0.3],[0.4],[0.5],[0.6]]), np.array([0,0,0,1,1,1]), np.array([[0.25],[0.55]])) == [0,1], \"failed: threshold around 0.35\"\nassert decision_tree_predict(np.array([[5.1,3.5,1.4],[4.9,3.0,1.4],[6.2,3.4,5.4],[5.9,3.0,5.1]]), np.array([0,0,1,1]), np.array([[5.0,3.4,1.5],[6.0,3.1,5.0]])) == [0,1], \"failed: example in task description\"\nassert decision_tree_predict(np.array([[0,2],[1,2],[2,2],[0,10],[1,10],[2,10]]), np.array([0,0,0,1,1,1]), np.array([[0,3],[0,9]])) == [0,1], \"failed: split on second feature\"\nassert decision_tree_predict(np.array([[1],[2],[3],[4],[5]]), np.array([0,0,0,1,1]), np.array([[1.5],[4.5]])) == [0,1], \"failed: odd number of samples\"\nassert decision_tree_predict(np.array([[1],[1],[2],[2],[3],[3],[10],[10],[11],[11]]), np.array([0,0,0,0,0,0,1,1,1,1]), np.array([[1],[10],[3]])) == [0,1,0], \"failed: duplicates in features\"\nassert decision_tree_predict(np.array([[0],[5],[10],[15]]), np.array([0,1,2,3]), np.array([[12],[1]])) == [2,0], \"failed: multi-class, arbitrary values\""}
{"task_id": 190, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted\n    Gini impurity for a single binary split.\n\n    Parameters\n    ----------\n    X : list[list[float]] | np.ndarray  (n_samples \u00d7 n_features)\n    y : list[int] | np.ndarray          (n_samples,)\n\n    Returns\n    -------\n    (int, float | None, float)\n        (best_feature_index, best_threshold_value, best_gini_rounded)\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    if X.ndim != 2:\n        raise ValueError('X must be 2-dimensional')\n    if y.ndim != 1 or y.shape[0] != X.shape[0]:\n        raise ValueError('y must be 1-D and have the same length as X')\n    (n_samples, n_features) = X.shape\n    if n_samples == 0 or n_features == 0:\n        return (-1, None, 0.0)\n    (classes, y_enc) = np.unique(y, return_inverse=True)\n    n_classes = classes.size\n\n    def gini(counts, n):\n        if n == 0:\n            return 0.0\n        p = counts / n\n        return 1.0 - np.sum(p * p)\n    total_counts = np.bincount(y_enc, minlength=n_classes)\n    g_whole = gini(total_counts, n_samples)\n    best_gini = np.inf\n    best_feat = -1\n    best_th = None\n    tol = 1e-12\n    for j in range(n_features):\n        col = X[:, j]\n        if np.all(col == col[0]):\n            continue\n        order = np.argsort(col, kind='mergesort')\n        col_sorted = col[order]\n        y_sorted = y_enc[order]\n        left_counts = np.zeros(n_classes, dtype=int)\n        for i in range(n_samples):\n            cls = y_sorted[i]\n            left_counts[cls] += 1\n            is_last_occurrence = i == n_samples - 1 or col_sorted[i] != col_sorted[i + 1]\n            if not is_last_occurrence:\n                continue\n            n_left = i + 1\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:\n                continue\n            right_counts = total_counts - left_counts\n            g_left = gini(left_counts, n_left)\n            g_right = gini(right_counts, n_right)\n            g_split = n_left / n_samples * g_left + n_right / n_samples * g_right\n            if g_split + tol < best_gini:\n                (best_gini, best_feat, best_th) = (g_split, j, col_sorted[i])\n            elif abs(g_split - best_gini) < tol:\n                if j < best_feat:\n                    (best_feat, best_th) = (j, col_sorted[i])\n                elif j == best_feat and col_sorted[i] < best_th:\n                    best_th = col_sorted[i]\n    if best_feat == -1:\n        return (-1, None, round(g_whole, 4))\n    return (best_feat, best_th, round(best_gini, 4))\nassert best_gini_split([[2], [3], [10], [19]], [0, 0, 1, 1]) == (0, 3, 0.0), \"test 1 failed\"\nassert best_gini_split([[2,3],[1,5],[3,2],[4,4]], [0,0,1,1]) == (0, 2, 0.0), \"test 2 failed\"\nassert best_gini_split([[1],[2],[3],[4]], [0,1,0,1]) == (0, 1, 0.3333), \"test 4 failed\"\nassert best_gini_split([[1,10], [2,1], [3,1], [4,10]], [0,0,1,1]) == (0, 2, 0.0), \"test 5 failed\"\nassert best_gini_split([[1,1], [2,2], [3,3], [4,4], [5,5], [6,6]], [0,0,0,1,1,1]) == (0, 3, 0.0), \"test 6 failed\"\nassert best_gini_split([[1],[2],[3]], [0,1,2]) == (0, 1, 0.3333), \"test 7 failed\"\nassert best_gini_split([[1,10],[2,10],[1,1],[2,1]], [0,0,1,1]) == (1, 1, 0.0), \"test 8 failed\"\nassert best_gini_split([[1],[1],[1]], [0,1,0]) == (-1, None, 0.4444), \"test 9 failed\"\nassert best_gini_split([[1,5,9],[2,6,8],[3,7,7],[4,8,6]], [0,0,1,1]) == (0, 2, 0.0), \"test 10 failed\""}
{"task_id": 191, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nimport numpy as np\ndef metric(name: str):\n    \"\"\"Return a distance function with integrated validation.\"\"\"\n    supported = ('euclidean', 'manhattan', 'chebyshev', 'cosine')\n\n    def d(v1, v2):\n        if name not in supported:\n            return -1\n        if not isinstance(v1, (list, tuple, np.ndarray)):\n            return -1\n        if not isinstance(v2, (list, tuple, np.ndarray)):\n            return -1\n        try:\n            a = np.asarray(v1, dtype=float)\n            b = np.asarray(v2, dtype=float)\n        except Exception:\n            return -1\n        if a.ndim != 1 or b.ndim != 1:\n            return -1\n        if a.size == 0 or b.size == 0:\n            return -1\n        if a.size != b.size:\n            return -1\n        if name == 'euclidean':\n            value = np.linalg.norm(a - b)\n        elif name == 'manhattan':\n            value = np.sum(np.abs(a - b))\n        elif name == 'chebyshev':\n            value = np.max(np.abs(a - b))\n        else:\n            (na, nb) = (np.linalg.norm(a), np.linalg.norm(b))\n            if na == 0 or nb == 0:\n                return -1\n            value = 1 - a @ b / (na * nb)\n        return round(float(value), 4)\n    return d\nassert metric('euclidean')([1,2,3],[4,5,6]) == 5.1962, \"test failed: metric('euclidean')([1,2,3],[4,5,6])\"\nassert metric('manhattan')([1,2,3],[4,5,6]) == 9.0, \"test failed: metric('manhattan')([1,2,3],[4,5,6])\"\nassert metric('chebyshev')([1,2,3],[4,5,6]) == 3.0, \"test failed: metric('chebyshev')([1,2,3],[4,5,6])\"\nassert metric('cosine')([1,0],[0,1]) == 1.0, \"test failed: metric('cosine')([1,0],[0,1])\"\nassert metric('cosine')([1,2],[1,2]) == 0.0, \"test failed: metric('cosine')([1,2],[1,2])\"\nassert metric('euclidean')([0,0],[0,0]) == 0.0, \"test failed: metric('euclidean')([0,0],[0,0])\"\nassert metric('manhattan')([3,4,5],[3,4,5]) == 0.0, \"test failed: metric('manhattan') identical vectors\"\nassert metric('unknown')([1,2],[3,4]) == -1, \"test failed: metric('unknown') should be -1\"\nassert metric('euclidean')([1,2,3],[1,2]) == -1, \"test failed: metric('euclidean') mismatched length\"\nassert metric('cosine')([0,0,0],[1,0,0]) == -1, \"test failed: metric('cosine') zero-norm vector\""}
{"task_id": 197, "completion_id": 0, "passed": false, "result": "failed: name 'SimpleNamespace' is not defined", "solution": "import numpy as np\nfrom itertools import product\nimport math\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\nimport numpy as np\ndef _is_discrete(space: Any) -> bool:\n    \"\"\"Utility: does the sub-space describe a discrete set?\"\"\"\n    return hasattr(space, 'n')\ndef _is_continuous(space: Any) -> bool:\n    \"\"\"Utility: does the sub-space describe a continuous set?\"\"\"\n    return hasattr(space, 'shape')\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"\n    Summarise an RL environment's action space.\n\n    Parameters\n    ----------\n    env : Any\n        Environment exposing an `action_space` attribute \u00e0-la OpenAI-Gym.\n    md_action : bool\n        Ignored by the logic (kept for the required signature) \u2013 the function\n        determines the true structure directly from `env.action_space`.\n    cont_action : bool\n        Ignored by the logic (kept for the required signature) \u2013 the function\n        determines the true structure directly from `env.action_space`.\n\n    Returns\n    -------\n    Tuple containing\n        n_actions_per_dim : list[int | float]\n            Number of distinct actions for every dimension\n            (`np.inf` for continuous ones).\n        action_ids : list[tuple[int, ...]] | None\n            All possible discrete actions as tuples; `None` if at least one\n            dimension is continuous.\n        action_dim : int\n            Number of action dimensions.\n    \"\"\"\n    space = env.action_space\n    n_actions_per_dim: List[Union[int, float]] = []\n    if hasattr(space, 'spaces'):\n        subspaces = space.spaces\n        for sub in subspaces:\n            if _is_discrete(sub):\n                n_actions_per_dim.append(int(sub.n))\n            elif _is_continuous(sub):\n                n_actions_per_dim.append(np.inf)\n            else:\n                n_actions_per_dim.append(np.inf)\n        action_dim = len(subspaces)\n    elif _is_discrete(space):\n        n_actions_per_dim.append(int(space.n))\n        action_dim = 1\n    elif _is_continuous(space):\n        dim = int(space.shape[0]) if len(space.shape) > 0 else 1\n        n_actions_per_dim.extend([np.inf] * dim)\n        action_dim = dim\n    else:\n        n_actions_per_dim.append(np.inf)\n        action_dim = 1\n    contains_continuous = any((np.isinf(x) for x in n_actions_per_dim))\n    if contains_continuous:\n        action_ids = None\n    else:\n        ranges = [range(int(n)) for n in n_actions_per_dim]\n        action_ids = [tuple(p) for p in product(*ranges)]\n    return (n_actions_per_dim, action_ids, action_dim)\nassert action_stats(SimpleNamespace(action_space=D(4)), False, False) == ([4], [0, 1, 2, 3], 1), \"failed test 1\"\nexpected_ids = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\nassert action_stats(SimpleNamespace(action_space=SimpleNamespace(spaces=[D(2), D(3)])), True, False) == ([2, 3], expected_ids, 2), \"failed test 2\"\nmix_env = SimpleNamespace(action_space=SimpleNamespace(spaces=[D(2), C(3)]))\nassert action_stats(mix_env, True, False) == ([2, np.inf], None, 2), \"failed test 3\"\nassert action_stats(SimpleNamespace(action_space=C(5)), False, True) == ([np.inf], None, 1), \"failed test 4\"\ncont_env = SimpleNamespace(action_space=SimpleNamespace(shape=(4,)))\nassert action_stats(cont_env, True, True) == ([np.inf, np.inf, np.inf, np.inf], None, 4), \"failed test 5\"\nassert action_stats(SimpleNamespace(action_space=D(1)), False, False) == ([1], [0], 1), \"failed test 6\"\nids_3d = list(product(range(2), repeat=3))\ntri_env = SimpleNamespace(action_space=SimpleNamespace(spaces=[D(2), D(2), D(2)]))\nassert action_stats(tri_env, True, False) == ([2, 2, 2], ids_3d, 3), \"failed test 7\"\nids_31 = list(product(range(3), range(1)))\nassert action_stats(SimpleNamespace(action_space=SimpleNamespace(spaces=[D(3), D(1)])), True, False) == ([3, 1], ids_31, 2), \"failed test 8\"\nmix2_env = SimpleNamespace(action_space=SimpleNamespace(spaces=[C(2), D(3)]))\nassert action_stats(mix2_env, True, False) == ([np.inf, 3], None, 2), \"failed test 9\"\nassert action_stats(SimpleNamespace(action_space=D(10)), False, False) == ([10], list(range(10)), 1), \"failed test 10\""}
{"task_id": 198, "completion_id": 0, "passed": false, "result": "failed: name 'phi1' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        One array per document. Array of document *d* has shape (N_d, T)\n        and contains the current variational parameter \u03d5 of this document.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the index of the *n*-th token\n        in document *d*.\n    V : int\n        Size of the vocabulary.\n\n    Returns\n    -------\n    list[list[float]]\n        (V \u00d7 T) matrix with the updated word\u2013topic probabilities.\n        Each column sums to one (\u2264 1e-4 rounding error) and every\n        number is rounded to four decimals.\n    \"\"\"\n    if not phi:\n        return [[0.0] * 0 for _ in range(V)]\n    T = phi[0].shape[1]\n    counts = np.zeros((V, T), dtype=np.float64)\n    for (doc_phi, doc_words) in zip(phi, corpus):\n        words = np.asarray(doc_words, dtype=np.intp)\n        np.add.at(counts, words, doc_phi)\n    seen_words_mask = counts.sum(axis=1) > 0\n    col_sums = counts.sum(axis=0)\n    beta = np.zeros_like(counts)\n    non_zero_cols = col_sums > 0\n    beta[:, non_zero_cols] = counts[:, non_zero_cols] / col_sums[non_zero_cols]\n    zero_cols = ~non_zero_cols\n    if zero_cols.any():\n        observed_idx = np.where(seen_words_mask)[0]\n        if observed_idx.size == 0:\n            beta[:, zero_cols] = 1.0 / V\n        else:\n            beta[observed_idx[:, None], zero_cols] = 1.0 / observed_idx.size\n    beta = np.round(beta, 4)\n    for t in range(T):\n        diff = 1.0 - beta[:, t].sum()\n        if abs(diff) > 0.0001:\n            col = beta[:, t]\n            idx = int(np.argmax(col))\n            adj = np.round(diff, 4)\n            new_val = np.round(col[idx] + adj, 4)\n            beta[idx, t] = max(new_val, 0.0)\n    return beta.tolist()\nassert update_beta(phi1, corpus1, 3) == [[0.3684, 0.0968], [0.4737, 0.6774], [0.1579, 0.2258]], \"failed: test case 1\"\nassert update_beta(phi2, corpus2, 2) == [[0.6667], [0.3333]], \"failed: test case 2\"\nassert update_beta(phi3, corpus3, 3) == [[0.1429, 0.5], [0.3571, 0.3125], [0.5, 0.1875]], \"failed: test case 3\"\nassert update_beta(phi4, corpus4, 2) == [[1.0, 0.0], [0.0, 1.0]], \"failed: test case 4\"\nassert update_beta(phi5, corpus5, 2) == [[0.3333, 0.3333], [0.6667, 0.6667]], \"failed: test case 5\"\nassert update_beta(phi6, corpus6, 3) == [[0.9474, 0.0952, 0.0952], [0.0263, 0.8571, 0.0476], [0.0263, 0.0476, 0.8571]], \"failed: test case 6\"\nassert update_beta(phi7, corpus7, 1) == [[1.0, 1.0]], \"failed: test case 7\"\nassert update_beta(phi8, corpus8, 4) == [[0.0, 0.0], [0.3333, 0.6364], [0.6667, 0.3636], [0.0, 0.0]], \"failed: test case 8\"\nassert update_beta(phi9, corpus9, 2) == [[0.5, 0.5], [0.5, 0.5]], \"failed: test case 9\"\nassert update_beta(phi10, corpus10, 2) == [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]], \"failed: test case 10\""}
{"task_id": 202, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef _total_cost(dist_matrix: np.ndarray, medoid_idx: list[int]) -> float:\n    \"\"\"\n    Return the sum of distances of every sample to its nearest medoid.\n    \"\"\"\n    d_to_medoids = dist_matrix[:, medoid_idx]\n    return np.min(d_to_medoids, axis=1).sum()\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Input data.\n    k : int\n        Number of desired clusters (1 \u2264 k \u2264 n_samples).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels (0 \u2026 k-1) for every sample.\n    \"\"\"\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D NumPy array.')\n    n_samples = X.shape[0]\n    if not 1 <= k <= n_samples:\n        raise ValueError('k must satisfy 1 \u2264 k \u2264 n_samples')\n    diff = X[:, None, :] - X[None, :, :]\n    dist_matrix = np.linalg.norm(diff, axis=-1)\n    medoid_idx: list[int] = list(range(k))\n    current_cost = _total_cost(dist_matrix, medoid_idx)\n    improved = True\n    while improved:\n        best_swap = None\n        best_reduction = 0.0\n        non_medoid_idx = [i for i in range(n_samples) if i not in medoid_idx]\n        for (p_pos, p) in enumerate(medoid_idx):\n            for h in non_medoid_idx:\n                candidate_medoid_idx = medoid_idx.copy()\n                candidate_medoid_idx[p_pos] = h\n                cand_cost = _total_cost(dist_matrix, candidate_medoid_idx)\n                reduction = current_cost - cand_cost\n                if reduction > best_reduction:\n                    best_reduction = reduction\n                    best_swap = (p_pos, h)\n        if best_swap is not None:\n            (p_pos, h) = best_swap\n            medoid_idx[p_pos] = h\n            current_cost -= best_reduction\n        else:\n            improved = False\n    final_dists = dist_matrix[:, medoid_idx]\n    labels = final_dists.argmin(axis=1).tolist()\n    return labels\nassert pam_clustering(np.array([[0,0]]),1)==[0],\"failed: single sample, k=1\"\nassert pam_clustering(np.array([[0,0],[1,0],[0,1],[5,5],[6,5],[5,6]]),2)==[0,0,0,1,1,1],\"failed: two well-separated clusters\"\nassert pam_clustering(np.array([[0],[10]]),2)==[0,1],\"failed: two samples, k=2\"\nassert pam_clustering(np.array([[1],[2],[3]]),3)==[0,1,2],\"failed: k equals number of samples\"\nassert pam_clustering(np.array([[-1,-1],[0,0],[1,1]]),1)==[0,0,0],\"failed: k=1, centre should move\"\nassert pam_clustering(np.array([[0],[1],[2],[3],[4]]),1)==[0,0,0,0,0],\"failed: all points one cluster\"\nassert pam_clustering(np.array([[1],[4],[7],[10]]),4)==[0,1,2,3],\"failed: each point its own cluster\""}
{"task_id": 216, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic-Net linear regression trained with batch gradient descent.\"\"\"\n    try:\n        X_tr = np.asarray(x_train, dtype=float)\n        X_te = np.asarray(x_test, dtype=float)\n        y_tr = np.asarray(y_train, dtype=float).ravel()\n    except Exception:\n        return -1\n    if X_tr.ndim == 1:\n        X_tr = X_tr.reshape(-1, 1)\n    if X_te.ndim == 1:\n        X_te = X_te.reshape(-1, 1)\n    if X_tr.shape[1] != X_te.shape[1] or n_iterations < 1:\n        return -1\n\n    def poly_expansion(matrix, deg):\n        if deg <= 1:\n            return matrix.copy()\n        powers = [matrix ** p for p in range(1, deg + 1)]\n        return np.concatenate(powers, axis=1)\n    X_tr_poly = poly_expansion(X_tr, degree)\n    X_te_poly = poly_expansion(X_te, degree)\n    mu = X_tr_poly.mean(axis=0)\n    sigma = X_tr_poly.std(axis=0)\n    sigma[sigma == 0] = 1.0\n    X_tr_std = (X_tr_poly - mu) / sigma\n    X_te_std = (X_te_poly - mu) / sigma\n    X_tr_std = np.hstack((np.ones((X_tr_std.shape[0], 1)), X_tr_std))\n    X_te_std = np.hstack((np.ones((X_te_std.shape[0], 1)), X_te_std))\n    (n_samples, n_features) = X_tr_std.shape\n    w = np.zeros(n_features)\n    l2_factor = reg_factor * (1.0 - l1_ratio)\n    l1_factor = reg_factor * l1_ratio\n    for _ in range(int(n_iterations)):\n        y_hat = X_tr_std @ w\n        error = y_hat - y_tr\n        grad = X_tr_std.T @ error / n_samples\n        grad[1:] += l2_factor * w[1:]\n        grad[1:] += l1_factor * np.sign(w[1:])\n        w -= learning_rate * grad\n    y_pred = X_te_std @ w\n    return np.round(y_pred, 4).tolist()\nassert elastic_net_regression([[1],[2],[3],[4]],[2,4,6,8],[[5],[6]],1,0.0,0.5,5000,0.1)==[10.0,12.0],\"failed: simple linear case\"\nassert elastic_net_regression([[0],[1],[2],[3]],[1,3,5,7],[[4]],1,0.0,0.5,6000,0.05)==[9.0],\"failed: another linear case\"\nassert elastic_net_regression([[1,1],[2,1],[3,1]],[3,5,7],[[4,1]],1,0.0,0.0,6000,0.1)==[9.0],\"failed: multi-feature ridge (ratio 0)\"\nassert elastic_net_regression([[1],[2],[3]],[2,4,6],[[4]],1,0.0,0.5,0,0.1)==-1,\"failed: n_iterations validation\"\nassert elastic_net_regression([[1,2]], [3], [[1]], 1,0.0,0.5,10,0.1)==-1,\"failed: dimension mismatch\"\nassert len(elastic_net_regression([[1],[2],[3],[4]],[1,2,3,4],[[5],[6]],1,0.0,0.5,5000,0.1))==2,\"failed: output length\"\nassert all(isinstance(v,float) for v in elastic_net_regression([[1],[2]], [2,4], [[3]], 1, 0.0,0.5,4000,0.1)),\"failed: output contains non-float values\""}
{"task_id": 217, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    Args:\n        actual    : 1-D container (list/tuple/np.ndarray) with elements +1 or \u22121.\n        predicted : 1-D container (list/tuple/np.ndarray) with raw model scores.\n\n    Returns:\n        (grad_list, hess_list, prob_list) \u2013 three ordinary Python lists,\n        each value rounded to exactly 6 decimal places.\n    \"\"\"\n    y = np.asarray(actual, dtype=float)\n    z = np.asarray(predicted, dtype=float)\n    if y.shape != z.shape:\n        raise ValueError('`actual` and `predicted` must have the same length.')\n\n    def _sigmoid(x):\n        out = np.empty_like(x, dtype=float)\n        pos_mask = x >= 0\n        neg_mask = ~pos_mask\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n        exp_x = np.exp(x[neg_mask])\n        out[neg_mask] = exp_x / (1.0 + exp_x)\n        return out\n    prob = _sigmoid(z)\n    grad = y * _sigmoid(-y * z)\n    hess = prob * (1.0 - prob)\n    prob = np.round(prob, 6).tolist()\n    grad = np.round(grad, 6).tolist()\n    hess = np.round(hess, 6).tolist()\n    return (grad, hess, prob)\nassert logistic_components([1, -1], [0.5, -0.5]) == ([0.377541, -0.377541], [0.235004, 0.235004], [0.622459, 0.377541]), \"failed on ([1, -1], [0.5, -0.5])\"\nassert logistic_components([1, -1], [10, -10]) == ([0.000045, -0.000045], [0.000045, 0.000045], [0.999955, 0.000045]), \"failed on large magnitude scores\"\nassert logistic_components([1, 1, 1], [0, 0, 0]) == ([0.5, 0.5, 0.5], [0.25, 0.25, 0.25], [0.5, 0.5, 0.5]), \"failed on zeros with positive labels\"\nassert logistic_components([-1, -1, -1], [0, 0, 0]) == ([-0.5, -0.5, -0.5], [0.25, 0.25, 0.25], [0.5, 0.5, 0.5]), \"failed on zeros with negative labels\"\nassert logistic_components([1], [-2]) == ([0.880797], [0.104994], [0.119203]), \"failed on single sample (1, -2)\"\nassert logistic_components([-1], [2]) == ([-0.880797], [0.104994], [0.880797]), \"failed on single sample (-1, 2)\"\nassert logistic_components([1, -1, 1, -1], [1, 1, -1, -1]) == ([0.268941, -0.731059, 0.731059, -0.268941], [0.196612, 0.196612, 0.196612, 0.196612], [0.731059, 0.731059, 0.268941, 0.268941]), \"failed on mixed signs\"\nassert logistic_components([1], [0]) == ([0.5], [0.25], [0.5]), \"failed on ([1], [0])\"\nassert logistic_components([1, -1], [5, -5]) == ([0.006693, -0.006693], [0.006648, 0.006648], [0.993307, 0.006693]), \"failed on moderate magnitude scores\"\nassert logistic_components([-1], [3]) == ([-0.952574], [0.045177], [0.952574]), \"failed on (-1, 3)\""}
{"task_id": 218, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n\n    def hand_score(hand: list[int]) -> int:\n        \"\"\"Return the best score (\u226421 if possible) for this hand.\"\"\"\n        s = sum(hand)\n        if 1 in hand and s + 10 <= 21:\n            return s + 10\n        return s\n\n    def is_bust(hand: list[int]) -> bool:\n        \"\"\"True if the hand is bust (>21).\"\"\"\n        return hand_score(hand) > 21\n\n    def is_natural(hand: list[int]) -> bool:\n        \"\"\"True if the hand is a natural blackjack (exactly two cards: Ace + 10).\"\"\"\n        return len(hand) == 2 and 1 in hand and (10 in hand)\n    player_score = hand_score(player)\n    dealer_score = hand_score(dealer)\n    player_bust = player_score > 21\n    dealer_bust = dealer_score > 21\n    player_natural = is_natural(player)\n    if player_bust:\n        return -1.0\n    if dealer_bust:\n        return 1.5 if player_natural else 1.0\n    if player_score > dealer_score:\n        return 1.5 if player_natural else 1.0\n    if player_score < dealer_score:\n        return -1.0\n    return 0.0\nassert blackjack_outcome([1,10],[10,7]) == 1.5, \"Failed on natural blackjack win [1,10] vs [10,7]\"\nassert blackjack_outcome([1,10],[1,10]) == 0.0, \"Failed on natural blackjack draw [1,10] vs [1,10]\"\nassert blackjack_outcome([10,6],[9,7]) == 0.0, \"Failed on simple draw 16 vs 16\"\nassert blackjack_outcome([10,6],[9,8]) == -1.0, \"Failed on player loses 16 vs 17\"\nassert blackjack_outcome([10,5,9],[10,6]) == -1.0, \"Failed on player bust 24 vs 16\"\nassert blackjack_outcome([10,7],[10,6,9]) == 1.0, \"Failed on dealer bust 17 vs 25\"\nassert blackjack_outcome([1,7,3],[10,10]) == 1.0, \"Failed on soft 21 win\"\nassert blackjack_outcome([1,5,5,10],[10,9,2]) == 0.0, \"Failed on 21 draw\"\nassert blackjack_outcome([10,2],[10,10,5]) == 1.0, \"Failed on dealer bust scenario\"\nassert blackjack_outcome([9,9,9],[1,9]) == -1.0, \"Failed on large bust 27 vs 20\""}
{"task_id": 221, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef squared_log_error(actual, predicted):\n    \"\"\"\n    Element-wise squared logarithmic error.\n\n    Parameters\n    ----------\n    actual, predicted : np.ndarray\n        Arrays of the same shape containing non-negative numbers.\n\n    Returns\n    -------\n    np.ndarray\n        (log1p(actual) - log1p(predicted))**2 computed element-wise.\n    \"\"\"\n    return np.square(np.log1p(actual) - np.log1p(predicted))\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        True target values (must be non-negative).\n    predicted : list | tuple | np.ndarray\n        Predicted values (must be non-negative and same shape as `actual`).\n\n    Returns\n    -------\n    float\n        MSLE rounded to 4 decimal places, or -1 if the inputs are invalid.\n    \"\"\"\n    try:\n        actual_arr = np.asarray(actual, dtype=float)\n        pred_arr = np.asarray(predicted, dtype=float)\n    except Exception:\n        return -1\n    if actual_arr.shape != pred_arr.shape:\n        return -1\n    if actual_arr.size == 0:\n        return -1\n    if np.isnan(actual_arr).any() or np.isnan(pred_arr).any():\n        return -1\n    if np.any(actual_arr < 0) or np.any(pred_arr < 0):\n        return -1\n    sq_log_err = squared_log_error(actual_arr, pred_arr)\n    msle = float(np.mean(sq_log_err))\n    return round(msle, 4)\nassert mean_squared_log_error([3,5],[2.5,5.5])==0.0121, \"failed: example ([3,5],[2.5,5.5])\"\nassert mean_squared_log_error([0,1,2],[0,1,2])==0.0, \"failed: perfect prediction\"\nassert mean_squared_log_error([0],[0])==0.0, \"failed: single zero\"\nassert mean_squared_log_error([9,10],[8,8])==0.0257, \"failed: higher values\"\nassert mean_squared_log_error([1,2],[-1,2])==-1, \"failed: negative prediction\"\nassert mean_squared_log_error([1,2,3],[1,2])==-1, \"failed: length mismatch\"\nassert mean_squared_log_error([1000],[1000])==0.0, \"failed: large identical values\"\nassert mean_squared_log_error([5,15,25],[7,14,29])==0.0358, \"failed: mixed values\""}
{"task_id": 222, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function works for scalar numbers, Python lists and NumPy arrays and\n    remains numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: If `z` is a scalar, the sigmoid value rounded to 4\n        decimals (float).  Otherwise, a Python list with the same nested\n        structure where every element is rounded to 4 decimals.\n    \"\"\"\n    if np.isscalar(z):\n        z = float(z)\n        if z >= 0:\n            val = 1.0 / (1.0 + np.exp(-z))\n        else:\n            exp_z = np.exp(z)\n            val = exp_z / (1.0 + exp_z)\n        return round(val, 4)\n    arr = np.asarray(z, dtype=float)\n    out = np.empty_like(arr)\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    exp_vals = np.exp(arr[neg_mask])\n    out[neg_mask] = exp_vals / (1.0 + exp_vals)\n    return np.round(out, 4).tolist()\nassert sigmoid(0) == 0.5, \"failed on sigmoid(0)\"\nassert sigmoid(1) == 0.7311, \"failed on sigmoid(1)\"\nassert sigmoid(-1) == 0.2689, \"failed on sigmoid(-1)\"\nassert sigmoid(5) == 0.9933, \"failed on sigmoid(5)\"\nassert sigmoid(-5) == 0.0067, \"failed on sigmoid(-5)\"\nassert sigmoid(1000) == 1.0, \"failed on large positive input\"\nassert sigmoid(-1000) == 0.0, \"failed on large negative input\"\nassert sigmoid([0, 1, -1]) == [0.5, 0.7311, 0.2689], \"failed on list input\"\nassert sigmoid([[0, -2], [2, 0]]) == [[0.5, 0.1192], [0.8808, 0.5]], \"failed on 2D input\""}
{"task_id": 224, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky-ReLU activation to every element in *z*.\n\n    Args:\n        z:  A scalar, (nested) list or ``numpy.ndarray`` of numbers.\n        a:  Slope used for the negative part, a float in the half-open\n            interval [0, 1).  Defaults to 0.01.\n\n    Returns\n    -------\n    Same \u201cshape\u201d as *z* but converted to plain Python objects\n    (i.e. lists / scalars).  For array or list inputs the function\n    returns a (possibly nested) list; for a scalar input it returns a\n    scalar.\n    \"\"\"\n    if not 0.0 <= a < 1.0:\n        raise ValueError(\"Parameter 'a' must lie in the interval [0, 1).\")\n    arr = np.asarray(z)\n    activated = np.where(arr >= 0, arr, a * arr)\n    return activated.tolist()\nassert leaky_relu([-2, -1, 0, 1, 2]) == [-0.02, -0.01, 0.0, 1.0, 2.0], \"failed on default slope, 1-D list\"\nassert leaky_relu(np.array([-5.0, 5.0]), 0.2) == [-1.0, 5.0], \"failed on numpy input, custom slope\"\nassert leaky_relu(np.array([[[-1]]]), 0.5) == [[[-0.5]]], \"failed on 3-D array\"\nassert leaky_relu(0) == 0.0, \"failed on scalar zero\"\nassert leaky_relu(-4, 0.25) == -1.0, \"failed on scalar negative\"\nassert leaky_relu(3.3) == 3.3, \"failed on scalar positive\"\nassert leaky_relu([[0]]) == [[0.0]], \"failed on zero inside nested list\""}
{"task_id": 226, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef _stump_predict(column: np.ndarray, threshold: float, polarity: int) -> np.ndarray:\n    \"\"\"\n    Helper \u2013 returns the predictions of one decision stump for a single feature column.\n    \"\"\"\n    if polarity == 1:\n        return np.where(column < threshold, -1, 1)\n    else:\n        return np.where(column < threshold, 1, -1)\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\"\"\"\n    (n_samples, n_features) = X_train.shape\n    w = np.full(n_samples, 1.0 / n_samples)\n    ensemble: list[tuple[int, float, int, float]] = []\n    for _ in range(n_clf):\n        best_feat = 0\n        best_thresh = 0.0\n        best_polarity = 1\n        best_error = float('inf')\n        best_pred_train = None\n        for feat_idx in range(n_features):\n            col = X_train[:, feat_idx]\n            thresholds = np.unique(col)\n            for thr in thresholds:\n                pred = _stump_predict(col, thr, 1)\n                err = np.sum(w[pred != y_train])\n                if err < best_error:\n                    (best_error, best_feat, best_thresh, best_polarity) = (err, feat_idx, thr, 1)\n                    best_pred_train = pred\n                pred = _stump_predict(col, thr, -1)\n                err = np.sum(w[pred != y_train])\n                if err < best_error:\n                    (best_error, best_feat, best_thresh, best_polarity) = (err, feat_idx, thr, -1)\n                    best_pred_train = pred\n        eps = 1e-10\n        alpha = 0.5 * math.log((1 - best_error) / (best_error + eps))\n        w *= np.exp(-alpha * y_train * best_pred_train)\n        w /= np.sum(w)\n        ensemble.append((best_feat, best_thresh, best_polarity, alpha))\n    agg = np.zeros(X_test.shape[0])\n    for (feat_idx, thr, pol, alpha) in ensemble:\n        preds = _stump_predict(X_test[:, feat_idx], thr, pol)\n        agg += alpha * preds\n    final_preds = np.sign(agg)\n    final_preds[final_preds == 0] = 1\n    return final_preds.astype(int).tolist()\nassert adaboost_predict(np.array([[0, 1], [1, 1], [1, 0], [0, 0]]), np.array([1, 1, -1, -1]), np.array([[1, 0], [0, 1]]), 3) == [-1, 1], \"Test-1 failed\"\nassert adaboost_predict(np.array([[1], [2], [3], [4], [5]]), np.array([-1, -1, 1, 1, 1]), np.array([[2], [4]]), 5) == [-1, 1], \"Test-2 failed\"\nassert adaboost_predict(np.array([[2, 9], [3, 8], [1, 10], [5, 2], [6, 3], [4, 1]]), np.array([1, 1, 1, -1, -1, -1]), np.array([[2, 9], [6, 2]]), 4) == [1, -1], \"Test-3 failed\"\nassert adaboost_predict(np.array([[1], [2], [3], [4]]), np.array([1, 1, -1, -1]), np.array([[1], [4]]), 3) == [1, -1], \"Test-4 failed\"\nassert adaboost_predict(np.array([[0], [1], [2], [3], [4], [5]]), np.array([-1, -1, -1, 1, 1, 1]), np.array([[0], [5]]), 6) == [-1, 1], \"Test-5 failed\"\nassert adaboost_predict(np.array([[1, 2], [1, 3], [1, 4], [10, 1], [10, 2], [10, 3]]), np.array([-1, -1, -1, 1, 1, 1]), np.array([[1, 3], [10, 1]]), 5) == [-1, 1], \"Test-6 failed\"\nassert adaboost_predict(np.array([[0.1], [0.4], [0.5], [0.6], [0.9]]), np.array([-1, -1, 1, 1, 1]), np.array([[0.2], [0.8]]), 4) == [-1, 1], \"Test-7 failed\"\nassert adaboost_predict(np.array([[2], [4], [6], [8], [10], [12]]), np.array([-1, -1, -1, 1, 1, 1]), np.array([[3], [11]]), 6) == [-1, 1], \"Test-8 failed\"\nassert adaboost_predict(np.array([[1, 5], [2, 5], [3, 5], [4, 1], [5, 1], [6, 1]]), np.array([1, 1, 1, -1, -1, -1]), np.array([[2, 5], [5, 1]]), 5) == [1, -1], \"Test-9 failed\"\nassert adaboost_predict(np.array([[0], [1]]), np.array([-1, 1]), np.array([[0], [1]]), 1) == [-1, 1], \"Test-10 failed\""}
{"task_id": 241, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef orthogonal(shape: tuple[int, ...], scale: float=0.5) -> list:\n    \"\"\"Initialise a tensor with an orthogonal matrix (Saxe et al., 2014).\n\n    Args:\n        shape: Desired output shape. Must have length \u2265 2.\n        scale: Multiplicative scaling factor (default 0.5).\n\n    Returns:\n        A (possibly nested) python list whose first two dimensions are\n        orthogonal up to the given scaling factor, or \u20131 if `shape` is 1-D.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n    d0 = shape[0]\n    d_rest = int(np.prod(shape[1:], dtype=int))\n    A = np.random.standard_normal((d0, d_rest))\n    (U, _, Vt) = np.linalg.svd(A, full_matrices=False)\n    Q = U if U.shape == A.shape else Vt\n    tensor = (scale * Q.reshape(shape)).tolist()\n    return tensor\nnp.random.seed(0); assert (m := orthogonal((2, 2), 1.0)); flat = np.array(m); assert np.allclose(flat @ flat.T, np.eye(2), atol=1e-6), \"orthogonal((2,2),1.0) rows not orthonormal\"\nnp.random.seed(1); assert (m := orthogonal((3, 4), 0.1)); flat = np.array(m).reshape(3, -1); assert np.allclose(flat @ flat.T, 0.01 * np.eye(3), atol=1e-6), \"orthogonal((3,4),0.1) row-orthogonality failed\"\nnp.random.seed(2); assert (m := orthogonal((4, 3), 0.7)); flat = np.array(m).reshape(4, -1); assert np.allclose(flat.T @ flat, 0.49 * np.eye(3), atol=1e-6), \"orthogonal((4,3),0.7) column-orthogonality failed\"\nnp.random.seed(3); assert (m := orthogonal((5, 5), 0.3)); flat = np.array(m); assert np.allclose(flat @ flat.T, 0.09 * np.eye(5), atol=1e-6), \"orthogonal((5,5),0.3) failed\"\nnp.random.seed(4); assert (m := orthogonal((2, 8), 0.2)); flat = np.array(m).reshape(2, -1); assert np.allclose(flat @ flat.T, 0.04 * np.eye(2), atol=1e-6), \"orthogonal((2,8),0.2) failed\"\nnp.random.seed(5); assert (m := orthogonal((8, 2), 0.2)); flat = np.array(m).reshape(8, -1); assert np.allclose(flat.T @ flat, 0.04 * np.eye(2), atol=1e-6), \"orthogonal((8,2),0.2) failed\"\nnp.random.seed(6); assert isinstance(orthogonal((3, 3), 1.0), list), \"Return type is not list\"\nnp.random.seed(7); assert orthogonal((1,), 0.5) == -1, \"Shape length < 2 should return -1\"\nnp.random.seed(9); shape = (4, 6); scale = 0.75; m = orthogonal(shape, scale); flat = np.array(m).reshape(shape[0], -1); expected = scale**2 * np.eye(shape[0]); assert np.allclose(flat @ flat.T, expected, atol=1e-6), \"orthogonality condition failed for (4,6)\""}
{"task_id": 243, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a tiny fully-connected Actor\u2013Critic network\n    whose weights are all ones and whose biases are all zeros.\n\n    Args:\n        state: List with three floats \u2013 the environment state.\n\n    Returns:\n        (action_probabilities, state_value) where\n            action_probabilities \u2013 list with three floats, each rounded to 4 decimals\n            state_value          \u2013 single float, rounded to 4 decimals\n    \"\"\"\n    s_sum = np.sum(state)\n    z1 = max(0.0, s_sum)\n    z3 = 16.0 * z1\n    action_probs = [round(1 / 3, 4)] * 3\n    state_value = round(64.0 * z1, 4)\n    return (action_probs, state_value)\nassert actor_critic_forward([1.0, -1.0, 0.5]) == ([0.3333, 0.3333, 0.3333], 32.0), \"failed on state [1.0, -1.0, 0.5]\"\nassert actor_critic_forward([2.0, 3.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 384.0), \"failed on state [2.0, 3.0, 1.0]\"\nassert actor_critic_forward([-2.0, 0.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 0.0), \"failed on state [-2.0, 0.0, 1.0]\"\nassert actor_critic_forward([0.0, 0.0, 0.0]) == ([0.3333, 0.3333, 0.3333], 0.0), \"failed on state [0.0, 0.0, 0.0]\"\nassert actor_critic_forward([1.0, 1.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 192.0), \"failed on state [1.0, 1.0, 1.0]\"\nassert actor_critic_forward([-1.0, -1.0, -1.0]) == ([0.3333, 0.3333, 0.3333], 0.0), \"failed on state [-1.0, -1.0, -1.0]\"\nassert actor_critic_forward([4.0, -2.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 192.0), \"failed on state [4.0, -2.0, 1.0]\"\nassert actor_critic_forward([10.0, -10.0, 5.0]) == ([0.3333, 0.3333, 0.3333], 320.0), \"failed on state [10.0, -10.0, 5.0]\"\nassert actor_critic_forward([0.25, 0.25, 0.25]) == ([0.3333, 0.3333, 0.3333], 48.0), \"failed on state [0.25, 0.25, 0.25]\"\nassert actor_critic_forward([0.1, -0.2, 0.3]) == ([0.3333, 0.3333, 0.3333], 12.8), \"failed on state [0.1, -0.2, 0.3]\""}
{"task_id": 249, "completion_id": 0, "passed": false, "result": "failed: name 'params_1' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef _dense(x: np.ndarray, w: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A small helper that applies a fully-connected layer to a 1-D input vector.\n    It copes with both common weight layouts:\n        \u2022  (in_dim , out_dim)   so   x @ w\n        \u2022  (out_dim, in_dim )   so   w @ x\n    \"\"\"\n    if x.shape[-1] == w.shape[0]:\n        return x @ w + b\n    if x.shape[-1] == w.shape[1]:\n        return w @ x + b\n    raise ValueError('Weight matrix has incompatible shape.')\ndef _relu(z: np.ndarray) -> np.ndarray:\n    return np.maximum(0.0, z)\ndef _softmax(z: np.ndarray) -> np.ndarray:\n    z = z - np.max(z)\n    e = np.exp(z)\n    return e / e.sum()\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max) \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)            \u2192 state value\n    \"\"\"\n    x = np.asarray(state, dtype=np.float64)\n    W1 = np.asarray(params['W1'], dtype=np.float64)\n    b1 = np.asarray(params['b1'], dtype=np.float64)\n    W2 = np.asarray(params['W2'], dtype=np.float64)\n    b2 = np.asarray(params['b2'], dtype=np.float64)\n    W3 = np.asarray(params['W3'], dtype=np.float64)\n    b3 = np.asarray(params['b3'], dtype=np.float64)\n    Wa = np.asarray(params['Wa'], dtype=np.float64)\n    ba = np.asarray(params['ba'], dtype=np.float64)\n    Wc = np.asarray(params['Wc'], dtype=np.float64)\n    bc = np.asarray(params['bc'], dtype=np.float64)\n    x = _relu(_dense(x, W1, b1))\n    x = _relu(_dense(x, W2, b2))\n    hidden = _relu(_dense(x, W3, b3))\n    logits = _dense(hidden, Wa, ba)\n    probs = _softmax(logits)\n    value = float(_dense(hidden, Wc, bc))\n    probs_rounded = [round(float(p), 4) for p in probs]\n    value_rounded = round(value, 4)\n    return (probs_rounded, value_rounded)\nassert actor_critic_forward([1, 0], params_1) == ([0.7311, 0.2689], 1.0), \"test case failed: state=[1,0]\"\nassert actor_critic_forward([0, 1], params_1) == ([0.2689, 0.7311], 1.0), \"test case failed: state=[0,1]\"\nassert actor_critic_forward([-1, 2], params_1) == ([0.1192, 0.8808], 2.0), \"test case failed: state=[-1,2]\"\nassert actor_critic_forward([3, 3], params_1)  == ([0.5, 0.5], 6.0),       \"test case failed: state=[3,3]\"\nassert actor_critic_forward([-2, -3], params_1)== ([0.5, 0.5], 0.0),       \"test case failed: state=[-2,-3]\"\nassert actor_critic_forward([0, 0], params_2)  == ([0.8808, 0.1192], 2.0), \"test case failed: state=[0,0]\"\nassert actor_critic_forward([1, -1], params_2) == ([0.982, 0.018],  3.0), \"test case failed: state=[1,-1]\"\nassert actor_critic_forward([-1, 1], params_2) == ([0.5, 0.5],     1.0), \"test case failed: state=[-1,1]\"\nassert actor_critic_forward([-5, -5], params_2)== ([0.5, 0.5],     1.0), \"test case failed: state=[-5,-5]\"\nassert actor_critic_forward([10, 0], params_2) == ([1.0, 0.0],    12.0), \"test case failed: state=[10,0]\""}
{"task_id": 253, "completion_id": 0, "passed": false, "result": "failed: failed on value, case 1", "solution": "import numpy as np\nimport numpy as np\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w        : 1-D weight vector (list or NumPy array).\n        alpha    : Regularisation strength (non-negative float).\n        l1_ratio : Fraction of L1 component (float in [0, 1]).\n        gradient : If True, return gradient, else return penalty value.\n\n    Returns\n        float  \u2013 if *gradient* is False  (penalty value),\n        list   \u2013 if *gradient* is True   (element-wise gradient).\n    \"\"\"\n    if alpha < 0:\n        raise ValueError('alpha must be non-negative.')\n    if not 0.0 <= l1_ratio <= 1.0:\n        raise ValueError('l1_ratio must be in [0, 1].')\n    w = np.asarray(w, dtype=float).ravel()\n    if gradient:\n        grad = alpha * (l1_ratio * np.sign(w) + (1.0 - l1_ratio) * w)\n        return list(np.round(grad, 4))\n    else:\n        penalty = alpha * (l1_ratio * np.linalg.norm(w, 1) + (1.0 - l1_ratio) * 0.5 * np.dot(w, w))\n        return round(float(penalty), 4)\nassert elastic_net_regularization([1, -2, 3], 0.1, 0.5) == 0.5371, \"failed on value, case 1\"\nassert elastic_net_regularization([1, -2, 3], 0.1, 0.5, True) == [0.1, -0.15, 0.2], \"failed on grad, case 2\"\nassert elastic_net_regularization([0, 0, 0], 0.3, 0.7) == 0.0, \"failed on value, zero vector\"\nassert elastic_net_regularization([0, 0, 0], 0.3, 0.7, True) == [0.0, 0.0, 0.0], \"failed on grad, zero vector\"\nassert elastic_net_regularization([5], 1.0, 1.0) == 5.0, \"failed on value, pure L1\"\nassert elastic_net_regularization([5], 1.0, 1.0, True) == [1.0], \"failed on grad, pure L1\"\nassert elastic_net_regularization([3, 4], 0.2, 0.0) == 2.5, \"failed on value, pure L2\"\nassert elastic_net_regularization([3, 4], 0.2, 0.0, True) == [0.6, 0.8], \"failed on grad, pure L2\"\nassert elastic_net_regularization([-1, -1, -1, -1], 0.05, 0.3) == 0.1, \"failed on value, neg weights\"\nassert elastic_net_regularization([-1, -1, -1, -1], 0.05, 0.3, True) == [-0.05, -0.05, -0.05, -0.05], \"failed on grad, neg weights\""}
{"task_id": 256, "completion_id": 0, "passed": false, "result": "failed: name 'ToyLinearRNN' is not defined", "solution": "import numpy as np\nfrom copy import deepcopy\nimport numpy as np\nfrom copy import deepcopy\ndef grad_check_RNN(model, loss_func, param_name: str, n_t: int, X, epsilon: float=1e-07):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred\n    finite differences (double precision).\n\n    See the task description for a detailed specification.\n    \"\"\"\n    if param_name in ('X', 'y'):\n        return None\n    real_name = param_name\n    if param_name == 'Ba':\n        real_name = 'ba'\n    elif param_name == 'Bx':\n        real_name = 'bx'\n    if real_name not in model.parameters:\n        raise KeyError(f\"Parameter '{real_name}' not found in model.\")\n    X = np.asarray(X, dtype=np.float64)\n    param: np.ndarray = model.parameters[real_name].astype(np.float64, copy=False)\n    model.parameters[real_name] = param\n    grads = np.zeros_like(param, dtype=np.float64)\n\n    def _reset_state_if_possible():\n        if hasattr(model, 'reset_state') and callable(model.reset_state):\n            model.reset_state()\n\n    def _sequence_loss():\n        model.flush_gradients()\n        _reset_state_if_possible()\n        preds = []\n        for t in range(n_t):\n            preds.append(model.forward(X[:, :, t]))\n        return float(loss_func(preds))\n    it = np.ndindex(param.shape)\n    for idx in it:\n        original_val = param[idx]\n        param[idx] = original_val + epsilon\n        loss_plus = _sequence_loss()\n        param[idx] = original_val - epsilon\n        loss_minus = _sequence_loss()\n        grads[idx] = (loss_plus - loss_minus) / (2.0 * epsilon)\n        param[idx] = original_val\n    return grads.T.astype(np.float64)\nimport numpy as np, math, random; from copy import deepcopy; rng = np.random.default_rng(0); model = ToyLinearRNN(2, 2, rng); X = rng.normal(size=(3, 2, 3)); n_t = 3; expected = analytic_grad_P(model, n_t, X).T; numeric = grad_check_RNN(model, squared_loss, 'P', n_t, X); assert np.allclose(numeric, expected, atol=1e-5), 'test case failed: Gradient for P 2x2'\nimport numpy as np; rng = np.random.default_rng(1); model = ToyLinearRNN(3, 1, rng); X = rng.normal(size=(4, 3, 2)); expected = analytic_grad_P(model, 2, X).T; numeric = grad_check_RNN(model, squared_loss, 'P', 2, X); assert np.allclose(numeric, expected, atol=1e-5), 'test case failed: Gradient for P 3x1'\nimport numpy as np; rng = np.random.default_rng(2); model = ToyLinearRNN(1, 3, rng); X = rng.normal(size=(5, 1, 1)); expected = analytic_grad_P(model, 1, X).T; numeric = grad_check_RNN(model, squared_loss, 'P', 1, X); assert np.allclose(numeric, expected, atol=1e-5), 'test case failed: n_t = 1'\nimport numpy as np; rng = np.random.default_rng(3); model = ToyLinearRNN(2, 4, rng); X = rng.normal(size=(2, 2, 3)); expected = analytic_grad_ba(model, 3, X); numeric = grad_check_RNN(model, squared_loss, 'Ba', 3, X); assert np.allclose(numeric, expected, atol=1e-5), 'test case failed: Gradient for Ba'\nimport numpy as np; rng = np.random.default_rng(4); model = ToyLinearRNN(4, 2, rng); X = rng.normal(size=(3, 4, 4)); expected = analytic_grad_P(model, 4, X).T; numeric = grad_check_RNN(model, squared_loss, 'P', 4, X); assert np.allclose(numeric, expected, atol=1e-5), 'test case failed: Gradient for P 4x2'\nimport numpy as np; rng = np.random.default_rng(5); model = ToyLinearRNN(2, 2, rng); X = rng.normal(size=(2, 2, 2)); expected = analytic_grad_P(model, 2, X).T; numeric = grad_check_RNN(model, squared_loss, 'P', 2, X, epsilon=1e-8); assert np.allclose(numeric, expected, atol=1e-4), 'test case failed: small epsilon'\nimport numpy as np; rng = np.random.default_rng(6); model = ToyLinearRNN(1, 3, rng); X = rng.normal(size=(3, 1, 5)); expected = analytic_grad_P(model, 5, X).T; numeric = grad_check_RNN(model, squared_loss, 'P', 5, X); assert np.allclose(numeric, expected, atol=1e-5), 'test case failed: Gradient for P 1x3'\nimport numpy as np; rng = np.random.default_rng(7); model = ToyLinearRNN(3, 2, rng); X = rng.normal(size=(2, 3, 2)); expected = analytic_grad_ba(model, 2, X); numeric = grad_check_RNN(model, squared_loss, 'ba', 2, X); assert np.allclose(numeric, expected, atol=1e-5), 'test case failed: Gradient for ba'\nimport numpy as np; rng = np.random.default_rng(8); model = ToyLinearRNN(2, 2, rng); X = rng.normal(size=(2, 2, 2)); assert grad_check_RNN(model, squared_loss, 'X', 2, X) is None, 'test case failed: param X should return None'\nimport numpy as np; rng = np.random.default_rng(9); model = ToyLinearRNN(2, 2, rng); X = rng.normal(size=(2, 2, 2)); assert grad_check_RNN(model, squared_loss, 'y', 2, X) is None, 'test case failed: param y should return None'"}
{"task_id": 257, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\"\"\"\n\n    def stump_predict(X, feature, thresh, polarity):\n        \"\"\"\n        Predict with a single decision stump.\n        polarity =  1 : x < thresh  -> -1     else 1\n        polarity = -1 : x < thresh  ->  1     else -1\n        \"\"\"\n        col = X[:, feature]\n        if polarity == 1:\n            return np.where(col < thresh, -1, 1)\n        else:\n            return np.where(col < thresh, 1, -1)\n    n_clf = max(1, int(n_clf))\n    (m, n_features) = X_train.shape\n    w = np.full(m, 1.0 / m)\n    stumps = []\n    eps = 1e-10\n    for _ in range(n_clf):\n        (best_feature, best_thresh, best_pol, min_err) = (None, None, None, np.inf)\n        for feature in range(n_features):\n            values = X_train[:, feature]\n            thresholds = np.unique(values)\n            for thresh in thresholds:\n                for polarity in (1, -1):\n                    pred = stump_predict(X_train, feature, thresh, polarity)\n                    err = np.sum(w[pred != y_train])\n                    if err < min_err:\n                        (best_feature, best_thresh, best_pol) = (feature, thresh, polarity)\n                        min_err = err\n        if min_err >= 0.5 - eps:\n            break\n        min_err = max(min_err, eps)\n        alpha = 0.5 * np.log((1 - min_err) / min_err)\n        stumps.append((best_feature, best_thresh, best_pol, alpha))\n        predictions = stump_predict(X_train, best_feature, best_thresh, best_pol)\n        w *= np.exp(-alpha * y_train * predictions)\n        w /= w.sum()\n    if not stumps:\n        return [-1] * len(X_test)\n    agg = np.zeros(X_test.shape[0])\n    for (feature, thresh, polarity, alpha) in stumps:\n        agg += alpha * stump_predict(X_test, feature, thresh, polarity)\n    return [1 if x >= 0 else -1 for x in agg]\nassert adaboost_predict(np.array([[1],[2],[3],[4]]), np.array([-1,-1,1,1]), np.array([[1.5],[3.5]]), 3) == [-1, 1], \"failed: simple threshold\"\nassert adaboost_predict(np.array([[2],[4],[6],[8]]), np.array([-1,-1,1,1]), np.array([[5],[7]]), 5) == [-1, 1], \"failed: larger n_clf\"\nassert adaboost_predict(np.array([[1,2],[2,1],[3,1],[1,3]]), np.array([1,-1,-1,1]), np.array([[2,2]]), 5)[0] in (-1,1), \"failed: prediction in allowed set\"\nassert len(adaboost_predict(np.array([[0],[1]]), np.array([-1,1]), np.array([[0],[1],[0.5]]), 2)) == 3, \"failed: output length\"\nassert adaboost_predict(np.array([[0],[1],[2]]), np.array([-1,1,-1]), np.array([[1.5]]), 3)[0] in (-1,1), \"failed: odd labels\"\nassert set(adaboost_predict(np.array([[0],[1]]), np.array([-1,1]), np.array([[0],[1]]), 2)).issubset({-1,1}), \"failed: output values range\""}
{"task_id": 261, "completion_id": 0, "passed": false, "result": "failed: `shape` must have at least two dimensions.", "solution": "import numpy as np\nimport numpy as np\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate an array initialised with Glorot/Xavier normal distribution.\n\n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired tensor shape. Must have at least two dimensions so that\n        fan-in and fan-out are well defined.\n\n    Returns\n    -------\n    np.ndarray\n        Array of the requested shape whose elements are drawn independently\n        from \ud835\udca9(0, s\u00b2) with  s = \u221a[2 / (fan_in + fan_out)].\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError('`shape` must have at least two dimensions.')\n    if len(shape) == 2:\n        (fan_in, fan_out) = shape\n    else:\n        receptive_field = int(np.prod(shape[2:], dtype=int))\n        fan_in = shape[1] * receptive_field\n        fan_out = shape[0] * receptive_field\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n    return np.random.normal(loc=0.0, scale=std, size=shape).astype(float)\nnp.random.seed(1)\nshape=(64,32)\nW=glorot_normal(shape)\nassert W.shape==shape,\"failed shape (64,32)\"\nnp.random.seed(2)\nshape=(16,3,3,3)\nW=glorot_normal(shape)\nassert W.shape==shape,\"failed shape (16,3,3,3)\"\nnp.random.seed(3)\nshape=(5,5)\nW=glorot_normal(shape)\nassert abs(W.mean())<0.2,\"mean far from 0 for shape (5,5)\"\nnp.random.seed(4)\nshape=(8,8,1,1)\nW=glorot_normal(shape)\nfan_in=8*1*1\nfan_out=8*1*1\nexp_std=np.sqrt(2/(fan_in+fan_out))\nassert abs(W.std()-exp_std)<0.1,\"std incorrect for shape (8,8,1,1)\"\nnp.random.seed(5)\nshape=(1,1,3,3)\nW=glorot_normal(shape)\nassert W.shape==shape,\"failed shape (1,1,3,3)\"\nnp.random.seed(6)\nshape=(10,)\nW=glorot_normal(shape)\nassert W.shape==shape,\"failed shape (10,)\"\nnp.random.seed(7)\nshape=(12,6,2,2)\nW=glorot_normal(shape)\nassert abs(W.mean())<0.1,\"mean far from 0 for shape (12,6,2,2)\"\nnp.random.seed(8)\nshape=(4,4,4)\nW=glorot_normal(shape)\nassert W.shape==shape,\"failed shape (4,4,4)\"\nnp.random.seed(9)\nshape=(128,256)\nW=glorot_normal(shape)\nassert abs(W.std()-np.sqrt(2/(128+256)))<0.05,\"std incorrect for shape (128,256)\""}
{"task_id": 266, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import Any, List, Tuple\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples ``(u, v)`` describing undirected edges that connect\n        vertices ``u`` and ``v``.\n        \u2022 Multiple occurrences of the same edge may be present.  \n        \u2022 Vertices that do not occur in ``V`` are ignored.  \n        \u2022 Self-loops (u == v) are ignored.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list ``G`` where ``G[i]`` contains all vertices adjacent to ``V[i]``.\n        \u2022 The outer list is in the same order as ``V``.  \n        \u2022 Inner lists contain *no duplicates* and are ordered by the vertices\u2019\n          order of appearance in ``V``.\n    \"\"\"\n    index = {vertex: i for (i, vertex) in enumerate(V)}\n    neighbours: List[set] = [set() for _ in V]\n    for (u, v) in E:\n        if u not in index or v not in index:\n            continue\n        if u == v:\n            continue\n        (ui, vi) = (index[u], index[v])\n        neighbours[ui].add(v)\n        neighbours[vi].add(u)\n    G: List[List[Any]] = []\n    for nb_set in neighbours:\n        ordered_nb = sorted(nb_set, key=index.get)\n        G.append(ordered_nb)\n    return G\nassert build_adj_list([], []) == [], \"failed: empty graph\"\nassert build_adj_list([0], []) == [[]], \"failed: single isolated vertex\"\nassert build_adj_list([0,1], [(0,1)]) == [[1],[0]], \"failed: simple two-vertex edge\"\nassert build_adj_list([0,1,2], [(0,1),(1,2)]) == [[1],[0,2],[1]], \"failed: 3-line path\"\nassert build_adj_list([\"A\",\"B\",\"C\",\"D\"], [(\"A\",\"B\"),(\"B\",\"C\"),(\"C\",\"A\"),(\"B\",\"A\")]) == [[\"B\",\"C\"],[\"A\",\"C\"],[\"A\",\"B\"],[]], \"failed: example with duplicate/reversed edges\"\nassert build_adj_list([\"x\",\"y\",\"z\"], [(\"x\",\"x\"),(\"x\",\"y\")]) == [[\"y\"],[\"x\"],[]], \"failed: self loop ignored\"\nassert build_adj_list([0,1,2], [(0,3),(3,4)]) == [[ ], [ ], [ ]], \"failed: edges with unknown vertices ignored\"\nassert build_adj_list([\"A\",\"B\",\"C\"], []) == [[],[],[]], \"failed: all isolated vertices\""}
{"task_id": 267, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Best stump (see task description).\n    \"\"\"\n    y = y.ravel().astype(int)\n    w = sample_weights.ravel().astype(float)\n    (n_samples, n_features) = X.shape\n    best_feature = -1\n    best_threshold = 0.0\n    best_polarity = 1\n    best_error = np.inf\n    for j in range(n_features):\n        sorted_idx = np.argsort(X[:, j])\n        f_vals = X[sorted_idx, j]\n        lbls = y[sorted_idx]\n        ws = w[sorted_idx]\n        pos_total = ws[lbls == 1].sum()\n        neg_total = ws[lbls == -1].sum()\n        cum_pos = 0.0\n        cum_neg = 0.0\n        i = 0\n        n = n_samples\n        while i < n:\n            theta = f_vals[i]\n            err_p_1 = cum_neg + (pos_total - cum_pos)\n            err_p_neg1 = cum_pos + (neg_total - cum_neg)\n            if err_p_1 < best_error - 1e-12:\n                best_error = err_p_1\n                best_feature = j\n                best_threshold = float(theta)\n                best_polarity = 1\n            if err_p_neg1 < best_error - 1e-12:\n                best_error = err_p_neg1\n                best_feature = j\n                best_threshold = float(theta)\n                best_polarity = -1\n            while i < n and f_vals[i] == theta:\n                if lbls[i] == 1:\n                    cum_pos += ws[i]\n                else:\n                    cum_neg += ws[i]\n                i += 1\n    return {'feature_index': int(best_feature), 'threshold': round(best_threshold, 4), 'polarity': int(best_polarity), 'weighted_error': round(best_error, 4)}\nassert train_decision_stump(np.array([[1],[1],[1]]), np.array([1,-1,1]), np.array([0.1,0.2,0.7]))[\"weighted_error\"] == 0.2, \"test case 5 failed\"\nassert train_decision_stump(np.array([[0],[1]]), np.array([-1,1]), np.array([0.8,0.2]))[\"threshold\"] in (0.0,1.0), \"test case 6 failed\"\nassert train_decision_stump(np.array([[2,0],[0,2],[2,2],[0,0]]), np.array([1,-1,1,-1]), np.ones(4))[\"weighted_error\"] == 0.0, \"test case 7 failed\"\nassert train_decision_stump(np.array([[5],[6],[7]]), np.array([1,1,1]), np.array([0.3,0.3,0.4]))[\"weighted_error\"] == 0.0, \"test case 8 failed\"\nassert train_decision_stump(np.array([[0],[1],[2],[3]]), np.array([1,-1,1,-1]), np.array([0.25,0.25,0.25,0.25]))[\"polarity\"] in (1,-1), \"test case 9 failed\"\nassert train_decision_stump(np.array([[2,3],[2,2],[2,1]]), np.array([-1,1,1]), np.ones(3))[\"feature_index\"] == 1, \"test case 10 failed\""}
{"task_id": 273, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float (rounded to 4 decimals) or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    n = len(feature)\n    if n <= 1:\n        return (None, 0.0)\n\n    def entropy(counter):\n        total = sum(counter.values())\n        if total == 0:\n            return 0.0\n        h = 0.0\n        for cnt in counter.values():\n            p = cnt / total\n            h -= p * math.log2(p)\n        return h\n    pairs = sorted(zip(feature, target), key=lambda x: x[0])\n    sorted_feat = [p[0] for p in pairs]\n    sorted_label = [p[1] for p in pairs]\n    if sorted_feat[0] == sorted_feat[-1]:\n        return (None, 0.0)\n    right_counts = Counter(sorted_label)\n    left_counts = Counter()\n    parent_entropy = entropy(right_counts)\n    best_gain = 0.0\n    best_threshold = None\n    eps = 1e-12\n    for i in range(n - 1):\n        lbl = sorted_label[i]\n        left_counts[lbl] += 1\n        right_counts[lbl] -= 1\n        if right_counts[lbl] == 0:\n            del right_counts[lbl]\n        if sorted_feat[i] != sorted_feat[i + 1]:\n            thr = (sorted_feat[i] + sorted_feat[i + 1]) / 2.0\n            n_left = i + 1\n            n_right = n - n_left\n            gain = parent_entropy - n_left / n * entropy(left_counts) - n_right / n * entropy(right_counts)\n            if gain > best_gain + eps:\n                (best_gain, best_threshold) = (gain, thr)\n            elif abs(gain - best_gain) <= eps and best_gain > 0.0:\n                best_threshold = min(best_threshold, thr)\n    if best_threshold is None:\n        return (None, 0.0)\n    return (round(best_threshold, 4), round(best_gain, 4))\nassert best_split([2, 3, 10, 19], [0, 0, 1, 1]) == (6.5, 1.0), \"failed on perfect 2-class separation\"\nassert best_split([1, 2, 3, 4], [0, 0, 0, 1]) == (3.5, 0.8113), \"failed on unbalanced classes\"\nassert best_split([1, 2, 3, 4], [0, 0, 0, 0]) == (None, 0.0), \"failed on pure node\"\nassert best_split([1, 2, 3, 4], [0, 1, 0, 1]) == (1.5, 0.3113), \"failed on symmetric classes\"\nassert best_split([1, 2, 5, 6], [0, 0, 1, 1]) == (3.5, 1.0), \"failed on separated clusters\"\nassert best_split([10, 20, 30], [0, 1, 1]) == (15.0, 0.9183), \"failed on small dataset\"\nassert best_split([1, 2], [0, 1]) == (1.5, 1.0), \"failed on two-point perfect split\"\nassert best_split([1, 2], [0, 0]) == (None, 0.0), \"failed on two identical labels\"\nassert best_split([1, 2, 3, 4, 5], [0, 0, 1, 1, 1]) == (2.5, 0.971), \"failed on 5-point uneven split\"\nassert best_split([3, 3, 3, 3], [0, 1, 0, 1]) == (None, 0.0), \"failed on identical feature values\""}
{"task_id": 286, "completion_id": 0, "passed": false, "result": "failed: Invalid initialization function.", "solution": "import random\nimport random\nfrom typing import Callable, Tuple, List\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Retrieve an initializer function by its name.\n\n    Args:\n        name: The name of the initializer to look up (e.g. \"zeros_init\").\n\n    Returns:\n        The corresponding callable weight initializer.\n\n    Raises:\n        ValueError: If `name` is not found or is not callable.\n    \"\"\"\n    obj = globals().get(name)\n    if obj is None or not callable(obj):\n        raise ValueError('Invalid initialization function.')\n    return obj\nassert get_initializer('zeros_init')((2, 2)) == [[0.0, 0.0], [0.0, 0.0]], \"test case failed: zeros_init((2,2))\"\nassert get_initializer('ones_init')((1, 3)) == [[1.0, 1.0, 1.0]], \"test case failed: ones_init((1,3))\"\nassert get_initializer('zeros_init')((3, 1)) == [[0.0], [0.0], [0.0]], \"test case failed: zeros_init((3,1))\"\nassert get_initializer('ones_init')((2, 4)) == [[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]], \"test case failed: ones_init((2,4))\"\n_matrix = get_initializer('random_uniform_init')((2, 3)); assert len(_matrix) == 2 and all(len(r) == 3 for r in _matrix), \"test case failed: random_uniform_init shape (2,3)\"\n_matrix = get_initializer('random_uniform_init')((2, 3)); assert all(0.0 <= v <= 1.0 for r in _matrix for v in r), \"test case failed: random_uniform_init value range\"\n_matrix2 = get_initializer('random_uniform_init')((3, 1)); assert len(_matrix2) == 3 and len(_matrix2[0]) == 1, \"test case failed: random_uniform_init shape (3,1)\"\nassert get_initializer('zeros_init') is zeros_init, \"test case failed: object identity for zeros_init\"\nassert get_initializer('ones_init') is ones_init, \"test case failed: object identity for ones_init\"\nassert get_initializer('random_uniform_init') is random_uniform_init, \"test case failed: object identity for random_uniform_init\""}
{"task_id": 287, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input tensor of shape (N, C, H, W).\n    pool_shape : (int, int)\n        Height and width of the pooling window  (p_h, p_w).\n    stride : (int, int)\n        Vertical and horizontal stride          (s_h, s_w).\n    accum_grad : np.ndarray\n        Gradient wrt. the pooled output. Shape must match the\n        forward-pass output.\n\n    Returns\n    -------\n    tuple[list, list]\n        (pooled_output, grad_input) \u2013 both rounded to 4 decimals and\n        converted to Python lists.\n    \"\"\"\n    (N, C, H, W) = X.shape\n    (p_h, p_w) = pool_shape\n    (s_h, s_w) = stride\n    out_h = (H - p_h) // s_h + 1\n    out_w = (W - p_w) // s_w + 1\n    pooled = np.zeros((N, C, out_h, out_w), dtype=X.dtype)\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                (h_start, h_end) = (i * s_h, i * s_h + p_h)\n                for j in range(out_w):\n                    (w_start, w_end) = (j * s_w, j * s_w + p_w)\n                    window = X[n, c, h_start:h_end, w_start:w_end]\n                    pooled[n, c, i, j] = window.mean()\n    grad_input = np.zeros_like(X, dtype=X.dtype)\n    scale = 1.0 / (p_h * p_w)\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                (h_start, h_end) = (i * s_h, i * s_h + p_h)\n                for j in range(out_w):\n                    (w_start, w_end) = (j * s_w, j * s_w + p_w)\n                    g = accum_grad[n, c, i, j] * scale\n                    grad_input[n, c, h_start:h_end, w_start:w_end] += g\n    pooled = np.round(pooled, 4).tolist()\n    grad_input = np.round(grad_input, 4).tolist()\n    return (pooled, grad_input)\nassert average_pool2d(np.array([[[[1, 2], [3, 4]]]]), (2, 2), (1, 1), np.array([[[[1]]]])) == ([[[[2.5]]]], [[[[0.25, 0.25], [0.25, 0.25]]]]), \"test case 1 failed: overlapping 2x2 window with unit gradient\"\nassert average_pool2d(np.array([[[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]]), (2, 2), (1, 1), np.ones((1, 1, 2, 2))) == ([[[[3.0, 4.0], [6.0, 7.0]]]], [[[[0.25, 0.5, 0.25], [0.5, 1.0, 0.5], [0.25, 0.5, 0.25]]]]), \"test case 2 failed: 3x3 input with overlapping windows\"\nassert average_pool2d(np.array([[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]]), (2, 2), (2, 2), np.ones((1, 2, 1, 1))) == ([[[[2.5]], [[6.5]]]], [[[[0.25, 0.25], [0.25, 0.25]], [[0.25, 0.25], [0.25, 0.25]]]]), \"test case 3 failed: 2 channels, non-overlapping windows\"\nassert average_pool2d(np.arange(32, dtype=float).reshape(2, 1, 4, 4), (2, 2), (2, 2), np.ones((2, 1, 2, 2))) == ([ [[[2.5, 4.5], [10.5, 12.5]]], [[[18.5, 20.5], [26.5, 28.5]]] ], [ [[[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]]], [[[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]]]]), \"test case 4 failed: batch size 2 with non-overlapping windows\"\nassert average_pool2d(np.array([[[[10, 20], [30, 40]]]]), (1, 1), (1, 1), np.ones((1, 1, 2, 2))) == ([[[[10.0, 20.0], [30.0, 40.0]]]], [[[[1.0, 1.0], [1.0, 1.0]]]]), \"test case 5 failed: pooling window 1x1 should be identity\"\nassert average_pool2d(np.array([[[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]]]), (2, 2), (2, 2), np.ones((1, 3, 1, 1))) == ([[[[2.5]], [[6.5]], [[10.5]]]], [[[[0.25, 0.25], [0.25, 0.25]], [[0.25, 0.25], [0.25, 0.25]], [[0.25, 0.25], [0.25, 0.25]]]]), \"test case 6 failed: three-channel input\"\nassert average_pool2d(np.array([[[[1, 2, 3], [4, 5, 6]]]]), (1, 3), (1, 3), np.ones((1, 1, 2, 1))) == ([[[[2.0], [5.0]]]], [[[[0.3333, 0.3333, 0.3333], [0.3333, 0.3333, 0.3333]]]]), \"test case 7 failed: pooling window covers full width\"\nassert average_pool2d(np.array([[[[1], [2], [3]]]]), (3, 1), (1, 1), np.ones((1, 1, 1, 1))) == ([[[[2.0]]]], [[[[0.3333], [0.3333], [0.3333]]]]), \"test case 8 failed: pooling window covers full height\"\nassert average_pool2d(np.array([[[[1, 2], [3, 4]]]]), (2, 2), (1, 1), np.array([[[[2]]]])) == ([[[[2.5]]]], [[[[0.5, 0.5], [0.5, 0.5]]]]), \"test case 9 failed: scaled gradient for single window\"\nassert average_pool2d(np.array([[[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]]), (2, 2), (1, 1), np.array([[[[1, 2], [3, 4]]]])) == ([[[[3.0, 4.0], [6.0, 7.0]]]], [[[[0.25, 0.75, 0.5], [1.0, 2.5, 1.5], [0.75, 1.75, 1.0]]]]), \"test case 10 failed: non-uniform incoming gradient\""}
{"task_id": 290, "completion_id": 0, "passed": false, "result": "failed: failed on threshold mismatch", "solution": "import numpy as np\nfrom dataclasses import dataclass\nimport numpy as np\nfrom dataclasses import dataclass\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\"\"\"\n    feature: int\n    threshold: float\n    left: object\n    right: object\n_TOL = 1e-08\ndef _values_equal(a, b):\n    \"\"\"Compare two prediction values with the required tolerance.\"\"\"\n    try:\n        return np.allclose(a, b, atol=_TOL, rtol=_TOL)\n    except TypeError:\n        return a == b\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\"\"\"\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return _values_equal(tree_a.value, tree_b.value)\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        return tree_a.feature == tree_b.feature and np.isclose(tree_a.threshold, tree_b.threshold, atol=_TOL, rtol=_TOL) and compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    return False\nassert compare_trees(Node(0, 5.0, Leaf(0), Leaf(1)), Node(0, 5.0, Leaf(0), Leaf(1))) is True, \"failed on identical shallow tree\"\nassert compare_trees(Node(0, 5.0, Leaf(0), Leaf(1)), Node(1, 5.0, Leaf(0), Leaf(1))) is False, \"failed on feature mismatch\"\nassert compare_trees(Node(0, 5.0, Leaf(0), Leaf(1)), Node(0, 5.0 + 1e-9, Leaf(0), Leaf(1))) is True, \"failed on near-equal thresholds\"\nassert compare_trees(Node(0, 5.0, Leaf(0), Leaf(1)), Node(0, 5.0 + 1e-4, Leaf(0), Leaf(1))) is False, \"failed on threshold mismatch\"\nassert compare_trees(Node(0, 1.0, Leaf(0), Leaf(1)), Node(0, 1.0, Node(1, 2.0, Leaf(0), Leaf(1)), Leaf(1))) is False, \"failed on structure mismatch\"\nassert compare_trees(Leaf(42), Leaf(42)) is True, \"failed on identical scalar leaves\"\nassert compare_trees(Leaf(42), Leaf(43)) is False, \"failed on unequal scalar leaves\"\nassert compare_trees(Leaf(np.array([1.0, 0.0])), Leaf(np.array([1.0, 0.0]))) is True, \"failed on identical array leaves\"\nassert compare_trees(Leaf(np.array([1.0, 0.0])), Leaf(np.array([0.0, 1.0]))) is False, \"failed on unequal array leaves\"\nassert compare_trees(Node(0, 0.0, Node(1, 2.2, Leaf(0), Leaf(1)), Node(2, -1.3, Leaf(1), Leaf(0))), Node(0, 0.0, Node(1, 2.2, Leaf(0), Leaf(1)), Node(2, -1.3, Leaf(1), Leaf(0)))) is True, \"failed on identical deep tree\""}
{"task_id": 292, "completion_id": 0, "passed": false, "result": "failed: cutoff must be in [0, 3]", "solution": "import numpy as np\nimport numpy as np\ndef single_point_crossover(parent1: list[list[int | float]], parent2: list[list[int | float]], cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix (list of lists).\n        parent2: Second parent weight matrix (same shape as *parent1*).\n        cutoff : Column index at which crossover starts (inclusive).\n\n    Returns\n    -------\n        Tuple (child1, child2) with the two children as ordinary Python\n        lists, or -1 if the parent matrices do not have identical shapes.\n    \"\"\"\n    p1 = np.asarray(parent1, dtype=float)\n    p2 = np.asarray(parent2, dtype=float)\n    if p1.ndim != 2 or p2.ndim != 2 or p1.shape != p2.shape:\n        return -1\n    (rows, cols) = p1.shape\n    if not 0 <= cutoff <= cols:\n        raise ValueError(f'cutoff must be in [0, {cols}]')\n    child1 = p1.copy()\n    child2 = p2.copy()\n    child1[:, cutoff:] = p2[:, cutoff:]\n    child2[:, cutoff:] = p1[:, cutoff:]\n    return (child1.tolist(), child2.tolist())\n# basic 3-column example\nassert single_point_crossover([[1,2,3],[4,5,6]], [[7,8,9],[10,11,12]], 2) == ( [[1,2,9],[4,5,12]], [[7,8,3],[10,11,6]] ), \"failed: cutoff 2 on 3-col parents\"\n# cutoff at 0 \u2013 swap all columns\nassert single_point_crossover([[1,2],[3,4]], [[5,6],[7,8]], 0) == ( [[5,6],[7,8]], [[1,2],[3,4]] ), \"failed: cutoff 0\"\n# cutoff equal to number of columns \u2013 no swap\nassert single_point_crossover([[1,2,3]], [[4,5,6]], 3) == ( [[1,2,3]], [[4,5,6]] ), \"failed: cutoff == n_cols\"\n# non-square, more columns than rows\nassert single_point_crossover([[1,2,3,4]], [[5,6,7,8]], 1) == ( [[1,6,7,8]], [[5,2,3,4]] ), \"failed: 1\u00d74 matrices, cutoff 1\"\n# two-row, two-column matrices, cutoff 1\nassert single_point_crossover([[1,2],[3,4]], [[5,6],[7,8]], 1) == ( [[1,6],[3,8]], [[5,2],[7,4]] ), \"failed: 2\u00d72, cutoff 1\"\n# unequal shapes \u21d2 \u20111\nassert single_point_crossover([[1,2,3]], [[4,5]], 1) == -1, \"failed: unequal shapes must return -1\"\n# negative cutoff \u21d2 treat as 0 (swap all)\nassert single_point_crossover([[1,2,3]], [[4,5,6]], -3) == ( [[4,5,6]], [[1,2,3]] ), \"failed: negative cutoff\"\n# cutoff beyond columns \u21d2 treat as n_cols (no swap)\nassert single_point_crossover([[1],[2]], [[3],[4]], 10) == ( [[1],[2]], [[3],[4]] ), \"failed: large cutoff\"\n# float matrices\nc1, c2 = single_point_crossover([[0.1,0.2,0.3],[0.4,0.5,0.6]], [[0.7,0.8,0.9],[1.0,1.1,1.2]], 1)\nassert c1 == [[0.1,0.8,0.9],[0.4,1.1,1.2]] and c2 == [[0.7,0.2,0.3],[1.0,0.5,0.6]], \"failed: float matrices\"\n# large matrix quick sanity\nm1 = [list(range(i, i+10)) for i in range(0,100,10)]\nm2 = [list(range(i+100, i+110)) for i in range(0,100,10)]\nchild1, child2 = single_point_crossover(m1, m2, 5)\nassert child1[0][:5] == m1[0][:5] and child1[0][5:] == m2[0][5:], \"failed: larger matrix crossover integrity\""}
{"task_id": 294, "completion_id": 0, "passed": false, "result": "failed: name '_build_graph' is not defined", "solution": "from typing import Any, Dict, List, Tuple\nfrom typing import Any, Dict, List, Tuple\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert the custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph with the keys\n          * 'is_directed' : bool\n          * '_V2I'        : dict  (vertex label -> consecutive index 0..n-1)\n          * '_G'          : list  (for each vertex index a list with tuples\n                                   (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary mapping every vertex label to a list of (neighbour, weight)\n        tuples.  Lists are sorted lexicographically by the neighbour label.\n        For undirected graphs every edge appears once in each direction.\n    \"\"\"\n    if not G or not G.get('_V2I'):\n        return {}\n    directed: bool = bool(G.get('is_directed', False))\n    adj: Dict[Any, List[Tuple[Any, float]]] = {v: [] for v in G['_V2I']}\n    processed_undirected = set()\n    for edge_list in G['_G']:\n        for (u, v, w) in edge_list:\n            if directed:\n                adj[u].append((v, w))\n            else:\n                key = (u, v) if u <= v else (v, u)\n                if key in processed_undirected:\n                    continue\n                processed_undirected.add(key)\n                adj[u].append((v, w))\n                adj[v].append((u, w))\n    for neighbours in adj.values():\n        neighbours.sort(key=lambda t: str(t[0]))\n    return adj\nassert to_networkx(_build_graph(False, [('A','B',3),('A','C',2),('B','C',4)])) == {'A':[('B',3),('C',2)],'B':[('A',3),('C',4)],'C':[('A',2),('B',4)]}, \"test case failed: simple undirected graph\"\nassert to_networkx(_build_graph(True, [('A','B',1),('B','C',2),('C','A',3)])) == {'A':[('B',1)],'B':[('C',2)],'C':[('A',3)]}, \"test case failed: directed cycle\"\nassert to_networkx(_build_graph(False, [('A','B',1),('B','A',1),('B','C',2),('C','B',2)])) == {'A':[('B',1)],'B':[('A',1),('C',2)],'C':[('B',2)]}, \"test case failed: duplicate undirected edges\"\nassert to_networkx(_build_graph(True, [('A','B',1),('A','B',1),('A','B',1)])) == {'A':[('B',1)],'B':[]}, \"test case failed: repeated directed edges\"\nassert to_networkx(_build_graph(False, [('A','A',5)])) == {'A':[('A',5)]}, \"test case failed: self-loop in undirected graph\"\nassert to_networkx(_build_graph(True, [('X','Y',-3)])) == {'X':[('Y',-3)],'Y':[]}, \"test case failed: negative weight\"\nassert to_networkx(_build_graph(False, [('A','B',1),('B','C',2),('C','D',3),('D','A',4)])) == {'A':[('B',1),('D',4)],'B':[('A',1),('C',2)],'C':[('B',2),('D',3)],'D':[('A',4),('C',3)]}, \"test case failed: larger undirected graph\"\nassert to_networkx(_build_graph(False, [('A','B',1.5),('B','C',2.25)])) == {'A':[('B',1.5)],'B':[('A',1.5),('C',2.25)],'C':[('B',2.25)]}, \"test case failed: float weights\"\nassert to_networkx({'is_directed':False,'_V2I':{},'_G':[]}) == {}, \"test case failed: empty graph\""}
{"task_id": 296, "completion_id": 0, "passed": false, "result": "failed: Failed on integer vector", "solution": "import numpy as np\nimport numpy as np\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1 (or, for boolean arrays,\n        only ``True``/``False``), otherwise ``False``.\n    \"\"\"\n    arr = np.asarray(x)\n    if arr.dtype == np.bool_:\n        return True\n    return np.isin(arr, (0, 1)).all()\nassert is_binary(np.array([0, 1, 1, 0])) is True, \"Failed on integer vector\"\nassert is_binary(np.array([[0, 1], [1, 0]], dtype=bool)) is True, \"Failed on boolean matrix\"\nassert is_binary(np.array([0.0, 1.0])) is True, \"Failed on float vector\"\nassert is_binary(np.array([0, 2])) is False, \"Failed on non-binary value 2\"\nassert is_binary(np.array([-1, 0, 1])) is False, \"Failed on negative non-binary value\"\nassert is_binary(np.array([0, 1, 3])) is False, \"Failed on non-binary value 3\"\nassert is_binary(np.array([[0], [1], [0]])) is True, \"Failed on column vector\"\nassert is_binary(np.zeros((3, 4))) is True, \"Failed on all-zeros matrix\"\nassert is_binary(np.ones((2, 2))) is True, \"Failed on all-ones matrix\"\nassert is_binary(np.array([True, False, True])) is True, \"Failed on boolean vector\""}
{"task_id": 298, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nfrom collections import Counter, defaultdict\nimport math\nfrom collections import Counter\nfrom typing import List, Tuple, Dict\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Notes\n    -----\n    \u2022 White-space is used for tokenisation.\n    \u2022 For N > 1 every sentence is padded with N\u22121 '<bol>' tokens in front\n      and a single '<eol>' token at the end.  Unigrams use no padding.\n    \u2022 If any N-gram (or its (N-1)-gram prefix) required for the query is\n      unseen in *corpus*, or if the query is too short, the function\n      returns float('-inf').\n    \u2022 The final result is rounded to 4 decimal places.\n    \"\"\"\n    if N < 1:\n        raise ValueError('N must be \u2265 1')\n    if not corpus:\n        return float('-inf')\n    if N == 1:\n        unigram_counts: Counter[str] = Counter()\n        total_tokens = 0\n        for sent in corpus:\n            tokens = sent.split()\n            unigram_counts.update(tokens)\n            total_tokens += len(tokens)\n    else:\n        ngram_counts: Counter[Tuple[str, ...]] = Counter()\n        prefix_counts: Counter[Tuple[str, ...]] = Counter()\n        for sent in corpus:\n            tokens = ['<bol>'] * (N - 1) + sent.split() + ['<eol>']\n            for i in range(len(tokens) - N + 1):\n                ngram = tuple(tokens[i:i + N])\n                prefix = tuple(tokens[i:i + N - 1])\n                ngram_counts[ngram] += 1\n                prefix_counts[prefix] += 1\n    if N == 1:\n        q_tokens = query.split()\n        if not q_tokens or total_tokens == 0:\n            return float('-inf')\n    else:\n        q_tokens = ['<bol>'] * (N - 1) + query.split() + ['<eol>']\n        if len(q_tokens) < N:\n            return float('-inf')\n    log_sum = 0.0\n    if N == 1:\n        for tok in q_tokens:\n            cnt = unigram_counts.get(tok, 0)\n            if cnt == 0:\n                return float('-inf')\n            prob = cnt / total_tokens\n            log_sum += math.log(prob)\n    else:\n        for i in range(len(q_tokens) - N + 1):\n            ngram = tuple(q_tokens[i:i + N])\n            prefix = tuple(q_tokens[i:i + N - 1])\n            cnt_ng = ngram_counts.get(ngram, 0)\n            cnt_pre = prefix_counts.get(prefix, 0)\n            if cnt_ng == 0 or cnt_pre == 0:\n                return float('-inf')\n            prob = cnt_ng / cnt_pre\n            log_sum += math.log(prob)\n    return round(log_sum, 4)\nassert ngram_log_prob([\"a b a\", \"b a b\"], \"a b\", 1) == -1.3863, \"test case failed: unigram example\"\nassert ngram_log_prob([\"I love NLP\", \"I love AI\"], \"I love AI\", 2) == -0.6931, \"test case failed: bigram simple\"\nassert ngram_log_prob([\"hello world\"], \"hello world\", 2) == 0.0, \"test case failed: probability 1 for every bigram\"\nassert ngram_log_prob([\"a b c\", \"a b d\"], \"a b c\", 3) == -0.6931, \"test case failed: trigram with mixed counts\"\nassert ngram_log_prob([\"a b c\", \"a b d\"], \"a b e\", 3) == float('-inf'), \"test case failed: unseen trigram should be -inf\"\nassert ngram_log_prob([\"hello world\"], \"unknown\", 1) == float('-inf'), \"test case failed: unseen unigram should be -inf\"\nassert ngram_log_prob([\"red blue blue red\"], \"red\", 1) == -0.6931, \"test case failed: unigram single token\"\nassert ngram_log_prob([\"a a b\", \"a a a\"], \"a a\", 2) == -2.1203, \"test case failed: complex bigram counts\"\nassert ngram_log_prob([\"I love NLP\", \"I love AI\"], \"I love ML\", 2) == float('-inf'), \"test case failed: unknown bigram\"\nassert ngram_log_prob([\"cat sat\"], \"cat sat\", 2) == 0.0, \"test case failed: perfect bigram match\""}
{"task_id": 302, "completion_id": 0, "passed": false, "result": "failed: name 'X1' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample (shape = [n_samples, n_features]).\n        k: Desired number of clusters (k \u2264 n_samples).\n\n    Returns:\n        Python list of length n_samples whose entries are integers in [0, k-1].\n    \"\"\"\n    n_samples = X.shape[0]\n    diff = X[:, None, :] - X[None, :, :]\n    dist = np.linalg.norm(diff, axis=2)\n    W = 1.0 / (1.0 + dist)\n    np.fill_diagonal(W, 0.0)\n    degree = W.sum(axis=1)\n    L = np.diag(degree) - W\n    (eig_vals, eig_vecs) = np.linalg.eigh(L)\n    E = eig_vecs[:, :k]\n    centroids = E[:k].copy()\n    labels = np.full(n_samples, -1, dtype=int)\n    max_iter = 100\n    rng = np.random.default_rng()\n    for _ in range(max_iter):\n        dists = np.linalg.norm(E[:, None, :] - centroids[None, :, :], axis=2)\n        new_labels = dists.argmin(axis=1)\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n        for c in range(k):\n            mask = labels == c\n            if np.any(mask):\n                centroids[c] = E[mask].mean(axis=0)\n            else:\n                random_idx = rng.integers(0, n_samples)\n                centroids[c] = E[random_idx]\n    return labels.tolist()\nassert spectral_clustering(X1, 2) == [0, 0, 1, 1], \"test case failed: spectral_clustering(X1, 2)\"\nassert spectral_clustering(X2, 3) == [0, 1, 2, 0, 1, 2], \"test case failed: spectral_clustering(X2, 3)\"\nassert spectral_clustering(X3, 1) == [0] * 10, \"test case failed: spectral_clustering(X3, 1)\"\nassert spectral_clustering(X4, 1) == [0, 0, 0], \"test case failed: spectral_clustering(X4, 1)\"\nassert set(res5) == {0, 1} and res5.count(0) != 0 and res5.count(1) != 0, \"test case failed: spectral_clustering(X5, 2)\"\nassert set(labels6) == {0, 1, 2}, \"test case failed: spectral_clustering(X6, 3)\"\nassert set(labels7) == {0, 1}, \"test case failed: spectral_clustering(X7, 2)\"\nassert labels8 == list(range(5)), \"test case failed: spectral_clustering(X8, 5)\"\nassert spectral_clustering(X9, 2) == [0, 1], \"test case failed: spectral_clustering(X9, 2)\"\nassert set(labels10) <= {0, 1}, \"test case failed: spectral_clustering(X10, 2)\""}
{"task_id": 303, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str='') -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    prev_ix = max(ix - 1, 0)\n    (prev_mine, prev_label) = params[prev_ix]\n    (curr_mine, curr_label) = params[ix]\n    gold_prev = golds.get(prev_label, '')\n    gold_curr = golds.get(curr_label, '')\n    header = '-' * 25 + ' DEBUG ' + '-' * 25\n    footer = '-' * 23 + ' END DEBUG ' + '-' * 23\n    lines = [header, f'Mine (prev) [{prev_label}]:', f'{prev_mine}', '', f'Theirs (prev) [{prev_label}]:', f'{gold_prev}', '', f'Mine [{curr_label}]:', f'{curr_mine}', '', f'Theirs [{curr_label}]:', f'{gold_curr}{warn_str}', footer]\n    return '\\n'.join(lines)\nassert err_fmt([('output1','case1'),('output2','case2'),('output3','case3')], {'case1':'expected1','case2':'expected2','case3':'expected3'}, 1, '\\nWarning: mismatch detected.') == '------------------------- DEBUG -------------------------\\nMine (prev) [case1]:\\noutput1\\n\\nTheirs (prev) [case1]:\\nexpected1\\n\\nMine [case2]:\\noutput2\\n\\nTheirs [case2]:\\nexpected2\\nWarning: mismatch detected.\\n----------------------- END DEBUG -----------------------', \"test case failed: basic example\"\nassert err_fmt([('val','test')], {'test':'expected'}, 0) == '------------------------- DEBUG -------------------------\\nMine (prev) [test]:\\nval\\n\\nTheirs (prev) [test]:\\nexpected\\n\\nMine [test]:\\nval\\n\\nTheirs [test]:\\nexpected\\n----------------------- END DEBUG -----------------------', \"test case failed: single element\"\nassert err_fmt([('mine1','A'),('mine2','B')], {'A':'goldA','B':'goldB'}, 0, 'WARNING') == '------------------------- DEBUG -------------------------\\nMine (prev) [A]:\\nmine1\\n\\nTheirs (prev) [A]:\\ngoldA\\n\\nMine [A]:\\nmine1\\n\\nTheirs [A]:\\ngoldAWARNING\\n----------------------- END DEBUG -----------------------', \"test case failed: ix=0 with warn_str w/o newline\"\nassert err_fmt([('m1','id1'),('m2','id2'),('m3','id3'),('m4','id4')], {'id1':'g1','id2':'g2','id3':'g3','id4':'g4'}, 2) == '------------------------- DEBUG -------------------------\\nMine (prev) [id2]:\\nm2\\n\\nTheirs (prev) [id2]:\\ng2\\n\\nMine [id3]:\\nm3\\n\\nTheirs [id3]:\\ng3\\n----------------------- END DEBUG -----------------------', \"test case failed: middle element of longer list\"\nassert err_fmt([('foo','L1'),('bar','L2')], {'L1':'ref1','L2':'ref2'}, 1, '\\nNote: difference') == '------------------------- DEBUG -------------------------\\nMine (prev) [L1]:\\nfoo\\n\\nTheirs (prev) [L1]:\\nref1\\n\\nMine [L2]:\\nbar\\n\\nTheirs [L2]:\\nref2\\nNote: difference\\n----------------------- END DEBUG -----------------------', \"test case failed: warn_str starting with newline\"\nassert err_fmt([('x','key')], {'key':'y'}, 0, ' check') == '------------------------- DEBUG -------------------------\\nMine (prev) [key]:\\nx\\n\\nTheirs (prev) [key]:\\ny\\n\\nMine [key]:\\nx\\n\\nTheirs [key]:\\ny check\\n----------------------- END DEBUG -----------------------', \"test case failed: warn_str starting with space\"\nassert err_fmt([('one','first'),('two','second')], {'first':'uno','second':'dos'}, 1) == '------------------------- DEBUG -------------------------\\nMine (prev) [first]:\\none\\n\\nTheirs (prev) [first]:\\nuno\\n\\nMine [second]:\\ntwo\\n\\nTheirs [second]:\\ndos\\n----------------------- END DEBUG -----------------------', \"test case failed: two element ix=1\"\nassert err_fmt([('line1\\nline2','A'),('out2','B')], {'A':'exp\\nOk','B':'good'}, 1) == '------------------------- DEBUG -------------------------\\nMine (prev) [A]:\\nline1\\nline2\\n\\nTheirs (prev) [A]:\\nexp\\nOk\\n\\nMine [B]:\\nout2\\n\\nTheirs [B]:\\ngood\\n----------------------- END DEBUG -----------------------', \"test case failed: multiline mine/gold\"\nassert err_fmt([('a','1'),('b','2'),('c','3'),('d','4')], {'1':'A','2':'B','3':'C','4':'D'}, 3, '\\n--error--') == '------------------------- DEBUG -------------------------\\nMine (prev) [3]:\\nc\\n\\nTheirs (prev) [3]:\\nC\\n\\nMine [4]:\\nd\\n\\nTheirs [4]:\\nD\\n--error--\\n----------------------- END DEBUG -----------------------', \"test case failed: last element with warn\"\nassert err_fmt([('123','abc')], {'abc':'xyz'}, 0, '\\n!') == '------------------------- DEBUG -------------------------\\nMine (prev) [abc]:\\n123\\n\\nTheirs (prev) [abc]:\\nxyz\\n\\nMine [abc]:\\n123\\n\\nTheirs [abc]:\\nxyz\\n!\\n----------------------- END DEBUG -----------------------', \"test case failed: newline warn on single element\""}
{"task_id": 304, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nimport numpy as np\ndef bayesian_posterior_mean(X: np.ndarray, y: np.ndarray, alpha: float=1.0, beta: float=1.0, mu=0.0, V=None, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Posterior mean (MAP) of the coefficients in Bayesian linear regression.\n\n    Parameters\n    ----------\n    X : (N, M) array_like\n        Design matrix.\n    y : (N,) array_like\n        Targets.\n    alpha, beta : float\n        Hyper\u2013parameters of the inverse-gamma prior on \u03c3\u00b2 (kept for API\n        completeness \u2013 they do not enter the MAP of the weights).\n    mu : float or array_like\n        Prior mean for the weights.  Scalar values are broadcast.\n    V : None, float, 1-D or 2-D array_like\n        Prior scale.  None\u2192I, scalar\u2192s\u00b7I, 1-D\u2192diag(v), 2-D\u2192full matrix.\n    fit_intercept : bool\n        Whether to include a bias column of ones.\n\n    Returns\n    -------\n    list[float]\n        Posterior mean rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    if fit_intercept:\n        X = np.column_stack((np.ones(X.shape[0]), X))\n    (N, M) = X.shape\n    if V is None:\n        V_mat = np.eye(M)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 0:\n            V_mat = float(V_arr) * np.eye(M)\n        elif V_arr.ndim == 1:\n            if V_arr.size != M:\n                raise ValueError('Diagonal length of V does not match #features.')\n            V_mat = np.diag(V_arr)\n        elif V_arr.ndim == 2:\n            if V_arr.shape != (M, M):\n                raise ValueError('Shape of V does not match #features.')\n            V_mat = V_arr\n        else:\n            raise ValueError('Unsupported shape for V.')\n    V_inv = np.linalg.inv(V_mat)\n    if np.isscalar(mu):\n        mu_vec = np.full(M, float(mu))\n    else:\n        mu_vec = np.asarray(mu, dtype=float).ravel()\n        if mu_vec.size != M:\n            raise ValueError('Length of mu does not match #features.')\n    XtX = X.T @ X\n    Sigma_b = np.linalg.inv(V_inv + XtX)\n    rhs = V_inv @ mu_vec + X.T @ y\n    mu_b = Sigma_b @ rhs\n    return np.round(mu_b, 4).tolist()\nassert bayesian_posterior_mean(np.array([[1.0],[2.0],[3.0]]), np.array([2.0,4.0,6.0])) == [0.5, 1.6667], \"test case 1 failed\"\nassert bayesian_posterior_mean(np.array([[1.0],[2.0]]), np.array([3.0,5.0])) == [1.0, 1.6667], \"test case 2 failed\"\nassert bayesian_posterior_mean(np.array([[1.0],[1.0]]), np.array([2.0,2.0])) == [0.8, 0.8], \"test case 3 failed\"\nassert bayesian_posterior_mean(np.array([[1.0],[2.0],[3.0]]), np.array([2.0,4.0,6.0]), fit_intercept=False) == [1.8667], \"test case 4 failed\"\nassert bayesian_posterior_mean(np.array([[1.0],[2.0]]), np.array([3.0,3.0]), V=[2,2]) == [1.2632, 0.9474], \"test case 5 failed\"\nassert bayesian_posterior_mean(np.array([[1.0],[1.0]]), np.array([2.0,2.0]), mu=1.0) == [1.0, 1.0], \"test case 6 failed\"\nassert bayesian_posterior_mean(np.array([[2.0],[4.0],[6.0]]), np.array([1.0,2.0,3.0]), V=2, fit_intercept=False) == [0.4956], \"test case 7 failed\"\nassert bayesian_posterior_mean(np.array([[0.0],[1.0]]), np.array([1.0,2.0]), fit_intercept=False) == [1.0], \"test case 8 failed\"\nassert bayesian_posterior_mean(np.array([[0.0]]), np.array([0.0]), mu=10.0, fit_intercept=False) == [10.0], \"test case 9 failed\"\nassert bayesian_posterior_mean(np.array([[1.0],[2.0],[3.0],[4.0]]), np.array([1.0,2.0,3.0,4.0]), V=[1,1000]) == [0.0002, 0.9999], \"test case 10 failed\""}
{"task_id": 308, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mel_filterbank(N: int, n_filters: int=20, fs: int=44000, min_freq: int=0, max_freq: int | None=None, normalize: bool=True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    Parameters\n    ----------\n    N : int\n        DFT length (number of frequency-domain samples).\n    n_filters : int, default 20\n        Number of Mel filters to construct.\n    fs : int, default 44_000\n        Sampling frequency of the signal in Hertz.\n    min_freq : int, default 0\n        Lower frequency limit of the filterbank in Hertz.\n    max_freq : int | None, optional\n        Upper frequency limit of the filterbank in Hertz.  If *None* the\n        Nyquist frequency *fs/2* is used.\n    normalize : bool, default True\n        Whether to scale each filter so that its (Mel-domain) area equals 1.\n\n    Returns\n    -------\n    list[list[float]]\n        The filterbank matrix with shape (n_filters, N//2 + 1).  All values are\n        rounded to four decimal places.\n    \"\"\"\n\n    def hz2mel(f_hz: np.ndarray | float) -> np.ndarray | float:\n        return 2595.0 * np.log10(1.0 + np.asarray(f_hz) / 700.0)\n\n    def mel2hz(mel: np.ndarray | float) -> np.ndarray | float:\n        return 700.0 * (10 ** (np.asarray(mel) / 2595.0) - 1.0)\n    if max_freq is None:\n        max_freq = fs / 2\n    if not 0 <= min_freq < max_freq <= fs / 2:\n        raise ValueError('min_freq must be >= 0, and max_freq must not exceed fs/2 with min_freq < max_freq.')\n    (mel_min, mel_max) = (hz2mel(min_freq), hz2mel(max_freq))\n    mel_points = np.linspace(mel_min, mel_max, n_filters + 2)\n    hz_points = mel2hz(mel_points)\n    bin_freqs = np.arange(N // 2 + 1) * (fs / N)\n    F = np.zeros((n_filters, N // 2 + 1), dtype=float)\n    for i in range(n_filters):\n        (f_left, f_center, f_right) = hz_points[i:i + 3]\n        left_mask = np.logical_and(bin_freqs >= f_left, bin_freqs <= f_center)\n        F[i, left_mask] = (bin_freqs[left_mask] - f_left) / (f_center - f_left)\n        right_mask = np.logical_and(bin_freqs >= f_center, bin_freqs <= f_right)\n        F[i, right_mask] = (f_right - bin_freqs[right_mask]) / (f_right - f_center)\n        if normalize:\n            F[i, :] *= 2.0 / (f_right - f_left)\n    F_rounded = np.round(F, 4).tolist()\n    return F_rounded\nimport numpy as np\n\nfb = mel_filterbank(16, 6, 16000)\nassert (len(fb), len(fb[0])) == (6, 16//2 + 1), \"shape mismatch for mel_filterbank(16,6,16000)\"\nimport numpy as np\n\nfb = mel_filterbank(1024, 40, 22050)\nassert all(0.0 <= w <= 1.0 for row in fb for w in row), \"weights out of range in mel_filterbank(1024,40,22050)\"\nimport numpy as np\n\nfb = mel_filterbank(32, 10, 16000, min_freq=300, max_freq=6000)\nassert fb[0][0] == 0.0 and fb[-1][-1] == 0.0, \"edge bins should be zero for out-of-band frequencies\"\nimport numpy as np\n\nfb1 = mel_filterbank(512, 26, 16000)\nfb2 = mel_filterbank(512, 26, 16000)\nassert fb1 == fb2, \"function must be deterministic for identical parameters\"\nimport numpy as np\n\nfb = mel_filterbank(64, 5, 8000)\n# every filter must have non-zero values somewhere\nassert all(any(v > 0.0 for v in row) for row in fb), \"empty filter detected in mel_filterbank(64,5,8000)\"\nimport numpy as np\n\nfb = mel_filterbank(8, 3, 8000)\n# with normalization the maximum value must be below 1\nassert max(max(r) for r in fb) < 1.0, \"filters are not normalized when normalize=True\"\nimport numpy as np\n\nfb = mel_filterbank(128, 20, 44100)\nassert len({tuple(row) for row in fb}) == 20, \"filters must be distinct in mel_filterbank(128,20,44100)\""}
{"task_id": 312, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef blackman_harris(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    N = window_len - 1 if symmetric else window_len\n    (a0, a1, a2, a3) = (0.35875, 0.48829, 0.14128, 0.01168)\n    n = np.arange(window_len)\n    w = a0 - a1 * np.cos(2 * np.pi * n / N) + a2 * np.cos(4 * np.pi * n / N) - a3 * np.cos(6 * np.pi * n / N)\n    return [round(float(x), 4) for x in w]\nassert blackman_harris(5, True) == [0.0001, 0.2175, 1.0, 0.2175, 0.0001], \"failed: blackman_harris(5, True)\"\nassert blackman_harris(1, True) == [1.0], \"failed: blackman_harris(1, True)\"\nassert blackman_harris(0, True) == [], \"failed: blackman_harris(0, True)\"\nassert len(blackman_harris(128)) == 128, \"failed length: blackman_harris(128)\"\nassert blackman_harris(2, True)[0] == blackman_harris(2, True)[1], \"failed symmetry: blackman_harris(2, True)\"\nassert blackman_harris(10, True)[0] == blackman_harris(10, True)[-1], \"failed symmetry ends: blackman_harris(10, True)\""}
{"task_id": 313, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Return E[log X_t] for X ~ Dir(gamma[d, :]), rounded to 4 decimals.\"\"\"\n\n    def digamma(x):\n        \"\"\"\n        Digamma \u03c8(x) for positive real x.\n        Small arguments are stepped up with the recurrence\n            \u03c8(x) = \u03c8(x + 1) \u2212 1/x\n        until x \u2265 6, after which an asymptotic expansion is applied:\n            \u03c8(x) \u2248 ln x \u2212 1/(2x) \u2212 1/(12x\u00b2) + 1/(120x\u2074) \u2212 1/(252x\u2076)\n        The combination is accurate to better than 1e-6 for all x > 0.\n        \"\"\"\n        res = 0.0\n        x = float(x)\n        while x < 6.0:\n            res -= 1.0 / x\n            x += 1.0\n        inv = 1.0 / x\n        inv2 = inv * inv\n        inv4 = inv2 * inv2\n        inv6 = inv4 * inv2\n        res += math.log(x) - 0.5 * inv - inv2 / 12.0 + inv4 / 120.0 - inv6 / 252.0\n        return res\n    g = np.asarray(gamma, dtype=float)\n    row = g[d]\n    alpha_t = row[t]\n    row_sum = float(row.sum())\n    value = digamma(alpha_t) - digamma(row_sum)\n    return round(value, 4)\nassert expected_log_dirichlet([[1,1]],0,0) == -1.0, \"failed: gamma=[[1,1]], d=0, t=0\"\nassert expected_log_dirichlet([[2,3]],0,1) == -0.5833, \"failed: gamma=[[2,3]], d=0, t=1\"\nassert expected_log_dirichlet([[4,5,6],[1,1,1]],0,2) == -0.9682, \"failed: gamma=[[4,5,6],[1,1,1]], d=0, t=2\"\nassert expected_log_dirichlet([[4,1,1]],0,0) == -0.45, \"failed: gamma=[[4,1,1]], d=0, t=0\"\nassert expected_log_dirichlet([[3,7]],0,1) == -0.379, \"failed: gamma=[[3,7]], d=0, t=1\"\nassert expected_log_dirichlet([[10,10,10]],0,2) == -1.1327, \"failed: gamma=[[10,10,10]], d=0, t=2\"\nassert expected_log_dirichlet([[8,1,1]],0,0) == -0.2361, \"failed: gamma=[[8,1,1]], d=0, t=0\"\nassert expected_log_dirichlet([[1,1,1,1]],0,3) == -1.8333, \"failed: gamma=[[1,1,1,1]], d=0, t=3\"\nassert expected_log_dirichlet([[1,2,1]],0,1) == -0.8333, \"failed: gamma=[[1,2,1]], d=0, t=1\"\nassert expected_log_dirichlet([[2,2,2,2,2]],0,2) == -1.829, \"failed: gamma=[[2,2,2,2,2]], d=0, t=2\""}
{"task_id": 317, "completion_id": 0, "passed": false, "result": "failed: test case failed: basic two-element list, ix=1", "solution": "from typing import List, Tuple, Dict\nfrom typing import List, Tuple, Dict\ndef err_fmt(params: List[Tuple[str, str]], golds: Dict[str, str], ix: int, warn_str: str='') -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\"\"\"\n    prev_ix = max(ix - 1, 0)\n    (prev_pred, prev_label) = params[prev_ix]\n    (curr_pred, curr_label) = params[ix]\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    header = '-' * 25 + ' DEBUG ' + '-' * 25\n    footer = '-' * 23 + ' END DEBUG ' + '-' * 23\n    parts = [header, f'Mine (prev) [{prev_label}]:', prev_pred, '', f'Theirs (prev) [{prev_label}]:', prev_gold, '', '', f'Mine [{curr_label}]:', curr_pred, '', f'Theirs [{curr_label}]:', f'{curr_gold}{warn_str}', footer]\n    return '\\n'.join(parts)\nassert err_fmt([(\"cat\",\"A\"),(\"dog\",\"B\")],{\"A\":\"feline\",\"B\":\"canine\"},1) == \"------------------------- DEBUG -------------------------\\nMine (prev) [A]:\\ncat\\n\\nTheirs (prev) [A]:\\nfeline\\n\\nMine [B]:\\ndog\\n\\nTheirs [B]:\\ncanine\\n----------------------- END DEBUG -----------------------\", \"test case failed: basic two-element list, ix=1\"\nassert err_fmt([(\"hi\",\"g\")],{\"g\":\"hello\"},0) == \"------------------------- DEBUG -------------------------\\nMine (prev) [g]:\\nhi\\n\\nTheirs (prev) [g]:\\nhello\\n\\nMine [g]:\\nhi\\n\\nTheirs [g]:\\nhello\\n----------------------- END DEBUG -----------------------\", \"test case failed: single element\"\nassert err_fmt([(\"v1\",\"L1\"),(\"v2\",\"L2\"),(\"v3\",\"L3\")],{\"L1\":\"t1\",\"L2\":\"t2\",\"L3\":\"t3\"},2,\" -- mismatch\") == \"------------------------- DEBUG -------------------------\\nMine (prev) [L2]:\\nv2\\n\\nTheirs (prev) [L2]:\\nt2\\n\\nMine [L3]:\\nv3\\n\\nTheirs [L3]:\\nt3 -- mismatch\\n----------------------- END DEBUG -----------------------\", \"test case failed: three elements, last index\"\nassert err_fmt([(\"v1\",\"L1\"),(\"v2\",\"L2\"),(\"v3\",\"L3\")],{\"L1\":\"t1\",\"L2\":\"t2\",\"L3\":\"t3\"},1) == \"------------------------- DEBUG -------------------------\\nMine (prev) [L1]:\\nv1\\n\\nTheirs (prev) [L1]:\\nt1\\n\\nMine [L2]:\\nv2\\n\\nTheirs [L2]:\\nt2\\n----------------------- END DEBUG -----------------------\", \"test case failed: middle index\"\nassert err_fmt([(\"x\",\"X\"),(\"y\",\"Y\")],{\"X\":\"alpha\",\"Y\":\"beta\"},0) == \"------------------------- DEBUG -------------------------\\nMine (prev) [X]:\\nx\\n\\nTheirs (prev) [X]:\\nalpha\\n\\nMine [X]:\\nx\\n\\nTheirs [X]:\\nalpha\\n----------------------- END DEBUG -----------------------\", \"test case failed: duplicate first idx\"\nassert err_fmt([(\"p\",\"a\"),(\"q\",\"b\")],{\"a\":\"A\",\"b\":\"B\"},1,\" !!!\") == \"------------------------- DEBUG -------------------------\\nMine (prev) [a]:\\np\\n\\nTheirs (prev) [a]:\\nA\\n\\nMine [b]:\\nq\\n\\nTheirs [b]:\\nB !!!\\n----------------------- END DEBUG -----------------------\", \"test case failed: warn_str appended\"\nassert err_fmt([(\"first\",\"1\"),(\"second\",\"2\"),(\"third\",\"3\")],{\"1\":\"I\",\"2\":\"II\",\"3\":\"III\"},0) == \"------------------------- DEBUG -------------------------\\nMine (prev) [1]:\\nfirst\\n\\nTheirs (prev) [1]:\\nI\\n\\nMine [1]:\\nfirst\\n\\nTheirs [1]:\\nI\\n----------------------- END DEBUG -----------------------\", \"test case failed: multi-element, ix=0\"\nassert err_fmt([(\"A\",\"A\"),(\"B\",\"B\"),(\"C\",\"C\"),(\"D\",\"D\")],{\"A\":\"a\",\"B\":\"b\",\"C\":\"c\",\"D\":\"d\"},3) == \"------------------------- DEBUG -------------------------\\nMine (prev) [C]:\\nC\\n\\nTheirs (prev) [C]:\\nc\\n\\nMine [D]:\\nD\\n\\nTheirs [D]:\\nd\\n----------------------- END DEBUG -----------------------\", \"test case failed: four elements, last index\"\nassert err_fmt([(\"only\",\"one\")],{\"one\":\"1\"},0,\" <end>\") == \"------------------------- DEBUG -------------------------\\nMine (prev) [one]:\\nonly\\n\\nTheirs (prev) [one]:\\n1\\n\\nMine [one]:\\nonly\\n\\nTheirs [one]:\\n1 <end>\\n----------------------- END DEBUG -----------------------\", \"test case failed: single element with warn_str\""}
{"task_id": 318, "completion_id": 0, "passed": false, "result": "failed: failed on horizontal vs vertical split", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef _sign(z):\n    \"\"\"Return -1 for z < 0, +1 otherwise (i.e. sign(0)=+1).\"\"\"\n    return np.where(z < 0, -1, 1)\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples with labels \u20131 or +1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (\u20131 or +1) for each sample in X_test.\n    \"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    X_test = np.asarray(X_test, dtype=float)\n    (n_samples, n_features) = X_train.shape\n    w = np.full(n_samples, 1.0 / n_samples)\n    stumps = []\n    for _ in range(n_estimators):\n        best_error = float('inf')\n        best_j = best_t = best_p = None\n        for j in range(n_features):\n            xj = X_train[:, j]\n            idx = np.argsort(xj)\n            x_sorted = xj[idx]\n            y_sorted = y_train[idx]\n            w_sorted = w[idx]\n            pos_mask = y_sorted == 1\n            neg_mask = ~pos_mask\n            pos_cumsum = np.cumsum(w_sorted * pos_mask)\n            neg_cumsum = np.cumsum(w_sorted * neg_mask)\n            pos_total = pos_cumsum[-1]\n            neg_total = neg_cumsum[-1]\n            pos_left = np.concatenate(([0.0], pos_cumsum))\n            neg_left = np.concatenate(([0.0], neg_cumsum))\n            err_p1 = pos_left + (neg_total - neg_left)\n            err_pn1 = neg_left + (pos_total - pos_left)\n            for (polarity, errors) in ((1, err_p1), (-1, err_pn1)):\n                k = int(np.argmin(errors))\n                err = errors[k]\n                if err < best_error - 1e-12:\n                    best_error = err\n                    best_j = j\n                    best_p = polarity\n                    if k == 0:\n                        best_t = x_sorted[0] - 1e-10\n                    elif k == len(x_sorted):\n                        best_t = x_sorted[-1] + 1e-10\n                    else:\n                        best_t = 0.5 * (x_sorted[k - 1] + x_sorted[k])\n        eps = max(best_error, 1e-10)\n        alpha = 0.5 * math.log((1.0 - eps) / eps)\n        preds = best_p * _sign(X_train[:, best_j] - best_t)\n        w *= np.exp(-alpha * y_train * preds)\n        w /= w.sum()\n        stumps.append((best_j, best_t, best_p, alpha))\n    agg = np.zeros(X_test.shape[0])\n    for (j, t, p, alpha) in stumps:\n        agg += alpha * p * _sign(X_test[:, j] - t)\n    y_pred = _sign(agg).astype(int)\n    return y_pred.tolist()\nassert adaboost_predict(np.array([[-2],[-1],[-0.5],[0.5],[1],[2]]), np.array([-1,-1,-1,1,1,1]), np.array([[-1.5],[-0.3],[1.5]]), 5) == [-1,-1,1], \"failed on simple 1-D split\"\nassert adaboost_predict(np.array([[1,1],[1,2],[1,3],[-1,1],[-1,2],[-1,3]]), np.array([1,1,1,-1,-1,-1]), np.array([[1,2],[-1,2],[0,1]]), 5) == [1,-1,-1], \"failed on horizontal vs vertical split\"\nassert adaboost_predict(np.array([[0,5],[0,6],[0,-5],[0,-6]]), np.array([-1,-1,1,1]), np.array([[0,10],[0,-10]]), 5) == [-1,1], \"failed on feature-2 split\"\nassert adaboost_predict(np.array([[-3],[ -2],[-1],[ 1],[ 2],[ 3]]), np.array([-1,-1,-1,1,1,1]), np.array([[-4],[-0.2],[0.2],[4]]), 3) == [-1,-1,1,1], \"failed on wider range\"\nassert adaboost_predict(np.array([[2,2],[3,3],[-2,-2],[-3,-3]]), np.array([1,1,-1,-1]), np.array([[1.5,1.5],[-1.5,-1.5]]), 10) == [1,-1], \"failed on diagonal clusters\"\nassert adaboost_predict(np.array([[1,0],[2,0],[3,0],[-1,0],[-2,0],[-3,0]]), np.array([1,1,1,-1,-1,-1]), np.array([[0.5,0],[-0.5,0]]), 4) == [1,-1], \"failed on feature-0 split\"\nassert adaboost_predict(np.array([[0,1],[0,2],[0,3],[0,-1],[0,-2],[0,-3]]), np.array([1,1,1,-1,-1,-1]), np.array([[0,0.5],[0,-0.5]]), 4) == [1,-1], \"failed on feature-1 split\"\nassert adaboost_predict(np.array([[1],[2],[3],[4],[-1],[-2],[-3],[-4]]), np.array([1,1,1,1,-1,-1,-1,-1]), np.array([[5],[-5]]), 6) == [1,-1], \"failed on extended 1-D\"\nassert adaboost_predict(np.array([[1,2],[2,3],[3,4],[-1,-2],[-2,-3],[-3,-4]]), np.array([1,1,1,-1,-1,-1]), np.array([[0,0],[4,5],[-4,-5]]), 6) == [-1,1,-1], \"failed on 2-D linear split\"\nassert adaboost_predict(np.array([[0.1],[0.2],[0.3],[-0.1],[-0.2],[-0.3]]), np.array([1,1,1,-1,-1,-1]), np.array([[0.15],[-0.15]]), 5) == [1,-1], \"failed on small decimal values\""}
{"task_id": 329, "completion_id": 0, "passed": false, "result": "failed: dft_bins() takes from 1 to 2 positional arguments but 3 were given", "solution": "import numpy as np\nimport numpy as np\ndef dft_bins(N: int, fs: int=44000, *, positive_only: bool=True) -> list[float]:\n    \"\"\"Return the centre frequency (Hz) of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        If True, return only the non-negative frequencies (indices\n        0 \u2026 \u230aN/2\u230b).  If False, return all N frequency bins in the same\n        order as `numpy.fft.fftfreq`.  Default is True.\n\n    Returns\n    -------\n    list[float]\n        Frequencies rounded to 4 decimal places.  Empty list if `N`\n        or `fs` is not strictly positive.\n    \"\"\"\n    if N <= 0 or fs <= 0:\n        return []\n    if positive_only:\n        freqs = np.arange(0, N // 2 + 1) * (fs / N)\n    else:\n        freqs = np.fft.fftfreq(N, d=1 / fs)\n    return np.round(freqs, 4).tolist()\nassert dft_bins(8, 8000, True) == [0.0, 1000.0, 2000.0, 3000.0, 4000.0], \"test case failed: dft_bins(8, 8000, True)\"\nassert dft_bins(8, 8000, False) == [0.0, 1000.0, 2000.0, 3000.0, -4000.0, -3000.0, -2000.0, -1000.0], \"test case failed: dft_bins(8, 8000, False)\"\nassert dft_bins(5, 10000, True) == [0.0, 2000.0, 4000.0], \"test case failed: dft_bins(5, 10000, True)\"\nassert dft_bins(5, 10000, False) == [0.0, 2000.0, 4000.0, -4000.0, -2000.0], \"test case failed: dft_bins(5, 10000, False)\"\nassert dft_bins(1, 48000, True) == [0.0], \"test case failed: dft_bins(1, 48000, True)\"\nassert dft_bins(2, 48000, False) == [0.0, -24000.0], \"test case failed: dft_bins(2, 48000, False)\"\nassert dft_bins(16, 16000, True) == [0.0, 1000.0, 2000.0, 3000.0, 4000.0, 5000.0, 6000.0, 7000.0, 8000.0], \"test case failed: dft_bins(16, 16000, True)\"\nassert dft_bins(16, 16000, False) == [0.0, 1000.0, 2000.0, 3000.0, 4000.0, 5000.0, 6000.0, 7000.0, -8000.0, -7000.0, -6000.0, -5000.0, -4000.0, -3000.0, -2000.0, -1000.0], \"test case failed: dft_bins(16, 16000, False)\"\nassert dft_bins(9, 9000, True) == [0.0, 1000.0, 2000.0, 3000.0, 4000.0], \"test case failed: dft_bins(9, 9000, True)\"\nassert dft_bins(3, 3000, False) == [0.0, 1000.0, -1000.0], \"test case failed: dft_bins(3, 3000, False)\""}
{"task_id": 331, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix whose element (i, j) is True/1 iff sample i\n        was out of bag for tree j, same shape as *predictions*.\n\n    Returns\n    -------\n    float\n        OOB mean squared error rounded to 4 decimal places, or -1 if it cannot\n        be computed.\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=float).ravel()\n    preds = np.asarray(predictions, dtype=float)\n    mask = np.asarray(oob_mask).astype(bool)\n    if preds.shape != mask.shape:\n        raise ValueError('`predictions` and `oob_mask` must have the same shape.')\n    oob_counts = mask.sum(axis=1)\n    valid_idx = oob_counts > 0\n    if not np.any(valid_idx):\n        return -1\n    oob_sums = (preds * mask).sum(axis=1)\n    oob_means = np.zeros_like(y_true)\n    oob_means[valid_idx] = oob_sums[valid_idx] / oob_counts[valid_idx]\n    sq_err = (oob_means[valid_idx] - y_true[valid_idx]) ** 2\n    mse = sq_err.mean()\n    return round(float(mse), 4)\nassert oob_mse([3, 5, 2, 7], [[2.5, 3.2], [4.8, 5.1], [2.1, 2.5], [6.5, 7.2]], [[True, False], [False, True], [True, True], [False, False]]) == 0.1167, \"test case 1 failed\"\nassert oob_mse([1, 2, 3], [[1.1, 0.9, 1.05], [2.5, 2.0, 2.0], [3.1, 3.2, 2.9]], [[True, False, True], [False, False, True], [True, True, True]]) == 0.0034, \"test case 2 failed\"\nassert oob_mse([4, 5], [[4.1, 3.9], [5.2, 4.8]], [[True, True], [True, True]]) == 0.0, \"test case 3 failed\"\nassert oob_mse([10, 20, 30], [[9, 11], [18, 19], [31, 29]], [[False, True], [False, False], [False, False]]) == 1.0, \"test case 4 failed\"\nassert oob_mse([0, 1, 2, 3, 4], [[0.2, -0.1, 0.1], [0.8, 1.2, 1.0], [1.9, 2.2, 2.1], [2.5, 3.5, 3.1], [3.9, 4.2, 4.0]], [[True, True, True], [False, True, False], [True, False, False], [False, False, False], [False, True, True]]) == 0.0161, \"test case 5 failed\"\nassert oob_mse([-1, -2], [[-0.8, -1.2], [-2.1, -1.9]], [[1, 0], [0, 1]]) == 0.025, \"test case 6 failed\"\nassert oob_mse([5], [[4.9, 5.1, 5.0]], [[0, 0, 1]]) == 0.0, \"test case 7 failed\"\nassert oob_mse([10, 20, 30, 40], [[10.1, 9.8, 10.3, 10.0], [19.5, 20.2, 21.0, 19.9], [31.5, 29.0, 30.2, 30.0], [39.8, 40.3, 40.2, 39.9]], [[1, 1, 1, 1], [0, 1, 1, 0], [1, 0, 1, 0], [0, 0, 0, 0]]) == 0.3617, \"test case 8 failed\"\nassert oob_mse([1, 2], [[1.1, 0.9], [2.1, 1.8]], [[0, 0], [0, 0]]) == -1.0, \"test case 9 failed\"\nassert oob_mse([0, 0, 0], [[0, 0], [0, 0], [0, 0]], [[1, 1], [1, 1], [1, 1]]) == 0.0, \"test case 10 failed\""}
{"task_id": 332, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import Counter\nfrom collections import Counter\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree represented with plain nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of *transactions* an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node is a mapping:\n        {\"support\": <int>, \"children\": {item -> child-node}}\n    \"\"\"\n    item_counter = Counter()\n    for trx in transactions:\n        for item in set(trx):\n            item_counter[item] += 1\n    freq_items = {item: sup for (item, sup) in item_counter.items() if sup >= min_support}\n    if not freq_items:\n        return {'support': 0, 'children': {}}\n    ordered = sorted(freq_items.items(), key=lambda pair: (-pair[1], pair[0]))\n    order_rank = {item: rank for (rank, (item, _)) in enumerate(ordered)}\n    frequent_set = set(freq_items)\n    root = {'support': 0, 'children': {}}\n    for trx in transactions:\n        filtered = [item for item in set(trx) if item in frequent_set]\n        if not filtered:\n            continue\n        filtered.sort(key=lambda it: order_rank[it])\n        node = root\n        for item in filtered:\n            children = node['children']\n            if item not in children:\n                children[item] = {'support': 0, 'children': {}}\n            node = children[item]\n            node['support'] += 1\n    return root\nassert build_fp_tree([['a','b','c'], ['a','b'], ['a','c']], 2) == {'support': 0, 'children': {'a': {'support': 3, 'children': {'b': {'support': 2, 'children': {'c': {'support': 1, 'children': {}}}}, 'c': {'support': 1, 'children': {}}}}}}, \"TC1 failed\"\nassert build_fp_tree([['a','b'], ['b','c']], 3) == {'support': 0, 'children': {}}, \"TC2 failed\"\nassert build_fp_tree([['x','y','z']], 1) == {'support': 0, 'children': {'x': {'support': 1, 'children': {'y': {'support': 1, 'children': {'z': {'support': 1, 'children': {}}}}}}}}, \"TC3 failed\"\nassert build_fp_tree([['1','2'], ['2','3'], ['2','4']], 2) == {'support': 0, 'children': {'2': {'support': 3, 'children': {}}}}, \"TC4 failed\"\nassert build_fp_tree([['a'], ['b']], 1) == {'support': 0, 'children': {'a': {'support': 1, 'children': {}}, 'b': {'support': 1, 'children': {}}}}, \"TC5 failed\"\nassert build_fp_tree([['d','c','b','a'], ['a','b','c','d']], 1) == {'support': 0, 'children': {'a': {'support': 2, 'children': {'b': {'support': 2, 'children': {'c': {'support': 2, 'children': {'d': {'support': 2, 'children': {}}}}}}}}}}, \"TC6 failed\"\nassert build_fp_tree([['a','a','b'], ['a','b','b','a'], ['b','b','c']], 2) == {'support': 0, 'children': {'b': {'support': 3, 'children': {'a': {'support': 2, 'children': {}}}}}}, \"TC7 failed\"\nassert build_fp_tree([['x','y']] * 4, 2) == {'support': 0, 'children': {'x': {'support': 4, 'children': {'y': {'support': 4, 'children': {}}}}}}, \"TC8 failed\"\nassert build_fp_tree([[], ['a']], 1) == {'support': 0, 'children': {'a': {'support': 1, 'children': {}}}}, \"TC9 failed\"\nassert build_fp_tree([['c'], ['b'], ['a']], 1) == {'support': 0, 'children': {'a': {'support': 1, 'children': {}}, 'b': {'support': 1, 'children': {}}, 'c': {'support': 1, 'children': {}}}}, \"TC10 failed\""}
{"task_id": 336, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.GenericAlias' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef linear_kernel(X: list[list[int | float]] | 'np.ndarray', Y: list[list[int | float]] | 'np.ndarray' | None=None, c0: int | float=0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n    Y : 2-D list or NumPy array with shape (M, C) or None.  If None, Y = X.\n    c0 : additive constant of the kernel (default 0).\n\n    Returns\n    -------\n    list[list[float]] | int\n        Kernel matrix rounded to 4 decimals, or \u20111 for incompatible shapes.\n    \"\"\"\n\n    def _to_2d_float_array(obj):\n        arr = np.asarray(obj, dtype=float)\n        if arr.ndim == 0:\n            arr = arr.reshape(1, 1)\n        elif arr.ndim == 1:\n            arr = arr.reshape(1, -1)\n        return arr\n    if isinstance(X, (list, tuple)) and len(X) == 0:\n        return []\n    X_arr = _to_2d_float_array(X)\n    if Y is None:\n        Y_arr = X_arr\n    elif isinstance(Y, (list, tuple)) and len(Y) == 0:\n        Y_arr = _to_2d_float_array(Y)\n    else:\n        Y_arr = _to_2d_float_array(Y)\n    if X_arr.shape[1] != Y_arr.shape[1]:\n        return -1\n    K = X_arr @ Y_arr.T\n    if c0 != 0:\n        K = K + c0\n    return np.round(K.astype(float), 4).tolist()\nassert linear_kernel([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], 0) == [[50.0], [122.0]], \"failed on basic example\"\nassert linear_kernel([[1, 2, 3]], None, 1) == [[15.0]], \"failed when Y is None\"\nassert linear_kernel([[0, 0], [0, 0]], [[1, 2], [3, 4]], 0) == [[0.0, 0.0], [0.0, 0.0]], \"failed on zero vectors\"\nassert linear_kernel([[1, 2]], [[3, 4, 5]], 0) == -1, \"failed on mismatched dimensions\"\nassert linear_kernel([[1, 0], [0, 1]], None, 0) == [[1.0, 0.0], [0.0, 1.0]], \"failed on identity check\"\nassert linear_kernel([[-1, -2], [3, 4]], [[5, 6], [-7, -8]], 2) == [[-15.0, 25.0], [41.0, -51.0]], \"failed on negative and positive mix\"\nimport numpy as np\narrX = np.array([[1.5, 2.5]])\narrY = np.array([[3.5, 4.5]])\nassert linear_kernel(arrX, arrY, 0.5) == [[17.0]], \"failed on numpy array input\"\nassert linear_kernel([], [], 0) == [], \"failed on empty input\"\nassert linear_kernel([[1,2],[3,4],[5,6]], None, 0)[0][1] == 11.0, \"failed on symmetric property\"\nassert linear_kernel([[1,2,3]], [[4,5]], 0) == -1, \"failed on second mismatched dimension check\""}
{"task_id": 340, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adam_update(param: np.ndarray, grad: np.ndarray, t: int, m: np.ndarray, v: np.ndarray, lr: float=0.001, beta1: float=0.9, beta2: float=0.999, eps: float=1e-08) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1.\n        beta1: Exponential decay rate for the first moment.\n        beta2: Exponential decay rate for the second moment.\n        eps:   Small constant to avoid division by zero.\n\n    Returns:\n        Tuple of python lists rounded to 8 decimal places:\n        (updated_parameter, new_moment, new_variance)\n    \"\"\"\n    param = np.asarray(param, dtype=np.float64)\n    grad = np.asarray(grad, dtype=np.float64)\n    m = np.asarray(m, dtype=np.float64)\n    v = np.asarray(v, dtype=np.float64)\n    m_t = beta1 * m + (1.0 - beta1) * grad\n    v_t = beta2 * v + (1.0 - beta2) * grad ** 2\n    m_hat = m_t / (1.0 - beta1 ** t)\n    v_hat = v_t / (1.0 - beta2 ** t)\n    if np.all(grad == 0):\n        param_t = param.copy()\n    else:\n        param_t = param - lr * m_hat / (np.sqrt(v_hat) + eps)\n\n    def to_rounded_list(arr: np.ndarray) -> list:\n        return np.round(arr, 8).tolist()\n    return (to_rounded_list(param_t), to_rounded_list(m_t), to_rounded_list(v_t))\nassert np.allclose(adam_update(np.array([0.1, -0.2]), np.array([0.01, -0.01]), 1, np.array([0., 0.]), np.array([0., 0.]))[0], [0.099, -0.199]), \"test case failed: basic 2-D example\"\nassert np.allclose(adam_update(np.array([0.099, -0.199]), np.array([0.02, -0.02]), 2, np.array([0.001, -0.001]), np.array([1e-07, 1e-07]))[0], [0.09803483, -0.19803483]), \"test case failed: second step parameters\"\nassert np.allclose(adam_update(1.0, 0.1, 1, 0.0, 0.0)[0], [0.999]), \"test case failed: scalar positive grad\"\nassert np.allclose(adam_update(np.array([0.5, -1.2, 0.0]), np.array([-0.05, 0.03, 0.0]), 1, np.zeros(3), np.zeros(3))[0], [0.501, -1.201, 0.0]), \"test case failed: mixed sign 3-D\"\nassert np.allclose(adam_update(1.5, -0.3, 1, 0.0, 0.0)[0], [1.501]), \"test case failed: scalar negative grad\"\nassert np.allclose(adam_update(np.array([[0.1, 0.2], [0.3, 0.4]]), np.zeros((2, 2)), 1, np.zeros((2, 2)), np.zeros((2, 2)))[0], [[0.1, 0.2], [0.3, 0.4]]), \"test case failed: zero grad param change\"\nassert np.allclose(adam_update(0.999, 0.1, 2, np.array([0.01]), np.array([1e-05]))[0], [0.998]), \"test case failed: scalar step 2\"\nassert np.allclose(adam_update(0.2, 0.05, 100, 0.0, 0.0)[0], [0.1990242]), \"test case failed: large t\"\nassert np.allclose(adam_update(np.array([0.2, 0.4]), np.array([0.05, -0.05]), 1, np.zeros(2), np.zeros(2))[0], [0.199, 0.401]), \"test case failed: opposite grads 2-D\"\nassert np.allclose(adam_update(np.array([[1.0, -1.0], [-1.0, 1.0]]), np.full((2, 2), 0.1), 1, np.zeros((2, 2)), np.zeros((2, 2)))[0], [[0.999, -1.001], [-1.001, 0.999]]), \"test case failed: matrix param\""}
{"task_id": 343, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_entropy_update(theta_samples: np.ndarray, rewards: list[float] | np.ndarray, retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists with the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    rewards = np.asarray(rewards)\n    n_samples = theta_samples.shape[0]\n    k = max(1, int(retain_prcnt * n_samples))\n    elite_idx = np.argsort(rewards)[::-1][:k]\n    elite_samples = theta_samples[elite_idx]\n    mean = np.mean(elite_samples, axis=0)\n    var = np.var(elite_samples, axis=0)\n    mean_rounded = np.round(mean, 4).tolist()\n    var_rounded = np.round(var, 4).tolist()\n    return (mean_rounded, var_rounded)\nassert cross_entropy_update(np.array([[1,2],[3,4],[5,6],[7,8],[9,10]]),[10,20,30,40,50],0.4) == ([8.0,9.0],[1.0,1.0]), \"failed on basic 2D example\"\nassert cross_entropy_update(np.array([[0,0],[1,1],[2,2],[3,3]]),[0.1,0.2,0.9,0.5],0.5) == ([2.5,2.5],[0.25,0.25]), \"failed on half retain\"\nassert cross_entropy_update(np.array([[1,1,1],[2,2,2],[3,3,3]]),[3,1,2],0.33) == ([1.0,1.0,1.0],[0.0,0.0,0.0]), \"failed when retaining single best sample\"\nassert cross_entropy_update(np.array([[4,5],[6,7]]),[7,2],1.0) == ([5.0,6.0],[1.0,1.0]), \"failed when retaining all samples\"\nassert cross_entropy_update(np.array([[1,2],[2,3],[3,4],[4,5],[5,6]]),[5,4,3,2,1],0.2)==([1.0,2.0],[0.0,0.0]), \"failed retain 20%\"\nassert cross_entropy_update(np.array([[2],[4],[6],[8]]),[1,2,3,4],0.5)==([7.0],[1.0]), \"failed single dimension example\"\nassert cross_entropy_update(np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]]),[12,11,10,9],0.5)==([2.5,3.5,4.5],[2.25,2.25,2.25]), \"failed 3D half retain\"\nassert cross_entropy_update(np.array([[5,5],[5,5],[5,5]]),[1,2,3],0.34)==([5.0,5.0],[0.0,0.0]), \"failed identical theta values\""}
{"task_id": 353, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nfrom collections import Counter\nimport numpy as np\nimport math\nfrom collections import Counter\nimport numpy as np\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n                class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    label_list = list(labels)\n    if not label_list:\n        return 0.0\n    total = len(label_list)\n    counts = Counter(label_list)\n    if len(counts) == 1:\n        return 0.0\n    entropy_val = 0.0\n    for count in counts.values():\n        p = count / total\n        if p > 0.0:\n            entropy_val -= p * math.log2(p)\n    return round(entropy_val, 5)\nassert entropy([0, 0, 1, 1, 1]) == 0.97095, \"failed: entropy([0, 0, 1, 1, 1])\"\nassert entropy([1, 1, 1, 1]) == 0.0, \"failed: entropy identical labels\"\nassert entropy([]) == 0.0, \"failed: entropy empty input\"\nassert entropy([0, 1]) == 1.0, \"failed: entropy([0,1])\"\nassert entropy(np.array([2, 2, 2, 3])) == 0.81128, \"failed: entropy numpy array\"\nassert entropy(['a', 'b', 'c', 'd']) == 2.0, \"failed: entropy([a,b,c,d])\"\nassert entropy([5]) == 0.0, \"failed: single element\"\nassert entropy([0,0,0,1,1,2,2,2,2]) == 1.53049, \"failed: multi-class\"\nassert entropy(range(8)) == 3.0, \"failed: entropy range(8)\""}
{"task_id": 354, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef _parse_padding(pad, k_r, k_c, dil, stride, in_r, in_c):\n    \"\"\"Returns the 4 individual paddings (top, bottom, left, right).\"\"\"\n    eff_k_r = k_r + (k_r - 1) * dil\n    eff_k_c = k_c + (k_c - 1) * dil\n    if isinstance(pad, int):\n        return (pad, pad, pad, pad)\n    if isinstance(pad, tuple) and len(pad) == 2:\n        (pr, pc) = pad\n        return (pr, pr, pc, pc)\n    if isinstance(pad, tuple) and len(pad) == 4:\n        return pad\n    if pad == 'same':\n        total_pad_r = max(eff_k_r - 1, 0)\n        total_pad_c = max(eff_k_c - 1, 0)\n        pr1 = total_pad_r // 2\n        pr2 = total_pad_r - pr1\n        pc1 = total_pad_c // 2\n        pc2 = total_pad_c - pc1\n        return (pr1, pr2, pc1, pc2)\n    return (0, 0, 0, 0)\ndef conv2D(X: 'np.ndarray', W: 'np.ndarray', stride: int, pad, dilation: int=0):\n    \"\"\"Performs a 2-D convolution (cross-correlation) via im2col + GEMM.\"\"\"\n    (n_ex, in_r, in_c, in_ch) = X.shape\n    (k_r, k_c, in_ch_w, out_ch) = W.shape\n    assert in_ch == in_ch_w, 'Input / kernel channels mismatch'\n    dil = dilation\n    eff_k_r = k_r + (k_r - 1) * dil\n    eff_k_c = k_c + (k_c - 1) * dil\n    (pr1, pr2, pc1, pc2) = _parse_padding(pad, k_r, k_c, dil, stride, in_r, in_c)\n    X_pad = np.pad(X, pad_width=((0, 0), (pr1, pr2), (pc1, pc2), (0, 0)), mode='constant')\n    (pad_r, pad_c) = (in_r + pr1 + pr2, in_c + pc1 + pc2)\n    out_r = (pad_r - eff_k_r) // stride + 1\n    out_c = (pad_c - eff_k_c) // stride + 1\n    cols = np.empty((n_ex, out_r, out_c, k_r, k_c, in_ch), dtype=X.dtype)\n    for kr in range(k_r):\n        row_start = kr * (dil + 1)\n        row_end = row_start + stride * out_r\n        row_slice = slice(row_start, row_end, stride)\n        for kc in range(k_c):\n            col_start = kc * (dil + 1)\n            col_end = col_start + stride * out_c\n            col_slice = slice(col_start, col_end, stride)\n            cols[:, :, :, kr, kc, :] = X_pad[:, row_slice, col_slice, :]\n    cols_2d = cols.reshape(n_ex * out_r * out_c, -1)\n    W_col = W.reshape(-1, out_ch)\n    out_mat = cols_2d @ W_col\n    out = out_mat.reshape(n_ex, out_r, out_c, out_ch)\n    return out.tolist()\nassert conv2D(np.arange(1, 10, dtype=float).reshape(1,3,3,1), np.array([[[[1.]],[[0.]]],[[[0.]],[[-1.]]]]), 1, 0) == [[[[-4.0], [-4.0]], [[-4.0], [-4.0]]]], \"basic 3\u00d73 no-pad\"\nassert conv2D(np.array([[[[1.],[2.]],[[3.],[4.]]]]), np.ones((1,1,1,1)), 1, \"same\") == [[[ [1.0], [2.0] ], [ [3.0], [4.0] ]]], \"same pad identity\"\nassert conv2D(np.arange(1,17,dtype=float).reshape(1,4,4,1), np.ones((2,2,1,1)), 2, 0) == [[[ [14.0], [22.0] ], [ [46.0], [54.0] ]]], \"stride 2\"\nassert conv2D(np.array([[[[1.],[2.]],[[3.],[4.]]]]), np.concatenate([np.ones((1,1,1,1)), -np.ones((1,1,1,1))], axis=3), 1, 0) == [[[[1.0,-1.0],[2.0,-2.0]],[[3.0,-3.0],[4.0,-4.0]]]], \"multi-out ch\"\nassert conv2D(np.array([[[[1.,10.],[2.,20.]],[[3.,30.],[4.,40.]]]]), np.array([[[[1.],[1.]]]]), 1, 0) == [[[ [11.0], [22.0] ], [ [33.0], [44.0] ]]], \"multi-in ch\"\nassert conv2D(np.array([[[[7.]]]]), np.ones((1,1,1,1)), 1, (1,1)) == [[[[0.0],[0.0],[0.0]],[[0.0],[7.0],[0.0]],[[0.0],[0.0],[0.0]]]], \"2-tuple pad\"\nassert conv2D(np.array([[[[1.],[2.]],[[3.],[4.]]]]), np.ones((1,1,1,1)), 1, (0,1,0,1)) == [[[[1.0],[2.0],[0.0]],[[3.0],[4.0],[0.0]],[[0.0],[0.0],[0.0]]]], \"4-tuple pad\"\nassert conv2D(np.array([[[[1.],[2.]],[[3.],[4.]]],[[[5.],[6.]],[[7.],[8.]]]]), np.full((1,1,1,1),2.), 1, 0) == [[[[2.0],[4.0]],[[6.0],[8.0]]],[[[10.0],[12.0]],[[14.0],[16.0]]]], \"batching\"\nassert conv2D(np.full((1,2,2,1),5.), np.zeros((1,1,1,1)), 1, 0) == [[[ [0.0], [0.0] ], [ [0.0], [0.0] ]]], \"zero kernel\""}
{"task_id": 355, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef _deterministic_sign_flip(components: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Flip the sign of each component so that the first non\u2013zero loading is \u2265 0.\n    The operation is done in-place and the resulting array is also returned.\n    \"\"\"\n    for i in range(components.shape[1]):\n        col = components[:, i]\n        for v in col:\n            if abs(v) > 1e-12:\n                if v < 0:\n                    components[:, i] *= -1\n                break\n    return components\ndef pca_transform(data: np.ndarray, n_components: int, solver: str='svd') -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis with a deterministic sign convention.\"\"\"\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array (samples \u00d7 features).')\n    if n_components < 1:\n        raise ValueError('`n_components` must be at least 1.')\n    if solver not in {'svd', 'eigen'}:\n        raise ValueError(\"`solver` must be either 'svd' or 'eigen'.\")\n    (n_samples, n_features) = data.shape\n    n_components = min(n_components, n_features)\n    X = data.astype(float) - data.mean(axis=0, keepdims=True)\n    if solver == 'svd':\n        (_, singular_vals, Vt) = np.linalg.svd(X, full_matrices=False)\n        components = Vt.T[:, :n_components]\n        expl_var = singular_vals ** 2 / (n_samples - 1)\n    else:\n        cov = np.cov(X, rowvar=False, ddof=1)\n        (eig_vals, eig_vecs) = np.linalg.eigh(cov)\n        idx = eig_vals.argsort()[::-1]\n        (eig_vals, eig_vecs) = (eig_vals[idx], eig_vecs[:, idx])\n        components = eig_vecs[:, :n_components]\n        expl_var = eig_vals\n    total_variance = expl_var.sum()\n    explained_variance_ratio = (expl_var / total_variance)[:n_components]\n    components = _deterministic_sign_flip(components)\n    scores = X @ components\n    scores_rounded = np.round(scores, 4).tolist()\n    evr_rounded = np.round(explained_variance_ratio, 4).tolist()\n    return (scores_rounded, evr_rounded)\nassert pca_transform(np.array([[1, 2], [3, 4], [5, 6]]), 1) == ([[-2.8284], [0.0], [2.8284]], [1.0]), \"failed: centred diagonal data \u2013 svd\"\nassert pca_transform(np.array([[1, 0], [2, 0], [3, 0]]), 1) == ([[-1.0], [0.0], [1.0]], [1.0]), \"failed: single varying axis \u2013 svd\"\nassert pca_transform(np.array([[2, 2], [4, 4], [6, 6]]), 1) == ([[-2.8284], [0.0], [2.8284]], [1.0]), \"failed: scaled diagonal \u2013 svd\"\nassert pca_transform(np.array([[1, 0], [1, 1], [1, 2]]), 1) == ([[-1.0], [0.0], [1.0]], [1.0]), \"failed: variation along second axis \u2013 svd\"\nassert pca_transform(np.array([[1], [2], [3], [4]]), 1) == ([[-1.5], [-0.5], [0.5], [1.5]], [1.0]), \"failed: one-dimensional data \u2013 svd\"\nassert pca_transform(np.array([[1, 0], [2, 0], [3, 0]]), 1, solver=\"eigen\") == ([[-1.0], [0.0], [1.0]], [1.0]), \"failed: single varying axis \u2013 eigen\"\nassert pca_transform(np.array([[1, 2], [1, 4]]), 1, solver=\"eigen\") == ([[-1.0], [1.0]], [1.0]), \"failed: two samples \u2013 eigen\"\nassert pca_transform(np.array([[0, 0, 1], [0, 0, 2], [0, 0, 3]]), 1) == ([[-1.0], [0.0], [1.0]], [1.0]), \"failed: third axis variation \u2013 svd\"\nassert pca_transform(np.array([[1, 2], [2, 4], [3, 6], [4, 8]]), 1) == ([[-3.3541], [-1.118 ], [1.118 ], [3.3541]], [1.0]), \"failed: perfectly collinear \u2013 svd\""}
{"task_id": 356, "completion_id": 0, "passed": false, "result": "failed: name 'Leaf' is not defined", "solution": "\ndef leaf_predict(leaf: 'Leaf', classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                    classification tree; otherwise treat it as regression.\n\n    Returns\n    -------\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        probs = list(leaf.value)\n        best_idx = 0\n        best_val = probs[0]\n        for (idx, val) in enumerate(probs[1:], start=1):\n            if val > best_val:\n                best_val = val\n                best_idx = idx\n        return best_idx\n    return leaf.value\nassert leaf_predict(Leaf([0.25, 0.55, 0.20]), True) == 1, \"failed on basic classification example\"\nassert leaf_predict(Leaf([0.4, 0.4, 0.2]), True) == 0, \"failed on tie-breaking (should pick smaller index)\"\nassert leaf_predict(Leaf([1.0]), True) == 0, \"failed when only one class present\"\nassert abs(leaf_predict(Leaf(3.7), False) - 3.7) < 1e-9, \"failed on basic regression example\"\nassert leaf_predict(Leaf(-2), False) == -2, \"failed on negative regression value\"\nassert leaf_predict(Leaf([0, 0, 1]), True) == 2, \"failed when max is last element\"\nassert leaf_predict(Leaf([0.33, 0.33, 0.34]), True) == 2, \"failed on close probabilities\"\nassert leaf_predict(Leaf(0), False) == 0, \"failed on zero regression value\"\nassert leaf_predict(Leaf([0.9, 0.1]), True) == 0, \"failed on two-class classification\"\nassert leaf_predict(Leaf([0.1, 0.2, 0.2, 0.5]), True) == 3, \"failed on multi-class classification\""}
{"task_id": 357, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kmeans(data: np.ndarray, K: int, max_iters: int=100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array of shape (n_samples, n_features).\n    K : int\n        Number of clusters.\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Sorted centroids, rounded to four decimals.\n    \"\"\"\n    data = np.asarray(data, dtype=float)\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array')\n    (n_samples, n_features) = data.shape\n    if K < 1 or K > n_samples:\n        raise ValueError('K must be between 1 and the number of samples')\n    if K == 1:\n        centroid = np.mean(data, axis=0, keepdims=True)\n        centroid = np.round(centroid, 4)\n        return centroid.tolist()\n    centroids = data[:K].astype(float, copy=True)\n    tol = 1e-06\n    for _ in range(max_iters):\n        diff = data[:, None, :] - centroids[None, :, :]\n        dists = np.sum(diff ** 2, axis=2)\n        labels = np.argmin(dists, axis=1)\n        new_centroids = centroids.copy()\n        for k in range(K):\n            members = data[labels == k]\n            if members.size:\n                new_centroids[k] = members.mean(axis=0)\n        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) <= tol):\n            centroids = new_centroids\n            break\n        centroids = new_centroids\n    sort_keys = tuple((centroids[:, i] for i in reversed(range(n_features))))\n    order = np.lexsort(sort_keys)\n    centroids_sorted = np.round(centroids[order], 4)\n    return centroids_sorted.tolist()\nassert kmeans(np.array([[1,1],[1,2],[2,1],[8,8],[9,8],[8,9]]),2)==[[1.3333,1.3333],[8.3333,8.3333]],\"failed on separated two-cluster example\"\nassert kmeans(np.array([[1,1],[2,2],[3,3]]),1)==[[2.0,2.0]],\"failed on single-cluster mean\"\nassert kmeans(np.array([[0,0],[1,0],[0,1],[1,1]]),2)==[[0.0,0.5],[1.0,0.5]],\"failed on square dataset\"\nassert kmeans(np.array([[1,0],[0,1],[-1,0],[0,-1]]),2)==[[-0.5,0.5],[0.5,-0.5]],\"failed on cross dataset\"\nassert kmeans(np.array([[2,2],[2,4],[4,2],[4,4],[10,10]]),2)==[[3.0,3.0],[10.0,10.0]],\"failed on distant outlier dataset\"\nassert kmeans(np.array([[0,0],[1,1]]),2)==[[0.0,0.0],[1.0,1.0]],\"failed on two-point two-cluster dataset\"\nassert kmeans(np.array([[0,0],[0,1],[1,0]]),3)==[[0.0,0.0],[0.0,1.0],[1.0,0.0]],\"failed on three-point three-cluster dataset\"\nassert kmeans(np.array([[1],[2],[3]]),2)==[[1.0],[2.5]],\"failed on one-dimensional dataset\"\nassert kmeans(np.array([[3,3],[4,3],[3,4],[4,4]]),2)==[[3.0,3.5],[4.0,3.5]],\"failed on 2x2 square dataset\"\nassert kmeans(np.array([[0,0,0],[0,0,1],[0,1,0],[1,0,0],[9,9,9],[9,9,8],[9,8,9],[8,9,9]]),2)==[[0.25,0.25,0.25],[8.75,8.75,8.75]],\"failed on 3-D dataset\""}
{"task_id": 362, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.GenericAlias' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef row_stochastic_matrix(data: list[list[int | float]] | 'np.ndarray') -> list[list[float]] | int:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row is divided by its own sum so that every row finally sums to 1.\n    If any row has a sum of exactly 0 normalisation is impossible and \u20131\n    is returned.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The row-normalised matrix (rounded to 4 decimals) or \u20131 when a\n        zero-sum row is encountered.\n    \"\"\"\n    arr = np.asarray(data, dtype=float)\n    if arr.ndim != 2:\n        raise ValueError('Input must be a 2-D structure')\n    row_sums = arr.sum(axis=1)\n    if np.any(row_sums == 0):\n        return -1\n    stochastic = arr / row_sums[:, None]\n    return np.round(stochastic, 4).tolist()\nassert row_stochastic_matrix([[1, 2, 3], [4, 5, 6]]) == [[0.1667, 0.3333, 0.5], [0.2667, 0.3333, 0.4]], \"failed: basic 3x3 matrix\"\nassert row_stochastic_matrix([[0, 0], [1, 1]]) == -1, \"failed: row with zero sum should return -1\"\nassert row_stochastic_matrix([[2, 2], [3, 1]]) == [[0.5, 0.5], [0.75, 0.25]], \"failed: 2x2 normalisation\"\nassert row_stochastic_matrix([[0.1, 0.2, 0.7]]) == [[0.1, 0.2, 0.7]], \"failed: single row unchanged\"\nassert row_stochastic_matrix([[10, 20, 30, 40], [5, 5, 5, 5]]) == [[0.1, 0.2, 0.3, 0.4], [0.25, 0.25, 0.25, 0.25]], \"failed: 4-column case\"\nassert row_stochastic_matrix([[9]]) == [[1.0]], \"failed: 1x1 matrix\"\nassert row_stochastic_matrix([[3, 5, 7], [0, 0, 0]]) == -1, \"failed: second row zero sum\"\nassert row_stochastic_matrix(np.array([[1, 1, 2], [2, 3, 5]])) == [[0.25, 0.25, 0.5], [0.2, 0.3, 0.5]], \"failed: numpy array input\"\nassert row_stochastic_matrix([[4, 0], [0, 8]]) == [[1.0, 0.0], [0.0, 1.0]], \"failed: rows with zeros\"\nassert row_stochastic_matrix([[1e-4, 1e-4], [2e-4, 3e-4]]) == [[0.5, 0.5], [0.4, 0.6]], \"failed: small numbers normalisation\""}
{"task_id": 363, "completion_id": 0, "passed": false, "result": "failed: name 'Discrete' is not defined", "solution": "\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\nclass Space:\n    pass\nclass Box(Space):\n\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\nclass Tuple(Space):\n\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\nclass Dict(Space):\n\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite\n    (`Tuple` or `Dict`) spaces the space is continuous only if **all** its\n    immediate sub-spaces are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: True if the action space is known to be composite.\n        tuple_obs:    True if the observation space is known to be composite.\n\n    Returns:\n        A tuple (cont_action, cont_obs) with the continuity result for the\n        action and observation spaces, respectively.\n    \"\"\"\n\n    def is_simple_box(space: Space) -> bool:\n        \"\"\"Return True only when `space` itself is a `Box`.\"\"\"\n        return isinstance(space, Box)\n\n    def all_subspaces_box(space: Space) -> bool:\n        \"\"\"Return True if every direct sub-space of `space` is a `Box`.\"\"\"\n        if isinstance(space, Tuple):\n            subspaces = space.spaces\n        elif isinstance(space, Dict):\n            subspaces = space.spaces.values()\n        else:\n            return False\n        return all((isinstance(s, Box) for s in subspaces))\n    if tuple_action:\n        cont_action = all_subspaces_box(env.action_space)\n    else:\n        cont_action = is_simple_box(env.action_space)\n    if tuple_obs:\n        cont_obs = all_subspaces_box(env.observation_space)\n    else:\n        cont_obs = is_simple_box(env.observation_space)\n    return (cont_action, cont_obs)\nassert is_continuous(Env(Box(-1.0, 1.0, (3,)), Box(-1.0, 1.0, (3,))), False, False) == (True, True), \"TC1 failed\"\nassert is_continuous(Env(Discrete(4), Discrete(4)), False, False) == (False, False), \"TC2 failed\"\nassert is_continuous(Env(Box(-1.0, 1.0, (3,)), Discrete(4)), False, False) == (True, False), \"TC3 failed\"\nassert is_continuous(Env(Tuple([Box(-1.0,1.0,(3,)), Box(-1.0,1.0,(3,))]), Box(-1.0,1.0,(3,))), True, False) == (True, True), \"TC4 failed\"\nassert is_continuous(Env(Tuple([Box(-1.0,1.0,(3,)), Discrete(5)]), Box(-1.0,1.0,(3,))), True, False) == (False, True), \"TC5 failed\"\nassert is_continuous(Env(Box(-1.0,1.0,(3,)), Dict({\"a\": Box(-1.0,1.0,(3,)), \"b\": Box(-1.0,1.0,(3,))})), False, True) == (True, True), \"TC6 failed\"\nassert is_continuous(Env(Box(-1.0,1.0,(3,)), Dict({\"a\": Box(-1.0,1.0,(3,)), \"b\": Discrete(2)})), False, True) == (True, False), \"TC7 failed\"\nassert is_continuous(Env(Tuple([]), Box(-1.0,1.0,(3,))), True, False) == (True, True), \"TC8 failed\"\nassert is_continuous(Env(Tuple([Box(-1.0,1.0,(3,)) for _ in range(10)]), Discrete(5)), True, False) == (True, False), \"TC9 failed\"\nassert is_continuous(Env(Discrete(2), Box(-1.0,1.0,(3,))), False, False) == (False, True), \"TC10 failed\""}
{"task_id": 369, "completion_id": 0, "passed": false, "result": "failed: `updates` must contain at least one tree output.", "solution": "import numpy as np\nimport numpy as np\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        raise ValueError('`updates` must contain at least one tree output.')\n    stacked_updates = np.stack(updates, axis=0)\n    running_pred = -learning_rate * stacked_updates.sum(axis=0)\n    if regression:\n        return np.round(running_pred.astype(float), 4)\n    logits = running_pred\n    logits -= logits.max(axis=1, keepdims=True)\n    exp_logits = np.exp(logits)\n    probas = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n    return probas.argmax(axis=1)\nassert gradient_boosting_predict([np.array([1.0, -2.0, 0.5])], 0.1, True).tolist() == [-0.1, 0.2, -0.05], \"test case failed: single-tree regression\"\nassert gradient_boosting_predict([np.array([1, 2, 3]), np.array([0.5, -0.5, 1])], 0.1, True).tolist() == [-0.15, -0.15, -0.4], \"test case failed: two-tree regression\"\nassert gradient_boosting_predict([], 0.2, True).size == 0, \"test case failed: empty updates\"\nassert gradient_boosting_predict([np.array([[0.2, -0.1, 0.1], [-0.3, 0.4, -0.1]]), np.array([[0.1, 0.2, -0.3], [0.2, -0.2, 0.0]])], 0.5, False).tolist() == [2, 0], \"test case failed: two-tree classification\"\nassert gradient_boosting_predict([np.array([[1.0,-1.0],[0.5,-0.5]])], 1.0, False).tolist() == [1, 1], \"test case failed: single-tree classification\"\nassert gradient_boosting_predict([np.array([[0.0,0.0],[1.0,-1.0]])], 2.0, False).tolist() == [0, 1], \"test case failed: equal-logit classification\"\nassert gradient_boosting_predict([np.array([[0.2,0.1],[-0.1,0.3]]), np.array([[0.1,-0.1],[0.05,-0.05]])], 0.3, False).tolist() == [1, 0], \"test case failed: two-tree small-lr classification\"\nassert gradient_boosting_predict([np.array([0.0,1.0]), np.array([1.0,1.0])], 0.5, True).tolist() == [-0.5, -1.0], \"test case failed: regression zeros first update\"\nassert gradient_boosting_predict([np.array([-1.0,-2.0])], 0.3, True).tolist() == [0.3, 0.6], \"test case failed: regression negative updates\"\nassert gradient_boosting_predict([np.array([[1.0,2.0,3.0]])], 1.0, False).tolist() == [0], \"test case failed: 3-class single-sample classification\""}
{"task_id": 371, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int=0) -> list:\n    \"\"\"1-D cross-correlation (a.k.a. \u201cconvolution\u201d in DL frameworks).\n\n    Parameters\n    ----------\n    X : ndarray, shape (batch, signal_length, in_channels)\n        Input signals.\n    W : ndarray, shape (kernel_width, in_channels, out_channels)\n        Convolution kernels (not flipped \u2013 cross-correlation).\n    stride : int\n        Step size of the sliding window.\n    pad : int | tuple(int, int) | \"same\"\n        Zero padding to the left / right of the signal.\n    dilation : int, default 0\n        Number of zeros inserted *between* kernel elements\n        (0 \u21d2 normal, contiguous kernel).\n\n    Returns\n    -------\n    list\n        Convolved output with shape\n        (batch, output_length, out_channels), converted via ``tolist()``.\n    \"\"\"\n    if X.ndim != 3:\n        raise ValueError('X must have shape (batch, length, in_ch).')\n    if W.ndim != 3:\n        raise ValueError('W must have shape (k_width, in_ch, out_ch).')\n    if X.shape[2] != W.shape[1]:\n        raise ValueError('Input-channel dimensions of X and W differ.')\n    if stride <= 0:\n        raise ValueError('stride must be a positive integer.')\n    if dilation < 0:\n        raise ValueError('dilation must be non-negative.')\n    (batch, L_in, C_in) = X.shape\n    (K, _, C_out) = W.shape\n    eff_K = (K - 1) * (dilation + 1) + 1\n    if isinstance(pad, str):\n        pad = pad.lower()\n    if pad == 'same':\n        out_len = int(np.ceil(L_in / stride))\n        total_pad = max(0, (out_len - 1) * stride + eff_K - L_in)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n    elif isinstance(pad, int):\n        p_left = p_right = int(pad)\n    elif isinstance(pad, (tuple, list)) and len(pad) == 2 and all((isinstance(x, int) for x in pad)):\n        (p_left, p_right) = pad\n    else:\n        raise ValueError('pad must be int, 2-tuple or \"same\".')\n    if p_left < 0 or p_right < 0:\n        raise ValueError('Padding values must be non-negative.')\n    X_pad = np.pad(X, ((0, 0), (p_left, p_right), (0, 0)), mode='constant')\n    L_pad = L_in + p_left + p_right\n    out_len = (L_pad - eff_K) // stride + 1\n    if out_len <= 0:\n        return np.empty((batch, 0, C_out)).tolist()\n    start_idx = np.arange(out_len) * stride\n    kernel_offset = np.arange(K) * (dilation + 1)\n    idx = start_idx[:, None] + kernel_offset[None, :]\n    windows = X_pad[:, idx, :]\n    out = np.tensordot(windows, W, axes=([2, 3], [0, 1]))\n    return out.tolist()\nassert conv1D(np.array([[[1],[2],[3],[4],[5]]]), np.array([[[1]],[[0]],[[-1]]]), 1, \"same\", 0) == [[[-2.0],[-2.0],[-2.0],[-2.0],[4.0]]], \"failed on same-pad basic example\"\nassert conv1D(np.array([[[1],[2],[3],[4],[5]]]), np.array([[[1]],[[0]],[[-1]]]), 1, 0) == [[[-2.0],[-2.0],[-2.0]]], \"failed on zero-pad example\"\nassert conv1D(np.array([[[1],[2],[1],[2],[1],[2]]]), np.array([[[1]],[[1]]]), 2, 0) == [[[3.0],[3.0],[3.0]]], \"failed on stride 2\"\nassert conv1D(np.array([[[1],[2],[3],[4],[5]]]), np.array([[[1]],[[1]]]), 1, 0, 1) == [[[4.0],[6.0],[8.0]]], \"failed on dilation 1\"\nassert conv1D(np.array([[[1,2],[3,4],[5,6],[7,8]]]),\n              np.array([[[1,0,1],[1,0,0]],[[1,0,0],[1,0,1]]]), 1, 0) == [[[10,0,5],[18,0,9],[26,0,13]]], \"failed on multi-channel\"\nassert conv1D(np.array([[[1],[2],[3],[4]]]), np.array([[[1]],[[1]],[[1]]]), 2, \"same\") == [[[6.0],[7.0]]], \"failed on same-pad + stride 2\"\nassert conv1D(np.array([[[1],[2]]]), np.array([[[1]],[[1]]]), 1, 1) == [[[1.0],[3.0],[2.0]]], \"failed on symmetric integer pad\"\nassert conv1D(np.array([[[1],[2],[3],[4],[5],[6],[7]]]), np.array([[[1]],[[1]],[[1]]]), 1, 0, 2) == [[[12.0]]], \"failed on dilation 2\"\nassert conv1D(np.array([[[1],[2],[3]],[[4],[5],[6]]]), np.array([[[1]],[[1]]]), 1, 0) == [[[3.0],[5.0]],[[9.0],[11.0]]], \"failed on batch processing\"\nassert conv1D(np.array([[[1],[2],[3]]]), np.array([[[2]]]), 1, (2,1)) == [[[0.0],[0.0],[2.0],[4.0],[6.0],[0.0]]], \"failed on asymmetric tuple pad\""}
{"task_id": 373, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.GenericAlias' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef gini(y: list[int] | 'np.ndarray') -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    y = np.asarray(y).ravel()\n    if y.size == 0:\n        return 0.0\n    (_, counts) = np.unique(y, return_counts=True)\n    probs = counts / counts.sum()\n    impurity = 1.0 - np.sum(probs ** 2)\n    return float(np.round(impurity, 4))\nassert gini([0, 1, 1, 0, 2]) == 0.64, \"failed: gini([0, 1, 1, 0, 2])\"\nassert gini([0, 0, 0, 0]) == 0.0, \"failed: gini([0, 0, 0, 0])\"\nassert gini([0, 1]) == 0.5, \"failed: gini([0, 1])\"\nassert gini([1, 2, 3, 4]) == 0.75, \"failed: gini([1, 2, 3, 4])\"\nassert gini([]) == 0.0, \"failed: gini([])\"\nassert gini([5, 5, 5, 5, 5]) == 0.0, \"failed: gini([5, 5, 5, 5, 5])\"\nassert gini([0, 1, 2, 3, 4, 5]) == 0.8333, \"failed: gini([0, 1, 2, 3, 4, 5])\"\nassert gini([2]) == 0.0, \"failed: gini([2])\"\nassert gini([1, 1, 1, 2, 2, 3]) == 0.6111, \"failed: gini([1, 1, 1, 2, 2, 3])\"\nassert gini(list(range(100))) == 0.99, \"failed: gini(range(100))\""}
{"task_id": 374, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float=0.01, n_iterations: int=10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple-linear\u2010regression model by batch gradient descent.\n\n    Parameters\n    ----------\n    X : np.ndarray (m \u00d7 n)\n        Feature matrix.\n    y : np.ndarray (m,)\n        Target vector.\n    learning_rate : float, default 0.01\n        Step size for gradient descent.\n    n_iterations : int, default 10_000\n        Number of gradient\u2010descent steps.\n\n    Returns\n    -------\n    (weights_list, bias) : tuple[list[float], float]\n        Learned parameters rounded to 4 decimal places.\n        If X and y have inconsistent sample sizes, returns -1.\n    \"\"\"\n    if X.ndim != 2:\n        X = np.atleast_2d(X)\n    y = np.ravel(y)\n    (m, n) = X.shape\n    if m != y.shape[0] or m == 0:\n        return -1\n    theta = np.zeros(n, dtype=float)\n    b = 0.0\n    for _ in range(n_iterations):\n        y_hat = X @ theta + b\n        error = y_hat - y\n        dw = 2 / m * (X.T @ error)\n        db = 2 / m * np.sum(error)\n        theta -= learning_rate * dw\n        b -= learning_rate * db\n    theta_rounded = np.round(theta, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return (theta_rounded, b_rounded)\nassert linear_regression_gd(np.array([[1, 2], [2, 0], [3, 1]]), np.array([5, 6, 7]), 0.01, 10000) == ([1.0, 0.0], 4.0), \"failed: basic 2-feature example\"\nassert linear_regression_gd(np.array([[1], [2], [3], [4]]), np.array([5, 7, 9, 11])) == ([2.0], 3.0), \"failed: y = 2x + 3\"\nassert linear_regression_gd(np.array([[0], [2], [4]]), np.array([4, 1, -2]), 0.01, 12000) == ([-1.5], 4.0), \"failed: negative slope\"\nassert linear_regression_gd(np.array([[1], [2], [3], [4]]), np.array([5, 5, 5, 5])) == ([0.0], 5.0), \"failed: constant target\"\nassert linear_regression_gd(np.array([[0, 0], [1, 5], [2, -1]]), np.array([4, 7, 10]), 0.01, 12000) == ([3.0, 0.0], 4.0), \"failed: irrelevant second feature\"\nassert linear_regression_gd(np.array([[0, 0]]), np.array([7]), 0.01, 5000) == ([0.0, 0.0], 7.0), \"failed: single sample two features\""}
{"task_id": 376, "completion_id": 0, "passed": false, "result": "failed: failed on empty matrix", "solution": "import numpy as np\nimport numpy as np\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if and only if it is square and equal (within\n    numerical tolerance) to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists of numbers.\n\n    Returns:\n        True if *X* is symmetric, otherwise False.\n    \"\"\"\n    if not X:\n        return False\n    n_rows = len(X)\n    for row in X:\n        if len(row) != n_rows:\n            return False\n    A = np.asarray(X, dtype=float)\n    return np.allclose(A, A.T, atol=1e-08, rtol=0.0)\nassert is_symmetric([[1]]) is True, \"failed on 1\u00d71 matrix\"\nassert is_symmetric([[1, 2], [2, 1]]) is True, \"failed on 2\u00d72 symmetric\"\nassert is_symmetric([[1, 0], [3, 1]]) is False, \"failed on 2\u00d72 non-symmetric\"\nassert is_symmetric([[1, 2, 3], [2, 5, 6], [3, 6, 9]]) is True, \"failed on 3\u00d73 symmetric\"\nassert is_symmetric([[1, 2], [2, 1], [3, 4]]) is False, \"failed on non-square\"\nassert is_symmetric([]) is True, \"failed on empty matrix\"\nassert is_symmetric([[0.0, 1e-09], [1e-09, 0.0]]) is True, \"failed on float tolerance\"\nassert is_symmetric([[3, -2, 5], [-2, 0, 4], [5, 4, 1]]) is True, \"failed on 3\u00d73 with negatives\"\nassert is_symmetric([[1, 2, 3], [2, 1, 4], [4, 5, 1]]) is False, \"failed on asymmetric 3\u00d73\"\nassert is_symmetric([[2, 3, 4, 5], [3, 2, 6, 7], [4, 6, 2, 8], [5, 7, 8, 2]]) is True, \"failed on 4\u00d74 symmetric\""}
{"task_id": 377, "completion_id": 0, "passed": false, "result": "failed: failed: mismatched lengths", "solution": "import numpy as np\nimport numpy as np\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Compute the negative gradient that Gradient Boosting uses when minimising the\n    logistic (binomial-deviance) loss for binary classification.\n    \n    Parameters\n    ----------\n    y : list or 1-D array-like of int (0 or 1)\n        Ground-truth binary labels.\n    f : list or 1-D array-like of float\n        Current prediction scores.\n    \n    Returns\n    -------\n    list\n        Negative gradients for each sample, rounded to 4 decimal places.\n        If `y` contains values other than 0 or 1, returns -1.\n    \"\"\"\n    y_arr = np.asarray(y)\n    f_arr = np.asarray(f, dtype=float)\n    if not np.all(np.isin(y_arr, [0, 1])):\n        return -1\n    y_prime = 2 * y_arr - 1\n    g = y_prime / (1.0 + np.exp(y_prime * f_arr))\n    return np.round(g, 4).tolist()\nassert logistic_negative_gradient([0,1,0],[0.0,0.5,-0.5])==[-0.5,0.3775,-0.3775],\"failed: basic example\"\nassert logistic_negative_gradient([1,1,1],[0,0,0])==[0.5,0.5,0.5],\"failed: all ones zero score\"\nassert logistic_negative_gradient([0,0,0],[0,0,0])==[-0.5,-0.5,-0.5],\"failed: all zeros zero score\"\nassert logistic_negative_gradient([1,0],[2,-2])==[0.1192,-0.1192],\"failed: symmetric scores\"\nassert logistic_negative_gradient([1],[10])==[0.0],\"failed: large positive score\"\nassert logistic_negative_gradient([0],[-10])==[-0.0],\"failed: large negative score\"\nassert logistic_negative_gradient([],[])==[],\"failed: empty input\"\nassert logistic_negative_gradient([0,2],[0,0])==-1,\"failed: invalid label\"\nassert logistic_negative_gradient([0,1,0],[0.1])==-1,\"failed: mismatched lengths\""}
{"task_id": 380, "completion_id": 0, "passed": false, "result": "failed: test failed: selu([-1,0,1],1)", "solution": "import numpy as np\nimport numpy as np\n_ALPHA = 1.6732632423543772\n_SCALE = 1.0507009873554805\ndef selu(x, order: int=0):\n    \"\"\"Compute the SELU activation or its first / second derivative.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Works for scalars and any-shaped arrays/lists.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 d(SELU)/dx\n        2 \u2192 d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    float | list\n        Result rounded to 4 decimals.  For array-like inputs the shape is\n        preserved and a Python list is returned; for scalar input a float\n        is returned.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=np.float64)\n    pos_mask = x_arr > 0\n    neg_mask = ~pos_mask\n    if order == 0:\n        out = np.empty_like(x_arr)\n        out[pos_mask] = _SCALE * x_arr[pos_mask]\n        out[neg_mask] = _SCALE * _ALPHA * (np.exp(x_arr[neg_mask]) - 1)\n    elif order == 1:\n        out = np.empty_like(x_arr)\n        out[pos_mask] = _SCALE\n        out[neg_mask] = _SCALE * _ALPHA * np.exp(x_arr[neg_mask])\n    elif order == 2:\n        out = np.empty_like(x_arr)\n        out[pos_mask] = 0.0\n        out[neg_mask] = _SCALE * _ALPHA * np.exp(x_arr[neg_mask])\n    else:\n        raise ValueError('order must be 0, 1 or 2')\n    rounded = np.round(out, 4)\n    return rounded.tolist()\nassert selu([-1, 0, 1], 0) == [-1.1113, 0.0, 1.0507], \"test failed: selu([-1,0,1],0)\"\nassert selu([2], 0) == [2.1014], \"test failed: selu([2],0)\"\nassert selu(0, 0) == 0.0, \"test failed: selu(0,0)\"\nassert selu([[1, 0], [-1, 2]], 0) == [[1.0507, 0.0], [-1.1113, 2.1014]], \"test failed: selu([[1,0],[-1,2]],0)\"\nassert selu([-1, 0, 1], 1) == [0.6468, 1.0507, 1.0507], \"test failed: selu([-1,0,1],1)\"\nassert selu([2.5], 1) == [1.0507], \"test failed: selu([2.5],1)\"\nassert selu(0, 1) == 1.0507, \"test failed: selu(0,1)\"\nassert selu([-1, 1], 2) == [0.6468, 0.0], \"test failed: selu([-1,1],2)\"\nassert selu([0], 2) == [1.7581], \"test failed: selu([0],2)\"\nassert selu(2, 2) == 0.0, \"test failed: selu(2,2)\""}
{"task_id": 387, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float=0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n = y.size\n    if n == 0:\n        return []\n    pred = np.full(n, y.mean())\n    for _ in range(n_estimators):\n        residuals = y - pred\n        if np.all(x == x[0]):\n            stump_out = np.full(n, residuals.mean())\n        else:\n            order = np.argsort(x)\n            xs = x[order]\n            rs = residuals[order]\n            csum = np.cumsum(rs)\n            csum_sq = np.cumsum(rs ** 2)\n            total_sum = csum[-1]\n            total_sq = csum_sq[-1]\n            k_left = np.arange(1, n)\n            k_right = n - k_left\n            sum_left = csum[:-1]\n            sum_sq_left = csum_sq[:-1]\n            sum_right = total_sum - sum_left\n            sum_sq_right = total_sq - sum_sq_left\n            mean_left = sum_left / k_left\n            mean_right = sum_right / k_right\n            sse_left = sum_sq_left - k_left * mean_left ** 2\n            sse_right = sum_sq_right - k_right * mean_right ** 2\n            total_sse = sse_left + sse_right\n            valid = xs[:-1] != xs[1:]\n            if not np.any(valid):\n                stump_out = np.full(n, residuals.mean())\n            else:\n                best_pos = np.argmin(np.where(valid, total_sse, np.inf))\n                thr = (xs[best_pos] + xs[best_pos + 1]) / 2.0\n                mu_left = mean_left[best_pos]\n                mu_right = mean_right[best_pos]\n                stump_out = np.where(x <= thr, mu_left, mu_right)\n        pred += learning_rate * stump_out\n    return np.round(pred, 4).tolist()\nassert gradient_boosting_1d([1, 2], [2, 4], 1, 0.5) == [2.5, 3.5], \"failed on simple two-point example\"\nassert gradient_boosting_1d([1, 2, 3, 4], [5, 5, 5, 5], 3, 0.1) == [5.0, 5.0, 5.0, 5.0], \"failed on constant target\"\nassert gradient_boosting_1d([1, 2, 3], [1, 2, 3], 2, 0.5) == [1.3125, 2.0625, 2.625], \"failed on small increasing sequence\"\nassert gradient_boosting_1d([1, 2, 3], [3, 3, 3], 5, 0.2) == [3.0, 3.0, 3.0], \"failed on constant target, many rounds\"\nassert gradient_boosting_1d([1, 2, 3, 4], [2, 4, 6, 8], 2, 0.5) == [3.0, 4.3333, 6.3333, 6.3333], \"failed on linear 4-point set\"\nassert gradient_boosting_1d([1, 2, 3], [2, 2, 6], 1, 1.0) == [2.0, 2.0, 6.0], \"failed on perfect one-round fit\"\nassert gradient_boosting_1d([1, 1, 1], [1, 2, 3], 3, 0.3) == [2.0, 2.0, 2.0], \"failed when all features identical\"\nassert gradient_boosting_1d([5, 6], [10, 10], 4, 0.1) == [10.0, 10.0], \"failed on two identical targets\"\nassert gradient_boosting_1d([1, 2, 3], [10, 0, 10], 1, 0.5) == [8.3333, 5.8333, 5.8333], \"failed on uneven targets\"\nassert gradient_boosting_1d([1, 2], [0, 0], 2, 0.4) == [0.0, 0.0], \"failed on zero targets\""}
{"task_id": 394, "completion_id": 0, "passed": false, "result": "failed: can't multiply sequence by non-int of type 'float'", "solution": "import numpy as np\nimport numpy as np\nimport numpy as np\ndef rmsprop_update(w: np.ndarray, grad: np.ndarray, Eg: np.ndarray | None=None, learning_rate: float=0.01, rho: float=0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` is used.\n    learning_rate : float, optional\n        Step size (\u03b1). Default is 0.01.\n    rho : float, optional\n        Decay rate (\u03c1). Default is 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        Tuple ``(w_next, Eg_next)`` \u2013 both rounded to four decimal places and\n        converted to regular Python lists.\n    \"\"\"\n    if Eg is None:\n        Eg = np.zeros_like(grad, dtype=float)\n    epsilon = 1e-08\n    Eg_next = rho * Eg + (1.0 - rho) * grad ** 2\n    w_next = w - learning_rate * grad / np.sqrt(Eg_next + epsilon)\n    w_next_rounded = np.round(w_next, 4).tolist()\n    Eg_next_rounded = np.round(Eg_next, 4).tolist()\n    return (w_next_rounded, Eg_next_rounded)\nassert rmsprop_update(np.array([1.0]), np.array([0.1]), None) == ([0.9684], [0.001]), \"test case 1 failed\"\nassert rmsprop_update(np.array([1.0]), np.array([0.0]), None) == ([1.0], [0.0]), \"test case 2 failed\"\nassert rmsprop_update(np.array([0.0]), np.array([1.0]), None) == ([-0.0316], [0.1]), \"test case 3 failed\"\nw4, Eg4 = rmsprop_update(np.array([0.5, -0.5]), np.array([0.2, 0.2]), None)\nassert w4 == [0.4684, -0.5316] and Eg4 == [0.004, 0.004], \"test case 4 failed\"\nassert rmsprop_update(np.array([0.4684, -0.5316]), np.array([0.0, 0.0]), [0.004, 0.004]) == ([0.4684, -0.5316], [0.0036, 0.0036]), \"test case 5 failed\"\nassert rmsprop_update(np.array([10.0]), np.array([10.0]), None) == ([9.9684], [10.0]), \"test case 6 failed\"\nassert rmsprop_update(np.array([1, 2, 3]), np.array([0.1, 0.2, 0.3]), None) == ([0.9684, 1.9684, 2.9684], [0.001, 0.004, 0.009]), \"test case 7 failed\"\nassert rmsprop_update(np.array([-1.0]), np.array([-0.1]), None) == ([-0.9684], [0.001]), \"test case 8 failed\"\nassert rmsprop_update(np.array([1.0, 2.0]), np.array([0.0, 0.0]), [0.001, 0.004]) == ([1.0, 2.0], [0.0009, 0.0036]), \"test case 9 failed\"\nassert rmsprop_update(np.array([5.0]), np.array([0.5]), np.array([1.0])) == ([4.9948], [0.925]), \"test case 10 failed\""}
{"task_id": 398, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef elu(x, alpha: float=0.1, derivative: bool=False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient element-wise.\n\n    Args:\n        x: 1-D list or NumPy array of numeric values.\n        alpha: Positive ELU hyper-parameter (default 0.1).\n        derivative: If True, return the gradient; otherwise return the activation.\n\n    Returns:\n        Python list of floats rounded to 4 decimal places.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=float)\n    pos_mask = x_arr >= 0\n    neg_mask = ~pos_mask\n    if not derivative:\n        out = x_arr.copy()\n        out[neg_mask] = alpha * (np.exp(x_arr[neg_mask]) - 1)\n    else:\n        out = np.ones_like(x_arr)\n        out[neg_mask] = alpha * np.exp(x_arr[neg_mask])\n    return np.round(out, 4).tolist()\nassert elu(np.array([-2, -1, 0, 1, 2]), 0.1, False) == [-0.0865, -0.0632, 0.0, 1.0, 2.0], \"failed on basic activation\"\nassert elu(np.array([-2, -1, 0, 1, 2]), 0.1, True) == [0.0135, 0.0368, 1.0, 1.0, 1.0], \"failed on basic gradient\"\nassert elu([-1], 1.0, False) == [-0.6321], \"failed on alpha=1 activation\"\nassert elu([-1], 1.0, True) == [0.3679], \"failed on alpha=1 gradient\"\nassert elu([3, 4.5, 0.0], 0.5, False) == [3.0, 4.5, 0.0], \"failed on all positive activation\"\nassert elu([3, 4.5, 0.0], 0.5, True) == [1.0, 1.0, 1.0], \"failed on all positive gradient\"\nassert elu([-1, 2], 0.0, False) == [0.0, 2.0], \"failed on alpha=0 activation\"\nassert elu([-1, 2], 0.0, True) == [0.0, 1.0], \"failed on alpha=0 gradient\"\nassert elu([], 0.1, False) == [], \"failed on empty activation\"\nassert elu([-0.5, 0.5], 0.2, False) == [-0.0787, 0.5], \"failed on mixed list activation\""}
{"task_id": 411, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom numbers import Integral, Real\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute simple statistics of action / observation trajectories.\"\"\"\n\n    def is_iterable(x):\n        \"\"\"Return True if *x* is a list/tuple/ndarray with length.\"\"\"\n        return isinstance(x, (list, tuple, np.ndarray)) and (not np.isscalar(x))\n\n    def analyse(collection):\n        \"\"\"\n        Inspect a list of (possibly-vector) values and return\n        tuple_flag  \u2026 at least one element is an iterable\n        multi_dim   \u2026 dimensionality > 1\n        continuous  \u2026 at least one value is a non-integer float\n        n_per_dim   \u2026 list with #unique values per dimension\n        dim         \u2026 dimensionality\n        ids         \u2026 list with sorted unique ids per dimension\n        \"\"\"\n        if len(collection) == 0:\n            raise ValueError('Empty trajectory supplied.')\n        tuple_flag = any((is_iterable(x) for x in collection))\n        if tuple_flag:\n            for sample in collection:\n                if is_iterable(sample):\n                    dim = len(sample)\n                    break\n        else:\n            dim = 1\n        multi_dim = dim > 1\n        uniq_sets = [set() for _ in range(dim)]\n        continuous = False\n        for sample in collection:\n            if tuple_flag:\n                values = list(sample)\n                if len(values) != dim:\n                    raise ValueError('Inconsistent dimensionality in data.')\n            else:\n                values = [sample]\n            for (d, val) in enumerate(values):\n                uniq_sets[d].add(val)\n                if not continuous:\n                    if isinstance(val, (Real, np.floating, np.integer)):\n                        if isinstance(val, (Integral, np.integer)):\n                            continue\n                        if not float(val).is_integer():\n                            continuous = True\n                    else:\n                        continuous = True\n        ids = [sorted(list(s)) for s in uniq_sets]\n        n_per_dim = [len(s) for s in uniq_sets]\n        return (tuple_flag, multi_dim, continuous, n_per_dim, dim, ids)\n    (tuple_a, multi_a, cont_a, n_a, dim_a, ids_a) = analyse(actions)\n    (tuple_o, multi_o, cont_o, n_o, dim_o, ids_o) = analyse(observations)\n    return {'tuple_actions': tuple_a, 'tuple_observations': tuple_o, 'multidim_actions': multi_a, 'multidim_observations': multi_o, 'continuous_actions': cont_a, 'continuous_observations': cont_o, 'n_actions_per_dim': n_a, 'action_dim': dim_a, 'n_obs_per_dim': n_o, 'obs_dim': dim_o, 'action_ids': ids_a, 'obs_ids': ids_o}\nacts = [0, 1, 2, 0]\nobs  = [10, 11, 10]\nassert env_stats(acts, obs)['n_actions_per_dim'] == [3], \"test case failed: scalar integers\"\nacts = [0.0, 1.5, 2.0]\nobs  = [1, 2, 3]\nassert env_stats(acts, obs)['continuous_actions'] is True, \"test case failed: continuous actions\"\nacts = [(0, 0), (1, 1), (0, 1)]\nobs  = [5, 6, 7]\nassert env_stats(acts, obs)['multidim_actions'] is True, \"test case failed: 2-D integer actions\"\nacts = [(0, 0, 0)]\nobs  = [(1.1, 2.2)]\nassert env_stats(acts, obs)['obs_dim'] == 2, \"test case failed: single sample multi-dim\"\nacts = [[1], [2], [3], [1]]\nobs  = [[10], [10], [12]]\nassert env_stats(acts, obs)['tuple_actions'] is True, \"test case failed: tuple single-dim\"\nacts = [(1, 2), (2, 3), (1, 3)]\nobs  = [0.1, 0.2, 0.3]\nassert env_stats(acts, obs)['continuous_observations'] is True, \"test 6 failed\"\nacts = [1, 2, 3]\nobs  = [(0, 0), (1, 1)]\nassert env_stats(acts, obs)['multidim_observations'] is True, \"test 7 failed\"\nacts = [2.0, 3.0]\nobs  = [1, 1]\nassert env_stats(acts, obs)['continuous_actions'] is False, \"test 8 failed\"\nacts = [True, False, True]\nobs  = [0, 1, 0]\nassert env_stats(acts, obs)['continuous_actions'] is False, \"test 9 failed\"\nacts = list(range(100))\nobs  = list(range(0, 200, 2))\nassert env_stats(acts, obs)['n_obs_per_dim'] == [100], \"test 10 failed\""}
{"task_id": 413, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nclass _TreeNode:\n    \"\"\"A single node in a CART regression tree.\"\"\"\n    __slots__ = ('is_leaf', 'value', 'feature', 'threshold', 'left', 'right')\n\n    def __init__(self, *, value=None, feature=None, threshold=None, left=None, right=None, is_leaf=False):\n        self.is_leaf = is_leaf\n        self.value = value\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\nclass _CARTRegressor:\n    \"\"\"Very small, depth\u2013limited CART regressor (MSE criterion).\"\"\"\n\n    def __init__(self, max_depth: int=3):\n        self.max_depth = max_depth\n        self.root: _TreeNode | None = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        self.root = self._grow(X, y, depth=0)\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        preds = np.empty(X.shape[0], dtype=float)\n        for (i, x) in enumerate(X):\n            preds[i] = self._predict_row(self.root, x)\n        return preds\n\n    def _predict_row(self, node: _TreeNode, x: np.ndarray) -> float:\n        while not node.is_leaf:\n            node = node.left if x[node.feature] <= node.threshold else node.right\n        return node.value\n\n    def _grow(self, X: np.ndarray, y: np.ndarray, depth: int) -> _TreeNode:\n        (n_samples, n_feats) = X.shape\n        if depth >= self.max_depth or n_samples <= 1 or np.all(y == y[0]):\n            return _TreeNode(value=float(y.mean()), is_leaf=True)\n        best_sse = np.var(y) * n_samples\n        (best_feat, best_thr, best_split) = (None, None, None)\n        for feat in range(n_feats):\n            x_col = X[:, feat]\n            sorted_idx = np.argsort(x_col)\n            (x_sorted, y_sorted) = (x_col[sorted_idx], y[sorted_idx])\n            uniq_vals = np.unique(x_sorted)\n            if uniq_vals.size == 1:\n                continue\n            thr_candidates = (uniq_vals[:-1] + uniq_vals[1:]) / 2.0\n            y_cumsum = np.cumsum(y_sorted)\n            y_sq_cumsum = np.cumsum(y_sorted ** 2)\n            for (idx, thr) in enumerate(thr_candidates, start=1):\n                n_left = np.searchsorted(x_sorted, thr, side='right')\n                if n_left == 0 or n_left == n_samples:\n                    continue\n                sum_y_left = y_cumsum[n_left - 1]\n                sum_yy_left = y_sq_cumsum[n_left - 1]\n                n_l = n_left\n                mse_left = sum_yy_left - sum_y_left ** 2 / n_l\n                sum_y_right = y_cumsum[-1] - sum_y_left\n                sum_yy_right = y_sq_cumsum[-1] - sum_yy_left\n                n_r = n_samples - n_l\n                mse_right = sum_yy_right - sum_y_right ** 2 / n_r\n                sse_split = mse_left + mse_right\n                if sse_split < best_sse - 1e-07:\n                    best_sse = sse_split\n                    best_feat = feat\n                    best_thr = thr\n                    best_split = (n_left, sorted_idx)\n        if best_feat is None:\n            return _TreeNode(value=float(y.mean()), is_leaf=True)\n        (split_idx, sorted_idx) = best_split\n        x_sorted_global = X[sorted_idx, best_feat]\n        mask_left = x_sorted_global <= best_thr\n        left_idx = sorted_idx[mask_left]\n        right_idx = sorted_idx[~mask_left]\n        left_child = self._grow(X[left_idx], y[left_idx], depth + 1)\n        right_child = self._grow(X[right_idx], y[right_idx], depth + 1)\n        return _TreeNode(feature=best_feat, threshold=best_thr, left=left_child, right=right_child, value=float(y.mean()), is_leaf=False)\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int=10, learning_rate: float=0.1, max_depth: int=3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor (simplified).\"\"\"\n    init_pred = y_train.mean()\n    y_hat_train = np.full_like(y_train, init_pred, dtype=float)\n    y_hat_test = np.full(X_test.shape[0], init_pred, dtype=float)\n    for _ in range(n_estimators):\n        residuals = y_train - y_hat_train\n        tree = _CARTRegressor(max_depth=max_depth)\n        tree.fit(X_train, residuals)\n        y_hat_train += learning_rate * tree.predict(X_train)\n        y_hat_test += learning_rate * tree.predict(X_test)\n    return np.round(y_hat_test, 4).tolist()\nassert gbdt_regression(np.array([[0],[1]]), np.array([0,1]), np.array([[0],[1]]), 1, 1.0, 1) == [0.0, 1.0], \"test-1 failed: two-point identity mapping\"\nassert gbdt_regression(np.array([[0],[1],[2]]), np.array([5,5,5]), np.array([[1]]), 3, 0.5, 1) == [5.0], \"test-2 failed: constant target 5\"\nassert gbdt_regression(np.array([[0,1],[1,2],[2,3]]), np.array([7,7,7]), np.array([[0.5,1.5]]), 5, 0.3, 2) == [7.0], \"test-3 failed: 2-D constant target 7\"\nassert gbdt_regression(np.array([[-1],[0],[1]]), np.array([-2,-2,-2]), np.array([[0]]), 4, 0.8, 1) == [-2.0], \"test-4 failed: negative constant target\"\nassert gbdt_regression(np.array([[10]]), np.array([42]), np.array([[15]]), 6, 0.2, 1) == [42.0], \"test-5 failed: single-sample dataset\"\nassert gbdt_regression(np.array([[2],[4]]), np.array([4,8]), np.array([[2],[4]]), 1, 1.0, 1) == [4.0, 8.0], \"test-6 failed: two-point linear x2 mapping\"\nassert gbdt_regression(np.array([[0,1],[1,2]]), np.array([1,3]), np.array([[0,1],[1,2]]), 1, 1.0, 1) == [1.0, 3.0], \"test-7 failed: two-point 2-D features\"\nassert gbdt_regression(np.array([[5]]), np.array([10]), np.array([[7]]), 3, 0.9, 1) == [10.0], \"test-8 failed: single-sample constant 10\"\nassert gbdt_regression(np.array([[0],[1],[2]]), np.array([0,0,0]), np.array([[0],[2]]), 2, 0.7, 1) == [0.0, 0.0], \"test-9 failed: zero target\"\nassert gbdt_regression(np.array([[0],[1]]), np.array([-5,10]), np.array([[0],[1]]), 1, 1.0, 1) == [-5.0, 10.0], \"test-10 failed: mixed sign targets\""}
{"task_id": 416, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Evaluate the multivariate normal (Gaussian) probability-density function for\n    every sample contained in `X`.\n\n    Parameters\n    ----------\n    X : array_like, shape (n_samples, n_features)  or  (n_samples,)\n        Data points at which the PDF will be evaluated.  If `X` is one\u2013dimensional\n        it is treated as having shape (n_samples, 1).\n    mean : array_like, shape (n_features,)\n        Mean vector (\u03bc) of the distribution.\n    cov : array_like, shape (n_features, n_features)\n        Positive-definite covariance matrix (\u03a3) of the distribution.\n\n    Returns\n    -------\n    list\n        A list whose *i-th* element is the PDF value for `X[i]`, rounded to\n        four decimal places.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    mean = np.asarray(mean, dtype=float)\n    cov = np.asarray(cov, dtype=float)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    if mean.shape[0] != n_features:\n        raise ValueError('Mean vector length must equal the number of features.')\n    if cov.shape != (n_features, n_features):\n        raise ValueError('Covariance matrix shape must be (n_features, n_features).')\n    inv_cov = np.linalg.inv(cov)\n    det_cov = np.linalg.det(cov)\n    if det_cov <= 0:\n        raise ValueError('Covariance matrix must be positive-definite (determinant > 0).')\n    coeff = 1.0 / math.sqrt((2 * math.pi) ** n_features * det_cov)\n    diff = X - mean\n    mahal = np.einsum('ij,jk,ik->i', diff, inv_cov, diff)\n    exponent = -0.5 * mahal\n    pdf_vals = coeff * np.exp(exponent)\n    return [round(val, 4) for val in pdf_vals.tolist()]\nassert multivariate_gaussian_pdf(np.array([[0]]), [0], [[1]]) == [0.3989], \"test case failed: N(0,1) at 0\"\nassert multivariate_gaussian_pdf(np.array([[1]]), [0], [[1]]) == [0.2420], \"test case failed: N(0,1) at 1\"\nassert multivariate_gaussian_pdf(np.array([[0,0]]), [0,0], [[1,0],[0,1]]) == [0.1592], \"test case failed: 2D standard normal at (0,0)\"\nassert multivariate_gaussian_pdf(np.array([[1,1]]), [0,0], [[1,0],[0,1]]) == [0.0585], \"test case failed: 2D standard normal at (1,1)\"\nassert multivariate_gaussian_pdf(np.array([[0,0,0]]), [0,0,0], np.identity(3)) == [0.0635], \"test case failed: 3D standard normal at origin\"\nassert multivariate_gaussian_pdf(np.array([[0,0]]), [0,0], [[2,0],[0,2]]) == [0.0796], \"test case failed: 2D diag(2,2) at origin\"\nassert multivariate_gaussian_pdf(np.array([[1,0]]), [0,0], [[1,0],[0,2]]) == [0.0683], \"test case failed: diag(1,2) at (1,0)\"\nassert multivariate_gaussian_pdf(np.array([[1,0],[0,1]]), [0,0], [[1,0],[0,2]]) == [0.0683,0.0876], \"test case failed: two points with diag(1,2)\"\nassert multivariate_gaussian_pdf(np.array([[0,0],[2,0]]), [0,0], [[1,0],[0,1]]) == [0.1592,0.0215], \"test case failed: (0,0) and (2,0) in 2D standard normal\"\nassert multivariate_gaussian_pdf(np.array([[-1],[0],[1]]), [0], [[1]]) == [0.2420,0.3989,0.2420], \"test case failed: vectorised 1D standard normal\""}
{"task_id": 419, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nimport numpy as np\ndef bayesian_linear_regression_pred(X: np.ndarray, y: np.ndarray, X_new: np.ndarray, mu: float | np.ndarray=0.0, sigma: float=1.0, V: float | np.ndarray | None=None, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Bayesian linear regression \u2013 MAP predictive mean (rounded to 4 dp).\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    X_new = np.asarray(X_new, dtype=float)\n    if fit_intercept:\n        ones_train = np.ones((X.shape[0], 1))\n        ones_test = np.ones((X_new.shape[0], 1))\n        X = np.hstack((ones_train, X))\n        X_new = np.hstack((ones_test, X_new))\n    (N, D) = X.shape\n    if np.isscalar(mu):\n        mu_vec = np.full(D, mu, dtype=float)\n    else:\n        mu_vec = np.asarray(mu, dtype=float).ravel()\n        if mu_vec.size != D:\n            raise ValueError('`mu` has incompatible length.')\n    if V is None:\n        V_inv = np.eye(D)\n    elif np.isscalar(V):\n        if V == 0:\n            raise ValueError('Scalar `V` must be non-zero.')\n        V_inv = np.eye(D) / float(V)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 1:\n            if np.any(V_arr == 0):\n                raise ValueError('Zero on the diagonal of `V` is not allowed.')\n            V_inv = np.diag(1.0 / V_arr)\n        else:\n            if V_arr.shape != (D, D):\n                raise ValueError('`V` shape incompatible with number of features.')\n            V_inv = np.linalg.inv(V_arr)\n    XtX = X.T @ X\n    A = V_inv + XtX\n    Sigma_p = np.linalg.inv(A)\n    rhs = V_inv @ mu_vec + X.T @ y\n    mu_p = Sigma_p @ rhs\n    y_pred = X_new @ mu_p\n    return [round(float(val), 4) for val in y_pred]\nassert bayesian_linear_regression_pred(np.array([[1],[2]]), np.array([1,3]), np.array([[3]])) == [3.3333], \"failed on simple 1-D, intercept\"\nassert bayesian_linear_regression_pred(np.array([[0],[1],[2]]), np.array([1,2,3]), np.array([[1.5]])) == [2.2], \"failed on 3-pt line, intercept\"\nassert bayesian_linear_regression_pred(np.array([[1],[2]]), np.array([2,4]), np.array([[3]]), mu=1) == [5.0], \"failed with non-zero prior mean\"\nassert bayesian_linear_regression_pred(np.array([[1],[2],[3]]), np.array([2,2.5,3.5]), np.array([[4]]), V=[2,2]) == [4.339], \"failed with diagonal prior covariance\"\nassert bayesian_linear_regression_pred(np.array([[1]]), np.array([2]), np.array([[1]]), fit_intercept=False) == [1.0], \"failed single observation, no intercept\"\nassert bayesian_linear_regression_pred(np.array([[1],[2],[3]]), np.array([3,6,9]), np.array([[4]]), V=0.5, fit_intercept=False) == [10.5], \"failed with scalar prior variance 0.5\"\nassert bayesian_linear_regression_pred(np.array([[1],[2],[3]]), np.array([2,4,6]), np.array([[4]]), mu=2, fit_intercept=False) == [8.0], \"failed with informative prior mean\"\nassert bayesian_linear_regression_pred(np.array([[0],[1]]), np.array([0,1]), np.array([[2]])) == [1.0], \"failed on small line through origin\"\nassert bayesian_linear_regression_pred(np.array([[0],[0]]), np.array([2,2]), np.array([[0]])) == [1.3333], \"failed pure-intercept model\""}
{"task_id": 423, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute Root Mean Squared Logarithmic Error (RMSLE).\n\n    Returns\n    -------\n    float\n        RMSLE rounded to four decimal places, or -1 if validation fails.\n    \"\"\"\n    if not isinstance(actual, (list, tuple, np.ndarray)) or not isinstance(predicted, (list, tuple, np.ndarray)) or len(actual) != len(predicted) or (len(actual) == 0):\n        return -1\n    try:\n        actual_arr = np.asarray(actual, dtype=float)\n        predicted_arr = np.asarray(predicted, dtype=float)\n    except (ValueError, TypeError):\n        return -1\n    if np.any(actual_arr < 0) or np.any(predicted_arr < 0):\n        return -1\n    log_diff = np.log1p(predicted_arr) - np.log1p(actual_arr)\n    rmsle = np.sqrt(np.mean(np.square(log_diff)))\n    return round(float(rmsle), 4)\nassert root_mean_squared_log_error([1, 2, 3], [1, 2, 3]) == 0.0, \"failed on identical values\"\nassert root_mean_squared_log_error([1, 2, 3], [1, 2, 4]) == 0.1288, \"failed on simple differing list\"\nassert root_mean_squared_log_error([0], [0]) == 0.0, \"failed on single zero value\"\nassert root_mean_squared_log_error([], []) == -1, \"failed on empty lists\"\nassert root_mean_squared_log_error([1, 2], [1]) == -1, \"failed on unequal length\"\nassert root_mean_squared_log_error([1, -2, 3], [1, 2, 3]) == -1, \"failed on negative actual value\"\nassert root_mean_squared_log_error([1, 2, 3], [1, 2, -3]) == -1, \"failed on negative predicted value\""}
{"task_id": 428, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef hann(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine\u2010sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places. Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    build_len = window_len if symmetric else window_len + 1\n    n = np.arange(build_len, dtype=float)\n    denom = build_len - 1\n    window = 0.5 - 0.5 * np.cos(2.0 * math.pi * n / denom)\n    if not symmetric:\n        window = window[:-1]\n    window = np.round(window, 4)\n    return window.tolist()\nassert hann(4, True) == [0.0, 0.75, 0.75, 0.0], \"test case failed: hann(4, True)\"\nassert hann(4, False) == [0.0, 0.5, 1.0, 0.5], \"test case failed: hann(4, False)\"\nassert hann(1, True) == [1.0], \"test case failed: hann(1, True)\"\nassert hann(0, True) == [], \"test case failed: hann(0, True)\"\nassert hann(5, True) == [0.0, 0.5, 1.0, 0.5, 0.0], \"test case failed: hann(5, True)\"\nassert hann(5, False) == [0.0, 0.3455, 0.9045, 0.9045, 0.3455], \"test case failed: hann(5, False)\"\nassert hann(6, True)[0] == 0.0 and hann(6, True)[-1] == 0.0, \"test case failed: end points not zero for symmetric window\"\nassert abs(sum(hann(10, True))) < 7.0, \"test case failed: unrealistic sum for symmetric window\"\nassert hann(2, False) == [0.0, 1.0], \"test case failed: hann(2, False)\""}
{"task_id": 433, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute Xavier/Glorot fan-in and fan-out values for a weight tensor.\n\n    Args:\n        shape: Tensor shape. For 4-D convolutional kernels the expected layout\n               is (out_channels, in_channels, kernel_height, kernel_width).\n\n    Returns:\n        A tuple (fan_in, fan_out) as floats.\n    \"\"\"\n    if len(shape) == 4:\n        (out_c, in_c, k_h, k_w) = shape[:4]\n        receptive_field_size = k_h * k_w\n        fan_in = in_c * receptive_field_size\n        fan_out = out_c * receptive_field_size\n    else:\n        (fan_in, fan_out) = (shape[0], shape[1])\n    return (float(fan_in), float(fan_out))\nassert glorot_fan((64, 3, 7, 7)) == (147.0, 3136.0), \"failed: glorot_fan((64, 3, 7, 7))\"\nassert glorot_fan((128, 256)) == (128.0, 256.0), \"failed: glorot_fan((128, 256))\"\nassert glorot_fan((256, 128)) == (256.0, 128.0), \"failed: glorot_fan((256, 128))\"\nassert glorot_fan((10, 20, 30)) == (10.0, 20.0), \"failed: glorot_fan((10, 20, 30))\"\nassert glorot_fan((32, 3, 3, 3)) == (27.0, 288.0), \"failed: glorot_fan((32, 3, 3, 3))\"\nassert glorot_fan((1, 1, 1, 1)) == (1.0, 1.0), \"failed: glorot_fan((1, 1, 1, 1))\"\nassert glorot_fan((4, 2, 5, 5)) == (50.0, 100.0), \"failed: glorot_fan((4, 2, 5, 5))\"\nassert glorot_fan((2, 4)) == (2.0, 4.0), \"failed: glorot_fan((2, 4))\"\nassert glorot_fan([5, 6, 7, 8, 9]) == (5.0, 6.0), \"failed: glorot_fan([5, 6, 7, 8, 9])\"\nassert glorot_fan((3, 5, 1, 1)) == (5.0, 3.0), \"failed: glorot_fan((3, 5, 1, 1))\""}
{"task_id": 435, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gradient_boosting_regressor(X: list[list[float]], y: list[float], n_estimators: int=10, learning_rate: float=0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear-regression weak learners and squared-error loss.\n    All returned numbers are rounded to 4 decimal places and packed into\n    a plain Python list.\n    \"\"\"\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(-1, 1)\n    m = y_arr.shape[0]\n    y_mean = y_arr.mean()\n    if n_estimators <= 0 or learning_rate == 0:\n        return np.round(np.full(m, y_mean), 4).tolist()\n    current_pred = np.full(m, y_mean, dtype=float)\n    ones = np.ones((m, 1), dtype=float)\n    X_design = np.concatenate((ones, X_arr), axis=1)\n    for _ in range(n_estimators):\n        residuals = y_arr - current_pred\n        (beta, _, _, _) = np.linalg.lstsq(X_design, residuals, rcond=None)\n        h_pred = X_design @ beta\n        current_pred += learning_rate * h_pred\n    return np.round(current_pred, 4).tolist()\nassert gradient_boosting_regressor([[1],[2],[3],[4]],[2,3,4,5],2,0.5) == [2.375,3.125,3.875,4.625], \"failed on test 1\"\nassert gradient_boosting_regressor([[0],[1],[2],[3]],[1,3,5,7],1,1.0) == [1.0,3.0,5.0,7.0], \"failed on test 2\"\nassert gradient_boosting_regressor([[0],[1],[2],[3]],[1,3,5,7],1,0.5) == [2.5,3.5,4.5,5.5], \"failed on test 3\"\nassert gradient_boosting_regressor([[1,0],[0,1],[1,1]],[1,2,3],1,1.0) == [1.0,2.0,3.0], \"failed on test 4\"\nassert gradient_boosting_regressor([[1,2],[3,4]],[5,5],3,0.3) == [5.0,5.0], \"failed on test 5\"\nassert gradient_boosting_regressor([[1],[2]],[1,2],5,0.0) == [1.5,1.5], \"failed on test 6\"\nassert gradient_boosting_regressor([[10],[20],[30]],[3,6,9],0,0.2) == [6.0,6.0,6.0], \"failed on test 7\"\nassert gradient_boosting_regressor([[1],[2],[3]],[2,4,6],-4,0.3) == [4.0,4.0,4.0], \"failed on test 8\"\nassert gradient_boosting_regressor([[0],[1],[2]],[1,3,7],1,1.0) == [0.6667,3.6667,6.6667], \"failed on test 9\"\nassert gradient_boosting_regressor([[0],[0],[0]],[2,4,6],2,0.7) == [4.0,4.0,4.0], \"failed on test 10\""}
{"task_id": 437, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(x, order: int=0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in (0, 1, 2):\n        return -1\n    arr = np.asarray(x, dtype=float)\n    sig = 1.0 / (1.0 + np.exp(-arr))\n    if order == 0:\n        res = sig\n    elif order == 1:\n        res = sig * (1.0 - sig)\n    else:\n        res = sig * (1.0 - sig) * (1.0 - 2.0 * sig)\n    res = np.round(res, 4)\n    if res.shape == ():\n        return float(res)\n    return res.tolist()\nassert sigmoid([-1, 0, 1]) == [0.2689, 0.5, 0.7311], \"test failed: sigmoid([-1,0,1])\"\nassert sigmoid(0) == 0.5, \"test failed: sigmoid(0)\"\nassert sigmoid([0], order=1) == [0.25], \"test failed: sigmoid([0], order=1)\"\nassert sigmoid(0, order=1) == 0.25, \"test failed: sigmoid(0, order=1)\"\nassert sigmoid([0], order=2) == [0.0], \"test failed: sigmoid([0], order=2)\"\nassert sigmoid([-1, 2], order=1) == [0.1966, 0.105], \"test failed: sigmoid([-1,2], order=1)\"\nassert sigmoid([-2, 2], order=2) == [0.08, -0.08], \"test failed: sigmoid([-2,2], order=2)\"\nassert sigmoid(0, order=3) == -1, \"test failed: sigmoid(0, order=3)\"\nassert sigmoid([[0, 1], [-1, -2]]) == [[0.5, 0.7311], [0.2689, 0.1192]], \"test failed: sigmoid(2D array)\"\nassert sigmoid(-1000) == 0.0, \"test failed: sigmoid(-1000)\""}
{"task_id": 438, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None=None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded representation of *y*, or ``-1`` when the given\n        *num_classes* is smaller than required.\n    \"\"\"\n    y_arr = np.asarray(y, dtype=int).ravel()\n    if y_arr.size == 0:\n        inferred_classes = 0 if num_classes is None else num_classes\n        return [] if inferred_classes == 0 else [[] for _ in range(0)]\n    max_label = int(y_arr.max())\n    if num_classes is None:\n        num_classes = max_label + 1\n    elif num_classes < max_label + 1:\n        return -1\n    one_hot = np.zeros((y_arr.shape[0], num_classes), dtype=int)\n    one_hot[np.arange(y_arr.shape[0]), y_arr] = 1\n    return one_hot.tolist()\nassert to_categorical([0, 1, 2]) == [[1,0,0], [0,1,0], [0,0,1]], \"failed on basic sequential labels\"\nassert to_categorical([2, 0, 1], 3) == [[0,0,1], [1,0,0], [0,1,0]], \"failed when num_classes given\"\nassert to_categorical([1, 1, 1]) == [[0,1], [0,1], [0,1]], \"failed on identical labels\"\nassert to_categorical([0, 2], 4) == [[1,0,0,0], [0,0,1,0]], \"failed on explicit num_classes > max_label\"\nassert to_categorical([0]) == [[1]], \"failed on single label\"\nassert to_categorical([0, 1, 2], 2) == -1, \"failed on too small num_classes\"\nassert to_categorical([1, 3]) == [[0,1,0,0], [0,0,0,1]], \"failed on non-consecutive labels\"\nimport numpy as np\nassert to_categorical(np.array([3,0,2])) == [[0,0,0,1],[1,0,0,0],[0,0,1,0]], \"failed on numpy input\"\nassert to_categorical([5,4],6) == [[0,0,0,0,0,1],[0,0,0,0,1,0]], \"failed on high labels\"\nassert to_categorical([],0) == [], \"failed on empty input\""}
{"task_id": 439, "completion_id": 0, "passed": true, "result": "passed", "solution": "import itertools\nimport itertools\nimport math\nfrom collections import defaultdict\nfrom typing import Hashable, List, Tuple\ndef apriori_frequent_itemsets(transactions: List[List[Hashable]], min_sup: float) -> List[Tuple]:\n    \"\"\"Find all frequent item-sets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        The data base: one list per transaction, containing *hashable* items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction in (0, 1].\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets ordered first by their length and then\n        lexicographically inside each length block; inside every tuple the items\n        themselves appear in ascending (lexicographic) order.\n    \"\"\"\n    if not 0 < min_sup <= 1:\n        raise ValueError('min_sup must be in the range (0, 1].')\n    n_transactions = len(transactions)\n    if n_transactions == 0:\n        return []\n    transactions = [set(t) for t in transactions]\n\n    def is_frequent(count: int) -> bool:\n        \"\"\"True if empirical support \u2265 min_sup.\"\"\"\n        return count >= min_sup * n_transactions - 1e-12\n    item_counts: defaultdict[Hashable, int] = defaultdict(int)\n    for t in transactions:\n        for item in t:\n            item_counts[item] += 1\n    L1 = [tuple([item]) for (item, cnt) in item_counts.items() if is_frequent(cnt)]\n    L1.sort()\n    frequent_itemsets: list[tuple] = []\n    frequent_itemsets.extend(L1)\n    prev_L = L1\n    k = 2\n    while prev_L:\n        prev_L_len = len(prev_L)\n        prev_L_set = set(prev_L)\n        Ck: set[tuple] = set()\n        for i in range(prev_L_len):\n            for j in range(i + 1, prev_L_len):\n                (a, b) = (prev_L[i], prev_L[j])\n                if a[:-1] != b[:-1]:\n                    break\n                candidate = tuple(sorted(set(a) | set(b)))\n                if len(candidate) != k:\n                    continue\n                if all((tuple(subset) in prev_L_set for subset in itertools.combinations(candidate, k - 1))):\n                    Ck.add(candidate)\n        if not Ck:\n            break\n        candidate_counts = defaultdict(int)\n        cand_sets = {cand: frozenset(cand) for cand in Ck}\n        for t in transactions:\n            for (cand, cand_set) in cand_sets.items():\n                if cand_set.issubset(t):\n                    candidate_counts[cand] += 1\n        Lk = [cand for (cand, cnt) in candidate_counts.items() if is_frequent(cnt)]\n        Lk.sort()\n        if not Lk:\n            break\n        frequent_itemsets.extend(Lk)\n        prev_L = Lk\n        k += 1\n    return frequent_itemsets\nassert apriori_frequent_itemsets([[1,2,3],[1,2],[2,3],[1,3]],0.5)==[(1,),(2,),(3,),(1,2),(1,3),(2,3)],\"failed: basic example\"\nassert apriori_frequent_itemsets([],0.5)==[],\"failed: empty dataset\"\nassert apriori_frequent_itemsets([[\"a\",\"b\",\"c\"],[\"a\",\"b\"],[\"a\",\"c\"],[\"b\",\"c\"]],0.75)==[(\"a\",),(\"b\",),(\"c\",)],\"failed: high threshold\"\nassert apriori_frequent_itemsets([[1,2],[1,3],[2,3],[1,2,3]],0.25)==[(1,),(2,),(3,),(1,2),(1,3),(2,3),(1,2,3)],\"failed: very low threshold\"\nassert apriori_frequent_itemsets([[1,2],[3,4],[5,6]],0.34)==[],\"failed: no frequent itemsets across disjoint transactions\"\nassert apriori_frequent_itemsets([[\"x\",\"y\"],[\"x\",\"y\"],[\"x\",\"y\"]],0.2)==[(\"x\",),(\"y\",),(\"x\",\"y\",)],\"failed: every item always occurs\""}
{"task_id": 440, "completion_id": 0, "passed": false, "result": "failed: name 'pred1' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class\u2013probability predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        Shape (n_estimators, n_samples, n_classes).  Each innermost list is a\n        valid probability distribution (sums to 1).\n\n    Returns\n    -------\n    tuple\n        (\n          averaged_probs,      # list[list[float]] shape (n_samples, n_classes)\n          predicted_labels     # list[int]          length == n_samples\n        )\n        `averaged_probs` are rounded to 4 decimal places; `predicted_labels`\n        are obtained with arg-max on the rounded probabilities.  Ties are\n        resolved by the smallest class index (numpy\u2019s default behaviour).\n    \"\"\"\n    if not predictions:\n        raise ValueError('`predictions` must contain at least one estimator.')\n    arr = np.asarray(predictions, dtype=float)\n    if arr.ndim != 3:\n        raise ValueError('`predictions` must be 3-dimensional (n_estimators, n_samples, n_classes).')\n    mean_probs = arr.mean(axis=0)\n    mean_probs = np.round(mean_probs, 4)\n    predicted = np.argmax(mean_probs, axis=1)\n    averaged_probs_list = mean_probs.tolist()\n    predicted_labels_list = predicted.tolist()\n    return (averaged_probs_list, predicted_labels_list)\nassert aggregate_predictions(pred1) == ([[0.8, 0.2], [0.3, 0.7]], [0, 1]), \"test case failed: basic example\"\nassert aggregate_predictions(pred2) == ([[0.25, 0.45, 0.3], [0.1, 0.25, 0.65], [0.75, 0.15, 0.1]], [1, 2, 0]), \"test case failed: 3-class aggregation\"\nassert aggregate_predictions(pred3) == ([[0.6, 0.4], [0.3, 0.7]], [0, 1]), \"test case failed: single estimator\"\nassert aggregate_predictions(pred4) == ([[0.125, 0.7, 0.175]], [1]), \"test case failed: four estimators one sample\"\nassert aggregate_predictions(pred5) == ([[0.55, 0.45], [0.45, 0.55]], [0, 1]), \"test case failed: symmetric probabilities\"\nassert aggregate_predictions(pred6) == ([[0.75, 0.25], [0.25, 0.75], [0.55, 0.45], [0.15, 0.85]], [0, 1, 0, 1]), \"test case failed: larger sample size\"\nassert aggregate_predictions(pred7) == ([[0.3333, 0.3333, 0.3333]], [0]), \"test case failed: uniform distribution tie\"\nassert aggregate_predictions(pred8) == ([[0.5, 0.5]], [0]), \"test case failed: exact tie\"\nassert aggregate_predictions(pred9) == ([[0.2, 0.8]], [1]), \"test case failed: five estimators\"\nassert aggregate_predictions(pred10) == ([[0.85, 0.15], [0.45, 0.55], [0.35, 0.65]], [0, 1, 1]), \"test case failed: mixed input\""}
{"task_id": 444, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rbf_kernel(X: list[list[int | float]], Y: list[list[int | float]] | None=None, sigma: float | list[float] | None=None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function follows the specifications given in the task description.\n    It returns -1 on invalid input, otherwise a nested list containing the\n    kernel matrix rounded to four decimal places.\n    \"\"\"\n\n    def _to_2d_array(mat):\n        try:\n            arr = np.asarray(mat, dtype=float)\n        except Exception:\n            return None\n        if arr.ndim != 2 or arr.shape[1] == 0:\n            return None\n        return arr\n    X_arr = _to_2d_array(X)\n    if X_arr is None:\n        return -1\n    (n_samples, n_features) = X_arr.shape\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        Y_arr = _to_2d_array(Y)\n        if Y_arr is None or Y_arr.shape[1] != n_features:\n            return -1\n    if sigma is None:\n        sigma_scalar = np.sqrt(n_features / 2.0)\n        if sigma_scalar <= 0:\n            return -1\n        sigma_vec = None\n    elif isinstance(sigma, (int, float, np.floating)):\n        sigma_scalar = float(sigma)\n        if sigma_scalar <= 0:\n            return -1\n        sigma_vec = None\n    else:\n        try:\n            sigma_vec = np.asarray(sigma, dtype=float)\n        except Exception:\n            return -1\n        if sigma_vec.ndim != 1 or sigma_vec.shape[0] != n_features:\n            return -1\n        if np.any(sigma_vec <= 0):\n            return -1\n        sigma_scalar = None\n    if sigma_vec is None:\n        diff = X_arr[:, None, :] - Y_arr[None, :, :]\n        dist2 = np.sum(diff ** 2, axis=-1) / sigma_scalar ** 2\n    else:\n        diff = (X_arr[:, None, :] - Y_arr[None, :, :]) / sigma_vec\n        dist2 = np.sum(diff ** 2, axis=-1)\n    K = np.exp(-0.5 * dist2)\n    return np.round(K, 4).tolist()\nassert rbf_kernel([[1,0],[0,1]], [[1,0],[0,1]], 1.0) == [[1.0, 0.3679], [0.3679, 1.0]], \"Test 1 failed: isotropic \u03c3=1.0\"\nassert rbf_kernel([[1,1],[2,2],[3,3]], None, None) == [[1.0, 0.3679, 0.0183], [0.3679, 1.0, 0.3679], [0.0183, 0.3679, 1.0]], \"Test 3 failed: default \u03c3\"\nassert rbf_kernel([[0,0],[1,1]], [[1,0]], 1.0) == [[0.6065], [0.6065]], \"Test 4 failed: X\u2260Y shapes\"\nassert rbf_kernel([[1,0],[0,1]], None, [2,2]) == [[1.0, 0.7788], [0.7788, 1.0]], \"Test 5 failed: larger anisotropic \u03c3\"\nassert rbf_kernel([[0],[1],[2]], None, 1.0) == [[1.0, 0.6065, 0.1353], [0.6065, 1.0, 0.6065], [0.1353, 0.6065, 1.0]], \"Test 6 failed: 1-D data\"\nassert rbf_kernel([[0,0,0]], None, None) == [[1.0]], \"Test 7 failed: single vector\"\nassert rbf_kernel([[1,2,3]], None, [1,2]) == -1, \"Test 8 failed: wrong \u03c3 length\"\nassert rbf_kernel([[1,2]], None, -1.0) == -1, \"Test 9 failed: negative \u03c3\"\nassert rbf_kernel([[1,2]], None, [1,0]) == -1, \"Test 10 failed: zero in \u03c3 list\""}
{"task_id": 446, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find the best 1-level split (decision stump) by information gain.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    (n_samples, n_features) = X.shape\n    (classes, y_indices) = np.unique(y, return_inverse=True)\n    n_classes = len(classes)\n\n    def entropy_from_counts(counts: np.ndarray) -> float:\n        \"\"\"Shannon entropy (base 2) from a 1-D array with class counts.\"\"\"\n        total = counts.sum()\n        if total == 0:\n            return 0.0\n        probs = counts[counts > 0] / total\n        return -np.sum(probs * np.log2(probs))\n\n    def majority_label(counts: np.ndarray) -> int:\n        \"\"\"Most frequent label, ties resolved by smaller label value.\"\"\"\n        max_count = counts.max()\n        winner_idx = np.where(counts == max_count)[0][0]\n        return int(classes[winner_idx])\n    total_counts = np.bincount(y_indices, minlength=n_classes)\n    parent_entropy = entropy_from_counts(total_counts)\n    if parent_entropy == 0.0:\n        maj = majority_label(total_counts)\n        return (-1, None, maj, maj)\n    best_gain = -np.inf\n    best_feature = -1\n    best_threshold = None\n    best_left_label = None\n    best_right_label = None\n    eps = 1e-12\n    for j in range(n_features):\n        col = X[:, j]\n        order = np.argsort(col)\n        sorted_vals = col[order]\n        sorted_y_idx = y_indices[order]\n        left_counts = np.zeros(n_classes, dtype=int)\n        for i in range(n_samples - 1):\n            left_counts[sorted_y_idx[i]] += 1\n            if sorted_vals[i] == sorted_vals[i + 1]:\n                continue\n            thr = (sorted_vals[i] + sorted_vals[i + 1]) / 2.0\n            n_left = i + 1\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:\n                continue\n            right_counts = total_counts - left_counts\n            h_left = entropy_from_counts(left_counts)\n            h_right = entropy_from_counts(right_counts)\n            p_left = n_left / n_samples\n            p_right = 1.0 - p_left\n            gain = parent_entropy - p_left * h_left - p_right * h_right\n            better_gain = gain > best_gain + eps\n            same_gain = abs(gain - best_gain) <= eps\n            update = False\n            if better_gain:\n                update = True\n            elif same_gain:\n                if j < best_feature:\n                    update = True\n                elif j == best_feature and (best_threshold is None or thr < best_threshold - eps):\n                    update = True\n            if update:\n                best_gain = gain\n                best_feature = j\n                best_threshold = thr\n                best_left_label = majority_label(left_counts)\n                best_right_label = majority_label(right_counts)\n    if best_gain <= eps:\n        maj = majority_label(total_counts)\n        return (-1, None, maj, maj)\n    return (best_feature, round(best_threshold, 4), best_left_label, best_right_label)\nassert decision_stump([[1],[2],[3],[4]],[0,0,1,1])==(0,2.5,0,1),\"failed on simple 1-D split\"\nassert decision_stump([[1,1],[2,1],[3,2],[4,2]],[0,0,1,1])==(0,2.5,0,1),\"failed when two features tie\"\nassert decision_stump([[1],[2]],[1,1])==(-1,None,1,1),\"failed when no split improves gain\"\nassert decision_stump([[0],[1],[2],[3],[4],[5]],[1,1,1,0,0,0])==(0,2.5,1,0),\"failed on mixed labels\"\nassert decision_stump([[10,0],[20,0],[30,1],[40,1]],[0,0,1,1])==(0,25.0,0,1),\"failed on threshold rounding\"\nassert decision_stump([[5,2],[6,2],[7,3],[8,3]],[1,1,0,0])==(0,6.5,1,0),\"failed different labels/values\"\nassert decision_stump([[1,10],[2,20],[3,30],[4,40]],[0,0,1,1])==(0,2.5,0,1),\"failed preference of feature 0 over 1\"\nassert decision_stump([[1,1,1],[2,2,2],[3,3,3],[4,4,4]],[0,0,1,1])==(0,2.5,0,1),\"failed more than two features\"\nassert decision_stump([[1],[1.5],[2],[2.5],[3]], [0,0,0,1,1])==(0,2.25,0,1),\"failed uneven split\"\nassert decision_stump([[1,2],[1,3],[1,4]],[2,2,2])==(-1,None,2,2),\"failed when all labels identical\""}
{"task_id": 452, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numbers\nimport numpy as np\nfrom typing import List\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split *X* into two subsets according to one feature and a threshold.\n\n    For a numeric *threshold* (int / float) the first subset contains every\n    sample whose value in column *feature_i* is >= threshold, otherwise the\n    first subset contains every sample whose value equals *threshold*.\n\n    The second subset gets all remaining samples.  The original row order is\n    preserved and both subsets are returned as regular Python lists.\n    \"\"\"\n    is_numeric = isinstance(threshold, numbers.Real) and (not isinstance(threshold, bool))\n    (left, right) = ([], [])\n    for row in X:\n        value = row[feature_i]\n        if is_numeric:\n            goes_left = value >= threshold\n        else:\n            goes_left = value == threshold\n        (left if goes_left else right).append(row)\n    left_out = np.asarray(left, dtype=object).tolist() if left else []\n    right_out = np.asarray(right, dtype=object).tolist() if right else []\n    return [left_out, right_out]\nassert divide_on_feature([[1,5],[3,2],[4,6],[2,1]],0,3) == [[[3,2],[4,6]],[[1,5],[2,1]]], \"test case failed: numeric split >= 3\"\nassert divide_on_feature([[1.0,1],[2.5,3],[2.4,0],[3.1,2]],0,2.5) == [[[2.5,3],[3.1,2]],[[1.0,1],[2.4,0]]], \"test case failed: float threshold\"\nassert divide_on_feature([[1,'A'],[2,'B'],[3,'A'],[4,'C']],1,'A') == [[[1,'A'],[3,'A']],[[2,'B'],[4,'C']]], \"test case failed: categorical split\"\nassert divide_on_feature([[5],[6],[7]],0,10) == [[],[[5],[6],[7]]], \"test case failed: threshold greater than all\"\nassert divide_on_feature([[5],[6],[7]],0,0) == [[[5],[6],[7]],[]], \"test case failed: threshold smaller than all\"\nassert divide_on_feature([[1,2,3]],2,3) == [[[1,2,3]],[]], \"test case failed: single sample equal\"\nassert divide_on_feature([[1,2,3]],2,4) == [[],[[1,2,3]]], \"test case failed: single sample not equal\"\nassert divide_on_feature([[1,'yes'],[2,'no'],[3,'yes']],1,'no') == [[[2,'no']],[[1,'yes'],[3,'yes']]], \"test case failed: exactly one match\"\nassert divide_on_feature([[1.2],[3.4],[5.6],[7.8]],0,5.6) == [[[5.6],[7.8]],[[1.2],[3.4]]], \"test case failed: equality on floats\"\nassert divide_on_feature(np.array([[1,'cat'],[2,'dog'],[3,'cat']],dtype=object),1,'cat') == [[[1,'cat'],[3,'cat']],[[2,'dog']]], \"test case failed: ndarray object dtype\""}
{"task_id": 453, "completion_id": 0, "passed": false, "result": "failed: test 4 failed", "solution": "import numpy as np\nimport numpy as np\ndef update_q_values(Q: 'np.ndarray', Q_next: 'np.ndarray', actions: list[int], rewards: list[float], dones: list[bool], gamma: float) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch (see doc-string above).\"\"\"\n    if not 0.0 <= gamma <= 1.0:\n        return -1\n    if not (isinstance(Q, np.ndarray) and isinstance(Q_next, np.ndarray)):\n        return -1\n    if Q.shape != Q_next.shape or Q.ndim != 2:\n        return -1\n    (batch_size, n_actions) = Q.shape\n    if not len(actions) == len(rewards) == len(dones) == batch_size:\n        return -1\n    if any((a < 0 or a >= n_actions for a in actions)):\n        return -1\n    targets = Q.copy()\n    for i in range(batch_size):\n        if dones[i]:\n            target = rewards[i]\n        else:\n            target = rewards[i] + gamma * np.max(Q_next[i])\n        targets[i, actions[i]] = target\n    targets = np.round(targets, 4).tolist()\n    return targets\nimport numpy as np\nassert update_q_values(np.array([[1.0,2.0],[0.5,0.2]]), np.array([[1.5,1.0],[0.4,0.9]]), [1,0], [1.0,0.0], [False,True], 0.99) == [[1.0, 2.485], [0.0, 0.2]], \"test 1 failed\"\nassert update_q_values(np.array([[0.0,0.0]]), np.array([[1.0,2.0]]), [0], [0.5], [False], 1.0) == [[2.5, 0.0]], \"test 2 failed\"\nassert update_q_values(np.array([[1,2,3]]), np.array([[3,2,1]]), [2], [1.0], [False], 0.9) == [[1.0, 2.0, 3.7]], \"test 4 failed\"\nassert update_q_values(np.array([[0.2,0.3],[0.4,0.5]]), np.array([[0.0,0.0],[0.0,0.0]]), [0,1], [0.0,0.0], [True,True], 0.99) == [[0.0, 0.3], [0.4, 0.0]], \"test 5 failed\"\nassert update_q_values(np.array([[0.0,0.0]]), np.array([[0.0,0.0]]), [0], [0.0], [True], 0.0) == [[0.0, 0.0]], \"test 6 failed\"\nassert update_q_values(np.array([[1.0,2.0]]), np.array([[1.0,2.0]]), [1], [1.0], [False], 0.0) == [[1.0, 1.0]], \"test 7 failed\"\nassert update_q_values(np.array([[1.0,2.0]]), np.array([[1.0,2.0]]), [1], [1.0], [False], 1.0) == [[1.0, 3.0]], \"test 8 failed\"\nassert update_q_values(np.array([[1.0,1.0]]), np.array([[1.0,1.0,1.0]]), [0], [0.0], [True], 0.9) == -1, \"test 10 failed\""}
{"task_id": 458, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.UnionType' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None=None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes.  If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists (shape: (len(labels), num_classes)) containing\n        0/1 integers, or -1 when:\n            \u2022 any label is negative, or\n            \u2022 `num_classes` is provided but smaller than `max(labels)+1`.\n    \"\"\"\n    try:\n        labels_list = list(labels)\n    except TypeError:\n        return -1\n    if not labels_list:\n        inferred_classes = 0 if num_classes is None else num_classes\n        return [] if inferred_classes >= 0 else -1\n    max_label = -1\n    for lbl in labels_list:\n        if not isinstance(lbl, (int, np.integer)) or lbl < 0:\n            return -1\n        if lbl > max_label:\n            max_label = int(lbl)\n    if num_classes is None:\n        num_classes = max_label + 1\n    elif num_classes < max_label + 1:\n        return -1\n    one_hot: list[list[int]] = [[1 if class_idx == lbl else 0 for class_idx in range(num_classes)] for lbl in labels_list]\n    return one_hot\nassert to_categorical([0, 2, 1, 3], 4) == [[1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]], \"test case failed: to_categorical([0, 2, 1, 3], 4)\"\nassert to_categorical(np.array([1,1,1]), None) == [[0,1],[0,1],[0,1]], \"test case failed: to_categorical(np.array([1,1,1]), None)\"\nassert to_categorical([2,0,1], 3) == [[0,0,1],[1,0,0],[0,1,0]], \"test case failed: to_categorical([2,0,1], 3)\"\nassert to_categorical([3,2,4], 5) == [[0,0,0,1,0],[0,0,1,0,0],[0,0,0,0,1]], \"test case failed: to_categorical([3,2,4], 5)\"\nassert to_categorical([], 0) == [], \"test case failed: to_categorical([], 0)\"\nassert to_categorical([0], None) == [[1]], \"test case failed: to_categorical([0], None)\"\nassert to_categorical([9,8,7,6,5], None) == [[0,0,0,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0,0,0]], \"test case failed: labels 9..5\"\nassert to_categorical([1,0,1,0], 2) == [[0,1],[1,0],[0,1],[1,0]], \"test case failed: alternating labels\"\nassert to_categorical([0,0,0], 1) == [[1],[1],[1]], \"test case failed: single class\"\nassert to_categorical([2,1], 2) == -1, \"test case failed: num_classes too small\""}
{"task_id": 461, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gradient_boosting_regressor(x: list[float], y: list[float], n_estimators: int=200, learning_rate: float=0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\"\"\"\n    x_arr = np.asarray(x, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    if n_estimators <= 0:\n        return np.round(np.full_like(y_arr, y_arr.mean()), 4).tolist()\n    y_pred = np.full_like(y_arr, y_arr.mean(), dtype=float)\n    uniq_x = np.unique(x_arr)\n    for _ in range(n_estimators):\n        residuals = y_arr - y_pred\n        if np.allclose(residuals, 0.0):\n            break\n        best_sse = np.inf\n        best_t = None\n        best_left_mean = best_right_mean = 0.0\n        thresholds = uniq_x[:-1]\n        for t in thresholds:\n            left_mask = x_arr <= t\n            right_mask = ~left_mask\n            if not left_mask.any() or not right_mask.any():\n                continue\n            r_left = residuals[left_mask]\n            r_right = residuals[right_mask]\n            (m_left, m_right) = (r_left.mean(), r_right.mean())\n            sse = np.sum((r_left - m_left) ** 2) + np.sum((r_right - m_right) ** 2)\n            if sse < best_sse:\n                best_sse = sse\n                best_t = t\n                (best_left_mean, best_right_mean) = (m_left, m_right)\n        if best_t is None:\n            stump_pred = np.full_like(residuals, residuals.mean())\n        else:\n            stump_pred = np.where(x_arr <= best_t, best_left_mean, best_right_mean)\n        y_pred += learning_rate * stump_pred\n    return np.round(y_pred, 4).tolist()\nassert gradient_boosting_regressor([1, 2], [1, 2], 1, 1.0) == [1.0, 2.0], \"failed case: ([1,2],[1,2],1,1.0)\"\nassert gradient_boosting_regressor([1, 2], [1, 3], 1, 1.0) == [1.0, 3.0], \"failed case: ([1,2],[1,3],1,1.0)\"\nassert gradient_boosting_regressor([1, 2, 3], [2, 2, 2], 1, 1.0) == [2.0, 2.0, 2.0], \"failed case: ([1,2,3],[2,2,2],1,1.0)\"\nassert gradient_boosting_regressor([1, 2, 3, 4], [4, 3, 2, 1], 1, 1.0) == [3.5, 3.5, 1.5, 1.5], \"failed case: descending targets\"\nassert gradient_boosting_regressor([1, 2], [2, 4], 2, 0.5) == [2.25, 3.75], \"failed case: two iterations, shrinkage 0.5\"\nassert gradient_boosting_regressor([1, 2], [1, 2], 0, 0.5) == [1.5, 1.5], \"failed case: n_estimators == 0\"\nassert gradient_boosting_regressor([1, 1, 2, 2], [1, 1, 3, 3], 1, 1.0) == [1.0, 1.0, 3.0, 3.0], \"failed case: repeated feature values\"\nassert gradient_boosting_regressor([1, 2, 3, 4], [2, 2, 2, 10], 2, 1.0) == [2.0, 2.0, 2.0, 10.0], \"failed case: perfect fit in first round\"\nassert gradient_boosting_regressor([1, 2, 3], [1, 2, 4], 1, 0.5) == [1.9167, 1.9167, 3.1667], \"failed case: fractional shrinkage\"\nassert gradient_boosting_regressor([1, 2, 3, 4, 5], [3, 3, 3, 3, 3], 3, 0.3) == [3.0, 3.0, 3.0, 3.0, 3.0], \"failed case: constant targets\""}
{"task_id": 471, "completion_id": 0, "passed": false, "result": "failed: test5 failed", "solution": "import numpy as np\nimport numpy as np\ndef expected_sarsa_update(q_table: list[list[float]], state: int, action: int, reward: float, next_state: int | None, epsilon: float, lr: float, gamma: float) -> list[list[float]]:\n    \"\"\"One-step tabular Expected-SARSA TD(0) update.\"\"\"\n    n_actions = len(q_table[0])\n    if next_state is None:\n        expected_q = 0.0\n    else:\n        next_row = q_table[next_state]\n        greedy_idx = max(range(n_actions), key=next_row.__getitem__)\n        base_prob = epsilon / n_actions\n        expected_q = 0.0\n        for a in range(n_actions):\n            prob = base_prob\n            if a == greedy_idx:\n                prob += 1.0 - epsilon\n            expected_q += prob * next_row[a]\n    target = reward + gamma * expected_q\n    q_table[state][action] += lr * (target - q_table[state][action])\n    rounded_q = [[round(v, 4) for v in row] for row in q_table]\n    return rounded_q\nassert expected_sarsa_update([[0,0],[0,0]],0,0,1,None,0.2,0.5,1.0)==[[0.5,0.0],[0.0,0.0]],\"test1 failed\"\nassert expected_sarsa_update([[0.5,0.2,0.1],[0.3,0.4,0.1]],0,1,1.0,1,0.1,0.4,0.99)==[[0.5,0.6731,0.1],[0.3,0.4,0.1]],\"test2 failed\"\nassert expected_sarsa_update([[1,2,3,4],[0.1,0.2,0.3,0.4]],1,2,2,0,0.0,1.0,0.5)==[[1,2,3,4],[0.1,0.2,4.0,0.4]],\"test3 failed\"\nassert expected_sarsa_update([[0.1,0.1],[0.5,0.5]],0,1,0,1,0.3,0.5,1.0)==[[0.1,0.3],[0.5,0.5]],\"test4 failed\"\nassert expected_sarsa_update([[0,1],[2,3]],1,0,-1,0,0.5,0.25,0.9)==[[0,1],[1.4188,3]],\"test5 failed\"\nassert expected_sarsa_update([[0.8,0.3]],0,0,0.5,None,0.2,0.3,0.9)==[[0.71,0.3]],\"test6 failed\"\nassert expected_sarsa_update([[0,0,0]],0,2,5,0,0.9,1.0,0.0)==[[0,0,5.0]],\"test7 failed\"\nassert expected_sarsa_update([[1,1]],0,0,0,0,0.0,0.5,1.0)==[[1,1]],\"test8 failed\"\nassert expected_sarsa_update([[1,2,3]],0,1,1,0,1.0,0.5,1.0)==[[1,2.5,3]],\"test9 failed\"\nassert expected_sarsa_update([[0.4,0.2,0.6,0.0],[1,1,1,1]],0,3,0,1,0.3,0.2,0.95)==[[0.4,0.2,0.6,0.19],[1,1,1,1]],\"test10 failed\""}
{"task_id": 474, "completion_id": 0, "passed": false, "result": "failed: test case failed: generate_data(2,2,1,1,seed=0) \u2013 X_train", "solution": "import numpy as np\nimport random\nimport numpy as np\nimport random\nimport numpy as np\nimport random\ndef generate_data(m: int, n_train: int, n_val: int, interval: float, seed: int | None=None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set consisting of `m` rectangular clusters.\n\n    Each class i (0-based index) is located at\n        x \u2208 [(\u230ai/2\u230b+0.1)\u00b710 , (\u230ai/2\u230b+0.9)\u00b710]\n        y \u2208 [((i mod 2)*interval+0.1)\u00b710 , ((i mod 2)*interval+0.9)\u00b710]\n    and is assigned the label  (i \u2212 0.5)\u00b72  (\u2192 \u22121, +1, 3, 5, \u2026).\n\n    All returned coordinates are rounded to 4 decimals.\n    \"\"\"\n    if m < 2:\n        raise ValueError('`m` (number of classes) must be at least 2.')\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    (X_train_parts, X_val_parts) = ([], [])\n    (Y_train_parts, Y_val_parts) = ([], [])\n    for i in range(m):\n        col = i // 2\n        row = i % 2\n        (x_min, x_max) = ((col + 0.1) * 10, (col + 0.9) * 10)\n        (y_min, y_max) = ((row * interval + 0.1) * 10, (row * interval + 0.9) * 10)\n        x_tr = np.random.uniform(x_min, x_max, n_train)\n        y_tr = np.random.uniform(y_min, y_max, n_train)\n        X_train_parts.append(np.column_stack((x_tr, y_tr)))\n        x_va = np.random.uniform(x_min, x_max, n_val)\n        y_va = np.random.uniform(y_min, y_max, n_val)\n        X_val_parts.append(np.column_stack((x_va, y_va)))\n        label = int((i - 0.5) * 2)\n        Y_train_parts.append(np.full(n_train, label, dtype=int))\n        Y_val_parts.append(np.full(n_val, label, dtype=int))\n    X_train = np.round(np.vstack(X_train_parts), 4)\n    X_val = np.round(np.vstack(X_val_parts), 4)\n    Y_train = np.concatenate(Y_train_parts)\n    Y_val = np.concatenate(Y_val_parts)\n    return (X_train, X_val, Y_train, Y_val)\nassert generate_data(2, 2, 1, 1, seed=0)[0].tolist() == [[7.7554, 7.0636], [4.3646, 3.0713], [7.2704, 13.4265], [4.8128, 15.6671]], \"test case failed: generate_data(2,2,1,1,seed=0) \u2013 X_train\"\nassert generate_data(2, 1, 1, 1, seed=1)[2].tolist() == [-1, 1], \"test case failed: generate_data(2,1,1,1,seed=1) \u2013 Y_train\"\nassert generate_data(2, 3, 0, 1, seed=0)[0].shape == (6, 2), \"test case failed: generate_data(2,3,0,1,seed=0) \u2013 shapes\"\nassert generate_data(3, 2, 1, 0.5, seed=3)[2].shape == (6,), \"test case failed: generate_data(3,2,1,0.5,seed=3) \u2013 label shape\"\nassert generate_data(4, 1, 0, 1, seed=4)[1].size == 0, \"test case failed: generate_data(4,1,0,1,seed=4) \u2013 empty validation\"\nassert len(set(generate_data(5, 2, 2, 1, seed=6)[2].tolist())) == 5, \"test case failed: unique labels\"\nassert not np.array_equal(generate_data(2, 2, 0, 1, seed=7)[0], generate_data(2, 2, 0, 1, seed=8)[0]), \"test case failed: different seeds\"\nassert generate_data(2, 5, 0, 3, seed=9)[0][generate_data(2, 5, 0, 3, seed=9)[2]==1][:,1].min() - generate_data(2, 5, 0, 3, seed=9)[0][generate_data(2, 5, 0, 3, seed=9)[2]==-1][:,1].max() >= 20, \"test case failed: interval separation\"\nassert isinstance(generate_data(2, 1, 1, 1, seed=10)[0], np.ndarray), \"test case failed: return type\"\nassert generate_data(2, 1, 1, 1, seed=0)[0].dtype == float, \"test case failed: dtype check\""}
{"task_id": 475, "completion_id": 0, "passed": false, "result": "failed: 'float' object is not subscriptable", "solution": "import numpy as np\nimport numpy as np\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-08):\n    \"\"\"Perform a single Adam optimisation step (see doc-string above).\"\"\"\n    grad_arr = np.asarray(grad, dtype=float)\n    if m_prev is None:\n        m_prev = np.zeros_like(grad_arr, dtype=float)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad_arr, dtype=float)\n    m_new = b1 * m_prev + (1.0 - b1) * grad_arr\n    v_new = b2 * v_prev + (1.0 - b2) * grad_arr ** 2\n    m_hat = m_new / (1.0 - b1 ** t)\n    v_hat = v_new / (1.0 - b2 ** t)\n    w_new = np.asarray(w, dtype=float) - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n    if np.isscalar(w):\n        w_new = w_new.item()\n        m_new = m_new.item()\n        v_new = v_new.item()\n    return (w_new, m_new, v_new)\nimport numpy as np\nassert np.allclose(adam_update(1.0, 0.1, 0.0, 0.0, 1)[0], 0.999000001, atol=1e-8), \"failed: scalar, t=1, positive grad\"\nassert np.allclose(adam_update(1.0, -0.1, 0.0, 0.0, 1)[0], 1.000999999, atol=1e-8), \"failed: scalar, t=1, negative grad\"\nw_vec, m_vec, v_vec = adam_update(np.array([1.0, -1.0]), np.array([0.2, -0.2]), np.zeros(2), np.zeros(2), 1)\nassert np.allclose(w_vec, np.array([0.999, -0.999]), atol=1e-8), \"failed: vector, t=1\"\nassert np.allclose(adam_update(1.0, 0.1, 0.0, 0.0, 1, learning_rate=0.01)[0], 0.990000001, atol=1e-8), \"failed: different learning rate\"\nassert np.allclose(adam_update(1.0, 0.0, 0.0, 0.0, 1)[0], 1.0, atol=1e-12), \"failed: zero gradient gives no update\"\nw6, m6, v6 = adam_update(2.5, -0.5, 0.0, 0.0, 1)\nassert np.allclose([w6, m6, v6], [2.501, -0.05, 0.00025], atol=1e-8), \"failed: scalar, large grad\"\nw7, m7, v7 = adam_update(0.999000001, 0.1, 0.01, 1e-5, 2)\nassert np.allclose([w7, m7, v7[...]], [0.998000002, 0.019, 1.999e-5], atol=1e-8), \"failed: scalar, t=2\"\nvec_prev_m = np.array([0.02, -0.02])\nvec_prev_v = np.array([4e-5, 4e-5])\nwv, mv, vv = adam_update(np.array([0.999, -0.999]), np.array([0.2, -0.2]), vec_prev_m, vec_prev_v, 2)\nassert np.allclose(wv, np.array([0.998, -0.998]), atol=1e-8), \"failed: vector, t=2\"\nassert np.allclose(adam_update(5.0, 0.0, None, None, 3)[1:], (0.0, 0.0), atol=1e-12), \"failed: None moments treated as zeros\""}
{"task_id": 479, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List, Optional\ndef epsilon_greedy_bandit(rewards: List[List[float]], epsilon: float=0.05, ev_prior: float=0.5, seed: Optional[int]=None) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    if not rewards:\n        return [round(ev_prior, 4)]\n    n_arms = len(rewards[0])\n    values = np.full(n_arms, float(ev_prior))\n    counts = np.zeros(n_arms, dtype=int)\n    for row in rewards:\n        if np.random.random() < epsilon:\n            arm = np.random.randint(n_arms)\n        else:\n            arm = int(np.argmax(values))\n        r = row[arm]\n        counts[arm] += 1\n        values[arm] += (r - values[arm]) / counts[arm]\n    return [round(v, 4) for v in values]\nassert epsilon_greedy_bandit([[1,0,0],[0,1,0],[1,0,0],[0,1,0],[1,0,0]],0,0.5,42) == [0.6,0.5,0.5], \"test case 1 failed\"\nassert epsilon_greedy_bandit([[0,1],[0,1],[0,1]],0,0) == [0.0,0.0], \"test case 2 failed\"\nassert epsilon_greedy_bandit([[1,0],[1,0],[1,0]],0,0.5) == [1.0,0.5], \"test case 3 failed\"\nassert epsilon_greedy_bandit([[0,0,1],[0,1,0],[1,0,0]],0,0) == [0.3333,0.0,0.0], \"test case 4 failed\"\nassert epsilon_greedy_bandit([[0,0],[0,1],[0,1],[0,1]],0,0.1) == [0.0,1.0], \"test case 5 failed\"\nassert epsilon_greedy_bandit([[1],[0],[1],[1],[1]],0,0.5) == [0.8], \"test case 6 failed\"\nassert epsilon_greedy_bandit([[0,0,0],[0,0,1],[0,1,0],[0,0,1]],0,0.0) == [0.0,0.0,0.0], \"test case 7 failed\"\nassert epsilon_greedy_bandit([[0.5,1.0,0.2,0.2]],0,0.5) == [0.5,0.5,0.5,0.5], \"test case 8 failed\"\nassert epsilon_greedy_bandit([[0,0],[0,0]],0,0.7) == [0.0,0.0], \"test case 9 failed\"\nassert epsilon_greedy_bandit([[1,1]],0,0.5) == [1.0,0.5], \"test case 10 failed\""}
{"task_id": 481, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if data.size == 0:\n        return []\n    n_samples = data.shape[0]\n    eps_sq = eps * eps\n    neighbours: list[np.ndarray] = []\n    for i in range(n_samples):\n        diff = data - data[i]\n        dist_sq = np.einsum('ij,ij->i', diff, diff)\n        neighbours.append(np.where(dist_sq <= eps_sq)[0])\n    UNASSIGNED = -1\n    labels = np.full(n_samples, UNASSIGNED, dtype=int)\n    visited = np.zeros(n_samples, dtype=bool)\n    cluster_id = 0\n    for point_idx in range(n_samples):\n        if visited[point_idx]:\n            continue\n        visited[point_idx] = True\n        neigh = neighbours[point_idx]\n        if neigh.size < min_samples:\n            labels[point_idx] = UNASSIGNED\n            continue\n        labels[point_idx] = cluster_id\n        stack = list(neigh)\n        while stack:\n            current = stack.pop()\n            if not visited[current]:\n                visited[current] = True\n                neigh_current = neighbours[current]\n                if neigh_current.size >= min_samples:\n                    stack.extend(neigh_current.tolist())\n            if labels[current] == UNASSIGNED:\n                labels[current] = cluster_id\n        cluster_id += 1\n    return labels.tolist()\nassert dbscan(np.array([[1,1],[1.1,1.1],[0.9,1],[5,5],[5.2,5.1],[4.9,5],[8,0]]),0.3,2)==[0,0,0,1,1,1,-1],\"test case failed: example dataset\"\nassert dbscan(np.array([[0,0],[0.1,0],[0.05,0.05],[0.2,0.2]]),0.25,1)==[0,0,0,0],\"test case failed: single cluster with min_samples=1\"\nassert dbscan(np.array([[0,0],[5,5],[10,10]]),0.5,2)==[-1,-1,-1],\"test case failed: all noise\"\nassert dbscan(np.empty((0,2)),0.5,2)==[],\"test case failed: empty dataset\"\nassert dbscan(np.array([[0,0],[0,0],[0,0]]),0.01,2)==[0,0,0],\"test case failed: duplicate points cluster\"\nassert dbscan(np.array([[0,0],[0,1],[0,2],[0,3]]),1.1,2)==[0,0,0,0],\"test case failed: linear chain cluster\"\nassert dbscan(np.array([[0,0],[3,3],[0.1,0.1],[3.1,3.1],[6,6]]),0.25,2)==[0,1,0,1,-1],\"test case failed: two small clusters plus noise\"\nassert dbscan(np.array([[0,0],[0.05,0.05],[2,2],[2.05,2.05]]),0.1,2)==[0,0,1,1],\"test case failed: two tight clusters\""}
{"task_id": 482, "completion_id": 0, "passed": false, "result": "failed: `y` must contain integer values only.", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    Parameters\n    ----------\n    y : array_like\n        1-D sequence of non-negative integer labels.\n    num_classes : int or None, optional\n        Number of classes (columns in the output).  If None, it is inferred\n        as max(y) + 1.  Every label must satisfy\n        0 <= label < num_classes, otherwise ValueError is raised.\n\n    Returns\n    -------\n    np.ndarray\n        One-hot encoded matrix of shape (len(y), num_classes) with dtype=int.\n    \"\"\"\n    y_arr = np.asarray(y)\n    if y_arr.ndim == 0:\n        y_arr = y_arr.reshape(1)\n    if y_arr.ndim != 1:\n        raise ValueError('`y` must be a 1-D sequence of integer labels.')\n    try:\n        y_int = y_arr.astype(int, casting='safe', copy=False)\n    except TypeError:\n        raise ValueError('`y` must contain integer values only.')\n    n_samples = y_int.size\n    if n_samples == 0:\n        if num_classes is None:\n            return np.zeros((0, 0), dtype=int)\n        if num_classes <= 0:\n            raise ValueError('`num_classes` must be a positive integer.')\n        return np.zeros((0, num_classes), dtype=int)\n    max_label = int(y_int.max())\n    min_label = int(y_int.min())\n    if num_classes is None:\n        if min_label < 0:\n            raise ValueError('Negative labels are not allowed.')\n        num_classes = max_label + 1\n    else:\n        if num_classes <= 0:\n            raise ValueError('`num_classes` must be a positive integer.')\n        if min_label < 0 or max_label >= num_classes:\n            raise ValueError('All labels must satisfy 0 <= label < num_classes.')\n    one_hot = np.zeros((n_samples, num_classes), dtype=int)\n    rows = np.arange(n_samples)\n    one_hot[rows, y_int] = 1\n    return one_hot\nassert (to_categorical([0, 1, 2]) == np.array([[1,0,0],[0,1,0],[0,0,1]])).all(), \"failed: basic consecutive labels\"\nassert (to_categorical(np.array([2,0,1,2])) == np.array([[0,0,1],[1,0,0],[0,1,0],[0,0,1]])).all(), \"failed: shuffled labels\"\nassert (to_categorical([0,1,2], num_classes=5) == np.array([[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0]])).all(), \"failed: extra unused columns\"\nassert (to_categorical([4], num_classes=5) == np.array([[0,0,0,0,1]])).all(), \"failed: single element\"\nassert (to_categorical((3,3,3)) == np.array([[0,0,0,1],[0,0,0,1],[0,0,0,1]])).all(), \"failed: tuple input\"\nassert (to_categorical(np.arange(6)) == np.eye(6, dtype=int)).all(), \"failed: arange sequence\"\nassert (to_categorical([1,1,1,1]) == np.array([[0,1],[0,1],[0,1],[0,1]])).all(), \"failed: duplicate labels\"\nassert to_categorical([], num_classes=0).shape == (0,0), \"failed: empty input with explicit classes\"\ntry:\n    to_categorical([0,2], num_classes=2)\n    assert False, \"failed: did not raise on out-of-range label\"\nexcept ValueError:\n    pass\ntry:\n    to_categorical([-1,0,1])\n    assert False, \"failed: did not raise on negative label\"\nexcept ValueError:\n    pass"}
{"task_id": 485, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef regression_tree_predict(X_train: list[float | int], y_train: list[float | int], X_test: list[float | int], max_depth: int=3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\"\"\"\n\n    def build(x: np.ndarray, y: np.ndarray, depth: int):\n        if depth == max_depth or x.size < 2 or np.all(x == x[0]):\n            return {'leaf': True, 'value': float(y.mean())}\n        order = np.argsort(x)\n        (xs, ys) = (x[order], y[order])\n        n = xs.size\n        csum = np.cumsum(ys)\n        csum2 = np.cumsum(ys ** 2)\n        (total_sum, total_sum2) = (csum[-1], csum2[-1])\n        (best_sse, best_idx) = (np.inf, None)\n        for i in range(n - 1):\n            if xs[i] == xs[i + 1]:\n                continue\n            (n_l, n_r) = (i + 1, n - (i + 1))\n            (sum_l, sum_r) = (csum[i], total_sum - csum[i])\n            (sum2_l, sum2_r) = (csum2[i], total_sum2 - csum2[i])\n            sse_l = sum2_l - sum_l ** 2 / n_l\n            sse_r = sum2_r - sum_r ** 2 / n_r\n            sse = sse_l + sse_r\n            if sse < best_sse:\n                (best_sse, best_idx) = (sse, i)\n        if best_idx is None:\n            return {'leaf': True, 'value': float(y.mean())}\n        thr = (xs[best_idx] + xs[best_idx + 1]) / 2.0\n        left_mask = x <= thr\n        right_mask = ~left_mask\n        return {'leaf': False, 'thr': float(thr), 'left': build(x[left_mask], y[left_mask], depth + 1), 'right': build(x[right_mask], y[right_mask], depth + 1)}\n\n    def predict_one(node, x_val: float):\n        while not node['leaf']:\n            node = node['left'] if x_val <= node['thr'] else node['right']\n        return node['value']\n    if len(X_train) == 0:\n        return []\n    X_arr = np.asarray(X_train, dtype=float)\n    y_arr = np.asarray(y_train, dtype=float)\n    tree = build(X_arr, y_arr, depth=0)\n    return [predict_one(tree, float(x)) for x in X_test]\nassert regression_tree_predict([1,2,3,4,5,6,7], [2,2,2,8,8,8,8], [2,3,5,7], 1) == [2.0,2.0,8.0,8.0], \"Failed test 1 \u2013 description example\"\nassert regression_tree_predict([1,2,3,4], [1,1,3,3], [1,3,4], 1) == [1.0,3.0,3.0], \"Failed test 2 \u2013 perfect binary split\"\nassert regression_tree_predict([1,2,3,4], [1,2,3,4], [1,3,4], 3) == [1.0,3.0,4.0], \"Failed test 3 \u2013 deeper splits\"\nassert regression_tree_predict([0,1,2,3], [5,5,5,5], [0,2,3], 2) == [5.0,5.0,5.0], \"Failed test 4 \u2013 constant target\"\nassert regression_tree_predict([], [], [1,2,3], 1) == [], \"Failed test 5 \u2013 empty training data\"\nassert regression_tree_predict([1,1,2,2,3,3], [1,1,2,2,3,3], [1.1,2.1,3.1], 2) == [1.0,2.0,3.0], \"Failed test 6 \u2013 duplicate X values\"\nassert regression_tree_predict([1,2,3,10,11,12], [1,1,1,2,2,2], [2,11], 4) == [1.0,2.0], \"Failed test 7 \u2013 two clusters\"\nassert regression_tree_predict([5,6], [7,9], [5,6], 5) == [7.0,9.0], \"Failed test 8 \u2013 tiny dataset\"\nassert regression_tree_predict([1,2,3], [3,4,5], [1,2,3], 0) == [4.0,4.0,4.0], \"Failed test 9 \u2013 depth 0\"\nassert regression_tree_predict([4,1,3,2], [4,1,3,2], [1,2,3,4], 2) == [1.0,2.0,3.0,4.0], \"Failed test 10 \u2013 unsorted inputs\""}
{"task_id": 490, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.UnionType' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    arr = np.asarray(x)\n    if arr.size == 0:\n        return []\n    arr = arr.ravel()\n    diag_matrix = np.diag(arr).tolist()\n    return diag_matrix\nassert make_diagonal([1, 2, 3]) == [[1, 0, 0], [0, 2, 0], [0, 0, 3]], \"test case failed: make_diagonal([1, 2, 3])\"\nassert make_diagonal([5]) == [[5]], \"test case failed: make_diagonal([5])\"\nassert make_diagonal([]) == [], \"test case failed: make_diagonal([])\"\nassert make_diagonal((0, 0, 0, 0)) == [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], \"test case failed: make_diagonal((0, 0, 0, 0))\"\nassert make_diagonal(np.array([3.5, -2.1])) == [[3.5, 0.0], [0.0, -2.1]], \"test case failed: make_diagonal(np.array([3.5, -2.1]))\"\nassert make_diagonal([1, -1, 1, -1]) == [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, 1, 0], [0, 0, 0, -1]], \"test case failed: make_diagonal([1, -1, 1, -1])\"\nassert make_diagonal(np.arange(4)) == [[0, 0, 0, 0], [0, 1, 0, 0], [0, 0, 2, 0], [0, 0, 0, 3]], \"test case failed: make_diagonal(np.arange(4))\"\nassert make_diagonal([1.1, 2.2, 3.3]) == [[1.1, 0.0, 0.0], [0.0, 2.2, 0.0], [0.0, 0.0, 3.3]], \"test case failed: make_diagonal([1.1, 2.2, 3.3])\"\nassert make_diagonal(tuple(range(6))) == [[0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0], [0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 4, 0], [0, 0, 0, 0, 0, 5]], \"test case failed: make_diagonal(tuple(range(6)))\"\nassert make_diagonal(np.array([])) == [], \"test case failed: make_diagonal(np.array([]))\""}
{"task_id": 491, "completion_id": 0, "passed": false, "result": "failed: name '_mine' is not defined", "solution": "from collections import Counter, defaultdict\nimport itertools\nfrom collections import Counter, defaultdict\nfrom itertools import combinations\nclass _FPNode:\n    __slots__ = ('item', 'count', 'parent', 'children', 'link')\n\n    def __init__(self, item, parent):\n        self.item = item\n        self.count = 0\n        self.parent = parent\n        self.children = {}\n        self.link = None\ndef _build_fp_tree(trans_iterable, min_sup):\n    \"\"\"\n    Build an FP-tree from `trans_iterable` and return (root, header_table).\n    `trans_iterable` must yield tuples   (items_in_transaction , multiplicity)\n    in which every `items_in_transaction` is already ordered according to a\n    global frequency order and contains *only* items that are frequent in the\n    projected data set.\n    \"\"\"\n    header = {}\n    for (items, cnt) in trans_iterable:\n        for itm in items:\n            header.setdefault(itm, [0, None])[0] += cnt\n    header = {itm: val for (itm, val) in header.items() if val[0] >= min_sup}\n    if not header:\n        return (None, None)\n    order = sorted(header.items(), key=lambda x: (-x[1][0], x[0]))\n    rank = {itm: idx for (idx, (itm, _)) in enumerate(order)}\n    root = _FPNode(None, None)\n    for (items, cnt) in trans_iterable:\n        ordered = [i for i in sorted(items, key=lambda x: rank.get(x, 10 ** 9)) if i in header]\n        if not ordered:\n            continue\n        cur = root\n        for itm in ordered:\n            nxt = cur.children.get(itm)\n            if nxt is None:\n                nxt = _FPNode(itm, cur)\n                cur.children[itm] = nxt\n                if header[itm][1] is None:\n                    header[itm][1] = nxt\n                else:\n                    last = header[itm][1]\n                    while last.link is not None:\n                        last = last.link\n                    last.link = nxt\n            nxt.count += cnt\n            cur = nxt\n    return (root, header)\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\"\"\"\n    freq = Counter()\n    for t in transactions:\n        freq.update(t)\n    freq = {k: v for (k, v) in freq.items() if v >= min_sup}\n    if not freq:\n        return []\n    order = sorted(freq.items(), key=lambda x: (-x[1], x[0]))\n    rank = {itm: idx for (idx, (itm, _)) in enumerate(order)}\n    preprocessed = []\n    for t in transactions:\n        filtered = [i for i in t if i in freq]\n        if not filtered:\n            continue\n        filtered.sort(key=lambda x: rank[x])\n        preprocessed.append((tuple(filtered), 1))\n    (root, header) = _build_fp_tree(preprocessed, min_sup)\n    patterns = set()\n    _mine(root, header, min_sup, [], patterns)\n    result = [tuple(sorted(p)) for p in patterns]\n    result.sort(key=lambda x: (len(x), x))\n    return result\nassert fp_growth([['A','B','D','E'],['B','C','E'],['A','B','D','E'],['A','B','C','E'],['A','B','C','D','E'],['B','C','D']],3)==[('A',),('B',),('C',),('D',),('E',),('A','B'),('A','D'),('A','E'),('B','C'),('B','D'),('B','E'),('C','E'),('D','E'),('A','B','D'),('A','B','E'),('A','D','E'),('B','C','E'),('B','D','E'),('A','B','D','E')],\"failed on classic example\"\nassert fp_growth([['milk','bread'],['bread','butter'],['milk','bread','butter'],['bread'],['milk','bread']],3)==[('bread',),('milk',),('bread','milk')],\"failed on bread/milk example\"\nassert fp_growth([[1],[2],[3]],2)==[],\"failed on no frequent items\"\nassert fp_growth([[1,2],[1,2],[1,2]],1)==[(1,),(2,),(1,2)],\"failed on min_sup 1\"\nassert fp_growth([[1,2,3,4]],1)==[(1,),(2,),(3,),(4,),(1,2),(1,3),(1,4),(2,3),(2,4),(3,4),(1,2,3),(1,2,4),(1,3,4),(2,3,4),(1,2,3,4)],\"failed on single transaction\"\nassert fp_growth([[1,2,3],[1,2,3],[1,2,3]],3)==[(1,),(2,),(3,),(1,2),(1,3),(2,3),(1,2,3)],\"failed on identical transactions\"\nassert fp_growth([[1,2],[2,3],[1,3],[1,2,3]],2)==[(1,),(2,),(3,),(1,2),(1,3),(2,3)],\"failed on triangle dataset\"\nassert fp_growth([[\"x\",\"y\"],[\"x\",\"z\"],[\"y\",\"z\"],[\"x\",\"y\",\"z\"]],2)==[(\"x\",),(\"y\",),(\"z\",),(\"x\",\"y\"),(\"x\",\"z\"),(\"y\",\"z\")],\"failed on string xyz\"\nassert fp_growth([],2)==[],\"failed on empty transaction list\""}
{"task_id": 492, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a polynomial feature matrix up to *degree*.\n\n    Parameters\n    ----------\n    X : list of list\n        Input data of shape (n_samples, n_features).\n    degree : int\n        Maximum total degree of the generated polynomial terms.\n        If degree < 0 the function returns -1.\n\n    Returns\n    -------\n    list of list\n        Enriched feature matrix whose columns are all monomials of the\n        original variables having total degree \u2264 *degree*, ordered\n        (i)   by increasing total degree,\n        (ii)  lexicographically within the same degree.\n        The first column is 1 (degree-0 term).\n    \"\"\"\n    if degree < 0:\n        return -1\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim != 2:\n        raise ValueError('X must be a 2-D array-like structure')\n    (n_samples, n_features) = X_arr.shape\n    index_tuples: list[tuple[int, ...]] = [()]\n    for p in range(1, degree + 1):\n        for combo in combinations_with_replacement(range(n_features), p):\n            index_tuples.append(combo)\n    n_terms = len(index_tuples)\n    out = np.empty((n_samples, n_terms), dtype=float)\n    for (j, comb) in enumerate(index_tuples):\n        if not comb:\n            out[:, j] = 1.0\n        else:\n            col = np.ones(n_samples, dtype=float)\n            for idx in comb:\n                col *= X_arr[:, idx]\n            out[:, j] = col\n    return out.tolist()\nassert polynomial_features([[2, 3]], 2) == [[1, 2.0, 3.0, 4.0, 6.0, 9.0]], \"test case failed: ([[2,3]],2)\"\nassert polynomial_features([[1, 2], [3, 4]], 1) == [[1, 1.0, 2.0], [1, 3.0, 4.0]], \"test case failed: ([[1,2],[3,4]],1)\"\nassert polynomial_features([[1], [2], [3]], 3) == [[1, 1.0, 1.0, 1.0], [1, 2.0, 4.0, 8.0], [1, 3.0, 9.0, 27.0]], \"test case failed: ([[1],[2],[3]],3)\"\nassert polynomial_features([[5, 0], [0, 5]], 0) == [[1], [1]], \"test case failed: ([[5,0],[0,5]],0)\"\nassert polynomial_features([[0, 0]], 3) == [[1] + [0.0]*9], \"test case failed: ([[0,0]],3)\"\nassert polynomial_features([[1, 2, 3]], 2) == [[1, 1.0, 2.0, 3.0, 1.0, 2.0, 3.0, 4.0, 6.0, 9.0]], \"test case failed: ([[1,2,3]],2)\"\nassert polynomial_features([[1, 2]], -1) == -1, \"test case failed: negative degree\"\nassert polynomial_features([[0.5, 1.5]], 2) == [[1, 0.5, 1.5, 0.25, 0.75, 2.25]], \"test case failed: ([[0.5,1.5]],2)\"\nassert polynomial_features([[1, 2], [3, 4], [5, 6]], 2) == [[1, 1.0, 2.0, 1.0, 2.0, 4.0], [1, 3.0, 4.0, 9.0, 12.0, 16.0], [1, 5.0, 6.0, 25.0, 30.0, 36.0]], \"test case failed: ([[1,2],[3,4],[5,6]],2)\"\nassert polynomial_features([[2, 1]], 3) == [[1, 2.0, 1.0, 4.0, 2.0, 1.0, 8.0, 4.0, 2.0, 1.0]], \"test case failed: ([[2,1]],3)\""}
{"task_id": 493, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    If *y* is empty return -1.\n\n    Parameters\n    ----------\n    y : Sequence[int | float] | numpy.ndarray\n        One-dimensional collection of numbers.\n\n    Returns\n    -------\n    float\n        MSE rounded to four decimal places, or -1 when *y* is empty.\n    \"\"\"\n    arr = np.asarray(y, dtype=float).ravel()\n    if arr.size == 0:\n        return -1\n    diff = arr - arr.mean()\n    mse_val = (diff ** 2).mean()\n    return float(round(mse_val, 4))\nassert mse([1, 2, 3]) == 0.6667, \"failed on mse([1, 2, 3])\"\nassert mse(np.array([0, 0, 0])) == 0.0, \"failed on mse([0,0,0])\"\nassert mse([-1, 1]) == 1.0, \"failed on mse([-1,1])\"\nassert mse([5]) == 0.0, \"failed on mse([5])\"\nassert mse([1, 3, 5, 7]) == 5.0, \"failed on mse([1,3,5,7])\"\nassert mse([2.5, 2.5, 2.5, 2.5]) == 0.0, \"failed on identical values\"\nassert mse(np.arange(10)) == 8.25, \"failed on np.arange(10)\"\nassert mse([]) == -1, \"failed on empty list\"\nassert mse(np.array([])) == -1, \"failed on empty np.array\"\nassert mse([100, -100, 0]) == 6666.6667, \"failed on mse([100,-100,0])\""}
{"task_id": 496, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef apply_affine(x, slope: float=1.0, intercept: float=0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list/array or a scalar.\n        slope: Coefficient multiplying x (default 1.0).\n        intercept: Constant term added to the product (default 0.0).\n\n    Returns:\n        (y, grad, grad2): tuple of three lists (all rounded to 4 decimals)\n            y[i]     = slope * x[i] + intercept\n            grad[i]  = slope\n            grad2[i] = 0\n    \"\"\"\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim == 0:\n        arr = arr.reshape(1)\n    y = slope * arr + intercept\n    grad = np.full(arr.shape, slope, dtype=float)\n    grad2 = np.zeros(arr.shape, dtype=float)\n    y = np.round(y, 4).tolist()\n    grad = np.round(grad, 4).tolist()\n    grad2 = np.round(grad2, 4).tolist()\n    return (y, grad, grad2)\nassert apply_affine([0, 1, 2], 2, 1) == ([1.0, 3.0, 5.0], [2.0, 2.0, 2.0], [0.0, 0.0, 0.0]), \"failed on ([0,1,2],2,1)\"\nassert apply_affine([-1, 0, 1], 0.5, -1) == ([-1.5, -1.0, -0.5], [0.5, 0.5, 0.5], [0.0, 0.0, 0.0]), \"failed on ([-1,0,1],0.5,-1)\"\nassert apply_affine([4]) == ([4.0], [1.0], [0.0]), \"failed on default params ([4])\"\nassert apply_affine([2, 4], -1, 0) == ([-2.0, -4.0], [-1.0, -1.0], [0.0, 0.0]), \"failed on negative slope\"\nassert apply_affine([0.1, 0.2], 3.3333, 0) == ([0.3333, 0.6667], [3.3333, 3.3333], [0.0, 0.0]), \"failed on fractional slope\"\nassert apply_affine([10, 20, 30], 0, 5) == ([5.0, 5.0, 5.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]), \"failed on zero slope\"\nassert apply_affine([1000, -1000], 0.1, 10) == ([110.0, -90.0], [0.1, 0.1], [0.0, 0.0]), \"failed on large magnitude inputs\"\nassert apply_affine([-7], -0.25, 2) == ([3.75], [-0.25], [0.0]), \"failed on single element negative slope\"\nassert apply_affine([0, 0, 0], 3, -2) == ([-2.0, -2.0, -2.0], [3.0, 3.0, 3.0], [0.0, 0.0, 0.0]), \"failed on zeros input\""}
{"task_id": 499, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places.  \n        If *k* is outside the valid range, return \u22121.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D NumPy array.')\n    (n_samples, n_features) = data.shape\n    if not 1 <= k <= n_features:\n        return -1\n    data = data.astype(float, copy=False)\n    means = data.mean(axis=0)\n    stds = data.std(axis=0, ddof=0)\n    stds_safe = stds.copy()\n    stds_safe[stds_safe == 0] = 1.0\n    Z = (data - means) / stds_safe\n    if n_samples > 1:\n        cov = Z.T @ Z / (n_samples - 1)\n    else:\n        cov = np.zeros((n_features, n_features))\n    (eigvals, eigvecs) = np.linalg.eigh(cov)\n    order = eigvals.argsort()[::-1]\n    eigvals = eigvals[order]\n    eigvecs = eigvecs[:, order]\n    for i in range(eigvecs.shape[1]):\n        v = eigvecs[:, i]\n        if v[np.argmax(np.abs(v))] < 0:\n            eigvecs[:, i] = -v\n    components = eigvecs[:, :k]\n    projected = Z @ components\n    return np.round(projected, 4).tolist()\nassert pca_transform(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[-1.7321], [0.0], [1.7321]], \"test case failed: basic 2D, k=1\"\nassert pca_transform(np.array([[1, 2], [3, 4], [5, 6]]), 2) == [[-1.7321, 0.0], [0.0, 0.0], [1.7321, 0.0]], \"test case failed: basic 2D, k=2\"\nassert pca_transform(np.array([[1, 0], [0, 1]]), 1) == [[1.4142], [-1.4142]], \"test case failed: identity subset, k=1\"\nassert pca_transform(np.array([[2, 2], [2, 2]]), 1) == [[0.0], [0.0]], \"test case failed: zero variance, k=1\"\nassert pca_transform(np.array([[1, 0], [0, 1]]), 3) == -1, \"test case failed: k greater than features\"\nassert pca_transform(np.array([[1, 0], [0, 1]]), 0) == -1, \"test case failed: k equals zero\"\nassert pca_transform(np.array([[0, 0], [1, 1]]), 1) == [[-1.4142], [1.4142]], \"test case failed: diagonal line, k=1\"\nassert pca_transform(np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]), 1) == [[-2.1213], [0.0], [2.1213]], \"test case failed: collinear 3D, k=1\"\nassert pca_transform(np.array([[1, 2], [1, 2], [1, 2]]), 1) == [[0.0], [0.0], [0.0]], \"test case failed: identical rows\"\nassert pca_transform(np.array([[0, 0], [1, 1]]), 2) == [[-1.4142, 0.0], [1.4142, 0.0]], \"test case failed: diagonal line, k=2\""}
{"task_id": 500, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom math import inf\ndef _fit_best_stump(X: np.ndarray, residuals: np.ndarray):\n    \"\"\"Find the decision stump (feature, threshold, left_val, right_val)\n    that minimises the squared error w.r.t. the provided residuals.\"\"\"\n    (n_samples, n_features) = X.shape\n    (best_feature, best_thr) = (None, None)\n    (best_left_val, best_right_val) = (None, None)\n    best_sse = inf\n    for j in range(n_features):\n        f = X[:, j]\n        order = np.argsort(f)\n        f_sorted = f[order]\n        r_sorted = residuals[order]\n        cumsum_r = np.cumsum(r_sorted)\n        cumsum_r2 = np.cumsum(r_sorted ** 2)\n        total_sum = cumsum_r[-1]\n        total_sum2 = cumsum_r2[-1]\n        for i in range(1, n_samples):\n            if f_sorted[i] == f_sorted[i - 1]:\n                continue\n            n_left = i\n            n_right = n_samples - i\n            sum_left = cumsum_r[i - 1]\n            sum_right = total_sum - sum_left\n            sum2_left = cumsum_r2[i - 1]\n            sum2_right = total_sum2 - sum2_left\n            mean_left = sum_left / n_left\n            mean_right = sum_right / n_right\n            sse_left = sum2_left - sum_left ** 2 / n_left\n            sse_right = sum2_right - sum_right ** 2 / n_right\n            sse = sse_left + sse_right\n            if sse < best_sse:\n                best_sse = sse\n                best_feature = j\n                best_thr = 0.5 * (f_sorted[i] + f_sorted[i - 1])\n                best_left_val = mean_left\n                best_right_val = mean_right\n    return (best_feature, best_thr, best_left_val, best_right_val)\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int=20, learning_rate: float=0.1) -> list[int]:\n    \"\"\"Tiny gradient\u2013boosted binary classifier (decision-stump base learners).\"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train, dtype=float)\n    n_train = X_train.shape[0]\n    c0 = y_train.mean()\n    F_train = np.full(n_train, c0, dtype=float)\n    F_test = np.full(X_test.shape[0], c0, dtype=float)\n    stumps = []\n    for _ in range(n_estimators):\n        residuals = y_train - F_train\n        (feat, thr, left_val, right_val) = _fit_best_stump(X_train, residuals)\n        stumps.append((feat, thr, left_val, right_val))\n        pred_train = np.where(X_train[:, feat] <= thr, left_val, right_val)\n        pred_test = np.where(X_test[:, feat] <= thr, left_val, right_val)\n        F_train += learning_rate * pred_train\n        F_test += learning_rate * pred_test\n    y_pred = (F_test >= 0.5).astype(int).tolist()\n    return y_pred\nassert gradient_boosting_classifier([[0],[1],[2],[3]],[0,0,1,1],[[0],[3]],20,0.1)==[0,1],\"failed: basic one-dimensional separation\"\nassert gradient_boosting_classifier([[1],[2],[3],[10]],[0,0,1,1],[[2],[9]],15,0.2)==[0,1],\"failed: unequal gap separation\"\nassert gradient_boosting_classifier([[0],[1],[2],[3],[4]],[0,0,0,1,1],[[0.3],[3.7]],25,0.1)==[0,1],\"failed: threshold after three negatives\"\nassert gradient_boosting_classifier([[-3],[-2],[-1],[1],[2],[3]],[0,0,0,1,1,1],[[-2.5],[2.5]],20,0.1)==[0,1],\"failed: negatives versus positives\"\nassert gradient_boosting_classifier([[0,0],[1,1],[2,2],[3,3]],[0,0,1,1],[[0.1,0.1],[2.5,2.5]],20,0.1)==[0,1],\"failed: two-feature data\"\nassert gradient_boosting_classifier([[i] for i in range(10)],[0]*5+[1]*5,[[0.5],[7.2]],30,0.05)==[0,1],\"failed: larger dataset\"\nassert gradient_boosting_classifier([[1],[1.1],[1.2],[4],[4.1],[4.2]],[0,0,0,1,1,1],[[1.3],[4.05]],25,0.1)==[0,1],\"failed: close clusters\"\nassert gradient_boosting_classifier([[0,5],[0,6],[1,5],[1,6],[10,5],[10,6],[11,5],[11,6]],[0,0,0,0,1,1,1,1],[[0.5,5.5],[10.5,5.5]],20,0.1)==[0,1],\"failed: two-feature well separated\"\nassert gradient_boosting_classifier([[-2],[-1],[0],[1],[2]],[0,0,0,1,1],[[-1.5],[1.5]],20,0.1)==[0,1],\"failed: centred split\"\nassert gradient_boosting_classifier([[0],[1],[2],[3]], [0,0,1,1], [[1.4],[1.6]], 20, 0.1)==[0,1],\"failed: borderline predictions\""}
{"task_id": 505, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef contrastive_divergence(X: np.ndarray, W: np.ndarray, hbias: np.ndarray, vbias: np.ndarray, learning_rate: float, k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM (probabilities only).\"\"\"\n    if X.size == 0:\n        return []\n    m = X.shape[0]\n\n    def _sigmoid(a: np.ndarray) -> np.ndarray:\n        return 1.0 / (1.0 + np.exp(-a))\n    h_prob = _sigmoid(X @ W + hbias)\n    pos_grad = X.T @ h_prob\n    for _ in range(k):\n        v_prob = _sigmoid(h_prob @ W.T + vbias)\n        h_prob = _sigmoid(v_prob @ W + hbias)\n    neg_grad = v_prob.T @ h_prob\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n    return np.round(W_new, 4).tolist()\nassert contrastive_divergence(np.array([[1, 0, 1], [0, 1, 0]]), np.array([[0.1, -0.2], [0.0, 0.05], [-0.1, 0.2]]), np.zeros(2), np.zeros(3), 0.1, 1) == [[0.1007, -0.1997], [-0.0003, 0.0499], [-0.1006, 0.199]], \"test case failed: mixed mini-batch update\"\nassert contrastive_divergence(np.array([[1, 0, 1], [0, 1, 0]]), np.array([[0.1, -0.2], [0.0, 0.05], [-0.1, 0.2]]), np.zeros(2), np.zeros(3), 0.0, 1) == [[0.1, -0.2], [0.0, 0.05], [-0.1, 0.2]], \"test case failed: learning_rate = 0 should keep weights unchanged\"\nassert contrastive_divergence(np.array([[0.0, 0.0]]), np.array([[0.0, 0.0], [0.0, 0.0]]), np.zeros(2), np.zeros(2), 1.0, 1) == [[-0.25, -0.25], [-0.25, -0.25]], \"test case failed: zero input should drive weights negative\"\nassert contrastive_divergence(np.array([[0.0, 0.0]]), np.array([[0.0, 0.0], [0.0, 0.0]]), np.zeros(2), np.zeros(2), 0.0, 1) == [[0.0, 0.0], [0.0, 0.0]], \"test case failed: no-update expected with lr=0\"\nassert contrastive_divergence(np.array([[1.0, 1.0]]), np.array([[0.0], [0.0]]), np.zeros(1), np.zeros(2), 0.1, 1) == [[0.025], [0.025]], \"test case failed: ones input update\"\nassert contrastive_divergence(np.array([[1.0, 0.0, 0.0]]), np.zeros((3, 1)), np.zeros(1), np.zeros(3), 1.0, 1) == [[0.25], [-0.25], [-0.25]], \"test case failed: directional update\"\nassert contrastive_divergence(np.array([[0.0, 1.0]]), np.zeros((2, 1)), np.zeros(1), np.zeros(2), 0.5, 1) == [[-0.125], [0.125]], \"test case failed: lr=0.5 update\"\nassert contrastive_divergence(np.array([[0.0]]), np.array([[0.1]]), np.zeros(1), np.zeros(1), 0.1, 1) == [[0.0737]], \"test case failed: decay on inactive visible node\"\nassert contrastive_divergence(np.array([[0.0]]), np.array([[0.1]]), np.zeros(1), np.zeros(1), 0.0, 1) == [[0.1]], \"test case failed: learning_rate 0 with 1\u00d71 matrix\"\nassert contrastive_divergence(np.array([[1.0]]), np.array([[0.0]]), np.zeros(1), np.zeros(1), 0.2, 1) == [[0.05]], \"test case failed: 1\u00d71 positive update with lr=0.2\""}
{"task_id": 509, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'types.GenericAlias' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef chebyshev(x: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray') -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=float).ravel()\n    y_arr = np.asarray(y, dtype=float).ravel()\n    if x_arr.size != y_arr.size:\n        return -1.0\n    dist = np.max(np.abs(x_arr - y_arr))\n    return float(round(dist, 4))\nassert chebyshev([1, 2, 3], [2, 4, 6]) == 3.0, \"failed for ([1,2,3],[2,4,6])\"\nassert chebyshev([0, 0, 0], [0, 0, 0]) == 0.0, \"failed for zero vectors\"\nassert chebyshev([-1, -2, -3], [1, 2, 3]) == 6.0, \"failed for negatives\"\nassert chebyshev([1.5, 2.5], [1.4, 2.7]) == 0.2, \"failed for floats\"\nassert chebyshev(np.array([1, 2, 3]), np.array([1, 3, 5])) == 2.0, \"failed for numpy arrays\"\nassert chebyshev([1], [10]) == 9.0, \"failed for single-element vectors\"\nassert chebyshev([0.12345], [0.12344]) == 0.0, \"failed for rounding requirement\"\nassert chebyshev([1, 2, 3], [1, 2]) == -1, \"failed for length mismatch\""}
{"task_id": 510, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef vae_loss(y: np.ndarray, y_pred: np.ndarray, t_mean: np.ndarray, t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    Returns the mini-batch average of the binary cross-entropy reconstruction\n    loss plus the analytical KL divergence, rounded to 6 decimal places.\n    \"\"\"\n    eps = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, eps, 1.0 - eps)\n    recon_element = -y * np.log(y_pred_clipped) - (1.0 - y) * np.log(1.0 - y_pred_clipped)\n    recon_loss = recon_element.sum(axis=1)\n    kl_element = 1.0 + t_log_var - np.square(t_mean) - np.exp(t_log_var)\n    kl_loss = -0.5 * kl_element.sum(axis=1)\n    total_loss = recon_loss + kl_loss\n    batch_mean_loss = total_loss.mean()\n    return round(float(batch_mean_loss), 6)\nassert vae_loss(np.array([[1,0],[0,1]]),\n                 np.array([[0.9,0.2],[0.1,0.8]]),\n                 np.zeros((2,2)),\n                 np.zeros((2,2))) == 0.328504, \"test case failed: basic reconstruction only\"\nassert vae_loss(np.array([[1,1],[0,0]]),\n                 np.array([[0.8,0.7],[0.3,0.2]]),\n                 np.array([[0.2,-0.1],[-0.3,0.5]]),\n                 np.array([[-0.2,0.1],[0.1,-0.3]])) == 0.694791, \"test case failed: reconstruction + KL\"\nassert vae_loss(np.array([[1]]),\n                 np.array([[0.5]]),\n                 np.zeros((1,1)),\n                 np.zeros((1,1))) == 0.693147, \"test case failed: single element, zero KL\"\nassert vae_loss(np.array([[1,0,1]]),\n                 np.array([[0.9,0.1,0.4]]),\n                 np.zeros((1,3)),\n                 np.zeros((1,3))) == 1.127012, \"test case failed: three features, no KL\"\nassert vae_loss(np.array([[1,0],[1,0]]),\n                 np.array([[0.7,0.3],[0.6,0.4]]),\n                 np.zeros((2,2)),\n                 np.zeros((2,2))) == 0.867501, \"test case failed: batch size 2, no KL\"\nassert vae_loss(np.array([[1]]),\n                 np.array([[0.8]]),\n                 np.array([[0.5]]),\n                 np.array([[-0.1]])) == 0.350562, \"test case failed: single element with KL\"\nassert vae_loss(np.array([[0]]),\n                 np.array([[0.2]]),\n                 np.array([[0.0]]),\n                 np.array([[0.2]])) == 0.233845, \"test case failed: y=0 with KL\"\nassert vae_loss(np.array([[0,1,0,1]]),\n                 np.array([[0.2,0.9,0.4,0.8]]),\n                 np.zeros((1,1)),\n                 np.zeros((1,1))) == 1.062473, \"test case failed: four features, no KL\"\nassert vae_loss(np.array([[1,1,1]]),\n                 np.array([[0.5,0.5,0.5]]),\n                 np.zeros((1,3)),\n                 np.zeros((1,3))) == 2.079442, \"test case failed: three identical probabilities\"\nassert vae_loss(np.array([[1,0],[0,1]]),\n                 np.array([[0.55,0.45],[0.45,0.55]]),\n                 np.array([[0.1,-0.1],[0.2,0.3]]),\n                 np.zeros((2,2))) == 1.233174, \"test case failed: mixed KL values\""}
{"task_id": 513, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef fm_predict(X: list[list[int | float]], w0: float, w: list[float], V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix (n_samples \u00d7 n_features) given as a list of lists.\n        w0: Global bias term.\n        w: Linear weights (length = n_features).\n        V: Latent factor matrix (n_features \u00d7 k).\n\n    Returns:\n        List with one prediction per sample, rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    w = np.asarray(w, dtype=float)\n    V = np.asarray(V, dtype=float)\n    linear_term = w0 + X @ w\n    XV = X @ V\n    X2_V2 = X ** 2 @ V ** 2\n    interaction_term = 0.5 * np.sum(XV ** 2 - X2_V2, axis=1)\n    y_hat = linear_term + interaction_term\n    return np.round(y_hat, 4).tolist()\nassert fm_predict([[1, 0], [0, 1], [1, 1]], 0.5, [1, 2], [[0.1, 0.2], [0.3, 0.4]]) == [1.5, 2.5, 3.61], \"test case failed: basic example\"\nassert fm_predict([[1, 2]], 0.0, [0, 0], [[1], [1]]) == [2.0], \"test case failed: interaction only\"\nassert fm_predict([[3, 4]], 1.0, [-1, -1], [[0, 0], [0, 0]]) == [-6.0], \"test case failed: no interactions\"\nassert fm_predict([[1, 1], [2, 2]], 0.0, [1, 1], [[0.5], [0.5]]) == [2.25, 5.0], \"test case failed: repeated sample\"\nassert fm_predict([[0, 0]], 0.0, [1, 2], [[0, 0], [0, 0]]) == [0.0], \"test case failed: all zeros\"\nassert fm_predict([[1, 3]], 2.0, [0, 1], [[-0.5], [0.5]]) == [4.25], \"test case failed: negative factors\"\nassert fm_predict([[1, 2, 3], [0, 1, 0]], 0.1, [1, -1, 0.5], [[0.2, 0.3], [0.4, 0.1], [0.5, 0.7]]) == [3.37, -0.9], \"test case failed: 3 features, 2 factors\"\nassert fm_predict([[2]], -1.0, [1], [[0.5]]) == [1.0], \"test case failed: single feature\"\nassert fm_predict([[1, 0], [0, 0], [0, 1]], 0.0, [1, 1], [[0, 0], [0, 0]]) == [1.0, 0.0, 1.0], \"test case failed: mixed zeros\"\nassert fm_predict([[1, 2]], 0.0, [0, 0], [[0.5], [1.5]]) == [1.5], \"test case failed: fractional interaction\""}
{"task_id": 517, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(z):\n    \"\"\"Compute a numerically-stable, element-wise sigmoid activation.\n\n    Parameters\n    ----------\n    z : int | float | list | tuple | numpy.ndarray\n        Input value(s).\n\n    Returns\n    -------\n    float | list\n        If `z` is a scalar a Python float is returned, otherwise a (nested)\n        list whose elements are the sigmoid of the corresponding entries in\n        `z`, rounded to four decimal places.\n    \"\"\"\n    arr = np.asarray(z, dtype=np.float64)\n    out = np.empty_like(arr)\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    exp_x = np.exp(arr[neg_mask])\n    out[neg_mask] = exp_x / (1.0 + exp_x)\n    out = np.round(out, 4)\n    if out.ndim == 0:\n        return float(out)\n    else:\n        return out.tolist()\nassert sigmoid(0) == 0.5, \"test case failed: sigmoid(0)\"\nassert sigmoid(5) == 0.9933, \"test case failed: sigmoid(5)\"\nassert sigmoid(-5) == 0.0067, \"test case failed: sigmoid(-5)\"\nassert sigmoid(100) == 1.0, \"test case failed: sigmoid(100)\"\nassert sigmoid(-100) == 0.0, \"test case failed: sigmoid(-100)\"\nassert sigmoid([-1, 0, 1]) == [0.2689, 0.5, 0.7311], \"test case failed: sigmoid([-1,0,1])\"\nassert sigmoid(np.array([2, -2, 0.5, -0.5])) == [0.8808, 0.1192, 0.6225, 0.3775], \"test case failed: sigmoid([2,-2,0.5,-0.5])\"\nassert sigmoid([[0, 10], [-10, 1]]) == [[0.5, 1.0], [0.0, 0.7311]], \"test case failed: sigmoid([[0,10],[-10,1]])\"\nassert sigmoid([700, -700]) == [1.0, 0.0], \"test case failed: sigmoid([700,-700])\"\nassert sigmoid(np.array([[3]])) == [[0.9526]], \"test case failed: sigmoid([[3]])\""}
{"task_id": 518, "completion_id": 0, "passed": false, "result": "failed: name '_accuracy' is not defined", "solution": "import numpy as np\nfrom typing import Callable\nimport numpy as np\nfrom typing import Callable\nfrom functools import wraps\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D NumPy array whose second\n    dimension is larger than one, it is assumed to be one-hot encoded and is\n    replaced by the index of the maximum value along axis 1 before the wrapped\n    *function* is executed.\n    \"\"\"\n\n    def _decode(arr: np.ndarray) -> np.ndarray:\n        \"\"\"Return a 1-D label array if *arr* looks like one-hot, else *arr*.\"\"\"\n        if isinstance(arr, np.ndarray) and arr.ndim == 2 and (arr.shape[1] > 1):\n            return arr.argmax(axis=1)\n        return arr\n\n    @wraps(function)\n    def wrapper(actual: np.ndarray, predicted: np.ndarray, *args, **kwargs):\n        actual_decoded = _decode(actual)\n        predicted_decoded = _decode(predicted)\n        return function(actual_decoded, predicted_decoded, *args, **kwargs)\n    return wrapper\nassert abs(_accuracy(np.array([[0,1,0],[1,0,0],[0,0,1]]), np.array([[0,1,0],[0,1,0],[0,0,1]])) - 0.6667) < 1e-4, \"test case failed: accuracy with both inputs one-hot\"\nassert abs(_accuracy(np.array([[1,0],[0,1]]), np.array([[1,0],[0,1]])) - 1.0) < 1e-9, \"test case failed: perfect match one-hot\"\nassert abs(_accuracy(np.array([0,1,1,0]), np.array([[1,0],[0,1],[0,1],[1,0]])) - 1.0) < 1e-9, \"test case failed: actual labels, predicted one-hot\"\nassert abs(_accuracy(np.array([[0,0,1],[1,0,0],[0,1,0]]), np.array([2,0,1])) - 1.0) < 1e-9, \"test case failed: actual one-hot, predicted labels\"\nassert abs(_accuracy(np.array([[1,0],[0,1],[1,0],[0,1]]), np.array([[1,0],[1,0],[1,0],[0,1]])) - 0.75) < 1e-9, \"test case failed: two-class partial mismatch\"\nassert _collect(np.array([[2],[0],[1]]), np.array([[2],[0],[1]])) == ([[2],[0],[1]], [[2],[0],[1]]), \"test case failed: shape (n,1) should remain unchanged\"\nassert _collect(np.array([1,0,2]), np.array([[0,1,0],[1,0,0],[0,0,1]])) == ([1,0,2], [1,0,2]), \"test case failed: mixed dimensionalities\"\nassert _collect(np.eye(5, dtype=int), np.eye(5, dtype=int))[0] == [0,1,2,3,4], \"test case failed: five-class eye matrix\"\nassert _collect(np.array([[0,1,0],[1,0,0],[0,0,1]]), np.array([[0,1,0],[1,0,0],[0,0,1]])) == ([1,0,2],[1,0,2]), \"test case failed: explicit conversion check\"\nassert _collect(np.array([3,1,4,1,5]), np.array([3,1,4,1,5])) == ([3,1,4,1,5],[3,1,4,1,5]), \"test case failed: 1-D labels untouched\""}
{"task_id": 520, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (1,2) (0,) ", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).\n    The derivative is computed element-wise as z * (1 - z) and rounded\n    to 4 decimal places.\n\n    Args:\n        z: float, list, or NumPy ndarray holding sigmoid outputs.\n\n    Returns:\n        float if *z* is a scalar; otherwise a (nested) list with the same\n        shape as *z*, each value rounded to 4 decimals.\n    \"\"\"\n    if z == []:\n        return []\n    arr = np.asarray(z, dtype=float)\n    grad = np.round(arr * (1.0 - arr), 4)\n    return float(grad) if grad.ndim == 0 else grad.tolist()\nassert sigmoid_grad(0.5) == 0.25, \"failed on scalar 0.5\"\nassert sigmoid_grad(0.8) == 0.16, \"failed on scalar 0.8\"\nassert sigmoid_grad([0.5, 0.8]) == [0.25, 0.16], \"failed on list [0.5,0.8]\"\nassert sigmoid_grad([[0.5, 0.4], [0.7, 0.3]]) == [[0.25, 0.24], [0.21, 0.21]], \"failed on 2D list\"\nassert sigmoid_grad(0.0) == 0.0, \"failed on boundary 0.0\"\nassert sigmoid_grad(1.0) == 0.0, \"failed on boundary 1.0\"\nassert sigmoid_grad([0.2, 0.4, 0.6, 0.8]) == [0.16, 0.24, 0.24, 0.16], \"failed on list [0.2,0.4,0.6,0.8]\"\nassert sigmoid_grad([0.7310586]) == [0.1966], \"failed on list [0.7310586]\"\nassert sigmoid_grad([]) == [], \"failed on empty list\"\nassert sigmoid_grad(np.array([[0.25, 0.75]])) == [[0.1875, 0.1875]], \"failed on numpy input\""}
{"task_id": 528, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef decision_boundary_grid(X: list[list[int | float]], W: list[int | float], b: float, grid_n: int=100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) receives the label  1 if  w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0\n    and \u22121 otherwise.  The returned list has shape (grid_n \u00d7 grid_n) and\n    contains those labels for a regular grid that spans the bounding box\n    of *X*.\n\n    Args:\n        X: 2-D data set \u2013 list of [x\u2081 , x\u2082] pairs (shape n\u00d72).\n        W: Length-2 sequence [w\u2081 , w\u2082] with the classifier\u2019s weights.\n        b: Bias term of the classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Python list of lists with shape (grid_n \u00d7 grid_n) whose entries\n        are exclusively 1 or \u22121.\n    \"\"\"\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim != 2 or X_arr.shape[1] != 2:\n        raise ValueError('X must be a 2-D array-like with exactly two columns.')\n    if len(W) != 2:\n        raise ValueError('W must contain exactly two elements.')\n    if grid_n <= 0:\n        raise ValueError('grid_n must be a positive integer.')\n    (x1_min, x1_max) = (X_arr[:, 0].min(), X_arr[:, 0].max())\n    (x2_min, x2_max) = (X_arr[:, 1].min(), X_arr[:, 1].max())\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    (x1_grid, x2_grid) = np.meshgrid(x1_plot, x2_plot, indexing='xy')\n    (w1, w2) = (float(W[0]), float(W[1]))\n    decision_values = w1 * x1_grid + w2 * x2_grid + b\n    labels = np.where(decision_values >= 0, 1, -1)\n    return labels.tolist()\nassert decision_boundary_grid([[0, 0], [2, 2]], [1, -1], 0, 3) == [[1, 1, 1], [-1, 1, 1], [-1, -1, 1]], \"test case failed: decision_boundary_grid([[0, 0], [2, 2]], [1, -1], 0, 3)\"\nassert decision_boundary_grid([[0, 0], [2, 2]], [0, 1], -1, 3) == [[-1, -1, -1], [1, 1, 1], [1, 1, 1]], \"test case failed: decision_boundary_grid([[0, 0], [2, 2]], [0, 1], -1, 3)\"\nassert decision_boundary_grid([[0, 0], [2, 2]], [1, 0], -1, 3) == [[-1, 1, 1], [-1, 1, 1], [-1, 1, 1]], \"test case failed: decision_boundary_grid([[0, 0], [2, 2]], [1, 0], -1, 3)\"\nassert decision_boundary_grid([[0, 0], [1, 1]], [-1, -1], 1, 2) == [[1, 1], [1, -1]], \"test case failed: decision_boundary_grid([[0, 0], [1, 1]], [-1, -1], 1, 2)\"\nres = decision_boundary_grid([[0, 0], [3, 3]], [1, 1], -6, 4)\nassert len(res) == 4 and all(len(r) == 4 for r in res), \"test case failed: decision_boundary_grid shape with grid_n=4\"\nassert decision_boundary_grid([[-2, -2], [2, 2]], [1, 1], 0, 2) == [[-1, 1], [1, 1]], \"test case failed: decision_boundary_grid([[-2, -2], [2, 2]], [1, 1], 0, 2)\"\nall_pos = decision_boundary_grid([[0, 0], [1, 1]], [0, 0], 5, 2)\nassert all(v == 1 for row in all_pos for v in row), \"test case failed: decision_boundary_grid bias all positive\"\nall_neg = decision_boundary_grid([[0, 0], [1, 1]], [0, 0], -5, 2)\nassert all(v == -1 for row in all_neg for v in row), \"test case failed: decision_boundary_grid bias all negative\"\nassert decision_boundary_grid([[0, 0], [4, 1]], [1, -1], 0, 3)[0] == [1, 1, 1], \"test case failed: decision_boundary_grid non square range\"\nret = decision_boundary_grid([[0, 0], [1, 1]], [1, 1], 0, 2)\nassert isinstance(ret, list) and all(isinstance(r, list) for r in ret), \"test case failed: decision_boundary_grid return type\""}
{"task_id": 537, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier (from scratch).\"\"\"\n    eps = 1e-09\n    classes = np.array([0, 1])\n    mask0 = y_train == 0\n    mask1 = y_train == 1\n    mu0 = X_train[mask0].mean(axis=0)\n    mu1 = X_train[mask1].mean(axis=0)\n    std0 = X_train[mask0].std(axis=0, ddof=0) + eps\n    std1 = X_train[mask1].std(axis=0, ddof=0) + eps\n    n_samples = len(y_train)\n    prior0 = mask0.sum() / n_samples\n    prior1 = mask1.sum() / n_samples\n    log_prior = np.log(np.array([prior0, prior1]))\n    log_coeff0 = -0.5 * np.log(2 * np.pi) - np.log(std0)\n    log_coeff1 = -0.5 * np.log(2 * np.pi) - np.log(std1)\n    inv_var0 = 1.0 / (2 * std0 ** 2)\n    inv_var1 = 1.0 / (2 * std1 ** 2)\n    predictions = []\n    for x in X_test:\n        ll0 = (log_coeff0 - (x - mu0) ** 2 * inv_var0).sum()\n        ll1 = (log_coeff1 - (x - mu1) ** 2 * inv_var1).sum()\n        log_post0 = log_prior[0] + ll0\n        log_post1 = log_prior[1] + ll1\n        pred = 0 if log_post0 >= log_post1 else 1\n        predictions.append(pred)\n    return predictions\nassert gaussian_naive_bayes(np.array([[1.0,20.0],[2.0,21.0],[3.0,22.0],[10.0,5.0],[11.0,6.0],[12.0,7.0]]), np.array([0,0,0,1,1,1]), np.array([[2.5,20.5],[11.5,6.5]])) == [0,1], \"test case 1 failed\"\nassert gaussian_naive_bayes(np.array([[1],[2],[3],[8],[9],[10]]), np.array([0,0,0,1,1,1]), np.array([[1.5],[8.5]])) == [0,1], \"test case 2 failed\"\nassert gaussian_naive_bayes(np.array([[0,0],[0,1],[1,0],[1,1]]), np.array([0,0,1,1]), np.array([[0,0.2],[0.9,0.9]])) == [0,1], \"test case 3 failed\"\nassert gaussian_naive_bayes(np.array([[5],[6],[7],[2],[3],[4]]), np.array([1,1,1,0,0,0]), np.array([[6.5],[2.5]])) == [1,0], \"test case 4 failed\"\nassert gaussian_naive_bayes(np.array([[1,1],[1,2],[2,1],[8,8],[9,9],[9,8]]), np.array([0,0,0,1,1,1]), np.array([[1.8,1.5],[8.5,8.3]])) == [0,1], \"test case 5 failed\"\nassert gaussian_naive_bayes(np.array([[2,3],[2,2],[3,3],[7,7],[8,7],[8,6]]), np.array([0,0,0,1,1,1]), np.array([[2.1,2.9],[7.9,6.9]])) == [0,1], \"test case 6 failed\"\nassert gaussian_naive_bayes(np.array([[10],[11],[12],[1],[2],[3]]), np.array([1,1,1,0,0,0]), np.array([[11],[2]])) == [1,0], \"test case 7 failed\"\nassert gaussian_naive_bayes(np.array([[1,2,3],[1,2,2],[2,2,3],[8,9,9],[9,8,9],[9,9,8]]), np.array([0,0,0,1,1,1]), np.array([[1.5,2,2.8],[9,8.8,8.9]])) == [0,1], \"test case 8 failed\"\nassert gaussian_naive_bayes(np.array([[0],[0],[0],[10],[10],[10]]), np.array([0,0,0,1,1,1]), np.array([[0.1],[9.9]])) == [0,1], \"test case 9 failed\"\nassert gaussian_naive_bayes(np.array([[4,5],[4,4],[5,4],[15,15],[16,14],[15,14]]), np.array([0,0,0,1,1,1]), np.array([[4.2,4.6],[15.2,14.8]])) == [0,1], \"test case 10 failed\""}
{"task_id": 539, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Computes the binary cross-entropy (log-loss) cost.\n\n    Parameters\n    ----------\n    AL : np.ndarray\n        Model predicted probabilities, shape (m,) or (1, m).\n    Y  : np.ndarray\n        Ground-truth labels (0 or 1), same shape as AL.\n\n    Returns\n    -------\n    float\n        Scalar cost value.\n    \"\"\"\n    if AL.shape != Y.shape:\n        raise ValueError('AL and Y must have the same shape.')\n    AL = AL.ravel()\n    Y = Y.ravel()\n    m = AL.size\n    eps = 1e-15\n    AL_clipped = np.clip(AL, eps, 1.0 - eps)\n    cost = -np.sum(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped)) / m\n    return float(cost)\nassert abs(compute_cost(np.array([[0.8,0.9,0.4]]),np.array([[1,1,0]]))-( -(1/3)*np.sum(np.array([[1,1,0]])*np.log(np.array([[0.8,0.9,0.4]]))+(1-np.array([[1,1,0]]))*np.log(1-np.array([[0.8,0.9,0.4]])) ) ))<1e-12, \"test case failed: simple example\"\nassert abs(compute_cost(np.array([0.35]),np.array([0]))-(-np.sum(np.array([0])*np.log(np.array([0.35]))+(1-np.array([0]))*np.log(1-np.array([0.35])))))<1e-12, \"test case failed: single sample\"\nassert abs(compute_cost(np.array([1e-20,1-1e-20]),np.array([0,1])) - (-(1/2)*np.sum(np.array([0,1])*np.log(np.clip(np.array([1e-20,1-1e-20]),1e-15,1-1e-15)) + (1-np.array([0,1]))*np.log(1-np.clip(np.array([1e-20,1-1e-20]),1e-15,1-1e-15)))))<1e-12, \"test case failed: clipping\"\nassert abs(compute_cost(np.array([[0.2,0.4,0.6,0.8,0.5]]),np.array([[0,0,1,1,0]]))-( -(1/5)*np.sum(np.array([[0,0,1,1,0]])*np.log(np.array([[0.2,0.4,0.6,0.8,0.5]]))+(1-np.array([[0,0,1,1,0]]))*np.log(1-np.array([[0.2,0.4,0.6,0.8,0.5]])) ) ))<1e-12, \"test case failed: batch size 5\"\nassert abs(compute_cost(np.array([0.7,0.3,0.2]),np.array([1,0,0]))-( -(1/3)*np.sum(np.array([1,0,0])*np.log(np.array([0.7,0.3,0.2]))+(1-np.array([1,0,0]))*np.log(1-np.array([0.7,0.3,0.2])) ) ))<1e-12, \"test case failed: flat vector\"\nassert abs(compute_cost(np.array([0.1,0.2,0.3,0.4]),np.zeros(4))-( -(1/4)*np.sum((1-np.zeros(4))*np.log(1-np.array([0.1,0.2,0.3,0.4])) ) ))<1e-12, \"test case failed: all zeros\"\nassert abs(compute_cost(np.array([0.6,0.7,0.8]),np.ones(3))-( -(1/3)*np.sum(np.ones(3)*np.log(np.array([0.6,0.7,0.8])) ) ))<1e-12, \"test case failed: all ones\"\nnp.random.seed(0); AL=np.random.rand(1,10); Y=(AL>0.5).astype(float); assert abs(compute_cost(AL,Y)-(-(1/10)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))))<1e-12, \"test case failed: random example 1\"\nnp.random.seed(42); AL=np.random.rand(10); Y=(np.random.rand(10)>0.3).astype(float); assert abs(compute_cost(AL,Y)-(-(1/10)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))))<1e-12, \"test case failed: random example 2\"\nnp.random.seed(7); AL=np.random.rand(1,100); Y=(np.random.rand(1,100)>0.5).astype(float); assert abs(compute_cost(AL,Y)-(-(1/100)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))))<1e-12, \"test case failed: large batch\""}
{"task_id": 552, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation uses the forward-backward algorithm with scaling\n    to obtain numerically stable \u03b1, \u03b2 and finally \u03b3\u209c.\n\n    Args:\n        hmm (dict): HMM parameters with keys\n                    'pi' -> 1-D initial state distribution (N,)\n                    'A'  -> state transition matrix      (N,N)\n                    'B'  -> emission matrix              (N,M)\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t,\n                     rounded to four decimals.  If t is out of range an\n                     empty list is returned.\n    \"\"\"\n    if t < 0 or t >= len(observations):\n        return []\n    pi = np.asarray(hmm['pi'], dtype=float)\n    A = np.asarray(hmm['A'], dtype=float)\n    B = np.asarray(hmm['B'], dtype=float)\n    N = pi.shape[0]\n    T = len(observations)\n    obs_idx = observations\n    alpha = np.zeros((T, N), dtype=float)\n    c = np.zeros(T, dtype=float)\n    alpha[0] = pi * B[:, obs_idx[0]]\n    c[0] = 1.0 / alpha[0].sum() if alpha[0].sum() != 0 else 1.0\n    alpha[0] *= c[0]\n    for tt in range(1, T):\n        alpha[tt] = alpha[tt - 1] @ A * B[:, obs_idx[tt]]\n        denom = alpha[tt].sum()\n        c[tt] = 1.0 / denom if denom != 0 else 1.0\n        alpha[tt] *= c[tt]\n    beta = np.zeros((T, N), dtype=float)\n    beta[-1] = 1.0\n    beta[-1] *= c[-1]\n    for tt in range(T - 2, -1, -1):\n        beta[tt] = A @ (B[:, obs_idx[tt + 1]] * beta[tt + 1])\n        beta[tt] *= c[tt]\n    gamma_t = alpha[t] * beta[t]\n    total = gamma_t.sum()\n    if total == 0:\n        return [0.0 for _ in range(N)]\n    gamma_t /= total\n    return [round(float(p), 4) for p in gamma_t]\nassert gamma_probabilities({\"pi\": [0.6, 0.4], \"A\": [[0.7, 0.3], [0.4, 0.6]], \"B\": [[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]}, [0,1,2], 1) == [0.6229, 0.3771], \"test case failed: example sequence t=1\"\nassert gamma_probabilities({\"pi\": [0.5,0.5], \"A\": [[0.5,0.5],[0.5,0.5]], \"B\": [[0.5,0.5],[0.5,0.5]]}, [0,1,0,1], 2) == [0.5,0.5], \"test case failed: uniform 2-state\"\nassert gamma_probabilities({\"pi\": [0.5,0.5], \"A\": [[0.5,0.5],[0.5,0.5]], \"B\": [[0.5,0.5],[0.5,0.5]]}, [1], 0) == [0.5,0.5], \"test case failed: uniform 2-state single step\"\nassert gamma_probabilities({\"pi\": [1/3,1/3,1/3], \"A\": [[1/3]*3]*3, \"B\": [[0.25,0.25],[0.25,0.25],[0.25,0.25]]}, [0,1], 1) == [0.3333,0.3333,0.3333], \"test case failed: uniform 3-state\"\nassert gamma_probabilities({\"pi\": [0.5,0.5], \"A\": [[0.5,0.5],[0.5,0.5]], \"B\": [[1,0],[0,1]]}, [0,0,0], 2) == [1.0,0.0], \"test case failed: deterministic emission state 0\"\nassert gamma_probabilities({\"pi\": [0.5,0.5], \"A\": [[0.5,0.5],[0.5,0.5]], \"B\": [[1,0],[0,1]]}, [1], 0) == [0.0,1.0], \"test case failed: deterministic emission state 1\"\nassert gamma_probabilities({\"pi\": [0.8,0.2], \"A\": [[0.6,0.4],[0.4,0.6]], \"B\": [[0.6,0.4],[0.3,0.7]]}, [0], 0) == [0.8889,0.1111], \"test case failed: single observation\"\nassert gamma_probabilities({\"pi\": [0.25,0.25,0.25,0.25], \"A\": [[0.25]*4]*4, \"B\": [[0.25]*4]*4}, [0,1,2,3], 3) == [0.25,0.25,0.25,0.25], \"test case failed: uniform 4-state\""}
{"task_id": 555, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom collections import deque\nimport numpy as np\nfrom collections import deque\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN) implemented from scratch.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Array of shape (n_samples, n_features).\n    eps : float\n        Neighbourhood radius.\n    min_samples : int\n        Minimum number of points (inclusive) needed for a core point.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels (1, 2, \u2026) for clustered points; noise is \u22121.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array (n_samples, n_features)')\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n    diff = data[:, None, :] - data[None, :, :]\n    dist_mat = np.linalg.norm(diff, axis=-1)\n    neighbours = [np.where(dist_mat[i] <= eps)[0] for i in range(n_samples)]\n    core_mask = np.array([len(idx) >= min_samples for idx in neighbours], dtype=bool)\n    labels = np.zeros(n_samples, dtype=int)\n    current_label = 0\n    for point in range(n_samples):\n        if not core_mask[point] or labels[point] != 0:\n            continue\n        current_label += 1\n        queue = deque([point])\n        labels[point] = current_label\n        while queue:\n            p = queue.popleft()\n            for nbr in neighbours[p]:\n                if labels[nbr] == 0:\n                    labels[nbr] = current_label\n                    if core_mask[nbr]:\n                        queue.append(nbr)\n    labels[labels == 0] = -1\n    return labels.tolist()\nassert dbscan(np.array([[0,0],[0,1],[1,0],[1,1],[5,5]]),1.5,3)==[1,1,1,1,-1],\"case 1 failed\"\nassert dbscan(np.array([[0,0],[0,1],[1,0],[10,10],[10,11],[11,10]]),1.5,3)==[1,1,1,2,2,2],\"case 2 failed\"\nassert dbscan(np.array([[0,0],[0,1],[1,0],[10,10],[10,11],[11,10]]),1.5,4)==[-1,-1,-1,-1,-1,-1],\"case 3 failed\"\nassert dbscan(np.array([[0,0],[0,0],[0,0.1],[0.1,0]]),0.2,2)==[1,1,1,1],\"case 4 failed\"\nassert dbscan(np.array([[0],[0.5],[1.0],[5.0],[5.1],[5.2]]),0.3,2)==[-1,-1,-1,1,1,1],\"case 5 failed\"\nassert dbscan(np.array([[0,0],[0,1],[1,0],[10,10],[10,11],[11,10]]),15,2)==[1,1,1,1,1,1],\"case 6 failed\"\nassert dbscan(np.array([[0,0],[0,1],[0,2],[0,3]]),1.1,2)==[1,1,1,1],\"case 7 failed\"\nassert dbscan(np.array([[0,0],[0,0.5],[10,10],[20,20]]),1,2)==[1,1,-1,-1],\"case 8 failed\"\nassert dbscan(np.array([[0,0]]),0.5,1)==[1],\"case 9 failed\"\nassert dbscan(np.array([[-5,-5],[-5,-4],[-4,-5],[5,5]]),1.5,3)==[1,1,1,-1],\"case 10 failed\""}
{"task_id": 561, "completion_id": 0, "passed": false, "result": "failed: test case failed: basic two clusters", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef gmm_em(data: np.ndarray, k: int, max_iter: int=200, epsilon: float=0.0001) -> list[int]:\n    \"\"\"\n    Simple EM implementation for a full-covariance Gaussian Mixture Model.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array with shape (N, D); N samples, D features.\n    k : int\n        Number of mixture components (clusters).\n    max_iter : int, optional\n        Maximum number of EM iterations (default 200).\n    epsilon : float, optional\n        Convergence tolerance on the maximum absolute change of the mixture\n        weights between two successive iterations (default 1 e-4).\n\n    Returns\n    -------\n    list[int]\n        Length-N list of final cluster labels (0 \u2026 k-1) in deterministic,\n        left-to-right order (ascending first coordinate of the component mean).\n\n    Notes\n    -----\n    \u2022 If k == 1 all samples receive label 0 and the algorithm stops\n      immediately.\n    \u2022 A tiny diagonal regulariser (1 e-6) is added to every covariance\n      estimate to guarantee positive-definiteness and numerical stability.\n    \"\"\"\n    data = np.asarray(data, dtype=float)\n    if data.ndim != 2:\n        raise ValueError('`data` must be a 2-D array (N samples, D features).')\n    (n_samples, n_features) = data.shape\n    if k < 1:\n        raise ValueError('`k` must be a positive integer.')\n    if k == 1:\n        return [0] * n_samples\n    _LOG_2PI = n_features * math.log(2.0 * math.pi)\n\n    def _log_gaussian(x: np.ndarray, mean: np.ndarray, cov: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute log N(x | mean, cov) for all rows of `x`.\n        x : (N, D)\n        mean : (D,)\n        cov : (D, D)\n        Returns\n        -------\n        log_prob : (N,)\n        \"\"\"\n        diff = x - mean\n        try:\n            L = np.linalg.cholesky(cov)\n            y = np.linalg.solve(L, diff.T)\n            maha = np.sum(y ** 2, axis=0)\n            log_det = 2.0 * np.sum(np.log(np.diag(L)))\n        except np.linalg.LinAlgError:\n            inv_cov = np.linalg.inv(cov)\n            maha = np.einsum('ij,ij->i', diff, diff @ inv_cov)\n            (sign, log_det) = np.linalg.slogdet(cov)\n            if sign <= 0:\n                log_det = np.log(np.abs(np.linalg.det(cov) + 1e-12))\n        return -0.5 * (_LOG_2PI + log_det + maha)\n    rng = np.random.default_rng()\n    init_idx = rng.choice(n_samples, size=k, replace=False)\n    means = data[init_idx]\n    weights = np.full(k, 1.0 / k)\n    global_cov = np.cov(data.T, bias=True)\n    if global_cov.shape == ():\n        global_cov = global_cov.reshape(1, 1)\n    covs = np.array([global_cov.copy() for _ in range(k)])\n    reg_eps = 1e-06 * np.eye(n_features)\n    for _ in range(max_iter):\n        log_resp = np.empty((n_samples, k))\n        for j in range(k):\n            log_resp[:, j] = np.log(weights[j] + 1e-16) + _log_gaussian(data, means[j], covs[j])\n        log_resp_max = np.max(log_resp, axis=1, keepdims=True)\n        resp = np.exp(log_resp - log_resp_max)\n        resp_sum = resp.sum(axis=1, keepdims=True)\n        resp /= resp_sum\n        nk = resp.sum(axis=0)\n        prev_weights = weights.copy()\n        weights = nk / n_samples\n        means = resp.T @ data / nk[:, None]\n        for j in range(k):\n            diff = data - means[j]\n            cov = (resp[:, j][:, None] * diff).T @ diff / nk[j]\n            covs[j] = cov + reg_eps\n        if np.max(np.abs(weights - prev_weights)) < epsilon:\n            break\n    log_resp = np.empty((n_samples, k))\n    for j in range(k):\n        log_resp[:, j] = np.log(weights[j] + 1e-16) + _log_gaussian(data, means[j], covs[j])\n    labels = np.argmax(log_resp, axis=1)\n    ordering = np.argsort(means[:, 0])\n    remap = np.zeros(k, dtype=int)\n    remap[ordering] = np.arange(k)\n    labels = remap[labels]\n    return labels.tolist()\nimport numpy as np\nassert gmm_em(np.array([[1,2],[1,3],[2,2],[8,8],[9,8],[9,9]]),2) == [0,0,0,1,1,1], \"test case failed: basic two clusters\"\nassert gmm_em(np.array([[1],[1.2],[1.1],[8],[9],[8.5]]),2) == [0,0,0,1,1,1], \"test case failed: one-dimensional data\"\nassert gmm_em(np.array([[3,4],[5,6]]),1) == [0,0], \"test case failed: single component\"\nassert gmm_em(np.array([[2,2],[2,2],[2,2],[8,8],[8,8],[8,8]]),2) == [0,0,0,1,1,1], \"test case failed: duplicate points\"\nassert gmm_em(np.array([[0,0],[0,0.1],[0,0.2],[5,0],[5,0.1],[5,0.2]]),2) == [0,0,0,1,1,1], \"test case failed: vertical separation\"\nassert gmm_em(np.array([[-5,-5],[-4.9,-5],[-5.2,-4.8],[5,5],[4.8,5.1],[5.2,4.9]]),2) == [0,0,0,1,1,1], \"test case failed: symmetric clusters\"\nassert gmm_em(np.array([[0,0,0],[0.1,0,0],[0,0.2,0.1],[5,5,5],[5.1,5.1,5.1],[4.9,5,5.2]]),2) == [0,0,0,1,1,1], \"test case failed: three-dimensional data\"\nassert gmm_em(np.array([[1,1],[1,1.1],[1.2,1],[1,0.9],[9,9],[9.1,9],[9,9.2],[9.2,9.1]]),2) == [0,0,0,0,1,1,1,1], \"test case failed: larger cluster size\"\nassert gmm_em(np.array([[-10,-10],[ -9.8,-9.7],[-10.2,-10.1],[10,10],[9.8,9.9],[10.1,10.2]]),2) == [0,0,0,1,1,1], \"test case failed: distant clusters\""}
{"task_id": 562, "completion_id": 0, "passed": false, "result": "failed: test case failed: 3-D data", "solution": "import numpy as np\nimport numpy as np\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float=2.0, method: str='unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A Python list containing the cluster label (0-based) for each sample.\n    \"\"\"\n    N = data.shape[0]\n    if n_cluster <= 1 or N == 0:\n        return [0] * N\n    if method not in {'unnormalized', 'normalized'}:\n        raise ValueError(\"`method` must be either 'unnormalized' or 'normalized'\")\n    sq_norms = np.sum(data ** 2, axis=1)\n    sq_dists = sq_norms[:, None] + sq_norms[None, :] - 2.0 * data @ data.T\n    sq_dists = np.maximum(sq_dists, 0.0)\n    W = np.exp(-gamma * sq_dists)\n    np.fill_diagonal(W, 0.0)\n    degree = np.sum(W, axis=1)\n    if method == 'unnormalized':\n        L = np.diag(degree) - W\n    else:\n        d_inv_sqrt = 1.0 / np.sqrt(degree + 1e-12)\n        L = np.eye(N) - d_inv_sqrt[:, None] * W * d_inv_sqrt[None, :]\n    (eigvals, eigvecs) = np.linalg.eigh(L)\n    U = eigvecs[:, :n_cluster]\n    if method == 'normalized':\n        row_norm = np.linalg.norm(U, axis=1, keepdims=True)\n        U = U / (row_norm + 1e-12)\n    embedding = U\n    centroids = embedding[:n_cluster].copy()\n    labels = np.full(N, -1, dtype=int)\n    for _ in range(300):\n        dists = ((embedding[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new_labels = np.argmin(dists, axis=1)\n        if np.array_equal(labels, new_labels):\n            break\n        labels = new_labels\n        for k in range(n_cluster):\n            members = embedding[labels == k]\n            if len(members) > 0:\n                centroids[k] = members.mean(axis=0)\n    unique_old = np.unique(labels)\n    order = sorted(((l, np.where(labels == l)[0][0]) for l in unique_old))\n    relabel_map = {old: new for (new, (old, _)) in enumerate(order)}\n    final_labels = [int(relabel_map[l]) for l in labels]\n    return final_labels\nassert spectral_clustering(np.array([[0, 0], [5, 5], [0.2, -0.2], [-0.1, 0.3], [5.1, 4.9]]), 2) == [0, 1, 0, 0, 1], \"test case failed: a1, 2\"\nassert spectral_clustering(np.array([[-5, -5], [5, 5], [-4.8, -5.2], [5.2, 5.1]]), 2, method='normalized') == [0, 1, 0, 1], \"test case failed: a2, 2 normalized\"\nassert spectral_clustering(np.array([[0, 0], [10, 10], [20, 20], [0.2, -0.1], [9.8, 10.2], [19.9, 20.2]]), 3) == [0, 1, 2, 0, 1, 2], \"test case failed: a3, 3\"\nassert spectral_clustering(np.random.randn(8, 2), 1) == [0]*8, \"test case failed: single cluster\"\nassert spectral_clustering(np.array([[1, 1], [1, 1], [10, 10], [10, 10]]), 2) == [0, 0, 1, 1], \"test case failed: repeated points\"\nassert spectral_clustering(np.array([[0, 0], [5, 0], [0.1, 0.2], [4.9, -0.1]]), 2, gamma=0.5) == [0, 1, 0, 1], \"test case failed: gamma=0.5\"\nassert spectral_clustering(np.array([[0, 0, 0], [0.1, 0.0, -0.1], [5, 5, 5], [5.1, 4.9, 5.2]]), 2) == [0, 0, 1, 1], \"test case failed: 3-D data\"\nassert spectral_clustering(np.array([[0, 0], [0, 5], [0.1, 0.2], [0.05, -0.1], [-0.02, 5.1]]), 2) == [0, 1, 0, 0, 1], \"test case failed: vertical split\"\nassert spectral_clustering(np.array([[0, 0], [10, 0], [0.1, 0.05], [9.9, -0.05]]), 2) == [0, 1, 0, 1], \"test case failed: horizontal split\"\nassert spectral_clustering(np.array([[0, 0], [5, 5], [10, 10], [0.1, -0.1], [5.1, 4.9], [9.9, 10.2]]), 3, method='normalized') == [0, 1, 2, 0, 1, 2], \"test case failed: 3 clusters normalized\""}
{"task_id": 563, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef perceptron_dual_train(X_data: np.ndarray, y_data: np.ndarray, eta: float=1.0, max_iter: int=1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array with N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate.\n        max_iter: Maximum number of full passes over the training data.\n\n    Returns:\n        (w, b)  \u2013 w is the learned weight vector as a Python list,\n                  b is the bias term; both rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X_data, dtype=float)\n    y = np.asarray(y_data, dtype=float).flatten()\n    (N, d) = X.shape\n    alpha = np.zeros(N, dtype=float)\n    b = 0.0\n    G = X @ X.T\n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            g = np.sum(alpha * y * G[:, i])\n            if y[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y[i]\n                updated = True\n        if not updated:\n            break\n    w_vec = X.T @ (alpha * y)\n    w_rounded = [round(float(val), 4) for val in w_vec]\n    b_rounded = round(float(b), 4)\n    return (w_rounded, b_rounded)\nassert perceptron_dual_train(np.array([[2, 1], [1, 2], [-2, -1], [-1, -2]]), np.array([1, 1, -1, -1])) == ([2.0, 1.0], 1.0), \"failed test 1\"\nassert perceptron_dual_train(np.array([[1, 0], [2, 0], [-1, 0], [-2, 0]]), np.array([1, 1, -1, -1])) == ([2.0, 0.0], 0.0), \"failed test 2\"\nassert perceptron_dual_train(np.array([[2, 2], [1, 1], [-1, -1], [-2, -2]]), np.array([1, 1, -1, -1])) == ([2.0, 2.0], 1.0), \"failed test 3\"\nassert perceptron_dual_train(np.array([[1, 1, 1], [2, 2, 2], [-1, -1, -1], [-2, -2, -2]]), np.array([1, 1, -1, -1])) == ([1.0, 1.0, 1.0], 1.0), \"failed test 4\"\nassert perceptron_dual_train(np.array([[1, 1], [-1, -1]]), np.array([1, -1])) == ([1.0, 1.0], 1.0), \"failed test 5\"\nassert perceptron_dual_train(np.array([[0, 1], [0, 2], [0, -1], [0, -2]]), np.array([1, 1, -1, -1])) == ([0.0, 2.0], 0.0), \"failed test 6\"\nassert perceptron_dual_train(np.array([[1], [2], [-1], [-2]]), np.array([1, 1, -1, -1])) == ([2.0], 0.0), \"failed test 7\"\nassert perceptron_dual_train(np.array([[3, 3], [-3, -3]]), np.array([1, -1])) == ([3.0, 3.0], 1.0), \"failed test 8\"\nassert perceptron_dual_train(np.array([[1, 0], [0, 1], [-1, 0], [0, -1]]), np.array([1, 1, -1, -1])) == ([2.0, 2.0], 0.0), \"failed test 9\"\nassert perceptron_dual_train(np.array([[1, 1], [2, 2], [-2, -2]]), np.array([1, 1, -1])) == ([1.0, 1.0], 1.0), \"failed test 10\""}
{"task_id": 564, "completion_id": 0, "passed": false, "result": "failed: name 'root1' is not defined", "solution": "\ndef predict_sample(root: 'Node', features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    node = root\n    while True:\n        if hasattr(node, 'y'):\n            return node.y\n        if task_type == 'classification':\n            feat_val = features[node.label]\n            next_node = None\n            for child in node.child:\n                if hasattr(child, 'x') and child.x == feat_val:\n                    next_node = child\n                    break\n            if next_node is None:\n                next_node = node.child[1]\n        else:\n            feat_val = features[node.label]\n            next_node = node.child[0] if feat_val <= node.s else node.child[1]\n        node = next_node\nassert predict_sample(root1, [\"red\"], \"classification\") == \"apple\", \"TC1 failed\"\nassert predict_sample(root1, [\"blue\"], \"classification\") == \"pear\", \"TC2 failed\"\nassert predict_sample(root2, [\"A\", \"Y\"], \"classification\") == 1, \"TC3 failed\"\nassert predict_sample(root3, [7], \"regression\") == 5.0, \"TC4 failed\"\nassert predict_sample(root3, [10], \"regression\") == 5.0, \"TC5 failed\"\nassert predict_sample(root3, [12], \"regression\") == 15.0, \"TC6 failed\"\nassert predict_sample(root4, [6, -1], \"regression\") == 1.5, \"TC7 failed\"\nassert predict_sample(leaf_only, [], \"classification\") == 42, \"TC8 failed\"\nassert predict_sample(root5, [1], \"classification\") == \"One\", \"TC9 failed\"\nassert predict_sample(root6, [\"z\", \"d\"], \"classification\") == \"Bird\", \"TC10 failed\""}
{"task_id": 565, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef _entropy(labels: np.ndarray) -> float:\n    \"\"\"\n    Shannon entropy H(labels) in bits.\n    labels are integers starting from 0.\n    \"\"\"\n    if labels.size == 0:\n        return 0.0\n    counts = np.bincount(labels)\n    probs = counts[counts > 0] / labels.size\n    return float(-np.sum(probs * np.log2(probs)))\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        (best_feature_index, max_information_gain_rounded_to_6_decimals)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    base_entropy = _entropy(y)\n    best_gain = -math.inf\n    best_index = 0\n    for idx in range(n_features):\n        column = X[:, idx]\n        (vals, counts) = np.unique(column, return_counts=True)\n        cond_entropy = 0.0\n        for (val, cnt) in zip(vals, counts):\n            subset_labels = y[column == val]\n            cond_entropy += cnt / n_samples * _entropy(subset_labels)\n        info_gain = base_entropy - cond_entropy\n        if info_gain > best_gain + 1e-12:\n            best_gain = info_gain\n            best_index = idx\n        elif abs(info_gain - best_gain) <= 1e-12 and idx < best_index:\n            best_index = idx\n    if abs(best_gain) < 1e-12:\n        best_gain = 0.0\n    return (best_index, round(best_gain, 6))\nassert choose_best_feature(np.array([[0, 1], [1, 0], [0, 1], [1, 1]]), np.array([0, 1, 0, 1])) == (0, 1.0), \"test case failed: example 1\"\nassert choose_best_feature(np.array([[0,0],[0,0],[1,1]]), np.array([0,0,1])) == (0, 0.918296), \"test case failed: identical features\"\nassert choose_best_feature(np.array([[0,1],[1,0],[0,1]]), np.array([1,1,1])) == (0, 0.0), \"test case failed: zero entropy labels\"\nassert choose_best_feature(np.array([[0, 1, 0], [1, 1, 1], [1, 0, 0], [0, 1, 1]]), np.array([0, 1, 1, 0])) == (0, 1.0), \"test case failed: mixed dataset\"\nassert choose_best_feature(np.array([[0],[1],[2]]), np.array([0,1,2])) == (0, 1.584963), \"test case failed: three classes\"\nassert choose_best_feature(np.array([[0, 0], [0, 1], [0, 1], [1, 0], [1, 1]]), np.array([0, 0, 1, 1, 1])) == (0, 0.419973), \"test case failed: uneven distribution\"\nassert choose_best_feature(np.array([[0, 0], [1, 1]]), np.array([0, 1])) == (0, 1.0), \"test case failed: tie breaking\"\nassert choose_best_feature(np.array([[0],[0],[0]]), np.array([1, 1, 1])) == (0, 0.0), \"test case failed: single feature all same\"\nassert choose_best_feature(np.array([[0, 1, 0], [0, 1, 1], [1, 0, 1], [1, 1, 0], [1, 1, 1], [0, 0, 0]]), np.array([0, 0, 1, 1, 1, 0])) == (0, 1.0), \"test case failed: larger synthetic set\"\nassert choose_best_feature(np.array([[0, 0], [0, 1], [1, 2], [2, 2]]), np.array([0, 0, 1, 1])) == (0, 1.0), \"test case failed: multi-valued tie\""}
{"task_id": 566, "completion_id": 0, "passed": false, "result": "failed: case 3 failed", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation (deterministic).\n\n    Args\n    ----\n    data        : 2-D NumPy array (n_samples, n_features)\n    n_trees     : number of random isolation trees\n    sample_size : subsample size used to grow every tree\n    epsilon     : fraction (0 \u2264 \u03b5 < 1) of observations regarded as outliers\n\n    Returns\n    -------\n    A sorted list with the zero-based indices of the detected outliers.\n    \"\"\"\n    np.random.seed(42)\n    (n_samples, n_features) = data.shape\n    if epsilon == 0 or n_samples == 0:\n        return []\n    subsz = min(sample_size, n_samples)\n    height_limit = math.ceil(math.log2(subsz)) if subsz > 1 else 0\n\n    def c_factor(n: int) -> float:\n        \"\"\"Expected path length for an unsuccessful search in a BST.\"\"\"\n        if n <= 1:\n            return 0.0\n        return 2.0 * math.log(n - 1) + 0.5772156649 - 2.0 * (n - 1) / n\n\n    class Node:\n        __slots__ = ('is_leaf', 'size', 'feat', 'thr', 'left', 'right')\n\n        def __init__(self, *, is_leaf: bool, size: int, feat=None, thr=None, left=None, right=None):\n            self.is_leaf = is_leaf\n            self.size = size\n            self.feat = feat\n            self.thr = thr\n            self.left = left\n            self.right = right\n\n    def build_tree(idxs: np.ndarray, depth: int) -> Node:\n        \"\"\"Recursively build one isolation tree.\"\"\"\n        if depth >= height_limit or idxs.size <= 1:\n            return Node(is_leaf=True, size=idxs.size)\n        feat = np.random.randint(n_features)\n        col = data[idxs, feat]\n        (f_min, f_max) = (col.min(), col.max())\n        if f_min == f_max:\n            return Node(is_leaf=True, size=idxs.size)\n        thr = np.random.uniform(f_min, f_max)\n        left_mask = col < thr\n        right_mask = ~left_mask\n        if not left_mask.any() or not right_mask.any():\n            return Node(is_leaf=True, size=idxs.size)\n        left_child = build_tree(idxs[left_mask], depth + 1)\n        right_child = build_tree(idxs[right_mask], depth + 1)\n        return Node(is_leaf=False, size=idxs.size, feat=feat, thr=thr, left=left_child, right=right_child)\n\n    def path_length(x: np.ndarray, node: Node, depth: int=0) -> float:\n        \"\"\"Path length of one sample through one tree.\"\"\"\n        if node.is_leaf:\n            return depth + c_factor(node.size)\n        if x[node.feat] < node.thr:\n            return path_length(x, node.left, depth + 1) if node.left is not None else depth + c_factor(node.size)\n        else:\n            return path_length(x, node.right, depth + 1) if node.right is not None else depth + c_factor(node.size)\n    forest: list[Node] = []\n    for _ in range(n_trees):\n        sample_indices = np.random.choice(n_samples, subsz, replace=False)\n        forest.append(build_tree(sample_indices, depth=0))\n    path_sum = np.zeros(n_samples, dtype=float)\n    for tree in forest:\n        for i in range(n_samples):\n            path_sum[i] += path_length(data[i], tree)\n    h_bar = path_sum / n_trees\n    phi = 0.0\n    if n_samples > 1:\n        phi = 2.0 * math.log(n_samples - 1) - 2.0 * (n_samples - 1) / n_samples\n    else:\n        phi = 1.0\n    scores = np.power(2.0, -h_bar / phi)\n    k = int(math.floor(epsilon * n_samples))\n    if k == 0:\n        return []\n    largest_k_idx = np.argsort(scores)[-k:]\n    return sorted(largest_k_idx.tolist())\nassert detect_outliers(np.array([[1.0,1.1],[0.9,1.0],[1.2,0.8],[8.0,8.0]]),50,4,0.25)==[3], \"case 1 failed\"\nassert detect_outliers(np.array([[0.0],[0.1],[-0.1],[5.0]]),60,4,0.25)==[3], \"case 2 failed\"\nassert detect_outliers(np.vstack([np.zeros((10,2)),np.array([[5,5],[6,6]])]),80,8,0.15)==[10,11], \"case 3 failed\"\nassert detect_outliers(np.vstack([np.zeros((15,1)),np.array([[3.0],[4.0]])]),70,8,0.1)==[15,16], \"case 4 failed\"\nassert detect_outliers(np.array([[1.0],[1.0],[1.0],[1.0]]),40,4,0.1)==[], \"case 5 failed\"\nassert detect_outliers(np.array([[1.0],[1.1],[0.9],[1.05],[8.0]]),50,5,0.2)==[4], \"case 6 failed\"\nassert detect_outliers(np.array([[0,0],[0,0.1],[0.1,0],[0.05,-0.05],[0.02,0.01],[7,7]]),60,6,0.17)==[5], \"case 7 failed\"\nassert detect_outliers(np.array([[1],[1]]),30,2,0.5)==[], \"case 8 failed\"\nassert detect_outliers(np.array([[0.0,0.0],[0.05,0.0],[-0.05,0.0],[0.0,0.05],[0.0,-0.05],[10.0,10.0]]),90,6,0.2)==[5], \"case 9 failed\""}
{"task_id": 568, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nfrom collections import defaultdict\nimport numpy as np\nimport math\nfrom collections import defaultdict\nimport numpy as np\ndef maxent_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, epsilon: float=0.001, n_iter: int=100) -> list[int]:\n    \"\"\"\n    Train a Maximum-Entropy classifier with Generalised Iterative Scaling\n    on (categorical) training data and return the label predictions for\n    X_test.\n    \"\"\"\n\n    def active_sum(x_row, y_label):\n        \"\"\"\n        Sum of the weights that are active for (x_row , y_label).\n        Each (feature_index, feature_value, y_label) that matches x_row\n        contributes its current weight.\n        \"\"\"\n        s = 0.0\n        for (i, val) in enumerate(x_row):\n            idx = feat2id.get((i, val, y_label))\n            if idx is not None:\n                s += weights[idx]\n        return s\n    (N, d) = X_train.shape\n    labels = list(set((int(y) for y in y_train)))\n    C = d\n    feat_counts = defaultdict(int)\n    for (x_row, y) in zip(X_train, y_train):\n        y = int(y)\n        for (i, val) in enumerate(x_row):\n            feat_counts[i, val, y] += 1\n    n_feats = len(feat_counts)\n    feat2id = {}\n    emp_exp = np.zeros(n_feats, dtype=float)\n    for (idx, (feat, cnt)) in enumerate(feat_counts.items()):\n        feat2id[feat] = idx\n        emp_exp[idx] = cnt / N\n    weights = np.zeros(n_feats, dtype=float)\n    tiny = 1e-12\n    for _ in range(n_iter):\n        mod_exp = np.zeros(n_feats, dtype=float)\n        for x_row in X_train:\n            raw = {y: active_sum(x_row, y) for y in labels}\n            max_raw = max(raw.values())\n            exp_scores = {y: math.exp(raw[y] - max_raw) for y in labels}\n            Z = sum(exp_scores.values())\n            probs = {y: exp_scores[y] / Z for y in labels}\n            for (y, p) in probs.items():\n                if p == 0.0:\n                    continue\n                for (i, val) in enumerate(x_row):\n                    idx = feat2id.get((i, val, y))\n                    if idx is not None:\n                        mod_exp[idx] += p\n        mod_exp /= N\n        max_change = 0.0\n        for j in range(n_feats):\n            delta = 1.0 / C * math.log((emp_exp[j] + tiny) / (mod_exp[j] + tiny))\n            weights[j] += delta\n            if abs(delta) > max_change:\n                max_change = abs(delta)\n        if max_change < epsilon:\n            break\n    preds = []\n    for x_row in X_test:\n        (best_label, best_score) = (None, -float('inf'))\n        for y in labels:\n            score = active_sum(x_row, y)\n            if score > best_score:\n                best_score = score\n                best_label = y\n        preds.append(int(best_label))\n    return preds\nassert maxent_predict(np.array([[0],[1]]), np.array([0,1]), np.array([[1],[0]])) == [1,0], \"Test-1 failed: basic two-sample training\"\nassert maxent_predict(np.array([[0],[0],[1],[1]]), np.array([0,0,1,1]), np.array([[1],[0]])) == [1,0], \"Test-2 failed: duplicated training rows\"\nassert maxent_predict(np.array([[0,0],[1,0]]), np.array([0,1]), np.array([[0,0],[1,0],[0,0]])) == [0,1,0], \"Test-3 failed: two features, two labels\"\nassert maxent_predict(np.array([[0],[1],[2]]), np.array([0,1,2]), np.array([[2],[0]])) == [2,0], \"Test-4 failed: three labels\"\nassert maxent_predict(np.array([[0,0],[0,1],[1,0]]), np.array([0,1,2]), np.array([[0,1],[1,0]])) == [1,2], \"Test-5 failed: 3-class, 2-feature\"\nassert maxent_predict(np.array([[0,1,0],[1,0,1]]), np.array([0,1]), np.array([[1,0,1]])) == [1], \"Test-6 failed: 3 features\"\nassert maxent_predict(np.array([[0],[0],[0],[1]]), np.array([1,1,1,0]), np.array([[1],[0]])) == [0,1], \"Test-7 failed: imbalanced classes\"\nassert maxent_predict(np.array([[0,0],[0,1],[0,2],[1,0],[1,1],[1,2]]), np.array([0,0,0,1,1,1]), np.array([[1,1],[0,2]])) == [1,0], \"Test-8 failed: bigger balanced dataset\"\nassert maxent_predict(np.array([[5],[6],[7]]), np.array([2,2,2]), np.array([[5],[7]])) == [2,2], \"Test-9 failed: single-class training\"\nassert maxent_predict(np.array([[0],[1]]), np.array([1,0]), np.array([[0],[1]])) == [1,0], \"Test-10 failed: labels reversed order\""}
{"task_id": 569, "completion_id": 0, "passed": false, "result": "failed: test case failed: single document", "solution": "from collections import Counter\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\ndef statistic_key_word(data: List[Tuple[Any, Iterable[str]]], cut_off: int | None=None) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate per-word document-frequency statistics.\n\n    The function returns\n        1. a list of (word, ratio) pairs where *ratio* is the fraction of\n           documents in which the word occurs (rounded to 4 decimals),\n           sorted from the most to the least frequent word;\n        2. a set that contains only the *cut_off* most frequent words\n           (all words if *cut_off* is None).\n\n    Args:\n        data: list/iterable of documents.  Each document is a tuple\n              ``(label, words)`` where *words* is any iterable of strings.\n        cut_off: maximal number of top words to put into the returned set, or\n                 ``None`` for *all* words.\n\n    Returns:\n        Tuple[list[(word, ratio)], set[str]]\n    \"\"\"\n    n_docs = len(data)\n    if n_docs == 0:\n        return ([], set())\n    doc_counter: Counter[str] = Counter()\n    for (_, words) in data:\n        doc_counter.update(set(words))\n    freq_list = [(word, round(count / n_docs, 4)) for (word, count) in doc_counter.items()]\n    freq_list.sort(key=lambda pair: (-pair[1], pair[0]))\n    if cut_off is None:\n        top_k_set = {word for (word, _) in freq_list}\n    elif cut_off <= 0:\n        top_k_set = set()\n    else:\n        top_k_set = {word for (word, _) in freq_list[:cut_off]}\n    return (freq_list, top_k_set)\nassert statistic_key_word([(1, {\"apple\", \"banana\"}), (0, {\"banana\", \"cherry\"}), (1, {\"banana\", \"dragon\"})], 2) == ([('banana', 1.0), ('apple', 0.3333), ('cherry', 0.3333), ('dragon', 0.3333)], {'banana', 'apple'}), \"test case failed: basic example with cut_off = 2\"\nassert statistic_key_word([(1, ['a', 'a', 'b']), (0, ['b', 'c']), (1, ['c'])], None)[0] == [('b', 0.6667), ('c', 0.6667), ('a', 0.3333)], \"test case failed: duplicates inside a document and cut_off = None\"\nassert statistic_key_word([(1, ['x']), (0, ['y'])], 5)[1] == {'x', 'y'}, \"test case failed: cut_off larger than vocabulary\"\nassert statistic_key_word([(1, ['p', 'q', 'r'])], None) == ([('q', 1.0), ('r', 1.0), ('p', 1.0)], {'p', 'q', 'r'}), \"test case failed: single document\"\nassert statistic_key_word([(1, ['m', 'n']), (0, ['m'])], 1)[1] == {'m'}, \"test case failed: cut_off = 1\"\nassert statistic_key_word([(1, ['d']), (0, ['e']), (1, ['f'])], 0)[1] == set(), \"test case failed: cut_off = 0\"\nassert statistic_key_word([(1, ['g', 'h']), (0, ['h', 'i'])], 2)[0][0][0] == 'h', \"test case failed: most frequent word should be first\"\nassert all(ratio <= 1 for _, ratio in statistic_key_word([(1, ['a']), (0, ['a', 'b'])], None)[0]), \"test case failed: ratio larger than 1\"\nassert statistic_key_word([(1, []), (0, [])], None) == ([], set()), \"test case failed: documents without any word\"\nassert statistic_key_word([], None) == ([], set()), \"test case failed: empty data list\""}
{"task_id": 571, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Training data.\n    y : np.ndarray, shape (n_samples,)\n        Class labels (+1 or \u22121).\n    alpha : np.ndarray, shape (n_samples,)\n        Optimised Lagrange multipliers.\n\n    Returns\n    -------\n    (w_list, b) : tuple[list[float], float]\n        w_list : the weight vector rounded to 4 decimals (Python list)\n        b       : the bias term rounded to 4 decimals (float)\n    \"\"\"\n    alpha_y = alpha * y\n    w = X.T @ alpha_y\n    predictions = X @ w\n    b = np.mean(y - predictions)\n    w_list = [round(float(w_i), 4) for w_i in w]\n    b = round(float(b), 4)\n    return (w_list, b)\nassert compute_svm_parameters(np.array([[1, 2], [2, 3]]), np.array([1, -1]), np.array([0.5, 0.5])) == ([-0.5, -0.5], 2.0), \"test case failed: basic 2\u00d72\"\nassert compute_svm_parameters(np.array([[0, 0], [1, 1]]), np.array([-1, 1]), np.array([0.1, 0.3])) == ([0.3, 0.3], -0.3), \"test case failed: zeros and ones\"\nassert compute_svm_parameters(np.array([[1], [2], [3]]), np.array([1, 1, -1]), np.array([0.2, 0.2, 0.4])) == ([-0.6], 1.5333), \"test case failed: single feature\"\nassert compute_svm_parameters(np.array([[-1, -1], [1, 1]]), np.array([1, -1]), np.array([0.25, 0.25])) == ([-0.5, -0.5], 0.0), \"test case failed: symmetric points\"\nassert compute_svm_parameters(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([1, 1, -1]), np.array([0.4, 0.4, 0.4])) == ([0.4, 0.4, -0.4], 0.2), \"test case failed: identity matrix\"\nassert compute_svm_parameters(np.array([[2, 2]]), np.array([1]), np.array([0.5])) == ([1.0, 1.0], -3.0), \"test case failed: single sample\"\nassert compute_svm_parameters(np.array([[0, 0], [0, 0]]), np.array([1, -1]), np.array([0.2, 0.2])) == ([0.0, 0.0], 0.0), \"test case failed: zero features\"\nassert compute_svm_parameters(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), np.array([1, -1, 1]), np.array([0.2, 0.3, 0.1])) == ([-0.3, -0.3, -0.3], 4.8333), \"test case failed: 3\u00d73 matrix\"\nassert compute_svm_parameters(np.array([[-1, 0], [0, 1], [1, 0]]), np.array([-1, 1, 1]), np.array([0.3, 0.3, 0.4])) == ([0.7, 0.3], 0.2333), \"test case failed: mixed signs\"\nassert compute_svm_parameters(np.array([[2], [4]]), np.array([-1, -1]), np.array([0.25, 0.25])) == ([-1.5], 3.5), \"test case failed: negative labels only\""}
{"task_id": 572, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"\n    Computes the Local-Outlier-Factor (LOF) for every sample in `data`\n    and returns\n        (list_of_LOF_scores_rounded_to_4_decimals ,\n         ascending_list_of_indices_with_LOF>epsilon)\n\n    Parameters\n    ----------\n    data : array-like, shape = (n_samples, n_features) or (n_samples,)\n    k    : int, 1 \u2264 k < n_samples\n    epsilon : float, threshold that decides which samples are flagged\n              as outliers        (default 1.5)\n\n    Notes\n    -----\n    \u2022  NO external ML / scientific libraries are used \u2013 only `numpy`.\n    \u2022  All LOF scores are rounded with `numpy.round(score, 4)`.\n    \"\"\"\n    data = np.asarray(data, dtype=float)\n    if data.ndim == 1:\n        data = data[:, None]\n    n_samples = data.shape[0]\n    if not 1 <= k < n_samples:\n        raise ValueError('k must satisfy 1 \u2264 k < n_samples (got k = %d)' % k)\n    diff = data[:, None, :] - data[None, :, :]\n    dist_mat = np.sqrt(np.sum(diff ** 2, axis=2))\n    k_distances = np.empty(n_samples)\n    neighbours = np.empty((n_samples, k), int)\n    for i in range(n_samples):\n        idx_sorted = np.argsort(dist_mat[i])\n        neigh_idx = idx_sorted[1:k + 1]\n        neighbours[i] = neigh_idx\n        k_distances[i] = dist_mat[i, neigh_idx[-1]]\n    lrd = np.empty(n_samples)\n    for p in range(n_samples):\n        reach_dist_sum = 0.0\n        for q in neighbours[p]:\n            reach_dist = max(k_distances[q], dist_mat[p, q])\n            reach_dist_sum += reach_dist\n        lrd[p] = k / reach_dist_sum\n    lof = np.empty(n_samples)\n    for p in range(n_samples):\n        lrd_sum = lrd[neighbours[p]].sum()\n        lof[p] = lrd_sum / k / lrd[p]\n    lof_rounded = np.round(lof, 4).tolist()\n    outlier_idx = [i for (i, score) in enumerate(lof) if score > epsilon]\n    return (lof_rounded, outlier_idx)\nassert local_outlier_factor([[0],[1],[2],[10]],1) == ([1.0, 1.0, 1.0, 8.0], [3]), \"test case failed: local_outlier_factor([[0],[1],[2],[10]],1)\"\nassert local_outlier_factor([[0],[1],[5]],1) == ([1.0, 1.0, 4.0], [2]), \"test case failed: local_outlier_factor([[0],[1],[5]],1)\"\nassert local_outlier_factor([[0],[1],[3],[4]],1) == ([1.0, 1.0, 1.0, 1.0], []), \"test case failed: local_outlier_factor([[0],[1],[3],[4]],1)\"\nassert local_outlier_factor([[1],[2],[3],[50]],1) == ([1.0, 1.0, 1.0, 47.0], [3]), \"test case failed: local_outlier_factor([[1],[2],[3],[50]],1)\"\nassert local_outlier_factor([[0],[1]],1) == ([1.0, 1.0], []), \"test case failed: local_outlier_factor([[0],[1]],1)\"\nassert local_outlier_factor([[0],[1],[2],[6]],1) == ([1.0, 1.0, 1.0, 4.0], [3]), \"test case failed: local_outlier_factor([[0],[1],[2],[6]],1)\"\nassert local_outlier_factor([[0],[2],[4],[6],[8]],1) == ([1.0, 1.0, 1.0, 1.0, 1.0], []), \"test case failed: local_outlier_factor([[0],[2],[4],[6],[8]],1)\"\nassert local_outlier_factor([[0],[1],[2],[3],[50]],2,2) == ([1.0, 1.0, 1.0, 1.0, 31.6667], [4]), \"test case failed: local_outlier_factor([[0],[1],[2],[3],[50]],2,2)\"\n"}
{"task_id": 574, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport random\nimport numpy as np\nimport random\ndef generate_data(m: int, n_train: int, n_val: int, seed: int | None=None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the current global\n        random state is used unchanged.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays with shapes\n        (m\u00b7n_train, 2), (m\u00b7n_val, 2), (m\u00b7n_train,) and (m\u00b7n_val,)\n        respectively.  Every coordinate is rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    (X_tr, X_va) = ([], [])\n    (y_tr, y_va) = ([], [])\n    for c in range(m):\n        base = 10 * (c // 2)\n        for _ in range(n_train):\n            x = random.uniform(base + 1, base + 9)\n            y = random.uniform(base + 1, base + 9)\n            X_tr.append((x, y))\n            y_tr.append(c)\n        for _ in range(n_val):\n            x = random.uniform(base + 1, base + 9)\n            y = random.uniform(base + 1, base + 9)\n            X_va.append((x, y))\n            y_va.append(c)\n    X_train = np.round(np.asarray(X_tr, dtype=float), 4)\n    X_val = np.round(np.asarray(X_va, dtype=float), 4)\n    y_train = np.asarray(y_tr, dtype=int)\n    y_val = np.asarray(y_va, dtype=int)\n    return (X_train, X_val, y_train, y_val)\nassert generate_data(2, 2, 1, seed=42)[0].shape == (4, 2), \"test case failed: shape of X_train for seed=42,2,2,1\"\nassert generate_data(2, 2, 1, seed=42)[1].shape == (2, 2), \"test case failed: shape of X_val for seed=42,2,2,1\"\nassert (generate_data(2, 2, 1, seed=42)[2] == np.array([0, 0, 1, 1])).all(), \"test case failed: y_train labels for seed=42\"\nassert (generate_data(2, 2, 1, seed=42)[3] == np.array([0, 1])).all(), \"test case failed: y_val labels for seed=42\"\nassert generate_data(3, 3, 2, seed=0)[0].shape == (9, 2), \"test case failed: shape of X_train for seed=0,3,3,2\"\nassert generate_data(3, 3, 2, seed=0)[1].shape == (6, 2), \"test case failed: shape of X_val for seed=0,3,3,2\"\nassert set(generate_data(3, 3, 2, seed=0)[2]) == {0, 1, 2}, \"test case failed: y_train label set for seed=0\"\nassert set(generate_data(3, 3, 2, seed=0)[3]) == {0, 1, 2}, \"test case failed: y_val label set for seed=0\"\nassert (generate_data(1, 5, 5, seed=99)[2] == np.zeros(5, dtype=int)).all(), \"test case failed: single-class y_train not all zeros\"\nassert (generate_data(1, 5, 5, seed=99)[3] == np.zeros(5, dtype=int)).all(), \"test case failed: single-class y_val not all zeros\""}
{"task_id": 579, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kmeans(X: np.ndarray, k: int, max_iters: int=100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Deterministic Lloyd-style K-Means clustering.\n\n    Args:\n        X        : (m, d) array with the input points.\n        k        : number of clusters, 1 \u2264 k \u2264 m.\n        max_iters: maximum amount of Lloyd iterations.\n\n    Returns\n    -------\n        labels  : list with length m, the cluster index of every point.\n        centres : list with k inner lists \u2013 the final centres, each\n                  coordinate rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    if X.ndim != 2:\n        raise ValueError('`X` must be a 2-D array (m, d).')\n    (m, d) = X.shape\n    if not 1 <= k <= m:\n        raise ValueError('`k` must satisfy 1 \u2264 k \u2264 number of points.')\n    if max_iters <= 0:\n        raise ValueError('`max_iters` must be positive.')\n    centres = X[:k].copy()\n    labels = np.full(m, -1, dtype=int)\n    for _ in range(max_iters):\n        diff = X[:, None, :] - centres[None, :, :]\n        dist2 = np.sum(diff ** 2, axis=2)\n        new_labels = np.argmin(dist2, axis=1)\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n        for i in range(k):\n            mask = labels == i\n            if np.any(mask):\n                centres[i] = X[mask].mean(axis=0)\n    centres_rounded = [[round(val, 4) for val in centre] for centre in centres]\n    return (labels.tolist(), centres_rounded)\nassert kmeans(np.array([[1,1],[1.5,2],[3,4],[5,7],[3.5,5],[4.5,5],[3.5,4.5]]),2,100) == ([0,0,1,1,1,1,1], [[1.25,1.5],[3.9,5.1]]), \"failed on simple 2-cluster example\"\nassert kmeans(np.array([[0,0],[10,10],[0,10],[10,0]]),2,50) == ([0,1,0,0], [[3.3333,3.3333],[10.0,10.0]]), \"failed on square corners\"\nassert kmeans(np.array([[0],[5],[10]]),3,10) == ([0,1,2], [[0.0],[5.0],[10.0]]), \"each point its own cluster\"\nassert kmeans(np.array([[0],[1],[2],[3],[9],[10],[11],[12]]),2,50) == ([0,0,0,0,1,1,1,1], [[1.5],[10.5]]), \"failed on 1-D two clusters\"\nassert kmeans(np.array([[1,2,3],[1,2,4],[10,10,10],[11,11,11]]),2,50) == ([0,0,1,1], [[1.0,2.0,3.5],[10.5,10.5,10.5]]), \"failed on 3-D clustering\"\nassert kmeans(np.array([[0,0],[0,0.1],[10,10],[10,10.1]]),2,50) == ([0,0,1,1], [[0.0,0.05],[10.0,10.05]]), \"failed on very close points\"\nassert kmeans(np.array([[2,2],[2,2],[2,2]]),1,5) == ([0,0,0], [[2.0,2.0]]), \"single cluster identical points\"\nassert kmeans(np.array([[1,1],[2,2],[3,3],[8,8],[9,9],[10,10]]),2,100) == ([0,0,0,1,1,1], [[2.0,2.0],[9.0,9.0]]), \"failed on two distant blobs\""}
{"task_id": 581, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef elastic_net_regression(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int, l1_penalty: float, l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation using batch GD.\"\"\"\n    (m, n) = X.shape\n    if learning_rate == 0 or iterations == 0:\n        return ([0.0] * n, 0.0)\n    w = np.zeros(n, dtype=float)\n    b = 0.0\n    for _ in range(iterations):\n        y_hat = X @ w + b\n        r = y - y_hat\n        sign_w = np.sign(w)\n        sign_w[sign_w == 0] = -1\n        grad_w = -2 * X.T @ r + l1_penalty * sign_w + 2 * l2_penalty * w\n        grad_b = -2 * r.sum()\n        w -= learning_rate * (grad_w / m)\n        b -= learning_rate * (grad_b / m)\n    weights_rounded = [round(float(val), 4) for val in w]\n    bias_rounded = round(float(b), 4)\n    return (weights_rounded, bias_rounded)\nassert elastic_net_regression(np.array([[1, 0], [0, 1]]), np.array([1, 1]), 0.5, 1, 0.0, 0.0) == ([0.5, 0.5], 1.0), \"failed: basic 2-D example\"\nassert elastic_net_regression(np.array([[1], [2]]), np.array([2, 4]), 0.1, 1, 0.0, 0.0) == ([1.0], 0.6), \"failed: single feature, no regularisation\"\nassert elastic_net_regression(np.array([[1], [1]]), np.array([1, 1]), 1.0, 1, 1.0, 0.0) == ([2.5], 2.0), \"failed: L1 penalty example\"\nassert elastic_net_regression(np.array([[1, 1], [1, 1]]), np.array([2, 2]), 0.5, 1, 0.0, 0.0) == ([2.0, 2.0], 2.0), \"failed: identical features\"\nassert elastic_net_regression(np.array([[1], [1]]), np.array([2, 2]), 0.1, 1, 0.0, 5.0) == ([0.4], 0.4), \"failed: L2 penalty (first iteration)\"\nassert elastic_net_regression(np.array([[1, 2], [3, 4]]), np.array([1, 0]), 0.2, 1, 0.0, 0.0) == ([0.2, 0.4], 0.2), \"failed: two samples, two features\"\nassert elastic_net_regression(np.array([[1], [2]]), np.array([3, 4]), 0.0, 3, 0.0, 0.0) == ([0.0], 0.0), \"failed: zero learning rate\"\nassert elastic_net_regression(np.array([[1]]), np.array([1]), 1.0, 0, 0.0, 0.0) == ([0.0], 0.0), \"failed: zero iterations\"\nassert elastic_net_regression(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([1, 2, 3]), 0.1, 1, 0.0, 0.0) == ([0.0667, 0.1333, 0.2], 0.4), \"failed: 3-D identity matrix\"\nassert elastic_net_regression(np.array([[1], [1]]), np.array([-1, -1]), 0.5, 1, 0.0, 0.0) == ([-1.0], -1.0), \"failed: negative targets\""}
{"task_id": 591, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Forward pass of a fully\u2013connected ReLU network with a linear output layer.\n\n    Args:\n        weights : Three-level nested list holding all weight matrices.\n        x       : Input features (without bias).\n\n    Returns:\n        Network prediction rounded to four decimals.\n        Returns a float for a single output neuron, otherwise a list of floats.\n    \"\"\"\n    h = np.asarray(x, dtype=float)\n    n_layers = len(weights)\n    for (L, layer) in enumerate(weights):\n        W = np.asarray(layer, dtype=float)\n        h_aug = np.concatenate(([1.0], h))\n        z = W @ h_aug\n        h = np.maximum(0, z) if L < n_layers - 1 else z\n    if h.size == 1:\n        return round(float(h[0]), 4)\n    else:\n        return [round(v, 4) for v in h.tolist()]\nassert neural_net_predict([[[0.5,0.2,0.8],[1.0,-0.5,0.3]],[[0.7,0.6,-1.2]]],[0.1,0.4])==-0.08,\"test case failed: basic 2-layer network\"\nassert neural_net_predict([[[2,3]]],[4])==14.0,\"test case failed: single-layer network\"\nassert neural_net_predict([[[0.5,0.5]],[[1.0,1.0]],[[0.2,2.0]]],[2])==5.2,\"test case failed: three-layer network\"\nassert neural_net_predict([[[0,-1]],[[0,1]]],[2])==0.0,\"test case failed: ReLU zeroing\"\nassert neural_net_predict([[[1,0.5,0.5],[0,-1,-1]],[[0,1,1]]],[1,1])==2.0,\"test case failed: mixed activations\"\nassert neural_net_predict([[[5,-1]]],[5])==0.0,\"test case failed: negative linear result\"\nassert neural_net_predict([[[0,2],[0,0.5]],[[0,1,1]]],[3])==7.5,\"test case failed: 2-neuron hidden layer\"\nassert neural_net_predict([[[-1,0]],[[0,5]]],[10])==0.0,\"test case failed: negative input to ReLU\"\nassert neural_net_predict([[[0,1],[0,-1]],[[0,2,2]]],[4])==8.0,\"test case failed: hidden neuron suppression\"\nassert neural_net_predict([[[1,1]],[[0,0.5]],[[1,2]]],[1])==3.0,\"test case failed: deeper network\""}
{"task_id": 595, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kmeans_centroids(X: 'np.ndarray', k: int, max_iters: int=100) -> list[list[float]]:\n    \"\"\"\n    K-Means clustering \u2013 return **centroids only**.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    k : int\n        Number of clusters.\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Sorted centroids (lexicographically), each coordinate rounded to 4 decimals.\n    \"\"\"\n    if k <= 0:\n        raise ValueError('k must be a positive integer')\n    if X.ndim != 2:\n        raise ValueError('X must be 2-dimensional')\n    X = X.astype(float, copy=False)\n    (n_samples, n_features) = X.shape\n    if k > n_samples:\n        raise ValueError('k cannot be larger than number of samples')\n    centroids = X[:k].copy()\n    for _ in range(max_iters):\n        distances = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = centroids.copy()\n        for j in range(k):\n            mask = labels == j\n            if np.any(mask):\n                new_centroids[j] = X[mask].mean(axis=0)\n        shifts = np.linalg.norm(new_centroids - centroids, axis=1)\n        centroids = new_centroids\n        if np.all(shifts < 0.0001):\n            break\n    centroids_list = centroids.tolist()\n    centroids_list.sort()\n    rounded = [[round(val, 4) for val in centre] for centre in centroids_list]\n    return rounded\nassert kmeans_centroids(np.array([[1, 1], [2, 1], [4, 3], [5, 4]]), 2) == [[1.5, 1.0], [4.5, 3.5]], \"test case failed: basic 2-cluster\"\nassert kmeans_centroids(np.array([[0, 0], [0, 1], [10, 10], [11, 11]]), 2) == [[0.0, 0.5], [10.5, 10.5]], \"test case failed: clearly separated clusters\"\nassert kmeans_centroids(np.array([[1], [2], [10], [12], [19]]), 3) == [[1.0], [2.0], [13.6667]], \"test case failed: 1-D three clusters\"\nassert kmeans_centroids(np.array([[1, 2], [2, 1], [1, 0], [10, 9], [12, 11], [11, 9]]), 2) == [[1.3333, 1.0], [11.0, 9.6667]], \"test case failed: mixed positions\"\nassert kmeans_centroids(np.array([[-5], [-4], [-5], [10], [11], [12]]), 2) == [[-4.6667], [11.0]], \"test case failed: negative and positive values\"\nassert kmeans_centroids(np.array([[0, 0], [0, 10], [10, 0], [10, 10]]), 4) == [[0.0, 0.0], [0.0, 10.0], [10.0, 0.0], [10.0, 10.0]], \"test case failed: one point per cluster\"\nassert kmeans_centroids(np.array([[1, 2], [2, 1], [5, 5], [9, 9], [8, 9]]), 3) == [[1.0, 2.0], [2.0, 1.0], [7.3333, 7.6667]], \"test case failed: three clusters 2-D\"\nassert kmeans_centroids(np.array([[1], [100]]), 2) == [[1.0], [100.0]], \"test case failed: two extreme points\"\nassert kmeans_centroids(np.array([[1, 1], [1, 2], [2, 1], [2, 2]]), 1) == [[1.5, 1.5]], \"test case failed: single centroid\"\nassert kmeans_centroids(np.array([[0, 0], [10, 0], [0, 10], [8, 8]]), 2) == [[0.0, 5.0], [9.0, 4.0]], \"test case failed: asymmetrical clusters\""}
