{"task_id": 0, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"\n    Closed-form Ridge (L2-regularised) linear-regression solver.\n    \n    Parameters\n    ----------\n    data  : np.ndarray\n        2-D array where the last column is the target y and the remaining\n        columns are the features X.\n    alpha : float, default 0.0\n        Regularisation strength (\u03b1 \u2265 0).\n        \n    Returns\n    -------\n    list[float]\n        Weight vector (including the bias/intercept as the first element)\n        rounded to 4 decimal places.  \n        If (X\u1d40X + \u03b1I) is numerically singular the function returns -1.\n    \"\"\"\n    # Split features and target\n    X, y = data[:, :-1], data[:, -1]\n    n_samples = X.shape[0]\n\n    # Add bias (intercept) column of 1s\n    X_bias = np.hstack((np.ones((n_samples, 1)), X))\n\n    # Form normal equation components\n    A = X_bias.T @ X_bias + alpha * np.eye(X_bias.shape[1])\n    \n    # Check invertibility via determinant\n    try:\n        det = np.linalg.det(A)\n    except np.linalg.LinAlgError:\n        return -1\n    if np.isclose(det, 0.0, atol=1e-12):\n        return -1\n    \n    # Closed-form solution\n    w = np.linalg.inv(A) @ X_bias.T @ y\n    \n    # Round to 4 decimals and return as Python list\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 1, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron learning algorithm.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples, shape (n_samples, n_features).\n    y_train : np.ndarray\n        Labels (+1 or -1), length n_samples.\n    eta : float, optional\n        Learning rate, default 1.0.\n    n_iter : int, optional\n        Maximum number of parameter updates, default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        Weight vector (as a list) and bias, both rounded to 4 decimals.\n    \"\"\"\n    # --- basic checks --------------------------------------------------------\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=float).ravel()\n    if not set(np.unique(y)).issubset({-1.0, 1.0}):\n        raise ValueError(\"y_train must contain only +1 and -1.\")\n\n    n_samples, n_features = X.shape\n\n    # --- initialise parameters ----------------------------------------------\n    alpha = np.zeros(n_samples, dtype=float)\n    b = 0.0\n    updates = 0          # how many updates have been performed\n\n    # --- pre-compute Gram matrix K(i,j) = x_i \u00b7 x_j --------------------------\n    gram = X @ X.T       # shape (n_samples, n_samples)\n\n    # --- training loop -------------------------------------------------------\n    while updates < n_iter:\n        no_mistake_in_pass = True\n\n        for i in range(n_samples):\n            # activation = \u03a3_j \u03b1_j y_j K(x_j , x_i)\n            activation = np.dot(alpha * y, gram[:, i])\n            margin = y[i] * (activation + b)\n\n            # mis-classified?\n            if margin <= 0.0:\n                alpha[i] += eta\n                b += eta * y[i]\n                updates += 1\n                no_mistake_in_pass = False\n                break                    # restart scan from the first sample\n\n        if no_mistake_in_pass:\n            # completed a full pass with no update \u2192 converged\n            break\n\n    # --- compute primal weight vector ---------------------------------------\n    w = np.dot(alpha * y, X)            # shape (n_features,)\n\n    # --- rounding and return -------------------------------------------------\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n\n    return w_rounded, b_rounded\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A dictionary with three entries  (\"identity\", \"log\", \"logit\").\n        Each entry itself is a dictionary with three callables\n\n            \u2022 \"link\"      :  g(\u03bc)\n            \u2022 \"inv_link\"  :  g\u207b\u00b9(\u03b7)\n            \u2022 \"derivative\":  g\u2032(\u03bc)\n\n        All functions work element\u2013wise on numpy scalars, 1-D and 2-D\n        numpy arrays.\n    \"\"\"\n\n    # ------------------ identity -------------------------------------------\n    def _id_link(mu):\n        return mu\n\n    def _id_inv_link(eta):\n        return eta\n\n    def _id_derivative(mu):\n        # g'(\u03bc) = 1                      (shape must follow \u03bc)\n        return np.ones_like(mu, dtype=float)\n\n    # ------------------ log -------------------------------------------------\n    def _log_link(mu):\n        #  g(\u03bc) = log(\u03bc)      (\u03bc > 0  is expected)\n        return np.log(mu)\n\n    def _log_inv_link(eta):\n        #  g\u207b\u00b9(\u03b7) = exp(\u03b7)\n        return np.exp(eta)\n\n    def _log_derivative(mu):\n        # g'(\u03bc) = 1/\u03bc\n        return 1.0 / mu\n\n    # ------------------ logit ----------------------------------------------\n    def _logit_link(mu):\n        # g(\u03bc) = log( \u03bc / (1-\u03bc) )\n        return np.log(mu / (1.0 - mu))\n\n    def _logit_inv_link(eta):\n        # numerically stable logistic / sigmoid\n        eta = np.asarray(eta)\n        return np.where(\n            eta >= 0,\n            1.0 / (1.0 + np.exp(-eta)),\n            np.exp(eta) / (1.0 + np.exp(eta)),\n        )\n\n    def _logit_derivative(mu):\n        # g'(\u03bc) = 1 / ( \u03bc (1-\u03bc) )\n        return 1.0 / (mu * (1.0 - mu))\n\n    # ------------------ assemble & return ----------------------------------\n    return {\n        \"identity\": {\n            \"link\": _id_link,\n            \"inv_link\": _id_inv_link,\n            \"derivative\": _id_derivative,\n        },\n        \"log\": {\n            \"link\": _log_link,\n            \"inv_link\": _log_inv_link,\n            \"derivative\": _log_derivative,\n        },\n        \"logit\": {\n            \"link\": _logit_link,\n            \"inv_link\": _logit_inv_link,\n            \"derivative\": _logit_derivative,\n        },\n    }\n```\n\nExample (rounded to 4 dp as requested):\n\n```python\nlinks = glm_links()\n\nmu  = np.array([0.2, 0.8])\neta = links[\"logit\"][\"link\"](mu)          # [-1.38629436, 1.38629436]\nprint(np.round(eta, 4))                   # [-1.3863  1.3863]\n\nmu2 = links[\"logit\"][\"inv_link\"](eta)     # [0.2, 0.8]\nprint(np.round(mu2, 4))                   # [0.2 0.8]\n\ndmu = links[\"logit\"][\"derivative\"](mu)    # [6.25, 6.25]\nprint(np.round(dmu, 4))                   # [6.25 6.25]\n```\n\nThe helper now provides fully-vectorised, numerically stable implementations of the three most common GLM link functions together with their inverses and derivatives."}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter      # kept because it was in the template\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Return, for every sample, the depth of the leaf it lands in inside a\n    single random isolation tree grown as described in the task statement.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # Reproducibility\n    # ------------------------------------------------------------------ #\n    np.random.seed(0)\n\n    # ------------------------------------------------------------------ #\n    # Trivial / corner-case handling\n    # ------------------------------------------------------------------ #\n    if data.size == 0:                        # no samples at all\n        return []\n\n    n_samples, n_features = data.shape\n    depths = [-1] * n_samples                # will be filled in exactly once\n\n    # ------------------------------------------------------------------ #\n    # Recursive construction and bookkeeping\n    # ------------------------------------------------------------------ #\n    def grow(indices: list[int], depth: int) -> None:\n        \"\"\"Recursively grow the isolation tree while filling `depths`.\"\"\"\n        # 1. Stopping condition \u2013 fewer than three samples \u21d2 leaf\n        if len(indices) < 3:\n            for idx in indices:\n                depths[idx] = depth\n            return\n\n        # 2. Pick a random feature\n        f = np.random.randint(0, n_features)\n\n        # 3. Determine split bounds and draw the split value\n        vals  = data[indices, f]\n        down, up = vals.min(), vals.max()\n\n        # If there is no variability on this feature we cannot split \u2013 make leaf\n        if down == up:\n            for idx in indices:\n                depths[idx] = depth\n            return\n\n        v = np.random.uniform(down, up)\n\n        # 4. Partition the samples\n        left  = [idx for idx in indices if data[idx, f] <= v]\n        right = [idx for idx in indices if data[idx, f] >  v]\n\n        # If the random split failed to separate anything, make this a leaf\n        if len(left) == 0 or len(right) == 0:\n            for idx in indices:\n                depths[idx] = depth\n            return\n\n        # 5. Recurse on both children\n        grow(left,  depth + 1)\n        grow(right, depth + 1)\n\n    # Kick off the recursion with the root node\n    grow(list(range(n_samples)), depth=0)\n\n    return depths\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable, Iterable, FrozenSet\n\n\ndef _sorted_tuple(it: Iterable[Hashable]) -> Tuple[Hashable, ...]:\n    \"\"\"\n    Return the items of *it* as an ascending tuple.\n\n    When item types are not mutually comparable we fall back to comparing their\n    textual representation, which is at least deterministic.\n    \"\"\"\n    try:                              # works for homogeneous, comparable types\n        return tuple(sorted(it))\n    except TypeError:                 # e.g. mix of str / int  -> not comparable\n        return tuple(sorted(it, key=repr))\n\n\ndef find_frequent_itemsets(\n    transactions: List[List[Hashable]], minsup: float\n) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent item-sets together with their absolute support.\n\n    Args\n    ----\n    transactions : list[list[hashable]]\n        Transactional data base \u2013 duplicates inside one transaction are ignored.\n    minsup       : float\n        Minimum support given as a fraction in the (0, 1] interval.\n\n    Returns\n    -------\n    dict[tuple, int]\n        Maps every frequent item-set (tuple with ascending items) to its support.\n        The empty dict is returned when no item-set fulfils the threshold.\n    \"\"\"\n    if not (0 < minsup <= 1):\n        raise ValueError(\"minsup must be in the interval (0, 1].\")\n\n    # --- basic preprocessing -------------------------------------------------\n    transactions = [set(t) for t in transactions]        # drop duplicates\n    n_transactions = len(transactions)\n    if n_transactions == 0:\n        return {}\n\n    min_count = max(1, math.ceil(minsup * n_transactions))\n\n    # -------------------------------------------------------------------------\n    # Apriori algorithm\n    # -------------------------------------------------------------------------\n    result: Dict[Tuple[Hashable, ...], int] = {}\n\n    # 1) frequent 1-itemsets ---------------------------------------------------\n    item_counter = Counter()\n    for t in transactions:\n        item_counter.update(t)\n\n    # L_k keeps the frequent k-itemsets *and* their support for the current k\n    L_k: Dict[FrozenSet[Hashable], int] = {\n        frozenset([item]): supp\n        for item, supp in item_counter.items()\n        if supp >= min_count\n    }\n\n    for fs, supp in L_k.items():                       # store L1 in the result\n        result[_sorted_tuple(fs)] = supp\n\n    k = 2\n    while L_k:\n        # 2) candidate generation (join + prune) ------------------------------\n        L_prev = list(L_k.keys())\n        candidates: set[FrozenSet[Hashable]] = set()\n\n        for i in range(len(L_prev)):\n            for j in range(i + 1, len(L_prev)):\n                union = L_prev[i] | L_prev[j]\n                if len(union) != k:\n                    continue\n                # Apriori-prune: all (k-1) subsets must themselves be frequent\n                if all((union - frozenset([item])) in L_k for item in union):\n                    candidates.add(union)\n\n        if not candidates:                             # nothing more to count\n            break\n\n        # 3) support counting -------------------------------------------------\n        cand_counter = Counter()\n        for t in transactions:\n            for cand in candidates:\n                if cand.issubset(t):\n                    cand_counter[cand] += 1\n\n        # 4) keep only the frequent ones --------------------------------------\n        L_k = {\n            cand: supp for cand, supp in cand_counter.items() if supp >= min_count\n        }\n\n        # 5) add to global result ---------------------------------------------\n        for fs, supp in L_k.items():\n            result[_sorted_tuple(fs)] = supp\n\n        k += 1\n\n    return result\n```\n\nExplanation of the main steps\n1. Transactions are converted to `set`s so every item appears at most once per basket (duplicates do not increase support).\n2. The minimum absolute support is `ceil(minsup \u00b7 N)`.\n3. Apriori is used:\n   \u2022 Count single items \u2192 keep only those reaching the threshold.  \n   \u2022 Repeatedly  \n     \u2013 join frequent (k\u22121)-itemsets to build length-k candidates,  \n     \u2013 prune candidates whose (k\u22121) subsets are not all frequent,  \n     \u2013 count their support with a single scan over the data,  \n     \u2013 retain the frequent ones and store them.  \n   The loop stops once no new frequent itemset is found.\n4. Keys are returned as tuples whose items are stored in ascending order. When direct comparison of mixed types would raise a `TypeError`, the `repr` of the items is used as a tiebreaker, guaranteeing a deterministic order.\n"}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef kmeans(data: np.ndarray,\n           k: int,\n           epsilon: float = 1e-3,\n           max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array (n_samples \u00d7 n_features).\n    k : int\n        Number of desired clusters (1 \u2264 k \u2264 n_samples).\n    epsilon : float, default 1e-3\n        Stop if the largest centre movement is smaller than *epsilon*.\n    max_iter : int, default 2000\n        Hard iteration cap.\n\n    Returns\n    -------\n    (centres, labels)\n        centres : list[list[float]]\n            k cluster centres rounded to 4 decimals.\n        labels : list[int]\n            Cluster index (0 \u2026 k-1) assigned to every sample.\n    \"\"\"\n    # --- basic checks --------------------------------------------------------\n    if data.ndim != 2:\n        raise ValueError('data must be a 2-D array')\n    n_samples, n_features = data.shape\n    if not (1 <= k <= n_samples):\n        raise ValueError('k must satisfy 1 \u2264 k \u2264 number of samples')\n\n    # Work with float copy to avoid surprising integer division\n    data = data.astype(float, copy=False)\n\n    # --- initialisation ------------------------------------------------------\n    centres = data[:k].copy()            # first k samples  (deterministic)\n\n    for _ in range(max_iter):\n        # ----- 1. assignment step ------------------------------------------\n        # squared Euclidean distances:  (n_samples \u00d7 k)\n        dists = np.sum((data[:, None, :] - centres[None, :, :]) ** 2, axis=2)\n        labels = np.argmin(dists, axis=1)     # (n_samples,)\n\n        # ----- 2. update step ----------------------------------------------\n        new_centres = centres.copy()\n        for j in range(k):\n            mask = labels == j\n            if mask.any():                       # normal: use mean\n                new_centres[j] = data[mask].mean(axis=0)\n            else:                                # empty cluster -> random sample\n                rnd_idx = np.random.randint(0, n_samples)\n                new_centres[j] = data[rnd_idx]\n\n        # ----- 3. convergence test -----------------------------------------\n        shifts = np.linalg.norm(new_centres - centres, axis=1)  # movement per centre\n        if np.max(shifts) < epsilon:\n            centres = new_centres\n            break\n        centres = new_centres\n\n    # --- rounding & output ---------------------------------------------------\n    centres_rounded = np.round(centres, 4).tolist()\n    return centres_rounded, labels.tolist()\n```"}
{"task_id": 13, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import defaultdict          # (import kept \u2013 not strictly needed)\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item\u2013based k-NN collaborative\u2013filtering recommender.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Rating matrix of shape (n_user, n_item); 0 = *not rated*.\n    user_ind : int\n        Index of the active user.\n    k : int\n        Maximal number of items to recommend.\n    criterion : {'cosine', 'pearson'}, default 'cosine'\n        Similarity measure to use.\n\n    Returns\n    -------\n    list[int]\n        Indices of at most *k* unseen items ordered by decreasing\n        predicted attractiveness.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 0. Some basic shapes / checks\n    # ------------------------------------------------------------------\n    if criterion not in ('cosine', 'pearson'):\n        raise ValueError(\"criterion must be either 'cosine' or 'pearson'\")\n\n    n_user, n_item = data.shape\n    if not (0 <= user_ind < n_user):\n        raise IndexError('user_ind out of bounds')\n\n    # ------------------------------------------------------------------\n    # 1. Build the item\u2013item similarity matrix S\n    # ------------------------------------------------------------------\n    S = np.zeros((n_item, n_item), dtype=np.float64)          # S[i, i] stays 0/unused\n\n    for i in range(n_item - 1):\n        col_i = data[:, i]\n        r_i_mask = col_i > 0                                   # users that rated i\n\n        for j in range(i + 1, n_item):\n            col_j = data[:, j]\n            common = r_i_mask & (col_j > 0)                    # users that rated *both*\n\n            if not np.any(common):                             # empty intersection\n                continue                                       # similarity already 0\n\n            v1 = col_i[common].astype(np.float64)\n            v2 = col_j[common].astype(np.float64)\n\n            # ---- cosine ------------------------------------------------\n            if criterion == 'cosine':\n                # optional mean\u2013centering (only if variance large enough)\n                if np.std(v1, ddof=1) > 1e-3:\n                    v1c = v1 - v1.mean()\n                else:\n                    v1c = v1\n\n                if np.std(v2, ddof=1) > 1e-3:\n                    v2c = v2 - v2.mean()\n                else:\n                    v2c = v2\n\n                n1, n2 = np.linalg.norm(v1c), np.linalg.norm(v2c)\n                sim = 0.0 if n1 == 0.0 or n2 == 0.0 else float(v1c @ v2c / (n1 * n2))\n\n            # ---- Pearson -----------------------------------------------\n            else:                                               # criterion == 'pearson'\n                # correlation needs at least 2 points with variance\n                if len(v1) < 2 or np.std(v1, ddof=1) < 1e-3 or np.std(v2, ddof=1) < 1e-3:\n                    sim = 0.0\n                else:\n                    sim = float(np.corrcoef(v1, v2, ddof=1)[0, 1])\n                    if np.isnan(sim):\n                        sim = 0.0\n\n            S[i, j] = S[j, i] = sim\n\n    # ------------------------------------------------------------------\n    # 2. Items already rated by the active user\n    # ------------------------------------------------------------------\n    user_row = data[user_ind]\n    rated_mask = user_row > 0\n    rated_idx = np.where(rated_mask)[0]\n\n    # If everything is already rated \u2192 nothing to recommend\n    if len(rated_idx) == n_item:\n        return []\n\n    # ------------------------------------------------------------------\n    # 3. Predict scores for every *un*-rated item\n    # ------------------------------------------------------------------\n    scores = {}                                                  # item_index -> score\n\n    for t in range(n_item):\n        if rated_mask[t]:                                        # already seen\n            continue\n\n        sims_to_rated = S[t, rated_idx]                          # similarities S[t, i]\n        ratings = user_row[rated_idx]                            # the r_i\n\n        denom = np.sum(np.abs(sims_to_rated))\n        if denom == 0.0:\n            score = 0.0\n        else:\n            score = float(ratings @ sims_to_rated / denom)       # weighted average\n\n        scores[t] = score\n\n    # ------------------------------------------------------------------\n    # 4. Sort and pick the best *k*\n    # ------------------------------------------------------------------\n    sorted_items = sorted(scores.items(),\n                          key=lambda x: (-x[1], x[0]))           # desc score, asc index\n\n    return [idx for idx, _ in sorted_items[:k]]\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split maximises the reduction in the regularised loss used by\n    gradient-boosting decision trees.  If no legal split gives a positive\n    gain the function returns ``None``.\n    \"\"\"\n    n_samples, n_features = X.shape\n    # A legal split must leave at least two samples on each side\n    if n_samples < 4:\n        return None\n\n    # Helper: regularised loss of a leaf that contains gradients G and H\n    def leaf_loss(G: float, H: float) -> float:\n        return -0.5 * (G * G) / (H + lam) + gamma\n\n    # Statistics of the current (unsplit) node\n    G_total = g.sum()\n    H_total = h.sum()\n    parent_loss = leaf_loss(G_total, H_total)\n\n    best_gain = 0.0          # strictly positive gain is required\n    best_feature = None\n    best_threshold = None\n\n    # Evaluate every feature separately\n    for feat_idx in range(n_features):\n        # Sort the samples by the current feature so we can sweep once\n        order = np.argsort(X[:, feat_idx], kind='mergesort')\n        x_sorted = X[order, feat_idx]\n        g_sorted = g[order]\n        h_sorted = h[order]\n\n        # Cumulative sums give statistics for the *left* side after each sample\n        g_cumsum = np.cumsum(g_sorted)\n        h_cumsum = np.cumsum(h_sorted)\n\n        # Iterate over potential split positions.\n        # A split is *after* position i (i.e. left uses 0..i, right i+1..end)\n        # We need at least two samples per side \u21d2 i \u2208 [1, n-3]\n        for i in range(1, n_samples - 2):\n            # Skip positions where the feature value doesn\u2019t change \u2013 no real split\n            if x_sorted[i] == x_sorted[i + 1]:\n                continue\n\n            G_left  = g_cumsum[i]\n            H_left  = h_cumsum[i]\n            G_right = G_total - G_left\n            H_right = H_total - H_left\n\n            # Ensure two samples on the right as well (implicit for left)\n            # (Because duplicates were skipped, right size \u2265 n-i-1)\n            if n_samples - i - 1 < 2:\n                break  # further positions will only shrink the right side\n\n            loss_left  = leaf_loss(G_left,  H_left)\n            loss_right = leaf_loss(G_right, H_right)\n\n            gain = parent_loss - loss_left - loss_right\n\n            if gain > best_gain:           # strictly larger (positive as well)\n                best_gain = gain\n                best_feature = feat_idx\n                best_threshold = x_sorted[i]   # threshold is the current value\n\n    if best_feature is None:     # no positive gain found\n        return None\n    return best_feature, float(best_threshold)\n```\n\nExplanation of the core steps\n1. The total gradient (`G_total`) and Hessian (`H_total`) of the current node are\n   computed once, giving the node\u2019s present loss (`parent_loss`).\n\n2. For every feature  \n   \u2022 Instances are sorted by that feature, giving a single left-to-right sweep  \n   \u2022 Cumulative sums of gradients/Hessians give statistics of the *left* child\n     for any potential split position  \n   \u2022 Only positions where the feature value changes and both sides retain at\n     least two samples are considered  \n   \u2022 For each legal split the gain is `parent_loss \u2212 loss_left \u2212 loss_right`\n\n3. The split with the largest strictly positive gain is returned; if no such\n   split exists the function yields `None`.\n"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    #-------------------------------------------------\n    # 1. Detect whether the input is a scalar\n    #-------------------------------------------------\n    is_scalar = np.isscalar(x)\n\n    #-------------------------------------------------\n    # 2. Convert the input to a NumPy array (for uniform math operations)\n    #-------------------------------------------------\n    x_arr = np.asarray(x, dtype=float)     # ensures float precision\n\n    #-------------------------------------------------\n    # 3. Compute sigmoid and its gradient\n    #-------------------------------------------------\n    sigmoid_arr  = 1.0 / (1.0 + np.exp(-x_arr))\n    gradient_arr = sigmoid_arr * (1.0 - sigmoid_arr)\n\n    #-------------------------------------------------\n    # 4. Round results to 4 decimal places\n    #-------------------------------------------------\n    sigmoid_arr  = np.round(sigmoid_arr, 4)\n    gradient_arr = np.round(gradient_arr, 4)\n\n    #-------------------------------------------------\n    # 5. Return results in the required format\n    #-------------------------------------------------\n    if is_scalar:                                # scalar \u2192 float\n        return float(sigmoid_arr), float(gradient_arr)\n    else:                                        # array-like \u2192 Python lists\n        return sigmoid_arr.tolist(), gradient_arr.tolist()\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\nfrom collections import defaultdict          # (needed by the template \u2013 not used)\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test:  list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble of decision stumps and returns test predictions.\"\"\"\n    \n    # --- helper that applies one stump ----------------------------------------\n    def stump_predict(x: np.ndarray, d: int, theta: float) -> np.ndarray:\n        \"\"\"Return \u00b11 predictions of the stump (d , \u03b8) on vector x.\"\"\"\n        if d == 0:                 #  +1  for  x \u2264 \u03b8\n            return np.where(x <= theta,  1, -1)\n        else:                      #  +1  for  x > \u03b8\n            return np.where(x >  theta,  1, -1)\n    # --------------------------------------------------------------------------\n\n    # ---- prepare data --------------------------------------------------------\n    x_train = np.asarray(x_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    x_test  = np.asarray(x_test , dtype=float)\n\n    n = len(x_train)\n    if n == 0:                            # trivial corner-case\n        return [1] * len(x_test)\n\n    # ---- enumerate all candidate thresholds ---------------------------------\n    order      = np.argsort(x_train)\n    x_sorted   = x_train[order]\n\n    thresholds = [x_sorted[0] - 1.0]      # a value below the smallest sample\n    for i in range(n - 1):                # mid-points between successive, *distinct* values\n        if x_sorted[i] != x_sorted[i + 1]:\n            thresholds.append(0.5 * (x_sorted[i] + x_sorted[i + 1]))\n    thresholds.append(x_sorted[-1] + 1.0) # a value above the largest sample\n\n    stumps = [(d, th) for th in thresholds for d in (0, 1)]\n    # -------------------------------------------------------------------------\n\n    # ---- AdaBoost ------------------------------------------------------------\n    w = np.full(n, 1.0 / n)               # uniform initial weights\n    alphas, chosen_stumps = [], []\n\n    while True:\n        # pick best stump wrt current weights\n        best_err   = float('inf')\n        best_pred  = None\n        best_stump = None\n\n        for d, th in stumps:\n            pred = stump_predict(x_train, d, th)\n            err  = w[pred != y_train].sum()           # weighted error\n            if err < best_err - 1e-12:                # strict improvement\n                best_err, best_stump, best_pred = err, (d, th), pred\n\n        # safety \u2013 if no weak learner beats chance we stop\n        if best_err >= 0.5 - 1e-12:\n            break\n\n        # stump weight\n        err = max(best_err, 1e-12)                    # avoid log(0)\n        alpha = 0.5 * math.log((1.0 - err) / err)\n\n        chosen_stumps.append(best_stump)\n        alphas.append(alpha)\n\n        # update sample weights\n        w *= np.exp(-alpha * y_train * best_pred)\n        w  = w / w.sum()\n\n        # check current training error of the ensemble\n        F = np.zeros(n)\n        for a, (d, th) in zip(alphas, chosen_stumps):\n            F += a * stump_predict(x_train, d, th)\n        train_pred   = np.where(F >= 0.0, 1, -1)\n        train_error  = (train_pred != y_train).mean()\n\n        if train_error <= epsilon:\n            break\n    # -------------------------------------------------------------------------\n\n    # ---- predict on test set -------------------------------------------------\n    if not alphas:                                # no stump selected\n        return [1] * len(x_test)\n\n    F_test = np.zeros(len(x_test))\n    for a, (d, th) in zip(alphas, chosen_stumps):\n        F_test += a * stump_predict(x_test, d, th)\n\n    y_pred = np.where(F_test >= 0.0, 1, -1)\n    return y_pred.tolist()\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math   # kept because the header requires it (not actually needed below)\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    \n    # Ensure 1-D views of vectors (no copy unless necessary)\n    y_train = y_train.ravel()\n    alpha   = alpha.ravel()\n    \n    # Pre-compute the joint coefficient \u03b1_i y_i\n    coef = alpha * y_train          # shape (n,)\n    \n    # Squared \u21132 norms of train and test points\n    train_sq = np.sum(X_train**2, axis=1)          # (n,)\n    test_sq  = np.sum(X_test**2,  axis=1)          # (m,)\n    \n    # Pairwise squared Euclidean distances using the identity\n    # ||x \u2212 z||\u00b2 = ||x||\u00b2 + ||z||\u00b2 \u2212 2 x\u00b7z\n    # produces an (m \u00d7 n) distance matrix without explicit Python loops\n    dists_sq = test_sq[:, None] + train_sq[None, :] - 2.0 * X_test @ X_train.T\n    \n    # Gaussian (RBF) kernel matrix\n    K = np.exp(-gamma * dists_sq)                  # (m, n)\n    \n    # Decision values for all test samples\n    g = K @ coef + b                               # (m,)\n    \n    # Predicted labels: use sign; map 0 \u2192 1 (arbitrary, but required \u00b11)\n    pred = np.sign(g)\n    pred[pred == 0] = 1\n    \n    return pred.astype(int).tolist()\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        (X_hat, mse) where\n            \u2022 X_hat \u2013 rank-k reconstruction of X (each entry rounded to 4 decimals)\n            \u2022 mse    \u2013 mean-squared reconstruction error (rounded to 4 decimals)\n        If k is not in [1, min(m, n)] the function returns -1.\n    \"\"\"\n    # ------------ basic validation ------------\n    if not isinstance(k, int):\n        return -1\n    try:\n        X_arr = np.asarray(X, dtype=float)\n    except Exception:\n        return -1\n    if X_arr.ndim != 2:\n        return -1\n\n    m, n = X_arr.shape\n    if k < 1 or k > min(m, n):\n        return -1\n\n    # ------------ truncated SVD ---------------\n    # X = U \u03a3 V^T  (full_matrices=False \u21d2 U: m\u00d7r, \u03a3: r, V^T: r\u00d7n, r=min(m,n))\n    U, S, VT = np.linalg.svd(X_arr, full_matrices=False)\n\n    # keep the first k singular values/vectors\n    U_k  = U[:, :k]              # m \u00d7 k\n    S_k  = S[:k]                 # k\n    VT_k = VT[:k, :]             # k \u00d7 n\n\n    # optimal rank-k reconstruction:  U_k \u03a3_k V_k^T\n    X_hat = (U_k * S_k) @ VT_k   # broadcasting: multiply cols of U_k by S_k\n\n    # ------------ error & rounding ------------\n    mse = float(np.round(np.mean((X_arr - X_hat) ** 2), 4))\n    X_hat_rounded = np.round(X_hat, 4).tolist()\n\n    return X_hat_rounded, mse\n```"}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np            # <- kept deliberately: required by the exercise\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"1-D gradient boosting with decision stumps and prediction for *x_query*.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Convert to numpy arrays and guard against trivial situations\n    # ------------------------------------------------------------------ #\n    x = np.asarray(x_train, dtype=float).ravel()\n    y = np.asarray(y_train, dtype=float).ravel()\n\n    if x.size == 0:                         # no data -> return NaN\n        return float('nan')\n    \n    # If all x are identical (or only one sample), the best we can do is the mean\n    if np.all(x == x[0]) or x.size == 1:\n        return round(float(y.mean()), 4)\n    \n    # ------------------------------------------------------------------ #\n    # 1. Sort the data once (makes the rest easier)\n    # ------------------------------------------------------------------ #\n    order          = np.argsort(x)\n    x_sorted       = x[order]\n    y_sorted       = y[order]\n\n    # ------------------------------------------------------------------ #\n    # 2. Candidate split points = mid points between different neighbours\n    # ------------------------------------------------------------------ #\n    distinct_mask  = x_sorted[1:] != x_sorted[:-1]\n    mids           = (x_sorted[:-1] + x_sorted[1:]) / 2.0\n    candidate_s    = mids[distinct_mask]                 # shape (K,)\n\n    if candidate_s.size == 0:                # All identical x\u2019s\n        return round(float(y.mean()), 4)\n\n    # ------------------------------------------------------------------ #\n    # 3. Gradient-boosting loop\n    # ------------------------------------------------------------------ #\n    residuals      = y_sorted.copy()\n    stumps         = []                      # list of (split, c_left, c_right)\n\n    rss = np.sum(residuals ** 2)\n\n    max_iter = 1000                          # simple safeguard\n    iteration = 0\n    while rss > epsilon and iteration < max_iter:\n        best_rss      = np.inf\n        best_params   = None                 # (split, c1, c2, mask_left)\n\n        for s in candidate_s:\n            mask_left     = x_sorted <= s\n            mask_right    = ~mask_left\n\n            # Both sides non-empty by construction, but check anyway\n            if not mask_left.any() or not mask_right.any():\n                continue\n\n            c1 = residuals[mask_left].mean()\n            c2 = residuals[mask_right].mean()\n\n            # residuals after adding the stump\n            new_residuals = residuals - np.where(mask_left, c1, c2)\n            rss_candidate = np.sum(new_residuals ** 2)\n\n            if rss_candidate < best_rss:\n                best_rss    = rss_candidate\n                best_params = (s, c1, c2, mask_left)\n\n        # If for some strange reason we cannot improve, stop\n        if best_params is None or best_rss >= rss - 1e-12:\n            break\n\n        # Commit the best stump found\n        s_best, c1_best, c2_best, mask_left_best = best_params\n        stumps.append((s_best, c1_best, c2_best))\n\n        residuals = residuals - np.where(mask_left_best, c1_best, c2_best)\n        rss       = best_rss\n        iteration += 1\n\n    # ------------------------------------------------------------------ #\n    # 4. Prediction for the query point\n    # ------------------------------------------------------------------ #\n    prediction = 0.0\n    for s, c1, c2 in stumps:\n        prediction += c1 if x_query <= s else c2\n\n    return round(float(prediction), 4)\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"Create a tensor whose entries are 0.0 or 1.0.\n    \n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired shape of the returned NumPy array.\n    sparsity : float, default=0.5\n        Probability that any individual entry is 1.0.  Must satisfy\n        0.0 \u2264 sparsity \u2264 1.0.  Values outside this range cause the\n        function to return -1.\n    seed : int | None, default=None\n        If provided, ``np.random.seed(seed)`` is called first so the\n        output is reproducible.\n    \n    Returns\n    -------\n    numpy.ndarray | int\n        \u2022 A floating-dtype NumPy array containing only 0.0 and 1.0, if\n          `sparsity` is legal.  \n        \u2022 The integer -1 when `sparsity` is outside the closed interval\n          [0, 1].\n    \"\"\"\n    \n    # 1. Validate sparsity ----------------------------------------------------\n    if not (0.0 <= sparsity <= 1.0):\n        return -1\n    \n    # 2. Handle optional seeding ---------------------------------------------\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # 3. Draw uniform random numbers and threshold ---------------------------\n    #    `True` where random value < sparsity  ->  1.0\n    #    `False` otherwise                     ->  0.0\n    random_values = np.random.rand(*shape)\n    binary_tensor = (random_values < sparsity).astype(float)\n    \n    return binary_tensor\n```\n\nUsage example\n--------------\n```python\n>>> arr = random_binary_tensor((2, 3), sparsity=0.3, seed=42)\n>>> arr\narray([[0., 1., 1.],\n       [0., 0., 0.]])\n>>> arr.dtype\ndtype('float64')\n```\n\nThe function obeys every requirement:  \n\u2022 Validates sparsity, returning -1 when invalid.  \n\u2022 Seeds NumPy\u2019s RNG when requested.  \n\u2022 Generates each entry as 1.0 with probability equal to `sparsity`.  \n\u2022 Ensures only floating 0.0/1.0 values in the returned `numpy.ndarray`."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median\n          (upper median for even *n*);\n        \u2022 left  is a list of row indices with strictly smaller values;\n        \u2022 right is a list of row indices with strictly larger  values.\n        The two lists are sorted increasingly.\n    \"\"\"\n    # column to be examined\n    col = data[:, d]\n    n   = col.shape[0]\n\n    # position of the median (upper median when n is even)\n    k = n // 2\n\n    # argpartition gives the indices that would place the k-th element\n    # in its sorted position without fully sorting the array (O(n))\n    part_idx    = np.argpartition(col, k)\n    pivot       = int(part_idx[k])        # row index of the median element\n    pivot_value = col[pivot]\n\n    # Build the lists of indices in one linear scan (keeps O(n) overall)\n    left  = []\n    right = []\n    for idx in range(n):          # idx already increases, so the result is sorted\n        if idx == pivot:\n            continue              # skip the pivot itself\n        v = col[idx]\n        if v < pivot_value:\n            left.append(idx)\n        elif v > pivot_value:\n            right.append(idx)\n        # elements equal to the median (if any) are ignored\n\n    return pivot, left, right\n```"}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into equally sized, possibly overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length N.\n    frame_width : int\n        Number of samples per frame.\n    stride : int\n        Hop size (number of samples between the starts of successive frames).\n    writeable : bool, default=False\n        If False the returned view is marked read-only.\n        If True the view is writeable only when the source array is writeable.\n\n    Returns\n    -------\n    np.ndarray\n        View on *x* with shape (n_frames, frame_width), where\n            n_frames = (len(x) - frame_width) // stride + 1\n    \"\"\"\n    # ---------- pre-condition checks ----------\n    assert x.ndim == 1, \"x must be one-dimensional\"\n    assert frame_width > 0, \"frame_width must be a positive integer\"\n    assert stride > 0, \"stride must be a positive integer\"\n    assert len(x) >= frame_width, \"frame_width larger than signal length\"\n\n    # ---------- compute framing parameters ----------\n    n_frames = (len(x) - frame_width) // stride + 1\n    shape = (n_frames, frame_width)\n    item_stride = x.strides[0]                   # bytes between two consecutive samples\n    strides = (item_stride * stride, item_stride)\n\n    # ---------- build the view ----------\n    # Make it read-only unless the caller explicitly requested writeability\n    # and the original array is itself writeable.\n    view_writeable = x.flags.writeable if writeable else False\n    frames = as_strided(x, shape=shape, strides=strides, writeable=view_writeable)\n\n    return frames\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\"\"\"\n    try:\n        # Convert to NumPy array of floats\n        A = np.asarray(X, dtype=float)\n    except Exception:\n        return False                      # non-numeric or unconvertible input\n\n    # Must be a 2-D matrix with at least one element\n    if A.ndim != 2 or A.size == 0:\n        return False\n\n    # All entries must be finite and lie within [0, 1]\n    if ( ~np.isfinite(A) ).any():        # catches NaN/Inf\n        return False\n    if ((A < 0) | (A > 1)).any():\n        return False\n\n    # Each row must sum to 1 (within numerical tolerance)\n    if not np.allclose(A.sum(axis=1), 1.0, atol=1e-8):\n        return False\n\n    return True\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n\n    n, d = X.shape\n    w = np.zeros(d)\n    s = np.zeros(d)           # running average of squared gradients\n\n    # use full-batch if the data set is smaller than the desired batch size\n    batch_size = n if n < batch_size else batch_size\n\n    for it in range(max_iter):\n        # ---- mini-batch selection (cyclic, wraps around) --------------------\n        start = (it * batch_size) % n\n        idx = np.arange(start, start + batch_size) % n\n        X_b = X[idx]\n        y_b = y[idx]\n\n        # ---- gradient on the current mini-batch ----------------------------\n        err = X_b @ w - y_b\n        g = (X_b.T @ err) / len(y_b)          # shape: (d,)\n\n        # ---- RMSprop parameter update --------------------------------------\n        s = rho * s + (1.0 - rho) * (g ** 2)\n        w -= eta * g / (np.sqrt(s) + eps_station)\n\n        # ---- early stopping (check gradient on the whole data set) ---------\n        full_grad = (X.T @ (X @ w - y)) / n\n        if np.linalg.norm(full_grad) < epsilon:\n            break\n\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"\n    Numerically\u2013stable SoftPlus activation (and its gradient).\n\n    Parameters\n    ----------\n    x     : scalar, list or np.ndarray\n        Input value(s).\n    deriv : bool, default False\n        If True return the gradient (sigmoid), otherwise SoftPlus.\n\n    Returns\n    -------\n    float or list\n        Rounded result(s) \u2013 float when the input was a scalar,\n        otherwise a Python list.\n    \"\"\"\n    # Remember if the caller passed a plain scalar\n    is_scalar = np.isscalar(x)\n\n    # Convert to a NumPy array for vectorised math\n    x = np.asarray(x, dtype=np.float64)\n\n    if deriv:                       # ----- gradient: sigmoid(x)\n        # stable sigmoid:\n        #  \u03c3(x) = 1 / (1 + e^-x)           for x >= 0\n        #  \u03c3(x) = e^x / (1 + e^x)          for x < 0\n        pos_mask = x >= 0\n        neg_mask = ~pos_mask\n\n        out = np.empty_like(x)\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n        exp_x = np.exp(x[neg_mask])\n        out[neg_mask] = exp_x / (1.0 + exp_x)\n\n    else:                           # ----- SoftPlus(x)\n        # stable softplus:\n        #  softplus(x) = log1p(exp(-|x|)) + max(x, 0)\n        out = np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n\n    # Round to 4 decimals\n    out = np.round(out, 4)\n\n    # Return in the requested format\n    if is_scalar:\n        return float(out)           # plain Python float\n    else:\n        return out.tolist()\n```\n\nThe implementation  \n\n\u2022 accepts scalars, lists or NumPy arrays,  \n\u2022 uses numerically-stable piece-wise formulas for both SoftPlus and the sigmoid,  \n\u2022 rounds every value to four decimal places,  \n\u2022 and returns a float for scalar input or a Python list otherwise."}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef _count_ngrams(tokens: list[str], n: int) -> Counter:\n    \"\"\"\n    Helper: return a Counter of all length-n n-grams found in *tokens*.\n    Unigrams are counted as the string itself, higher orders as tuples.\n    \"\"\"\n    if n == 1:\n        return Counter(tokens)\n    counts = Counter()\n    for i in range(len(tokens) - n + 1):\n        counts[tuple(tokens[i:i + n])] += 1\n    return counts\n\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Unsmooth\u00aded MLE N-gram log-probability of *sequence* given *corpus*.\"\"\"\n    if N < 1:\n        raise ValueError('N must be >= 1')\n\n    # Special cases: empty sequence or shorter than the requested n-gram\n    if len(sequence) < N:\n        return 0.0\n\n    # Build the required count tables\n    if N == 1:\n        unigram_counts = _count_ngrams(corpus, 1)\n        corpus_len = len(corpus)\n    else:\n        ngram_counts = _count_ngrams(corpus, N)\n        prefix_counts = _count_ngrams(corpus, N - 1)\n\n    total_log_prob = 0.0\n\n    # Iterate over every length-N window in the sequence\n    for i in range(len(sequence) - N + 1):\n        if N == 1:\n            word = sequence[i]\n            numerator = unigram_counts.get(word, 0)\n            denominator = corpus_len\n        else:\n            ngram = tuple(sequence[i:i + N])\n            prefix = ngram[:-1]\n            numerator = ngram_counts.get(ngram, 0)\n            denominator = prefix_counts.get(prefix, 0)\n\n        # Unseen n-gram or prefix \u21d2 probability 0 \u21d2 log-probability \u2013inf\n        if numerator == 0 or denominator == 0:\n            return float('-inf')\n\n        prob = numerator / denominator\n        total_log_prob += float(np.log(prob))\n\n    # Round to 4 decimal places as required\n    return round(total_log_prob, 4)\n```\n\nThe function:\n\n1. Counts all n-grams (and prefixes) in the training corpus.  \n2. Slides a window of length `N` over the target sequence.  \n3. For each window, computes the MLE probability  \n\u2003 `count(ngram) / count(prefix)` (unigram case: `count(word)/|corpus|`).  \n4. As soon as a required count is zero it returns `float('-inf')`.  \n5. Otherwise it sums log-probabilities and returns the total rounded to 4 decimals."}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef lasso_regression(\n    X: list[float] | \"np.ndarray\",\n    y: list[float] | \"np.ndarray\",\n    degree: int,\n    reg_factor: float,\n    n_iterations: int = 1000,\n    tol: float = 1e-6,\n) -> list[float]:\n    \"\"\"Univariate Lasso regression with polynomial features (coordinate descent).\"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. Convert input to 1-D float arrays\n    # ------------------------------------------------------------------ #\n    X = np.asarray(X, dtype=float).ravel()\n    y = np.asarray(y, dtype=float).ravel()\n    m = X.size\n\n    if m == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n    if y.size != m:\n        raise ValueError(\"X and y must have the same length.\")\n    if degree < 0:\n        raise ValueError(\"degree must be non-negative.\")\n    if reg_factor < 0:\n        raise ValueError(\"reg_factor must be non-negative.\")\n\n    # ------------------------------------------------------------------ #\n    # 2. Build the design matrix [1, x, x\u00b2, \u2026 , x^degree]\n    # ------------------------------------------------------------------ #\n    X_poly = np.column_stack([np.ones_like(X)] + [X ** k for k in range(1, degree + 1)])\n\n    # ------------------------------------------------------------------ #\n    # 3. Coordinate-descent initialisation\n    # ------------------------------------------------------------------ #\n    d_plus_1 = degree + 1                      # number of coefficients (incl. bias)\n    w = np.zeros(d_plus_1)                    # weights initialised to 0\n\n    # helper: soft-thresholding operator\n    def soft_threshold(rho: float, lam: float) -> float:\n        if rho > lam:\n            return rho - lam\n        if rho < -lam:\n            return rho + lam\n        return 0.0\n\n    # ------------------------------------------------------------------ #\n    # 4. Optimisation loop\n    # ------------------------------------------------------------------ #\n    for _ in range(n_iterations):\n        w_old = w.copy()\n\n        # ---- update the bias exactly (NOT regularised) ----------------\n        # w0 \u2190 mean( y \u2212 X_{\u00ac0}\u00b7w_{\u00ac0} )\n        w[0] = np.mean(y - X_poly[:, 1:] @ w[1:])\n\n        # ---- update the remaining coordinates -------------------------\n        # iterate over j = 1 \u2026 degree\n        y_minus_pred = None   # will lazily compute A @ w once per pass\n        for j in range(1, d_plus_1):\n            x_j = X_poly[:, j]\n\n            if y_minus_pred is None:                 # compute only once per pass\n                y_minus_pred = y - X_poly @ w\n\n            # \u03c1 = x\u2c7c\u1d40 (y \u2212 (X\u00b7w) + w\u2c7c x\u2c7c)\n            rho = x_j @ (y_minus_pred + w[j] * x_j)\n\n            # w\u2c7c \u2190 soft_threshold(\u03c1, \u03bb) / (x\u2c7c\u1d40x\u2c7c)\n            if reg_factor == 0.0:\n                w[j] = rho / (x_j @ x_j)\n            else:\n                w[j] = soft_threshold(rho, reg_factor) / (x_j @ x_j)\n\n            # predictions changed -> refresh residual for later coordinates\n            y_minus_pred = y_minus_pred - (w[j] - w_old[j]) * x_j\n\n        # ---- convergence check ----------------------------------------\n        if np.max(np.abs(w - w_old)) < tol:\n            break\n\n    # ------------------------------------------------------------------ #\n    # 5. Return rounded coefficients\n    # ------------------------------------------------------------------ #\n    return [round(float(coef), 4) for coef in w]\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]],\n                  B: list[list[float]],\n                  obs: list[int],\n                  t: int) -> list[float]:\n    \"\"\"\n    Compute the backward-probability vector \u03b2_t for a discrete Hidden Markov\n    Model.\n\n    Parameters\n    ----------\n    A   : transition-probability matrix, shape (N, N)\n    B   : emission-probability matrix, shape (N, M)\n    obs : list with the indices of the observed symbols; length = T\n    t   : time index for which \u03b2_t is required  (0 \u2264 t < T)\n\n    Returns\n    -------\n    list[float]\n        The N backward probabilities \u03b2_t(i) (i = 0 \u2026 N-1), each rounded\n        to 4 decimal places.\n    \"\"\"\n    A = np.asarray(A, dtype=float)          # (N, N)\n    B = np.asarray(B, dtype=float)          # (N, M)\n    T  = len(obs)\n    N  = A.shape[0]\n\n    if not (0 <= t < T):\n        raise ValueError(\"t must satisfy 0 \u2264 t < len(obs)\")\n\n    # \u03b2_{T-1}(i) = 1  for all i\n    beta_next = np.ones(N)\n\n    # Work backwards: k = T-2, \u2026, t\n    for k in range(T - 2, t - 1, -1):\n        # element-wise product b_j(o_{k+1}) * \u03b2_{k+1}(j)\n        emis_beta = B[:, obs[k + 1]] * beta_next           # shape (N,)\n        # \u03b2_k(i) = \u03a3_j a_ij * b_j(o_{k+1}) * \u03b2_{k+1}(j)\n        beta_curr = A.dot(emis_beta)                       # shape (N,)\n        beta_next = beta_curr                              # becomes \u03b2_{k} for next iteration\n\n    # beta_next now holds \u03b2_t\n    return np.round(beta_next, 4).tolist()\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Args:\n        A: transition matrix (NxN)\n        B: emission   matrix (NxM)\n        pi: initial state distribution (N)\n        obs: observation index sequence (length T)\n\n    Returns:\n        Probability P(O | \u03bb) rounded to 6 decimals.\n    \"\"\"\n    # If any list is empty -> nothing to compute\n    if not A or not B or not pi or not obs:\n        return 0.0\n\n    # convert to numpy arrays for fast vectorised computation\n    A  = np.asarray(A,  dtype=float)   # (N,N)\n    B  = np.asarray(B,  dtype=float)   # (N,M)\n    pi = np.asarray(pi, dtype=float)   # (N,)\n    obs = np.asarray(obs, dtype=int)   # (T,)\n\n    N = len(pi)\n    T = len(obs)\n\n    # \u03b2_{T-1}(i) = 1   for every state i\n    beta = np.ones(N, dtype=float)\n\n    # Iterate backwards: t = T-2 \u2026 0\n    for t in range(T - 2, -1, -1):\n        # factor_j = B[j, o_{t+1}] * \u03b2_{t+1}(j)\n        factor = B[:, obs[t + 1]] * beta         # shape (N,)\n        # \u03b2_t(i) = \u03a3_j  A[i,j] * factor_j\n        beta = A.dot(factor)                     # new \u03b2 becomes \u03b2_t\n\n    # Finally P(O|\u03bb) = \u03a3_i \u03c0_i B[i, o_0] \u03b2_0(i)\n    prob = float(np.dot(pi * B[:, obs[0]], beta))\n\n    return round(prob, 6)\n```"}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # -----------------------------------------------------------\n    # 1. Basic size checks\n    # -----------------------------------------------------------\n    n = len(S)                                  # #states\n    if n == 0 or not observations:              # empty S or observation seq\n        return -1\n\n    # A must be n \u00d7 n\n    if len(A) != n or any(len(row) != n for row in A):\n        return -1\n\n    # B must be n \u00d7 m   (all rows same length)\n    if len(B) != n or len(B[0]) == 0:\n        return -1\n    m = len(B[0])\n    if any(len(row) != m for row in B):\n        return -1\n\n    # -----------------------------------------------------------\n    # 2. Observation indices in range\n    # -----------------------------------------------------------\n    for o in observations:\n        if o < 0 or o >= m:\n            return -1\n\n    # -----------------------------------------------------------\n    # 3. Convert to numpy for vectorised computation\n    # -----------------------------------------------------------\n    S = np.asarray(S, dtype=float)          # (n,)\n    A = np.asarray(A, dtype=float)          # (n, n)\n    B = np.asarray(B, dtype=float)          # (n, m)\n\n    # -----------------------------------------------------------\n    # 4. Forward algorithm\n    # -----------------------------------------------------------\n    # \u03b1_0(i) = S(i) * B(i, o_0)\n    alpha = S * B[:, observations[0]]       # (n,)\n\n    # \u03b1_t(j) = B(j, o_t) * \u03a3_i \u03b1_{t-1}(i) A(i, j)\n    for ot in observations[1:]:\n        alpha = B[:, ot] * (alpha @ A)      # (n,)  \u2190  (n,) @ (n,n) gives (n,)\n\n    likelihood = float(alpha.sum())\n\n    # -----------------------------------------------------------\n    # 5. Return rounded result\n    # -----------------------------------------------------------\n    return round(likelihood, 4)\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Optional, Dict, List\n\n\nclass _FPNode:\n    \"\"\"A node in an FP-tree.\"\"\"\n    __slots__ = ('item', 'count', 'parent', 'children', 'link')\n\n    def __init__(self, item: Optional[str], parent: Optional['._FPNode']):\n        self.item: Optional[str] = item          # item name (None for root)\n        self.count: int = 0                      # support counter\n        self.parent: Optional['_FPNode'] = parent\n        self.children: Dict[str, _FPNode] = {}   # item -> child node\n        self.link: Optional['_FPNode'] = None    # next node with the same item\n\n\n# ----------------------------------------------------------------------\n# FP-tree construction\n# ----------------------------------------------------------------------\ndef _build_fp_tree(transactions: List[List[str]],\n                   min_support: int) -> tuple[Optional[_FPNode], Optional[dict]]:\n    \"\"\"\n    Build an FP-tree and the accompanying header table from *transactions*.\n    Returns (root, header_table) or (None, None) if the tree would be empty.\n    The header table maps an item -> [support, head-of-node-link].\n    \"\"\"\n    # 1. global item frequencies\n    freq = Counter()\n    for t in transactions:\n        freq.update(t)\n    freq = {item: c for item, c in freq.items() if c >= min_support}\n    if not freq:                       # no frequent single items\n        return None, None\n\n    # 2. prepare header table  item -> [support, first_node]\n    header: dict[str, list] = {i: [c, None] for i, c in freq.items()}\n\n    # 3. create the root of the FP-tree\n    root = _FPNode(None, None)\n\n    # 4. insert every transaction\n    for transaction in transactions:\n        # keep only frequent items and sort by (\u2012support, item)  (Han et al.)\n        filtered = [i for i in transaction if i in freq]\n        if not filtered:\n            continue\n        ordered = sorted(filtered, key=lambda x: (-freq[x], x))\n\n        # walk / extend the tree\n        current = root\n        for item in ordered:\n            if item in current.children:\n                child = current.children[item]\n                child.count += 1\n            else:\n                child = _FPNode(item, current)\n                child.count = 1\n                current.children[item] = child\n\n                # attach to header table\u2019s node-link structure\n                head = header[item][1]\n                if head is None:\n                    header[item][1] = child\n                else:\n                    while head.link is not None:\n                        head = head.link\n                    head.link = child\n            current = child\n\n    return root, header\n\n\n# ----------------------------------------------------------------------\n# Mining\n# ----------------------------------------------------------------------\ndef _mine_tree(header: dict, prefix: List[str], freq_itemsets: List[List[str]],\n               min_support: int) -> None:\n    \"\"\"\n    Recursively mine an FP-tree that is represented by *header*.\n    All frequent item-sets found are appended (in lexicographic order)\n    to *freq_itemsets*.  *prefix* is the current prefix (already sorted).\n    \"\"\"\n    # items in ascending support, then lexicographic \u2192 deterministic order\n    items = sorted(header.items(), key=lambda x: (x[1][0], x[0]))\n\n    for item, (support, node) in items:\n        new_prefix = sorted(prefix + [item])         # alphabetically sorted\n        freq_itemsets.append(new_prefix)             # every prefix is frequent\n\n        # ---------------- conditional pattern base -----------------\n        cond_transactions: List[List[str]] = []\n        link_node = node\n        while link_node is not None:\n            path: List[str] = []\n            parent = link_node.parent\n            while parent and parent.item is not None:\n                path.append(parent.item)\n                parent = parent.parent\n            if path:\n                path.reverse()                       # root \u2192 leaf order\n                # replicate *path* link_node.count times\n                cond_transactions.extend([path] * link_node.count)\n            link_node = link_node.link\n\n        # --------------- build / mine conditional tree --------------\n        cond_root, cond_header = _build_fp_tree(cond_transactions, min_support)\n        if cond_header:\n            _mine_tree(cond_header, new_prefix, freq_itemsets, min_support)\n\n\n# ----------------------------------------------------------------------\n# User facing function\n# ----------------------------------------------------------------------\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings). `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result is deterministic:\n      \u2022 inside each item-set items are sorted alphabetically;\n      \u2022 the outer list is sorted by `(len(itemset), itemset)`.\n    \"\"\"\n    if not transactions or min_support < 1:\n        return []\n\n    # 1. build the global FP-tree\n    root, header = _build_fp_tree(transactions, min_support)\n    if header is None:                 # no frequent items at all\n        return []\n\n    # 2. mine the tree\n    frequent_itemsets: list[list[str]] = []\n    _mine_tree(header, [], frequent_itemsets, min_support)\n\n    # 3. final deterministic ordering\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    return frequent_itemsets\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list[int],\n                X_test:  list[list[float]],\n                k: int) -> list[int]:\n    \"\"\"k-Nearest Neighbour classifier (Euclidean metric).\n\n    Returns a list with the predicted class for every row in `X_test`.\n    If *k* is not a positive integer or *k* > n_train the function\n    returns the scalar -1 (as required by the specification).\n    \"\"\"\n    # 1) --- sanity-check k ----------------------------------------------------\n    if not isinstance(k, int) or k <= 0 or k > len(X_train):\n        return -1\n\n    # 2) --- convert input lists to numpy arrays ------------------------------\n    X_train = np.asarray(X_train, dtype=float)\n    X_test  = np.asarray(X_test,  dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n\n    # 3) --- prediction --------------------------------------------------------\n    preds: list[int] = []\n\n    for x in X_test:\n        # Squared Euclidean distances (square-root not necessary)\n        diff   = X_train - x\n        dists  = np.einsum('ij,ij->i', diff, diff)   # fast row-wise dot\n\n        # indices of the k smallest distances\n        if k == 1:\n            nn_idx = [int(np.argmin(dists))]\n        else:\n            nn_idx = np.argpartition(dists, k-1)[:k]\n\n        # majority vote among the neighbours\n        votes: dict[int, int] = {}\n        for lbl in y_train[nn_idx]:\n            votes[int(lbl)] = votes.get(int(lbl), 0) + 1\n\n        max_vote = max(votes.values())\n        # tie-break by picking the smallest class label\n        winner   = min(lbl for lbl, cnt in votes.items() if cnt == max_vote)\n\n        preds.append(winner)\n\n    return preds\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities (same shape).\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    # Convert to NumPy arrays for fast vectorised operations\n    y = np.asarray(y, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    # Machine epsilon for numerical stability (avoids log(0))\n    eps = np.finfo(float).eps\n\n    # Cross-entropy computation (unnormalised, i.e., summed over the batch)\n    loss = -np.sum(y * np.log(y_pred + eps))\n\n    # Round to 4 decimal places and return as a Python float\n    return float(np.round(loss, 4))\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]):\n    \"\"\"Forward propagation for an L-layer fully-connected neural network\n    that uses ReLU in all hidden layers and a sigmoid in the output layer.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing weights W1 \u2026 WL and biases b1 \u2026 bL.\n\n    Returns\n    -------\n    tuple[list[list[float]], list]\n        \u2022 AL_list  \u2013 the final activation rounded to 4 decimals\n                     and converted to a regular Python list.\n        \u2022 caches   \u2013 list with one cache per layer (kept for a backward pass).\n    \"\"\"\n    # ---------- helper functions ----------\n    def linear_forward(A_prev, W, b):\n        Z = W @ A_prev + b\n        return Z, (A_prev, W, b)\n\n    def relu(Z):\n        A = np.maximum(0, Z)\n        return A, Z  # activation-cache is simply Z\n\n    def sigmoid(Z):\n        A = 1.0 / (1.0 + np.exp(-Z))\n        return A, Z\n\n    # ---------- main forward loop ----------\n    caches = []\n    A = X\n    L = len(parameters) // 2  # number of layers\n\n    # layers 1 \u2026 L-1  (ReLU)\n    for l in range(1, L):\n        W = parameters[f\"W{l}\"]\n        b = parameters[f\"b{l}\"]\n        Z, lin_cache = linear_forward(A, W, b)\n        A, act_cache = relu(Z)\n        caches.append((lin_cache, act_cache))\n\n    # layer L (sigmoid)\n    WL = parameters[f\"W{L}\"]\n    bL = parameters[f\"b{L}\"]\n    ZL, lin_cache = linear_forward(A, WL, bL)\n    AL, act_cache = sigmoid(ZL)\n    caches.append((lin_cache, act_cache))\n\n    # ---------- formatting the required output ----------\n    AL_list = np.round(AL, 4).tolist()  # (1, m) \u2192 list\n\n    return AL_list, caches\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n    \n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n    \n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    eps = 1e-15                       # small constant to avoid log(0)\n    A2 = np.clip(A2, eps, 1.0 - eps)  # clip predictions\n    m = Y.size                        # number of samples\n\n    cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n    return float(np.round(cost, 6))\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft action\u2013selection probabilities.\n\n    Parameters\n    ----------\n    Q : 1-D sequence (list or np.ndarray)\n        Q-values for every available action in the current state.\n    epsilon : float\n        Exploration parameter 0 \u2264 \u03b5 \u2264 1.\n\n    Returns\n    -------\n    list[float]\n        A list with the probability of choosing every action under the \u03b5-soft\n        policy.  All probabilities are rounded to 4 decimal places and sum to\n        1 (within 1\u00d710\u207b\u00b9\u00b2).\n    \"\"\"\n    # ---- basic checks -------------------------------------------------------\n    q = np.asarray(Q, dtype=float).ravel()          # ensure 1-D float array\n    if q.ndim != 1:\n        raise ValueError(\"Q must be a one-dimensional sequence of numbers.\")\n    if not (0.0 <= epsilon <= 1.0):\n        raise ValueError(\"epsilon must satisfy 0 \u2264 \u03b5 \u2264 1.\")\n\n    n = q.size\n    if n == 0:\n        raise ValueError(\"Q must contain at least one action.\")\n\n    # ---- \u03b5-soft probabilities (full precision) -----------------------------\n    greedy_idx = int(np.argmax(q))                  # first arg-max\n    base_prob = epsilon / n\n    probs = np.full(n, base_prob, dtype=float)\n    probs[greedy_idx] += (1.0 - epsilon)            # favour greedy action\n\n    # ---- convert to 4-decimal representation that still sums to one --------\n    #\n    # Work with integer \u201cticks\u201d of 0.0001 = 1/10000 to avoid rounding drift.\n    #\n    ticks = probs * 10000.0\n    int_part = np.floor(ticks).astype(int)          # 4-decimal truncation\n    remainder = ticks - int_part                    # fractional part in ticks\n    deficit = 10000 - int_part.sum()                # ticks left to distribute\n\n    if deficit > 0:                                # need to add \u2018deficit\u2019 ticks\n        # give extra ticks to the largest remainders\n        order = np.argsort(-remainder)              # descending remainders\n        int_part[order[:deficit]] += 1\n    elif deficit < 0:                              # (very rare) too many ticks\n        order = np.argsort(remainder)               # smallest remainders first\n        int_part[order[: -deficit]] -= 1\n\n    final_probs = int_part / 10000.0                # back to floats (4 dp)\n\n    # ---- final sanity check -------------------------------------------------\n    if abs(final_probs.sum() - 1.0) > 1e-12:\n        raise RuntimeError(\"Probabilities do not sum to one within tolerance.\")\n\n    return final_probs.round(4).tolist()\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List, Union\n\nLabel = Union[int, float, str]\n\ndef aggregate_random_forest_votes(predictions: List[List[Label]]) -> List[Label]:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # Handle edge-cases: empty forest or empty sample set\n    if not predictions or not predictions[0]:\n        return []\n\n    n_samples = len(predictions[0])                # number of samples\n    majority_votes: List[Label] = []\n\n    for idx in range(n_samples):\n        # Collect all tree votes for the current sample\n        votes = [tree_preds[idx] for tree_preds in predictions]\n\n        # Count occurrences of each label\n        vote_counts = Counter(votes)\n        max_count = max(vote_counts.values())\n\n        # All labels that reached the highest vote count\n        tied_labels = [label for label, cnt in vote_counts.items()\n                       if cnt == max_count]\n\n        # Tie-break by picking the smallest label (numeric or lexicographic)\n        majority_votes.append(min(tied_labels))\n\n    return majority_votes\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    # Determine if the incoming object should be treated as a scalar\n    is_scalar = np.isscalar(z) or (isinstance(z, np.ndarray) and z.shape == ())\n\n    # Convert to NumPy array for vectorized computation\n    z_arr = np.asarray(z, dtype=float)\n\n    # Numerically stable softplus: ln(1 + e**z) == logaddexp(0, z)\n    soft = np.logaddexp(0.0, z_arr)\n\n    # Round to 4 decimal places\n    soft = np.round(soft, 4)\n\n    # Return according to the input type requirement\n    if is_scalar:\n        return float(soft)\n    else:\n        return soft.tolist()\n```\n"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Any\n\n\ndef _get_field(obj: Any, name: str):\n    \"\"\"Return attribute or (key) item `name` from `obj`, else raise KeyError.\"\"\"\n    if hasattr(obj, name):\n        return getattr(obj, name)\n    try:\n        return obj[name]\n    except Exception as exc:          # noqa: BLE001, E722\n        raise KeyError(f\"{name!r} not found in supplied object.\") from exc\n\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Mean-squared error between a policy\u2019s value estimates and the truth.\"\"\"\n    # Extract data ------------------------------------------------------------\n    try:\n        arm_evs = _get_field(bandit, \"arm_evs\")\n    except KeyError:\n        raise ValueError(\"`bandit` must supply an `arm_evs` field.\")\n\n    try:\n        ev_estimates = _get_field(policy, \"ev_estimates\")\n    except KeyError:\n        return np.nan  # no estimates supplied\n\n    # No estimates or empty dict \u2192 nan\n    if not ev_estimates:\n        return np.nan\n\n    # Compute squared errors --------------------------------------------------\n    errors = []\n    for arm_idx in sorted(ev_estimates):\n        # Skip indices that have no true EV (out-of-range); could also raise.\n        if 0 <= arm_idx < len(arm_evs):\n            diff = ev_estimates[arm_idx] - arm_evs[arm_idx]\n            errors.append(diff * diff)\n\n    # If nothing could be compared \u2192 nan\n    if not errors:\n        return np.nan\n\n    mse_val = float(np.mean(errors))\n    return round(mse_val, 4)\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter, defaultdict\n\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\"\"\"\n    # --------------------------- guard clauses ----------------------------\n    if N <= 0:\n        raise ValueError(\"N must be a positive integer.\")\n    seq_tokens = sequence.split()\n    if len(seq_tokens) < N:                 # \u201cno N-grams exist\u201d\n        return 0.0\n\n    # ----------------------------- training -------------------------------\n    # Build vocabulary (words seen in the training corpus)\n    vocab = set()\n    # n-gram counters: index 1..N  (Counter of tuples)\n    ngram_counts: dict[int, Counter] = {n: Counter() for n in range(1, N + 1)}\n\n    for sent in corpus:\n        tokens = sent.split()\n        vocab.update(tokens)\n\n        # populate n-gram counters\n        for n in range(1, N + 1):\n            if len(tokens) < n:\n                continue\n            for i in range(len(tokens) - n + 1):\n                ngram = tuple(tokens[i : i + n])\n                ngram_counts[n][ngram] += 1\n\n    # add the explicit <UNK> token to the vocabulary\n    vocab.add(\"<UNK>\")\n    V = len(vocab)                          # vocabulary size |V|\n\n    # total token count (needed for unigram denominator)\n    total_tokens = sum(ngram_counts[1].values())\n\n    # ----------------------- prepare target sentence ----------------------\n    # replace unseen words by <UNK>\n    seq_tokens = [tok if tok in vocab else \"<UNK>\" for tok in seq_tokens]\n\n    # --------------------- compute sentence log-prob ----------------------\n    log_prob = 0.0\n    for i in range(len(seq_tokens) - N + 1):\n        ngram = tuple(seq_tokens[i : i + N])\n        context = ngram[:-1]                # empty tuple for unigrams\n        target  = ngram[-1]\n\n        # counts from training corpus\n        count_ngram = ngram_counts[N].get(ngram, 0)\n\n        if N == 1:                          # unigram model: context is \u201c\u201d\n            context_count = total_tokens\n        else:\n            context_count = ngram_counts[N - 1].get(context, 0)\n\n        # Additive (Lidstone/Laplace) smoothing\n        prob = (count_ngram + K) / (context_count + K * V)\n        log_prob += math.log(prob)\n\n    return round(log_prob, 4)\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    # Ensure X is float64 for numerical stability\n    X = np.asarray(X, dtype=np.float64)\n    N, M = X.shape\n\n    # -------- initialisation -------------------------------------------------\n    rng = np.random.RandomState(0)          # deterministic seed\n    W = rng.randn(N, K)                     # (N, K)\n    H = rng.randn(K, M)                     # (K, M)\n\n    # Identity matrix used for the ridge terms\n    I_K = np.eye(K, dtype=np.float64)\n\n    def _solve(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n        \"\"\"Solve AX = B for X with fallback to pseudo-inverse if singular.\"\"\"\n        try:\n            return np.linalg.solve(A, B)\n        except np.linalg.LinAlgError:\n            return np.linalg.pinv(A) @ B\n\n    # --------------- main ALS loop ------------------------------------------\n    best_W, best_H = W.copy(), H.copy()\n    best_loss = np.inf\n\n    for _ in range(max_iter):\n        # ---- update W (fix H) ---------------------------------------------\n        HHT = H @ H.T + alpha * I_K            # (K, K)\n        # Solve (H H\u1d40 + \u03b1I) W\u1d40 = H X\u1d40   \u21d2   W = ( (H H\u1d40 + \u03b1I)\u207b\u00b9 H X\u1d40 )\u1d40\n        W = _solve(HHT, H @ X.T).T            # (N, K)\n\n        # ---- update H (fix W) ---------------------------------------------\n        WTW = W.T @ W + alpha * I_K           # (K, K)\n        # Solve (W\u1d40 W + \u03b1I) H = W\u1d40 X\n        H = _solve(WTW, W.T @ X)              # (K, M)\n\n        # ---- compute regularised loss -------------------------------------\n        diff = X - W @ H\n        loss = np.sum(diff ** 2) + alpha * (np.sum(W ** 2) + np.sum(H ** 2))\n\n        # Keep the best factors seen so far (helps in non-converging cases)\n        if loss < best_loss:\n            best_loss = loss\n            best_W, best_H = W.copy(), H.copy()\n\n        # Termination criterion\n        if loss <= tol:\n            break\n\n    # ------------------- reconstruction -------------------------------------\n    X_hat = (best_W @ best_H).round(4).tolist()\n    return X_hat\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\n\ndef _euclidean_squared(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"Squared Euclidean distance between every row of `a` and `b` (broadcast).\"\"\"\n    # a : (m, n) , b : (k, n)\n    # returns (m, k)\n    return ((a[:, None, :] - b[None, :, :]) ** 2).sum(axis=2)\n\n\ndef _kmeans_pp_init(X: np.ndarray, K: int) -> np.ndarray:\n    \"\"\"Return K initial centroids picked with the K-Means++ heuristic.\"\"\"\n    m = X.shape[0]\n    # Choose the first centroid completely at random from the points.\n    centroids = [X[np.random.randint(m)]]\n\n    for _ in range(1, K):\n        # Compute squared distance to the nearest existing centroid\n        d2 = _euclidean_squared(X, np.array(centroids)).min(axis=1)\n        probs = d2 / d2.sum()\n\n        # Pick a new centroid with probability \u221d distance\u00b2\n        r = random.random()\n        cumulative = np.cumsum(probs)\n        idx = np.searchsorted(cumulative, r)\n        centroids.append(X[idx])\n\n    return np.array(centroids)\n\n\ndef kmeans(\n    X: np.ndarray,\n    K: int,\n    max_iter: int = 100,\n    random_state: int | None = None,\n) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\"\"\"\n    # ------------------------------------------------------------------ seeds\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n\n    X = np.asarray(X, dtype=float)\n    m, n = X.shape\n\n    if K <= 0 or K > m:\n        raise ValueError(\"K must be a positive integer \u2264 number of samples.\")\n\n    # ---------------------------------------------------- K-Means++ seeding\n    centroids = _kmeans_pp_init(X, K)\n\n    # ------------------------------------------------------- Lloyd\u2019s loop\n    labels = np.full(m, -1, dtype=int)  # previous assignment\n    for _ in range(max_iter):\n        # ASSIGN step\n        distances = _euclidean_squared(X, centroids)  # shape (m, K)\n        new_labels = distances.argmin(axis=1)\n\n        # Convergence check\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # UPDATE step\n        for k in range(K):\n            mask = labels == k\n            if mask.any():  # non-empty cluster\n                centroids[k] = X[mask].mean(axis=0)\n            # else: keep centroid unchanged (specification)\n\n    # ----------------------------------------------------- Post-processing\n    centroids_rounded = [\n        [round(float(v), 4) for v in centroid] for centroid in centroids\n    ]\n    centroids_sorted = sorted(centroids_rounded, key=lambda c: (c[0], c))\n\n    return centroids_sorted\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nTIME_STEPS = 20               # fixed length expected by the model\nPAD_TOKEN  = 0                # we reserve the id \u201c0\u201d for padding / \u201cunknown\u201d\n\n\ndef string_to_int(text: str,\n                  time_steps: int,\n                  vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"\n    Encode a raw string into a fixed-length list of integer ids.\n\n    \u2022 Each character is looked up in `vocabulary`.\n    \u2022 Unknown characters map to PAD_TOKEN (0).\n    \u2022 The returned list is exactly `time_steps` long:\n        \u2013 shorter input  \u2192 pad with 0 on the right\n        \u2013 longer  input  \u2192 truncate on the right\n    \"\"\"\n    # Encode (truncate if necessary)\n    encoded = [vocabulary.get(ch, PAD_TOKEN) for ch in text[:time_steps]]\n\n    # Pad up to the required length\n    if len(encoded) < time_steps:\n        encoded.extend([PAD_TOKEN] * (time_steps - len(encoded)))\n\n    return encoded\n\n\ndef int_to_string(indices,\n                  inverse_vocab: dict[int, str]) -> str:\n    \"\"\"\n    Decode a sequence of integer ids back into a string.\n\n    \u2022 Works with Python lists, tuples or NumPy arrays.\n    \u2022 Padding / unknown tokens (id 0) are ignored.\n    \"\"\"\n    # Flatten in case a batched array comes in\n    indices = np.ravel(indices)\n\n    chars = []\n    for idx in indices:\n        if idx == PAD_TOKEN:          # skip padding\n            continue\n        char = inverse_vocab.get(idx)\n        if char is not None:          # skip truly unknown ids\n            chars.append(char)\n\n    return \"\".join(chars)\n\n\ndef run_example(model,\n                input_vocabulary: dict[str, int],\n                inv_output_vocabulary: dict[int, str],\n                text: str) -> str:\n    \"\"\"\n    End-to-end helper:\n    1. encode text\n    2. feed the single example to `model.predict`\n    3. take arg-max over the last axis\n    4. decode the predicted ids back to text\n    \"\"\"\n    # 1. Encode ------------------------------------------------------------\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    batch   = np.array(encoded, dtype=np.int64)[np.newaxis, :]   # shape (1, TIME_STEPS)\n\n    # 2. Model prediction --------------------------------------------------\n    logits = model.predict(batch)        # expected shape (1, TIME_STEPS, vocab_size)\n\n    # 3. Convert logits to ids --------------------------------------------\n    pred_ids = np.argmax(logits, axis=-1)   # shape (1, TIME_STEPS)\n\n    # 4. Decode ------------------------------------------------------------\n    decoded = int_to_string(pred_ids[0], inv_output_vocabulary)\n\n    return decoded\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str] or None\n        A collection of input strings.  If *None*, the function uses the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    # Fall back to a global constant if `examples` was not supplied\n    if examples is None:\n        try:\n            examples = EXAMPLES\n        except NameError:\n            raise ValueError(\n                \"No `examples` provided and the global constant `EXAMPLES` \"\n                \"is not defined.\"\n            )\n\n    predictions = []\n\n    # Iterate over each example, run the model, and collect / print the output\n    for example in examples:\n        # run_example is assumed to exist in the runtime\n        output_chars = run_example(\n            model,\n            input_vocabulary,\n            inv_output_vocabulary,\n            example,\n        )\n\n        # `run_example` is specified to return a list of characters;\n        # join them into a single string.\n        predicted_str = \"\".join(output_chars)\n\n        # Display the input / output pair\n        print(f\"input:  {example}\")\n        print(f\"output: {predicted_str}\")\n\n        predictions.append(predicted_str)\n\n    return predictions\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(\n        X: np.ndarray,\n        y: np.ndarray,\n        w: np.ndarray\n    ) -> tuple[float, list[list[float]]]:\n    \"\"\"Binary cross-entropy loss and gradient for logistic regression.\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (m, n)\n    y : np.ndarray\n        Binary targets of shape (m,) or (m, 1)\n    w : np.ndarray\n        Weight vector of shape (n,) or (n, 1)\n    \n    Returns\n    -------\n    tuple\n        (loss, gradient) where\n          \u2022 loss is a float rounded to 4 decimals\n          \u2022 gradient is a nested list (shape (n, 1)) whose\n            elements are rounded to 4 decimals\n    \"\"\"\n    # Ensure correct shapes\n    y = y.reshape(-1)                  # (m,)\n    w = w.reshape(-1)                  # (n,)\n    m = X.shape[0]\n\n    # Forward pass \u2013 sigmoid predictions\n    z = X @ w                          # (m,)\n    p = 1 / (1 + np.exp(-z))           # (m,)\n    \n    # Numerical stability\n    eps = 1e-20\n    p = np.clip(p, eps, 1 - eps)\n\n    # Binary cross-entropy loss\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    loss = float(np.round(loss, 4))\n\n    # Gradient\n    grad = (X.T @ (p - y)) / m         # (n,)\n    grad = grad.reshape(-1, 1)         # (n, 1) column-vector\n    grad = np.round(grad, 4).tolist()  # nested Python list\n\n    return loss, grad\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef adaboost_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    n_clf: int = 5\n) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array (m, n) with train features.\n        y_train: 1-D NumPy array (m,) with labels **-1** or **1**.\n        X_test : 2-D NumPy array (k, n) with test features.\n        n_clf  : Number of weak classifiers (\u22651).\n\n    Returns\n        Python list (length k) with predicted labels (-1 or 1).\n    \"\"\"\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def _best_stump(X, y, w):\n        \"\"\"Find the decision stump that minimises weighted error.\"\"\"\n        m, n_features = X.shape\n        best_err = np.inf\n        stump = {}          # feature, thresh, polarity\n        best_pred = None\n\n        for j in range(n_features):\n            Xj = X[:, j]\n            uniq = np.unique(Xj)\n\n            # candidate thresholds: mid-points + two extremes\n            if uniq.size == 1:\n                thresholds = [uniq[0] - 1e-10, uniq[0] + 1e-10]\n            else:\n                thresholds = list((uniq[:-1] + uniq[1:]) / 2.0)\n                thresholds.append(uniq[0] - 1e-10)\n                thresholds.append(uniq[-1] + 1e-10)\n\n            for thresh in thresholds:\n                for polarity in (1, -1):\n                    # prediction rule\n                    preds = np.ones(m, dtype=int)\n                    if polarity == 1:\n                        preds[Xj < thresh] = -1\n                    else:                       # polarity == -1\n                        preds[Xj < thresh] = 1\n\n                    err = np.sum(w[preds != y])\n\n                    if err < best_err:\n                        best_err = err\n                        stump = {'feature': j,\n                                 'threshold': thresh,\n                                 'polarity': polarity}\n                        best_pred = preds\n\n        return stump, best_err, best_pred\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  parameter checks \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if n_clf < 1:\n        n_clf = 1\n    y_train = y_train.astype(int)\n    m = X_train.shape[0]\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  AdaBoost training \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    weights = np.full(m, 1 / m, dtype=float)\n    stumps = []\n\n    eps = 1e-10  # to avoid division by zero / log(0)\n\n    for _ in range(n_clf):\n        stump, err, preds = _best_stump(X_train, y_train, weights)\n\n        # safeguard for perfect or very bad stumps\n        err = np.clip(err, eps, 1 - eps)\n        alpha = 0.5 * np.log((1 - err) / err)\n\n        # update sample weights\n        weights *= np.exp(-alpha * y_train * preds)\n        weights /= weights.sum()\n\n        stump['alpha'] = alpha\n        stumps.append(stump)\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  prediction on X_test \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    k = X_test.shape[0]\n    agg = np.zeros(k)\n\n    for stump in stumps:\n        j = stump['feature']\n        thresh = stump['threshold']\n        pol = stump['polarity']\n        pred = np.ones(k, dtype=int)\n        if pol == 1:\n            pred[X_test[:, j] < thresh] = -1\n        else:\n            pred[X_test[:, j] < thresh] = 1\n        agg += stump['alpha'] * pred\n\n    final_pred = np.sign(agg)\n    final_pred[final_pred == 0] = 1  # tie-break\n\n    return final_pred.astype(int).tolist()\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Fisher LDA and project the data.\"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Quick exit\n    # ------------------------------------------------------------------ #\n    if n_components == 0:\n        return []\n\n    n_samples, n_features = X.shape\n    classes            = np.unique(y)\n    n_classes          = classes.size\n\n    # ------------------------------------------------------------------ #\n    # 1. Class means, global mean, scatters\n    # ------------------------------------------------------------------ #\n    global_mean = X.mean(axis=0)\n\n    # initialise scatter matrices\n    S_W = np.zeros((n_features, n_features), dtype=float)\n    S_B = np.zeros_like(S_W)\n\n    for cls in classes:\n        X_c      = X[y == cls]\n        n_c      = X_c.shape[0]\n        mean_c   = X_c.mean(axis=0)\n\n        # within\u2013class scatter\n        diff     = X_c - mean_c\n        S_W     += diff.T @ diff\n\n        # between\u2013class scatter\n        mean_diff = (mean_c - global_mean).reshape(-1, 1)\n        S_B      += n_c * (mean_diff @ mean_diff.T)\n\n    # ------------------------------------------------------------------ #\n    # 2. Solve  pinv(S_W) S_B  eigen-problem\n    # ------------------------------------------------------------------ #\n    A = np.linalg.pinv(S_W) @ S_B\n\n    # A is (theoretically) symmetric \u2013 use eigh\n    eig_vals, eig_vecs = np.linalg.eigh(A)\n\n    # ------------------------------------------------------------------ #\n    # 3. Sort eigen-pairs (descending eigen-values)\n    # ------------------------------------------------------------------ #\n    sorted_idx = np.argsort(eig_vals)[::-1]\n    eig_vecs   = eig_vecs[:, sorted_idx]          # columns are eigenvectors\n\n    # keep only the requested components\n    eig_vecs = eig_vecs[:, :n_components]\n\n    # ------------------------------------------------------------------ #\n    # 4. Deterministic sign flipping\n    # ------------------------------------------------------------------ #\n    for i in range(eig_vecs.shape[1]):\n        vec = eig_vecs[:, i]\n        # first non-zero element decides the sign\n        for v in vec:\n            if np.abs(v) > 1e-12:      # treat as non-zero\n                if v < 0:\n                    eig_vecs[:, i] = -vec\n                break\n\n    # ------------------------------------------------------------------ #\n    # 5. Project data and round\n    # ------------------------------------------------------------------ #\n    projected = X @ eig_vecs                # shape (n_samples, n_components)\n    projected = np.round(projected, 4)\n\n    return projected.tolist()\n```"}
{"task_id": 140, "completion_id": 0, "completion": "```python\nfrom collections import deque\nimport math\n\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Return the minimum number of minutes required for the agent to reach\n    the lower-right corner of a square maze that contains spreading fire.\n\n    The fire spreads first every minute, then the agent moves.  The agent may\n    enter a cell only if that cell is not burning at the moment he arrives.\n\n    Parameters\n    ----------\n    grid : list[str]\n        An n\u00d7n list of strings consisting only of '.', '#', 'F'.\n\n    Returns\n    -------\n    int\n        The minimum time to reach the goal, or \u20111 if it is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 0:\n        return -1\n\n    # ------------------------------------------------------------------\n    # 1.  Multi\u2013source BFS \u2013 when does every cell catch fire?\n    # ------------------------------------------------------------------\n    INF = math.inf\n    fire_time = [[INF] * n for _ in range(n)]\n    q_fire = deque()\n\n    for r in range(n):\n        for c, ch in enumerate(grid[r]):\n            if ch == 'F':\n                fire_time[r][c] = 0\n                q_fire.append((r, c))\n\n    dirs = ((1, 0), (-1, 0), (0, 1), (0, -1))\n\n    while q_fire:\n        r, c = q_fire.popleft()\n        for dr, dc in dirs:\n            nr, nc = r + dr, c + dc\n            if 0 <= nr < n and 0 <= nc < n and grid[nr][nc] != '#':\n                if fire_time[nr][nc] == INF:\n                    fire_time[nr][nc] = fire_time[r][c] + 1\n                    q_fire.append((nr, nc))\n\n    # ------------------------------------------------------------------\n    # 2.  Basic impossibility checks\n    # ------------------------------------------------------------------\n    def burning_at_start(cell_r, cell_c) -> bool:\n        \"\"\"True if that cell is already burning at t = 0.\"\"\"\n        return fire_time[cell_r][cell_c] == 0\n\n    # walls, or burning instantly\n    if (\n        grid[0][0] == '#' or grid[n - 1][n - 1] == '#' or\n        burning_at_start(0, 0) or burning_at_start(n - 1, n - 1)\n    ):\n        return -1\n\n    # 1\u00d71 maze that is free and not burning\n    if n == 1:\n        return 0\n\n    # ------------------------------------------------------------------\n    # 3.  BFS for the agent, respecting fire-arrival times\n    # ------------------------------------------------------------------\n    q = deque([(0, 0, 0)])               # (row, col, time-of-arrival)\n    best_arrival = [[INF] * n for _ in range(n)]\n    best_arrival[0][0] = 0\n\n    while q:\n        r, c, t = q.popleft()\n\n        for dr, dc in dirs:\n            nr, nc = r + dr, c + dc\n            if 0 <= nr < n and 0 <= nc < n and grid[nr][nc] != '#':\n                next_t = t + 1\n\n                # Fire spreads FIRST, then we move \u2192 cell must start burning\n                # strictly AFTER the agent arrives.\n                if next_t >= fire_time[nr][nc]:\n                    continue\n\n                if next_t < best_arrival[nr][nc]:\n                    best_arrival[nr][nc] = next_t\n                    if nr == n - 1 and nc == n - 1:       # reached goal\n                        return next_t\n                    q.append((nr, nc, next_t))\n\n    return -1\n```\n\nExplanation of the algorithm\n----------------------------\n1. Fire time table  \n   \u2022 Treat every \u2018F\u2019 cell as a source at minute 0.  \n   \u2022 Multi-source BFS (four directions, ignoring walls `#`) gives `fire_time[r][c]` = the earliest minute that cell starts burning (`\u221e` if it never burns).\n\n2. Early rejection  \n   \u2022 If start or goal is a wall or already burning at \\(t=0\\) \u2192 return -1.  \n   \u2022 For a \\(1\\times1\\) clear, safe cell the answer is 0.\n\n3. Agent BFS  \n   \u2022 Queue stores `(row, col, minute-of-arrival)`.  \n   \u2022 From a position reached at time `t` the agent can enter a neighbour at `t+1`.  \n   \u2022 Because the fire spreads first each minute, the neighbour must satisfy  \n     `t + 1  <  fire_time[neighbour]`.  \n   \u2022 Standard BFS (shortest paths in an un-weighted graph) with this extra\n     constraint yields the minimum travel time.  \n   \u2022 If the goal is dequeued, that time is the answer; if the queue exhausts,\n     the goal is unreachable \u2192 return -1.\n\nComplexities  \n\u2022 Fire BFS: \\(O(n^2)\\).  \n\u2022 Agent BFS: \\(O(n^2)\\).  \n\u2022 Memory: \\(O(n^2)\\)."}
{"task_id": 141, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"\n    k\u2013Nearest Neighbours classifier.\n    \n    Parameters\n    ----------\n    X_train : (n_train, n_features) ndarray\n        Training samples.\n    y_train : (n_train,) ndarray\n        Labels of the training samples.\n    X_test  : (n_test,  n_features) ndarray\n        Samples to classify.\n    k       : int\n        Number of neighbours to use (if k > n_train, k == n_train).\n    metric  : {'euclidean', 'manhattan', 'cosine'}, optional\n        Distance metric. Anything else falls back to 'euclidean'.\n    \n    Returns\n    -------\n    preds : (n_test,) ndarray\n        Predicted labels for X_test.\n    \"\"\"\n    if k <= 0:\n        raise ValueError(\"k must be a positive integer.\")\n\n    # ------------------------------------------------------------------\n    # Distance matrix D  shape: (n_test, n_train)\n    # ------------------------------------------------------------------\n    metric = metric.lower()\n    n_train = X_train.shape[0]\n    k = min(k, n_train)\n\n    if metric == 'manhattan':\n        # \u21131 distance\n        D = np.abs(X_test[:, None, :] - X_train[None, :, :]).sum(axis=2)\n\n    elif metric == 'cosine':\n        # cosine distance = 1 - cosine similarity\n        eps = 1e-10                                  # avoid division by zero\n        t_norm = np.linalg.norm(X_test,  axis=1, keepdims=True) + eps\n        r_norm = np.linalg.norm(X_train, axis=1)     + eps          # (n_train,)\n        sim = (X_test @ X_train.T) / (t_norm * r_norm)              # broadcasting\n        D = 1.0 - sim\n\n    else:  # default / 'euclidean'\n        diff = X_test[:, None, :] - X_train[None, :, :]\n        D = np.sum(diff ** 2, axis=2)                # squared \u21132 is enough for ranking\n\n    # ------------------------------------------------------------------\n    # k nearest neighbours' indices for every test point\n    # ------------------------------------------------------------------\n    nn_idx = np.argpartition(D, kth=k-1, axis=1)[:, :k]  # (n_test, k)\n\n    # ------------------------------------------------------------------\n    # Majority vote\n    # ------------------------------------------------------------------\n    preds = np.empty(X_test.shape[0], dtype=y_train.dtype)\n\n    for i, neighbours in enumerate(nn_idx):\n        labels = y_train[neighbours]\n        uniq, counts = np.unique(labels, return_counts=True)\n        majority = uniq[counts == counts.max()]      # handle ties\n        preds[i] = majority.min()                    # smallest label wins in a tie\n\n    return preds\n```"}
{"task_id": 146, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\"\"\"\n    \n    if X.ndim != 2 or X_test.ndim != 2:\n        raise ValueError('X and X_test must be 2-D arrays')\n    if len(X) != len(y):\n        raise ValueError('X and y must contain the same number of samples')\n    if k <= 0:\n        raise ValueError('k must be a positive integer')\n    \n    # ------------------------------------------------------------------ #\n    # 1. distance matrix: shape = (n_test, n_train)                       #\n    # ------------------------------------------------------------------ #\n    metric = metric.lower()\n    n_train = X.shape[0]\n    n_test  = X_test.shape[0]\n    k = min(k, n_train)                       # use all samples if k > n_train\n    \n    if metric == 'euclidean':\n        # ||x - x\u0302||\u2082\n        diff   = X_test[:, None, :] - X[None, :, :]      # (n_test, n_train, n_feat)\n        dists  = np.linalg.norm(diff, axis=2)            # (n_test, n_train)\n        \n    elif metric == 'manhattan':\n        # \u2211 |x - x\u0302|\n        diff   = X_test[:, None, :] - X[None, :, :]\n        dists  = np.abs(diff).sum(axis=2)\n        \n    elif metric == 'cosine':\n        # 1 - (x\u00b7x\u0302)/(\u2016x\u2016\u2016x\u0302\u2016)\n        eps      = 1e-12\n        x_norm   = np.linalg.norm(X,      axis=1) + eps  # (n_train,)\n        xt_norm  = np.linalg.norm(X_test, axis=1) + eps  # (n_test,)\n        dot_prod = X_test @ X.T                          # (n_test, n_train)\n        denom    = np.outer(xt_norm, x_norm)             # (n_test, n_train)\n        dists    = 1.0 - dot_prod / denom\n    else:\n        raise ValueError(\"metric must be 'euclidean', 'manhattan', or 'cosine'\")\n    \n    # ------------------------------------------------------------------ #\n    # 2. choose k nearest, 3. majority vote with smallest-label tie break #\n    # ------------------------------------------------------------------ #\n    predictions = []\n    for i in range(n_test):\n        # indices of k smallest distances\n        idx     = np.argpartition(dists[i], k-1)[:k]     # O(n)\n        neigh_y = y[idx]\n        \n        # count occurrences\n        unique, counts = np.unique(neigh_y, return_counts=True)\n        max_count      = counts.max()\n        \n        # labels with the highest vote\n        winners = unique[counts == max_count]\n        pred    = winners.min()          # smallest label in case of tie\n        predictions.append(pred)\n    \n    # ------------------------------------------------------------------ #\n    return predictions\n```\n\nThe function\n\n1. Computes all pairwise distances between test and training samples using the chosen metric (Euclidean, Manhattan or Cosine; the latter is stabilised with \u03b5 = 1 \u00d7 10\u207b\u00b9\u00b2).\n2. For every test point selects the `k` nearest neighbours (or all training samples if `k` is larger).\n3. Performs a majority vote on neighbour labels; if several labels share the highest count, the smallest label is chosen.\n4. Returns the predictions as a plain Python list."}
{"task_id": 155, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np      # keeps the two-import header exactly as requested\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Forward pass of a two-hidden-layer actor network (Linear \u2192 ReLU \u2192\n    Linear \u2192 ReLU \u2192 Linear \u2192 tanh).\n\n    Args:\n        state (list[float] | np.ndarray): 1-D state vector.\n        weights (dict): Dictionary containing the NumPy arrays\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector, each element rounded to four decimals.\n    \"\"\"\n    # Ensure state is a 1-D NumPy array of floats\n    x = np.asarray(state, dtype=float)\n\n    # Unpack weights and biases\n    W1, b1 = weights[\"W1\"], weights[\"b1\"]\n    W2, b2 = weights[\"W2\"], weights[\"b2\"]\n    W3, b3 = weights[\"W3\"], weights[\"b3\"]\n\n    # Layer 1: Linear \u2192 ReLU\n    z1 = x @ W1 + b1\n    a1 = np.maximum(0, z1)\n\n    # Layer 2: Linear \u2192 ReLU\n    z2 = a1 @ W2 + b2\n    a2 = np.maximum(0, z2)\n\n    # Output layer: Linear \u2192 tanh\n    z3 = a2 @ W3 + b3\n    actions = np.tanh(z3)\n\n    # Round to four decimals and convert to Python list\n    actions_rounded = np.round(actions, 4)\n    if actions_rounded.shape == ():                       # scalar action\n        return [float(actions_rounded)]\n    return actions_rounded.tolist()\n```"}
{"task_id": 160, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef _soft_threshold(a: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S(a, \u03c4).\"\"\"\n    if a >  tau:\n        return a - tau\n    if a < -tau:\n        return a + tau\n    return 0.0\n\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Lasso (L1) linear regression fitted with coordinate descent.\"\"\"\n    # ---- basic shapes -------------------------------------------------------\n    X  = np.asarray(X, dtype=float)\n    y  = np.asarray(y, dtype=float).ravel()\n    m, n = X.shape                           # m samples, n original features\n\n    # ---- add intercept column (never penalised) ----------------------------\n    intercept_col = np.ones((m, 1)) if fit_intercept else np.zeros((m, 1))\n    XA = np.hstack((intercept_col, X))       # (m , n + 1)\n    p  = n + 1                               # total number of coordinates\n\n    # ---- parameter initialisation ------------------------------------------\n    w = np.zeros(p)                          # first entry is the intercept\n    col_sq_norms = (XA ** 2).sum(axis=0)     # \u2016x_j\u2016\u00b2, used in closed form\n\n    # ---- outer optimisation loop -------------------------------------------\n    for _ in range(max_iters):\n\n        # (re-)estimate intercept as the mean residual\n        if fit_intercept:\n            y_hat_wo_b = XA[:, 1:].dot(w[1:])\n            w[0] = (y - y_hat_wo_b).mean()\n\n        # ---- coordinate updates (excluding intercept) ----------------------\n        for j in range(1, p):\n            x_j = XA[:, j]\n\n            # partial residual with w_j temporarily set to 0\n            r_j = y - XA.dot(w) + w[j] * x_j\n\n            rho = x_j.dot(r_j)               # \u27e8x_j , r_j\u27e9\n            w[j] = _soft_threshold(rho, lambda_param * m) / col_sq_norms[j]\n\n    # ---- prepare rounded output --------------------------------------------\n    weights = np.round(w[1:], 4).tolist()    # exclude intercept\n    bias    = round(float(w[0]), 4)\n\n    return weights, bias\n```"}
{"task_id": 165, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\",\n                     y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length vectors.\n\n    Args:\n        x: 1-D list or NumPy array of integers.\n        y: 1-D list or NumPy array of integers.\n\n    Returns:\n        Float rounded to 4 decimal places representing the distance, or \u20111\n        if the vectors have different lengths.\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorized comparison\n    x_arr = np.asarray(x)\n    y_arr = np.asarray(y)\n\n    # Ensure vectors are 1-D\n    if x_arr.ndim != 1 or y_arr.ndim != 1:\n        raise ValueError(\"Input vectors must be 1-D.\")\n\n    # Length check\n    if x_arr.size != y_arr.size:\n        return -1\n\n    N = x_arr.size\n    if N == 0:        # Both vectors are empty \u2192 distance conventionally 0\n        return 0.0\n\n    # Count differing positions and normalize\n    diff = np.count_nonzero(x_arr != y_arr)\n    distance = diff / N\n\n    return round(float(distance), 4)\n```"}
{"task_id": 169, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef generate_window(\n    window: str,\n    N: int,\n    coefficients: list[float] | None = None\n) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Window name: \"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate (must be > 0).\n    coefficients : list[float] | None, optional\n        Cosine-series coefficients a\u2080\u2026a_M, required only when\n        window == \"generalized_cosine\".\n\n    Returns\n    -------\n    list[float]\n        List with N floats rounded to 4 decimal places.\n    \"\"\"\n    # ------------------------------------------------------------------ checks\n    if not isinstance(N, int) or N <= 0:\n        raise ValueError(\"N must be a positive integer.\")\n    if N == 1:                      # conventional definition\n        return [1.0]\n\n    name = window.lower()\n    if name not in {\n        \"hamming\",\n        \"hann\",\n        \"blackman_harris\",\n        \"generalized_cosine\",\n    }:\n        raise ValueError(f\"Unknown window type: {window!r}\")\n\n    # ----------------------------------------------------------------- helpers\n    n = np.arange(N)\n    denom = N - 1\n    two_pi_term = 2.0 * np.pi * n / denom   # 2\u03c0n/(N\u22121)\n\n    # ------------------------------------------------------------- window core\n    if name == \"hamming\":\n        w = 0.54 - 0.46 * np.cos(two_pi_term)\n\n    elif name == \"hann\":\n        w = 0.5 - 0.5 * np.cos(two_pi_term)\n\n    elif name == \"blackman_harris\":\n        a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n        w = (\n            a0\n            - a1 * np.cos(two_pi_term)\n            + a2 * np.cos(2 * two_pi_term)   # 4\u03c0n/(N\u22121)\n            - a3 * np.cos(3 * two_pi_term)   # 6\u03c0n/(N\u22121)\n        )\n\n    elif name == \"generalized_cosine\":\n        if not coefficients:\n            raise ValueError(\n                \"For 'generalized_cosine' you must supply a non-empty \"\n                \"'coefficients' list.\"\n            )\n        w = np.zeros(N, dtype=float)\n        for k, ak in enumerate(coefficients):\n            w += ak * np.cos(2.0 * np.pi * k * n / denom)\n\n    # --------------------------------------------------------------- rounding\n    return [round(float(v), 4) for v in w]\n```"}
{"task_id": 171, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Binary Logistic-Regression trained with mini-batch Gradient Descent.\"\"\"\n    # --------------- preparation & basic checks ----------------------------\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train)\n\n    uniq = np.unique(y_train)\n    if len(uniq) == 1:  # degenerate case \u2013 only one class present\n        return [int(uniq[0])] * len(X_test)\n\n    if len(uniq) != 2:\n        raise ValueError(\"Exactly two distinct labels are required for training.\")\n\n    # map original labels -> {0,1}\n    #   uniq[0] ==> 0 ,  uniq[1] ==> 1\n    pos_label = uniq[1]\n    y_bin = (y_train == pos_label).astype(float)\n\n    # add intercept term\n    ones_train = np.ones((X_train.shape[0], 1))\n    ones_test = np.ones((X_test.shape[0], 1))\n    X_train = np.hstack([ones_train, X_train])\n    X_test_aug = np.hstack([ones_test, X_test])\n\n    n_samples, n_features = X_train.shape\n    w = np.zeros(n_features)  # parameter vector\n\n    def sigmoid(z):\n        z = np.clip(z, -500, 500)  # numerical stability\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # -------------------- mini-batch gradient descent ----------------------\n    for _ in range(epochs):\n        permutation = np.random.permutation(n_samples)\n        for start in range(0, n_samples, batch_size):\n            idx = permutation[start : start + batch_size]\n            X_b = X_train[idx]\n            y_b = y_bin[idx]\n\n            preds = sigmoid(X_b @ w)\n            # gradient of NLL wrt w\n            grad = X_b.T @ (preds - y_b) / len(idx)\n            w -= learning_rate * grad\n\n    # ------------------------------ predict --------------------------------\n    probs = sigmoid(X_test_aug @ w)\n    preds_bin = (probs >= 0.5).astype(int)\n\n    # map back to original labels\n    label_map = {0: int(uniq[0]), 1: int(uniq[1])}\n    preds_original = [label_map[int(p)] for p in preds_bin]\n\n    return preds_original\n```"}
{"task_id": 176, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    \"\"\"AdaBoost (SAMME-R) with decision stumps, implemented from scratch.\n\n    Parameters\n    ----------\n    X_train : list of list of float\n        Training features.\n    y_train : list of int (0/1)\n        Training labels, 0 = negative class, 1 = positive class.\n    X_test  : list of list of float\n        Samples whose labels must be predicted.\n    n_estimators : int, default=10\n        Maximum number of boosting rounds.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (0/1) for every row of *X_test*.\n    \"\"\"\n\n    # ------------------------------------------------------------------ helpers\n    def train_best_stump(X: np.ndarray,\n                         y: np.ndarray,\n                         w: np.ndarray):\n        \"\"\"\n        Exhaustively search the decision-stump with the smallest\n        *weighted* classification error.  Tie-breaking is deterministic:\n            1) smallest error\n            2) smallest feature index\n            3) smallest threshold\n            4) polarity  +1 before \u20131\n        \"\"\"\n        n_samples, n_features = X.shape\n        best = {\n            'feature': None,\n            'threshold': None,\n            'polarity': None,\n            'pred': None,\n            'error': np.inf\n        }\n\n        for j in range(n_features):                                #  tie 1\n            column = X[:, j]\n            thresholds = np.unique(column)                         #  tie 2\n            for thr in thresholds:                                 #  tie 3\n                for pol in (1, -1):                                #  tie 4\n                    if pol == 1:\n                        h = np.where(column < thr, 1, -1)\n                    else:  # pol == -1\n                        h = np.where(column >= thr, 1, -1)\n\n                    err = np.sum(w * (h != y))\n                    if err < best['error'] - 1e-12:                #  strict improvement\n                        best.update(feature=j,\n                                    threshold=float(thr),\n                                    polarity=pol,\n                                    pred=h,\n                                    error=err)\n        return best\n\n    def stump_predict(stump, X):\n        \"\"\"Vectorised prediction of a single stump on data matrix X.\"\"\"\n        col = X[:, stump['feature']]\n        if stump['polarity'] == 1:\n            return np.where(col < stump['threshold'], 1, -1)\n        else:\n            return np.where(col >= stump['threshold'], 1, -1)\n\n    # ----------------------------------------------------------- preparation\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=int)\n    y = 2 * y - 1        # {0,1}  ->  {-1, +1}\n\n    n_samples = X.shape[0]\n    # Initial uniform weights\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    stumps: list[dict] = []\n    alphas: list[float] = []\n\n    # -------------------------------------------------------------- boosting\n    for _ in range(n_estimators):\n        stump = train_best_stump(X, y, w)\n        error = stump['error']\n\n        # If perfect stump found, keep it and stop early\n        if error <= 1e-12:\n            alpha = 0.5 * np.log((1.0 - 1e-12) / (1e-12))\n            stumps.append(stump)\n            alphas.append(alpha)\n            break\n\n        # Compute the learner\u2019s vote weight\n        alpha = 0.5 * np.log((1.0 - error) / (error + 1e-10))\n\n        # Update sample weights\n        w *= np.exp(-alpha * y * stump['pred'])\n        w /= np.sum(w)          # normalise\n\n        stumps.append(stump)\n        alphas.append(alpha)\n\n    # ------------------------------------------------------------- inference\n    X_te = np.asarray(X_test, dtype=float)\n    agg = np.zeros(X_te.shape[0], dtype=float)\n\n    for stump, alpha in zip(stumps, alphas):\n        agg += alpha * stump_predict(stump, X_te)\n\n    # sign(agg)  ->  {-1,1} ; map back to {0,1}\n    preds = (agg >= 0).astype(int).tolist()\n    return preds\n```"}
{"task_id": 178, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function with Particle Swarm Optimisation.\n\n    Args:\n        n_dims:            number of decision variables  (>0)\n        num_particles:     swarm size                   (>0)\n        num_iterations:    optimisation iterations      (>0)\n        seed:              RNG seed (default 1)\n\n    Returns\n        Best objective value found, rounded to 4 decimals, or \u20131 on bad input.\n    \"\"\"\n    # ----------------- sanity checks -----------------\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n\n    rng = np.random.default_rng(seed)\n\n    # PSO hyper-parameters\n    w, c1, c2 = 0.5, 1.5, 1.5           # inertia, cognitive & social weights\n    lo, hi     = -1.0, 1.0              # search\u2010space bounds\n\n    # ----------------- initial swarm -----------------\n    positions   = rng.uniform(lo, hi, size=(num_particles, n_dims))\n    velocities  = rng.uniform(lo, hi, size=(num_particles, n_dims))\n\n    pbest_pos   = positions.copy()                      # personal best positions\n    pbest_val   = np.sum(pbest_pos ** 2, axis=1)        # personal best values\n\n    gbest_idx   = np.argmin(pbest_val)                  # global best index\n    gbest_pos   = pbest_pos[gbest_idx].copy()\n    gbest_val   = pbest_val[gbest_idx]\n\n    # ----------------- optimisation loop -------------\n    for _ in range(num_iterations):\n        # random coefficients for every particle & dimension\n        r1 = rng.random(size=(num_particles, n_dims))\n        r2 = rng.random(size=(num_particles, n_dims))\n\n        # velocity & position updates\n        velocities = (w * velocities\n                      + c1 * r1 * (pbest_pos - positions)\n                      + c2 * r2 * (gbest_pos - positions))\n        positions  = np.clip(positions + velocities, lo, hi)\n\n        # evaluate objective\n        curr_val = np.sum(positions ** 2, axis=1)\n\n        # update personal bests\n        improved         = curr_val < pbest_val\n        pbest_pos[improved] = positions[improved]\n        pbest_val[improved] = curr_val[improved]\n\n        # update global best\n        gbest_idx = np.argmin(pbest_val)\n        gbest_pos = pbest_pos[gbest_idx].copy()\n        gbest_val = pbest_val[gbest_idx]\n\n    # ----------------- result ------------------------\n    return round(float(gbest_val), 4)\n```"}
{"task_id": 180, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\"\"\"\n    # Convert to numpy arrays for convenience\n    X_tr = np.asarray(X_train, dtype=float)\n    X_te = np.asarray(X_test,  dtype=float)\n    y_tr = np.asarray(y_train, dtype=object)   # keep objects (numbers or strings)\n\n    n_train = X_tr.shape[0]\n\n    # Guard-clauses / normalise k\n    if k <= 0 or k > n_train:\n        k = n_train\n\n    predictions: list = []\n\n    for x in X_te:\n        # Euclidean distances to every training sample\n        dists = np.linalg.norm(X_tr - x, axis=1)\n\n        # Indices of the k closest points\n        if k == n_train:\n            nn_idx = np.arange(n_train)\n        else:\n            nn_idx = np.argpartition(dists, k - 1)[:k]\n\n        neighbours = y_tr[nn_idx]\n\n        if task == \"regression\":\n            # Numeric mean rounded to 4 decimals\n            mean_val = float(np.mean(neighbours.astype(float)))\n            predictions.append(round(mean_val, 4))\n        else:  # classification (default)\n            counts = Counter(neighbours)\n            max_cnt = max(counts.values())\n            # smallest label among the ties (Python\u2019s normal ordering)\n            best_label = min(lbl for lbl, cnt in counts.items() if cnt == max_cnt)\n            predictions.append(best_label)\n\n    return predictions\n```"}
{"task_id": 184, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"CART-style binary decision tree (Gini) completely from scratch.\"\"\"\n    # ------------------------------------------------------------------ #\n    # helpers\n    # ------------------------------------------------------------------ #\n    class Node:  # simple container for a tree node\n        __slots__ = (\n            \"is_leaf\",\n            \"prediction\",\n            \"feature\",\n            \"threshold\",\n            \"left\",\n            \"right\",\n        )\n\n        def __init__(\n            self,\n            *,\n            is_leaf: bool,\n            prediction: int | None = None,\n            feature: int | None = None,\n            threshold: float | None = None,\n            left: \"Node | None\" = None,\n            right: \"Node | None\" = None,\n        ):\n            self.is_leaf = is_leaf\n            self.prediction = prediction\n            self.feature = feature\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n\n    y_train = y_train.astype(int, copy=False)\n    n_classes = int(y_train.max()) + 1  # labels are non-negative ints\n\n    def gini(counts: np.ndarray) -> float:\n        \"\"\"Gini impurity from class counts.\"\"\"\n        total = counts.sum()\n        if total == 0:\n            return 0.0\n        probs = counts / total\n        return 1.0 - np.sum(probs**2)\n\n    def majority_class(counts: np.ndarray) -> int:\n        \"\"\"Smallest label in case of ties keeps result deterministic.\"\"\"\n        return int(np.argmax(counts))\n\n    # ------------------------------------------------------------------ #\n    # best split search for a node\n    # ------------------------------------------------------------------ #\n    def find_best_split(indices: np.ndarray) -> tuple[int, float, float, np.ndarray, np.ndarray] | None:\n        \"\"\"\n        Returns (best_feature, best_threshold, best_gain, left_idx, right_idx)\n        or None if no split brings positive gain.\n        \"\"\"\n        X_node = X_train[indices]\n        y_node = y_train[indices]\n        parent_counts = np.bincount(y_node, minlength=n_classes)\n        parent_gini = gini(parent_counts)\n\n        best_gain = 0.0\n        best_feature = -1\n        best_threshold = 0.0\n        best_left_idx = best_right_idx = None\n\n        n_samples, n_features = X_node.shape\n\n        # iterate through every feature\n        for feat in range(n_features):\n            values = X_node[:, feat]\n            order = np.argsort(values)\n            sorted_y = y_node[order]\n            sorted_vals = values[order]\n\n            left_counts = np.zeros(n_classes, dtype=int)\n            right_counts = parent_counts.copy()\n\n            # iterate over all possible split positions\n            for i in range(1, n_samples):\n                cls = sorted_y[i - 1]\n                left_counts[cls] += 1\n                right_counts[cls] -= 1\n\n                # skip if feature value does not change \u2013 cannot split here\n                if sorted_vals[i] == sorted_vals[i - 1]:\n                    continue\n\n                left_n = i\n                right_n = n_samples - i\n                g_left = gini(left_counts)\n                g_right = gini(right_counts)\n                weighted = (left_n * g_left + right_n * g_right) / n_samples\n                gain = parent_gini - weighted\n\n                if gain > best_gain + 1e-12:  # numeric tolerance\n                    best_gain = gain\n                    best_feature = feat\n                    best_threshold = (sorted_vals[i] + sorted_vals[i - 1]) / 2.0\n                    # store actual indices of the split for later recursion\n                    mask = X_train[indices, feat] <= best_threshold\n                    best_left_idx = indices[mask]\n                    best_right_idx = indices[~mask]\n\n        if best_gain <= 0.0 or best_left_idx.size == 0 or best_right_idx.size == 0:\n            return None  # no useful split\n        return best_feature, best_threshold, best_gain, best_left_idx, best_right_idx\n\n    # ------------------------------------------------------------------ #\n    # recursive tree grow\n    # ------------------------------------------------------------------ #\n    def build(indices: np.ndarray, depth: int) -> Node:\n        y_subset = y_train[indices]\n        counts = np.bincount(y_subset, minlength=n_classes)\n\n        # stop-conditions ------------------------------------------------\n        if np.unique(y_subset).size == 1:  # pure node\n            return Node(is_leaf=True, prediction=int(y_subset[0]))\n        if max_depth is not None and depth >= max_depth:\n            return Node(is_leaf=True, prediction=majority_class(counts))\n\n        # search for the best split -------------------------------------\n        split = find_best_split(indices)\n        if split is None:\n            return Node(is_leaf=True, prediction=majority_class(counts))\n\n        feat, thr, _, left_idx, right_idx = split\n        left_child = build(left_idx, depth + 1)\n        right_child = build(right_idx, depth + 1)\n        return Node(\n            is_leaf=False,\n            feature=feat,\n            threshold=thr,\n            left=left_child,\n            right=right_child,\n        )\n\n    # ------------------------------------------------------------------ #\n    # build the tree\n    # ------------------------------------------------------------------ #\n    root = build(np.arange(X_train.shape[0]), depth=0)\n\n    # ------------------------------------------------------------------ #\n    # prediction traversal\n    # ------------------------------------------------------------------ #\n    def predict_single(x: np.ndarray) -> int:\n        node = root\n        while not node.is_leaf:\n            node = node.left if x[node.feature] <= node.threshold else node.right\n        return node.prediction\n\n    return [predict_single(row) for row in X_test]\n```"}
{"task_id": 190, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted\n    Gini impurity for a single binary split.\n\n    Parameters\n    ----------\n    X : list[list[float]] | np.ndarray  (n_samples \u00d7 n_features)\n    y : list[int] | np.ndarray          (n_samples,)\n\n    Returns\n    -------\n    (int, float | None, float)\n        (best_feature_index, best_threshold_value, best_gini_rounded)\n    \"\"\"\n    # ---------- Normalise the inputs ----------\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    if X.ndim != 2:\n        raise ValueError(\"X must be 2-dimensional\")\n    if y.ndim != 1 or y.shape[0] != X.shape[0]:\n        raise ValueError(\"y must be 1-D and have the same length as X\")\n    n_samples, n_features = X.shape\n    if n_samples == 0 or n_features == 0:          # empty data set\n        return (-1, None, 0.0)\n\n    # ---------- Encode class labels as 0 \u2026 C-1 ----------\n    classes, y_enc = np.unique(y, return_inverse=True)\n    n_classes = classes.size\n\n    # ---------- Helper : Gini of a node with counts  ----------\n    def gini(counts, n):\n        if n == 0:\n            return 0.0\n        p = counts / n\n        return 1.0 - np.sum(p * p)\n\n    # ---------- Gini of the whole data set ----------\n    total_counts = np.bincount(y_enc, minlength=n_classes)\n    g_whole = gini(total_counts, n_samples)\n\n    # ---------- Initialise \u201cbest\u201d record ----------\n    best_gini     = np.inf\n    best_feat     = -1\n    best_th       = None\n    tol           = 1e-12      # for impurity equality test\n\n    # ---------- Examine every feature ----------\n    for j in range(n_features):\n        col = X[:, j]\n        if np.all(col == col[0]):        # constant \u21d2 no valid split\n            continue\n\n        # sort by the feature\n        order = np.argsort(col, kind='mergesort')   # stable for tie\u2010breaks\n        col_sorted = col[order]\n        y_sorted   = y_enc[order]\n\n        # cumulative class counts for the \u201cleft\u201d partition\n        left_counts = np.zeros(n_classes, dtype=int)\n\n        # iterate through the sorted samples\n        for i in range(n_samples):\n            cls = y_sorted[i]\n            left_counts[cls] += 1\n\n            # Only evaluate a split at the *last* occurrence of a value.\n            is_last_occurrence = (i == n_samples - 1) or (col_sorted[i] != col_sorted[i + 1])\n            if not is_last_occurrence:\n                continue\n\n            n_left  = i + 1\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:        # should never happen here\n                continue\n\n            right_counts = total_counts - left_counts\n\n            g_left  = gini(left_counts,  n_left)\n            g_right = gini(right_counts, n_right)\n\n            g_split = (n_left  / n_samples) * g_left \\\n                    + (n_right / n_samples) * g_right\n\n            # ---------- Update the best split if necessary ----------\n            if g_split + tol < best_gini:\n                best_gini, best_feat, best_th = g_split, j, col_sorted[i]\n            elif abs(g_split - best_gini) < tol:\n                if j < best_feat:\n                    best_feat, best_th = j, col_sorted[i]\n                elif j == best_feat and col_sorted[i] < best_th:\n                    best_th = col_sorted[i]\n\n    # ---------- No valid split? ----------\n    if best_feat == -1:\n        return (-1, None, round(g_whole, 4))\n\n    return (best_feat, best_th, round(best_gini, 4))\n```"}
{"task_id": 191, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np          # (kept twice as in the template)\n\ndef metric(name: str):\n    \"\"\"Return a distance function with integrated validation.\"\"\"\n    \n    supported = ('euclidean', 'manhattan', 'chebyshev', 'cosine')\n\n    def d(v1, v2):\n        # ------------------------------------------------------------------ #\n        # 1.  Metric name must be supported\n        # ------------------------------------------------------------------ #\n        if name not in supported:\n            return -1\n\n        # ------------------------------------------------------------------ #\n        # 2.  Type check \u2013 only list, tuple, np.ndarray are allowed\n        # ------------------------------------------------------------------ #\n        if not isinstance(v1, (list, tuple, np.ndarray)):\n            return -1\n        if not isinstance(v2, (list, tuple, np.ndarray)):\n            return -1\n\n        # ------------------------------------------------------------------ #\n        # 3.  Convert to numpy 1-D float arrays; catch non-numerical failures\n        # ------------------------------------------------------------------ #\n        try:\n            a = np.asarray(v1, dtype=float)\n            b = np.asarray(v2, dtype=float)\n        except Exception:          # non-convertible element encountered \u2026\n            return -1\n\n        # ------------------------------------------------------------------ #\n        # 4.  Shape, length, emptiness checks\n        # ------------------------------------------------------------------ #\n        if a.ndim != 1 or b.ndim != 1:          # one-dimensional only\n            return -1\n        if a.size == 0 or b.size == 0:          # non-empty\n            return -1\n        if a.size != b.size:                    # equal length\n            return -1\n\n        # ------------------------------------------------------------------ #\n        # 5.  Metric computations\n        # ------------------------------------------------------------------ #\n        if name == 'euclidean':\n            value = np.linalg.norm(a - b)\n\n        elif name == 'manhattan':\n            value = np.sum(np.abs(a - b))\n\n        elif name == 'chebyshev':\n            value = np.max(np.abs(a - b))\n\n        else:                                   # cosine distance\n            na, nb = np.linalg.norm(a), np.linalg.norm(b)\n            if na == 0 or nb == 0:              # zero-norm vectors forbidden\n                return -1\n            value = 1 - (a @ b) / (na * nb)\n\n        # ------------------------------------------------------------------ #\n        # 6.  Round & return\n        # ------------------------------------------------------------------ #\n        return round(float(value), 4)\n\n    return d\n```"}
{"task_id": 197, "completion_id": 0, "completion": "```python\nimport math\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\nimport numpy as np\n\n\ndef _is_discrete(space: Any) -> bool:\n    \"\"\"Utility: does the sub-space describe a discrete set?\"\"\"\n    return hasattr(space, \"n\")\n\n\ndef _is_continuous(space: Any) -> bool:\n    \"\"\"Utility: does the sub-space describe a continuous set?\"\"\"\n    return hasattr(space, \"shape\")\n\n\ndef action_stats(\n    env: Any,\n    md_action: bool,\n    cont_action: bool,\n) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"\n    Summarise an RL environment's action space.\n\n    Parameters\n    ----------\n    env : Any\n        Environment exposing an `action_space` attribute \u00e0-la OpenAI-Gym.\n    md_action : bool\n        Ignored by the logic (kept for the required signature) \u2013 the function\n        determines the true structure directly from `env.action_space`.\n    cont_action : bool\n        Ignored by the logic (kept for the required signature) \u2013 the function\n        determines the true structure directly from `env.action_space`.\n\n    Returns\n    -------\n    Tuple containing\n        n_actions_per_dim : list[int | float]\n            Number of distinct actions for every dimension\n            (`np.inf` for continuous ones).\n        action_ids : list[tuple[int, ...]] | None\n            All possible discrete actions as tuples; `None` if at least one\n            dimension is continuous.\n        action_dim : int\n            Number of action dimensions.\n    \"\"\"\n    space = env.action_space\n\n    # --------------------------------------------------------------------- #\n    # 1. Collect per-dimension information\n    # --------------------------------------------------------------------- #\n    n_actions_per_dim: List[Union[int, float]] = []\n\n    # a) Composed spaces \u2013 e.g. gym.spaces.Tuple, gym.spaces.Dict -----------\n    if hasattr(space, \"spaces\"):\n        subspaces = space.spaces\n        for sub in subspaces:\n            if _is_discrete(sub):\n                n_actions_per_dim.append(int(sub.n))\n            elif _is_continuous(sub):\n                n_actions_per_dim.append(np.inf)\n            else:           # Fallback: treat as continuous\n                n_actions_per_dim.append(np.inf)\n\n        action_dim = len(subspaces)\n\n    # b) Single-container spaces ------------------------------------------\n    else:\n        if _is_discrete(space):\n            # Purely discrete, single logical dimension\n            n_actions_per_dim.append(int(space.n))\n            action_dim = 1\n\n        elif _is_continuous(space):\n            # A continuous \"vector\": every component is continuous\n            dim = int(space.shape[0]) if len(space.shape) > 0 else 1\n            n_actions_per_dim.extend([np.inf] * dim)\n            action_dim = dim\n\n        else:   # Unknown \u2013 assume one continuous dimension\n            n_actions_per_dim.append(np.inf)\n            action_dim = 1\n\n    # --------------------------------------------------------------------- #\n    # 2. Build the list of discrete actions (cartesian product)\n    # --------------------------------------------------------------------- #\n    contains_continuous = any(np.isinf(x) for x in n_actions_per_dim)\n\n    if contains_continuous:\n        action_ids = None\n    else:\n        ranges = [range(int(n)) for n in n_actions_per_dim]  # type: ignore[arg-type]\n        # Always return tuples \u2013 even for 1-D spaces (tuple length 1)\n        action_ids = [tuple(p) for p in product(*ranges)]\n\n    # --------------------------------------------------------------------- #\n    # 3. Return immutable summary\n    # --------------------------------------------------------------------- #\n    return n_actions_per_dim, action_ids, action_dim\n```"}
{"task_id": 198, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        One array per document. Array of document *d* has shape (N_d, T)\n        and contains the current variational parameter \u03d5 of this document.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the index of the *n*-th token\n        in document *d*.\n    V : int\n        Size of the vocabulary.\n\n    Returns\n    -------\n    list[list[float]]\n        (V \u00d7 T) matrix with the updated word\u2013topic probabilities.\n        Each column sums to one (\u2264 1e-4 rounding error) and every\n        number is rounded to four decimals.\n    \"\"\"\n    if not phi:                       # empty corpus \u21d2 return zeros\n        return [[0.0] * 0 for _ in range(V)]\n\n    T = phi[0].shape[1]               # number of topics\n    counts = np.zeros((V, T), dtype=np.float64)\n\n    # ---------- accumulate the expected counts ------------------------------\n    for doc_phi, doc_words in zip(phi, corpus):\n        # doc_words must be a NumPy array for np.add.at\n        words = np.asarray(doc_words, dtype=np.intp)\n        # add the whole matrix of the document at once\n        np.add.at(counts, words, doc_phi)\n\n    # which words have been observed at least once?\n    seen_words_mask = counts.sum(axis=1) > 0\n\n    # ---------- column normalisation ----------------------------------------\n    col_sums = counts.sum(axis=0)                     # shape (T,)\n    beta = np.zeros_like(counts)\n\n    non_zero_cols = col_sums > 0\n    beta[:, non_zero_cols] = counts[:, non_zero_cols] / col_sums[non_zero_cols]\n\n    # if a complete column is still zero (should be very rare) we distribute\n    # the mass uniformly over the *observed* words so that unseen rows keep 0.\n    zero_cols = ~non_zero_cols\n    if zero_cols.any():\n        observed_idx = np.where(seen_words_mask)[0]\n        if observed_idx.size == 0:            # no word occurs in the corpus\n            beta[:, zero_cols] = 1.0 / V      # fall back to a uniform column\n        else:\n            beta[observed_idx[:, None], zero_cols] = 1.0 / observed_idx.size\n\n    # ---------- round to four decimals & re-adjust tiny drift ----------------\n    beta = np.round(beta, 4)\n\n    # small numerical drift because of rounding \u2013 repair if necessary\n    for t in range(T):\n        diff = 1.0 - beta[:, t].sum()\n        if abs(diff) > 1e-4:\n            # add/subtract the missing mass to the largest (non-zero) entry\n            col = beta[:, t]\n            # pick an index that may actually carry probability mass\n            idx = int(np.argmax(col))\n            adj = np.round(diff, 4)            # keep four-decimal precision\n            new_val = np.round(col[idx] + adj, 4)\n            # guard against negative values due to rare corner cases\n            beta[idx, t] = max(new_val, 0.0)\n\n    return beta.tolist()\n```"}
{"task_id": 202, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _total_cost(dist_matrix: np.ndarray, medoid_idx: list[int]) -> float:\n    \"\"\"\n    Return the sum of distances of every sample to its nearest medoid.\n    \"\"\"\n    # distances to the current medoids -> shape (n_samples, k)\n    d_to_medoids = dist_matrix[:, medoid_idx]\n    # nearest distance for every sample\n    return np.min(d_to_medoids, axis=1).sum()\n\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Input data.\n    k : int\n        Number of desired clusters (1 \u2264 k \u2264 n_samples).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels (0 \u2026 k-1) for every sample.\n    \"\"\"\n    # ----------------------------- Checks ---------------------------------- #\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D NumPy array.\")\n    n_samples = X.shape[0]\n    if not (1 <= k <= n_samples):\n        raise ValueError(\"k must satisfy 1 \u2264 k \u2264 n_samples\")\n\n    # ---------------------- Pre-compute distances -------------------------- #\n    # Pair-wise Euclidean distance matrix (symmetric, dtype=float64)\n    # D[i, j] = ||X[i] \u2013 X[j]||\n    diff = X[:, None, :] - X[None, :, :]\n    dist_matrix = np.linalg.norm(diff, axis=-1)\n\n    # ------------------------ Initial medoids ----------------------------- #\n    medoid_idx: list[int] = list(range(k))          # indices of current medoids\n    current_cost = _total_cost(dist_matrix, medoid_idx)\n\n    # ----------------------- Main optimisation loop ----------------------- #\n    improved = True\n    while improved:\n        best_swap = None\n        best_reduction = 0.0\n\n        non_medoid_idx = [i for i in range(n_samples) if i not in medoid_idx]\n\n        # Try every single (medoid \u2194 non-medoid) swap\n        for p_pos, p in enumerate(medoid_idx):          # position & index of medoid to replace\n            for h in non_medoid_idx:                    # candidate that may become a medoid\n                # Build the candidate medoid list: keep order, just replace p by h\n                candidate_medoid_idx = medoid_idx.copy()\n                candidate_medoid_idx[p_pos] = h\n\n                # Total cost with the candidate swap\n                cand_cost = _total_cost(dist_matrix, candidate_medoid_idx)\n                reduction = current_cost - cand_cost\n\n                # Keep the swap with the largest strictly positive reduction.\n                if reduction > best_reduction:           # \u201c>\u201d  \u2192 strictly lower cost\n                    best_reduction = reduction\n                    best_swap = (p_pos, h)               # remember position & new index\n\n        # Apply the best swap found in this iteration (if any)\n        if best_swap is not None:                        # some improvement exists\n            p_pos, h = best_swap\n            medoid_idx[p_pos] = h\n            current_cost -= best_reduction              # new lower cost\n        else:\n            improved = False                            # no further improvement possible\n\n    # ---------------------------- Labelling -------------------------------- #\n    # (n_samples, k) distances from every sample to every final medoid\n    final_dists = dist_matrix[:, medoid_idx]\n    # index (0..k-1) of the closest medoid for every sample;\n    # NumPy\u2019s argmin returns the first occurrence when ties happen \u2192 required rule.\n    labels = final_dists.argmin(axis=1).tolist()\n\n    return labels\n```"}
{"task_id": 216, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, \n                            n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic-Net linear regression trained with batch gradient descent.\"\"\"\n    \n    # --------------- basic validation & conversion ----------------\n    try:\n        X_tr = np.asarray(x_train, dtype=float)\n        X_te = np.asarray(x_test,  dtype=float)\n        y_tr = np.asarray(y_train, dtype=float).ravel()\n    except Exception:          # non-numeric data etc.\n        return -1\n    \n    if X_tr.ndim == 1:   # allow 1-D feature vectors\n        X_tr = X_tr.reshape(-1, 1)\n    if X_te.ndim == 1:\n        X_te = X_te.reshape(-1, 1)\n        \n    if X_tr.shape[1] != X_te.shape[1] or n_iterations < 1:\n        return -1\n    \n    # --------------- polynomial expansion -------------------------\n    def poly_expansion(matrix, deg):\n        if deg <= 1:\n            return matrix.copy()\n        powers = [matrix ** p for p in range(1, deg + 1)]\n        return np.concatenate(powers, axis=1)\n    \n    X_tr_poly = poly_expansion(X_tr, degree)\n    X_te_poly = poly_expansion(X_te, degree)\n    \n    # --------------- standardisation (zero mean / unit var) -------\n    mu = X_tr_poly.mean(axis=0)\n    sigma = X_tr_poly.std(axis=0)\n    sigma[sigma == 0] = 1.0          # avoid division by zero\n    \n    X_tr_std = (X_tr_poly - mu) / sigma\n    X_te_std = (X_te_poly - mu) / sigma     # use train statistics!\n    \n    # --------------- add bias column ------------------------------\n    X_tr_std = np.hstack((np.ones((X_tr_std.shape[0], 1)), X_tr_std))\n    X_te_std = np.hstack((np.ones((X_te_std.shape[0], 1)), X_te_std))\n    \n    # --------------- gradient-descent training --------------------\n    n_samples, n_features = X_tr_std.shape\n    w = np.zeros(n_features)                 # weight vector (bias is w[0])\n    \n    l2_factor = reg_factor * (1.0 - l1_ratio)\n    l1_factor = reg_factor * l1_ratio\n    \n    for _ in range(int(n_iterations)):\n        y_hat   = X_tr_std @ w\n        error   = y_hat - y_tr\n        \n        # Gradient of mean-squared error part (1/n \u03a3 (y\u0302\u2212y)x )\n        grad = (X_tr_std.T @ error) / n_samples\n        \n        # ---- Elastic-Net penalties (do NOT regularise bias) -------\n        # L2 part\n        grad[1:] += l2_factor * w[1:]\n        # L1 part  (sub-gradient sign(w), sign(0)=0)\n        grad[1:] += l1_factor * np.sign(w[1:])\n        \n        # Parameter update\n        w -= learning_rate * grad\n    \n    # --------------- prediction on unseen data --------------------\n    y_pred = X_te_std @ w\n    return np.round(y_pred, 4).tolist()\n```"}
{"task_id": 217, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    Args:\n        actual    : 1-D container (list/tuple/np.ndarray) with elements +1 or \u22121.\n        predicted : 1-D container (list/tuple/np.ndarray) with raw model scores.\n\n    Returns:\n        (grad_list, hess_list, prob_list) \u2013 three ordinary Python lists,\n        each value rounded to exactly 6 decimal places.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    y = np.asarray(actual,    dtype=float)\n    z = np.asarray(predicted, dtype=float)\n\n    if y.shape != z.shape:\n        raise ValueError(\"`actual` and `predicted` must have the same length.\")\n\n    # ------------------------------------------------------------------ #\n    # Helper: numerically stable sigmoid\n    # ------------------------------------------------------------------ #\n    def _sigmoid(x):\n        out = np.empty_like(x, dtype=float)\n\n        pos_mask = x >= 0\n        neg_mask = ~pos_mask\n\n        # For non-negative x: 1 / (1 + exp(\u2212x))\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n\n        # For negative x use: exp(x) / (1 + exp(x)) to avoid overflow\n        exp_x = np.exp(x[neg_mask])\n        out[neg_mask] = exp_x / (1.0 + exp_x)\n\n        return out\n\n    # ------------------------------------------------------------------ #\n    # Core computations\n    # ------------------------------------------------------------------ #\n    prob  = _sigmoid(z)               # sigmoid(predicted)\n    grad  = y * _sigmoid(-y * z)      # actual * sigmoid(\u2212actual * predicted)\n    hess  = prob * (1.0 - prob)       # sigmoid * (1 \u2212 sigmoid)\n\n    # ------------------------------------------------------------------ #\n    # Rounding & conversion back to plain Python lists\n    # ------------------------------------------------------------------ #\n    prob  = np.round(prob,  6).tolist()\n    grad  = np.round(grad,  6).tolist()\n    hess  = np.round(hess,  6).tolist()\n\n    return grad, hess, prob\n```"}
{"task_id": 218, "completion_id": 0, "completion": "```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n\n    # ---------- helper functions ---------- #\n    def hand_score(hand: list[int]) -> int:\n        \"\"\"Return the best score (\u226421 if possible) for this hand.\"\"\"\n        s = sum(hand)\n        # If there is at least one Ace and we can add 10 without busting,\n        # one Ace counts as 11 instead of 1 (usable ace rule).\n        if 1 in hand and s + 10 <= 21:\n            return s + 10\n        return s\n\n    def is_bust(hand: list[int]) -> bool:\n        \"\"\"True if the hand is bust (>21).\"\"\"\n        return hand_score(hand) > 21\n\n    def is_natural(hand: list[int]) -> bool:\n        \"\"\"True if the hand is a natural blackjack (exactly two cards: Ace + 10).\"\"\"\n        return len(hand) == 2 and 1 in hand and 10 in hand\n\n    # ---------- evaluate hands ---------- #\n    player_score = hand_score(player)\n    dealer_score = hand_score(dealer)\n\n    player_bust = player_score > 21\n    dealer_bust = dealer_score > 21\n    player_natural = is_natural(player)\n\n    # ---------- apply reward rules ---------- #\n    # 1. Player busts\n    if player_bust:\n        return -1.0\n\n    # 2. Dealer busts\n    if dealer_bust:\n        return 1.5 if player_natural else 1.0\n\n    # 3. Neither busts \u2013 compare scores\n    if player_score > dealer_score:           # Player wins\n        return 1.5 if player_natural else 1.0\n    if player_score < dealer_score:           # Dealer wins\n        return -1.0\n\n    # 4. Draw\n    return 0.0\n```"}
{"task_id": 221, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef squared_log_error(actual, predicted):\n    \"\"\"\n    Element-wise squared logarithmic error.\n\n    Parameters\n    ----------\n    actual, predicted : np.ndarray\n        Arrays of the same shape containing non-negative numbers.\n\n    Returns\n    -------\n    np.ndarray\n        (log1p(actual) - log1p(predicted))**2 computed element-wise.\n    \"\"\"\n    return np.square(np.log1p(actual) - np.log1p(predicted))\n\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        True target values (must be non-negative).\n    predicted : list | tuple | np.ndarray\n        Predicted values (must be non-negative and same shape as `actual`).\n\n    Returns\n    -------\n    float\n        MSLE rounded to 4 decimal places, or -1 if the inputs are invalid.\n    \"\"\"\n    # --- Convert to NumPy arrays ------------------------------------------------\n    try:\n        actual_arr = np.asarray(actual, dtype=float)\n        pred_arr   = np.asarray(predicted, dtype=float)\n    except Exception:\n        return -1  # conversion failed (e.g., non-numeric input)\n\n    # --- Validation -------------------------------------------------------------\n    if actual_arr.shape != pred_arr.shape:          # shape / length check\n        return -1\n    if actual_arr.size == 0:                        # empty input\n        return -1\n    if np.isnan(actual_arr).any() or np.isnan(pred_arr).any():\n        return -1                                   # NaNs are invalid\n    if np.any(actual_arr < 0) or np.any(pred_arr < 0):\n        return -1                                   # negatives not allowed\n\n    # --- MSLE computation -------------------------------------------------------\n    sq_log_err = squared_log_error(actual_arr, pred_arr)\n    msle = float(np.mean(sq_log_err))\n\n    return round(msle, 4)\n```"}
{"task_id": 222, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function works for scalar numbers, Python lists and NumPy arrays and\n    remains numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: If `z` is a scalar, the sigmoid value rounded to 4\n        decimals (float).  Otherwise, a Python list with the same nested\n        structure where every element is rounded to 4 decimals.\n    \"\"\"\n    # ----- scalar case ------------------------------------------------------\n    if np.isscalar(z):\n        z = float(z)\n        if z >= 0:\n            val = 1.0 / (1.0 + np.exp(-z))\n        else:                       # rewrite to avoid overflow when z << 0\n            exp_z = np.exp(z)\n            val  = exp_z / (1.0 + exp_z)\n        return round(val, 4)\n\n    # ----- array-like case ---------------------------------------------------\n    arr = np.asarray(z, dtype=float)\n\n    # allocate output array\n    out = np.empty_like(arr)\n\n    # boolean masks for positive / negative elements\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n\n    # stable computation for z >= 0\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n\n    # stable computation for z < 0 using the alternative form\n    exp_vals = np.exp(arr[neg_mask])\n    out[neg_mask] = exp_vals / (1.0 + exp_vals)\n\n    # round and convert back to a (nested) Python list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 224, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky-ReLU activation to every element in *z*.\n\n    Args:\n        z:  A scalar, (nested) list or ``numpy.ndarray`` of numbers.\n        a:  Slope used for the negative part, a float in the half-open\n            interval [0, 1).  Defaults to 0.01.\n\n    Returns\n    -------\n    Same \u201cshape\u201d as *z* but converted to plain Python objects\n    (i.e. lists / scalars).  For array or list inputs the function\n    returns a (possibly nested) list; for a scalar input it returns a\n    scalar.\n    \"\"\"\n    # Basic validation of `a`\n    if not (0.0 <= a < 1.0):\n        raise ValueError(\"Parameter 'a' must lie in the interval [0, 1).\")\n\n    # Convert the input to a NumPy array (no data copying if `z` is\n    # already an ndarray)\n    arr = np.asarray(z)\n\n    # Vectorised Leaky ReLU: x if x >= 0 else a*x\n    activated = np.where(arr >= 0, arr, a * arr)\n\n    # Convert the NumPy result back to pure Python objects.\n    # `.tolist()` keeps the original nesting and for a 0-d array\n    # returns a plain Python scalar, which satisfies the \u201csame shape\u201d\n    # requirement for scalar inputs.\n    return activated.tolist()\n```"}
{"task_id": 226, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef _stump_predict(column: np.ndarray, threshold: float, polarity: int) -> np.ndarray:\n    \"\"\"\n    Helper \u2013 returns the predictions of one decision stump for a single feature column.\n    \"\"\"\n    if polarity == 1:          # values < threshold  -> -1\n        return np.where(column < threshold, -1, 1)\n    else:                      # polarity == -1: values < threshold ->  1\n        return np.where(column < threshold, 1, -1)\n\n\ndef adaboost_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test:  np.ndarray,\n    n_clf:   int = 5\n) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\"\"\"\n    \n    n_samples, n_features = X_train.shape\n\n    # ------------------------------------------------------------------\n    # Initial (uniform) sample weights\n    # ------------------------------------------------------------------\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    # Each element of `ensemble` is (feature_index, threshold, polarity, alpha)\n    ensemble: list[tuple[int, float, int, float]] = []\n\n    # ------------------------------------------------------------------\n    # Boosting rounds\n    # ------------------------------------------------------------------\n    for _ in range(n_clf):\n        best_feat       = 0\n        best_thresh     = 0.0\n        best_polarity   = 1\n        best_error      = float(\"inf\")\n        best_pred_train = None\n\n        # --------------------------------------------------------------\n        # Search best stump w.r.t. weighted error\n        # --------------------------------------------------------------\n        for feat_idx in range(n_features):\n            col = X_train[:, feat_idx]\n            thresholds = np.unique(col)              # candidate splits\n\n            for thr in thresholds:\n                # polarity  1  :  x < thr -> -1\n                pred = _stump_predict(col, thr, 1)\n                err  = np.sum(w[pred != y_train])\n                if err < best_error:\n                    best_error, best_feat, best_thresh, best_polarity = err, feat_idx, thr, 1\n                    best_pred_train = pred\n\n                # polarity -1 :  x < thr ->  1\n                pred = _stump_predict(col, thr, -1)\n                err  = np.sum(w[pred != y_train])\n                if err < best_error:\n                    best_error, best_feat, best_thresh, best_polarity = err, feat_idx, thr, -1\n                    best_pred_train = pred\n\n        # --------------------------------------------------------------\n        # Compute learner weight (alpha) and update sample weights\n        # --------------------------------------------------------------\n        eps   = 1e-10                          # avoids div-by-zero\n        alpha = 0.5 * math.log((1 - best_error) / (best_error + eps))\n\n        # w_i <- w_i * exp(-alpha y_i h_i(x_i))\n        w *= np.exp(-alpha * y_train * best_pred_train)\n        w /= np.sum(w)                         # normalise\n\n        ensemble.append((best_feat, best_thresh, best_polarity, alpha))\n\n    # ------------------------------------------------------------------\n    # Predict unseen samples\n    # ------------------------------------------------------------------\n    agg = np.zeros(X_test.shape[0])\n\n    for feat_idx, thr, pol, alpha in ensemble:\n        preds = _stump_predict(X_test[:, feat_idx], thr, pol)\n        agg  += alpha * preds\n\n    final_preds = np.sign(agg)\n    final_preds[final_preds == 0] = 1          # replace 0 with positive class\n\n    return final_preds.astype(int).tolist()\n```"}
{"task_id": 241, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list:\n    \"\"\"Initialise a tensor with an orthogonal matrix (Saxe et al., 2014).\n\n    Args:\n        shape: Desired output shape. Must have length \u2265 2.\n        scale: Multiplicative scaling factor (default 0.5).\n\n    Returns:\n        A (possibly nested) python list whose first two dimensions are\n        orthogonal up to the given scaling factor, or \u20131 if `shape` is 1-D.\n    \"\"\"\n    # the algorithm only makes sense for matrices / higher-order tensors\n    if len(shape) < 2:\n        return -1\n\n    # ------------------------------------------------------------------\n    # 1. draw a random matrix  A \u2208 \u211d^{d0 \u00d7 (d1\u22efdn)}  with i.i.d. \ud835\udca9(0,1)\n    # ------------------------------------------------------------------\n    d0 = shape[0]\n    d_rest = int(np.prod(shape[1:], dtype=int))\n    A = np.random.standard_normal((d0, d_rest))\n\n    # ------------------------------------------------------------------\n    # 2. full SVD (reduced form)     A = U \u03a3 V\u1d40\n    # ------------------------------------------------------------------\n    U, _, Vt = np.linalg.svd(A, full_matrices=False)\n\n    # ------------------------------------------------------------------\n    # 3. pick the factor that has the same shape as A\n    # ------------------------------------------------------------------\n    Q = U if U.shape == A.shape else Vt\n\n    # ------------------------------------------------------------------\n    # 4. reshape back to the requested tensor shape and scale it\n    # ------------------------------------------------------------------\n    tensor = (scale * Q.reshape(shape)).tolist()\n    return tensor\n```"}
{"task_id": 243, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a tiny fully-connected Actor\u2013Critic network\n    whose weights are all ones and whose biases are all zeros.\n\n    Args:\n        state: List with three floats \u2013 the environment state.\n\n    Returns:\n        (action_probabilities, state_value) where\n            action_probabilities \u2013 list with three floats, each rounded to 4 decimals\n            state_value          \u2013 single float, rounded to 4 decimals\n    \"\"\"\n    # ----- shared feature extractor -----\n    s_sum = np.sum(state)              # s\u2080 + s\u2081 + s\u2082\n    z1 = max(0.0, s_sum)               # ReLU\n    z3 = 16.0 * z1                     # after three hidden layers (see task description)\n\n    # ----- actor head -----\n    # All three logits are identical (= 4\u00b7z\u2083 = 64\u00b7z\u2081), therefore soft-max \u21d2 uniform distribution.\n    action_probs = [round(1 / 3, 4)] * 3\n\n    # ----- critic head -----\n    state_value = round(64.0 * z1, 4)\n\n    return action_probs, state_value\n```"}
{"task_id": 249, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _dense(x: np.ndarray, w: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A small helper that applies a fully-connected layer to a 1-D input vector.\n    It copes with both common weight layouts:\n        \u2022  (in_dim , out_dim)   so   x @ w\n        \u2022  (out_dim, in_dim )   so   w @ x\n    \"\"\"\n    # try   x @ W   first (Keras / TF layout)\n    if x.shape[-1] == w.shape[0]:\n        return x @ w + b\n    # otherwise fall back to   W @ x   (PyTorch layout)\n    if x.shape[-1] == w.shape[1]:\n        return w @ x + b\n\n    raise ValueError(\"Weight matrix has incompatible shape.\")\n\n\ndef _relu(z: np.ndarray) -> np.ndarray:\n    return np.maximum(0.0, z)\n\n\ndef _softmax(z: np.ndarray) -> np.ndarray:\n    z = z - np.max(z)            # numerical stability\n    e = np.exp(z)\n    return e / e.sum()\n\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max) \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)            \u2192 state value\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # Convert input state and every parameter to NumPy arrays\n    # ------------------------------------------------------------------ #\n    x = np.asarray(state, dtype=np.float64)\n\n    W1 = np.asarray(params[\"W1\"], dtype=np.float64)\n    b1 = np.asarray(params[\"b1\"], dtype=np.float64)\n\n    W2 = np.asarray(params[\"W2\"], dtype=np.float64)\n    b2 = np.asarray(params[\"b2\"], dtype=np.float64)\n\n    W3 = np.asarray(params[\"W3\"], dtype=np.float64)\n    b3 = np.asarray(params[\"b3\"], dtype=np.float64)\n\n    Wa = np.asarray(params[\"Wa\"], dtype=np.float64)\n    ba = np.asarray(params[\"ba\"], dtype=np.float64)\n\n    Wc = np.asarray(params[\"Wc\"], dtype=np.float64)\n    bc = np.asarray(params[\"bc\"], dtype=np.float64)\n\n    # ------------------------------------------------------------------ #\n    # Shared trunk: three Dense layers with ReLU activations\n    # ------------------------------------------------------------------ #\n    x = _relu(_dense(x, W1, b1))\n    x = _relu(_dense(x, W2, b2))\n    hidden = _relu(_dense(x, W3, b3))          # final shared representation\n\n    # ------------------------------------------------------------------ #\n    # Actor head: logits \u2192 soft-max probabilities\n    # ------------------------------------------------------------------ #\n    logits = _dense(hidden, Wa, ba)\n    probs = _softmax(logits)\n\n    # ------------------------------------------------------------------ #\n    # Critic head: single scalar value\n    # ------------------------------------------------------------------ #\n    value = float(_dense(hidden, Wc, bc))       # ensure plain Python float\n\n    # ------------------------------------------------------------------ #\n    # Rounding to four decimal places\n    # ------------------------------------------------------------------ #\n    probs_rounded = [round(float(p), 4) for p in probs]\n    value_rounded = round(value, 4)\n\n    return probs_rounded, value_rounded\n```"}
{"task_id": 253, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w        : 1-D weight vector (list or NumPy array).\n        alpha    : Regularisation strength (non-negative float).\n        l1_ratio : Fraction of L1 component (float in [0, 1]).\n        gradient : If True, return gradient, else return penalty value.\n\n    Returns\n        float  \u2013 if *gradient* is False  (penalty value),\n        list   \u2013 if *gradient* is True   (element-wise gradient).\n    \"\"\"\n    # --- basic checks --------------------------------------------------------\n    if alpha < 0:\n        raise ValueError(\"alpha must be non-negative.\")\n    if not (0.0 <= l1_ratio <= 1.0):\n        raise ValueError(\"l1_ratio must be in [0, 1].\")\n\n    # --- make sure w is a 1-D NumPy array -----------------------------------\n    w = np.asarray(w, dtype=float).ravel()\n\n    # --- compute -------------------------------------------------------------\n    if gradient:\n        # \u2207R(w) = \u03b1 \u00b7 [ \u03bb\u00b7sign(w) + (1\u2212\u03bb)\u00b7w ]\n        grad = alpha * (l1_ratio * np.sign(w) + (1.0 - l1_ratio) * w)\n        return list(np.round(grad, 4))\n    else:\n        # R(w) = \u03b1 \u00b7 [ \u03bb\u00b7||w||\u2081 + (1\u2212\u03bb)\u00b70.5\u00b7w\u1d40w ]\n        penalty = alpha * (\n            l1_ratio * np.linalg.norm(w, 1) +\n            (1.0 - l1_ratio) * 0.5 * np.dot(w, w)\n        )\n        return round(float(penalty), 4)\n```"}
{"task_id": 256, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom copy import deepcopy\n\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred\n    finite differences (double precision).\n\n    See the task description for a detailed specification.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Early exits / name canonicalisation ---------------------------- #\n    # ------------------------------------------------------------------ #\n    if param_name in (\"X\", \"y\"):              # not trainable\n        return None\n\n    real_name = param_name\n    if param_name == \"Ba\":\n        real_name = \"ba\"\n    elif param_name == \"Bx\":\n        real_name = \"bx\"\n\n    if real_name not in model.parameters:     # safety net\n        raise KeyError(f\"Parameter '{real_name}' not found in model.\")\n\n    # ------------------------------------------------------------------ #\n    # 1. Convenience wrappers ------------------------------------------ #\n    # ------------------------------------------------------------------ #\n    # Guarantee float64 everywhere\n    X = np.asarray(X, dtype=np.float64)\n    param: np.ndarray = model.parameters[real_name].astype(np.float64, copy=False)\n    model.parameters[real_name] = param           # insure reference is same\n    grads = np.zeros_like(param, dtype=np.float64)\n\n    # (re-)initialise the model\u2019s hidden state if a reset method exists\n    def _reset_state_if_possible():\n        if hasattr(model, \"reset_state\") and callable(model.reset_state):\n            model.reset_state()\n\n    # Forward pass through the *whole* unrolled RNN and return the scalar loss\n    def _sequence_loss():\n        model.flush_gradients()\n        _reset_state_if_possible()\n\n        preds = []\n        for t in range(n_t):\n            # single-time-step forward\n            preds.append(model.forward(X[:, :, t]))\n        return float(loss_func(preds))        # enforce python scalar\n\n    # ------------------------------------------------------------------ #\n    # 2. Centred finite-difference loop -------------------------------- #\n    # ------------------------------------------------------------------ #\n    it = np.ndindex(param.shape)              # iterate over every element\n    for idx in it:\n        original_val = param[idx]\n\n        # +\u03b5 ------------------------------------------------------------ #\n        param[idx] = original_val + epsilon\n        loss_plus = _sequence_loss()\n\n        # \u2013\u03b5 ------------------------------------------------------------ #\n        param[idx] = original_val - epsilon\n        loss_minus = _sequence_loss()\n\n        # Numerical gradient ------------------------------------------- #\n        grads[idx] = (loss_plus - loss_minus) / (2.0 * epsilon)\n\n        # Restore original parameter value ----------------------------- #\n        param[idx] = original_val\n\n    # ------------------------------------------------------------------ #\n    # 3. Return transposed gradient (requirement of the task) ----------- #\n    # ------------------------------------------------------------------ #\n    return grads.T.astype(np.float64)\n\n\n```"}
{"task_id": 257, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray,\n                     X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\"\"\"\n    \n    # ---------- helpers -------------------------------------------------- #\n    def stump_predict(X, feature, thresh, polarity):\n        \"\"\"\n        Predict with a single decision stump.\n        polarity =  1 : x < thresh  -> -1     else 1\n        polarity = -1 : x < thresh  ->  1     else -1\n        \"\"\"\n        col = X[:, feature]\n        if polarity == 1:\n            return np.where(col < thresh, -1, 1)\n        else:                     # polarity == -1\n            return np.where(col < thresh, 1, -1)\n    \n    # ---------- sanity checks / init ------------------------------------ #\n    n_clf = max(1, int(n_clf))\n    m, n_features = X_train.shape\n    w = np.full(m, 1.0 / m)                     # initial sample weights\n    \n    stumps = []                                 # stores (feature, thresh, polarity, alpha)\n    eps = 1e-10                                 # to avoid division-by-zero\n    \n    # ---------- boosting ------------------------------------------------- #\n    for _ in range(n_clf):\n        best_feature, best_thresh, best_pol, min_err = None, None, None, np.inf\n        \n        # brute-force search over all possible decision stumps\n        for feature in range(n_features):\n            values = X_train[:, feature]\n            thresholds = np.unique(values)      # candidate split points\n            for thresh in thresholds:\n                for polarity in (1, -1):\n                    pred = stump_predict(X_train, feature, thresh, polarity)\n                    err  = np.sum(w[pred != y_train])\n                    if err < min_err:\n                        best_feature, best_thresh, best_pol = feature, thresh, polarity\n                        min_err = err\n                        \n        # if the stump is worse than random, stop early\n        if min_err >= 0.5 - eps:\n            break\n        \n        # stump weight\n        min_err = max(min_err, eps)             # avoid log(0)\n        alpha = 0.5 * np.log((1 - min_err) / min_err)\n        \n        # save the stump\n        stumps.append((best_feature, best_thresh, best_pol, alpha))\n        \n        # update sample weights\n        predictions = stump_predict(X_train, best_feature, best_thresh, best_pol)\n        w *= np.exp(-alpha * y_train * predictions)\n        w /= w.sum()                            # normalise\n    \n    # ---------- prediction for X_test ----------------------------------- #\n    if not stumps:                              # should only happen with degenerate data\n        return [-1] * len(X_test)\n    \n    agg = np.zeros(X_test.shape[0])\n    for feature, thresh, polarity, alpha in stumps:\n        agg += alpha * stump_predict(X_test, feature, thresh, polarity)\n    \n    return [1 if x >= 0 else -1 for x in agg]\n```"}
{"task_id": 261, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate an array initialised with Glorot/Xavier normal distribution.\n\n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired tensor shape. Must have at least two dimensions so that\n        fan-in and fan-out are well defined.\n\n    Returns\n    -------\n    np.ndarray\n        Array of the requested shape whose elements are drawn independently\n        from \ud835\udca9(0, s\u00b2) with  s = \u221a[2 / (fan_in + fan_out)].\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError(\"`shape` must have at least two dimensions.\")\n\n    # fan_in / fan_out --------------------------------------------------------\n    if len(shape) == 2:                                 # e.g. dense layer\n        fan_in, fan_out = shape\n    else:                                               # e.g. conv kernels\n        receptive_field = int(np.prod(shape[2:], dtype=int))\n        fan_in  = shape[1] * receptive_field            # in-channels \u22c5 RF\n        fan_out = shape[0] * receptive_field            # out-channels \u22c5 RF\n\n    # standard deviation ------------------------------------------------------\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n\n    # sample from N(0, std^2) -------------------------------------------------\n    return np.random.normal(loc=0.0, scale=std, size=shape).astype(float)\n```"}
{"task_id": 266, "completion_id": 0, "completion": "```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples ``(u, v)`` describing undirected edges that connect\n        vertices ``u`` and ``v``.\n        \u2022 Multiple occurrences of the same edge may be present.  \n        \u2022 Vertices that do not occur in ``V`` are ignored.  \n        \u2022 Self-loops (u == v) are ignored.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list ``G`` where ``G[i]`` contains all vertices adjacent to ``V[i]``.\n        \u2022 The outer list is in the same order as ``V``.  \n        \u2022 Inner lists contain *no duplicates* and are ordered by the vertices\u2019\n          order of appearance in ``V``.\n    \"\"\"\n    # Position lookup: vertex -> index in V  (O(|V|))\n    index = {vertex: i for i, vertex in enumerate(V)}\n\n    # Start with an empty set for the neighbours of every vertex\n    neighbours: List[set] = [set() for _ in V]\n\n    # Fill the neighbour sets  (O(|E|))\n    for u, v in E:\n        # Skip edges that reference unknown vertices\n        if u not in index or v not in index:\n            continue\n        # Skip self-loops\n        if u == v:\n            continue\n\n        ui, vi = index[u], index[v]\n        neighbours[ui].add(v)   # u \u2014 v\n        neighbours[vi].add(u)   # v \u2014 u\n\n    # Convert each neighbours set to a list whose order\n    # follows the global order in V by sorting on the index\n    G: List[List[Any]] = []\n    for nb_set in neighbours:\n        # sort according to appearance order in V\n        ordered_nb = sorted(nb_set, key=index.get)\n        G.append(ordered_nb)\n\n    return G\n```\n\nExplanation of the key steps\n1. Build `index`, a dictionary that maps every vertex in `V` to its position.  \n   This allows O(1) translation from a vertex label to its place in the final\n   structure and lets us sort neighbours according to the original order.\n\n2. Traverse the edge list `E` once and add both directions of every edge\n   (because the graph is undirected).  \n   Sets are used to ensure that duplicated/parallel edges do not create\n   duplicates in the neighbourhood.\n\n3. Finally turn every neighbour set into a list, ordering the vertices with\n   `sorted(..., key=index.get)` so that they appear in the order dictated by\n   `V`.\n\nThe resulting `G` satisfies all required constraints:\n\u2022 outer order identical to `V`,  \n\u2022 inner lists free of duplicates,  \n\u2022 inner order consistent with `V`,  \n\u2022 isolated vertices represented by an empty list `[]`."}
{"task_id": 267, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef train_decision_stump(\n    X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray\n) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Best stump (see task description).\n    \"\"\"\n    # make sure the vectors are 1-D np arrays\n    y = y.ravel().astype(int)\n    w = sample_weights.ravel().astype(float)\n\n    n_samples, n_features = X.shape\n\n    best_feature = -1\n    best_threshold = 0.0\n    best_polarity = 1\n    best_error = np.inf\n\n    for j in range(n_features):\n        # sort the j-th feature\n        sorted_idx = np.argsort(X[:, j])\n        f_vals = X[sorted_idx, j]\n        lbls = y[sorted_idx]\n        ws = w[sorted_idx]\n\n        pos_total = ws[lbls == 1].sum()\n        neg_total = ws[lbls == -1].sum()\n\n        cum_pos = 0.0  # cumulative positive weight   (x_ij  < \u03b8)\n        cum_neg = 0.0  # cumulative negative weight   (x_ij  < \u03b8)\n\n        i = 0\n        n = n_samples\n        while i < n:\n            # value that will act as a candidate threshold\n            theta = f_vals[i]\n\n            # weighted errors BEFORE adding the samples that equal `theta`\n            err_p_1 = (  # polarity  +1   (+1 left, -1 right)\n                cum_neg + (pos_total - cum_pos)\n            )\n            err_p_neg1 = (  # polarity  -1   (-1 left, +1 right)\n                cum_pos + (neg_total - cum_neg)\n            )\n\n            # update best stump if necessary\n            if err_p_1 < best_error - 1e-12:\n                best_error = err_p_1\n                best_feature = j\n                best_threshold = float(theta)\n                best_polarity = 1\n\n            if err_p_neg1 < best_error - 1e-12:\n                best_error = err_p_neg1\n                best_feature = j\n                best_threshold = float(theta)\n                best_polarity = -1\n\n            # move forward \u2013 add all samples that have feature value == theta\n            while i < n and f_vals[i] == theta:\n                if lbls[i] == 1:\n                    cum_pos += ws[i]\n                else:\n                    cum_neg += ws[i]\n                i += 1\n\n    return {\n        \"feature_index\": int(best_feature),\n        \"threshold\": round(best_threshold, 4),\n        \"polarity\": int(best_polarity),\n        \"weighted_error\": round(best_error, 4),\n    }\n```"}
{"task_id": 273, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float (rounded to 4 decimals) or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n\n    n = len(feature)\n    if n <= 1:                          # nothing to split\n        return (None, 0.0)\n\n    # ------------------------------------------------------------------ #\n    # Helper : Shannon entropy for a multiset of class labels            #\n    # ------------------------------------------------------------------ #\n    def entropy(counter):\n        total = sum(counter.values())\n        if total == 0:\n            return 0.0\n        h = 0.0\n        for cnt in counter.values():\n            p = cnt / total\n            h -= p * math.log2(p)\n        return h\n\n    # ------------------------------------------------------------------ #\n    # Sort by the feature \u2013 makes \u201c< threshold / \u2265 threshold\u201d contiguous #\n    # ------------------------------------------------------------------ #\n    pairs = sorted(zip(feature, target), key=lambda x: x[0])\n    sorted_feat  = [p[0] for p in pairs]\n    sorted_label = [p[1] for p in pairs]\n\n    if sorted_feat[0] == sorted_feat[-1]:          # all values identical\n        return (None, 0.0)\n\n    # initial class\u2013frequency tables\n    right_counts = Counter(sorted_label)           # all points start on the right\n    left_counts  = Counter()\n\n    parent_entropy = entropy(right_counts)\n\n    best_gain       = 0.0\n    best_threshold  = None\n    eps             = 1e-12                        # for floating-point ties\n\n    # ------------------------------------------------------------------ #\n    # Sweep once from left to right, updating class counts on the fly    #\n    # ------------------------------------------------------------------ #\n    for i in range(n - 1):\n        lbl = sorted_label[i]\n        left_counts[lbl]  += 1\n        right_counts[lbl] -= 1\n        if right_counts[lbl] == 0:\n            del right_counts[lbl]\n\n        # Only between *distinct* consecutive feature values a threshold is legal\n        if sorted_feat[i] != sorted_feat[i + 1]:\n            thr = (sorted_feat[i] + sorted_feat[i + 1]) / 2.0\n\n            n_left  = i + 1\n            n_right = n - n_left\n\n            gain = (parent_entropy\n                    - (n_left  / n) * entropy(left_counts)\n                    - (n_right / n) * entropy(right_counts))\n\n            # keep the smallest threshold in case of equal maximal gain\n            if gain > best_gain + eps:\n                best_gain, best_threshold = gain, thr\n            elif abs(gain - best_gain) <= eps and best_gain > 0.0:\n                best_threshold = min(best_threshold, thr)\n\n    if best_threshold is None:                     # no positive gain found\n        return (None, 0.0)\n\n    # Round as required\n    return (round(best_threshold, 4), round(best_gain, 4))\n```"}
{"task_id": 286, "completion_id": 0, "completion": "```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef _validate_shape(shape: Tuple[int, int]) -> Tuple[int, int]:\n    \"\"\"Internal helper to validate the (rows, cols) shape tuple.\"\"\"\n    if (not isinstance(shape, tuple) or len(shape) != 2 or\n        not all(isinstance(x, int) and x >= 0 for x in shape)):\n        raise ValueError(\"shape must be a tuple of two non-negative integers.\")\n    return shape  # the same object is fine \u2013 just signal validation\n\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    rows, cols = _validate_shape(shape)\n    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    rows, cols = _validate_shape(shape)\n    return [[1.0 for _ in range(cols)] for _ in range(rows)]\n\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix whose elements are drawn uniformly from [low, high].\"\"\"\n    rows, cols = _validate_shape(shape)\n\n    # Use a dedicated RNG instance so we don't interfere with global state.\n    rng = random.Random(seed)\n    return [[rng.uniform(low, high) for _ in range(cols)] for _ in range(rows)]\n\n\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Retrieve an initializer function by its name.\n\n    Args:\n        name: The name of the initializer to look up (e.g. \"zeros_init\").\n\n    Returns:\n        The corresponding callable weight initializer.\n\n    Raises:\n        ValueError: If `name` is not found or is not callable.\n    \"\"\"\n    obj = globals().get(name)\n    if obj is None or not callable(obj):\n        raise ValueError(\"Invalid initialization function.\")\n    return obj\n```"}
{"task_id": 287, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef average_pool2d(\n    X: np.ndarray,\n    pool_shape: tuple[int, int],\n    stride: tuple[int, int],\n    accum_grad: np.ndarray\n) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input tensor of shape (N, C, H, W).\n    pool_shape : (int, int)\n        Height and width of the pooling window  (p_h, p_w).\n    stride : (int, int)\n        Vertical and horizontal stride          (s_h, s_w).\n    accum_grad : np.ndarray\n        Gradient wrt. the pooled output. Shape must match the\n        forward-pass output.\n\n    Returns\n    -------\n    tuple[list, list]\n        (pooled_output, grad_input) \u2013 both rounded to 4 decimals and\n        converted to Python lists.\n    \"\"\"\n    # Unpack shapes and hyper-parameters\n    N, C, H, W   = X.shape\n    p_h, p_w     = pool_shape\n    s_h, s_w     = stride\n\n    # Output spatial dimensions\n    out_h = (H - p_h) // s_h + 1\n    out_w = (W - p_w) // s_w + 1\n\n    # Forward pass ----------------------------------------------------------\n    pooled = np.zeros((N, C, out_h, out_w), dtype=X.dtype)\n\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                h_start, h_end = i * s_h, i * s_h + p_h\n                for j in range(out_w):\n                    w_start, w_end = j * s_w, j * s_w + p_w\n                    window = X[n, c, h_start:h_end, w_start:w_end]\n                    pooled[n, c, i, j] = window.mean()\n\n    # Back-propagation ------------------------------------------------------\n    grad_input = np.zeros_like(X, dtype=X.dtype)\n    scale = 1.0 / (p_h * p_w)         # each element gets an equal share\n\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                h_start, h_end = i * s_h, i * s_h + p_h\n                for j in range(out_w):\n                    w_start, w_end = j * s_w, j * s_w + p_w\n                    g = accum_grad[n, c, i, j] * scale\n                    grad_input[n, c, h_start:h_end, w_start:w_end] += g\n\n    # Round to 4 decimals and convert to lists ------------------------------\n    pooled = np.round(pooled, 4).tolist()\n    grad_input = np.round(grad_input, 4).tolist()\n\n    return pooled, grad_input\n```"}
{"task_id": 290, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\n\n_TOL = 1e-8  # Numerical tolerance used everywhere\n\n\ndef _values_equal(a, b):\n    \"\"\"Compare two prediction values with the required tolerance.\"\"\"\n    try:                       # works for scalars, lists and nd-arrays\n        return np.allclose(a, b, atol=_TOL, rtol=_TOL)\n    except TypeError:          # fall-back for objects that allclose cannot handle\n        return a == b\n\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\"\"\"\n    # Same node type?\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return _values_equal(tree_a.value, tree_b.value)\n\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        return (\n            tree_a.feature == tree_b.feature and\n            np.isclose(tree_a.threshold, tree_b.threshold, atol=_TOL, rtol=_TOL) and\n            compare_trees(tree_a.left, tree_b.left) and\n            compare_trees(tree_a.right, tree_b.right)\n        )\n\n    # One is a Node, the other a Leaf (different shape) \u2192 not equivalent\n    return False\n```"}
{"task_id": 292, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix (list of lists).\n        parent2: Second parent weight matrix (same shape as *parent1*).\n        cutoff : Column index at which crossover starts (inclusive).\n\n    Returns\n    -------\n        Tuple (child1, child2) with the two children as ordinary Python\n        lists, or -1 if the parent matrices do not have identical shapes.\n    \"\"\"\n    # Convert to NumPy arrays (no copy yet, so parents stay untouched)\n    p1 = np.asarray(parent1, dtype=float)\n    p2 = np.asarray(parent2, dtype=float)\n\n    # Shape validation --------------------------------------------------------\n    if p1.ndim != 2 or p2.ndim != 2 or p1.shape != p2.shape:\n        return -1                                          # shape mismatch\n\n    rows, cols = p1.shape\n    if not (0 <= cutoff <= cols):                         # sanity check\n        raise ValueError(f\"cutoff must be in [0, {cols}]\")\n\n    # -------------------------------------------------------------------------\n    # Create children.  Start with copies of the respective \u201cown\u201d parent and\n    # then overwrite the columns *from* cutoff with the other parent\u2019s columns.\n    # -------------------------------------------------------------------------\n    child1 = p1.copy()\n    child2 = p2.copy()\n\n    child1[:, cutoff:] = p2[:, cutoff:]                   # swap right side\n    child2[:, cutoff:] = p1[:, cutoff:]\n\n    # Convert back to plain Python lists and return\n    return child1.tolist(), child2.tolist()\n```"}
{"task_id": 294, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert the custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph with the keys\n          * 'is_directed' : bool\n          * '_V2I'        : dict  (vertex label -> consecutive index 0..n-1)\n          * '_G'          : list  (for each vertex index a list with tuples\n                                   (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary mapping every vertex label to a list of (neighbour, weight)\n        tuples.  Lists are sorted lexicographically by the neighbour label.\n        For undirected graphs every edge appears once in each direction.\n    \"\"\"\n    # ---------- trivial / empty graph cases ---------------------------------\n    if not G or not G.get('_V2I'):\n        return {}                       # no vertices at all\n\n    directed: bool = bool(G.get('is_directed', False))\n\n    # create an empty list for every vertex first (so isolated vertices stay in\n    # the result even if they have no incident edges)\n    adj: Dict[Any, List[Tuple[Any, float]]] = {v: [] for v in G['_V2I']}\n\n    # helper structure to suppress duplicates in the *undirected* case\n    processed_undirected = set()        # stores unordered vertex-pairs\n\n    # iterate over all stored edges\n    for edge_list in G['_G']:\n        for u, v, w in edge_list:\n            if directed:\n                adj[u].append((v, w))\n            else:\n                # build a key that is independent of edge orientation\n                key = (u, v) if u <= v else (v, u)\n                if key in processed_undirected:\n                    continue            # already handled that undirected edge\n                processed_undirected.add(key)\n\n                # add the edge in *both* directions\n                adj[u].append((v, w))\n                adj[v].append((u, w))\n\n    # sort every neighbour list lexicographically by neighbour label\n    for neighbours in adj.values():\n        neighbours.sort(key=lambda t: str(t[0]))\n\n    return adj\n```"}
{"task_id": 296, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1 (or, for boolean arrays,\n        only ``True``/``False``), otherwise ``False``.\n    \"\"\"\n    # Convert any array-like input to a NumPy array\n    arr = np.asarray(x)\n\n    # Boolean arrays are by definition binary\n    if arr.dtype == np.bool_:\n        return True\n\n    # For all other dtypes, every element must be exactly 0 or 1\n    return np.isin(arr, (0, 1)).all()\n```"}
{"task_id": 298, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom typing import List, Tuple, Dict\n\n\ndef _extract_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n    \"\"\"Return a list of all consecutive n-grams from *tokens*.\"\"\"\n    return [tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1)]\n\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Notes\n    -----\n    \u2022 White-space is used for tokenisation.\n    \u2022 For N > 1 every sentence is padded with N\u22121 '<bol>' tokens in front\n      and a single '<eol>' token at the end.  Unigrams use no padding.\n    \u2022 If any N-gram (or its (N-1)-gram prefix) required for the query is\n      unseen in *corpus*, or if the query is too short, the function\n      returns float('-inf').\n    \u2022 The final result is rounded to 4 decimal places.\n    \"\"\"\n    # --- Guard conditions -------------------------------------------------\n    if N < 1:\n        raise ValueError(\"N must be \u2265 1\")\n    if not corpus:\n        return float(\"-inf\")\n\n    # ---------------------------------------------------------------------\n    # 1. Count N-grams in the training corpus (and prefixes for N > 1)\n    # ---------------------------------------------------------------------\n    if N == 1:\n        unigram_counts: Counter[str] = Counter()\n        total_tokens = 0\n\n        for sent in corpus:\n            tokens = sent.split()\n            unigram_counts.update(tokens)\n            total_tokens += len(tokens)\n\n    else:\n        ngram_counts: Counter[Tuple[str, ...]] = Counter()\n        prefix_counts: Counter[Tuple[str, ...]] = Counter()\n\n        for sent in corpus:\n            tokens = [\"<bol>\"] * (N - 1) + sent.split() + [\"<eol>\"]\n            for i in range(len(tokens) - N + 1):\n                ngram = tuple(tokens[i : i + N])\n                prefix = tuple(tokens[i : i + N - 1])\n                ngram_counts[ngram] += 1\n                prefix_counts[prefix] += 1\n\n    # ---------------------------------------------------------------------\n    # 2. Prepare the query sentence in exactly the same way\n    # ---------------------------------------------------------------------\n    if N == 1:\n        q_tokens = query.split()\n        if not q_tokens or total_tokens == 0:\n            return float(\"-inf\")\n    else:\n        q_tokens = [\"<bol>\"] * (N - 1) + query.split() + [\"<eol>\"]\n        if len(q_tokens) < N:\n            return float(\"-inf\")\n\n    # ---------------------------------------------------------------------\n    # 3. Compute the summed log-probability\n    # ---------------------------------------------------------------------\n    log_sum = 0.0\n\n    if N == 1:  # ---------- Unigram model --------------------------------\n        for tok in q_tokens:\n            cnt = unigram_counts.get(tok, 0)\n            if cnt == 0:\n                return float(\"-inf\")\n            prob = cnt / total_tokens\n            log_sum += math.log(prob)\n\n    else:       # ---------- N \u2265 2 ----------------------------------------\n        for i in range(len(q_tokens) - N + 1):\n            ngram = tuple(q_tokens[i : i + N])\n            prefix = tuple(q_tokens[i : i + N - 1])\n\n            cnt_ng = ngram_counts.get(ngram, 0)\n            cnt_pre = prefix_counts.get(prefix, 0)\n\n            if cnt_ng == 0 or cnt_pre == 0:\n                return float(\"-inf\")\n\n            prob = cnt_ng / cnt_pre\n            log_sum += math.log(prob)\n\n    # ---------------------------------------------------------------------\n    # 4. Round and return\n    # ---------------------------------------------------------------------\n    return round(log_sum, 4)\n```"}
{"task_id": 302, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample (shape = [n_samples, n_features]).\n        k: Desired number of clusters (k \u2264 n_samples).\n\n    Returns:\n        Python list of length n_samples whose entries are integers in [0, k-1].\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1. Weighted adjacency matrix W  (w_ij = 1 / (1 + ||x_i - x_j||_2))\n    # ------------------------------------------------------------------\n    n_samples = X.shape[0]\n\n    # Pair-wise Euclidean distance matrix  (broadcasting \u2013 O(n\u00b2d) time, O(n\u00b2) memory)\n    diff = X[:, None, :] - X[None, :, :]        # shape (n, n, d)\n    dist = np.linalg.norm(diff, axis=2)         # shape (n, n)\n\n    W = 1.0 / (1.0 + dist)                      # weights\n    np.fill_diagonal(W, 0.0)                   # zero self-loops\n\n    # ------------------------------------------------------------------\n    # 2. Un-normalised graph Laplacian  L = D \u2212 W\n    # ------------------------------------------------------------------\n    degree = W.sum(axis=1)                      # shape (n,)\n    L = np.diag(degree) - W                    # shape (n, n)\n\n    # ------------------------------------------------------------------\n    # 3. k eigenvectors associated with the k smallest eigen-values\n    # ------------------------------------------------------------------\n    # L is symmetric => use eigh (eigenvalues are returned in ascending order)\n    eig_vals, eig_vecs = np.linalg.eigh(L)      # eig_vecs columns are eigenvectors\n    E = eig_vecs[:, :k]                         # projection matrix  (shape n \u00d7 k)\n\n    # ------------------------------------------------------------------\n    # 4. k-means on rows of E\n    #    \u2013 Initial centroids: first k rows of E (deterministic)\n    # ------------------------------------------------------------------\n    centroids = E[:k].copy()                    # shape (k, k)  (centroid i is row i of E)\n    labels = np.full(n_samples, -1, dtype=int)  # current assignments\n    max_iter = 100\n\n    rng = np.random.default_rng()               # for possible re-initialisation\n\n    for _ in range(max_iter):\n        # ----- Assignment step -----\n        # Compute distances of every point to every centroid\n        # distances[i, c] = ||E[i] - centroids[c]||_2\n        dists = np.linalg.norm(E[:, None, :] - centroids[None, :, :], axis=2)  # (n, k)\n        new_labels = dists.argmin(axis=1)\n\n        # Check for convergence\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # ----- Update step -----\n        for c in range(k):\n            mask = labels == c\n            if np.any(mask):                    # non-empty cluster\n                centroids[c] = E[mask].mean(axis=0)\n            else:                               # empty cluster -> re-initialise\n                random_idx = rng.integers(0, n_samples)\n                centroids[c] = E[random_idx]\n\n    # ------------------------------------------------------------------\n    # 5. Return labels as a Python list\n    # ------------------------------------------------------------------\n    return labels.tolist()\n```"}
{"task_id": 303, "completion_id": 0, "completion": "```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    # Determine current and previous indices\n    prev_ix = max(ix - 1, 0)\n\n    # Extract previous and current elements\n    prev_mine, prev_label = params[prev_ix]\n    curr_mine, curr_label = params[ix]\n\n    # Fetch gold-standard outputs\n    gold_prev = golds.get(prev_label, \"\")\n    gold_curr = golds.get(curr_label, \"\")\n\n    # Header and footer lines\n    header = \"-\" * 25 + \" DEBUG \" + \"-\" * 25\n    footer = \"-\" * 23 + \" END DEBUG \" + \"-\" * 23\n\n    # Assemble all parts respecting the exact required layout\n    lines = [\n        header,\n        f\"Mine (prev) [{prev_label}]:\",\n        f\"{prev_mine}\",\n        \"\",\n        f\"Theirs (prev) [{prev_label}]:\",\n        f\"{gold_prev}\",\n        \"\",\n        f\"Mine [{curr_label}]:\",\n        f\"{curr_mine}\",\n        \"\",\n        f\"Theirs [{curr_label}]:\",\n        f\"{gold_curr}{warn_str}\",\n        footer,\n    ]\n\n    return \"\\n\".join(lines)\n```"}
{"task_id": 304, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu = 0.0,\n    V = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Posterior mean (MAP) of the coefficients in Bayesian linear regression.\n\n    Parameters\n    ----------\n    X : (N, M) array_like\n        Design matrix.\n    y : (N,) array_like\n        Targets.\n    alpha, beta : float\n        Hyper\u2013parameters of the inverse-gamma prior on \u03c3\u00b2 (kept for API\n        completeness \u2013 they do not enter the MAP of the weights).\n    mu : float or array_like\n        Prior mean for the weights.  Scalar values are broadcast.\n    V : None, float, 1-D or 2-D array_like\n        Prior scale.  None\u2192I, scalar\u2192s\u00b7I, 1-D\u2192diag(v), 2-D\u2192full matrix.\n    fit_intercept : bool\n        Whether to include a bias column of ones.\n\n    Returns\n    -------\n    list[float]\n        Posterior mean rounded to 4 decimals.\n    \"\"\"\n    # ---- 1. Basic checks & shapes ------------------------------------------\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n\n    if fit_intercept:\n        X = np.column_stack((np.ones(X.shape[0]), X))\n\n    N, M = X.shape\n\n    # ---- 2. Prior scale V ---------------------------------------------------\n    if V is None:                                   # Identity\n        V_mat = np.eye(M)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 0:                         # Scalar\n            V_mat = float(V_arr) * np.eye(M)\n        elif V_arr.ndim == 1:                       # Diagonal specified\n            if V_arr.size != M:\n                raise ValueError(\"Diagonal length of V does not match #features.\")\n            V_mat = np.diag(V_arr)\n        elif V_arr.ndim == 2:                       # Full matrix\n            if V_arr.shape != (M, M):\n                raise ValueError(\"Shape of V does not match #features.\")\n            V_mat = V_arr\n        else:\n            raise ValueError(\"Unsupported shape for V.\")\n\n    # Inverse of V\n    V_inv = np.linalg.inv(V_mat)\n\n    # ---- 3. Prior mean \u03bc ----------------------------------------------------\n    if np.isscalar(mu):\n        mu_vec = np.full(M, float(mu))\n    else:\n        mu_vec = np.asarray(mu, dtype=float).ravel()\n        if mu_vec.size != M:\n            raise ValueError(\"Length of mu does not match #features.\")\n\n    # ---- 4. Posterior mean (MAP) -------------------------------------------\n    XtX = X.T @ X\n    Sigma_b = np.linalg.inv(V_inv + XtX)            # (V^{-1} + X\u1d40X)^{-1}\n    rhs = V_inv @ mu_vec + X.T @ y                 # V^{-1} \u03bc + X\u1d40 y\n    mu_b = Sigma_b @ rhs                            # Posterior mean\n\n    return np.round(mu_b, 4).tolist()\n```"}
{"task_id": 308, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    Parameters\n    ----------\n    N : int\n        DFT length (number of frequency-domain samples).\n    n_filters : int, default 20\n        Number of Mel filters to construct.\n    fs : int, default 44_000\n        Sampling frequency of the signal in Hertz.\n    min_freq : int, default 0\n        Lower frequency limit of the filterbank in Hertz.\n    max_freq : int | None, optional\n        Upper frequency limit of the filterbank in Hertz.  If *None* the\n        Nyquist frequency *fs/2* is used.\n    normalize : bool, default True\n        Whether to scale each filter so that its (Mel-domain) area equals 1.\n\n    Returns\n    -------\n    list[list[float]]\n        The filterbank matrix with shape (n_filters, N//2 + 1).  All values are\n        rounded to four decimal places.\n    \"\"\"\n\n    # ------------------------------------------------------------------\n    # Helper conversions\n    # ------------------------------------------------------------------\n    def hz2mel(f_hz: np.ndarray | float) -> np.ndarray | float:\n        return 2595.0 * np.log10(1.0 + np.asarray(f_hz) / 700.0)\n\n    def mel2hz(mel: np.ndarray | float) -> np.ndarray | float:\n        return 700.0 * (10**(np.asarray(mel) / 2595.0) - 1.0)\n\n    # ------------------------------------------------------------------\n    # Sanity checks & parameter handling\n    # ------------------------------------------------------------------\n    if max_freq is None:\n        max_freq = fs / 2\n\n    if not (0 <= min_freq < max_freq <= fs / 2):\n        raise ValueError(\"min_freq must be >= 0, and max_freq must not \"\n                         \"exceed fs/2 with min_freq < max_freq.\")\n\n    # ------------------------------------------------------------------\n    # 1. Equally spaced points on the Mel axis (corners of the triangles)\n    # ------------------------------------------------------------------\n    mel_min, mel_max = hz2mel(min_freq), hz2mel(max_freq)\n    mel_points = np.linspace(mel_min, mel_max, n_filters + 2)\n    hz_points = mel2hz(mel_points)                       # (n_filters + 2,)\n\n    # ------------------------------------------------------------------\n    # 2. Centre frequency of every DFT bin\n    # ------------------------------------------------------------------\n    bin_freqs = np.arange(N // 2 + 1) * (fs / N)         # (N//2 + 1,)\n\n    # ------------------------------------------------------------------\n    # 3. Build the triangular filters\n    # ------------------------------------------------------------------\n    F = np.zeros((n_filters, N // 2 + 1), dtype=float)\n\n    for i in range(n_filters):\n        f_left, f_center, f_right = hz_points[i:i + 3]\n\n        # Rising slope (left part of the triangle)\n        left_mask = np.logical_and(bin_freqs >= f_left, bin_freqs <= f_center)\n        F[i, left_mask] = (bin_freqs[left_mask] - f_left) / (f_center - f_left)\n\n        # Falling slope (right part of the triangle)\n        right_mask = np.logical_and(bin_freqs >= f_center, bin_freqs <= f_right)\n        F[i, right_mask] = (f_right - bin_freqs[right_mask]) / (f_right - f_center)\n\n        # Optional normalisation so the area in Mel space equals 1\n        if normalize:\n            F[i, :] *= 2.0 / (f_right - f_left)\n\n    # ------------------------------------------------------------------\n    # 4. Round to 4 decimal places and convert to plain Python list\n    # ------------------------------------------------------------------\n    F_rounded = np.round(F, 4).tolist()\n    return F_rounded\n```"}
{"task_id": 312, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    # ---- special-case handling ---------------------------------------------\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n\n    # ---- choose the normalising divisor N ----------------------------------\n    # For the symmetric form we follow the textbook definition N = L-1.\n    # For the periodic form we mimic a window of length L+1 with the\n    # last sample dropped, which is equivalent to setting N = L.\n    N = window_len - 1 if symmetric else window_len\n\n    # ---- Blackman\u2013Harris coefficients --------------------------------------\n    a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n\n    # ---- sample indices -----------------------------------------------------\n    n = np.arange(window_len)\n\n    # ---- window calculation -------------------------------------------------\n    w = (a0\n         - a1 * np.cos(2 * np.pi * n / N)\n         + a2 * np.cos(4 * np.pi * n / N)\n         - a3 * np.cos(6 * np.pi * n / N))\n\n    # ---- round to 4 decimal places and return as a list ---------------------\n    return [round(float(x), 4) for x in w]\n```\n"}
{"task_id": 313, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Return E[log X_t] for X ~ Dir(gamma[d, :]), rounded to 4 decimals.\"\"\"\n    \n    # -------- digamma implementation ------------------------------------ #\n    def digamma(x):\n        \"\"\"\n        Digamma \u03c8(x) for positive real x.\n        Small arguments are stepped up with the recurrence\n            \u03c8(x) = \u03c8(x + 1) \u2212 1/x\n        until x \u2265 6, after which an asymptotic expansion is applied:\n            \u03c8(x) \u2248 ln x \u2212 1/(2x) \u2212 1/(12x\u00b2) + 1/(120x\u2074) \u2212 1/(252x\u2076)\n        The combination is accurate to better than 1e-6 for all x > 0.\n        \"\"\"\n        res = 0.0\n        x = float(x)\n        while x < 6.0:     # step up to a comfortable range\n            res -= 1.0 / x\n            x += 1.0\n\n        inv  = 1.0 / x\n        inv2 = inv * inv\n        inv4 = inv2 * inv2\n        inv6 = inv4 * inv2\n\n        res += (math.log(x)\n                - 0.5 * inv\n                - inv2 / 12.0\n                + inv4 / 120.0\n                - inv6 / 252.0)\n        return res\n    # -------------------------------------------------------------------- #\n\n    # ensure `gamma` behaves like a 2-D array\n    g = np.asarray(gamma, dtype=float)\n\n    row      = g[d]\n    alpha_t  = row[t]\n    row_sum  = float(row.sum())\n\n    value = digamma(alpha_t) - digamma(row_sum)\n    return round(value, 4)\n```"}
{"task_id": 317, "completion_id": 0, "completion": "```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\"\"\"\n    # indices and data\n    prev_ix = max(ix - 1, 0)\n    prev_pred, prev_label = params[prev_ix]\n    curr_pred, curr_label = params[ix]\n\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n\n    header  = \"-\" * 25 + \" DEBUG \"     + \"-\" * 25\n    footer  = \"-\" * 23 + \" END DEBUG \" + \"-\" * 23\n\n    parts = [\n        header,\n        f\"Mine (prev) [{prev_label}]:\",\n        prev_pred,\n        \"\",                                   # blank line before Theirs (prev)\n        f\"Theirs (prev) [{prev_label}]:\",\n        prev_gold,\n        \"\",                                   # 1st blank line after prev block\n        \"\",                                   # 2nd blank line after prev block\n        f\"Mine [{curr_label}]:\",\n        curr_pred,\n        \"\",                                   # blank line before Theirs (curr)\n        f\"Theirs [{curr_label}]:\",\n        f\"{curr_gold}{warn_str}\",\n        footer\n    ]\n\n    return \"\\n\".join(parts)\n```"}
{"task_id": 318, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\n\ndef _sign(z):\n    \"\"\"Return -1 for z < 0, +1 otherwise (i.e. sign(0)=+1).\"\"\"\n    return np.where(z < 0, -1, 1)\n\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples with labels \u20131 or +1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (\u20131 or +1) for each sample in X_test.\n    \"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    X_test = np.asarray(X_test, dtype=float)\n\n    n_samples, n_features = X_train.shape\n\n    # ---------------------------------------------------------------------\n    # Initial weights\n    # ---------------------------------------------------------------------\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    # To keep all learned stumps\n    stumps = []          # list of (j, t, p, alpha)\n\n    # ---------------------------------------------------------------------\n    # Boosting rounds\n    # ---------------------------------------------------------------------\n    for _ in range(n_estimators):\n        best_error = float(\"inf\")\n        best_j = best_t = best_p = None\n\n        # -----------------------------------------------------------------\n        # Search the best decision stump\n        # -----------------------------------------------------------------\n        for j in range(n_features):\n            xj = X_train[:, j]\n            idx = np.argsort(xj)\n            x_sorted = xj[idx]\n            y_sorted = y_train[idx]\n            w_sorted = w[idx]\n\n            # cumulative weights of positive / negative samples\n            pos_mask = (y_sorted == 1)\n            neg_mask = ~pos_mask\n\n            pos_cumsum = np.cumsum(w_sorted * pos_mask)\n            neg_cumsum = np.cumsum(w_sorted * neg_mask)\n\n            pos_total = pos_cumsum[-1]\n            neg_total = neg_cumsum[-1]\n\n            # left side includes first k samples (k = 0..n_samples)\n            pos_left = np.concatenate(([0.0], pos_cumsum))\n            neg_left = np.concatenate(([0.0], neg_cumsum))\n\n            # error if polarity p = +1   (left predicts \u22121, right +1)\n            err_p1 = pos_left + (neg_total - neg_left)\n\n            # error if polarity p = \u22121   (left predicts +1, right \u22121)\n            err_pn1 = neg_left + (pos_total - pos_left)\n\n            for polarity, errors in ((1, err_p1), (-1, err_pn1)):\n                k = int(np.argmin(errors))\n                err = errors[k]\n\n                if err < best_error - 1e-12:   # strict improvement\n                    best_error = err\n                    best_j = j\n                    best_p = polarity\n\n                    # choose threshold: between x_sorted[k-1] and x_sorted[k]\n                    if k == 0:\n                        best_t = x_sorted[0] - 1e-10\n                    elif k == len(x_sorted):\n                        best_t = x_sorted[-1] + 1e-10\n                    else:\n                        best_t = 0.5 * (x_sorted[k - 1] + x_sorted[k])\n\n        # -----------------------------------------------------------------\n        # Compute learner weight alpha\n        # -----------------------------------------------------------------\n        eps = max(best_error, 1e-10)         # avoid log(0)\n        alpha = 0.5 * math.log((1.0 - eps) / eps)\n\n        # -----------------------------------------------------------------\n        # Update sample weights\n        # -----------------------------------------------------------------\n        preds = best_p * _sign(X_train[:, best_j] - best_t)\n        w *= np.exp(-alpha * y_train * preds)\n        w /= w.sum()                         # normalise\n\n        stumps.append((best_j, best_t, best_p, alpha))\n\n    # ---------------------------------------------------------------------\n    # Prediction on test data\n    # ---------------------------------------------------------------------\n    agg = np.zeros(X_test.shape[0])\n    for j, t, p, alpha in stumps:\n        agg += alpha * p * _sign(X_test[:, j] - t)\n\n    y_pred = _sign(agg).astype(int)\n    return y_pred.tolist()\n```"}
{"task_id": 329, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef dft_bins(N: int, fs: int = 44_000, *, positive_only: bool = True) -> list[float]:\n    \"\"\"Return the centre frequency (Hz) of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        If True, return only the non-negative frequencies (indices\n        0 \u2026 \u230aN/2\u230b).  If False, return all N frequency bins in the same\n        order as `numpy.fft.fftfreq`.  Default is True.\n\n    Returns\n    -------\n    list[float]\n        Frequencies rounded to 4 decimal places.  Empty list if `N`\n        or `fs` is not strictly positive.\n    \"\"\"\n    # Guard against non-positive inputs\n    if N <= 0 or fs <= 0:\n        return []\n\n    if positive_only:\n        # Non-negative part (same as np.fft.rfftfreq)\n        freqs = np.arange(0, N // 2 + 1) * (fs / N)\n    else:\n        # Full set (same ordering as np.fft.fftfreq)\n        freqs = np.fft.fftfreq(N, d=1 / fs)\n\n    # Round to 4 decimal places and convert to a plain Python list\n    return np.round(freqs, 4).tolist()\n```"}
{"task_id": 331, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix whose element (i, j) is True/1 iff sample i\n        was out of bag for tree j, same shape as *predictions*.\n\n    Returns\n    -------\n    float\n        OOB mean squared error rounded to 4 decimal places, or -1 if it cannot\n        be computed.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y_true = np.asarray(y_true, dtype=float).ravel()\n    preds   = np.asarray(predictions, dtype=float)\n    mask    = np.asarray(oob_mask).astype(bool)\n\n    if preds.shape != mask.shape:\n        raise ValueError(\"`predictions` and `oob_mask` must have the same shape.\")\n\n    # Number of OOB predictions for every sample\n    oob_counts = mask.sum(axis=1)\n\n    # Samples with at least one OOB estimate\n    valid_idx = oob_counts > 0\n    if not np.any(valid_idx):\n        return -1                                   # No OOB information\n\n    # Mean of OOB predictions for each sample\n    oob_sums  = (preds * mask).sum(axis=1)\n    oob_means = np.zeros_like(y_true)\n    oob_means[valid_idx] = oob_sums[valid_idx] / oob_counts[valid_idx]\n\n    # Squared errors on valid samples\n    sq_err = (oob_means[valid_idx] - y_true[valid_idx]) ** 2\n    mse    = sq_err.mean()\n\n    return round(float(mse), 4)\n```"}
{"task_id": 332, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree represented with plain nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of *transactions* an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node is a mapping:\n        {\"support\": <int>, \"children\": {item -> child-node}}\n    \"\"\"\n    # ---------------------------------------------------------------------\n    # 1) Support per item (occurrence in distinct transactions)\n    # ---------------------------------------------------------------------\n    item_counter = Counter()\n    for trx in transactions:\n        for item in set(trx):          # duplicates inside a transaction do not matter\n            item_counter[item] += 1\n\n    # ---------------------------------------------------------------------\n    # 2) Keep only frequent items\n    # ---------------------------------------------------------------------\n    freq_items = {item: sup for item, sup in item_counter.items()\n                  if sup >= min_support}\n    if not freq_items:                         # nothing is frequent \u2192 empty FP-tree\n        return {\"support\": 0, \"children\": {}}\n\n    # ---------------------------------------------------------------------\n    # 3) Global ordering: by decreasing support, then alphabetically\n    # ---------------------------------------------------------------------\n    ordered = sorted(freq_items.items(),\n                     key=lambda pair: (-pair[1], pair[0]))\n    order_rank = {item: rank for rank, (item, _) in enumerate(ordered)}\n    frequent_set = set(freq_items)\n\n    # ---------------------------------------------------------------------\n    # 4) Build the tree\n    # ---------------------------------------------------------------------\n    root = {\"support\": 0, \"children\": {}}      # root support stays 0\n\n    for trx in transactions:\n        # keep only frequent, unique items and order them\n        filtered = [item for item in set(trx) if item in frequent_set]\n        if not filtered:\n            continue\n        filtered.sort(key=lambda it: order_rank[it])\n\n        # walk / extend path, updating supports\n        node = root\n        for item in filtered:\n            children = node[\"children\"]\n            if item not in children:\n                children[item] = {\"support\": 0, \"children\": {}}\n            node = children[item]\n            node[\"support\"] += 1               # visited by this transaction\n\n    return root\n```"}
{"task_id": 336, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_kernel(\n    X: list[list[int | float]] | \"np.ndarray\",\n    Y: list[list[int | float]] | \"np.ndarray\" | None = None,\n    c0: int | float = 0,\n) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n    Y : 2-D list or NumPy array with shape (M, C) or None.  If None, Y = X.\n    c0 : additive constant of the kernel (default 0).\n\n    Returns\n    -------\n    list[list[float]] | int\n        Kernel matrix rounded to 4 decimals, or \u20111 for incompatible shapes.\n    \"\"\"\n    # ---------- helper: convert to 2-D float array ----------\n    def _to_2d_float_array(obj):\n        arr = np.asarray(obj, dtype=float)\n        if arr.ndim == 0:               # scalar \u2192 1\u00d71\n            arr = arr.reshape(1, 1)\n        elif arr.ndim == 1:             # 1-D \u2192 row-vector 1\u00d7C\n            arr = arr.reshape(1, -1)\n        return arr\n\n    # Empty X \u2192 return empty list immediately\n    if isinstance(X, (list, tuple)) and len(X) == 0:\n        return []\n\n    X_arr = _to_2d_float_array(X)\n\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        # Empty X but non-empty Y: still convert X correctly\n        if isinstance(Y, (list, tuple)) and len(Y) == 0:\n            Y_arr = _to_2d_float_array(Y)\n        else:\n            Y_arr = _to_2d_float_array(Y)\n\n    # Validation rule: same number of columns\n    if X_arr.shape[1] != Y_arr.shape[1]:\n        return -1\n\n    # Compute linear kernel matrix\n    K = X_arr @ Y_arr.T\n    if c0 != 0:\n        K = K + c0\n\n    # Round to 4 decimal places and return as plain Python lists\n    return np.round(K.astype(float), 4).tolist()\n```"}
{"task_id": 340, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1.\n        beta1: Exponential decay rate for the first moment.\n        beta2: Exponential decay rate for the second moment.\n        eps:   Small constant to avoid division by zero.\n\n    Returns:\n        Tuple of python lists rounded to 8 decimal places:\n        (updated_parameter, new_moment, new_variance)\n    \"\"\"\n    # Ensure all inputs are numpy arrays of float type (no copies if not needed)\n    param = np.asarray(param, dtype=np.float64)\n    grad  = np.asarray(grad,  dtype=np.float64)\n    m     = np.asarray(m,     dtype=np.float64)\n    v     = np.asarray(v,     dtype=np.float64)\n\n    # Update biased first and second raw moments\n    m_t = beta1 * m + (1.0 - beta1) * grad\n    v_t = beta2 * v + (1.0 - beta2) * (grad ** 2)\n\n    # Bias-corrected moments\n    m_hat = m_t / (1.0 - beta1 ** t)\n    v_hat = v_t / (1.0 - beta2 ** t)\n\n    # Parameter update (freeze if gradient is exactly zero everywhere)\n    if np.all(grad == 0):\n        param_t = param.copy()\n    else:\n        param_t = param - lr * m_hat / (np.sqrt(v_hat) + eps)\n\n    # Helper: convert ndarray to nested python list rounded to 8 decimals\n    def to_rounded_list(arr: np.ndarray) -> list:\n        return np.round(arr, 8).tolist()\n\n    return (\n        to_rounded_list(param_t),\n        to_rounded_list(m_t),\n        to_rounded_list(v_t),\n    )\n```"}
{"task_id": 343, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists with the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    # Ensure rewards is a NumPy array\n    rewards = np.asarray(rewards)\n\n    # How many samples to retain (at least one)\n    n_samples = theta_samples.shape[0]\n    k = max(1, int(retain_prcnt * n_samples))\n\n    # Indices of the top-k rewards (higher reward \u21d2 better)\n    elite_idx = np.argsort(rewards)[::-1][:k]\n\n    # Elite parameter vectors\n    elite_samples = theta_samples[elite_idx]\n\n    # Per-dimension statistics\n    mean = np.mean(elite_samples, axis=0)\n    var  = np.var(elite_samples, axis=0)  # population variance (ddof=0)\n\n    # Round to 4 decimals and convert to Python lists\n    mean_rounded = np.round(mean, 4).tolist()\n    var_rounded  = np.round(var, 4).tolist()\n\n    return mean_rounded, var_rounded\n```"}
{"task_id": 353, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n                class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    # Convert the (possibly lazy) iterable to a list so we can compute its length\n    label_list = list(labels)\n\n    # Special case 1: empty input\n    if not label_list:                       # works for list/tuple/np.ndarray converted to list\n        return 0.0\n\n    total = len(label_list)\n\n    # Count occurrences of each label\n    counts = Counter(label_list)\n\n    # Special case 2: all labels identical (only one unique key)\n    if len(counts) == 1:\n        return 0.0\n\n    # Compute entropy\n    entropy_val = 0.0\n    for count in counts.values():\n        p = count / total\n        # skip p == 0 to avoid log(0); by convention it contributes 0\n        if p > 0.0:\n            entropy_val -= p * math.log2(p)\n\n    # Round to five decimal places, as required\n    return round(entropy_val, 5)\n```"}
{"task_id": 354, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _parse_padding(pad, k_r, k_c, dil, stride, in_r, in_c):\n    \"\"\"Returns the 4 individual paddings (top, bottom, left, right).\"\"\"\n    # Effective kernel size with dilation\n    eff_k_r = k_r + (k_r - 1) * dil\n    eff_k_c = k_c + (k_c - 1) * dil\n\n    # Integer\n    if isinstance(pad, int):\n        return pad, pad, pad, pad\n\n    # 2-tuple\n    if isinstance(pad, tuple) and len(pad) == 2:\n        pr, pc = pad\n        return pr, pr, pc, pc\n\n    # 4-tuple\n    if isinstance(pad, tuple) and len(pad) == 4:\n        return pad\n\n    # 'same'  \u2192 smallest symmetric padding that keeps the\n    # output size equal to the input size (only strictly true for stride=1)\n    if pad == \"same\":\n        total_pad_r = max(eff_k_r - 1, 0)\n        total_pad_c = max(eff_k_c - 1, 0)\n\n        pr1 = total_pad_r // 2\n        pr2 = total_pad_r - pr1\n\n        pc1 = total_pad_c // 2\n        pc2 = total_pad_c - pc1\n        return pr1, pr2, pc1, pc2\n\n    # Fallback (undefined \u2013 per problem statement)\n    return 0, 0, 0, 0\n\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"Performs a 2-D convolution (cross-correlation) via im2col + GEMM.\"\"\"\n    # -----------------------------------------------------------\n    # Shapes & parameters\n    # -----------------------------------------------------------\n    n_ex, in_r, in_c, in_ch = X.shape\n    k_r, k_c, in_ch_w, out_ch = W.shape       # kernel dims\n\n    assert in_ch == in_ch_w, \"Input / kernel channels mismatch\"\n\n    dil = dilation                          # keep name short\n    eff_k_r = k_r + (k_r - 1) * dil         # effective kernel size (rows)\n    eff_k_c = k_c + (k_c - 1) * dil         # effective kernel size (cols)\n\n    # -----------------------------------------------------------\n    # Padding\n    # -----------------------------------------------------------\n    pr1, pr2, pc1, pc2 = _parse_padding(pad, k_r, k_c, dil, stride, in_r, in_c)\n\n    X_pad = np.pad(\n        X,\n        pad_width=((0, 0), (pr1, pr2), (pc1, pc2), (0, 0)),\n        mode=\"constant\",\n    )\n\n    pad_r, pad_c = in_r + pr1 + pr2, in_c + pc1 + pc2\n\n    # -----------------------------------------------------------\n    # Output spatial dimensions\n    # -----------------------------------------------------------\n    out_r = (pad_r - eff_k_r) // stride + 1\n    out_c = (pad_c - eff_k_c) // stride + 1\n\n    # -----------------------------------------------------------\n    # im2col \u2013 build the big \u201cpatch\u201d matrix\n    # Resulting shape: (n_ex*out_r*out_c, k_r*k_c*in_ch)\n    # -----------------------------------------------------------\n    cols = np.empty(\n        (n_ex, out_r, out_c, k_r, k_c, in_ch), dtype=X.dtype\n    )\n\n    for kr in range(k_r):\n        row_start = kr * (dil + 1)\n        row_end = row_start + stride * out_r\n        row_slice = slice(row_start, row_end, stride)\n\n        for kc in range(k_c):\n            col_start = kc * (dil + 1)\n            col_end = col_start + stride * out_c\n            col_slice = slice(col_start, col_end, stride)\n\n            cols[:, :, :, kr, kc, :] = X_pad[:, row_slice, col_slice, :]\n\n    # reshape to 2-D matrix (im2col)\n    cols_2d = cols.reshape(n_ex * out_r * out_c, -1)  # (N*OH*OW, KR*KC*IC)\n\n    # -----------------------------------------------------------\n    # Reshape kernels and perform GEMM\n    # -----------------------------------------------------------\n    W_col = W.reshape(-1, out_ch)  # (KR*KC*IC, OC)\n\n    out_mat = cols_2d @ W_col      # (N*OH*OW, OC)\n\n    # -----------------------------------------------------------\n    # Back to 4-D tensor and to Python list\n    # -----------------------------------------------------------\n    out = out_mat.reshape(n_ex, out_r, out_c, out_ch)\n\n    return out.tolist()\n```"}
{"task_id": 355, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _deterministic_sign_flip(components: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Flip the sign of each component so that the first non\u2013zero loading is \u2265 0.\n    The operation is done in-place and the resulting array is also returned.\n    \"\"\"\n    for i in range(components.shape[1]):                      # iterate over columns\n        col = components[:, i]\n        # locate the first (in feature order) loading whose absolute value\n        # is not numerically zero\n        for v in col:\n            if abs(v) > 1e-12:\n                if v < 0:\n                    components[:, i] *= -1                    # flip the column\n                break                                         # go to next column\n    return components\n\n\ndef pca_transform(\n    data: np.ndarray,\n    n_components: int,\n    solver: str = \"svd\"\n) -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis with a deterministic sign convention.\"\"\"\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D array (samples \u00d7 features).\")\n    if n_components < 1:\n        raise ValueError(\"`n_components` must be at least 1.\")\n    if solver not in {\"svd\", \"eigen\"}:\n        raise ValueError(\"`solver` must be either 'svd' or 'eigen'.\")\n\n    n_samples, n_features = data.shape\n    n_components = min(n_components, n_features)              # cannot keep more than #features\n\n    # ------------------------------------------------------------------\n    # 1. Mean-centre the data\n    # ------------------------------------------------------------------\n    X = data.astype(float) - data.mean(axis=0, keepdims=True)\n\n    # ------------------------------------------------------------------\n    # 2. Obtain principal directions + explained variance\n    # ------------------------------------------------------------------\n    if solver == \"svd\":\n        # full_matrices=False gives Vt of shape (min(n_samples,n_features), n_features)\n        _, singular_vals, Vt = np.linalg.svd(X, full_matrices=False)\n        components = Vt.T[:, :n_components]                   # (n_features, n_components)\n        # eigenvalues from singular values\n        expl_var = (singular_vals**2) / (n_samples - 1)\n    else:                                                     # solver == \"eigen\"\n        cov = np.cov(X, rowvar=False, ddof=1)                 # (n_features, n_features)\n        eig_vals, eig_vecs = np.linalg.eigh(cov)              # ascending order\n        idx = eig_vals.argsort()[::-1]                        # descending\n        eig_vals, eig_vecs = eig_vals[idx], eig_vecs[:, idx]\n        components = eig_vecs[:, :n_components]               # (n_features, n_components)\n        expl_var = eig_vals                                   # already variance values\n\n    total_variance = expl_var.sum()\n    explained_variance_ratio = (expl_var / total_variance)[:n_components]\n\n    # ------------------------------------------------------------------\n    # 3. Deterministic sign fix\n    # ------------------------------------------------------------------\n    components = _deterministic_sign_flip(components)\n\n    # ------------------------------------------------------------------\n    # 4. Project the centred data\n    # ------------------------------------------------------------------\n    scores = X @ components                                   # (n_samples, n_components)\n\n    # Every column that was flipped in components is already consistent\n    # because we used the flipped components for the projection.\n\n    # ------------------------------------------------------------------\n    # 5. Round the FINAL results and convert to Python built-ins\n    # ------------------------------------------------------------------\n    scores_rounded = np.round(scores, 4).tolist()\n    evr_rounded = np.round(explained_variance_ratio, 4).tolist()\n\n    return scores_rounded, evr_rounded\n```"}
{"task_id": 356, "completion_id": 0, "completion": "```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                    classification tree; otherwise treat it as regression.\n\n    Returns\n    -------\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    # Classification: pick the index of the maximum probability\n    if classifier:\n        # Make sure we can index the sequence (works for list, tuple, np.ndarray, etc.)\n        probs = list(leaf.value)\n\n        best_idx = 0\n        best_val = probs[0]\n        for idx, val in enumerate(probs[1:], start=1):\n            # Strict \">\" guarantees that the first (smallest index) wins ties,\n            # matching numpy.argmax's behaviour.\n            if val > best_val:\n                best_val = val\n                best_idx = idx\n        return best_idx\n\n    # Regression: just return the stored scalar\n    return leaf.value\n```"}
{"task_id": 357, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array of shape (n_samples, n_features).\n    K : int\n        Number of clusters.\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Sorted centroids, rounded to four decimals.\n    \"\"\"\n    # Ensure 2-D float array\n    data = np.asarray(data, dtype=float)\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D array\")\n\n    n_samples, n_features = data.shape\n    if K < 1 or K > n_samples:\n        raise ValueError(\"K must be between 1 and the number of samples\")\n\n    # Special case K == 1\n    if K == 1:\n        centroid = np.mean(data, axis=0, keepdims=True)\n        centroid = np.round(centroid, 4)\n        return centroid.tolist()\n\n    # 1. Deterministic initialisation with first K samples\n    centroids = data[:K].astype(float, copy=True)\n\n    tol = 1e-6\n    for _ in range(max_iters):\n        # 2a. Assign each sample to closest centroid (Euclidean)\n        #     Compute squared distances for efficiency\n        diff = data[:, None, :] - centroids[None, :, :]     # shape (n_samples, K, n_features)\n        dists = np.sum(diff ** 2, axis=2)                   # shape (n_samples, K)\n        labels = np.argmin(dists, axis=1)                   # shape (n_samples,)\n\n        # 2b. Recompute centroids\n        new_centroids = centroids.copy()\n        for k in range(K):\n            members = data[labels == k]\n            if members.size:                                # only update if cluster non-empty\n                new_centroids[k] = members.mean(axis=0)\n\n        # 2c. Convergence test\n        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) <= tol):\n            centroids = new_centroids\n            break\n\n        centroids = new_centroids\n\n    # 3. Sort lexicographically and round to four decimals\n    sort_keys = tuple(centroids[:, i] for i in reversed(range(n_features)))\n    order = np.lexsort(sort_keys)\n    centroids_sorted = np.round(centroids[order], 4)\n\n    return centroids_sorted.tolist()\n```"}
{"task_id": 362, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]] | int:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row is divided by its own sum so that every row finally sums to 1.\n    If any row has a sum of exactly 0 normalisation is impossible and \u20131\n    is returned.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The row-normalised matrix (rounded to 4 decimals) or \u20131 when a\n        zero-sum row is encountered.\n    \"\"\"\n    # 1. Convert to float NumPy array\n    arr = np.asarray(data, dtype=float)\n\n    # Ensure a proper 2-D structure\n    if arr.ndim != 2:\n        raise ValueError(\"Input must be a 2-D structure\")\n\n    # 2. Row sums\n    row_sums = arr.sum(axis=1)\n\n    # 3. Check for zero-sum rows\n    if np.any(row_sums == 0):\n        return -1\n\n    # 4. Row-wise normalisation (broadcasting)\n    stochastic = arr / row_sums[:, None]\n\n    # 5. Round to 4 decimals and convert back to Python list\n    return np.round(stochastic, 4).tolist()\n```\n\nExample usage\n-------------\n```python\n>>> row_stochastic_matrix([[1, 2], [3, 3]])\n[[0.3333, 0.6667], [0.5, 0.5]]\n\n>>> row_stochastic_matrix([[0, 0], [1, 2]])\n-1\n```"}
{"task_id": 363, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite\n    (`Tuple` or `Dict`) spaces the space is continuous only if **all** its\n    immediate sub-spaces are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: True if the action space is known to be composite.\n        tuple_obs:    True if the observation space is known to be composite.\n\n    Returns:\n        A tuple (cont_action, cont_obs) with the continuity result for the\n        action and observation spaces, respectively.\n    \"\"\"\n    def is_simple_box(space: Space) -> bool:\n        \"\"\"Return True only when `space` itself is a `Box`.\"\"\"\n        return isinstance(space, Box)\n\n    def all_subspaces_box(space: Space) -> bool:\n        \"\"\"Return True if every direct sub-space of `space` is a `Box`.\"\"\"\n        if isinstance(space, Tuple):\n            subspaces = space.spaces\n        elif isinstance(space, Dict):\n            subspaces = space.spaces.values()\n        else:  # Not a composite space\n            return False\n        return all(isinstance(s, Box) for s in subspaces)\n\n    # Evaluate action space\n    if tuple_action:\n        cont_action = all_subspaces_box(env.action_space)\n    else:\n        cont_action = is_simple_box(env.action_space)\n\n    # Evaluate observation space\n    if tuple_obs:\n        cont_obs = all_subspaces_box(env.observation_space)\n    else:\n        cont_obs = is_simple_box(env.observation_space)\n\n    return cont_action, cont_obs\n```"}
{"task_id": 369, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        raise ValueError(\"`updates` must contain at least one tree output.\")\n\n    # Stack all tree outputs: shape \u2192 (n_trees, *) where * is (n_samples,) or (n_samples, n_classes)\n    stacked_updates = np.stack(updates, axis=0)\n\n    # Running prediction starts at zero and every tree is *subtracted* after learning-rate scaling\n    running_pred = -learning_rate * stacked_updates.sum(axis=0)\n\n    if regression:\n        # 1-D array, rounded to 4 decimal places\n        return np.round(running_pred.astype(float), 4)\n\n    # ----------  multi-class classification  ----------\n    # Soft-max (numerically stable)\n    logits = running_pred                          # shape (n_samples, n_classes)\n    logits -= logits.max(axis=1, keepdims=True)    # stability trick\n    exp_logits = np.exp(logits)\n    probas = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n\n    # Argmax \u2192 predicted class indices\n    return probas.argmax(axis=1)\n```"}
{"task_id": 371, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef conv1D(X: np.ndarray,\n           W: np.ndarray,\n           stride: int,\n           pad,\n           dilation: int = 0) -> list:\n    \"\"\"1-D cross-correlation (a.k.a. \u201cconvolution\u201d in DL frameworks).\n\n    Parameters\n    ----------\n    X : ndarray, shape (batch, signal_length, in_channels)\n        Input signals.\n    W : ndarray, shape (kernel_width, in_channels, out_channels)\n        Convolution kernels (not flipped \u2013 cross-correlation).\n    stride : int\n        Step size of the sliding window.\n    pad : int | tuple(int, int) | \"same\"\n        Zero padding to the left / right of the signal.\n    dilation : int, default 0\n        Number of zeros inserted *between* kernel elements\n        (0 \u21d2 normal, contiguous kernel).\n\n    Returns\n    -------\n    list\n        Convolved output with shape\n        (batch, output_length, out_channels), converted via ``tolist()``.\n    \"\"\"\n    # ------------- basic sanity checks -------------------------------------\n    if X.ndim != 3:\n        raise ValueError('X must have shape (batch, length, in_ch).')\n    if W.ndim != 3:\n        raise ValueError('W must have shape (k_width, in_ch, out_ch).')\n    if X.shape[2] != W.shape[1]:\n        raise ValueError('Input-channel dimensions of X and W differ.')\n    if stride <= 0:\n        raise ValueError('stride must be a positive integer.')\n    if dilation < 0:\n        raise ValueError('dilation must be non-negative.')\n\n    batch, L_in, C_in = X.shape\n    K, _, C_out = W.shape\n    eff_K = (K - 1) * (dilation + 1) + 1      # effective receptive field\n\n    # ------------- determine padding amounts --------------------------------\n    if isinstance(pad, str):\n        pad = pad.lower()\n    if pad == \"same\":\n        # TensorFlow\u2019s SAME rule\n        out_len = int(np.ceil(L_in / stride))\n        total_pad = max(0, (out_len - 1) * stride + eff_K - L_in)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n    elif isinstance(pad, int):\n        p_left = p_right = int(pad)\n    elif (isinstance(pad, (tuple, list)) and len(pad) == 2\n          and all(isinstance(x, int) for x in pad)):\n        p_left, p_right = pad\n    else:\n        raise ValueError('pad must be int, 2-tuple or \"same\".')\n\n    if p_left < 0 or p_right < 0:\n        raise ValueError('Padding values must be non-negative.')\n\n    # ------------- pad the input signal -------------------------------------\n    X_pad = np.pad(X,\n                   ((0, 0), (p_left, p_right), (0, 0)),\n                   mode='constant')\n\n    L_pad = L_in + p_left + p_right\n\n    # ------------- output length --------------------------------------------\n    out_len = (L_pad - eff_K) // stride + 1\n    if out_len <= 0:\n        # Nothing fits \u2013 return an empty tensor with proper shape.\n        return np.empty((batch, 0, C_out)).tolist()\n\n    # ------------- build the index matrix for im2col ------------------------\n    start_idx = np.arange(out_len) * stride                     # (out_len,)\n    kernel_offset = np.arange(K) * (dilation + 1)               # (K,)\n    idx = start_idx[:, None] + kernel_offset[None, :]           # (out_len, K)\n\n    # ------------- gather sliding windows -----------------------------------\n    # Resulting shape: (batch, out_len, K, C_in)\n    windows = X_pad[:, idx, :]\n\n    # ------------- correlation via tensordot -------------------------------\n    # Match axes: windows(..., K, C_in)  with  W(K, C_in, C_out)\n    out = np.tensordot(windows, W, axes=([2, 3], [0, 1]))       # (batch, out_len, C_out)\n\n    return out.tolist()\n```"}
{"task_id": 373, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    # Convert to a 1-D NumPy array\n    y = np.asarray(y).ravel()\n\n    # Empty input \u2192 impurity 0.0\n    if y.size == 0:\n        return 0.0\n\n    # Count occurrences of each class\n    _, counts = np.unique(y, return_counts=True)\n\n    # Relative frequencies p_c\n    probs = counts / counts.sum()\n\n    # Gini impurity\n    impurity = 1.0 - np.sum(probs ** 2)\n\n    # Round to 4 decimal places and return as Python float\n    return float(np.round(impurity, 4))\n```"}
{"task_id": 374, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray,\n                         y: np.ndarray,\n                         learning_rate: float = 0.01,\n                         n_iterations: int = 10_000\n                         ) -> tuple[list[float], float]:\n    \"\"\"Train a multiple-linear\u2010regression model by batch gradient descent.\n\n    Parameters\n    ----------\n    X : np.ndarray (m \u00d7 n)\n        Feature matrix.\n    y : np.ndarray (m,)\n        Target vector.\n    learning_rate : float, default 0.01\n        Step size for gradient descent.\n    n_iterations : int, default 10_000\n        Number of gradient\u2010descent steps.\n\n    Returns\n    -------\n    (weights_list, bias) : tuple[list[float], float]\n        Learned parameters rounded to 4 decimal places.\n        If X and y have inconsistent sample sizes, returns -1.\n    \"\"\"\n    # --- Basic input checking -------------------------------------------------\n    if X.ndim != 2:\n        X = np.atleast_2d(X)\n    y = np.ravel(y)                        # ensure 1-D\n\n    m, n = X.shape\n    if m != y.shape[0] or m == 0:          # inconsistent or empty data\n        return -1\n\n    # --- Initialise parameters -----------------------------------------------\n    theta = np.zeros(n, dtype=float)       # weights\n    b = 0.0                                # bias\n\n    # --- Gradient Descent -----------------------------------------------------\n    for _ in range(n_iterations):\n        y_hat = X @ theta + b              # predictions, shape (m,)\n\n        error = y_hat - y                  # residuals\n        dw = (2 / m) * (X.T @ error)       # gradient w.r.t. weights\n        db = (2 / m) * np.sum(error)       # gradient w.r.t. bias\n\n        theta -= learning_rate * dw\n        b     -= learning_rate * db\n\n    # --- Rounding and output --------------------------------------------------\n    theta_rounded = np.round(theta, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n\n    return theta_rounded, b_rounded\n```"}
{"task_id": 376, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if and only if it is square and equal (within\n    numerical tolerance) to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists of numbers.\n\n    Returns:\n        True if *X* is symmetric, otherwise False.\n    \"\"\"\n    # Empty input \u2192 not a valid square matrix\n    if not X:\n        return False\n\n    n_rows = len(X)\n\n    # Verify each row has the same length as the number of rows\n    for row in X:\n        if len(row) != n_rows:\n            return False            # Not square (or ragged)\n\n    # Convert to NumPy array (now guaranteed to be square)\n    A = np.asarray(X, dtype=float)\n\n    # Compare with its transpose using a tight tolerance\n    return np.allclose(A, A.T, atol=1e-8, rtol=0.0)\n```"}
{"task_id": 377, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Compute the negative gradient that Gradient Boosting uses when minimising the\n    logistic (binomial-deviance) loss for binary classification.\n    \n    Parameters\n    ----------\n    y : list or 1-D array-like of int (0 or 1)\n        Ground-truth binary labels.\n    f : list or 1-D array-like of float\n        Current prediction scores.\n    \n    Returns\n    -------\n    list\n        Negative gradients for each sample, rounded to 4 decimal places.\n        If `y` contains values other than 0 or 1, returns -1.\n    \"\"\"\n    # Convert to numpy arrays\n    y_arr = np.asarray(y)\n    f_arr = np.asarray(f, dtype=float)\n\n    # Validate labels\n    if not np.all(np.isin(y_arr, [0, 1])):\n        return -1\n\n    # Convert labels from {0,1} to {-1,1}\n    y_prime = 2 * y_arr - 1        # shape (n_samples,)\n\n    # Compute negative gradient: g_i = y'_i / (1 + exp(y'_i * f_i))\n    g = y_prime / (1.0 + np.exp(y_prime * f_arr))\n\n    # Round to 4 decimal places and return as Python list\n    return np.round(g, 4).tolist()\n```"}
{"task_id": 380, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n# SELU constants from the original paper\n_ALPHA = 1.6732632423543772848170429916717\n_SCALE = 1.0507009873554804934193349852946\n\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its first / second derivative.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Works for scalars and any-shaped arrays/lists.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 d(SELU)/dx\n        2 \u2192 d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    float | list\n        Result rounded to 4 decimals.  For array-like inputs the shape is\n        preserved and a Python list is returned; for scalar input a float\n        is returned.\n    \"\"\"\n    # convert input to ndarray\n    x_arr = np.asarray(x, dtype=np.float64)\n\n    # masks for the two SELU regions\n    pos_mask = x_arr > 0\n    neg_mask = ~pos_mask  # includes x <= 0\n\n    if order == 0:  # SELU itself\n        out = np.empty_like(x_arr)\n        out[pos_mask] = _SCALE * x_arr[pos_mask]\n        out[neg_mask] = _SCALE * _ALPHA * (np.exp(x_arr[neg_mask]) - 1)\n\n    elif order == 1:  # first derivative\n        out = np.empty_like(x_arr)\n        out[pos_mask] = _SCALE\n        out[neg_mask] = _SCALE * _ALPHA * np.exp(x_arr[neg_mask])\n\n    elif order == 2:  # second derivative\n        out = np.empty_like(x_arr)\n        out[pos_mask] = 0.0\n        # for x <= 0 the 2nd derivative equals the first derivative there\n        out[neg_mask] = _SCALE * _ALPHA * np.exp(x_arr[neg_mask])\n\n    else:\n        raise ValueError(\"order must be 0, 1 or 2\")\n\n    # round to 4 decimals and convert back\n    rounded = np.round(out, 4)\n\n    # for scalar inputs return a scalar, else a (nested) Python list\n    return rounded.tolist()\n```"}
{"task_id": 387, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gradient_boosting_1d(\n    x: list[float],\n    y: list[float],\n    n_estimators: int,\n    learning_rate: float = 0.1,\n) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\"\"\"\n    # --- data to numpy -------------------------------------------------------\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n = y.size\n    if n == 0:\n        return []\n\n    # --- initial (round-0) prediction ---------------------------------------\n    pred = np.full(n, y.mean())\n\n    # ------------------------------------------------------------------------\n    for _ in range(n_estimators):\n        residuals = y - pred  # negative gradients\n\n        # --------------------------------------------------------------------\n        # CASE 1 \u2013 all x are identical  \u2192  one-side stump with global mean\n        # --------------------------------------------------------------------\n        if np.all(x == x[0]):\n            stump_out = np.full(n, residuals.mean())\n\n        # --------------------------------------------------------------------\n        # CASE 2 \u2013 we can really split the data\n        # --------------------------------------------------------------------\n        else:\n            order = np.argsort(x)\n            xs = x[order]\n            rs = residuals[order]\n\n            # prefix sums for fast SSE computation\n            csum = np.cumsum(rs)\n            csum_sq = np.cumsum(rs ** 2)\n\n            total_sum = csum[-1]\n            total_sq = csum_sq[-1]\n\n            # left block sizes 1 \u2026 n-1\n            k_left = np.arange(1, n)\n            k_right = n - k_left\n\n            sum_left = csum[:-1]\n            sum_sq_left = csum_sq[:-1]\n\n            sum_right = total_sum - sum_left\n            sum_sq_right = total_sq - sum_sq_left\n\n            mean_left = sum_left / k_left\n            mean_right = sum_right / k_right\n\n            sse_left = sum_sq_left - k_left * mean_left**2\n            sse_right = sum_sq_right - k_right * mean_right**2\n            total_sse = sse_left + sse_right\n\n            # we may only split between *different* consecutive x values\n            valid = xs[:-1] != xs[1:]\n            if not np.any(valid):  # this should not happen (all-x-identical branch)\n                stump_out = np.full(n, residuals.mean())\n            else:\n                best_pos = np.argmin(np.where(valid, total_sse, np.inf))\n                thr = (xs[best_pos] + xs[best_pos + 1]) / 2.0\n\n                mu_left = mean_left[best_pos]\n                mu_right = mean_right[best_pos]\n\n                stump_out = np.where(x <= thr, mu_left, mu_right)\n\n        # --- update running prediction --------------------------------------\n        pred += learning_rate * stump_out\n\n    # round & return python list\n    return np.round(pred, 4).tolist()\n```"}
{"task_id": 394, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np     # (kept twice as required by the template)\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` is used.\n    learning_rate : float, optional\n        Step size (\u03b1). Default is 0.01.\n    rho : float, optional\n        Decay rate (\u03c1). Default is 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        Tuple ``(w_next, Eg_next)`` \u2013 both rounded to four decimal places and\n        converted to regular Python lists.\n    \"\"\"\n    \n    # 1. Initialise Eg if necessary\n    if Eg is None:\n        Eg = np.zeros_like(grad, dtype=float)\n    \n    epsilon = 1e-8\n    \n    # 2. Update the running average of squared gradients\n    Eg_next = rho * Eg + (1.0 - rho) * (grad ** 2)\n    \n    # 3. Parameter update\n    w_next = w - (learning_rate * grad) / np.sqrt(Eg_next + epsilon)\n    \n    # 4. Round to 4 decimal places and convert to Python lists\n    w_next_rounded = np.round(w_next, 4).tolist()\n    Eg_next_rounded = np.round(Eg_next, 4).tolist()\n    \n    return w_next_rounded, Eg_next_rounded\n```"}
{"task_id": 398, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient element-wise.\n\n    Args:\n        x: 1-D list or NumPy array of numeric values.\n        alpha: Positive ELU hyper-parameter (default 0.1).\n        derivative: If True, return the gradient; otherwise return the activation.\n\n    Returns:\n        Python list of floats rounded to 4 decimal places.\n    \"\"\"\n    # Convert the input to a NumPy array for vectorised computation\n    x_arr = np.asarray(x, dtype=float)\n\n    # Masks for positive (x >= 0) and negative (x < 0) elements\n    pos_mask = x_arr >= 0\n    neg_mask = ~pos_mask\n\n    if not derivative:                                  # ----- ELU -----\n        out = x_arr.copy()\n        out[neg_mask] = alpha * (np.exp(x_arr[neg_mask]) - 1)\n    else:                                               # ---- d(ELU)/dx ----\n        out = np.ones_like(x_arr)\n        # ELU'(x) for x < 0  -->  ELU(x) + alpha = alpha * e^x\n        out[neg_mask] = alpha * np.exp(x_arr[neg_mask])\n\n    # Round to 4 decimal places and return as a Python list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 411, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numbers import Integral, Real\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute simple statistics of action / observation trajectories.\"\"\"\n    \n    def is_iterable(x):\n        \"\"\"Return True if *x* is a list/tuple/ndarray with length.\"\"\"\n        return isinstance(x, (list, tuple, np.ndarray)) and not np.isscalar(x)\n    \n    def analyse(collection):\n        \"\"\"\n        Inspect a list of (possibly-vector) values and return\n        tuple_flag  \u2026 at least one element is an iterable\n        multi_dim   \u2026 dimensionality > 1\n        continuous  \u2026 at least one value is a non-integer float\n        n_per_dim   \u2026 list with #unique values per dimension\n        dim         \u2026 dimensionality\n        ids         \u2026 list with sorted unique ids per dimension\n        \"\"\"\n        if len(collection) == 0:\n            raise ValueError(\"Empty trajectory supplied.\")\n        \n        tuple_flag = any(is_iterable(x) for x in collection)\n        \n        # find dimensionality\n        if tuple_flag:\n            # find first iterable element to determine length\n            for sample in collection:\n                if is_iterable(sample):\n                    dim = len(sample)\n                    break\n        else:\n            dim = 1\n        \n        multi_dim = dim > 1\n        \n        # prepare containers\n        uniq_sets = [set() for _ in range(dim)]\n        continuous = False\n        \n        for sample in collection:\n            if tuple_flag:\n                # convert to 1-D sequence\n                values = list(sample)\n                if len(values) != dim:\n                    raise ValueError(\"Inconsistent dimensionality in data.\")\n            else:\n                values = [sample]\n            \n            for d, val in enumerate(values):\n                uniq_sets[d].add(val)\n                \n                # discrete / continuous test\n                if not continuous:          # quit test as soon as True\n                    if isinstance(val, (Real, np.floating, np.integer)):\n                        if isinstance(val, (Integral, np.integer)):\n                            continue        # definitely discrete\n                        # real number \u2013 check if fractional\n                        if not float(val).is_integer():\n                            continuous = True\n                    else:\n                        # anything non-numeric \u2013 treat as continuous\n                        continuous = True\n        \n        ids = [sorted(list(s)) for s in uniq_sets]\n        n_per_dim = [len(s) for s in uniq_sets]\n        \n        return tuple_flag, multi_dim, continuous, n_per_dim, dim, ids\n    \n    # ---------- process both collections ----------\n    (tuple_a, multi_a, cont_a, n_a, dim_a, ids_a) = analyse(actions)\n    (tuple_o, multi_o, cont_o, n_o, dim_o, ids_o) = analyse(observations)\n    \n    return {\n        'tuple_actions':        tuple_a,\n        'tuple_observations':   tuple_o,\n        'multidim_actions':     multi_a,\n        'multidim_observations':multi_o,\n        'continuous_actions':   cont_a,\n        'continuous_observations': cont_o,\n        'n_actions_per_dim':    n_a,\n        'action_dim':           dim_a,\n        'n_obs_per_dim':        n_o,\n        'obs_dim':              dim_o,\n        'action_ids':           ids_a,\n        'obs_ids':              ids_o\n    }\n```\n\nThe helper function `analyse` performs all required computations on one trajectory, and the main `env_stats` function simply calls it for both the actions and the observations and arranges the returned values in the requested dictionary format."}
{"task_id": 413, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\nclass _TreeNode:\n    \"\"\"A single node in a CART regression tree.\"\"\"\n    __slots__ = (\"is_leaf\", \"value\", \"feature\", \"threshold\", \"left\", \"right\")\n\n    def __init__(self, *, value=None, feature=None, threshold=None,\n                 left=None, right=None, is_leaf=False):\n        self.is_leaf = is_leaf\n        self.value = value            # float, mean of y in this node\n        self.feature = feature        # int\n        self.threshold = threshold    # float\n        self.left = left              # _TreeNode\n        self.right = right            # _TreeNode\n\n\nclass _CARTRegressor:\n    \"\"\"Very small, depth\u2013limited CART regressor (MSE criterion).\"\"\"\n\n    def __init__(self, max_depth: int = 3):\n        self.max_depth = max_depth\n        self.root: _TreeNode | None = None\n\n    # ---------- public helpers ----------\n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        self.root = self._grow(X, y, depth=0)\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        # vectorised traversal\n        preds = np.empty(X.shape[0], dtype=float)\n        for i, x in enumerate(X):\n            preds[i] = self._predict_row(self.root, x)\n        return preds\n\n    # ---------- private helpers ----------\n    def _predict_row(self, node: _TreeNode, x: np.ndarray) -> float:\n        while not node.is_leaf:\n            node = node.left if x[node.feature] <= node.threshold else node.right\n        return node.value\n\n    def _grow(self, X: np.ndarray, y: np.ndarray, depth: int) -> _TreeNode:\n        n_samples, n_feats = X.shape\n        # leaf if depth limit, pure node, or not enough points\n        if depth >= self.max_depth or n_samples <= 1 or np.all(y == y[0]):\n            return _TreeNode(value=float(y.mean()), is_leaf=True)\n\n        # current impurity (sum of squared errors)\n        best_sse = np.var(y) * n_samples\n        best_feat, best_thr, best_split = None, None, None\n\n        for feat in range(n_feats):\n            x_col = X[:, feat]\n            # sort to evaluate splits fast\n            sorted_idx = np.argsort(x_col)\n            x_sorted, y_sorted = x_col[sorted_idx], y[sorted_idx]\n\n            # unique candidate thresholds \u2013 mid-points between distinct values\n            uniq_vals = np.unique(x_sorted)\n            if uniq_vals.size == 1:       # cannot split on constant column\n                continue\n            thr_candidates = (uniq_vals[:-1] + uniq_vals[1:]) / 2.0\n\n            # cumulative sums for fast left/right statistics\n            y_cumsum = np.cumsum(y_sorted)\n            y_sq_cumsum = np.cumsum(y_sorted ** 2)\n\n            for idx, thr in enumerate(thr_candidates, start=1):\n                # position where x <= thr  (since x_sorted is ascending)\n                # find index of last value <= thr \u21d2 here it is idx because\n                # thr lies between uniq_vals[idx-1] and uniq_vals[idx]\n                n_left = np.searchsorted(x_sorted, thr, side=\"right\")\n                # quick skip if split would put all points on one side\n                if n_left == 0 or n_left == n_samples:\n                    continue\n\n                # left stats\n                sum_y_left = y_cumsum[n_left - 1]\n                sum_yy_left = y_sq_cumsum[n_left - 1]\n                n_l = n_left\n                mse_left = sum_yy_left - (sum_y_left ** 2) / n_l\n\n                # right stats\n                sum_y_right = y_cumsum[-1] - sum_y_left\n                sum_yy_right = y_sq_cumsum[-1] - sum_yy_left\n                n_r = n_samples - n_l\n                mse_right = sum_yy_right - (sum_y_right ** 2) / n_r\n\n                sse_split = mse_left + mse_right\n                if sse_split < best_sse - 1e-7:   # tolerance to avoid tiny gains\n                    best_sse = sse_split\n                    best_feat = feat\n                    best_thr = thr\n                    best_split = n_left, sorted_idx  # store index break\n\n        # no useful split found \u2192 leaf\n        if best_feat is None:\n            return _TreeNode(value=float(y.mean()), is_leaf=True)\n\n        # build children\n        split_idx, sorted_idx = best_split\n        x_sorted_global = X[sorted_idx, best_feat]\n        mask_left = x_sorted_global <= best_thr\n        left_idx = sorted_idx[mask_left]\n        right_idx = sorted_idx[~mask_left]\n\n        left_child = self._grow(X[left_idx], y[left_idx], depth + 1)\n        right_child = self._grow(X[right_idx], y[right_idx], depth + 1)\n\n        return _TreeNode(feature=best_feat,\n                         threshold=best_thr,\n                         left=left_child,\n                         right=right_child,\n                         value=float(y.mean()),   # not used but nice to store\n                         is_leaf=False)\n\n\ndef gbdt_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    n_estimators: int = 10,\n    learning_rate: float = 0.1,\n    max_depth: int = 3,\n) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor (simplified).\"\"\"\n    # --- initial prediction ---\n    init_pred = y_train.mean()\n    y_hat_train = np.full_like(y_train, init_pred, dtype=float)\n    y_hat_test = np.full(X_test.shape[0], init_pred, dtype=float)\n\n    # --- iterative boosting ---\n    for _ in range(n_estimators):\n        residuals = y_train - y_hat_train              # negative gradient\n        tree = _CARTRegressor(max_depth=max_depth)\n        tree.fit(X_train, residuals)\n\n        # update predictions\n        y_hat_train += learning_rate * tree.predict(X_train)\n        y_hat_test += learning_rate * tree.predict(X_test)\n\n    # --- output ---\n    return np.round(y_hat_test, 4).tolist()\n```"}
{"task_id": 416, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Evaluate the multivariate normal (Gaussian) probability-density function for\n    every sample contained in `X`.\n\n    Parameters\n    ----------\n    X : array_like, shape (n_samples, n_features)  or  (n_samples,)\n        Data points at which the PDF will be evaluated.  If `X` is one\u2013dimensional\n        it is treated as having shape (n_samples, 1).\n    mean : array_like, shape (n_features,)\n        Mean vector (\u03bc) of the distribution.\n    cov : array_like, shape (n_features, n_features)\n        Positive-definite covariance matrix (\u03a3) of the distribution.\n\n    Returns\n    -------\n    list\n        A list whose *i-th* element is the PDF value for `X[i]`, rounded to\n        four decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X = np.asarray(X, dtype=float)\n    mean = np.asarray(mean, dtype=float)\n    cov = np.asarray(cov, dtype=float)\n\n    # If X is 1-D treat it as (n_samples, 1)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n\n    n_samples, n_features = X.shape\n\n    # Basic shape validations\n    if mean.shape[0] != n_features:\n        raise ValueError(\"Mean vector length must equal the number of features.\")\n    if cov.shape != (n_features, n_features):\n        raise ValueError(\"Covariance matrix shape must be (n_features, n_features).\")\n\n    # Inverse and determinant of covariance matrix\n    inv_cov = np.linalg.inv(cov)\n    det_cov = np.linalg.det(cov)\n\n    if det_cov <= 0:\n        raise ValueError(\"Covariance matrix must be positive-definite (determinant > 0).\")\n\n    # Constant coefficient 1 / sqrt((2\u03c0)^d * det(\u03a3))\n    coeff = 1.0 / math.sqrt(((2 * math.pi) ** n_features) * det_cov)\n\n    # Mahalanobis distance for every sample: (x-\u03bc)^T \u03a3^{-1} (x-\u03bc)\n    diff = X - mean                      # shape: (n_samples, n_features)\n    mahal = np.einsum('ij,jk,ik->i', diff, inv_cov, diff)  # shape: (n_samples,)\n\n    # Exponent part and the final pdf values\n    exponent = -0.5 * mahal\n    pdf_vals = coeff * np.exp(exponent)\n\n    # Round to 4 decimal places and return as a regular Python list\n    return [round(val, 4) for val in pdf_vals.tolist()]\n```"}
{"task_id": 419, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np   # (kept twice to respect the given header)\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,          # kept for signature completeness (not used)\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression \u2013 MAP predictive mean (rounded to 4 dp).\"\"\"\n    \n    # 1. Ensure 2-D/1-D numpy arrays\n    X      = np.asarray(X,      dtype=float)\n    y      = np.asarray(y,      dtype=float).ravel()\n    X_new  = np.asarray(X_new,  dtype=float)\n\n    # 2. (Optional) intercept handling\n    if fit_intercept:\n        ones_train = np.ones((X.shape[0], 1))\n        ones_test  = np.ones((X_new.shape[0], 1))\n        X      = np.hstack((ones_train, X))\n        X_new  = np.hstack((ones_test,  X_new))\n\n    N, D = X.shape   # D \u2013 number of parameters\n\n    # 3. Prior mean \u03bc\n    if np.isscalar(mu):\n        mu_vec = np.full(D, mu, dtype=float)\n    else:\n        mu_vec = np.asarray(mu, dtype=float).ravel()\n        if mu_vec.size != D:\n            raise ValueError(\"`mu` has incompatible length.\")\n    \n    # 4. Prior covariance V  \u2794  precision V_inv\n    if V is None:                  # Identity\n        V_inv = np.eye(D)\n    elif np.isscalar(V):           # Scalar multiple of I\n        if V == 0:\n            raise ValueError(\"Scalar `V` must be non-zero.\")\n        V_inv = np.eye(D) / float(V)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 1:        # Diagonal given as 1-D array\n            if np.any(V_arr == 0):\n                raise ValueError(\"Zero on the diagonal of `V` is not allowed.\")\n            V_inv = np.diag(1.0 / V_arr)\n        else:                      # Full square matrix\n            if V_arr.shape != (D, D):\n                raise ValueError(\"`V` shape incompatible with number of features.\")\n            V_inv = np.linalg.inv(V_arr)\n\n    # 5. Posterior covariance \u03a3_p and mean \u03bc_p\n    XtX   = X.T @ X\n    A     = V_inv + XtX                   # Posterior precision\n    Sigma_p = np.linalg.inv(A)            # Posterior covariance\n    rhs     = V_inv @ mu_vec + X.T @ y\n    mu_p    = Sigma_p @ rhs               # Posterior mean (MAP estimator)\n\n    # 6. Predictive MAP means for X_new\n    y_pred = X_new @ mu_p\n\n    # 7. Round to 4 decimal places and return as Python list\n    return [round(float(val), 4) for val in y_pred]\n```"}
{"task_id": 423, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute Root Mean Squared Logarithmic Error (RMSLE).\n\n    Returns\n    -------\n    float\n        RMSLE rounded to four decimal places, or -1 if validation fails.\n    \"\"\"\n    # 1. Basic validation \u2013 same length and non-empty\n    if not isinstance(actual, (list, tuple, np.ndarray)) or \\\n       not isinstance(predicted, (list, tuple, np.ndarray)) or \\\n       len(actual) != len(predicted) or len(actual) == 0:\n        return -1\n\n    # 2. Convert to numpy arrays and ensure numeric dtype\n    try:\n        actual_arr    = np.asarray(actual,    dtype=float)\n        predicted_arr = np.asarray(predicted, dtype=float)\n    except (ValueError, TypeError):\n        return -1  # Non-numeric value encountered\n\n    # 3. Check non-negativity\n    if np.any(actual_arr < 0) or np.any(predicted_arr < 0):\n        return -1\n\n    # 4. Compute RMSLE\n    log_diff = np.log1p(predicted_arr) - np.log1p(actual_arr)\n    rmsle = np.sqrt(np.mean(np.square(log_diff)))\n\n    # 5. Return rounded result\n    return round(float(rmsle), 4)\n```"}
{"task_id": 428, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine\u2010sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places. Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    # -------- special-case handling ----------------------------------------\n    if window_len <= 0:\n        return []\n    if window_len == 1:               # the only valid coefficient is 1.0\n        return [1.0]\n\n    # -------- choose build length ------------------------------------------\n    # Periodic window needs one extra point before discarding the last.\n    build_len = window_len if symmetric else window_len + 1\n\n    # -------- actual Hann formula ------------------------------------------\n    n = np.arange(build_len, dtype=float)\n    denom = build_len - 1\n    window = 0.5 - 0.5 * np.cos(2.0 * math.pi * n / denom)\n\n    # -------- periodic case: drop last (duplicated) sample -----------------\n    if not symmetric:\n        window = window[:-1]\n\n    # -------- round to 4 decimals and return plain Python list -------------\n    window = np.round(window, 4)\n\n    return window.tolist()\n```"}
{"task_id": 433, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute Xavier/Glorot fan-in and fan-out values for a weight tensor.\n\n    Args:\n        shape: Tensor shape. For 4-D convolutional kernels the expected layout\n               is (out_channels, in_channels, kernel_height, kernel_width).\n\n    Returns:\n        A tuple (fan_in, fan_out) as floats.\n    \"\"\"\n    # Ensure we are working with a sequence we can index\n    if len(shape) == 4:          # 2-D convolutional kernel\n        out_c, in_c, k_h, k_w = shape[:4]\n        receptive_field_size = k_h * k_w\n        fan_in  = in_c  * receptive_field_size\n        fan_out = out_c * receptive_field_size\n    else:                        # Dense / embedding / other tensors\n        fan_in, fan_out = shape[0], shape[1]\n\n    # Return as floats, as requested\n    return float(fan_in), float(fan_out)\n```"}
{"task_id": 435, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear-regression weak learners and squared-error loss.\n    All returned numbers are rounded to 4 decimal places and packed into\n    a plain Python list.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1. Convert inputs to NumPy arrays of type float\n    # ------------------------------------------------------------------\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n\n    # Ensure X is two-dimensional\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(-1, 1)\n\n    m = y_arr.shape[0]                         # number of samples\n    y_mean = y_arr.mean()                      # scalar target mean\n\n    # ------------------------------------------------------------------\n    # 2. Handle the special cases\n    # ------------------------------------------------------------------\n    if n_estimators <= 0 or learning_rate == 0:\n        return np.round(np.full(m, y_mean), 4).tolist()\n\n    # ------------------------------------------------------------------\n    # 3. Initial prediction is the mean of y\n    # ------------------------------------------------------------------\n    current_pred = np.full(m, y_mean, dtype=float)\n\n    # Pre-compute the design matrix with an intercept term\n    ones = np.ones((m, 1), dtype=float)\n    X_design = np.concatenate((ones, X_arr), axis=1)  # shape (m, d+1)\n\n    # ------------------------------------------------------------------\n    # 4. Gradient-boosting iterations\n    # ------------------------------------------------------------------\n    for _ in range(n_estimators):\n        # Residuals of the current model\n        residuals = y_arr - current_pred\n\n        # Ordinary Least Squares fit to the residuals\n        # beta has shape (d+1,)\n        beta, _, _, _ = np.linalg.lstsq(X_design, residuals, rcond=None)\n\n        # Weak learner predictions\n        h_pred = X_design @ beta                 # shape (m,)\n\n        # Update the overall prediction\n        current_pred += learning_rate * h_pred\n\n    # ------------------------------------------------------------------\n    # 5. Return rounded predictions as a plain Python list\n    # ------------------------------------------------------------------\n    return np.round(current_pred, 4).tolist()\n```"}
{"task_id": 437, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in (0, 1, 2):\n        return -1\n\n    arr = np.asarray(x, dtype=float)\n    sig = 1.0 / (1.0 + np.exp(-arr))\n\n    if order == 0:\n        res = sig\n    elif order == 1:\n        res = sig * (1.0 - sig)\n    else:                      # order == 2\n        res = sig * (1.0 - sig) * (1.0 - 2.0 * sig)\n\n    res = np.round(res, 4)     # round to 4 decimal places\n\n    # Preserve original type/shape convention\n    if res.shape == ():        # scalar input \u2192 Python float\n        return float(res)\n    return res.tolist()         # list or ndarray input \u2192 Python list\n```"}
{"task_id": 438, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List\n\ndef to_categorical(\n    y: list[int] | np.ndarray, \n    num_classes: int | None = None\n) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded representation of *y*, or ``-1`` when the given\n        *num_classes* is smaller than required.\n    \"\"\"\n    # Convert the input to a 1-D NumPy array of integers\n    y_arr = np.asarray(y, dtype=int).ravel()\n\n    # Handle the trivial empty-input case early\n    if y_arr.size == 0:\n        inferred_classes = 0 if num_classes is None else num_classes\n        # produce [] no matter what \u2013 an empty one-hot matrix\n        return [] if inferred_classes == 0 else [[] for _ in range(0)]\n\n    # Maximum label present\n    max_label = int(y_arr.max())\n\n    # Determine / validate num_classes\n    if num_classes is None:\n        num_classes = max_label + 1\n    elif num_classes < max_label + 1:\n        # impossible to encode \u2013 not enough columns\n        return -1\n\n    # Build the one-hot matrix\n    one_hot = np.zeros((y_arr.shape[0], num_classes), dtype=int)\n    one_hot[np.arange(y_arr.shape[0]), y_arr] = 1\n\n    # Return as a plain Python list of lists\n    return one_hot.tolist()\n```"}
{"task_id": 439, "completion_id": 0, "completion": "```python\nimport itertools\nimport math\nfrom collections import defaultdict\nfrom typing import Hashable, List, Tuple\n\ndef apriori_frequent_itemsets(\n    transactions: List[List[Hashable]], min_sup: float\n) -> List[Tuple]:\n    \"\"\"Find all frequent item-sets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        The data base: one list per transaction, containing *hashable* items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction in (0, 1].\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets ordered first by their length and then\n        lexicographically inside each length block; inside every tuple the items\n        themselves appear in ascending (lexicographic) order.\n    \"\"\"\n    # ------------ basic checks ------------------------------------------------\n    if not (0 < min_sup <= 1):\n        raise ValueError(\"min_sup must be in the range (0, 1].\")\n\n    n_transactions = len(transactions)\n    if n_transactions == 0:\n        return []\n\n    # Turn each transaction into a set \u2013 duplicates inside a transaction do not\n    # influence support counting and membership checks become O(1).\n    transactions = [set(t) for t in transactions]\n\n    # Convenience helper -------------------------------------------------------\n    def is_frequent(count: int) -> bool:\n        \"\"\"True if empirical support \u2265 min_sup.\"\"\"\n        # Faster than division, but still exact:  count / n \u2265 min_sup   <=>  \n        # count \u2265 min_sup * n\n        return count >= min_sup * n_transactions - 1e-12\n\n    # -------------------------------------------------------------------------\n    # 1-item candidate counting\n    # -------------------------------------------------------------------------\n    item_counts: defaultdict[Hashable, int] = defaultdict(int)\n    for t in transactions:\n        for item in t:\n            item_counts[item] += 1\n\n    L1 = [tuple([item]) for item, cnt in item_counts.items() if is_frequent(cnt)]\n    L1.sort()  # alphabetical order for length-1 block\n\n    frequent_itemsets: list[tuple] = []\n    frequent_itemsets.extend(L1)\n\n    # Frequent (k-1)-itemsets of the previous iteration\n    prev_L = L1\n    k = 2\n\n    while prev_L:\n        # ---------------------------------------------------------------------\n        # Candidate generation: self-join + Apriori subset pruning\n        # ---------------------------------------------------------------------\n        prev_L_len = len(prev_L)\n        prev_L_set = set(prev_L)                # for O(1) subset membership test\n        Ck: set[tuple] = set()                  # size-k candidates\n\n        for i in range(prev_L_len):\n            for j in range(i + 1, prev_L_len):\n                a, b = prev_L[i], prev_L[j]\n                # Join if the first k-2 items are identical\n                if a[:-1] != b[:-1]:\n                    break                       # because prev_L is sorted\n                candidate = tuple(sorted(set(a) | set(b)))  # union\n                if len(candidate) != k:\n                    continue\n\n                # Subset pruning: all (k-1)-subsets of candidate must be frequent\n                if all(tuple(subset) in prev_L_set\n                       for subset in itertools.combinations(candidate, k - 1)):\n                    Ck.add(candidate)\n\n        if not Ck:\n            break\n\n        # ---------------------------------------------------------------------\n        # Count supports of candidates\n        # ---------------------------------------------------------------------\n        candidate_counts = defaultdict(int)\n        # Convert candidates to frozenset once for fast \u2286 test inside loop\n        cand_sets = {cand: frozenset(cand) for cand in Ck}\n\n        for t in transactions:\n            for cand, cand_set in cand_sets.items():\n                if cand_set.issubset(t):\n                    candidate_counts[cand] += 1\n\n        # ---------------------------------------------------------------------\n        # Keep only the frequent candidates -> Lk\n        # ---------------------------------------------------------------------\n        Lk = [cand for cand, cnt in candidate_counts.items() if is_frequent(cnt)]\n        Lk.sort()            # lexicographic order inside size-k block\n\n        if not Lk:\n            break\n\n        frequent_itemsets.extend(Lk)\n        prev_L = Lk\n        k += 1\n\n    return frequent_itemsets\n```"}
{"task_id": 440, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]\n                          ) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class\u2013probability predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        Shape (n_estimators, n_samples, n_classes).  Each innermost list is a\n        valid probability distribution (sums to 1).\n\n    Returns\n    -------\n    tuple\n        (\n          averaged_probs,      # list[list[float]] shape (n_samples, n_classes)\n          predicted_labels     # list[int]          length == n_samples\n        )\n        `averaged_probs` are rounded to 4 decimal places; `predicted_labels`\n        are obtained with arg-max on the rounded probabilities.  Ties are\n        resolved by the smallest class index (numpy\u2019s default behaviour).\n    \"\"\"\n    # --- Basic validation ----------------------------------------------------\n    if not predictions:\n        raise ValueError(\"`predictions` must contain at least one estimator.\")\n\n    # Convert to a NumPy array of shape (n_estimators, n_samples, n_classes)\n    arr = np.asarray(predictions, dtype=float)\n\n    if arr.ndim != 3:\n        raise ValueError(\"`predictions` must be 3-dimensional \"\n                         \"(n_estimators, n_samples, n_classes).\")\n\n    # --- 1. Average over estimators -----------------------------------------\n    mean_probs = arr.mean(axis=0)            # shape -> (n_samples, n_classes)\n\n    # --- 2. Round to 4 decimal places ---------------------------------------\n    mean_probs = np.round(mean_probs, 4)\n\n    # --- 3. Arg-max per sample ----------------------------------------------\n    predicted = np.argmax(mean_probs, axis=1)   # shape -> (n_samples,)\n\n    # Convert results back to regular Python lists (as required)\n    averaged_probs_list = mean_probs.tolist()\n    predicted_labels_list = predicted.tolist()\n\n    return averaged_probs_list, predicted_labels_list\n```\n\nExplanation of the steps\n1. The nested list is cast to a NumPy array for easy vectorised operations.\n2. `mean(axis=0)` computes the average probability vector for each sample across all estimators.\n3. `np.round(..., 4)` performs rounding to four decimal places.\n4. `np.argmax` (after rounding) selects the index of the maximum probability per sample, breaking ties in favour of the lowest index.\n5. The results are converted back to plain Python lists to satisfy the output constraints."}
{"task_id": 444, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function follows the specifications given in the task description.\n    It returns -1 on invalid input, otherwise a nested list containing the\n    kernel matrix rounded to four decimal places.\n    \"\"\"\n    # ------------------ helper: convert input to proper 2-D numpy array -------\n    def _to_2d_array(mat):\n        try:\n            arr = np.asarray(mat, dtype=float)\n        except Exception:\n            return None\n        # Reject ragged or 1-D structures\n        if arr.ndim != 2 or arr.shape[1] == 0:\n            return None\n        return arr\n\n    # ------------------ convert / validate X ----------------------------------\n    X_arr = _to_2d_array(X)\n    if X_arr is None:\n        return -1\n    n_samples, n_features = X_arr.shape\n\n    # ------------------ convert / validate Y ----------------------------------\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        Y_arr = _to_2d_array(Y)\n        if Y_arr is None or Y_arr.shape[1] != n_features:\n            return -1\n\n    # ------------------ handle / validate sigma ------------------------------\n    if sigma is None:                                 # default\n        sigma_scalar = np.sqrt(n_features / 2.0)\n        if sigma_scalar <= 0:\n            return -1\n        sigma_vec = None                               # isotropic\n    elif isinstance(sigma, (int, float, np.floating)):  # scalar provided\n        sigma_scalar = float(sigma)\n        if sigma_scalar <= 0:\n            return -1\n        sigma_vec = None                               # isotropic\n    else:                                              # list / array\n        try:\n            sigma_vec = np.asarray(sigma, dtype=float)\n        except Exception:\n            return -1\n        if sigma_vec.ndim != 1 or sigma_vec.shape[0] != n_features:\n            return -1\n        if np.any(sigma_vec <= 0):\n            return -1\n        sigma_scalar = None                            # anisotropic\n\n    # ------------------ compute the kernel matrix ----------------------------\n    if sigma_vec is None:  # isotropic case\n        diff = X_arr[:, None, :] - Y_arr[None, :, :]       # (N, M, C)\n        dist2 = np.sum(diff ** 2, axis=-1) / (sigma_scalar ** 2)\n    else:               # anisotropic case\n        diff = (X_arr[:, None, :] - Y_arr[None, :, :]) / sigma_vec\n        dist2 = np.sum(diff ** 2, axis=-1)\n\n    K = np.exp(-0.5 * dist2)\n\n    # ------------------ round & return as nested Python list -----------------\n    return np.round(K, 4).tolist()\n```"}
{"task_id": 446, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find the best 1-level split (decision stump) by information gain.\"\"\"\n    # ------------------------------------------------------------------\n    # Basic preparation\n    # ------------------------------------------------------------------\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    n_samples, n_features = X.shape\n\n    # Map the labels to 0, \u2026 , (n_classes-1) so that we can use bincount\n    classes, y_indices = np.unique(y, return_inverse=True)\n    n_classes = len(classes)\n\n    # ------------------------------------------------------------------\n    # Helper functions\n    # ------------------------------------------------------------------\n    def entropy_from_counts(counts: np.ndarray) -> float:\n        \"\"\"Shannon entropy (base 2) from a 1-D array with class counts.\"\"\"\n        total = counts.sum()\n        if total == 0:\n            return 0.0\n        probs = counts[counts > 0] / total\n        return -np.sum(probs * np.log2(probs))\n\n    def majority_label(counts: np.ndarray) -> int:\n        \"\"\"Most frequent label, ties resolved by smaller label value.\"\"\"\n        max_count = counts.max()\n        # classes are already sorted, so np.where returns indices in order\n        winner_idx = np.where(counts == max_count)[0][0]\n        return int(classes[winner_idx])\n\n    # ------------------------------------------------------------------\n    # Compute parent entropy and check the trivial case\n    # ------------------------------------------------------------------\n    total_counts = np.bincount(y_indices, minlength=n_classes)\n    parent_entropy = entropy_from_counts(total_counts)\n\n    if parent_entropy == 0.0:                     # all samples same label\n        maj = majority_label(total_counts)\n        return (-1, None, maj, maj)\n\n    # ------------------------------------------------------------------\n    # Search the best split\n    # ------------------------------------------------------------------\n    best_gain = -np.inf\n    best_feature = -1\n    best_threshold = None\n    best_left_label = None\n    best_right_label = None\n    eps = 1e-12                                   # tolerance for float ties\n\n    for j in range(n_features):\n        col = X[:, j]\n        order = np.argsort(col)                   # sort by feature value\n        sorted_vals = col[order]\n        sorted_y_idx = y_indices[order]\n\n        # cumulative counts of classes for the left partition\n        left_counts = np.zeros(n_classes, dtype=int)\n\n        # iterate over possible split points (between two distinct values)\n        for i in range(n_samples - 1):\n            # move sample i to the left side\n            left_counts[sorted_y_idx[i]] += 1\n\n            # if the next value is identical, no threshold here\n            if sorted_vals[i] == sorted_vals[i + 1]:\n                continue\n\n            # candidate threshold: midpoint between the two consecutive values\n            thr = (sorted_vals[i] + sorted_vals[i + 1]) / 2.0\n\n            n_left = i + 1\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:\n                continue\n\n            right_counts = total_counts - left_counts\n\n            # entropies\n            h_left = entropy_from_counts(left_counts)\n            h_right = entropy_from_counts(right_counts)\n\n            p_left = n_left / n_samples\n            p_right = 1.0 - p_left\n\n            gain = parent_entropy - p_left * h_left - p_right * h_right\n\n            # ------------------------------------------------------------------\n            # Update the best split following the tie-breaking rules\n            # ------------------------------------------------------------------\n            better_gain = gain > best_gain + eps\n            same_gain = abs(gain - best_gain) <= eps\n\n            update = False\n            if better_gain:\n                update = True\n            elif same_gain:\n                if j < best_feature:\n                    update = True\n                elif j == best_feature and (best_threshold is None or thr < best_threshold - eps):\n                    update = True\n\n            if update:\n                best_gain = gain\n                best_feature = j\n                best_threshold = thr\n                best_left_label = majority_label(left_counts)\n                best_right_label = majority_label(right_counts)\n\n    # ------------------------------------------------------------------\n    # Final answer: did we actually improve the information gain?\n    # ------------------------------------------------------------------\n    if best_gain <= eps:                          # no improvement found\n        maj = majority_label(total_counts)\n        return (-1, None, maj, maj)\n\n    return (\n        best_feature,\n        round(best_threshold, 4),\n        best_left_label,\n        best_right_label,\n    )\n```"}
{"task_id": 452, "completion_id": 0, "completion": "```python\nimport numbers\nimport numpy as np\nfrom typing import List\n\n\ndef divide_on_feature(\n    X: np.ndarray | List[list], feature_i: int, threshold\n) -> List[list]:\n    \"\"\"Split *X* into two subsets according to one feature and a threshold.\n\n    For a numeric *threshold* (int / float) the first subset contains every\n    sample whose value in column *feature_i* is >= threshold, otherwise the\n    first subset contains every sample whose value equals *threshold*.\n\n    The second subset gets all remaining samples.  The original row order is\n    preserved and both subsets are returned as regular Python lists.\n    \"\"\"\n    # --------------------------------------------------------------------- #\n    # Decide whether we are dealing with a numeric split (int/float) or not\n    # (exclude bool which is a subclass of int):\n    is_numeric = isinstance(threshold, numbers.Real) and not isinstance(\n        threshold, bool\n    )\n\n    left, right = [], []  # left == first subset, right == second subset\n\n    # Iterate through the samples in *X* (works for both ndarray and list):\n    for row in X:\n        value = row[feature_i]\n\n        if is_numeric:\n            goes_left = value >= threshold\n        else:\n            goes_left = value == threshold\n\n        (left if goes_left else right).append(row)\n\n    # Convert the subsets to regular Python lists (as requested)\n    left_out = np.asarray(left, dtype=object).tolist() if left else []\n    right_out = np.asarray(right, dtype=object).tolist() if right else []\n\n    return [left_out, right_out]\n```"}
{"task_id": 453, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch (see doc-string above).\"\"\"\n    # --------  basic validation  ------------------------------------------------\n    # gamma within [0, 1]\n    if not (0.0 <= gamma <= 1.0):\n        return -1\n\n    # Q and Q_next must be 2-D numpy arrays of identical shape\n    if not (isinstance(Q, np.ndarray) and isinstance(Q_next, np.ndarray)):\n        return -1\n    if Q.shape != Q_next.shape or Q.ndim != 2:\n        return -1\n\n    batch_size, n_actions = Q.shape\n\n    # actions, rewards, dones lengths must match batch size\n    if not (len(actions) == len(rewards) == len(dones) == batch_size):\n        return -1\n\n    # check every action index is valid\n    if any(a < 0 or a >= n_actions for a in actions):\n        return -1\n\n    # --------  target computation  ---------------------------------------------\n    # copy because we must not overwrite the original array\n    targets = Q.copy()\n\n    # compute target for every sample and replace only the chosen action\n    for i in range(batch_size):\n        if dones[i]:\n            target = rewards[i]\n        else:\n            target = rewards[i] + gamma * np.max(Q_next[i])\n        targets[i, actions[i]] = target\n\n    # --------  rounding & conversion to plain python list  ---------------------\n    targets = np.round(targets, 4).tolist()\n    return targets\n```"}
{"task_id": 458, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef to_categorical(\n    labels: list | tuple | \"np.ndarray\",\n    num_classes: int | None = None,\n) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes.  If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists (shape: (len(labels), num_classes)) containing\n        0/1 integers, or -1 when:\n            \u2022 any label is negative, or\n            \u2022 `num_classes` is provided but smaller than `max(labels)+1`.\n    \"\"\"\n    # --- Step 1: make a flat Python list ---------------------------------\n    try:\n        labels_list = list(labels)\n    except TypeError:\n        return -1  # labels not iterable\n\n    if not labels_list:                                   # empty input\n        inferred_classes = 0 if num_classes is None else num_classes\n        return [] if inferred_classes >= 0 else -1\n\n    # --- Step 2: basic validation ----------------------------------------\n    max_label = -1\n    for lbl in labels_list:\n        # Accept Python int or NumPy integer (but reject float/bool/other)\n        if not isinstance(lbl, (int, np.integer)) or lbl < 0:\n            return -1\n        if lbl > max_label:\n            max_label = int(lbl)\n\n    # --- Step 3: determine num_classes -----------------------------------\n    if num_classes is None:\n        num_classes = max_label + 1\n    elif num_classes < max_label + 1:\n        return -1\n\n    # --- Step 4: build one-hot matrix ------------------------------------\n    one_hot: list[list[int]] = [\n        [1 if class_idx == lbl else 0 for class_idx in range(num_classes)]\n        for lbl in labels_list\n    ]\n    return one_hot\n```"}
{"task_id": 461, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\"\"\"\n    # convert to numpy once\n    x_arr = np.asarray(x, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n\n    # constant prediction if no boosting iterations requested\n    if n_estimators <= 0:\n        return np.round(np.full_like(y_arr, y_arr.mean()), 4).tolist()\n\n    # initial prediction: mean of targets\n    y_pred = np.full_like(y_arr, y_arr.mean(), dtype=float)\n\n    # pre-compute unique feature values, sorted\n    uniq_x = np.unique(x_arr)\n\n    for _ in range(n_estimators):\n        # residuals\n        residuals = y_arr - y_pred\n        if np.allclose(residuals, 0.0):\n            break                                    # perfect fit\n\n        # ---- search for best split -------------------------------------------------\n        best_sse = np.inf\n        best_t = None\n        best_left_mean = best_right_mean = 0.0\n\n        # all possible thresholds: all but the largest unique x\n        thresholds = uniq_x[:-1]\n        for t in thresholds:\n            left_mask = x_arr <= t\n            right_mask = ~left_mask\n            if not left_mask.any() or not right_mask.any():\n                continue                            # invalid split, skip\n\n            r_left = residuals[left_mask]\n            r_right = residuals[right_mask]\n\n            m_left, m_right = r_left.mean(), r_right.mean()\n            sse = np.sum((r_left - m_left) ** 2) + np.sum((r_right - m_right) ** 2)\n\n            if sse < best_sse:                      # first minimum kept if tie\n                best_sse = sse\n                best_t = t\n                best_left_mean, best_right_mean = m_left, m_right\n\n        # ---- build stump prediction ------------------------------------------------\n        if best_t is None:                          # no valid split\n            stump_pred = np.full_like(residuals, residuals.mean())\n        else:\n            stump_pred = np.where(x_arr <= best_t, best_left_mean, best_right_mean)\n\n        # ---- update ensemble prediction --------------------------------------------\n        y_pred += learning_rate * stump_pred\n\n    # final rounded predictions\n    return np.round(y_pred, 4).tolist()\n```"}
{"task_id": 471, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One-step tabular Expected-SARSA TD(0) update.\"\"\"\n    \n    n_actions = len(q_table[0])                 # |A|\n    \n    # ---------- compute the expectation term ----------\n    if next_state is None:                      # terminal transition\n        expected_q = 0.0\n    else:\n        next_row = q_table[next_state]\n        greedy_idx = max(range(n_actions), key=next_row.__getitem__)   # first max\n        \n        # \u03b5-soft probabilities\n        base_prob = epsilon / n_actions\n        expected_q = 0.0\n        for a in range(n_actions):\n            prob = base_prob\n            if a == greedy_idx:                 # greedy action gets extra mass\n                prob += (1.0 - epsilon)\n            expected_q += prob * next_row[a]\n    \n    # ---------- TD target ----------\n    target = reward + gamma * expected_q\n    \n    # ---------- Q update ----------\n    q_table[state][action] += lr * (target - q_table[state][action])\n    \n    # ---------- round everything to 4 decimals and return ----------\n    rounded_q = [[round(v, 4) for v in row] for row in q_table]\n    return rounded_q\n```"}
{"task_id": 474, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\nimport numpy as np          # duplicated on purpose \u2013 harmless\nimport random               # duplicated on purpose \u2013 harmless\n\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray,\n                                                    np.ndarray,\n                                                    np.ndarray,\n                                                    np.ndarray]:\n    \"\"\"Generate a 2-D toy data set consisting of `m` rectangular clusters.\n\n    Each class i (0-based index) is located at\n        x \u2208 [(\u230ai/2\u230b+0.1)\u00b710 , (\u230ai/2\u230b+0.9)\u00b710]\n        y \u2208 [((i mod 2)*interval+0.1)\u00b710 , ((i mod 2)*interval+0.9)\u00b710]\n    and is assigned the label  (i \u2212 0.5)\u00b72  (\u2192 \u22121, +1, 3, 5, \u2026).\n\n    All returned coordinates are rounded to 4 decimals.\n    \"\"\"\n    if m < 2:\n        raise ValueError(\"`m` (number of classes) must be at least 2.\")\n    if seed is not None:\n        # Make both the Python stdlib RNG and NumPy RNG deterministic\n        random.seed(seed)\n        np.random.seed(seed)\n\n    X_train_parts, X_val_parts = [], []\n    Y_train_parts, Y_val_parts = [], []\n\n    for i in range(m):\n        col = i // 2             # horizontal position (0, 1, 2, \u2026)\n        row = i % 2              # 0 for bottom row, 1 for top row\n\n        # Coordinate ranges\n        x_min, x_max = (col + 0.1) * 10, (col + 0.9) * 10\n        y_min, y_max = (row * interval + 0.1) * 10, (row * interval + 0.9) * 10\n\n        # Training samples\n        x_tr = np.random.uniform(x_min, x_max, n_train)\n        y_tr = np.random.uniform(y_min, y_max, n_train)\n        X_train_parts.append(np.column_stack((x_tr, y_tr)))\n\n        # Validation samples\n        x_va = np.random.uniform(x_min, x_max, n_val)\n        y_va = np.random.uniform(y_min, y_max, n_val)\n        X_val_parts.append(np.column_stack((x_va, y_va)))\n\n        # Labels\n        label = int((i - 0.5) * 2)         # \u22121, +1, 3, 5, \u2026\n        Y_train_parts.append(np.full(n_train, label, dtype=int))\n        Y_val_parts.append(np.full(n_val, label, dtype=int))\n\n    # Concatenate parts for all classes\n    X_train = np.round(np.vstack(X_train_parts), 4)\n    X_val   = np.round(np.vstack(X_val_parts),   4)\n    Y_train = np.concatenate(Y_train_parts)\n    Y_val   = np.concatenate(Y_val_parts)\n\n    return X_train, X_val, Y_train, Y_val\n```"}
{"task_id": 475, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef adam_update(w, grad, m_prev, v_prev, t,\n                learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step (see doc-string above).\"\"\"\n    # Convert inputs to numpy arrays for uniform maths (scalars are fine too)\n    grad_arr = np.asarray(grad, dtype=float)\n\n    # Initialise moments if this is the first call\n    if m_prev is None:\n        m_prev = np.zeros_like(grad_arr, dtype=float)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad_arr, dtype=float)\n\n    # ---- 1. Update the (un-biased) first and second moments -------------\n    m_new = b1 * m_prev + (1.0 - b1) * grad_arr\n    v_new = b2 * v_prev + (1.0 - b2) * (grad_arr ** 2)\n\n    # ---- 2. Bias correction --------------------------------------------\n    m_hat = m_new / (1.0 - b1 ** t)\n    v_hat = v_new / (1.0 - b2 ** t)\n\n    # ---- 3. Parameter update -------------------------------------------\n    w_new = np.asarray(w, dtype=float) - learning_rate * m_hat / (\n        np.sqrt(v_hat) + eps\n    )\n\n    # Keep the same (scalar / array) \"shape type\" as the input parameter w\n    if np.isscalar(w):\n        w_new = w_new.item()          # back to Python float\n        m_new = m_new.item()\n        v_new = v_new.item()\n\n    return w_new, m_new, v_new\n```\n\nThe function exactly follows the Adam update equations:\n\n    m_t = \u03b2\u2081 m_{t\u22121} + (1\u2212\u03b2\u2081) grad\n    v_t = \u03b2\u2082 v_{t\u22121} + (1\u2212\u03b2\u2082) grad\u00b2\n    m\u0302_t = m_t / (1\u2212\u03b2\u2081\u1d57)\n    v\u0302_t = v_t / (1\u2212\u03b2\u2082\u1d57)\n    w_new = w \u2212 \u03b1 \u00b7 m\u0302_t / (\u221av\u0302_t + \u03b5)\n\nIt works for scalars as well as arbitrary-shaped NumPy arrays and honours the provided hyper-parameters."}
{"task_id": 479, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Optional\n\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\"\"\"\n    # Reproducibility ----------------------------------------------------------\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Basic set-up -------------------------------------------------------------\n    if not rewards:                                 # no time-steps -> return priors\n        return [round(ev_prior, 4)]\n\n    n_arms = len(rewards[0])\n    values = np.full(n_arms, float(ev_prior))       # current value estimates V_i\n    counts = np.zeros(n_arms, dtype=int)            # pull counters  C_i\n\n    # Main interaction loop ----------------------------------------------------\n    for row in rewards:\n        # 1. Pick an arm -------------------------------------------------------\n        if np.random.random() < epsilon:            # exploration\n            arm = np.random.randint(n_arms)\n        else:                                       # exploitation  (argmax breaks ties by the\n            arm = int(np.argmax(values))            # first / smallest index)\n\n        # 2. Observe reward ----------------------------------------------------\n        r = row[arm]\n\n        # 3. Update estimates --------------------------------------------------\n        counts[arm] += 1\n        # incremental mean: V_i \u2190 V_i + (r \u2212 V_i)/C_i\n        values[arm] += (r - values[arm]) / counts[arm]\n\n    # -------------------------------------------------------------------------\n    return [round(v, 4) for v in values]\n```"}
{"task_id": 481, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # Safety checks / trivial case\n    # ------------------------------------------------------------------\n    if data.size == 0:               # empty input \u2192 nothing to do\n        return []\n\n    n_samples = data.shape[0]\n\n    # ------------------------------------------------------------------\n    # Pre\u2013compute \u03b5\u2013neighbourhoods for every point\n    # ------------------------------------------------------------------\n    eps_sq = eps * eps                              # compare squared dists\n    neighbours: list[np.ndarray] = []\n\n    # (n, d) - (1, d) broadcasting; numpy\u2019s einsum is cheap for 2-D arrays\n    for i in range(n_samples):\n        diff = data - data[i]                       # (n, d)\n        dist_sq = np.einsum(\"ij,ij->i\", diff, diff) # squared \u21132 distance\n        neighbours.append(np.where(dist_sq <= eps_sq)[0])\n\n    # ------------------------------------------------------------------\n    # Main DBSCAN\n    # ------------------------------------------------------------------\n    UNASSIGNED = -1\n    labels   = np.full(n_samples, UNASSIGNED, dtype=int)\n    visited  = np.zeros(n_samples, dtype=bool)\n    cluster_id = 0\n\n    for point_idx in range(n_samples):\n        if visited[point_idx]:\n            continue\n\n        visited[point_idx] = True\n        neigh = neighbours[point_idx]\n\n        # --------------------------------------------------------------\n        # Not enough neighbours \u2192 noise (may be changed later to border)\n        # --------------------------------------------------------------\n        if neigh.size < min_samples:\n            labels[point_idx] = UNASSIGNED\n            continue\n\n        # --------------------------------------------------------------\n        # Found a new core \u2192 start new cluster and expand it\n        # --------------------------------------------------------------\n        labels[point_idx] = cluster_id\n        stack = list(neigh)          # depth-first expansion\n\n        while stack:\n            current = stack.pop()\n\n            if not visited[current]:\n                visited[current] = True\n                neigh_current = neighbours[current]\n                if neigh_current.size >= min_samples:   # current is a core\n                    # add unvisited neighbours to the stack\n                    stack.extend(neigh_current.tolist())\n\n            # Assign the point to the cluster if it is not yet in any cluster\n            if labels[current] == UNASSIGNED:\n                labels[current] = cluster_id\n\n        # finished expanding the cluster\n        cluster_id += 1\n\n    return labels.tolist()\n```\n\nExplanation of the essential steps\n\u2022 \u03b5\u2013neighbourhoods are pre-computed once so they can be reused while clusters are expanded (cost: O(n\u00b2) for n samples).  \n\u2022 Each sample is processed exactly once in discovery order.  \n\u2022 When we enter `expandCluster` (implemented via `stack`) we keep adding reachable core points\u2019 neighbours to that stack, thereby labelling every density-reachable point with the current `cluster_id`.  \n\u2022 Points initially marked as noise can still be relabelled as border points if they later become reachable from a core point.  \n\u2022 Cluster indices start at 0 and increase exactly in the order in which the first core point of every cluster is encountered during the linear scan through the data."}
{"task_id": 482, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    Parameters\n    ----------\n    y : array_like\n        1-D sequence of non-negative integer labels.\n    num_classes : int or None, optional\n        Number of classes (columns in the output).  If None, it is inferred\n        as max(y) + 1.  Every label must satisfy\n        0 <= label < num_classes, otherwise ValueError is raised.\n\n    Returns\n    -------\n    np.ndarray\n        One-hot encoded matrix of shape (len(y), num_classes) with dtype=int.\n    \"\"\"\n    # Convert input to a 1-D NumPy array\n    y_arr = np.asarray(y)\n    if y_arr.ndim == 0:                      # scalar \u2192 length-1 vector\n        y_arr = y_arr.reshape(1)\n    if y_arr.ndim != 1:\n        raise ValueError(\"`y` must be a 1-D sequence of integer labels.\")\n    # Ensure integer type\n    try:\n        y_int = y_arr.astype(int, casting='safe', copy=False)\n    except TypeError:\n        raise ValueError(\"`y` must contain integer values only.\")\n    # Empty input edge-case\n    n_samples = y_int.size\n    if n_samples == 0:\n        if num_classes is None:\n            return np.zeros((0, 0), dtype=int)\n        if num_classes <= 0:\n            raise ValueError(\"`num_classes` must be a positive integer.\")\n        return np.zeros((0, num_classes), dtype=int)\n\n    # Infer / validate num_classes\n    max_label = int(y_int.max())\n    min_label = int(y_int.min())\n    if num_classes is None:\n        if min_label < 0:\n            raise ValueError(\"Negative labels are not allowed.\")\n        num_classes = max_label + 1\n    else:\n        if num_classes <= 0:\n            raise ValueError(\"`num_classes` must be a positive integer.\")\n        if min_label < 0 or max_label >= num_classes:\n            raise ValueError(\n                \"All labels must satisfy 0 <= label < num_classes.\"\n            )\n\n    # Create the one-hot matrix and fill the appropriate positions with 1\n    one_hot = np.zeros((n_samples, num_classes), dtype=int)\n    rows = np.arange(n_samples)\n    one_hot[rows, y_int] = 1\n    return one_hot\n```"}
{"task_id": 485, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\"\"\"\n    # ------------------------------------------------------------------ #\n    # Helper : build the tree ------------------------------------------ #\n    # ------------------------------------------------------------------ #\n    def build(x: np.ndarray, y: np.ndarray, depth: int):\n        # stopping conditions\n        if (depth == max_depth) or (x.size < 2) or (np.all(x == x[0])):\n            return {\"leaf\": True, \"value\": float(y.mean())}\n\n        # sort by feature value\n        order = np.argsort(x)\n        xs, ys = x[order], y[order]\n        n = xs.size\n\n        # prefix sums for O(n) split search\n        csum = np.cumsum(ys)\n        csum2 = np.cumsum(ys ** 2)\n        total_sum, total_sum2 = csum[-1], csum2[-1]\n\n        best_sse, best_idx = np.inf, None\n        for i in range(n - 1):\n            if xs[i] == xs[i + 1]:                # identical feature value -> skip\n                continue\n            n_l, n_r = i + 1, n - (i + 1)\n\n            sum_l, sum_r = csum[i], total_sum - csum[i]\n            sum2_l, sum2_r = csum2[i], total_sum2 - csum2[i]\n\n            sse_l = sum2_l - (sum_l ** 2) / n_l\n            sse_r = sum2_r - (sum_r ** 2) / n_r\n            sse = sse_l + sse_r\n\n            if sse < best_sse:\n                best_sse, best_idx = sse, i\n\n        # no admissible split -> make leaf\n        if best_idx is None:\n            return {\"leaf\": True, \"value\": float(y.mean())}\n\n        # split threshold = midpoint between consecutive feature values\n        thr = (xs[best_idx] + xs[best_idx + 1]) / 2.0\n\n        left_mask = x <= thr\n        right_mask = ~left_mask\n\n        return {\n            \"leaf\": False,\n            \"thr\": float(thr),\n            \"left\":  build(x[left_mask],  y[left_mask],  depth + 1),\n            \"right\": build(x[right_mask], y[right_mask], depth + 1),\n        }\n\n    # ------------------------------------------------------------------ #\n    # Helper : prediction ---------------------------------------------- #\n    # ------------------------------------------------------------------ #\n    def predict_one(node, x_val: float):\n        while not node[\"leaf\"]:\n            node = node[\"left\"] if x_val <= node[\"thr\"] else node[\"right\"]\n        return node[\"value\"]\n\n    # ------------------------------------------------------------------ #\n    # Main routine ------------------------------------------------------ #\n    # ------------------------------------------------------------------ #\n    if len(X_train) == 0:\n        return []\n\n    X_arr = np.asarray(X_train, dtype=float)\n    y_arr = np.asarray(y_train, dtype=float)\n    tree = build(X_arr, y_arr, depth=0)\n\n    return [predict_one(tree, float(x)) for x in X_test]\n```"}
{"task_id": 490, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    # Convert the input to a NumPy array (this also copes with list/tuple inputs)\n    arr = np.asarray(x)\n\n    # Handle the empty-vector case early\n    if arr.size == 0:\n        return []\n\n    # Ensure we are working with a 1-D view of the data\n    arr = arr.ravel()\n\n    # Build the diagonal matrix and convert it back to a Python list of lists\n    diag_matrix = np.diag(arr).tolist()\n\n    return diag_matrix\n```"}
{"task_id": 491, "completion_id": 0, "completion": "```python\nfrom collections import Counter, defaultdict\nfrom itertools import combinations\n\n# ----------  FP-tree data structure -----------------------------------------\nclass _FPNode:                       # very small helper class\n    __slots__ = ('item', 'count', 'parent', 'children', 'link')\n\n    def __init__(self, item, parent):\n        self.item     = item         # item label, None for the root\n        self.count    = 0            # support counter\n        self.parent   = parent       # parent node\n        self.children = {}           # item  -> _FPNode\n        self.link     = None         # next node that contains the same item\n\n\n# ----------  FP-tree construction ------------------------------------------\ndef _build_fp_tree(trans_iterable, min_sup):\n    \"\"\"\n    Build an FP-tree from `trans_iterable` and return (root, header_table).\n    `trans_iterable` must yield tuples   (items_in_transaction , multiplicity)\n    in which every `items_in_transaction` is already ordered according to a\n    global frequency order and contains *only* items that are frequent in the\n    projected data set.\n    \"\"\"\n    # header_table : item -> [overall_support , first_node_in_linked_list]\n    header = {}\n    for items, cnt in trans_iterable:         # 1st scan \u2013 frequency counting\n        for itm in items:\n            header.setdefault(itm, [0, None])[0] += cnt\n    # eliminate infrequent items inside this (sub-)database\n    header = {itm: val for itm, val in header.items() if val[0] >= min_sup}\n    if not header:\n        return None, None                     # nothing left \u2013 stop here\n\n    # global order in this (sub-)database:  descending support, tie\u2192lexicographic\n    order = sorted(header.items(),\n                   key=lambda x: (-x[1][0], x[0]))\n    rank  = {itm: idx for idx, (itm, _) in enumerate(order)}\n\n    # second scan \u2013 real tree construction\n    root = _FPNode(None, None)\n    for items, cnt in trans_iterable:\n        ordered = [i for i in sorted(items, key=lambda x: rank.get(x, 10**9))\n                   if i in header]             # keep only still-frequent items\n        if not ordered:\n            continue\n        cur = root\n        for itm in ordered:\n            nxt = cur.children.get(itm)\n            if nxt is None:                    # create new node\n                nxt = _FPNode(itm, cur)\n                cur.children[itm] = nxt\n                # link into header table\n                if header[itm][1] is None:\n                    header[itm][1] = nxt\n                else:\n                    last = header[itm][1]\n                    while last.link is not None:\n                        last = last.link\n                    last.link = nxt\n            nxt.count += cnt\n            cur = nxt\n    return root, header\n\n\n# ----------  Mining ---------------------------------------------------------\ndef _is_single_path(node):\n    \"\"\"return list of nodes if the subtree under `node` is a single path\"\"\"\n    path = []\n    while node:\n        if len(node.children) > 1:\n            return None\n        if len(node.children) == 0:\n            return path\n        nxt = next(iter(node.children.values()))\n        path.append(nxt)\n        node = nxt\n    return path\n\n\ndef _mine(tree_root, header, min_sup, prefix, out):\n    \"\"\"\n    Recursively mine an FP-tree;  write every frequent pattern (as a tuple\n    sorted lexicographically) into set `out`.\n    `prefix` is a list that already belongs to (super-)patterns.\n    \"\"\"\n    # process items in *ascending* lexicographic order to obtain deterministic\n    # output that is easy to sort later\n    for itm in sorted(header):\n        new_prefix = prefix + [itm]\n        out.add(tuple(sorted(new_prefix)))\n\n        # ---------- build conditional pattern base --------------------------\n        cond_transactions = []\n        node = header[itm][1]\n        while node:\n            path = []\n            parent = node.parent\n            while parent and parent.item is not None:\n                path.append(parent.item)\n                parent = parent.parent\n            if path:\n                cond_transactions.append((tuple(reversed(path)), node.count))\n            node = node.link\n\n        # ---------- build conditional FP-tree -------------------------------\n        cond_tree, cond_header = _build_fp_tree(cond_transactions, min_sup)\n        if cond_tree is None:\n            continue                          # nothing to mine deeper\n\n        # ---------- optimisation: single path -------------------------------\n        single_path = _is_single_path(cond_tree)\n        if single_path is not None:           # linear tree \u21d2 enumerate directly\n            items_on_path = [n.item for n in single_path]\n            for r in range(1, len(items_on_path)+1):\n                for comb in combinations(items_on_path, r):\n                    out.add(tuple(sorted(new_prefix + list(comb))))\n        else:                                 # general case \u2013 mine recursively\n            _mine(cond_tree, cond_header, min_sup, new_prefix, out)\n\n\n# ----------  Public interface ----------------------------------------------\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\"\"\"\n    # 1st pass over the original database \u2013 global frequency\n    freq = Counter()\n    for t in transactions:\n        freq.update(t)\n\n    freq = {k: v for k, v in freq.items() if v >= min_sup}\n    if not freq:                              # nothing is frequent\n        return []\n\n    # global order for the *original* tree\n    order = sorted(freq.items(), key=lambda x: (-x[1], x[0]))\n    rank  = {itm: idx for idx, (itm, _) in enumerate(order)}\n\n    # prepare (ordered, filtered) transactions with multiplicity = 1\n    preprocessed = []\n    for t in transactions:\n        filtered = [i for i in t if i in freq]\n        if not filtered:\n            continue\n        filtered.sort(key=lambda x: rank[x])\n        preprocessed.append((tuple(filtered), 1))\n\n    # build initial FP-tree and mine it\n    root, header = _build_fp_tree(preprocessed, min_sup)\n    patterns = set()\n    _mine(root, header, min_sup, [], patterns)\n\n    # final, deterministic sorting as required by the task description\n    result = [tuple(sorted(p)) for p in patterns]\n    result.sort(key=lambda x: (len(x), x))\n    return result\n```\n\nThe implementation strictly follows the FP-Growth procedure:\n\n1. Counts global item frequencies and discards infrequent ones.\n2. Builds the FP-tree while maintaining node links in the header table.\n3. Recursively mines conditional FP-trees, using the single-path shortcut whenever possible.\n4. Collects every frequent item-set in a set (to avoid duplicates), finally returning them\n   as a list that is\n      \u2022 internally lexicographically sorted,\n      \u2022 externally sorted by length and then lexicographically,\n   exactly as demanded."}
{"task_id": 492, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a polynomial feature matrix up to *degree*.\n\n    Parameters\n    ----------\n    X : list of list\n        Input data of shape (n_samples, n_features).\n    degree : int\n        Maximum total degree of the generated polynomial terms.\n        If degree < 0 the function returns -1.\n\n    Returns\n    -------\n    list of list\n        Enriched feature matrix whose columns are all monomials of the\n        original variables having total degree \u2264 *degree*, ordered\n        (i)   by increasing total degree,\n        (ii)  lexicographically within the same degree.\n        The first column is 1 (degree-0 term).\n    \"\"\"\n    # ---- Guard clause --------------------------------------------------------\n    if degree < 0:\n        return -1\n\n    # ---- Convert to ndarray for convenience ---------------------------------\n    X_arr = np.asarray(X, dtype=float)        # ensures 2-D float array\n    if X_arr.ndim != 2:\n        raise ValueError(\"X must be a 2-D array-like structure\")\n\n    n_samples, n_features = X_arr.shape\n\n    # ---- Build list of index tuples specifying each monomial ----------------\n    # ()  -> constant term\n    # (0,)             -> x0\n    # (0, 0)           -> x0^2\n    # (0, 1)           -> x0 x1\n    # ...\n    index_tuples: list[tuple[int, ...]] = [()]   # degree-0 term first\n\n    for p in range(1, degree + 1):               # ascending total degree\n        for combo in combinations_with_replacement(range(n_features), p):\n            index_tuples.append(combo)           # lexicographic order\n\n    # ---- Compute each column -------------------------------------------------\n    n_terms = len(index_tuples)\n    out = np.empty((n_samples, n_terms), dtype=float)\n\n    for j, comb in enumerate(index_tuples):\n        if not comb:                             # constant column\n            out[:, j] = 1.0\n        else:\n            col = np.ones(n_samples, dtype=float)\n            for idx in comb:                     # multiply the chosen features\n                col *= X_arr[:, idx]\n            out[:, j] = col\n\n    return out.tolist()\n```"}
{"task_id": 493, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    If *y* is empty return -1.\n\n    Parameters\n    ----------\n    y : Sequence[int | float] | numpy.ndarray\n        One-dimensional collection of numbers.\n\n    Returns\n    -------\n    float\n        MSE rounded to four decimal places, or -1 when *y* is empty.\n    \"\"\"\n    # Convert to NumPy array to unify computations\n    arr = np.asarray(y, dtype=float).ravel()          # ensures 1-D\n    \n    # Empty input \u2192 -1\n    if arr.size == 0:\n        return -1\n\n    # Mean-squared error\n    diff = arr - arr.mean()\n    mse_val = (diff ** 2).mean()\n\n    # Round to four decimal places and return as plain Python float\n    return float(round(mse_val, 4))\n```"}
{"task_id": 496, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list/array or a scalar.\n        slope: Coefficient multiplying x (default 1.0).\n        intercept: Constant term added to the product (default 0.0).\n\n    Returns:\n        (y, grad, grad2): tuple of three lists (all rounded to 4 decimals)\n            y[i]     = slope * x[i] + intercept\n            grad[i]  = slope\n            grad2[i] = 0\n    \"\"\"\n    # Turn the input into a 1-D NumPy array (scalar \u2192 length-1 array)\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim == 0:          # scalar case\n        arr = arr.reshape(1)\n\n    # Affine activation and its derivatives\n    y     = slope * arr + intercept\n    grad  = np.full(arr.shape, slope, dtype=float)\n    grad2 = np.zeros(arr.shape, dtype=float)\n\n    # Round to 4 decimal places and convert to Python lists\n    y     = np.round(y,     4).tolist()\n    grad  = np.round(grad,  4).tolist()\n    grad2 = np.round(grad2, 4).tolist()\n\n    return y, grad, grad2\n```"}
{"task_id": 499, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places.  \n        If *k* is outside the valid range, return \u22121.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Basic checks\n    # ------------------------------------------------------------------ #\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D NumPy array.\")\n        \n    n_samples, n_features = data.shape\n    if not (1 <= k <= n_features):\n        return -1\n\n    data = data.astype(float, copy=False)          # work in float\n\n    # ------------------------------------------------------------------ #\n    # 1. Standardise the features  (zero mean, unit *population* variance)\n    # ------------------------------------------------------------------ #\n    means = data.mean(axis=0)                      # shape (n_features,)\n    stds  = data.std(axis=0, ddof=0)               # population \u03c3  (ddof = 0)\n\n    # avoid division by zero \u2192 keep centred column at all zeros\n    stds_safe = stds.copy()\n    stds_safe[stds_safe == 0] = 1.0\n\n    Z = (data - means) / stds_safe                # standardised data\n\n    # ------------------------------------------------------------------ #\n    # 2. Sample covariance matrix  (divide by n \u2212 1 \u2192 Bessel correction)\n    # ------------------------------------------------------------------ #\n    if n_samples > 1:\n        cov = Z.T @ Z / (n_samples - 1)            # shape (n_features, n_features)\n    else:                                          # degenerate case n = 1\n        cov = np.zeros((n_features, n_features))\n\n    # ------------------------------------------------------------------ #\n    # 3. Eigen-decomposition (covariance matrix is symmetric)\n    # ------------------------------------------------------------------ #\n    eigvals, eigvecs = np.linalg.eigh(cov)         # ascending order\n\n    # ------------------------------------------------------------------ #\n    # 4. Sort eigenvalues / eigenvectors in *descending* order\n    # ------------------------------------------------------------------ #\n    order = eigvals.argsort()[::-1]\n    eigvals = eigvals[order]\n    eigvecs = eigvecs[:, order]                    # columns are eigenvectors\n\n    # ------------------------------------------------------------------ #\n    # 5. Fix sign ambiguity of each eigenvector\n    # ------------------------------------------------------------------ #\n    for i in range(eigvecs.shape[1]):\n        v = eigvecs[:, i]\n        if v[np.argmax(np.abs(v))] < 0:            # make dominant entry positive\n            eigvecs[:, i] = -v\n\n    # ------------------------------------------------------------------ #\n    # 6. Project data onto the first k principal components\n    # ------------------------------------------------------------------ #\n    components = eigvecs[:, :k]                    # shape (n_features, k)\n    projected  = Z @ components                    # shape (n_samples,  k)\n\n    # ------------------------------------------------------------------ #\n    # 7. Round & return as a regular Python list\n    # ------------------------------------------------------------------ #\n    return np.round(projected, 4).tolist()\n```"}
{"task_id": 500, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom math import inf\n\ndef _fit_best_stump(X: np.ndarray, residuals: np.ndarray):\n    \"\"\"Find the decision stump (feature, threshold, left_val, right_val)\n    that minimises the squared error w.r.t. the provided residuals.\"\"\"\n    n_samples, n_features = X.shape\n    best_feature, best_thr = None, None\n    best_left_val, best_right_val = None, None\n    best_sse = inf\n\n    for j in range(n_features):\n        f = X[:, j]\n        order = np.argsort(f)\n        f_sorted = f[order]\n        r_sorted = residuals[order]\n\n        # prefix sums for fast variance / SSE computation\n        cumsum_r = np.cumsum(r_sorted)\n        cumsum_r2 = np.cumsum(r_sorted ** 2)\n        total_sum = cumsum_r[-1]\n        total_sum2 = cumsum_r2[-1]\n\n        for i in range(1, n_samples):            # split is between i-1 and i\n            if f_sorted[i] == f_sorted[i - 1]:\n                continue                          # identical value \u2013 no real split\n            n_left = i\n            n_right = n_samples - i\n\n            sum_left = cumsum_r[i - 1]\n            sum_right = total_sum - sum_left\n\n            sum2_left = cumsum_r2[i - 1]\n            sum2_right = total_sum2 - sum2_left\n\n            mean_left = sum_left / n_left\n            mean_right = sum_right / n_right\n\n            sse_left = sum2_left - (sum_left ** 2) / n_left\n            sse_right = sum2_right - (sum_right ** 2) / n_right\n            sse = sse_left + sse_right\n\n            if sse < best_sse:\n                best_sse = sse\n                best_feature = j\n                best_thr = 0.5 * (f_sorted[i] + f_sorted[i - 1])  # midpoint\n                best_left_val = mean_left\n                best_right_val = mean_right\n\n    return best_feature, best_thr, best_left_val, best_right_val\n\n\ndef gradient_boosting_classifier(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test:  list[list[float]],\n    n_estimators: int = 20,\n    learning_rate: float = 0.1\n) -> list[int]:\n    \"\"\"Tiny gradient\u2013boosted binary classifier (decision-stump base learners).\"\"\"\n    # Convert to numpy arrays\n    X_train = np.asarray(X_train, dtype=float)\n    X_test  = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train, dtype=float)\n\n    n_train = X_train.shape[0]\n\n    # 1) Initial constant predictor\n    c0 = y_train.mean()\n    F_train = np.full(n_train, c0, dtype=float)\n    F_test  = np.full(X_test.shape[0], c0, dtype=float)\n\n    stumps = []\n\n    # 2) Boosting iterations\n    for _ in range(n_estimators):\n        residuals = y_train - F_train          # r_i = y_i - F(x_i)\n\n        feat, thr, left_val, right_val = _fit_best_stump(X_train, residuals)\n        stumps.append((feat, thr, left_val, right_val))\n\n        # Predictions of the new stump on train & test\n        pred_train = np.where(X_train[:, feat] <= thr, left_val, right_val)\n        pred_test  = np.where(X_test[:,  feat] <= thr, left_val, right_val)\n\n        # Update ensemble scores\n        F_train += learning_rate * pred_train\n        F_test  += learning_rate * pred_test\n\n    # 3) Turn final scores into class labels (threshold at 0.5)\n    y_pred = (F_test >= 0.5).astype(int).tolist()\n    return y_pred\n```"}
{"task_id": 505, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM (probabilities only).\"\"\"\n    # Empty mini-batch \u2500> nothing to do\n    if X.size == 0:\n        return []\n\n    m = X.shape[0]       # mini-batch size\n\n    # ------------------------------------------------------------------\n    # Helper: logistic sigmoid\n    # ------------------------------------------------------------------\n    def _sigmoid(a: np.ndarray) -> np.ndarray:\n        return 1. / (1. + np.exp(-a))\n\n    # ------------------------------------------------------------------\n    # Positive phase  (v0 = X  \u2192  h0)\n    # ------------------------------------------------------------------\n    h_prob = _sigmoid(X @ W + hbias)          # h0 probabilities\n    pos_grad = X.T @ h_prob                   # X\u1d40 \u00b7 h0_prob\n\n    # ------------------------------------------------------------------\n    # Gibbs sampling: k full steps  (h \u2192 v \u2192 h)  using probabilities\n    # ------------------------------------------------------------------\n    for _ in range(k):\n        v_prob = _sigmoid(h_prob @ W.T + vbias)   # p(v | h)\n        h_prob = _sigmoid(v_prob @ W + hbias)     # p(h | v)\n\n    # At this point: v_prob = v_k_prob, h_prob = h_k_prob\n    neg_grad = v_prob.T @ h_prob              # v_k_prob\u1d40 \u00b7 h_k_prob\n\n    # ------------------------------------------------------------------\n    # Weight update\n    # ------------------------------------------------------------------\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n\n    # ------------------------------------------------------------------\n    # Return as plain Python list rounded to 4 decimals\n    # ------------------------------------------------------------------\n    return np.round(W_new, 4).tolist()\n```"}
{"task_id": 509, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    # Convert inputs to 1-D NumPy arrays\n    x_arr = np.asarray(x, dtype=float).ravel()\n    y_arr = np.asarray(y, dtype=float).ravel()\n\n    # Length mismatch \u279c return -1\n    if x_arr.size != y_arr.size:\n        return -1.0\n\n    # Chebyshev distance = max(|x_i \u2212 y_i|)\n    dist = np.max(np.abs(x_arr - y_arr))\n\n    # Round to four decimal places and cast to built-in float\n    return float(round(dist, 4))\n```"}
{"task_id": 510, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    Returns the mini-batch average of the binary cross-entropy reconstruction\n    loss plus the analytical KL divergence, rounded to 6 decimal places.\n    \"\"\"\n    # Numerical-stability clipping for the logits\n    eps = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, eps, 1.0 - eps)\n\n    # 1. Reconstruction (binary cross-entropy) loss\n    recon_element = (\n        -y * np.log(y_pred_clipped) - (1.0 - y) * np.log(1.0 - y_pred_clipped)\n    )                       # shape: (batch_size, n_features)\n    recon_loss = recon_element.sum(axis=1)   # per sample\n\n    # 2. KL divergence between q(t|x)=N(mu,diag(sigma^2)) and p(t)=N(0,I)\n    kl_element = (\n        1.0 + t_log_var - np.square(t_mean) - np.exp(t_log_var)\n    )                       # shape: (batch_size, latent_dim)\n    kl_loss = -0.5 * kl_element.sum(axis=1)  # per sample\n\n    # Total loss per sample and then mini-batch mean\n    total_loss = recon_loss + kl_loss\n    batch_mean_loss = total_loss.mean()\n\n    # Return value rounded to 6 decimal places\n    return round(float(batch_mean_loss), 6)\n```"}
{"task_id": 513, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix (n_samples \u00d7 n_features) given as a list of lists.\n        w0: Global bias term.\n        w: Linear weights (length = n_features).\n        V: Latent factor matrix (n_features \u00d7 k).\n\n    Returns:\n        List with one prediction per sample, rounded to 4 decimals.\n    \"\"\"\n    # Convert everything to numpy arrays\n    X = np.asarray(X, dtype=float)        # shape: (N, n)\n    w = np.asarray(w, dtype=float)        # shape: (n,)\n    V = np.asarray(V, dtype=float)        # shape: (n, k)\n    \n    # Linear part: w0 + X\u00b7w\n    linear_term = w0 + X @ w              # shape: (N,)\n    \n    # Pair-wise interaction part\n    XV = X @ V                            # shape: (N, k)      \u2013  \u03a3_j V_{j,f} x_j\n    X2_V2 = (X ** 2) @ (V ** 2)           # shape: (N, k)      \u2013  \u03a3_j V_{j,f}\u00b2 x_j\u00b2\n    interaction_term = 0.5 * np.sum(XV**2 - X2_V2, axis=1)  # shape: (N,)\n    \n    # Final prediction\n    y_hat = linear_term + interaction_term\n    \n    # Round to 4 decimal places and return as Python list\n    return np.round(y_hat, 4).tolist()\n```"}
{"task_id": 517, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute a numerically-stable, element-wise sigmoid activation.\n\n    Parameters\n    ----------\n    z : int | float | list | tuple | numpy.ndarray\n        Input value(s).\n\n    Returns\n    -------\n    float | list\n        If `z` is a scalar a Python float is returned, otherwise a (nested)\n        list whose elements are the sigmoid of the corresponding entries in\n        `z`, rounded to four decimal places.\n    \"\"\"\n    # Convert to numpy array for unified, vectorised processing\n    arr = np.asarray(z, dtype=np.float64)\n\n    # Allocate output array of the same shape\n    out = np.empty_like(arr)\n\n    # Numerically stable sigmoid:\n    # For non-negative x  : \u03c3(x) = 1 / (1 + exp(-x))\n    # For negative x      : \u03c3(x) = exp(x) / (1 + exp(x))\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    exp_x = np.exp(arr[neg_mask])           # this is safe because arr[neg_mask] < 0\n    out[neg_mask] = exp_x / (1.0 + exp_x)\n\n    # Round to four decimal places\n    out = np.round(out, 4)\n\n    # Preserve the input structure\n    if out.ndim == 0:            # scalar case (also covers 0-D ndarray)\n        return float(out)\n    else:                        # list/tuple/ndarray case\n        return out.tolist()\n```\n\nExample usage\n```python\nprint(sigmoid([-1, 0, 1]))   # \u279c [0.2689, 0.5, 0.7311]\nprint(sigmoid(800))          # \u279c 1.0      (no overflow)\nprint(sigmoid(-800))         # \u279c 0.0      (no overflow)\n```"}
{"task_id": 518, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Callable\nfrom functools import wraps\n\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D NumPy array whose second\n    dimension is larger than one, it is assumed to be one-hot encoded and is\n    replaced by the index of the maximum value along axis 1 before the wrapped\n    *function* is executed.\n    \"\"\"\n\n    def _decode(arr: np.ndarray) -> np.ndarray:\n        \"\"\"Return a 1-D label array if *arr* looks like one-hot, else *arr*.\"\"\"\n        if isinstance(arr, np.ndarray) and arr.ndim == 2 and arr.shape[1] > 1:\n            return arr.argmax(axis=1)\n        return arr\n\n    @wraps(function)\n    def wrapper(actual: np.ndarray, predicted: np.ndarray, *args, **kwargs):\n        actual_decoded    = _decode(actual)\n        predicted_decoded = _decode(predicted)\n        return function(actual_decoded, predicted_decoded, *args, **kwargs)\n\n    return wrapper\n```"}
{"task_id": 520, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).\n    The derivative is computed element-wise as z * (1 - z) and rounded\n    to 4 decimal places.\n\n    Args:\n        z: float, list, or NumPy ndarray holding sigmoid outputs.\n\n    Returns:\n        float if *z* is a scalar; otherwise a (nested) list with the same\n        shape as *z*, each value rounded to 4 decimals.\n    \"\"\"\n    # Empty-list shortcut\n    if z == []:\n        return []\n\n    # Convert to NumPy array for vectorised computation\n    arr = np.asarray(z, dtype=float)\n\n    # Element-wise gradient, rounded to 4 decimals\n    grad = np.round(arr * (1.0 - arr), 4)\n\n    # Scalar \u2192 float, anything else \u2192 nested list\n    return float(grad) if grad.ndim == 0 else grad.tolist()\n```"}
{"task_id": 528, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef decision_boundary_grid(\n    X: list[list[int | float]],\n    W: list[int | float],\n    b: float,\n    grid_n: int = 100,\n) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) receives the label  1 if  w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0\n    and \u22121 otherwise.  The returned list has shape (grid_n \u00d7 grid_n) and\n    contains those labels for a regular grid that spans the bounding box\n    of *X*.\n\n    Args:\n        X: 2-D data set \u2013 list of [x\u2081 , x\u2082] pairs (shape n\u00d72).\n        W: Length-2 sequence [w\u2081 , w\u2082] with the classifier\u2019s weights.\n        b: Bias term of the classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Python list of lists with shape (grid_n \u00d7 grid_n) whose entries\n        are exclusively 1 or \u22121.\n    \"\"\"\n    # --- basic validation -------------------------------------------------\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim != 2 or X_arr.shape[1] != 2:\n        raise ValueError(\"X must be a 2-D array-like with exactly two columns.\")\n    if len(W) != 2:\n        raise ValueError(\"W must contain exactly two elements.\")\n    if grid_n <= 0:\n        raise ValueError(\"grid_n must be a positive integer.\")\n\n    # --- build the 1-D plotting axes --------------------------------------\n    x1_min, x1_max = X_arr[:, 0].min(), X_arr[:, 0].max()\n    x2_min, x2_max = X_arr[:, 1].min(), X_arr[:, 1].max()\n\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n\n    # --- generate the full grid (shape grid_n \u00d7 grid_n) -------------------\n    x1_grid, x2_grid = np.meshgrid(x1_plot, x2_plot, indexing=\"xy\")\n\n    # --- apply the linear decision rule -----------------------------------\n    w1, w2 = float(W[0]), float(W[1])\n    decision_values = w1 * x1_grid + w2 * x2_grid + b\n    labels = np.where(decision_values >= 0, 1, -1)\n\n    # --- convert to a plain Python list and return ------------------------\n    return labels.tolist()\n```"}
{"task_id": 537, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier (from scratch).\"\"\"\n    # ------------------------------------------------------------------\n    # 1. Split the training set by class and estimate parameters\n    # ------------------------------------------------------------------\n    eps = 1e-9                      # to avoid division by zero\n    classes = np.array([0, 1])      # binary classifier: class 0 or 1\n    \n    # Masks selecting the rows that belong to each class\n    mask0 = y_train == 0\n    mask1 = y_train == 1\n    \n    # Means and (non-zero) standard deviations of every feature per class\n    mu0  = X_train[mask0].mean(axis=0)\n    mu1  = X_train[mask1].mean(axis=0)\n    std0 = X_train[mask0].std(axis=0, ddof=0) + eps\n    std1 = X_train[mask1].std(axis=0, ddof=0) + eps\n    \n    # ------------------------------------------------------------------\n    # 2. Class priors  P(class)\n    # ------------------------------------------------------------------\n    n_samples = len(y_train)\n    prior0 = mask0.sum() / n_samples\n    prior1 = mask1.sum() / n_samples\n    \n    # Pre-compute parts of the Gaussian log-pdf that do not depend on x\n    log_prior = np.log(np.array([prior0, prior1]))\n    log_coeff0 = -0.5 * np.log(2 * np.pi) - np.log(std0)\n    log_coeff1 = -0.5 * np.log(2 * np.pi) - np.log(std1)\n    inv_var0 = 1.0 / (2 * std0 ** 2)\n    inv_var1 = 1.0 / (2 * std1 ** 2)\n    \n    # ------------------------------------------------------------------\n    # 3. Predict every sample in X_test\n    # ------------------------------------------------------------------\n    predictions = []\n    for x in X_test:\n        # log-likelihoods for class 0 and 1\n        ll0 = (log_coeff0 - ((x - mu0) ** 2) * inv_var0).sum()\n        ll1 = (log_coeff1 - ((x - mu1) ** 2) * inv_var1).sum()\n        \n        # log posterior = log prior + log likelihood\n        log_post0 = log_prior[0] + ll0\n        log_post1 = log_prior[1] + ll1\n        \n        # choose the class with the higher log posterior\n        pred = 0 if log_post0 >= log_post1 else 1\n        predictions.append(pred)\n    \n    return predictions\n```"}
{"task_id": 539, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Computes the binary cross-entropy (log-loss) cost.\n\n    Parameters\n    ----------\n    AL : np.ndarray\n        Model predicted probabilities, shape (m,) or (1, m).\n    Y  : np.ndarray\n        Ground-truth labels (0 or 1), same shape as AL.\n\n    Returns\n    -------\n    float\n        Scalar cost value.\n    \"\"\"\n    if AL.shape != Y.shape:\n        raise ValueError(\"AL and Y must have the same shape.\")\n    \n    # flatten to 1-D (works for (m,) or (1, m))\n    AL = AL.ravel()\n    Y  = Y.ravel()\n    m  = AL.size\n\n    # numerical stability: clip probabilities\n    eps = 1e-15\n    AL_clipped = np.clip(AL, eps, 1.0 - eps)\n\n    # cross-entropy\n    cost = -np.sum(Y * np.log(AL_clipped) +\n                   (1 - Y) * np.log(1 - AL_clipped)) / m\n\n    return float(cost)\n```"}
{"task_id": 552, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation uses the forward-backward algorithm with scaling\n    to obtain numerically stable \u03b1, \u03b2 and finally \u03b3\u209c.\n\n    Args:\n        hmm (dict): HMM parameters with keys\n                    'pi' -> 1-D initial state distribution (N,)\n                    'A'  -> state transition matrix      (N,N)\n                    'B'  -> emission matrix              (N,M)\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t,\n                     rounded to four decimals.  If t is out of range an\n                     empty list is returned.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # Basic sanity checks\n    # ------------------------------------------------------------------ #\n    if t < 0 or t >= len(observations):\n        return []\n\n    # Convert HMM parameters to numpy arrays for vectorised math\n    pi = np.asarray(hmm[\"pi\"], dtype=float)           # (N,)\n    A  = np.asarray(hmm[\"A\"],  dtype=float)           # (N,N)\n    B  = np.asarray(hmm[\"B\"],  dtype=float)           # (N,M)\n\n    N          = pi.shape[0]                          # #states\n    T          = len(observations)                    # length of sequence\n    obs_idx    = observations                         # just a shorter name\n\n    # ------------------------------------------------------------------ #\n    # Forward pass (\u03b1) with scaling                                      #\n    # ------------------------------------------------------------------ #\n    alpha      = np.zeros((T, N), dtype=float)\n    c          = np.zeros(T,      dtype=float)        # scaling factors\n\n    # t = 0\n    alpha[0] = pi * B[:, obs_idx[0]]\n    c[0]     = 1.0 / alpha[0].sum() if alpha[0].sum() != 0 else 1.0\n    alpha[0] *= c[0]\n\n    # t = 1 .. T-1\n    for tt in range(1, T):\n        alpha[tt]  = (alpha[tt-1] @ A) * B[:, obs_idx[tt]]\n        denom      = alpha[tt].sum()\n        c[tt]      = 1.0 / denom if denom != 0 else 1.0\n        alpha[tt] *= c[tt]\n\n    # ------------------------------------------------------------------ #\n    # Backward pass (\u03b2) with the same scaling factors                    #\n    # ------------------------------------------------------------------ #\n    beta = np.zeros((T, N), dtype=float)\n\n    # t = T-1\n    beta[-1]  = 1.0\n    beta[-1] *= c[-1]\n\n    # t = T-2 .. 0\n    for tt in range(T-2, -1, -1):\n        beta[tt]  = (A @ (B[:, obs_idx[tt+1]] * beta[tt+1]))\n        beta[tt] *= c[tt]\n\n    # ------------------------------------------------------------------ #\n    # Posterior state distribution \u03b3\u209c                                   #\n    # ------------------------------------------------------------------ #\n    gamma_t = alpha[t] * beta[t]\n    total   = gamma_t.sum()\n    if total == 0:                                     # avoid division by 0\n        return [0.0 for _ in range(N)]\n    gamma_t /= total\n\n    # Round to 4 decimal places and return as a Python list\n    return [round(float(p), 4) for p in gamma_t]\n```"}
{"task_id": 555, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN) implemented from scratch.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Array of shape (n_samples, n_features).\n    eps : float\n        Neighbourhood radius.\n    min_samples : int\n        Minimum number of points (inclusive) needed for a core point.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels (1, 2, \u2026) for clustered points; noise is \u22121.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D array (n_samples, n_features)\")\n\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n\n    # ------------------------------------------------------------------\n    # 1. Pair-wise distance matrix (Euclidean).\n    # ------------------------------------------------------------------\n    # Broadcasting builds an (n, n, n_features) tensor of differences\n    # whose L2-norm along the last axis gives the distance matrix.\n    diff = data[:, None, :] - data[None, :, :]\n    dist_mat = np.linalg.norm(diff, axis=-1)          # shape (n, n)\n\n    # ------------------------------------------------------------------\n    # 2. \u03b5-neighbourhood list for every sample.\n    # ------------------------------------------------------------------\n    neighbours = [np.where(dist_mat[i] <= eps)[0]     # inclusive of i itself\n                  for i in range(n_samples)]\n\n    # ------------------------------------------------------------------\n    # 3. Identify core points.\n    # ------------------------------------------------------------------\n    core_mask = np.array([len(idx) >= min_samples for idx in neighbours],\n                         dtype=bool)\n\n    # ------------------------------------------------------------------\n    # 4. Cluster expansion (BFS).\n    # ------------------------------------------------------------------\n    labels = np.zeros(n_samples, dtype=int)           # 0 \u2192 unlabeled\n    current_label = 0\n\n    for point in range(n_samples):\n        # Skip non-core or already labelled points\n        if not core_mask[point] or labels[point] != 0:\n            continue\n\n        # Start a new cluster\n        current_label += 1\n        queue = deque([point])\n        labels[point] = current_label\n\n        # Breadth-first search\n        while queue:\n            p = queue.popleft()\n            for nbr in neighbours[p]:\n                if labels[nbr] == 0:                  # not yet assigned\n                    labels[nbr] = current_label\n                    if core_mask[nbr]:\n                        queue.append(nbr)\n\n    # ------------------------------------------------------------------\n    # 5. Mark remaining unlabeled points as noise (\u22121).\n    # ------------------------------------------------------------------\n    labels[labels == 0] = -1\n    return labels.tolist()\n```"}
{"task_id": 561, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef gmm_em(data: np.ndarray,\n           k: int,\n           max_iter: int = 200,\n           epsilon: float = 1e-4) -> list[int]:\n    \"\"\"\n    Simple EM implementation for a full-covariance Gaussian Mixture Model.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array with shape (N, D); N samples, D features.\n    k : int\n        Number of mixture components (clusters).\n    max_iter : int, optional\n        Maximum number of EM iterations (default 200).\n    epsilon : float, optional\n        Convergence tolerance on the maximum absolute change of the mixture\n        weights between two successive iterations (default 1 e-4).\n\n    Returns\n    -------\n    list[int]\n        Length-N list of final cluster labels (0 \u2026 k-1) in deterministic,\n        left-to-right order (ascending first coordinate of the component mean).\n\n    Notes\n    -----\n    \u2022 If k == 1 all samples receive label 0 and the algorithm stops\n      immediately.\n    \u2022 A tiny diagonal regulariser (1 e-6) is added to every covariance\n      estimate to guarantee positive-definiteness and numerical stability.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    #  Basic checks and early exit                                       #\n    # ------------------------------------------------------------------ #\n    data = np.asarray(data, dtype=float)\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D array (N samples, D features).\")\n\n    n_samples, n_features = data.shape\n\n    if k < 1:\n        raise ValueError(\"`k` must be a positive integer.\")\n    if k == 1:\n        return [0] * n_samples\n\n    # ------------------------------------------------------------------ #\n    #  Helper: log multivariate normal                                   #\n    # ------------------------------------------------------------------ #\n    _LOG_2PI = n_features * math.log(2.0 * math.pi)\n\n    def _log_gaussian(x: np.ndarray,\n                      mean: np.ndarray,\n                      cov: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute log N(x | mean, cov) for all rows of `x`.\n        x : (N, D)\n        mean : (D,)\n        cov : (D, D)\n        Returns\n        -------\n        log_prob : (N,)\n        \"\"\"\n        # Solution of cov^{-1} * (x - mean).T for every sample\n        diff = x - mean\n        try:\n            L = np.linalg.cholesky(cov)\n            # Solve L * y = diff^T  -> y\n            y = np.linalg.solve(L, diff.T)\n            # Mahalanobis distance\n            maha = np.sum(y ** 2, axis=0)\n            log_det = 2.0 * np.sum(np.log(np.diag(L)))\n        except np.linalg.LinAlgError:\n            # Fallback: slower but more stable\n            inv_cov = np.linalg.inv(cov)\n            maha = np.einsum(\"ij,ij->i\", diff, diff @ inv_cov)\n            sign, log_det = np.linalg.slogdet(cov)\n            if sign <= 0:\n                log_det = np.log(np.abs(np.linalg.det(cov) + 1e-12))\n\n        return -0.5 * (_LOG_2PI + log_det + maha)\n\n    # ------------------------------------------------------------------ #\n    #  Initialisation                                                    #\n    # ------------------------------------------------------------------ #\n    rng = np.random.default_rng()\n    # Randomly choose k different samples as initial means\n    init_idx = rng.choice(n_samples, size=k, replace=False)\n    means = data[init_idx]\n\n    # Mixture weights (uniform)\n    weights = np.full(k, 1.0 / k)\n\n    # Start with the global empirical covariance for every component\n    global_cov = np.cov(data.T, bias=True)\n    if global_cov.shape == ():       # handled when D==1\n        global_cov = global_cov.reshape(1, 1)\n    covs = np.array([global_cov.copy() for _ in range(k)])\n\n    reg_eps = 1e-6 * np.eye(n_features)  # regulariser\n\n    # ------------------------------------------------------------------ #\n    #  EM loop                                                           #\n    # ------------------------------------------------------------------ #\n    for _ in range(max_iter):\n        # ------------------------ E-step ------------------------------ #\n        log_resp = np.empty((n_samples, k))\n        for j in range(k):\n            # log \u03c0_k + log N(x | \u03bc_k, \u03a3_k)\n            log_resp[:, j] = np.log(weights[j] + 1e-16) + \\\n                             _log_gaussian(data, means[j], covs[j])\n\n        # Normalise in log-domain to avoid under/overflow\n        log_resp_max = np.max(log_resp, axis=1, keepdims=True)\n        resp = np.exp(log_resp - log_resp_max)\n        resp_sum = resp.sum(axis=1, keepdims=True)\n        resp /= resp_sum  # shape (N, k)\n\n        # Effective number of points per component\n        nk = resp.sum(axis=0)  # shape (k,)\n\n        # ------------------------ M-step ------------------------------ #\n        prev_weights = weights.copy()\n        weights = nk / n_samples\n\n        # Means\n        means = (resp.T @ data) / nk[:, None]  # shape (k, D)\n\n        # Covariances\n        for j in range(k):\n            diff = data - means[j]\n            # weighted outer products and regularisation\n            cov = (resp[:, j][:, None] * diff).T @ diff / nk[j]\n            covs[j] = cov + reg_eps\n\n        # -------------------- Convergence test ------------------------ #\n        if np.max(np.abs(weights - prev_weights)) < epsilon:\n            break\n\n    # ------------------------------------------------------------------ #\n    #  Final assignment                                                  #\n    # ------------------------------------------------------------------ #\n    # Responsibilities for the last parameter set\n    log_resp = np.empty((n_samples, k))\n    for j in range(k):\n        log_resp[:, j] = np.log(weights[j] + 1e-16) + \\\n                         _log_gaussian(data, means[j], covs[j])\n\n    labels = np.argmax(log_resp, axis=1)  # (N,)\n\n    # ------------------------------------------------------------------ #\n    #  Deterministic re-labelling (left \u2192 right)                         #\n    # ------------------------------------------------------------------ #\n    ordering = np.argsort(means[:, 0])           # ascending first coordinate\n    remap = np.zeros(k, dtype=int)\n    remap[ordering] = np.arange(k)               # old id -> new id\n\n    labels = remap[labels]                       # apply remap\n    return labels.tolist()\n```"}
{"task_id": 562, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef spectral_clustering(\n    data: np.ndarray,\n    n_cluster: int,\n    gamma: float = 2.0,\n    method: str = \"unnormalized\",\n) -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A Python list containing the cluster label (0-based) for each sample.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Trivial case\n    # ------------------------------------------------------------------ #\n    N = data.shape[0]\n    if n_cluster <= 1 or N == 0:\n        return [0] * N\n\n    if method not in {\"unnormalized\", \"normalized\"}:\n        raise ValueError(\"`method` must be either 'unnormalized' or 'normalized'\")\n\n    # ------------------------------------------------------------------ #\n    # 1. Similarity matrix W using the Gaussian (RBF) kernel\n    # ------------------------------------------------------------------ #\n    # pairwise squared Euclidean distances\n    sq_norms = np.sum(data ** 2, axis=1)\n    sq_dists = (\n        sq_norms[:, None] + sq_norms[None, :] - 2.0 * data @ data.T\n    )  # shape (N, N)\n    sq_dists = np.maximum(sq_dists, 0.0)  # numerical stability\n    W = np.exp(-gamma * sq_dists)\n    np.fill_diagonal(W, 0.0)  # remove self-loops, standard in spectral clustering\n\n    # ------------------------------------------------------------------ #\n    # 2. Laplacian construction\n    # ------------------------------------------------------------------ #\n    degree = np.sum(W, axis=1)  # shape (N,)\n    if method == \"unnormalized\":\n        L = np.diag(degree) - W\n    else:  # symmetric normalized Laplacian\n        # Avoid division by zero\n        d_inv_sqrt = 1.0 / np.sqrt(degree + 1e-12)\n        L = np.eye(N) - (d_inv_sqrt[:, None] * W * d_inv_sqrt[None, :])\n\n    # ------------------------------------------------------------------ #\n    # 3. Eigen-decomposition & embedding\n    # ------------------------------------------------------------------ #\n    eigvals, eigvecs = np.linalg.eigh(L)  # L is symmetric\n    U = eigvecs[:, :n_cluster]  # eigenvectors for the smallest eigenvalues\n\n    if method == \"normalized\":\n        # Row-normalize\n        row_norm = np.linalg.norm(U, axis=1, keepdims=True)\n        U = U / (row_norm + 1e-12)\n\n    embedding = U  # shape (N, n_cluster)\n\n    # ------------------------------------------------------------------ #\n    # 4. Deterministic k-means in the embedding space\n    # ------------------------------------------------------------------ #\n    centroids = embedding[:n_cluster].copy()  # deterministic initialisation\n    labels = np.full(N, -1, dtype=int)\n\n    for _ in range(300):  # hard stop to guarantee termination\n        # Assign step\n        dists = ((embedding[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2)\n        new_labels = np.argmin(dists, axis=1)\n\n        # Check for convergence\n        if np.array_equal(labels, new_labels):\n            break\n        labels = new_labels\n\n        # Update step\n        for k in range(n_cluster):\n            members = embedding[labels == k]\n            if len(members) > 0:  # normal case\n                centroids[k] = members.mean(axis=0)\n            # else: keep centroid unchanged (deterministic tie-break)\n\n    # ------------------------------------------------------------------ #\n    # 5. Relabel clusters so that the cluster containing the\n    #    smallest original index becomes 0, next 1, etc.\n    # ------------------------------------------------------------------ #\n    unique_old = np.unique(labels)\n    # pair (old_label, smallest index of data point with that label)\n    order = sorted((l, np.where(labels == l)[0][0]) for l in unique_old)\n    relabel_map = {old: new for new, (old, _) in enumerate(order)}\n    final_labels = [int(relabel_map[l]) for l in labels]\n\n    return final_labels\n```"}
{"task_id": 563, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array with N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate.\n        max_iter: Maximum number of full passes over the training data.\n\n    Returns:\n        (w, b)  \u2013 w is the learned weight vector as a Python list,\n                  b is the bias term; both rounded to 4 decimals.\n    \"\"\"\n    # Ensure proper NumPy types/shapes\n    X = np.asarray(X_data, dtype=float)\n    y = np.asarray(y_data, dtype=float).flatten()\n    N, d = X.shape\n\n    # 1. initialise alpha and bias\n    alpha = np.zeros(N, dtype=float)\n    b = 0.0\n\n    # 2. Gram matrix  G[i, j] = X_i \u00b7 X_j\n    G = X @ X.T        # shape (N, N)\n\n    # 3. training loop\n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            g = np.sum(alpha * y * G[:, i])      # \u03a3_j \u03b1_j y_j G[j, i]\n            if y[i] * (g + b) <= 0:              # misclassified (or on margin)\n                alpha[i] += eta                  # update \u03b1_i\n                b += eta * y[i]                  # update bias\n                updated = True\n        if not updated:                          # no update in pass \u2192 converged\n            break\n\n    # 4. recover weight vector  w = \u03a3_i \u03b1_i y_i x_i\n    w_vec = X.T @ (alpha * y)                    # shape (d,)\n\n    # 5. round and convert to requested output types\n    w_rounded = [round(float(val), 4) for val in w_vec]\n    b_rounded = round(float(b), 4)\n\n    return w_rounded, b_rounded\n```"}
{"task_id": 564, "completion_id": 0, "completion": "```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    node = root\n\n    while True:\n        # 1. Leaf check \u2013 identical for both task types\n        if hasattr(node, \"y\"):\n            return node.y\n\n        # 2. Internal\u2013node handling\n        if task_type == \"classification\":\n            feat_val = features[node.label]\n\n            # Find the first child whose x matches `feat_val`\n            next_node = None\n            for child in node.child:\n                if hasattr(child, \"x\") and child.x == feat_val:\n                    next_node = child\n                    break\n\n            # Default to the second child if no match\n            if next_node is None:\n                next_node = node.child[1]\n\n        else:  # task_type == 'regression'\n            feat_val = features[node.label]\n            # <= goes left (index 0); > goes right (index 1)\n            next_node = node.child[0] if feat_val <= node.s else node.child[1]\n\n        node = next_node\n```"}
{"task_id": 565, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef _entropy(labels: np.ndarray) -> float:\n    \"\"\"\n    Shannon entropy H(labels) in bits.\n    labels are integers starting from 0.\n    \"\"\"\n    if labels.size == 0:\n        return 0.0\n    # np.bincount is very fast for non-negative integers\n    counts = np.bincount(labels)\n    probs = counts[counts > 0] / labels.size        # skip zero counts\n    return float(-np.sum(probs * np.log2(probs)))\n\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        (best_feature_index, max_information_gain_rounded_to_6_decimals)\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Base entropy of the label distribution\n    base_entropy = _entropy(y)\n\n    best_gain = -math.inf\n    best_index = 0\n\n    for idx in range(n_features):\n        column = X[:, idx]\n        # unique values of the feature and their frequencies\n        vals, counts = np.unique(column, return_counts=True)\n\n        cond_entropy = 0.0\n        for val, cnt in zip(vals, counts):\n            subset_labels = y[column == val]\n            cond_entropy += (cnt / n_samples) * _entropy(subset_labels)\n\n        info_gain = base_entropy - cond_entropy\n\n        # tie-breaking: choose the smallest index\n        if info_gain > best_gain + 1e-12:\n            best_gain = info_gain\n            best_index = idx\n        elif abs(info_gain - best_gain) <= 1e-12 and idx < best_index:\n            best_index = idx\n\n    # Avoid returning \u22120.0\n    if abs(best_gain) < 1e-12:\n        best_gain = 0.0\n\n    return best_index, round(best_gain, 6)\n```"}
{"task_id": 566, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef detect_outliers(data: np.ndarray,\n                    n_trees: int,\n                    sample_size: int,\n                    epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation (deterministic).\n\n    Args\n    ----\n    data        : 2-D NumPy array (n_samples, n_features)\n    n_trees     : number of random isolation trees\n    sample_size : subsample size used to grow every tree\n    epsilon     : fraction (0 \u2264 \u03b5 < 1) of observations regarded as outliers\n\n    Returns\n    -------\n    A sorted list with the zero-based indices of the detected outliers.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    #  Preliminary checks / constants\n    # ------------------------------------------------------------------ #\n    np.random.seed(42)                        # global, deterministic seed\n    n_samples, n_features = data.shape\n\n    # no outliers requested\n    if epsilon == 0 or n_samples == 0:\n        return []\n\n    # size of every subsample actually drawn\n    subsz = min(sample_size, n_samples)\n    # height limit for every tree\n    height_limit = math.ceil(math.log2(subsz)) if subsz > 1 else 0\n\n    # ------------------------------------------------------------------ #\n    #  Helper functions\n    # ------------------------------------------------------------------ #\n    def c_factor(n: int) -> float:\n        \"\"\"Expected path length for an unsuccessful search in a BST.\"\"\"\n        if n <= 1:\n            return 0.0\n        return 2.0 * math.log(n - 1) + 0.5772156649 - 2.0 * (n - 1) / n\n\n    class Node:\n        __slots__ = (\"is_leaf\", \"size\",\n                     \"feat\", \"thr\", \"left\", \"right\")\n\n        def __init__(self, *, is_leaf: bool, size: int,\n                     feat=None, thr=None, left=None, right=None):\n            self.is_leaf = is_leaf\n            self.size = size\n            self.feat = feat\n            self.thr = thr\n            self.left = left\n            self.right = right\n\n    # -------------  tree building  ------------------------------------ #\n    def build_tree(idxs: np.ndarray, depth: int) -> Node:\n        \"\"\"Recursively build one isolation tree.\"\"\"\n        if depth >= height_limit or idxs.size <= 1:\n            return Node(is_leaf=True, size=idxs.size)\n\n        # random feature and its range\n        feat = np.random.randint(n_features)\n        col = data[idxs, feat]\n        f_min, f_max = col.min(), col.max()\n\n        # if all values equal => cannot split\n        if f_min == f_max:\n            return Node(is_leaf=True, size=idxs.size)\n\n        # random split point\n        thr = np.random.uniform(f_min, f_max)\n\n        # partition\n        left_mask = col < thr\n        right_mask = ~left_mask\n        if not left_mask.any() or not right_mask.any():\n            # degenerate split, fallback to leaf\n            return Node(is_leaf=True, size=idxs.size)\n\n        left_child = build_tree(idxs[left_mask], depth + 1)\n        right_child = build_tree(idxs[right_mask], depth + 1)\n        return Node(is_leaf=False, size=idxs.size,\n                    feat=feat, thr=thr,\n                    left=left_child, right=right_child)\n\n    # -------------  path length evaluation  --------------------------- #\n    def path_length(x: np.ndarray, node: Node, depth: int = 0) -> float:\n        \"\"\"Path length of one sample through one tree.\"\"\"\n        if node.is_leaf:\n            return depth + c_factor(node.size)\n\n        if x[node.feat] < node.thr:\n            return path_length(x, node.left, depth + 1) \\\n                if node.left is not None else depth + c_factor(node.size)\n        else:\n            return path_length(x, node.right, depth + 1) \\\n                if node.right is not None else depth + c_factor(node.size)\n\n    # ------------------------------------------------------------------ #\n    #  Grow the forest\n    # ------------------------------------------------------------------ #\n    forest: list[Node] = []\n    for _ in range(n_trees):\n        sample_indices = np.random.choice(n_samples, subsz, replace=False)\n        forest.append(build_tree(sample_indices, depth=0))\n\n    # ------------------------------------------------------------------ #\n    #  Compute mean path length for every observation\n    # ------------------------------------------------------------------ #\n    path_sum = np.zeros(n_samples, dtype=float)\n    for tree in forest:\n        for i in range(n_samples):\n            path_sum[i] += path_length(data[i], tree)\n\n    h_bar = path_sum / n_trees\n\n    # ------------------------------------------------------------------ #\n    #  Anomaly scores  s(x) = 2^{-(h_bar / \u03c6)}\n    #  with  \u03c6 = 2 ln(n-1) - 2 (n-1) / n\n    # ------------------------------------------------------------------ #\n    phi = 0.0\n    if n_samples > 1:\n        phi = 2.0 * math.log(n_samples - 1) - 2.0 * (n_samples - 1) / n_samples\n    else:                                   # single point (corner case)\n        phi = 1.0                           # arbitrary non-zero to avoid /0\n\n    scores = np.power(2.0, -h_bar / phi)\n\n    # ------------------------------------------------------------------ #\n    #  Pick the \u03b5 \u00b7 n_samples largest scores \u2192 outliers\n    # ------------------------------------------------------------------ #\n    k = int(math.floor(epsilon * n_samples))\n    if k == 0:\n        return []\n\n    largest_k_idx = np.argsort(scores)[-k:]      # indices of k largest\n    return sorted(largest_k_idx.tolist())\n```"}
{"task_id": 568, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"\n    Train a Maximum-Entropy classifier with Generalised Iterative Scaling\n    on (categorical) training data and return the label predictions for\n    X_test.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # helpers -----------------------------------------------------------\n    # ------------------------------------------------------------------\n    def active_sum(x_row, y_label):\n        \"\"\"\n        Sum of the weights that are active for (x_row , y_label).\n        Each (feature_index, feature_value, y_label) that matches x_row\n        contributes its current weight.\n        \"\"\"\n        s = 0.0\n        for i, val in enumerate(x_row):\n            idx = feat2id.get((i, val, y_label))\n            if idx is not None:\n                s += weights[idx]\n        return s\n\n    # ------------------------------------------------------------------\n    # build the feature set and empirical expectations -----------------\n    # ------------------------------------------------------------------\n    N, d = X_train.shape\n    labels = list(set(int(y) for y in y_train))          # all classes\n    C = d                                                # \u03a3_j f_j(x,y) is constant (=d)\n\n    feat_counts = defaultdict(int)                       # empirical counts\n    for x_row, y in zip(X_train, y_train):\n        y = int(y)\n        for i, val in enumerate(x_row):\n            feat_counts[(i, val, y)] += 1\n\n    n_feats = len(feat_counts)\n    feat2id = {}                                         # map feature \u2192 column in weight vector\n    emp_exp = np.zeros(n_feats, dtype=float)             # empirical expectations\n    for idx, (feat, cnt) in enumerate(feat_counts.items()):\n        feat2id[feat] = idx\n        emp_exp[idx] = cnt / N                           # 1/N \u03a3_i f_j(x_i , y_i)\n\n    # ------------------------------------------------------------------\n    # training with GIS -------------------------------------------------\n    # ------------------------------------------------------------------\n    weights = np.zeros(n_feats, dtype=float)\n    tiny = 1e-12                                         # to avoid div/0\n\n    for _ in range(n_iter):\n\n        mod_exp = np.zeros(n_feats, dtype=float)         # model expectations\n\n        # expectation under the current model\n        for x_row in X_train:\n            # raw scores for every label\n            raw = {y: active_sum(x_row, y) for y in labels}\n            max_raw = max(raw.values())                  # for numerical stability\n\n            # normalised probabilities P(y|x)\n            exp_scores = {y: math.exp(raw[y] - max_raw) for y in labels}\n            Z = sum(exp_scores.values())\n            probs = {y: exp_scores[y] / Z for y in labels}\n\n            # accumulate expectations\n            for y, p in probs.items():\n                if p == 0.0:\n                    continue\n                for i, val in enumerate(x_row):\n                    idx = feat2id.get((i, val, y))\n                    if idx is not None:\n                        mod_exp[idx] += p\n\n        mod_exp /= N                                     # 1/N \u03a3_i \u03a3_y P(y|x_i) f_j(x_i , y)\n\n        # weight update\n        max_change = 0.0\n        for j in range(n_feats):\n            delta = (1.0 / C) * math.log((emp_exp[j] + tiny) / (mod_exp[j] + tiny))\n            weights[j] += delta\n            if abs(delta) > max_change:\n                max_change = abs(delta)\n\n        if max_change < epsilon:\n            break\n\n    # ------------------------------------------------------------------\n    # prediction --------------------------------------------------------\n    # ------------------------------------------------------------------\n    preds = []\n    for x_row in X_test:\n        best_label, best_score = None, -float(\"inf\")\n        for y in labels:\n            score = active_sum(x_row, y)\n            if score > best_score:\n                best_score = score\n                best_label = y\n        preds.append(int(best_label))\n\n    return preds\n```"}
{"task_id": 569, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate per-word document-frequency statistics.\n\n    The function returns\n        1. a list of (word, ratio) pairs where *ratio* is the fraction of\n           documents in which the word occurs (rounded to 4 decimals),\n           sorted from the most to the least frequent word;\n        2. a set that contains only the *cut_off* most frequent words\n           (all words if *cut_off* is None).\n\n    Args:\n        data: list/iterable of documents.  Each document is a tuple\n              ``(label, words)`` where *words* is any iterable of strings.\n        cut_off: maximal number of top words to put into the returned set, or\n                 ``None`` for *all* words.\n\n    Returns:\n        Tuple[list[(word, ratio)], set[str]]\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. Basic checks / early exit\n    # ------------------------------------------------------------------ #\n    n_docs = len(data)\n    if n_docs == 0:                       # empty input \u2192 empty result\n        return [], set()\n\n    # ------------------------------------------------------------------ #\n    # 2. Count in how many documents each word appears\n    # ------------------------------------------------------------------ #\n    doc_counter: Counter[str] = Counter()\n\n    for _, words in data:\n        doc_counter.update(set(words))    # use a set \u2192 count each word once/-doc\n\n    # ------------------------------------------------------------------ #\n    # 3. Build (word, ratio) list (rounded to 4 decimals)\n    # ------------------------------------------------------------------ #\n    freq_list = [\n        (word, round(count / n_docs, 4))\n        for word, count in doc_counter.items()\n    ]\n\n    # Sort by ratio (descending).  Secondary sort on the word itself simply\n    # guarantees deterministic output when two ratios are identical.\n    freq_list.sort(key=lambda pair: (-pair[1], pair[0]))\n\n    # ------------------------------------------------------------------ #\n    # 4. Build the \u201ctop\u2010k\u201d set\n    # ------------------------------------------------------------------ #\n    if cut_off is None:\n        top_k_set = {word for word, _ in freq_list}\n    elif cut_off <= 0:\n        top_k_set = set()\n    else:\n        # If cut_off is larger than #words slicing will just give all of them.\n        top_k_set = {word for word, _ in freq_list[:cut_off]}\n\n    return freq_list, top_k_set\n```"}
{"task_id": 571, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Training data.\n    y : np.ndarray, shape (n_samples,)\n        Class labels (+1 or \u22121).\n    alpha : np.ndarray, shape (n_samples,)\n        Optimised Lagrange multipliers.\n\n    Returns\n    -------\n    (w_list, b) : tuple[list[float], float]\n        w_list : the weight vector rounded to 4 decimals (Python list)\n        b       : the bias term rounded to 4 decimals (float)\n    \"\"\"\n    # Element-wise product \u03b1 \u2299 y   (shape: n_samples,)\n    alpha_y = alpha * y\n\n    # Weight vector:  w = X\u1d40 (\u03b1 \u2299 y)      (shape: n_features,)\n    w = X.T @ alpha_y\n\n    # Bias: b = 1/n \u03a3_i (y_i \u2212 w\u1d40 x_i)\n    predictions = X @ w                    # w\u1d40 x_i for every sample (shape: n_samples,)\n    b = np.mean(y - predictions)\n\n    # Rounding\n    w_list = [round(float(w_i), 4) for w_i in w]\n    b = round(float(b), 4)\n\n    return w_list, b\n```"}
{"task_id": 572, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"\n    Computes the Local-Outlier-Factor (LOF) for every sample in `data`\n    and returns\n        (list_of_LOF_scores_rounded_to_4_decimals ,\n         ascending_list_of_indices_with_LOF>epsilon)\n\n    Parameters\n    ----------\n    data : array-like, shape = (n_samples, n_features) or (n_samples,)\n    k    : int, 1 \u2264 k < n_samples\n    epsilon : float, threshold that decides which samples are flagged\n              as outliers        (default 1.5)\n\n    Notes\n    -----\n    \u2022  NO external ML / scientific libraries are used \u2013 only `numpy`.\n    \u2022  All LOF scores are rounded with `numpy.round(score, 4)`.\n    \"\"\"\n\n    # ---- 0. Sanity checks ----------------------------------------------------\n    data = np.asarray(data, dtype=float)\n    if data.ndim == 1:                             # make 1-D data 2-D\n        data = data[:, None]\n    n_samples = data.shape[0]\n\n    if not (1 <= k < n_samples):\n        raise ValueError(\"k must satisfy 1 \u2264 k < n_samples (got k = %d)\" % k)\n\n    # ---- 1. Full pair-wise Euclidean distance matrix ------------------------\n    # shape : (n_samples, n_samples)\n    diff = data[:, None, :] - data[None, :, :]\n    dist_mat = np.sqrt(np.sum(diff**2, axis=2))\n\n    # ---- 2. k-distance and k nearest neighbours -----------------------------\n    k_distances   = np.empty(n_samples)            # k-distance(p)\n    neighbours    = np.empty((n_samples, k), int)  # N_k(p)\n\n    for i in range(n_samples):\n        idx_sorted = np.argsort(dist_mat[i])       # includes index i (dist = 0)\n        neigh_idx  = idx_sorted[1:k+1]             # skip itself\n        neighbours[i]  = neigh_idx\n        k_distances[i] = dist_mat[i, neigh_idx[-1]]\n\n    # ---- 3 & 4. Reachability distances and Local Reachability Density -------\n    lrd = np.empty(n_samples)\n    for p in range(n_samples):\n        reach_dist_sum = 0.0\n        for q in neighbours[p]:\n            reach_dist = max(k_distances[q], dist_mat[p, q])\n            reach_dist_sum += reach_dist\n        lrd[p] = k / reach_dist_sum\n\n    # ---- 5. Local Outlier Factor --------------------------------------------\n    lof = np.empty(n_samples)\n    for p in range(n_samples):\n        lrd_sum = lrd[neighbours[p]].sum()\n        lof[p]  = (lrd_sum / k) / lrd[p]\n\n    # ---- 6. Rounding & outlier indices --------------------------------------\n    lof_rounded = np.round(lof, 4).tolist()\n    outlier_idx = [i for i, score in enumerate(lof) if score > epsilon]\n\n    return (lof_rounded, outlier_idx)\n```\n\nThe routine follows the exact mathematical steps of the LOF algorithm:\n\n1. full pair-wise Euclidean distances  \n2. extraction of each point\u2019s k-distance and its k nearest neighbours  \n3. reach-dist\u2096(p,q)=max(k-distance(q),\u2006dist(p,q))  \n4. LRD\u2096(p)=k / \u03a3 reach-dist\u2096(p,q)  \n5. LOF\u2096(p)= (\u03a3 LRD\u2096(q)/k) / LRD\u2096(p)  \n\nAll LOF scores are rounded to four decimals for the return value, while the raw (un-rounded) scores are used for the outlier decision (`score > epsilon`)."}
{"task_id": 574, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the current global\n        random state is used unchanged.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays with shapes\n        (m\u00b7n_train, 2), (m\u00b7n_val, 2), (m\u00b7n_train,) and (m\u00b7n_val,)\n        respectively.  Every coordinate is rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    X_tr, X_va = [], []\n    y_tr, y_va = [], []\n\n    for c in range(m):\n        base = 10 * (c // 2)          # 10\u00b7\u230ac/2\u230b\n\n        # Training points for class c\n        for _ in range(n_train):\n            x = random.uniform(base + 1, base + 9)\n            y = random.uniform(base + 1, base + 9)\n            X_tr.append((x, y))\n            y_tr.append(c)\n\n        # Validation points for class c\n        for _ in range(n_val):\n            x = random.uniform(base + 1, base + 9)\n            y = random.uniform(base + 1, base + 9)\n            X_va.append((x, y))\n            y_va.append(c)\n\n    # Convert to NumPy arrays and round coordinates\n    X_train = np.round(np.asarray(X_tr, dtype=float), 4)\n    X_val   = np.round(np.asarray(X_va, dtype=float), 4)\n    y_train = np.asarray(y_tr, dtype=int)\n    y_val   = np.asarray(y_va, dtype=int)\n\n    return X_train, X_val, y_train, y_val\n```"}
{"task_id": 579, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Deterministic Lloyd-style K-Means clustering.\n\n    Args:\n        X        : (m, d) array with the input points.\n        k        : number of clusters, 1 \u2264 k \u2264 m.\n        max_iters: maximum amount of Lloyd iterations.\n\n    Returns\n    -------\n        labels  : list with length m, the cluster index of every point.\n        centres : list with k inner lists \u2013 the final centres, each\n                  coordinate rounded to 4 decimals.\n    \"\"\"\n    # --- basic checks / preparation -----------------------------------------\n    X = np.asarray(X, dtype=float)\n    if X.ndim != 2:\n        raise ValueError(\"`X` must be a 2-D array (m, d).\")\n    m, d = X.shape\n    if not (1 <= k <= m):\n        raise ValueError(\"`k` must satisfy 1 \u2264 k \u2264 number of points.\")\n    if max_iters <= 0:\n        raise ValueError(\"`max_iters` must be positive.\")\n\n    # --- initialisation ------------------------------------------------------\n    centres = X[:k].copy()                # (k, d) \u2013 first k points, deterministic\n    labels  = np.full(m, -1, dtype=int)   # previous assignment (initially none)\n\n    # --- Lloyd iterations ----------------------------------------------------\n    for _ in range(max_iters):\n        # Assignment step -----------------------------------------------------\n        # squared Euclidean distances \u2013 avoids the costly square root,\n        # argmin still gives the same result and breaks ties towards\n        # the smaller index automatically.\n        diff   = X[:, None, :] - centres[None, :, :]   # (m, k, d)\n        dist2  = np.sum(diff ** 2, axis=2)             # (m, k)\n        new_labels = np.argmin(dist2, axis=1)          # (m,)\n\n        # Convergence test ----------------------------------------------------\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # Update step ---------------------------------------------------------\n        for i in range(k):\n            mask = labels == i\n            if np.any(mask):                           # centre received points\n                centres[i] = X[mask].mean(axis=0)\n            # else: centre keeps its previous position (per task statement)\n\n    # --- rounding of the final centres ---------------------------------------\n    centres_rounded = [[round(val, 4) for val in centre] for centre in centres]\n\n    return labels.tolist(), centres_rounded\n```"}
{"task_id": 581, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation using batch GD.\"\"\"\n    \n    m, n = X.shape                          # m \u2026 samples, n \u2026 features\n    \n    # Return the initial parameters if nothing can change\n    if learning_rate == 0 or iterations == 0:\n        return ([0.0] * n, 0.0)\n\n    w = np.zeros(n, dtype=float)            # weights w1 \u2026 wn\n    b = 0.0                                 # bias\n\n    for _ in range(iterations):\n        y_hat = X @ w + b                   # predictions\n        r = y - y_hat                       # residuals\n\n        # sign(w) with the special rule sign(0) == -1\n        sign_w = np.sign(w)\n        sign_w[sign_w == 0] = -1\n\n        # Gradients\n        grad_w = (-2 * X.T @ r) + l1_penalty * sign_w + 2 * l2_penalty * w\n        grad_b = -2 * r.sum()\n\n        # Mean gradients and parameter update\n        w -= learning_rate * (grad_w / m)\n        b -= learning_rate * (grad_b / m)\n\n    # Round the results to four decimal places and return as Python types\n    weights_rounded = [round(float(val), 4) for val in w]\n    bias_rounded = round(float(b), 4)\n\n    return (weights_rounded, bias_rounded)\n```"}
{"task_id": 591, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Forward pass of a fully\u2013connected ReLU network with a linear output layer.\n\n    Args:\n        weights : Three-level nested list holding all weight matrices.\n        x       : Input features (without bias).\n\n    Returns:\n        Network prediction rounded to four decimals.\n        Returns a float for a single output neuron, otherwise a list of floats.\n    \"\"\"\n    # current layer input as a NumPy array\n    h = np.asarray(x, dtype=float)\n\n    n_layers = len(weights)\n\n    for L, layer in enumerate(weights):\n        # shape: (n_neurons, prev_layer_size + 1)\n        W = np.asarray(layer, dtype=float)\n\n        # prepend bias input \u201c1\u201d\n        h_aug = np.concatenate(([1.0], h))\n\n        # linear combination\n        z = W @ h_aug     # shape: (n_neurons,)\n\n        # activation: ReLU for hidden layers, identity for output layer\n        h = np.maximum(0, z) if L < n_layers - 1 else z\n\n    # round to 4 decimals\n    if h.size == 1:\n        return round(float(h[0]), 4)\n    else:\n        return [round(v, 4) for v in h.tolist()]\n```"}
{"task_id": 595, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"\n    K-Means clustering \u2013 return **centroids only**.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    k : int\n        Number of clusters.\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Sorted centroids (lexicographically), each coordinate rounded to 4 decimals.\n    \"\"\"\n    # ------ basic sanity check ------------------------------------------------\n    if k <= 0:\n        raise ValueError(\"k must be a positive integer\")\n    if X.ndim != 2:\n        raise ValueError(\"X must be 2-dimensional\")\n\n    X = X.astype(float, copy=False)           # ensure float for arithmetic\n    n_samples, n_features = X.shape\n    if k > n_samples:\n        raise ValueError(\"k cannot be larger than number of samples\")\n    \n    # ------ 1. deterministic initialisation ----------------------------------\n    centroids = X[:k].copy()                 # shape (k, n_features)\n\n    # ------ 2. main loop ------------------------------------------------------\n    for _ in range(max_iters):\n        # 2.a assign to nearest centroid (squared Euclidean distance)\n        #    distances: (n_samples, k)\n        distances = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        # 2.b update centroids\n        new_centroids = centroids.copy()\n        for j in range(k):\n            mask = labels == j\n            if np.any(mask):                          # cluster non-empty\n                new_centroids[j] = X[mask].mean(axis=0)\n            # else: keep old centroid unchanged\n\n        # 3. early stopping\n        shifts = np.linalg.norm(new_centroids - centroids, axis=1)\n        centroids = new_centroids\n        if np.all(shifts < 1e-4):\n            break\n\n    # ------ 4. final formatting ----------------------------------------------\n    # sort lexicographically\n    centroids_list = centroids.tolist()\n    centroids_list.sort()                    # python\u2019s default list sort is lexicographic\n\n    # round each coordinate to 4 decimals\n    rounded = [[round(val, 4) for val in centre] for centre in centroids_list]\n    return rounded\n```"}
