{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Compute the dot product of a matrix `a` with a vector `b`.\n    \n    Parameters\n    ----------\n    a : list[list[int | float]]\n        An n \u00d7 m matrix represented as a list of n rows, \n        each row being a list of length m.\n    b : list[int | float]\n        A vector (list) of length m.\n    \n    Returns\n    -------\n    list[int | float]\n        A list of length n, where each entry is the dot product of\n        the corresponding row of `a` with `b`.\n    int\n        Returns -1 if dimensions are incompatible.\n    \"\"\"\n    if not isinstance(a, list) or not isinstance(b, list):\n        return -1\n    if len(a) == 0:\n        return [] if len(b) == 0 else -1\n    num_cols = len(a[0])\n    if num_cols != len(b):\n        return -1\n    for row in a:\n        if len(row) != num_cols:\n            return -1\n    result: list[int | float] = []\n    for row in a:\n        dot_val = sum((row[i] * b[i] for i in range(num_cols)))\n        result.append(dot_val)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "from typing import List, Union\nNumber = Union[int, float]\ndef transpose_matrix(a: List[List[Number]]) -> List[List[Number]]:\n    \"\"\"\n    Return the transpose of the 2-D matrix `a`.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        A rectangular list-of-lists representing the matrix.\n\n    Returns\n    -------\n    list[list[int | float]]\n        The transposed matrix.\n\n    Raises\n    ------\n    ValueError\n        If `a` is not rectangular (i.e., its rows differ in length).\n\n    Examples\n    --------\n    >>> transpose_matrix([[1, 2, 3], [4, 5, 6]])\n    [[1, 4], [2, 5], [3, 6]]\n    >>> transpose_matrix([])\n    []\n    \"\"\"\n    if not a:\n        return []\n    row_lengths = {len(row) for row in a}\n    if len(row_lengths) != 1:\n        raise ValueError('All rows must have the same length to form a proper matrix.')\n    return [list(col) for col in zip(*a)]"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape a matrix (list of lists) to the requested shape.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        Input matrix.\n    new_shape : tuple[int, int]\n        Desired shape in the form (rows, columns).\n\n    Returns\n    -------\n    list[list[int | float]]\n        The reshaped matrix as a list of lists if possible;\n        otherwise an empty list ([]).\n    \"\"\"\n    if not isinstance(new_shape, tuple) or len(new_shape) != 2 or (not all((isinstance(x, int) and x > 0 for x in new_shape))):\n        return []\n    (rows, cols) = new_shape\n    flat = [item for row in a for item in row]\n    if len(flat) != rows * cols:\n        return []\n    reshaped = np.array(flat).reshape(rows, cols).tolist()\n    return reshaped"}
{"task_id": 4, "completion_id": 0, "solution": "from typing import List\nimport statistics\ndef calculate_matrix_mean(matrix: List[List[float]], mode: str) -> List[float]:\n    \"\"\"\n    Calculate the arithmetic mean of a 2-D numeric matrix either row-wise or column-wise.\n\n    Parameters\n    ----------\n    matrix : list[list[float]]\n        A rectangular list-of-lists (all rows must have the same length for\n        column mode).\n    mode : str\n        Either \"row\" or \"column\" (case\u2013insensitive).\n\n    Returns\n    -------\n    list[float]\n        A list containing the mean of every row (for mode=\"row\") or every\n        column (for mode=\"column\").\n\n    Raises\n    ------\n    ValueError\n        If the mode is not \"row\" or \"column\", or if the matrix is jagged when\n        column means are requested.\n    \"\"\"\n    mode = mode.lower()\n    if mode not in {'row', 'column'}:\n        raise ValueError('mode must be either \"row\" or \"column\"')\n    if not matrix:\n        return []\n    if mode == 'row':\n        return [statistics.mean(row) if row else float('nan') for row in matrix]\n    num_cols = len(matrix[0])\n    if any((len(row) != num_cols for row in matrix)):\n        raise ValueError('All rows must have the same length to compute column means')\n    return [statistics.mean(col) for col in zip(*matrix)]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiply every element of a 2-D matrix by a scalar.\n\n    Parameters\n    ----------\n    matrix : list[list[int | float]]\n        A rectangular (rows of equal length) 2-D list representing the matrix.\n    scalar : int | float\n        The value by which each element of the matrix is to be multiplied.\n\n    Returns\n    -------\n    list[list[int | float]]\n        A new matrix where each element is the product of the corresponding\n        element in `matrix` and `scalar`.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "import cmath\nfrom typing import Union, List\nNumber = Union[int, float, complex]\ndef calculate_eigenvalues(matrix: List[List[Number]]) -> List[Number]:\n    \"\"\"\n    Calculate (and return in descending order) the eigenvalues of a 2\u00d72 matrix.\n\n    Parameters\n    ----------\n    matrix : list[list[float | int]]\n        2\u00d72 matrix represented as nested lists\n        [[a, b],\n         [c, d]]\n\n    Returns\n    -------\n    list[Number]\n        Eigenvalues \u03bb\u2081, \u03bb\u2082 sorted from highest to lowest (by real part).\n        They can be real or complex numbers.\n    \"\"\"\n    if len(matrix) != 2 or any((len(row) != 2 for row in matrix)):\n        raise ValueError('Input must be a 2\u00d72 matrix (list of two lists, each of length two).')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_disc = cmath.sqrt(discriminant)\n    \u03bb1 = (trace + sqrt_disc) / 2\n    \u03bb2 = (trace - sqrt_disc) / 2\n    eigenvalues = sorted([\u03bb1, \u03bb2], key=lambda x: (x.real, x.imag), reverse=True)\n    return eigenvalues"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Perform the matrix transformation  T^{-1} \u00b7 A \u00b7 S\n    --------------------------------------------------\n    Parameters\n    ----------\n    A : 2-D list\n        Base matrix to be transformed.\n    T : 2-D list (square)\n        Row-basis change matrix. Must be invertible and its order\n        must equal the number of rows of A.\n    S : 2-D list (square)\n        Column-basis change matrix. Must be invertible and its order\n        must equal the number of columns of A.\n\n    Returns\n    -------\n    2-D python list (numbers rounded to 4 decimals) or -1 if\n    * matrices are not conformable, or\n    * T or S is not invertible.\n    \"\"\"\n    try:\n        A_np = np.asarray(A, dtype=float)\n        T_np = np.asarray(T, dtype=float)\n        S_np = np.asarray(S, dtype=float)\n        if T_np.ndim != 2 or S_np.ndim != 2 or A_np.ndim != 2:\n            return -1\n        (nT, mT) = T_np.shape\n        (nS, mS) = S_np.shape\n        (nA, mA) = A_np.shape\n        if nT != mT or nS != mS:\n            return -1\n        if nT != nA or nS != mA:\n            return -1\n        if abs(np.linalg.det(T_np)) < 1e-12 or abs(np.linalg.det(S_np)) < 1e-12:\n            return -1\n        T_inv = np.linalg.inv(T_np)\n        transformed = T_inv @ A_np @ S_np\n        transformed = np.round(transformed, 4)\n        return transformed.tolist()\n    except Exception:\n        return -1"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    \"\"\"\n    Return the inverse of a 2\u00d72 matrix.\n    \n    Args:\n        matrix (list[list[float]]): A 2\u00d72 matrix in row-major order\n                                    [[a, b],\n                                     [c, d]]\n    \n    Returns:\n        list[list[float]] | None: The inverse matrix if it exists,\n                                  otherwise None when the matrix\n                                  is singular (non-invertible).\n    \"\"\"\n    if not isinstance(matrix, list) or len(matrix) != 2 or any((not isinstance(row, list) or len(row) != 2 for row in matrix)):\n        raise ValueError('Input must be a 2\u00d72 matrix provided as [[a, b], [c, d]].')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if abs(det) < 1e-12:\n        return None\n    inv_det = 1.0 / det\n    return [[d * inv_det, -b * inv_det], [-c * inv_det, a * inv_det]]"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Multiply two matrices a and b using classic row-by-column multiplication.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        Left matrix  (m \u00d7 n)\n    b : list[list[int | float]]\n        Right matrix (n \u00d7 p)\n\n    Returns\n    -------\n    list[list[int | float]]\n        Resulting matrix (m \u00d7 p) if the inner dimensions match.\n    -1\n        Returned when matrices are empty, jagged, or dimensions are\n        incompatible for multiplication.\n    \"\"\"\n    if not a or not b:\n        return -1\n    a_row_len = len(a[0])\n    if any((len(row) != a_row_len for row in a)):\n        return -1\n    b_row_len = len(b[0])\n    if any((len(row) != b_row_len for row in b)):\n        return -1\n    if a_row_len != len(b):\n        return -1\n    result: list[list[int | float]] = []\n    for row_a in a:\n        new_row = []\n        for col_idx in range(b_row_len):\n            val = sum((row_a[k] * b[k][col_idx] for k in range(a_row_len)))\n            new_row.append(val)\n        result.append(new_row)\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "from typing import List\nimport math\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the (sample) covariance matrix for a list of feature-vectors.\n\n    Parameters\n    ----------\n    vectors : list[list[float]]\n        Each inner list is a feature (all observations for that feature).\n        All inner lists must be the same length (same number of observations).\n\n    Returns\n    -------\n    list[list[float]]\n        A square matrix (as a list of lists) where entry (i, j)\n        is the sample covariance between feature i and feature j.\n    \"\"\"\n    if not vectors:\n        raise ValueError(\"Input list 'vectors' is empty.\")\n    n_features = len(vectors)\n    n_observations = len(vectors[0])\n    for v in vectors:\n        if len(v) != n_observations:\n            raise ValueError('All feature vectors must have the same number of observations.')\n        if n_observations < 2:\n            raise ValueError('At least two observations are required to compute covariance.')\n    means = [sum(v) / n_observations for v in vectors]\n    cov_matrix = [[0.0 for _ in range(n_features)] for _ in range(n_features)]\n    for i in range(n_features):\n        for j in range(i, n_features):\n            cov_ij = 0.0\n            for k in range(n_observations):\n                cov_ij += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            cov_ij /= n_observations - 1\n            cov_matrix[i][j] = cov_ij\n            cov_matrix[j][i] = cov_ij\n    return cov_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve Ax = b using the Jacobi iterative method.\n\n    Parameters\n    ----------\n    A : np.ndarray\n        Square coefficient matrix.\n    b : np.ndarray\n        Right-hand-side vector (or 1-D array).\n    n : int\n        Number of Jacobi iterations to perform.\n\n    Returns\n    -------\n    list\n        Approximate solution vector (rounded to 4 decimals) as a Python list.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).flatten()\n    (rows, cols) = A.shape\n    if rows != cols:\n        raise ValueError('Matrix A must be square.')\n    if b.size != rows:\n        raise ValueError(\"Vector b must have the same length as A's dimension.\")\n    if np.any(np.isclose(np.diag(A), 0.0)):\n        raise ValueError(\"Zero detected on A's diagonal; Jacobi method cannot proceed.\")\n    D = np.diag(A)\n    R = A - np.diagflat(D)\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x = (b - R @ x) / D\n        x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the singular values of a 2\u00d72 matrix using a single-sweep Jacobi\n    (a.k.a. Jacobi \u201cJacobian\u201d) rotation on A\u1d40A.  The algorithm is exact for a\n    2\u00d72, yet the flow mirrors what would be done for larger matrices with a\n    full Jacobi SVD.\n\n    Parameters\n    ----------\n    A : np.ndarray\n        A real 2\u00d72 matrix.\n\n    Returns\n    -------\n    tuple\n        (\u03c3\u2081, \u03c3\u2082) \u2014 the singular values sorted in descending order and rounded\n        to 4 decimal places.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input must be a 2\u00d72 matrix.')\n    B = A.T @ A\n    (b00, b01, b11) = (B[0, 0], B[0, 1], B[1, 1])\n    if abs(b01) > 1e-12:\n        tau = (b11 - b00) / (2.0 * b01)\n        t = np.sign(tau) / (abs(tau) + np.sqrt(1.0 + tau * tau))\n        c = 1.0 / np.sqrt(1.0 + t * t)\n        s = c * t\n        V = np.array([[c, -s], [s, c]])\n        Sigma2 = V.T @ B @ V\n    else:\n        Sigma2 = B\n    (sigma1, sigma2) = np.sqrt(np.diag(Sigma2))\n    if sigma1 < sigma2:\n        (sigma1, sigma2) = (sigma2, sigma1)\n    sigma1 = float(np.round(sigma1, 4))\n    sigma2 = float(np.round(sigma2, 4))\n    return (sigma1, sigma2)"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Compute the determinant of a 4\u00d74 matrix via Laplace\u2019s expansion.\n\n    Parameters\n    ----------\n    matrix : list[list[int | float]]\n        A 4\u00d74 matrix represented as a list of four lists, each of length four.\n\n    Returns\n    -------\n    float\n        Determinant of the given matrix.\n\n    Raises\n    ------\n    ValueError\n        If the supplied matrix is not 4\u00d74.\n    \"\"\"\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4\u00d74 matrix (list of four lists, each of length four).')\n\n    def det(mat: list[list[int | float]]) -> float:\n        n = len(mat)\n        if n == 1:\n            return mat[0][0]\n        if n == 2:\n            return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n        total = 0.0\n        for col in range(n):\n            sign = -1 if col % 2 else 1\n            minor = [[mat[r][c] for c in range(n) if c != col] for r in range(1, n)]\n            total += sign * mat[0][col] * det(minor)\n        return total\n    return float(det(matrix))"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Fits a linear regression model using the normal equation.\n\n    Parameters\n    ----------\n    X : list[list[float]]\n        Feature matrix (each inner list is one sample\u2019s features).\n    y : list[float]\n        Target vector.\n\n    Returns\n    -------\n    list[float]\n        The regression coefficients, including the intercept as the\n        first element.  All coefficients are rounded to 4 decimal places.\n    \"\"\"\n    X_mat = np.asarray(X, dtype=float)\n    y_vec = np.asarray(y, dtype=float).reshape(-1, 1)\n    ones = np.ones((X_mat.shape[0], 1), dtype=float)\n    X_aug = np.hstack((ones, X_mat))\n    theta = np.linalg.pinv(X_aug.T @ X_aug) @ X_aug.T @ y_vec\n    theta = np.round(theta.flatten(), 4)\n    return theta.tolist()"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Perform linear regression with gradient descent.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with a leading column of ones for the intercept term.\n        Shape (m, n) where m is the number of samples.\n    y : np.ndarray\n        Target values. Shape (m,) or (m, 1).\n    alpha : float\n        Learning-rate.\n    iterations : int\n        Number of gradient-descent steps.\n\n    Returns\n    -------\n    list\n        Model coefficients (including intercept) rounded to 4 decimals.\n    \"\"\"\n    y = y.reshape(-1, 1)\n    (m, n) = X.shape\n    theta = np.zeros((n, 1))\n    for _ in range(iterations):\n        predictions = X @ theta\n        errors = predictions - y\n        gradient = X.T @ errors / m\n        theta -= alpha * gradient\n    theta = np.round(theta, 4).reshape(-1).tolist()\n    return theta"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform column-wise feature scaling on a 2-D NumPy array.\n    \n    Returns\n    -------\n    standardized_list : list[list[float]]\n        Z-score scaled features: (x - \u03bc) / \u03c3, rounded to 4 decimals.\n    minmax_list : list[list[float]]\n        Min-max scaled features: (x - min) / (max - min), rounded to 4 decimals.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('Input array must be 2-D (samples \u00d7 features).')\n    means = data.mean(axis=0)\n    stds = data.std(axis=0, ddof=0)\n    stds_safe = np.where(stds == 0, 1, stds)\n    standardized = (data - means) / stds_safe\n    standardized[:, stds == 0] = 0\n    standardized = np.round(standardized, 4)\n    mins = data.min(axis=0)\n    maxs = data.max(axis=0)\n    ranges = maxs - mins\n    ranges_safe = np.where(ranges == 0, 1, ranges)\n    minmax = (data - mins) / ranges_safe\n    minmax[:, ranges == 0] = 0\n    minmax = np.round(minmax, 4)\n    return (standardized.tolist(), minmax.tolist())"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Simple, fully-vectorised k-Means clustering.\n\n    Parameters\n    ----------\n    points : list[tuple[float, float]]\n        Samples to cluster (works for any dimensionality, not only 2-D).\n    k : int\n        Number of clusters.\n    initial_centroids : list[tuple[float, float]]\n        Starting positions of the centroids.\n    max_iterations : int\n        Maximum number of updates before the algorithm stops.\n\n    Returns\n    -------\n    list[tuple[float, float]]\n        Final centroid coordinates, rounded to 4 decimal places.\n    \"\"\"\n    pts = np.asarray(points, dtype=float)\n    centroids = np.asarray(initial_centroids, dtype=float)\n    if centroids.shape[0] != k:\n        raise ValueError('Length of initial_centroids must equal k.')\n    if pts.ndim != 2 or centroids.ndim != 2:\n        raise ValueError('Points and centroids must be 2-D arrays.')\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(pts[:, None, :] - centroids[None, :, :], axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.empty_like(centroids)\n        for idx in range(k):\n            cluster_pts = pts[labels == idx]\n            if len(cluster_pts):\n                new_centroids[idx] = cluster_pts.mean(axis=0)\n            else:\n                new_centroids[idx] = centroids[idx]\n        if np.allclose(new_centroids, centroids):\n            centroids = new_centroids\n            break\n        centroids = new_centroids\n    return [tuple(np.round(c, 4)) for c in centroids]"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k: int=5, shuffle: bool=True, random_seed: int | None=None):\n    \"\"\"\n    Generate train / test indices for K-Fold cross-validation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, \u2026).\n    y : np.ndarray\n        Target vector/array of shape (n_samples, \u2026).  Only the first\n        dimension is used to validate consistency with `X`.\n    k : int, default=5\n        Number of folds.\n    shuffle : bool, default=True\n        Whether to shuffle samples before splitting.\n    random_seed : int or None, default=None\n        Seed for the RNG used when `shuffle=True`.\n\n    Returns\n    -------\n    folds : list[tuple[np.ndarray, np.ndarray]]\n        List of length `k`.  Each element is a tuple\n        `(train_indices, test_indices)` where both are 1-D `np.ndarray`\n        objects containing the integer indices of samples that belong\n        to the training and test set, respectively, for that fold.\n\n    Raises\n    ------\n    ValueError\n        If `k` is less than 2 or greater than the number of samples.\n    \"\"\"\n    n_samples = len(X)\n    if n_samples != len(y):\n        raise ValueError('`X` and `y` must have the same number of samples.')\n    if k < 2:\n        raise ValueError('`k` must be at least 2.')\n    if k > n_samples:\n        raise ValueError('`k` cannot be greater than the number of samples.')\n    if shuffle:\n        rng = np.random.default_rng(random_seed)\n        indices = rng.permutation(n_samples)\n    else:\n        indices = np.arange(n_samples)\n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[:n_samples % k] += 1\n    folds = []\n    current = 0\n    for fold_size in fold_sizes:\n        (start, stop) = (current, current + fold_size)\n        test_idx = indices[start:stop]\n        train_idx = np.concatenate((indices[:start], indices[stop:]))\n        folds.append((train_idx, test_idx))\n        current = stop\n    return folds"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis on a dataset.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array with shape (n_samples, n_features).\n    k : int\n        Number of principal components to return (k <= n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        A list containing the first k principal components.\n        Each inner list is an eigenvector (length = n_features),\n        rounded to 4 decimal places.\n    \"\"\"\n    X = data.astype(float)\n    mean = X.mean(axis=0)\n    std = X.std(axis=0, ddof=1)\n    X_std = (X - mean) / std\n    cov_mat = np.cov(X_std, rowvar=False)\n    (eig_vals, eig_vecs) = np.linalg.eigh(cov_mat)\n    sorted_idx = np.argsort(eig_vals)[::-1]\n    eig_vals = eig_vals[sorted_idx]\n    eig_vecs = eig_vecs[:, sorted_idx]\n    principal_components = eig_vecs[:, :k].T\n    principal_components = np.round(principal_components, 4).tolist()\n    return principal_components"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom copy import deepcopy\nfrom typing import Any, Dict, List\ndef entropy(examples: List[dict], target_attr: str) -> float:\n    \"\"\"\n    Shannon entropy of the class label in `examples`.\n    \"\"\"\n    total = len(examples)\n    if not total:\n        return 0.0\n    counts = Counter((ex[target_attr] for ex in examples))\n    return -sum((n / total * math.log2(n / total) for n in counts.values()))\ndef information_gain(examples: List[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    Information gain obtained by splitting `examples` on `attr`.\n    \"\"\"\n    total = len(examples)\n    if not total:\n        return 0.0\n    partitions: Dict[Any, List[dict]] = {}\n    for ex in examples:\n        key = ex[attr]\n        partitions.setdefault(key, []).append(ex)\n    weighted_entropy = sum((len(part) / total * entropy(part, target_attr) for part in partitions.values()))\n    return entropy(examples, target_attr) - weighted_entropy\ndef majority_class(examples: List[dict], target_attr: str) -> Any:\n    \"\"\"\n    Most common class label in `examples`.\n    \"\"\"\n    if not examples:\n        return None\n    return Counter((ex[target_attr] for ex in examples)).most_common(1)[0][0]\ndef learn_decision_tree(examples: List[dict], attributes: List[str], target_attr: str) -> Dict[str, Any]:\n    \"\"\"\n    Learn a decision tree (ID3 style) using entropy / information gain.\n\n    Parameters\n    ----------\n    examples : list of dict\n        Each element is a sample represented as {attr_name: value, ...}.\n    attributes : list of str\n        The candidate attributes we may still split on (target excluded).\n    target_attr : str\n        Name of the class label attribute.\n\n    Returns\n    -------\n    tree : dict\n        A nested dictionary encoding the learned tree.\n        Leaf nodes are represented directly by the class label\n        (e.g., 'yes' or 0 or 'setosa').\n        Internal nodes have the form:\n            {attribute_name: {value1: subtree1, value2: subtree2, ...}}\n    \"\"\"\n    if not examples:\n        return None\n    first_label = examples[0][target_attr]\n    if all((ex[target_attr] == first_label for ex in examples)):\n        return first_label\n    if not attributes:\n        return majority_class(examples, target_attr)\n    gains = [(information_gain(examples, attr, target_attr), attr) for attr in attributes]\n    (_, best_attr) = max(gains, key=lambda t: t[0])\n    tree: Dict[str, Any] = {best_attr: {}}\n    values = {ex[best_attr] for ex in examples}\n    for v in values:\n        subset = [ex for ex in examples if ex[best_attr] == v]\n        remaining_attrs = [a for a in attributes if a != best_attr]\n        subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n        if subtree is None:\n            subtree = majority_class(examples, target_attr)\n        tree[best_attr][v] = subtree\n    return tree"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Deterministic Pegasos for kernel\u2010SVMs (binary {-1 , +1} labels).\n\n    Returns\n    -------\n    alpha : list\n        Dual coefficients (shape = n_samples) rounded to 4 decimals.\n    bias  : float\n        Intercept term rounded to 4 decimals.\n    \"\"\"\n\n    def linear_kernel(X):\n        return X @ X.T\n\n    def rbf_kernel(X):\n        sq = np.sum(X * X, 1)[:, None]\n        dist2 = sq + sq.T - 2 * (X @ X.T)\n        return np.exp(-dist2 / (2.0 * sigma ** 2))\n    X = data.astype(float)\n    y = labels.astype(float).ravel()\n    n = X.shape[0]\n    if kernel == 'linear':\n        K = linear_kernel(X)\n    elif kernel == 'rbf':\n        K = rbf_kernel(X)\n    else:\n        raise ValueError(\"kernel must be 'linear' or 'rbf'\")\n    beta = np.zeros(n)\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        beta *= 1.0 - eta * lambda_val\n        f = K @ beta\n        viol = y * f < 1\n        idx = np.where(viol)[0]\n        if idx.size:\n            beta[idx] += eta / idx.size * y[idx]\n    alpha = beta * y\n    C = 1.0 / (lambda_val * n)\n    f_final = K @ beta\n    sv_mask = (alpha > 1e-06) & (alpha < C - 1e-06)\n    if np.any(sv_mask):\n        bias = np.mean(y[sv_mask] - f_final[sv_mask])\n    else:\n        bias = np.mean(y - f_final)\n    alpha = np.round(alpha, 4).tolist()\n    bias = round(float(bias), 4)\n    return (alpha, bias)"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation for the input `z`.\n\n    Parameters\n    ----------\n    z : float\n        Input value.\n\n    Returns\n    -------\n    float\n        Sigmoid output rounded to four decimal places.\n    \"\"\"\n    s = 1.0 / (1.0 + math.exp(-z))\n    return round(s, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax activation values for a list of scores.\n\n    Parameters\n    ----------\n    scores : list[float]\n        Raw scores (logits).\n\n    Returns\n    -------\n    list[float]\n        Softmax probabilities, each rounded to four decimal places.\n    \"\"\"\n    if not scores:\n        return []\n    max_score = max(scores)\n    exp_shifted = [math.exp(s - max_score) for s in scores]\n    total = sum(exp_shifted)\n    softmax_vals = [round(val / total, 4) for val in exp_shifted]\n    return softmax_vals"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nfrom typing import List, Tuple\nimport numpy as np\ndef single_neuron_model(features: List[List[float]], labels: List[int], weights: List[float], bias: float) -> Tuple[List[float], float]:\n    \"\"\"\n    Simulates a single\u2010neuron binary classifier with a sigmoid activation.\n\n    Parameters\n    ----------\n    features : list[list[float]]\n        2-D list where each inner list contains the feature values of one sample.\n    labels   : list[int]\n        Ground-truth binary labels (0/1) for every sample.\n    weights  : list[float]\n        Weight assigned to every feature (must match feature dimension).\n    bias     : float\n        Scalar bias term for the neuron.\n\n    Returns\n    -------\n    Tuple[List[float], float]\n        \u2022 List with the predicted probabilities (rounded to 4 decimals).  \n        \u2022 Mean-squared error between predictions & labels (rounded to 4 decimals).\n    \"\"\"\n    X = np.asarray(features, dtype=float)\n    w = np.asarray(weights, dtype=float)\n    y_true = np.asarray(labels, dtype=float)\n    z = X.dot(w) + bias\n    y_pred = 1.0 / (1.0 + np.exp(-z))\n    mse = np.mean((y_pred - y_true) ** 2)\n    y_pred = np.round(y_pred, 4).tolist()\n    mse = float(np.round(mse, 4))\n    return (y_pred, mse)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Train a single sigmoid-activated neuron with mean\u2013squared-error loss.\n\n    Parameters\n    ----------\n    features        : 2-D array of shape (n_samples, n_features)\n    labels          : 1-D array of shape (n_samples,) containing 0/1 targets\n    initial_weights : 1-D array of shape (n_features,)\n    initial_bias    : scalar bias term\n    learning_rate   : gradient\u2013descent step size\n    epochs          : number of passes over the data\n\n    Returns\n    -------\n    weights : numpy.ndarray  (rounded to 4 decimals)\n    bias    : float          (rounded to 4 decimals)\n    losses  : list[float]    (epoch-wise MSE, each rounded to 4 decimals)\n    \"\"\"\n    X = np.asarray(features, dtype=float)\n    y = np.asarray(labels, dtype=float).reshape(-1)\n    w = np.asarray(initial_weights, dtype=float).reshape(-1).copy()\n    b = float(initial_bias)\n    n_samples = X.shape[0]\n\n    def _sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    losses = []\n    for _ in range(epochs):\n        z = X @ w + b\n        y_pred = _sigmoid(z)\n        error = y_pred - y\n        mse = np.mean(error ** 2)\n        losses.append(mse)\n        factor = 2.0 / n_samples * error * y_pred * (1 - y_pred)\n        grad_w = X.T @ factor\n        grad_b = np.sum(factor)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n    w_rounded = np.round(w, 4)\n    b_rounded = round(b, 4)\n    losses_list = np.round(losses, 4).tolist()\n    return (w_rounded, b_rounded, losses_list)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = float(data)\n        self.grad = 0.0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0.0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (0.0 if out.data == 0 else 1.0) * out.grad\n        out._backward = _backward\n        return out\n    __radd__ = __add__\n    __rmul__ = __mul__\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'\n\n    def backward(self):\n        (topo, visited) = ([], set())\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the change\u2013of\u2013basis matrix P that converts coordinates expressed in\n    basis B to coordinates expressed in basis C for R\u00b3.\n\n    Given any vector v:\n        [v]_C = P \u00b7 [v]_B\n\n    Parameters\n    ----------\n    B : list[list[int | float]]\n        Three vectors of the B\u2013basis, each given in the standard basis.\n    C : list[list[int | float]]\n        Three vectors of the C\u2013basis, each given in the standard basis.\n\n    Returns\n    -------\n    list[list[float]]\n        3\u00d73 transformation matrix rounded to 4 decimal places (row\u2013major list).\n    \"\"\"\n    B_mat = np.array(B, dtype=float).T\n    C_mat = np.array(C, dtype=float).T\n    P = np.linalg.inv(C_mat) @ B_mat\n    return np.round(P, 4).tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Singular Value Decomposition for a real 2\u00d72 matrix using nothing\n    but eigen\u2013analysis as described in\n    https://metamerist.blogspot.com/2006/10/linear-algebra-for-graphics-geeks-svd.html\n    \n    Parameters\n    ----------\n    A : np.ndarray\n        A real 2\u00d72 matrix.\n    \n    Returns\n    -------\n    (U, S, V) : tuple\n        U, S and V are python lists (obtained with .tolist()) holding the\n        matrices rounded to 4 decimals such that\n\n            A \u2248 U \u00b7 S \u00b7 V              (all 2\u00d72)\n\n        Note that V here is already the transpose of the usual right\u2013\n        singular\u2013vector matrix, so the simple triple product reproduces A.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Only 2\u00d72 matrices are supported.')\n    A = A.astype(float)\n    AtA = A.T @ A\n    (eig_vals, eig_vecs) = np.linalg.eigh(AtA)\n    idx = np.argsort(eig_vals)[::-1]\n    eig_vals = eig_vals[idx]\n    V = eig_vecs[:, idx]\n    if np.linalg.det(V) < 0:\n        V[:, -1] *= -1\n    sing = np.sqrt(np.clip(eig_vals, 0, None))\n    S = np.diag(sing)\n    eps = 1e-10\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if sing[i] > eps:\n            U[:, i] = A @ V[:, i] / sing[i]\n    if sing[0] < eps and sing[1] < eps:\n        U = np.eye(2)\n    elif sing[0] < eps or sing[1] < eps:\n        nz = 0 if sing[0] > eps else 1\n        u = U[:, nz]\n        U[:, 1 - nz] = np.array([-u[1], u[0]])\n        U[:, 1 - nz] /= np.linalg.norm(U[:, 1 - nz])\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    Vt = np.round(V.T, 4)\n    return (U.tolist(), S.tolist(), Vt.tolist())"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle two NumPy arrays in unison while keeping the correspondence\n    between samples in X and y intact.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature array of shape (n_samples, \u2026).\n    y : np.ndarray\n        Label/target array of shape (n_samples, \u2026).\n    seed : int, optional\n        Seed for reproducible shuffling.  Default is None (non-deterministic).\n\n    Returns\n    -------\n    tuple(list, list)\n        A tuple containing the shuffled X and y, both converted to native\n        Python lists via NumPy\u2019s `tolist()` method.\n    \"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y)\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples.')\n    rng = np.random.default_rng(seed)\n    perm = rng.permutation(X.shape[0])\n    X_shuffled = X[perm]\n    y_shuffled = y[perm]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generator that returns consecutive mini-batches from X (and y, if given).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix with shape (n_samples, \u2026).\n    y : np.ndarray or None, optional\n        Target array with length n_samples. If None, only X is batched.\n    batch_size : int, default=64\n        Number of samples per batch.\n\n    Yields\n    ------\n    list\n        \u2022 If y is None  : list representation of an X batch.  \n        \u2022 If y is given : tuple (X_batch_list, y_batch_list).\n    \"\"\"\n    X = np.asarray(X)\n    if batch_size <= 0:\n        raise ValueError('batch_size must be a positive integer.')\n    if y is not None:\n        y = np.asarray(y)\n        if X.shape[0] != y.shape[0]:\n            raise ValueError('X and y must contain the same number of samples.')\n    n_samples = X.shape[0]\n    for start in range(0, n_samples, batch_size):\n        end = start + batch_size\n        X_batch = X[start:end]\n        if y is None:\n            yield X_batch.tolist()\n        else:\n            y_batch = y[start:end]\n            yield (X_batch.tolist(), y_batch.tolist())"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Split a dataset into two subsets based on a feature-value threshold.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Input dataset.\n    feature_i : int\n        Index of the feature (column) on which to split.\n    threshold : float or int\n        Threshold value to compare against the chosen feature.\n\n    Returns\n    -------\n    tuple(list, list)\n        \u2022 First list  -> samples where X[:, feature_i] >= threshold  \n        \u2022 Second list -> samples where X[:, feature_i]  < threshold\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    if not 0 <= feature_i < X.shape[1]:\n        raise IndexError('feature_i is out of bounds for the number of features in X.')\n    mask = X[:, feature_i] >= threshold\n    left = X[mask]\n    right = X[~mask]\n    return (left.tolist(), right.tolist())"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a 2-D numpy array.\n\n    Parameters\n    ----------\n    X : array_like, shape (n_samples, n_features)\n        Input data.\n    degree : int\n        Maximum polynomial degree to generate (must be >= 0).\n\n    Returns\n    -------\n    list\n        Python list (of lists) containing the new feature matrix with all\n        polynomial combinations of the input features up to the given degree,\n        including the bias (constant) term.\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D array, got shape {}.'.format(X.shape))\n    if not isinstance(degree, int) or degree < 0:\n        raise ValueError('degree must be a non-negative integer, got {}.'.format(degree))\n    (n_samples, n_features) = X.shape\n    cols = []\n    for d in range(degree + 1):\n        for comb in combinations_with_replacement(range(n_features), d):\n            if not comb:\n                col = np.ones((n_samples, 1), dtype=X.dtype)\n            else:\n                col = np.prod(X[:, comb], axis=1, keepdims=True)\n            cols.append(col)\n    poly_X = np.hstack(cols)\n    return poly_X.tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets (bootstraps/permutations) of a dataset.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Feature matrix.\n    y : array-like, shape (n_samples,)\n        Target vector.\n    n_subsets : int\n        Number of random subsets to generate.\n    replacements : bool, default=True\n        If True, sample with replacement (bootstrap); otherwise, sample\n        without replacement (permutation).\n    seed : int, default=42\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    list\n        A list of length `n_subsets`. Each element is a tuple\n        (X_subset, y_subset) where both `X_subset` and `y_subset`\n        have been converted to pure Python lists via `.tolist()`.\n    \"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y)\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D array.')\n    if y.ndim != 1:\n        raise ValueError('y must be a 1-D array.')\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must contain the same number of samples.')\n    if n_subsets <= 0:\n        raise ValueError('n_subsets must be a positive integer.')\n    rng = np.random.default_rng(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = rng.integers(low=0, high=n_samples, size=n_samples)\n        else:\n            indices = rng.choice(n_samples, size=n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    One-hot encode a 1-D array of integer class labels.\n\n    Parameters\n    ----------\n    x : array-like (1-D)\n        Integer class labels to be encoded.\n    n_col : int, optional\n        Total number of columns (i.e., distinct classes).  \n        If None, it is inferred as max(x) + 1.\n\n    Returns\n    -------\n    list\n        One-hot encoded representation as a Python list of lists.\n    \"\"\"\n    x = np.asarray(x, dtype=int).ravel()\n    if n_col is None:\n        n_col = int(x.max()) + 1\n    one_hot = np.eye(n_col, dtype=int)[x]\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1-D numpy array into a diagonal matrix and return it as a\n    native Python list (via NumPy\u2019s `tolist()` method).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        A one-dimensional array whose elements will form the diagonal\n        of the resulting matrix.\n\n    Returns\n    -------\n    list\n        A 2-D list representing the diagonal matrix.\n    \"\"\"\n    x = np.asarray(x)\n    if x.ndim != 1:\n        raise ValueError('Input must be a 1D numpy array.')\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : array-like (1D)\n        Ground-truth labels.\n    y_pred : array-like (1D)\n        Predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy score (proportion of correct predictions).\n\n    Raises\n    ------\n    ValueError\n        If the input arrays have different lengths or are empty.\n    \"\"\"\n    y_true = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    if y_true.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.size != y_pred.size:\n        raise ValueError('y_true and y_pred must have the same length.')\n    correct = np.sum(y_true == y_pred)\n    accuracy = correct / y_true.size\n    return float(accuracy)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate a Pearson\u2010correlation matrix.\n    \n    Parameters\n    ----------\n    X : 2-D numpy array (n_samples \u00d7 n_features_x)\n    Y : 2-D numpy array (n_samples \u00d7 n_features_y), optional\n        If omitted, Y is taken to be X.\n    \n    Returns\n    -------\n    list\n        2-D Python list containing the correlation coefficients,\n        rounded to 4 decimal places.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D array.')\n    if Y is None:\n        Y = X\n    else:\n        Y = np.asarray(Y, dtype=float)\n        if Y.ndim != 2:\n            raise ValueError('Y must be a 2-D array.')\n        if Y.shape[0] != X.shape[0]:\n            raise ValueError('X and Y must have the same number of rows (samples).')\n    X_mean = X.mean(axis=0)\n    Y_mean = Y.mean(axis=0)\n    X_std = X.std(axis=0, ddof=1)\n    Y_std = Y.std(axis=0, ddof=1)\n    X_std[X_std == 0] = np.nan\n    Y_std[Y_std == 0] = np.nan\n    Xz = (X - X_mean) / X_std\n    Yz = (Y - Y_mean) / Y_std\n    n = X.shape[0]\n    corr = Xz.T @ Yz / (n - 1)\n    corr = np.round(corr, 4)\n    return corr.tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Train an AdaBoost ensemble of decision stumps.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Training data.\n    y : ndarray of shape (n_samples,)\n        Labels. Accepts {-1, 1} or {0, 1}. The latter will be converted.\n    n_clf : int\n        Number of weak classifiers (decision stumps) to learn.\n\n    Returns\n    -------\n    clfs : list\n        A list with one dictionary per learned stump.  Each dictionary holds\n        \u2022 feature      : index of the feature used\n        \u2022 threshold    : threshold value\n        \u2022 polarity     : 1  or -1 (`1` means: predict -1 for x < thr,\n                                    -1 means: predict -1 for x > thr)\n        \u2022 alpha        : classifier weight  \n        All numeric items are rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    if set(np.unique(y)) == {0, 1}:\n        y = y * 2 - 1\n    (n_samples, n_features) = X.shape\n    w = np.full(n_samples, 1.0 / n_samples)\n    clfs = []\n    eps = 1e-10\n    for _ in range(n_clf):\n        best = {'feature': None, 'threshold': None, 'polarity': 1, 'error': float('inf'), 'prediction': None}\n        for j in range(n_features):\n            feature_values = X[:, j]\n            thresholds = np.unique(feature_values)\n            for thr in thresholds:\n                for pol in (1, -1):\n                    pred = np.ones(n_samples)\n                    if pol == 1:\n                        pred[feature_values < thr] = -1\n                    else:\n                        pred[feature_values > thr] = -1\n                    err = np.sum(w[pred != y])\n                    if err < best['error']:\n                        best.update(feature=j, threshold=thr, polarity=pol, error=err, prediction=pred.copy())\n        err = max(best['error'], eps)\n        alpha = 0.5 * math.log((1 - err) / err)\n        w *= np.exp(-alpha * y * best['prediction'])\n        w /= w.sum()\n        clf = dict(feature=best['feature'], threshold=round(float(best['threshold']), 4), polarity=int(best['polarity']), alpha=round(float(alpha), 4))\n        clfs.append(clf)\n    return clfs"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a 1-D list/array of scores.\n\n    Parameters\n    ----------\n    scores : list or np.ndarray\n        Input scores.\n\n    Returns\n    -------\n    list\n        Log-softmax values rounded to 4 decimal places.\n    \"\"\"\n    x = np.asarray(scores, dtype=np.float64)\n    x_shifted = x - np.max(x)\n    log_softmax_vals = x_shifted - np.log(np.sum(np.exp(x_shifted)))\n    return np.round(log_softmax_vals, 4).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        \"\"\"\n        Receives the dimensionality of the input that will be fed\n        to the layer.  (Called by the network-builder before the\n        real training starts.)\n        \"\"\"\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n    \"\"\"\n    Fully-connected (a.k.a. \u201cdense\u201d) layer.\n    --------------------------------------------------------------------\n    1) W  : weight matrix                    \u2013 shape  (n_in , n_units)\n    2) w0 : bias vector                      \u2013 shape  (n_units, )\n    3) *Opt: (deep-copies of) the optimiser  \u2013 one for W, one for w0\n    --------------------------------------------------------------------\n    All public methods return python lists and are 4-decimal rounded,\n    as demanded in the exercise statement.\n    \"\"\"\n\n    def __init__(self, n_units, input_shape=None):\n        self.n_units = n_units\n        self.input_shape = input_shape\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n        self.layer_input = None\n\n    def initialize(self, optimizer):\n        \"\"\"\n        Called once the network knows the real input dimensionality and\n        once an optimizer object is available.\n        \"\"\"\n        if self.input_shape is None:\n            raise ValueError('Input shape is undefined. Call `set_input_shape` before `initialize`.')\n        limit = 1.0 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros(self.n_units)\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        if self.W is None or self.w0 is None:\n            return 0\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        \"\"\"\n        X : ndarray (batch_size, n_in)\n        returns list \u2013 4-decimal rounded\n        \"\"\"\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return np.round(output, 4).tolist()\n\n    def backward_pass(self, accum_grad):\n        \"\"\"\n        accum_grad : ndarray/list  (batch_size, n_units)\n                     gradient arriving from the next layer\n        returns     : list \u2013 gradient wrt the layer input (n_in)\n        \"\"\"\n        accum_grad = np.asarray(accum_grad)\n        grad_W = np.dot(self.layer_input.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0)\n        if self.trainable and self.W_opt is not None:\n            self.W = self.W_opt.update(self.W, grad_W)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        grad_input = np.dot(accum_grad, self.W.T)\n        return np.round(grad_input, 4).tolist()\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int=0, stride: int=1):\n    \"\"\"\n    A very small, framework\u2013free 2-D convolution layer (single input-,\n    single output-channel).\n\n    Parameters\n    ----------\n    input_matrix : np.ndarray\n        2-D array (H \u00d7 W) representing the input feature map.\n    kernel       : np.ndarray\n        2-D array (kH \u00d7 kW) holding the convolutional weights.\n        (No flip is performed \u2192 cross-correlation, as in most DL libs.)\n    padding      : int\n        How many zero rows / columns to add symmetrically around\n        the input.\n    stride       : int\n        Number of pixels to move the kernel each step.\n\n    Returns\n    -------\n    list\n        Convolved feature map rounded to 4 dp and converted to a\n        regular Python list.\n    \"\"\"\n    if input_matrix.ndim != 2 or kernel.ndim != 2:\n        raise ValueError('input_matrix and kernel must both be 2-D arrays.')\n    if padding < 0 or stride <= 0:\n        raise ValueError('padding must be >= 0 and stride must be > 0.')\n    padded = np.pad(input_matrix, pad_width=((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    (H_p, W_p) = padded.shape\n    (kH, kW) = kernel.shape\n    out_h = (H_p - kH) // stride + 1\n    out_w = (W_p - kW) // stride + 1\n    if out_h <= 0 or out_w <= 0:\n        raise ValueError('Kernel size / padding / stride combination produces invalid output dimension.')\n    out = np.empty((out_h, out_w), dtype=float)\n    for i in range(out_h):\n        for j in range(out_w):\n            hs = i * stride\n            ws = j * stride\n            window = padded[hs:hs + kH, ws:ws + kW]\n            out[i, j] = np.sum(window * kernel)\n    out = np.round(out, 4)\n    return out.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Rectified Linear Unit (ReLU) activation function.\n\n    Parameters\n    ----------\n    z : float\n        Input value.\n\n    Returns\n    -------\n    float\n        z if z > 0, else 0.\n    \"\"\"\n    return z if z > 0 else 0.0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D feature matrix of shape (n_samples, n_features).\n    w : np.ndarray\n        1-D coefficient vector of shape (n_features, ).\n    y_true : np.ndarray\n        1-D array with the true target values of shape (n_samples, ).\n    alpha : float\n        Regularisation strength (\u03bb).\n\n    Returns\n    -------\n    float\n        Ridge loss rounded to 4 decimal places.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_pred - y_true) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    total_loss = mse + reg_term\n    return float(np.round(total_loss, 4))"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Compute the Leaky ReLU activation for a single scalar input.\n\n    Parameters\n    ----------\n    z : float\n        The input value.\n    alpha : float, optional\n        Slope applied when z is negative (default is 0.01).\n\n    Returns\n    -------\n    float | int\n        The transformed value after applying the Leaky ReLU function.\n    \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel (dot product) between two input vectors.\n\n    Parameters\n    ----------\n    x1, x2 : array-like\n        Input vectors of the same length.\n\n    Returns\n    -------\n    float\n        The dot product \u27e8x1, x2\u27e9.\n    \"\"\"\n    v1 = np.asarray(x1).ravel()\n    v2 = np.asarray(x2).ravel()\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same length.')\n    return float(np.dot(v1, v2))"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Compute the precision for binary classification.\n    \n    Parameters\n    ----------\n    y_true : array-like\n        Ground\u2013truth (correct) binary labels (0 or 1).\n    y_pred : array-like\n        Predicted binary labels (0 or 1).\n    \n    Returns\n    -------\n    float\n        Precision = TP / (TP + FP).  \n        Returns 0.0 if there are no positive predictions (TP + FP == 0).\n    \"\"\"\n    y_true = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    if y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError('y_true and y_pred must have the same number of elements.')\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    denom = tp + fp\n    return tp / denom if denom != 0 else 0.0"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Gradient\u2013descent optimizer that supports\n        1. 'batch'       : Batch Gradient Descent\n        2. 'stochastic'  : Stochastic Gradient Descent\n        3. 'mini-batch'  : Mini-Batch Gradient Descent\n\n    Parameters\n    ----------\n    X : ndarray, shape (m, n)\n        Feature matrix.\n    y : ndarray, shape (m,)  or  (m, 1)\n        Target vector.\n    weights : ndarray, shape (n,)  or  (n, 1)\n        Initial weight vector.\n    learning_rate : float\n        Learning rate (\u03b1).\n    n_iterations : int\n        Number of epochs (full passes over the training data).\n    batch_size : int, default=1\n        Size of mini-batches when method='mini-batch'.\n        Ignored for 'batch'.  For 'stochastic', an internal\n        batch size of 1 is always used.\n    method : str, {'batch', 'stochastic', 'mini-batch'}, default='batch'\n        Variant of gradient descent to apply.\n\n    Returns\n    -------\n    list\n        Optimised weights rounded to 4 decimals, in native Python list\n        form (via ndarray.tolist()).\n    \"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y).reshape(-1)\n    weights = np.asarray(weights).reshape(-1)\n    (m, n) = X.shape\n\n    def _batch_grad(x_batch, y_batch, w):\n        \"\"\"\n        Computes gradient of MSE w.r.t. w for a batch.\n        \"\"\"\n        b_m = x_batch.shape[0]\n        preds = x_batch.dot(w)\n        errors = preds - y_batch\n        grad = 2.0 / b_m * x_batch.T.dot(errors)\n        return grad\n    for epoch in range(n_iterations):\n        if method == 'batch':\n            grad = _batch_grad(X, y, weights)\n            weights -= learning_rate * grad\n        elif method == 'stochastic':\n            indices = np.random.permutation(m)\n            for idx in indices:\n                x_i = X[idx:idx + 1]\n                y_i = y[idx]\n                grad = _batch_grad(x_i, y_i, weights)\n                weights -= learning_rate * grad\n        elif method == 'mini-batch':\n            indices = np.random.permutation(m)\n            for start in range(0, m, batch_size):\n                end = start + batch_size\n                batch_idx = indices[start:end]\n                x_b = X[batch_idx]\n                y_b = y[batch_idx]\n                grad = _batch_grad(x_b, y_b, weights)\n                weights -= learning_rate * grad\n        else:\n            raise ValueError(\"method must be 'batch', 'stochastic' or 'mini-batch'\")\n    weights = np.round(weights, 4)\n    return weights.reshape(-1).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix, *, tol=1e-12):\n    \"\"\"\n    Convert the given matrix to Reduced Row Echelon Form (RREF).\n\n    Parameters\n    ----------\n    matrix : array-like\n        Input matrix (list of lists, tuple of tuples, numpy.ndarray \u2026).\n    tol : float, optional\n        Numerical tolerance used to decide when a value is effectively zero.\n        Default is 1e-12.\n\n    Returns\n    -------\n    list\n        The RREF of `matrix`, returned as (nested) Python lists\n        via NumPy\u2019s ``tolist()`` method.\n    \"\"\"\n    A = np.array(matrix, dtype=float, copy=True)\n    (m, n) = A.shape\n    pivot_row = 0\n    for col in range(n):\n        if pivot_row >= m:\n            break\n        rows_below = np.abs(A[pivot_row:, col])\n        max_idx_rel = np.argmax(rows_below)\n        max_val = rows_below[max_idx_rel]\n        if max_val < tol:\n            continue\n        pivot_idx = pivot_row + max_idx_rel\n        if pivot_idx != pivot_row:\n            A[[pivot_row, pivot_idx]] = A[[pivot_idx, pivot_row]]\n        pivot_val = A[pivot_row, col]\n        A[pivot_row] = A[pivot_row] / pivot_val\n        for r in range(m):\n            if r != pivot_row:\n                factor = A[r, col]\n                if abs(factor) > tol:\n                    A[r] -= factor * A[pivot_row]\n        pivot_row += 1\n    A[np.abs(A) < tol] = 0.0\n    return A.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Adam optimizer implementation.\n\n    Parameters\n    ----------\n    f : callable\n        Objective function to minimize. Not directly used in the core\n        algorithm, but kept for completeness / potential monitoring.\n    grad : callable\n        Function that computes the gradient of `f`. It must accept a\n        single argument `x` and return the gradient array of the same\n        shape as `x`.\n    x0 : array-like\n        Initial parameter vector.\n    learning_rate : float, optional\n        Step size (\u03b1). Default is 1e-3.\n    beta1 : float, optional\n        Exponential decay rate for first-moment estimates (\u03b2\u2081). Default 0.9.\n    beta2 : float, optional\n        Exponential decay rate for second-moment estimates (\u03b2\u2082). Default 0.999.\n    epsilon : float, optional\n        Small constant for numerical stability. Default 1e-8.\n    num_iterations : int, optional\n        Number of update steps. Default 1000.\n\n    Returns\n    -------\n    list\n        Optimized parameters, flattened and rounded to 4 decimal places.\n    \"\"\"\n    x = np.asarray(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(1, num_iterations + 1):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1.0 - beta1) * g\n        v = beta2 * v + (1.0 - beta2) * g ** 2\n        m_hat = m / (1.0 - beta1 ** t)\n        v_hat = v / (1.0 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).reshape(-1).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Lasso (L1-regularised) Linear Regression trained with (batch) Gradient Descent.\n\n    Parameters\n    ----------\n    X : np.array, shape = (n_samples, n_features)\n        Design / feature matrix.\n    y : np.array, shape = (n_samples,) or (n_samples, 1)\n        Target vector.\n    alpha : float, default=0.1\n        Strength of the L1 penalty term.\n    learning_rate : float, default=0.01\n        Step size for the gradient descent updates.\n    max_iter : int, default=1000\n        Maximum number of iterations.\n    tol : float, default=1e-4\n        Threshold for early stopping based on absolute change in the\n        objective function between two consecutive iterations.\n\n    Returns\n    -------\n    (weights_list, bias)\n        weights_list : python list of shape (n_features,) rounded to 4 decimals.\n        bias         : float rounded to 4 decimals.\n    \"\"\"\n    y = y.reshape(-1)\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n\n    def objective(pred, w):\n        mse_term = 0.5 / n_samples * np.sum((y - pred) ** 2)\n        l1_term = alpha * np.sum(np.abs(w))\n        return mse_term + l1_term\n    prev_cost = float('inf')\n    for _ in range(max_iter):\n        y_pred = X @ w + b\n        error = y_pred - y\n        grad_w = 1.0 / n_samples * (X.T @ error) + alpha * np.sign(w)\n        grad_b = 1.0 / n_samples * np.sum(error)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n        curr_cost = objective(y_pred, w)\n        if abs(prev_cost - curr_cost) < tol:\n            break\n        prev_cost = curr_cost\n    w = np.round(w, 4)\n    b = round(b, 4)\n    return (w.tolist(), b)"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Optimal String Alignment (restricted Damerau-Levenshtein) distance\n    ---------------------------------------------------------------\n    Allowed unit-cost operations:\n        \u2022 insertion\n        \u2022 deletion\n        \u2022 substitution\n        \u2022 transposition of two adjacent characters\n    A character (or character pair) can be edited at most once \u2014 the\n    restriction that separates OSA from the full Damerau-Levenshtein metric.\n    \n    Parameters\n    ----------\n    source : str\n        String to be transformed.\n    target : str\n        Desired result string.\n    \n    Returns\n    -------\n    int\n        Minimum number of edits needed to turn `source` into `target`.\n    \"\"\"\n    if source == target:\n        return 0\n    if not source:\n        return len(target)\n    if not target:\n        return len(source)\n    (m, n) = (len(source), len(target))\n    D = np.zeros((m + 1, n + 1), dtype=int)\n    D[0, :] = np.arange(n + 1)\n    D[:, 0] = np.arange(m + 1)\n    for i in range(1, m + 1):\n        s_i = source[i - 1]\n        for j in range(1, n + 1):\n            t_j = target[j - 1]\n            cost = 0 if s_i == t_j else 1\n            deletion = D[i - 1, j] + 1\n            insertion = D[i, j - 1] + 1\n            substitution = D[i - 1, j - 1] + cost\n            D[i, j] = min(deletion, insertion, substitution)\n            if i > 1 and j > 1 and (s_i == target[j - 2]) and (source[i - 2] == t_j):\n                transposition = D[i - 2, j - 2] + 1\n                if transposition < D[i, j]:\n                    D[i, j] = transposition\n    return int(D[m, n])"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Compute the recall score for binary classification.\n\n    Parameters\n    ----------\n    y_true : list or array-like\n        Ground-truth binary labels (0 or 1).\n    y_pred : list or array-like\n        Predicted binary labels (0 or 1).\n\n    Returns\n    -------\n    float\n        Recall rounded to three decimal places. Returns 0.0 if there are\n        no actual positive samples (i.e., TP + FN == 0).\n    \"\"\"\n    y_true = np.asarray(y_true).astype(int)\n    y_pred = np.asarray(y_pred).astype(int)\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    rec = tp / denominator\n    return round(rec, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Computes scaled-dot-product self-attention.\n    \n    Parameters\n    ----------\n    Q, K, V : np.ndarray\n        \u2022 2-D shape : (seq_len , d_k)  \n        \u2022 3-D shape : (batch , seq_len , d_k)\n        The three inputs must share the same last-dimension size (d_k).\n    \n    Returns\n    -------\n    list\n        Attention output rounded to 4 decimal places and converted to Python list.\n    \"\"\"\n    Q = np.asarray(Q, dtype=np.float64)\n    K = np.asarray(K, dtype=np.float64)\n    V = np.asarray(V, dtype=np.float64)\n    d_k = K.shape[-1]\n    scale = np.sqrt(d_k)\n    if Q.ndim == 2:\n        scores = Q @ K.T / scale\n    else:\n        scores = np.matmul(Q, K.transpose(0, 2, 1)) / scale\n    max_score = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - max_score)\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    if V.ndim == 2:\n        output = attention_weights @ V\n    else:\n        output = np.matmul(attention_weights, V)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Runs a simple RNN forward pass over an input sequence and returns the\n    final hidden state (rounded to 4 decimal places).\n\n    Parameters\n    ----------\n    input_sequence : list of list of floats\n        A sequence of T input vectors, each of dimensionality `input_size`.\n    initial_hidden_state : list of floats\n        Initial hidden state vector of dimensionality `hidden_size`.\n    Wx : list of list of floats\n        Input-to-hidden weight matrix of shape (hidden_size, input_size).\n    Wh : list of list of floats\n        Hidden-to-hidden weight matrix of shape (hidden_size, hidden_size).\n    b : list of floats\n        Bias vector of shape (hidden_size,).\n\n    Returns\n    -------\n    list of floats\n        Final hidden state after processing the whole sequence, with every\n        element rounded to 4 decimal places.\n    \"\"\"\n    Wx = np.asarray(Wx, dtype=float)\n    Wh = np.asarray(Wh, dtype=float)\n    b = np.asarray(b, dtype=float)\n    h = np.asarray(initial_hidden_state, dtype=float)\n    for x_t in input_sequence:\n        x_t = np.asarray(x_t, dtype=float)\n        h = np.tanh(Wx @ x_t + Wh @ h + b)\n    return np.round(h, 4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Apply a 2-D translation to a collection of points.\n\n    Parameters\n    ----------\n    points : list[list[float]]\n        List of points, each defined as [x, y].\n    tx : float\n        Translation distance along the x-axis.\n    ty : float\n        Translation distance along the y-axis.\n\n    Returns\n    -------\n    list[list[float]]\n        Translated points, converted back to a pure Python list\n        via NumPy\u2019s ``tolist()`` method.\n    \"\"\"\n    pts = np.asarray(points, dtype=float)\n    if pts.ndim != 2 or pts.shape[1] != 2:\n        raise ValueError('`points` must be a list/array of shape (N, 2).')\n    ones = np.ones((pts.shape[0], 1), dtype=float)\n    homogeneous_pts = np.hstack((pts, ones))\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]], dtype=float)\n    translated_homogeneous = homogeneous_pts @ translation_matrix.T\n    translated_pts = translated_homogeneous[:, :2]\n    return translated_pts.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the KL divergence   D_KL( P || Q )  between two normal distributions\n    \n        P ~ N(mu_p, sigma_p^2)\n        Q ~ N(mu_q, sigma_q^2)\n    \n    Parameters\n    ----------\n    mu_p, mu_q : float or array-like\n        Means of the two distributions.\n    sigma_p, sigma_q : float or array-like\n        Standard deviations (not variances!) of the two distributions.\n        Must be strictly positive.\n    \n    Returns\n    -------\n    float\n        KL divergence.  If the inputs are arrays, the divergence is summed\n        over all corresponding dimensions (i.e. treating each dimension as\n        independent and additive in KL).\n    \"\"\"\n    mu_p = np.asarray(mu_p, dtype=float)\n    mu_q = np.asarray(mu_q, dtype=float)\n    sigma_p = np.asarray(sigma_p, dtype=float)\n    sigma_q = np.asarray(sigma_q, dtype=float)\n    if np.any(sigma_p <= 0) or np.any(sigma_q <= 0):\n        raise ValueError('Standard deviations must be positive.')\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n    kl_element = np.log(sigma_q / sigma_p) + (var_p + (mu_p - mu_q) ** 2) / (2 * var_q) - 0.5\n    return float(np.sum(kl_element))"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the linear system Ax = b using the Gauss-Seidel iterative method.\n\n    Parameters\n    ----------\n    A : array-like (m \u00d7 m)\n        Coefficient matrix (assumed square and non-singular).\n    b : array-like (m,)\n        Right-hand-side vector.\n    n : int\n        Number of Gauss-Seidel iterations to perform.\n    x_ini : array-like, optional\n        Initial guess for the solution.  If None, a zero vector is used.\n\n    Returns\n    -------\n    list\n        Approximated solution vector (rounded to 4 decimals) as a Python list.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).reshape(-1)\n    m = A.shape[0]\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.asarray(x_ini, dtype=float).reshape(-1)\n    for _ in range(n):\n        for i in range(m):\n            sigma_left = np.dot(A[i, :i], x[:i])\n            sigma_right = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - sigma_left - sigma_right) / A[i, i]\n    return np.round(x, 4).reshape(-1).tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian Elimination\n    with partial pivoting.\n    \n    Parameters\n    ----------\n    A :   (n, n) array_like\n          Coefficient matrix.\n    b :   (n,) array_like\n          Right\u2013hand-side vector.\n    \n    Returns\n    -------\n    list\n          Solution vector rounded to 4 decimals and converted\n          to a regular Python list.\n    \"\"\"\n    A = np.array(A, dtype=float).copy()\n    b = np.array(b, dtype=float).copy().flatten()\n    n = b.size\n    for k in range(n - 1):\n        pivot_row = k + np.argmax(np.abs(A[k:, k]))\n        if np.isclose(A[pivot_row, k], 0):\n            raise ValueError('Matrix is singular or nearly singular.')\n        if pivot_row != k:\n            A[[k, pivot_row]] = A[[pivot_row, k]]\n            b[[k, pivot_row]] = b[[pivot_row, k]]\n        for i in range(k + 1, n):\n            factor = A[i, k] / A[k, k]\n            A[i, k:] -= factor * A[k, k:]\n            b[i] -= factor * b[k]\n    if np.isclose(A[-1, -1], 0):\n        raise ValueError('Matrix is singular or nearly singular.')\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        s = np.dot(A[i, i + 1:], x[i + 1:])\n        x[i] = (b[i] - s) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        limit = np.sqrt(1.0 / (input_size + hidden_size))\n        self.Wf = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n        self.Wi = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n        self.Wc = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n        self.Wo = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Parameters\n        ----------\n        x : np.ndarray, shape (seq_len, input_size)\n            Sequence to process.  Each row is the input vector at that time-step.\n        initial_hidden_state : np.ndarray, shape (hidden_size,) or (hidden_size, 1)\n        initial_cell_state   : np.ndarray, shape (hidden_size,) or (hidden_size, 1)\n\n        Returns\n        -------\n        hidden_states : list\n            Rounded (4-dp) hidden state for every time-step, converted to python list.\n            Shape conceptually (seq_len, hidden_size)\n        final_hidden_state : list\n            Rounded (4-dp) hidden state after last step.\n        final_cell_state : list\n            Rounded (4-dp) cell state after last step.\n        \"\"\"\n        h_t = np.asarray(initial_hidden_state).reshape(self.hidden_size, 1).astype(float)\n        c_t = np.asarray(initial_cell_state).reshape(self.hidden_size, 1).astype(float)\n        seq = np.asarray(x, dtype=float)\n        seq_len = seq.shape[0]\n        hidden_seq = []\n        for t in range(seq_len):\n            x_t = seq[t].reshape(self.input_size, 1)\n            concat = np.vstack((h_t, x_t))\n            f_t = sigmoid(self.Wf @ concat + self.bf)\n            i_t = sigmoid(self.Wi @ concat + self.bi)\n            c_hat_t = np.tanh(self.Wc @ concat + self.bc)\n            o_t = sigmoid(self.Wo @ concat + self.bo)\n            c_t = f_t * c_t + i_t * c_hat_t\n            h_t = o_t * np.tanh(c_t)\n            hidden_seq.append(h_t.T)\n        hidden_seq = np.vstack(hidden_seq)\n        hidden_seq_rounded = np.round(hidden_seq, 4)\n        final_h_rounded = np.round(h_t.T, 4)\n        final_c_rounded = np.round(c_t.T, 4)\n        return (hidden_seq_rounded.tolist(), final_h_rounded.flatten().tolist(), final_c_rounded.flatten().tolist())"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nfrom math import log\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Computes the TF-IDF matrix (docs  \u00d7 query_terms).\n\n    Parameters\n    ----------\n    corpus : List[List[str]]\n        Each sub-list represents a document (tokenised list of words).\n    query :  List[str]\n        List of words for which TF-IDF scores are needed.\n\n    Returns\n    -------\n    List[List[float]]\n        A 2-D list where element [i][j] is the TF-IDF of query[j] in\n        document i.  Values are rounded to 4 decimal places.\n    \"\"\"\n    if len(query) == 0:\n        return []\n    if len(corpus) == 0:\n        return []\n    n_docs = len(corpus)\n    df = {term: 0 for term in query}\n    for doc in corpus:\n        words_in_doc = set(doc)\n        for term in query:\n            if term in words_in_doc:\n                df[term] += 1\n    idf = {}\n    for term in query:\n        idf_val = log((n_docs + 1) / (df[term] + 1)) + 1\n        idf[term] = idf_val\n    tf_idf_matrix = np.zeros((n_docs, len(query)), dtype=float)\n    for (i, doc) in enumerate(corpus):\n        doc_len = len(doc)\n        if doc_len == 0:\n            continue\n        counts = {}\n        for word in doc:\n            counts[word] = counts.get(word, 0) + 1\n        for (j, term) in enumerate(query):\n            tf = counts.get(term, 0) / doc_len\n            tf_idf_matrix[i, j] = tf * idf[term]\n    tf_idf_matrix = np.round(tf_idf_matrix, 4)\n    return tf_idf_matrix.tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels (0/1)\n    :param y_pred: Numpy array of predicted labels (0/1)\n    :param beta: The weight of recall relative to precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    if beta < 0:\n        raise ValueError('beta has to be non-negative')\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n    tp = np.sum(y_true & y_pred)\n    fp = np.sum(~y_true & y_pred)\n    fn = np.sum(y_true & ~y_pred)\n    precision = tp / (tp + fp) if tp + fp else 0.0\n    recall = tp / (tp + fn) if tp + fn else 0.0\n    if precision == 0.0 and recall == 0.0:\n        return 0.0\n    beta_sq = beta ** 2\n    f_beta = (1 + beta_sq) * precision * recall / (beta_sq * precision + recall)\n    return round(f_beta, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Build a tiny RNN.\n        -------------------------------------------------\n            input_size   : dimensionality of every item x_t\n            hidden_size  : dimensionality of hidden state  h_t\n            output_size  : dimensionality of every output  y_t\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.initialize_weights(input_size, hidden_size, output_size)\n\n    def initialize_weights(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Weight matrices are drawn from \ud835\udcdd(0,1) and scaled by 0.01.\n        Biases are initialised to 0.\n        \"\"\"\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def rnn_forward(self, input_sequence):\n        \"\"\"\n        input_sequence : ndarray  (seq_len, input_size)\n\n        Returns\n        -------\n        outputs       : list of ndarray (output_size, 1) \u2013  y_1 \u2026 y_T\n        last_inputs   : list of ndarray (input_size , 1) \u2013  x_1 \u2026 x_T\n        last_hiddens  : list of ndarray (hidden_size, 1) \u2013  h_1 \u2026 h_T\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        for x_t in input_sequence:\n            x_t = x_t.reshape(-1, 1)\n            h_t = np.tanh(self.W_xh @ x_t + self.W_hh @ h_prev + self.b_h)\n            y_t = self.W_hy @ h_t + self.b_y\n            outputs.append(y_t)\n            last_inputs.append(x_t)\n            last_hiddens.append(h_t)\n            h_prev = h_t\n        return (outputs, last_inputs, last_hiddens)\n\n    def forward(self, input_sequence):\n        (outputs, _, _) = self.rnn_forward(input_sequence)\n        return outputs\n\n    def rnn_backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate=0.01):\n        \"\"\"\n        Back-propagate the error through time and update parameters.\n\n        expected_output : ndarray (seq_len, output_size)\n        Returns\n        -------\n        total_loss : float   (aggregated \u00bd\u00b7MSE over the whole sequence)\n        \"\"\"\n        seq_len = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros((self.hidden_size, 1))\n        total_loss = 0.0\n        for t in reversed(range(seq_len)):\n            y_pred = outputs[t]\n            y_true = expected_output[t].reshape(-1, 1)\n            dy = y_pred - y_true\n            total_loss += 0.5 * np.sum(dy ** 2)\n            dW_hy += dy @ last_hiddens[t].T\n            db_y += dy\n            dh = self.W_hy.T @ dy + dh_next\n            dh_raw = (1 - last_hiddens[t] ** 2) * dh\n            dW_xh += dh_raw @ last_inputs[t].T\n            prev_h = np.zeros_like(last_hiddens[0]) if t == 0 else last_hiddens[t - 1]\n            dW_hh += dh_raw @ prev_h.T\n            db_h += dh_raw\n            dh_next = self.W_hh.T @ dh_raw\n        for dparam in (dW_xh, dW_hh, dW_hy, db_h, db_y):\n            np.clip(dparam, -1, 1, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return total_loss\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate=0.01):\n        return self.rnn_backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n\n    def train_on_sequence(self, input_sequence, expected_output, learning_rate=0.01):\n        \"\"\"\n        Runs a forward pass then BPTT update.  Returns the loss.\n        \"\"\"\n        (outputs, last_inputs, last_hiddens) = self.rnn_forward(input_sequence)\n        loss = self.rnn_backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n        return loss"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol: float=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix (m \u00d7 m)\n    :param b: Right-hand side vector (m,)\n    :param n: Maximum number of CG iterations allowed\n    :param x0: Optional initial guess (defaults to the zero vector)\n    :param tol: Euclidean-norm tolerance for \u2016r_k\u2016 = \u2016b \u2212 A x_k\u2016\n    :return: Solution vector x (rounded to 8 decimals and returned as a Python list)\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float)\n    (m, m2) = A.shape\n    if m != m2 or b.shape[0] != m:\n        raise ValueError('Incompatible matrix/vector dimensions.')\n    x = np.zeros_like(b) if x0 is None else np.asarray(x0, dtype=float).copy()\n    if x.shape != b.shape:\n        raise ValueError('Initial guess x0 has incorrect shape.')\n    r = b - A @ x\n    p = r.copy()\n    rs_old = float(r @ r)\n    if np.sqrt(rs_old) < tol:\n        return np.round(x, 8).tolist()\n    for _ in range(n):\n        Ap = A @ p\n        alpha = rs_old / float(p @ Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = float(r @ r)\n        if np.sqrt(rs_new) < tol:\n            break\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n    return np.round(x, 8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    labels = np.asarray(y)\n    (_, counts) = np.unique(labels, return_counts=True)\n    probs = counts / counts.sum()\n    gini = 1.0 - np.sum(probs ** 2)\n    return round(float(gini), 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    Parameters\n    ----------\n    dense_matrix : list[list[Number]]\n        A 2-D (row \u2011 major) Python list representing the dense matrix.\n\n    Returns\n    -------\n    tuple[list, list, list]\n        (values, column_indices, row_pointer) where\n            values          : non\u2013zero elements in row-major order\n            column_indices  : corresponding column indices\n            row_pointer     : cumulative counts of non-zeros per row.\n                              Always starts with 0 and has length rows + 1.\n    \"\"\"\n    if not dense_matrix:\n        return ([], [], [0])\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        for (col_idx, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_idx)\n        row_pointer.append(len(values))\n    return (values, column_indices, row_pointer)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: list/tuple of numbers \u2013 the vector to be projected\n    :param L: list/tuple of numbers \u2013 the line vector defining the direction\n    :return: list \u2013 projection of v onto L, each coordinate rounded to 3 dp\n    \"\"\"\n    if len(v) != len(L):\n        raise ValueError('Vectors v and L must have the same dimension.')\n    if not any(L):\n        raise ValueError('Line vector L must be non-zero.')\n    dot_vL = sum((vi * Li for (vi, Li) in zip(v, L)))\n    dot_LL = sum((Li * Li for Li in L))\n    scale = dot_vL / dot_LL\n    proj = [round(scale * Li, 3) for Li in L]\n    return proj"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    Parameters\n    ----------\n    dense_matrix : list[list]\n        Rectangular (m \u00d7 n) matrix stored as a list of rows.\n\n    Returns\n    -------\n    tuple\n        (values, row_indices, col_ptr) where\n            values       : list of all non-zero entries, column by column\n            row_indices  : list of row indices for each stored value\n            col_ptr      : list whose length is n_columns + 1;\n                           col_ptr[j] is the index in `values` where column j starts,\n                           and col_ptr[-1] == len(values)\n    \"\"\"\n    if not dense_matrix:\n        return ([], [], [0])\n    n_rows = len(dense_matrix)\n    n_cols = len(dense_matrix[0])\n    for (r, row) in enumerate(dense_matrix):\n        if len(row) != n_cols:\n            raise ValueError(f'Row {r} has length {len(row)}; expected {n_cols}')\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(n_cols):\n        for row in range(n_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Return a basis of the column-space (image) of the matrix A.\n\n    Parameters\n    ----------\n    A : array_like, shape (m, n)\n        The input matrix.\n\n    Returns\n    -------\n    list\n        A list\u2010of\u2010lists representing the columns of A that form a basis\n        of its column space (rounded to 8 decimals).\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    (m, n) = A.shape\n    B = A.copy()\n    pivot_cols = []\n    row = 0\n    tol = 1e-12\n    for col in range(n):\n        pivot = None\n        for r in range(row, m):\n            if abs(B[r, col]) > tol:\n                pivot = r\n                break\n        if pivot is None:\n            continue\n        if pivot != row:\n            B[[row, pivot]] = B[[pivot, row]]\n        pivot_cols.append(col)\n        for r in range(row + 1, m):\n            factor = B[r, col] / B[row, col]\n            if abs(factor) > tol:\n                B[r, col:] -= factor * B[row, col:]\n        row += 1\n        if row == m:\n            break\n    if pivot_cols:\n        basis = A[:, pivot_cols]\n        basis = np.round(basis, 8)\n        return basis.tolist()\n    else:\n        return []"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the coefficient of determination (R-squared).\n\n    Parameters\n    ----------\n    y_true : array-like\n        Actual/observed target values.\n    y_pred : array-like\n        Predicted target values from the regression model.\n\n    Returns\n    -------\n    float\n        R-squared value, rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    if ss_tot == 0:\n        return round(0.0, 3)\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Parameters\n    ----------\n    img : list[list[int or float]]\n        2-D list where each inner list represents a row of pixel values\n        ranging from 0 (black) to 255 (white).\n\n    Returns\n    -------\n    float\n        Average brightness rounded to two decimals, or\n        -1 for any invalid input:\n          \u2022 empty image / empty rows\n          \u2022 inconsistent row lengths\n          \u2022 pixel value outside 0-255 or non-numeric\n    \"\"\"\n    if not img or not isinstance(img, list):\n        return -1\n    row_len = len(img[0])\n    if row_len == 0:\n        return -1\n    total = 0.0\n    count = 0\n    for row in img:\n        if not isinstance(row, (list, tuple)) or len(row) != row_len:\n            return -1\n        for px in row:\n            try:\n                val = float(px)\n            except (TypeError, ValueError):\n                return -1\n            if val < 0 or val > 255:\n                return -1\n            total += val\n        count += row_len\n    average = total / count\n    return round(average, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Compute the Root Mean Square Error (RMSE) between two equal-shaped\n    numeric sequences.\n\n    Parameters\n    ----------\n    y_true : array-like\n        Actual/true target values.\n    y_pred : array-like\n        Predicted target values.\n\n    Returns\n    -------\n    float\n        RMSE rounded to three decimal places.\n\n    Raises\n    ------\n    ValueError\n        * The two inputs cannot be converted to numeric arrays.\n        * The arrays are empty.\n        * The arrays have mismatched shapes.\n        * The arrays contain NaN or Inf values.\n    \"\"\"\n    try:\n        y_true = np.asarray(y_true, dtype=float)\n        y_pred = np.asarray(y_pred, dtype=float)\n    except (TypeError, ValueError):\n        raise ValueError('Inputs must be array-like and contain numeric values.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if np.isnan(y_true).any() or np.isnan(y_pred).any():\n        raise ValueError('Input arrays must not contain NaN values.')\n    if np.isinf(y_true).any() or np.isinf(y_pred).any():\n        raise ValueError('Input arrays must not contain Inf values.')\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_val = np.sqrt(mse)\n    return round(float(rmse_val), 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Compute the Jaccard Index for two binary label arrays.\n\n    Parameters\n    ----------\n    y_true : array-like\n        Ground-truth binary labels (0/1).\n    y_pred : array-like\n        Predicted binary labels (0/1).\n\n    Returns\n    -------\n    float\n        Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(float(jaccard), 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Compute the Dice (S\u00f8rensen\u2013Dice) coefficient for two binary label arrays.\n\n    Parameters\n    ----------\n    y_true : array-like\n        Ground-truth binary labels (0/1 or False/True).\n    y_pred : array-like\n        Predicted binary labels (0/1 or False/True).\n\n    Returns\n    -------\n    float\n        Dice score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('`y_true` and `y_pred` must have the same shape.')\n    intersection = np.logical_and(y_true, y_pred).sum()\n    positives_true = y_true.sum()\n    positives_pred = y_pred.sum()\n    denominator = positives_true + positives_pred\n    if denominator == 0:\n        dice = 1.0\n    else:\n        dice = 2.0 * intersection / denominator\n    return round(float(dice), 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef _bipolar_hv(seed: int, dim: int) -> np.ndarray:\n    \"\"\"\n    Create a deterministic bipolar (+1 / \u20131) hypervector of dimensionality *dim*\n    using *seed*.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    return rng.randint(0, 2, dim, dtype=np.int8) * 2 - 1\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a dataset *row* using Hyper-dimensional\n    Computing.\n    \n    Parameters\n    ----------\n    row : dict\n        Keys are feature names, values are the corresponding data values.\n    dim : int\n        Dimensionality of the hypervectors.\n    random_seeds : dict\n        Mapping from feature name -> integer seed.  The same seed is used to\n        create both the feature-name hypervector and, in a deterministic way,\n        the value hypervector so that results are fully reproducible.\n    \n    Returns\n    -------\n    list\n        Bipolar composite hypervector representing the whole row.\n    \"\"\"\n    if not row:\n        return [0] * dim\n    bundle = np.zeros(dim, dtype=int)\n    for (feature, value) in row.items():\n        seed = random_seeds.get(feature, 0)\n        feat_hv = _bipolar_hv(seed, dim)\n        value_seed = (seed ^ hash(str(value)) & 4294967295) & 4294967295\n        value_hv = _bipolar_hv(value_seed, dim)\n        bound_hv = feat_hv * value_hv\n        bundle += bound_hv\n    composite = np.where(bundle >= 0, 1, -1)\n    return composite.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Build a 2\\u2006\u00d7\\u20062 confusion matrix for a binary-classification task.\n\n    Parameters\n    ----------\n    data : list[list]\n        Each inner list/tuple is [y_true, y_pred].\n\n    Returns\n    -------\n    list[list]\n        A 2\\u2006\u00d7\\u20062 matrix in the same layout used by\n        sklearn.metrics.confusion_matrix:\n\n                    predicted\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502  0   \u2502  1   \u2502\n        \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n        true   0  \u2502 TN   \u2502 FP   \u2502\n               1  \u2502 FN   \u2502 TP   \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        i.e. rows correspond to the actual class, columns to the prediction.\n    \"\"\"\n    labels = sorted({true for (true, _) in data} | {pred for (_, pred) in data})\n    if len(labels) != 2:\n        raise ValueError('Exactly two distinct class labels are required.')\n    idx = {label: i for (i, label) in enumerate(labels)}\n    matrix = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        i = idx[y_true]\n        j = idx[y_pred]\n        matrix[i][j] += 1\n    return matrix"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Parameters\n    ----------\n    v1, v2 : np.ndarray\n        Input vectors of identical shape.\n\n    Returns\n    -------\n    float\n        Cosine similarity rounded to three decimal places.\n\n    Raises\n    ------\n    ValueError\n        If the vectors have different shapes, are empty,\n        or have zero magnitude.\n    \"\"\"\n    v1 = np.asarray(v1, dtype=float)\n    v2 = np.asarray(v2, dtype=float)\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    norm1 = np.linalg.norm(v1)\n    norm2 = np.linalg.norm(v2)\n    if norm1 == 0 or norm2 == 0:\n        raise ValueError('Input vectors must have non-zero magnitude.')\n    similarity = np.dot(v1.ravel(), v2.ravel()) / (norm1 * norm2)\n    return round(float(similarity), 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[list[list[int]], float, float, float, float]:\n    \"\"\"\n    Calculate confusion-matrix, accuracy, F1-score, specificity and\n    negative-predictive-value for a binary-classification result.\n\n    Parameters\n    ----------\n    actual     : list[int]   \u2013 ground-truth labels (0/1)\n    predicted  : list[int]   \u2013 model predictions (0/1)\n\n    Returns\n    -------\n    (\n        [[TN, FP],\n         [FN, TP]],         # confusion matrix\n        accuracy,            # float (3 d.p.)\n        f1_score,            # float (3 d.p.)\n        specificity,         # float (3 d.p.)\n        negative_pred_value  # float (3 d.p.)\n    )\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('`actual` and `predicted` must have the same length.')\n    if not all((x in (0, 1) for x in actual + predicted)):\n        raise ValueError('Elements of `actual` and `predicted` must be 0 or 1.')\n    tn = fp = fn = tp = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            tp += 1\n        elif a == 1 and p == 0:\n            fn += 1\n        elif a == 0 and p == 1:\n            fp += 1\n        elif a == 0 and p == 0:\n            tn += 1\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    total = tn + fp + fn + tp\n    accuracy = (tp + tn) / total if total else 0\n    precision = tp / (tp + fp) if tp + fp else 0\n    recall = tp / (tp + fn) if tp + fn else 0\n    f1_score = 2 * precision * recall / (precision + recall) if precision + recall else 0\n    specificity = tn / (tn + fp) if tn + fp else 0\n    negative_predictive_value = tn / (tn + fn) if tn + fn else 0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate several descriptive\u2013statistics metrics for a 1-D numeric\n    iterable (list, tuple, NumPy array \u2026).\n\n    Parameters\n    ----------\n    data : 1-D array-like\n        Numeric data.\n\n    Returns\n    -------\n    dict\n        Dictionary whose keys are\n        'mean', 'median', 'mode', 'variance', 'standard_deviation',\n        '25th_percentile', '50th_percentile', '75th_percentile',\n        'interquartile_range'.\n\n        Every value is rounded to four decimal places **except** the\n        mode, which is returned exactly as it appears in the data\n        (or, when several modes exist, as a list of the un-rounded\n        modal values).\n    \"\"\"\n    arr = np.asarray(data, dtype=float)\n    if arr.size == 0:\n        raise ValueError('`data` must contain at least one value.')\n    arr = arr[~np.isnan(arr)]\n    if arr.size == 0:\n        raise ValueError('`data` contains only NaNs.')\n    mean_ = round(float(np.mean(arr)), 4)\n    median_ = round(float(np.median(arr)), 4)\n    counts = Counter(arr)\n    max_freq = max(counts.values())\n    modes = [k for (k, v) in counts.items() if v == max_freq]\n    mode_ = modes[0] if len(modes) == 1 else modes\n    var_ = round(float(np.var(arr, ddof=0)), 4)\n    std_ = round(float(np.std(arr, ddof=0)), 4)\n    p25 = round(float(np.percentile(arr, 25)), 4)\n    p50 = round(float(np.percentile(arr, 50)), 4)\n    p75 = round(float(np.percentile(arr, 75)), 4)\n    iqr_ = round(float(p75 - p25), 4)\n    return {'mean': mean_, 'median': median_, 'mode': mode_, 'variance': var_, 'standard_deviation': std_, '25th_percentile': p25, '50th_percentile': p50, '75th_percentile': p75, 'interquartile_range': iqr_}"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    \n    :param n: Total number of trials (non-negative integer)\n    :param k: Number of successes (non-negative integer, 0 \u2264 k \u2264 n)\n    :param p: Probability of success on each trial (0 \u2264 p \u2264 1)\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    if not 0 <= p <= 1 or n < 0 or k < 0 or (k > n):\n        return 0.0\n    try:\n        comb = math.comb(n, k)\n    except AttributeError:\n\n        def _comb(n_, k_):\n            k_ = min(k_, n_ - k_)\n            numerator = 1\n            denominator = 1\n            for i in range(1, k_ + 1):\n                numerator *= n_ - (k_ - i)\n                denominator *= i\n            return numerator // denominator\n        comb = _comb(n, k)\n    prob = comb * p ** k * (1 - p) ** (n - k)\n    return round(prob, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    :param x:        The value at which the PDF is evaluated.\n    :param mean:     The mean (\u03bc) of the distribution.\n    :param std_dev:  The standard deviation (\u03c3) of the distribution.\n    :return:         PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be positive.')\n    coeff = 1.0 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coeff * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n\n    P(X = k) = (lam ** k) * e^(\u2212lam) / k!\n\n    Parameters\n    ----------\n    k : int\n        Number of events (must be a non-negative integer).\n    lam : float or int\n        Average rate (mean) of occurrences in the given interval (must be \u2265 0).\n\n    Returns\n    -------\n    float\n        Probability rounded to 5 decimal places.\n\n    Raises\n    ------\n    ValueError\n        If k is negative or not an integer, or if lam is negative.\n    \"\"\"\n    if not (isinstance(k, int) and k >= 0):\n        raise ValueError('k must be a non-negative integer.')\n    if lam < 0:\n        raise ValueError('lam (\u03bb) must be non-negative.')\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n\n    Contrast (simple range-based definition) = max_pixel_value \u2212 min_pixel_value.\n\n    Args\n    ----\n    img : numpy.ndarray\n        2-D array (H \u00d7 W) representing a grayscale image whose pixel values are\n        expected to be in the range 0\u2013255 (integer or float).\n\n    Returns\n    -------\n    float\n        The contrast value. It is zero for a completely flat image and at most\n        255 for an 8-bit image.\n\n    Raises\n    ------\n    ValueError\n        If `img` is not a 2-D numpy array.\n    \"\"\"\n    img = np.asarray(img)\n    if img.ndim != 2:\n        raise ValueError(f'Expected a 2-D grayscale image, got array with shape {img.shape}')\n    img_min = float(np.min(img))\n    img_max = float(np.max(img))\n    contrast = img_max - img_min\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1-D array representing the first vector.\n        vec2 (numpy.ndarray): 1-D array representing the second vector.\n\n    Returns\n    -------\n        float or int:\n            The dot product of `vec1` and `vec2`.\n\n    Raises\n    ------\n        ValueError: If either input is not one-dimensional or if the vectors\n                    do not have the same length.\n        TypeError:  If the inputs cannot be interpreted as NumPy arrays.\n    \"\"\"\n    try:\n        vec1 = np.asarray(vec1)\n        vec2 = np.asarray(vec2)\n    except Exception as exc:\n        raise TypeError('Both inputs must be array-like objects that can be converted to NumPy arrays.') from exc\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1-D vectors.')\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError('Vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation (polynomial feature expansion).\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The maximum degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: Nested list where each inner list contains the polynomial\n                           features of the corresponding input, rounded to 8 decimals.\n                           If `degree` is less than 0 an empty list is returned.\n    \"\"\"\n    if degree < 0:\n        return []\n    if not data:\n        return []\n    vander = np.vander(np.asarray(data, dtype=float), N=degree + 1, increasing=True)\n    return np.round(vander, 8).tolist()"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Create positional encodings as described in \u201cAttention Is All You Need\u201d.\n    \n    Parameters\n    ----------\n    position : int\n        Maximum length of the input sequence (number of positions).\n    d_model : int\n        Dimensionality of the model / embedding size.\n    \n    Returns\n    -------\n    list  |  int\n        A `position \u00d7 d_model` list (dtype=float16) containing the positional \n        encodings, or -1 for invalid inputs.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_indices = np.arange(position)[:, np.newaxis]\n    dim_indices = np.arange(d_model)[np.newaxis, :]\n    angle_rates = 1 / np.power(10000, 2 * (dim_indices // 2) / d_model)\n    angle_rads = pos_indices * angle_rates\n    pe = np.empty_like(angle_rads, dtype=np.float32)\n    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    return pe.astype(np.float16).tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine whether a model is overfitting, underfitting, or a good fit.\n    \n    Parameters\n    ----------\n    training_accuracy : float\n        Training accuracy of the model (expected range: 0.0 \u2013 1.0).\n    test_accuracy : float\n        Test/validation accuracy of the model (expected range: 0.0 \u2013 1.0).\n\n    Returns\n    -------\n    int\n        1  -> Overfitting\n        -1 -> Underfitting\n        0  -> Good fit\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    \n    Adjusts the learning rate based on the moving averages of the\n    gradient and squared gradient, with bias-correction.\n    \n    Parameters\n    ----------\n    parameter : scalar or np.ndarray\n        Current parameter value \u03b8\u209c\n    grad : scalar or np.ndarray\n        Current gradient  \u2207\u03b8\u209c\n    m : scalar or np.ndarray\n        Exponential moving average of the gradient (1\u02e2\u1d57 moment)\n    v : scalar or np.ndarray\n        Exponential moving average of the squared gradient (2\u207f\u1d48 moment)\n    t : int\n        Current timestep (starts at 1)\n    learning_rate : float, optional\n        Step size \u03b1\n    beta1 : float, optional\n        Decay rate for the 1\u02e2\u1d57 moment  (default 0.9)\n    beta2 : float, optional\n        Decay rate for the 2\u207f\u1d48 moment  (default 0.999)\n    epsilon : float, optional\n        Numerical-stability term (default 1e-8)\n    \n    Returns\n    -------\n    tuple\n        (updated_parameter, updated_m, updated_v) \u2013 each rounded to\n        5 decimals and converted to list(s).\n    \"\"\"\n    parameter = np.asarray(parameter, dtype=np.float64)\n    grad = np.asarray(grad, dtype=np.float64)\n    m = np.asarray(m, dtype=np.float64)\n    v = np.asarray(v, dtype=np.float64)\n    m = beta1 * m + (1.0 - beta1) * grad\n    v = beta2 * v + (1.0 - beta2) * np.square(grad)\n    m_hat = m / (1.0 - beta1 ** t)\n    v_hat = v / (1.0 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    param_out = np.round(parameter, 5).tolist()\n    m_out = np.round(m, 5).tolist()\n    v_out = np.round(v, 5).tolist()\n    return (param_out, m_out, v_out)"}
{"task_id": 88, "completion_id": 0, "solution": "def layer_norm(x, g, b, eps: float=1e-05):\n    \"\"\"Applies layer normalisation over last dimension.\"\"\"\n    mu = x.mean(-1, keepdims=True)\n    var = ((x - mu) ** 2).mean(-1, keepdims=True)\n    x_hat = (x - mu) / np.sqrt(var + eps)\n    return g * x_hat + b\ndef split_heads(x, n_head):\n    \"\"\"\n    (seq, emb) -> (head, seq, head_dim)\n    \"\"\"\n    (seq_len, emb) = x.shape\n    head_dim = emb // n_head\n    x = x.reshape(seq_len, n_head, head_dim)\n    return np.transpose(x, (1, 0, 2))\ndef merge_heads(x):\n    \"\"\"\n    (head, seq, head_dim) -> (seq, emb)\n    \"\"\"\n    (n_head, seq_len, head_dim) = x.shape\n    return np.transpose(x, (1, 0, 2)).reshape(seq_len, n_head * head_dim)\ndef causal_attention(q, k, v):\n    \"\"\"\n    q, k, v : (head, seq, head_dim)\n    returns : (head, seq, head_dim)\n    \"\"\"\n    dk = q.shape[-1]\n    scores = np.matmul(q, k.transpose(0, 2, 1)) / np.sqrt(dk)\n    seq_len = scores.shape[-1]\n    mask = np.tril(np.ones((seq_len, seq_len), dtype=np.float32))\n    scores = scores * mask - 10000000000.0 * (1 - mask)\n    probs = np.exp(scores - scores.max(-1, keepdims=True))\n    probs = probs / probs.sum(-1, keepdims=True)\n    return np.matmul(probs, v)\ndef make_tiny_block(embed_dim, n_head, seed=0):\n    rng = np.random.RandomState(seed)\n    head_dim = embed_dim // n_head\n    return {'wq': rng.randn(embed_dim, embed_dim) * 0.02, 'wk': rng.randn(embed_dim, embed_dim) * 0.02, 'wv': rng.randn(embed_dim, embed_dim) * 0.02, 'wo': rng.randn(embed_dim, embed_dim) * 0.02, 'w1': rng.randn(embed_dim, 4 * embed_dim) * 0.02, 'w2': rng.randn(4 * embed_dim, embed_dim) * 0.02, 'ln1_g': np.ones(embed_dim), 'ln1_b': np.zeros(embed_dim), 'ln2_g': np.ones(embed_dim), 'ln2_b': np.zeros(embed_dim), 'n_head': n_head}\ndef transformer_block(x, block):\n    \"\"\"\n    x : (seq, emb)\n    \"\"\"\n    h = layer_norm(x, block['ln1_g'], block['ln1_b'])\n    q = h @ block['wq']\n    k = h @ block['wk']\n    v = h @ block['wv']\n    q = split_heads(q, block['n_head'])\n    k = split_heads(k, block['n_head'])\n    v = split_heads(v, block['n_head'])\n    attn_out = causal_attention(q, k, v)\n    attn_out = merge_heads(attn_out)\n    h = attn_out @ block['wo']\n    x = x + h\n    h2 = layer_norm(x, block['ln2_g'], block['ln2_b'])\n    h2 = h2 @ block['w1']\n    h2 = np.maximum(h2, 0)\n    h2 = h2 @ block['w2']\n    return x + h2\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    \"\"\"\n    Very small, didactic GPT-2-style generator.\n    \"\"\"\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    n_embd = hparams['n_embd']\n    if not params['blocks']:\n        params['blocks'].append(make_tiny_block(n_embd, n_head, seed=123))\n    tokens = encoder.encode(prompt)\n    if len(tokens) == 0:\n        raise ValueError('Prompt must contain at least one recognised token.')\n    for _ in range(n_tokens_to_generate):\n        if len(tokens) >= n_ctx:\n            break\n        seq_len = len(tokens)\n        idx = np.arange(seq_len)\n        wte = params['wte'][tokens]\n        wpe = params['wpe'][idx]\n        h = wte + wpe\n        h = transformer_block(h, params['blocks'][0])\n        h = layer_norm(h, params['ln_f']['g'], params['ln_f']['b'])\n        last_hidden = h[-1]\n        logits = last_hidden @ params['wte'].T\n        probs = np.exp(logits - logits.max())\n        probs = probs / probs.sum()\n        next_tok = int(np.random.choice(len(probs), p=probs))\n        tokens.append(next_tok)\n    return encoder.decode(tokens)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    A minimal self-attention style calculation on a 1-D sequence of crystal\n    values.\n\n    Parameters\n    ----------\n    n : int\n        Number of crystals (must match len(crystal_values)).\n    crystal_values : list/tuple/np.ndarray\n        Numeric value for each crystal.\n    dimension : int\n        Unused in this simplified setting, kept only to respect the signature.\n\n    Returns\n    -------\n    list[float]\n        The enhanced pattern (attention-weighted value) for every crystal,\n        each rounded to four decimal places.\n    \"\"\"\n\n    def softmax(x):\n        \"\"\"\n        Stable softmax applied along the last axis.\n        \"\"\"\n        x = np.asarray(x, dtype=np.float64)\n        x -= x.max()\n        exp_x = np.exp(x)\n        return exp_x / exp_x.sum()\n    values = np.asarray(crystal_values, dtype=np.float64)\n    if values.shape[0] != n:\n        raise ValueError('`n` must match the number of provided crystal values.')\n    patterns = []\n    for i in range(n):\n        scores = values[i] * values\n        attn_weights = softmax(scores)\n        enhanced_value = np.dot(attn_weights, values)\n        patterns.append(round(float(enhanced_value), 4))\n    return patterns"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    corpus : list[str] \u2013 collection of documents\n    query  : str       \u2013 query string\n    k1, b  : float     \u2013 BM25 parameters\n    returns: list[float] \u2013 BM25 score for each document (rounded to 3 decimals)\n    \"\"\"\n    docs_tokens = [doc.lower().split() for doc in corpus]\n    query_terms = query.lower().split()\n    N = len(corpus)\n    doc_lens = [len(toks) for toks in docs_tokens]\n    avgdl = np.mean(doc_lens) if doc_lens else 0.0\n    df = {}\n    for tokens in docs_tokens:\n        for term in set(tokens):\n            df[term] = df.get(term, 0) + 1\n    scores = []\n    for (tokens, dl) in zip(docs_tokens, doc_lens):\n        tf = Counter(tokens)\n        score = 0.0\n        for term in query_terms:\n            if term not in df:\n                continue\n            idf = np.log((N - df[term] + 0.5) / (df[term] + 0.5) + 1)\n            freq = tf.get(term, 0)\n            denom = freq + k1 * (1 - b + b * dl / avgdl) if avgdl else 1\n            score += idf * (freq * (k1 + 1)) / denom\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('`y_true` and `y_pred` must be of the same length.')\n    tp = fp = fn = 0\n    for (t, p) in zip(y_true, y_pred):\n        if p == 1 and t == 1:\n            tp += 1\n        elif p == 1 and t != 1:\n            fp += 1\n        elif p != 1 and t == 1:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp else 0.0\n    recall = tp / (tp + fn) if tp + fn else 0.0\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n    return round(f1, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    consumption_data : list/tuple with 10 daily consumption values\n    returns          : integer forecast for day-15, including 5 % safety margin\n    \"\"\"\n    detrended = []\n    for (idx, value) in enumerate(consumption_data):\n        day = idx + 1\n        fluct = 10 * math.sin(2 * PI * day / 10)\n        detrended.append(value - fluct)\n    n = len(detrended)\n    xs = list(range(1, n + 1))\n    sum_x = sum(xs)\n    sum_y = sum(detrended)\n    sum_x2 = sum((x * x for x in xs))\n    sum_xy = sum((x * y for (x, y) in zip(xs, detrended)))\n    denom = n * sum_x2 - sum_x ** 2\n    slope = (n * sum_xy - sum_x * sum_y) / denom\n    intercept = (sum_y - slope * sum_x) / n\n    day15_base = slope * 15 + intercept\n    day15_fluct = 10 * math.sin(2 * PI * 15 / 10)\n    day15_total = day15_base + day15_fluct\n    rounded = round(day15_total)\n    with_margin = math.ceil(rounded * 1.05)\n    return int(with_margin)"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    mae_value = np.mean(np.abs(y_true - y_pred))\n    return round(float(mae_value), 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef _softmax(x, axis=-1):\n    \"\"\"Numerically-stable softmax.\"\"\"\n    x = x - np.max(x, axis=axis, keepdims=True)\n    exp = np.exp(x)\n    return exp / np.sum(exp, axis=axis, keepdims=True)\ndef self_attention(Q, K, V):\n    \"\"\"\n    Q, K, V : (m, d)  \u2013 same length m, feature size d\n    Returns : (m, d)\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    weights = _softmax(scores, axis=-1)\n    return weights @ V\ndef _split_heads(x, n_heads):\n    \"\"\"(m, n) -> (m, n_heads, depth) where depth = n // n_heads\"\"\"\n    (m, n) = x.shape\n    assert n % n_heads == 0, 'n must be divisible by n_heads'\n    depth = n // n_heads\n    return x.reshape(m, n_heads, depth)\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Perform multi-head attention (no final linear projection for brevity).\n\n    Q, K, V : (m, n)\n    n_heads : int  \u2013 number of attention heads\n\n    Returns\n    -------\n    (m, n) \u2013 concatenated outputs from all heads\n    \"\"\"\n    Q_h = _split_heads(Q, n_heads)\n    K_h = _split_heads(K, n_heads)\n    V_h = _split_heads(V, n_heads)\n    head_outputs = [self_attention(Q_h[:, h, :], K_h[:, h, :], V_h[:, h, :]) for h in range(n_heads)]\n    return np.concatenate(head_outputs, axis=-1)"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n\n    Raises:\n    ValueError: If the two lists have different lengths, contain values\n                other than 0/1, or if the coefficient is undefined\n                because one of the variables has no variance.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length.')\n    n11 = n10 = n01 = n00 = 0\n    for (xi, yi) in zip(x, y):\n        if xi not in (0, 1) or yi not in (0, 1):\n            raise ValueError('Both input lists must contain only 0s and 1s.')\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        else:\n            n00 += 1\n    n1_ = n11 + n10\n    n0_ = n01 + n00\n    n_1 = n11 + n01\n    n_0 = n10 + n00\n    denom = (n1_ * n0_ * n_1 * n_0) ** 0.5\n    if denom == 0:\n        raise ValueError('Phi coefficient is undefined (zero variance in at least one variable).')\n    phi = (n11 * n00 - n10 * n01) / denom\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    if x >= 2.5:\n        return 1.0\n    return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU (Exponential Linear Unit) activation function.\n\n    ELU(x) = x                              if x >= 0\n             alpha * (exp(x) - 1)           if x < 0\n\n    The result is rounded to 4 decimal places.\n\n    Args:\n        x (float): Input value.\n        alpha (float): Scaling factor applied to negative inputs (default: 1.0).\n\n    Returns:\n        float: ELU activation value rounded to 4 decimal places.\n    \"\"\"\n    elu_val = x if x >= 0 else alpha * (math.exp(x) - 1)\n    return round(elu_val, 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the numerically-stable Softplus activation function.\n\n    Softplus(x) = log(1 + e**x)\n\n    Args:\n        x (float): Input value.\n\n    Returns:\n        float: Softplus value rounded to 4 decimal places.\n    \"\"\"\n    if math.isinf(x):\n        return float('inf') if x > 0 else 0.0\n    if math.isnan(x):\n        return float('nan')\n    if x > 0:\n        value = x + math.log1p(math.exp(-x))\n    else:\n        value = math.log1p(math.exp(x))\n    return round(value, 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    The Softsign function is defined as:\n        f(x) = x / (1 + |x|)\n    It smoothly maps any real-valued input to the interval (-1, 1).\n\n    Args:\n        x (float): Input value.\n\n    Returns:\n        float: Softsign of the input, rounded to 4 decimal places.\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List / 1-D array of likelihood ratios \u03c1_i = \u03c0_\u03b8(a_i|s_i) / \u03c0_\u03b8_old(a_i|s_i).\n        A:    List / 1-D array of advantage estimates \ud835\udc34_i.\n        pi_theta_old: List / 1-D array of old\u2013policy probabilities \u03c0_\u03b8_old(a_i|s_i).\n        pi_theta_ref: List / 1-D array of reference\u2013policy probabilities \u03c0_ref(a_i|s_i).\n        epsilon: PPO-style clipping parameter \u03b5.\n        beta:    Coefficient on the KL(\u03c0_\u03b8 || \u03c0_ref) penalty term.\n\n    Returns:\n        GRPO objective value (float, rounded to 6 decimals).\n    \"\"\"\n    rhos = np.asarray(rhos, dtype=np.float64)\n    A = np.asarray(A, dtype=np.float64)\n    pi_theta_old = np.asarray(pi_theta_old, dtype=np.float64)\n    pi_theta_ref = np.asarray(pi_theta_ref, dtype=np.float64)\n    pi_theta = rhos * pi_theta_old\n    clipped_rhos = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon)\n    surrogate = np.minimum(rhos * A, clipped_rhos * A)\n    L_clip = np.mean(surrogate)\n    tiny = 1e-10\n    p = np.clip(pi_theta, tiny, 1.0)\n    q = np.clip(pi_theta_ref, tiny, 1.0)\n    KL = np.mean(p * (np.log(p) - np.log(q)))\n    objective = L_clip - beta * KL\n    return float(np.round(objective, 6))"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Swish(x) = x * sigmoid(x)\n    with sigmoid(x) = 1 / (1 + e^(\u2212x))\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to 4 decimal places.\n    \"\"\"\n    sigmoid_x = 1.0 / (1.0 + math.exp(-x))\n    swish_val = x * sigmoid_x\n    return round(swish_val, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value (rounded to 4 decimal places)\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        selu_val = scale * x\n    else:\n        selu_val = scale * alpha * math.expm1(x)\n    return round(selu_val, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary-classification prediction using Logistic Regression.\n\n    Args:\n        X      : Input feature matrix of shape (N, D)\n        weights: Weight vector of shape (D,)  (or anything broadcastable to (D,))\n        bias   : Scalar bias term\n\n    Returns:\n        List with N binary predictions (0 or 1)\n    \"\"\"\n    weights = np.asarray(weights).reshape(-1)\n    logits = X @ weights + bias\n    logits_clipped = np.clip(logits, -709, 709)\n    probabilities = 1.0 / (1.0 + np.exp(-logits_clipped))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient\u2013descent training algorithm for Softmax regression, optimized\n    with Cross-Entropy loss.\n\n    Parameters\n    ----------\n    X : ndarray of shape (N, D)\n        Feature matrix.\n    y : ndarray of shape (N,)\n        Integer class labels ranging from 0 \u2026 (M-1).\n    learning_rate : float\n        Step size (\u03b1) for gradient descent.\n    iterations : int\n        Number of parameter-update steps.\n\n    Returns\n    -------\n    B : list[float]\n        Learned (D+1)\u00d7M parameter matrix flattened to 1-D, rounded to 4 dp.\n        The first row corresponds to the bias term.\n    losses : list[float]\n        Cross-entropy loss values collected at each iteration, rounded to 4 dp.\n    \"\"\"\n    (N, D) = X.shape\n    Xb = np.hstack([np.ones((N, 1)), X])\n    classes = np.unique(y)\n    M = classes.size\n    Y = np.zeros((N, M))\n    Y[np.arange(N), y.astype(int)] = 1.0\n    W = np.zeros((D + 1, M))\n    losses = []\n    for _ in range(iterations):\n        logits = Xb @ W\n        logits -= logits.max(axis=1, keepdims=True)\n        exp_logits = np.exp(logits)\n        probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n        ce_loss = -np.mean(np.log(probs[np.arange(N), y.astype(int)] + 1e-15))\n        losses.append(ce_loss)\n        grad = Xb.T @ (probs - Y) / N\n        W -= learning_rate * grad\n    W_rounded = np.round(W, 4).flatten().tolist()\n    losses_rounded = np.round(losses, 4).tolist()\n    return (W_rounded, losses_rounded)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient\u2013descent training algorithm for logistic regression using\n    Binary Cross-Entropy (BCE) loss.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary target vector of shape (n_samples,).\n    learning_rate : float\n        Step size for gradient descent.\n    iterations : int\n        Number of gradient\u2013descent iterations.\n\n    Returns\n    -------\n    tuple[list[float], list[float]]\n        \u2022 List of learned coefficients (weights + bias) rounded to 4 dp.  \n        \u2022 List of BCE losses collected at every iteration rounded to 4 dp.\n    \"\"\"\n    (m, n) = X.shape\n    X_bias = np.hstack([np.ones((m, 1)), X])\n    w = np.zeros(n + 1)\n    eps = 1e-15\n    losses = []\n    for _ in range(iterations):\n        z = X_bias @ w\n        p = 1 / (1 + np.exp(-z))\n        loss = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n        losses.append(loss)\n        grad = X_bias.T @ (p - y) / m\n        w -= learning_rate * grad\n    w_rounded = np.round(w, 4).tolist()\n    losses_rounded = np.round(losses, 4).tolist()\n    return (w_rounded, losses_rounded)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters\n    ----------\n    Q, K, V : np.ndarray\n        Query, Key, and Value tensors.  Shapes can be\n            \u2022 (seq_len, d_k)               \u2013 single sequence\n            \u2022 (batch, seq_len, d_k)        \u2013 batched sequences\n    mask : np.ndarray\n        Boolean or 0/1 mask indicating which positions can be attended to.\n        Shapes accepted:\n            \u2022 (seq_len, seq_len)\n            \u2022 (batch, seq_len, seq_len)\n        A value of 0 (False) means the position is **masked out**.\n    \n    Returns\n    -------\n    list\n        Attention output converted to a Python list.\n    \"\"\"\n    added_batch_dim = False\n    if Q.ndim == 2:\n        Q = Q[None, ...]\n        K = K[None, ...]\n        V = V[None, ...]\n        added_batch_dim = True\n    (batch_size, seq_len, d_k) = Q.shape\n    scores = np.matmul(Q, np.swapaxes(K, -2, -1)) / np.sqrt(d_k)\n    if mask.ndim == 2:\n        mask = mask[None, ...]\n    mask = mask.astype(bool)\n    scores = np.where(mask, scores, -1000000000.0)\n    scores_max = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - scores_max)\n    attn_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = np.matmul(attn_weights, V)\n    if added_batch_dim:\n        output = output[0]\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "from collections import Counter\nfrom math import log2\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n\n    The metric used is Shannon entropy (base-2) of the color distribution.\n    \u2022  Entropy is 0 when every apple has the same color.\n    \u2022  Entropy grows as the color distribution becomes more uniform across\n       a larger number of different colors.\n\n    The result is rounded to 4 decimal places, as required.\n    \"\"\"\n    if not apples:\n        return 0.0\n    n = len(apples)\n    counts = Counter(apples)\n    if len(counts) == 1:\n        return 0.0\n    entropy = 0.0\n    for c in counts.values():\n        p = c / n\n        entropy -= p * log2(p)\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Layer-normalizes a 3-D tensor (batch, seq_len, feature_dim).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input of shape (B, L, D)\n    gamma : np.ndarray\n        Scale parameter of shape (D,) or broadcast-compatible with the last dim.\n    beta : np.ndarray\n        Shift parameter of shape (D,) or broadcast-compatible with the last dim.\n    epsilon : float, optional\n        Small constant for numerical stability.\n\n    Returns\n    -------\n    list\n        Layer-normalized tensor (same shape as X) rounded to 5 decimals and\n        converted to a Python list.\n    \"\"\"\n    X = np.asarray(X, dtype=np.float32)\n    gamma = np.asarray(gamma, dtype=np.float32)\n    beta = np.asarray(beta, dtype=np.float32)\n    mean = X.mean(axis=-1, keepdims=True)\n    var = X.var(axis=-1, keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    out = X_hat * gamma + beta\n    return np.round(out, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import re\nimport numpy as np\nfrom collections import Counter\ndef _tokenize(text):\n    \"\"\"\n    Very small helper: lowercase, keep only alphanumerics, split on whitespace.\n    \"\"\"\n    return re.findall('\\\\w+', text.lower())\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute a simplified METEOR score between `reference` and `candidate`.\n\n    Parameters\n    ----------\n    reference : str\n        Gold/ground-truth sentence.\n    candidate : str\n        Machine-translation output.\n    alpha, beta, gamma : float\n        Standard METEOR parameters. Defaults reproduce the common setting:\n        alpha=0.9 (recall-weighted F-mean), beta=3, gamma=0.5.\n\n    Returns\n    -------\n    float\n        METEOR score rounded to 3 decimals.\n    \"\"\"\n    ref_tokens = _tokenize(reference)\n    cand_tokens = _tokenize(candidate)\n    if not ref_tokens or not cand_tokens:\n        return 0.0\n    ref_used = [False] * len(ref_tokens)\n    aligned_ref_positions = []\n    last_pos = -1\n    for tok in cand_tokens:\n        found = -1\n        for idx in range(last_pos + 1, len(ref_tokens)):\n            if not ref_used[idx] and ref_tokens[idx] == tok:\n                found = idx\n                break\n        if found == -1:\n            for idx in range(len(ref_tokens)):\n                if not ref_used[idx] and ref_tokens[idx] == tok:\n                    found = idx\n                    break\n        if found != -1:\n            ref_used[found] = True\n            aligned_ref_positions.append(found)\n            last_pos = found\n    m = len(aligned_ref_positions)\n    if m == 0:\n        return 0.0\n    precision = m / len(cand_tokens)\n    recall = m / len(ref_tokens)\n    f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall)\n    chunks = 1\n    for i in range(1, m):\n        if aligned_ref_positions[i] != aligned_ref_positions[i - 1] + 1:\n            chunks += 1\n    frag_ratio = chunks / m\n    penalty = gamma * frag_ratio ** beta\n    score = (1 - penalty) * f_mean\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) between two events.\n\n    Parameters\n    ----------\n    joint_counts : int or float\n        Number of times the two events occurred together.\n    total_counts_x : int or float\n        Number of times event X occurred.\n    total_counts_y : int or float\n        Number of times event Y occurred.\n    total_samples : int or float\n        Total number of observations / samples.\n\n    Returns\n    -------\n    float\n        PMI rounded to 3 decimal places. If any of the required counts is zero\n        (making PMI undefined), the function returns 0.0 by convention.\n    \"\"\"\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0:\n        return 0.0\n    ratio = joint_counts * total_samples / (total_counts_x * total_counts_y)\n    pmi_value = np.log2(ratio)\n    return round(float(pmi_value), 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Scale a list of integers to the range [0, 1] using Min-Max Normalization.\n\n    Parameters\n    ----------\n    x : list[int]\n        The input list of integers.\n\n    Returns\n    -------\n    list[float]\n        A list of floats, each rounded to 4 decimal places, representing\n        the normalized values.\n    \"\"\"\n    if not x:\n        return []\n    (mn, mx) = (min(x), max(x))\n    if mn == mx:\n        return [0.0 for _ in x]\n    rng = mx - mn\n    return [round((xi - mn) / rng, 4) for xi in x]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Simple residual (ResNet-style) block.\n    \n    Parameters\n    ----------\n    x  : np.ndarray (1-D)\n        Input vector.\n    w1 : np.ndarray (2-D)\n        First weight matrix. Its shape must be (x.size, hidden_units).\n    w2 : np.ndarray (2-D)\n        Second weight matrix. Its shape must be (hidden_units, x.size).\n    \n    Returns\n    -------\n    list\n        Output vector after the residual block, rounded to 4 decimal places.\n    \"\"\"\n    relu = lambda z: np.maximum(0, z)\n    h1 = relu(x @ w1)\n    h2 = relu(h1 @ w2)\n    out = relu(h2 + x)\n    return np.round(out, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3-D NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input array of shape (height, width, channels).\n\n    Returns\n    -------\n    np.ndarray\n        1-D array of shape (channels,) where each element is the mean\n        of all spatial locations (height \u00d7 width) in the corresponding channel.\n\n    Raises\n    ------\n    ValueError\n        If `x` is not a 3-D array.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError(f'Expected a 3-D tensor of shape (H, W, C); got array with shape {x.shape}')\n    return x.mean(axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Batch\u2013Normalization for BCHW input.\n\n    Parameters\n    ----------\n    X       : ndarray, shape (B, C, H, W)\n              Mini\u2013batch of feature-maps.\n    gamma   : ndarray, shape (C,) or broadcastable to (1,C,1,1)\n              Per\u2013channel scale factors.\n    beta    : ndarray, shape (C,) or broadcastable to (1,C,1,1)\n              Per\u2013channel shift factors.\n    epsilon : float\n              Numerical stability term.\n\n    Returns\n    -------\n    list\n        Normalized, scaled and shifted tensor rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=np.float64)\n    gamma = np.asarray(gamma, dtype=np.float64)\n    beta = np.asarray(beta, dtype=np.float64)\n    if gamma.ndim == 1:\n        gamma = gamma.reshape(1, -1, 1, 1)\n    elif gamma.ndim == 3:\n        gamma = gamma.reshape(1, *gamma.shape)\n    if beta.ndim == 1:\n        beta = beta.reshape(1, -1, 1, 1)\n    elif beta.ndim == 3:\n        beta = beta.reshape(1, *beta.shape)\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)\n    var = X.var(axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    Y = gamma * X_hat + beta\n    return np.round(Y, 4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the value of the derivative of a single-term polynomial c * x^n\n    at a given point x.\n\n    Parameters\n    ----------\n    c : float\n        Coefficient of the polynomial term.\n    x : float\n        Point at which the derivative is evaluated.\n    n : float\n        Exponent (power) of x in the term.\n\n    Returns\n    -------\n    float\n        The derivative value, rounded to 4 decimal places.\n    \"\"\"\n    if n == 0:\n        return 0.0\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the span of the given 2-D vectors\n    using the Gram\u2013Schmidt process.\n\n    Parameters\n    ----------\n    vectors : list[list[float]]\n        A list of 2-D column-vectors (length-2 lists or tuples).\n    tol : float, optional\n        Tolerance below which a vector is treated as the zero vector\n        (i.e. not linearly independent from the current basis).\n\n    Returns\n    -------\n    list[list[float]]\n        A list of orthonormal basis vectors rounded to 4 decimals.\n    \"\"\"\n    if not vectors:\n        return []\n    for v in vectors:\n        if len(v) != 2:\n            raise ValueError('All input vectors must be 2-dimensional.')\n    basis = []\n    for v in vectors:\n        v = np.asarray(v, dtype=float)\n        for b in basis:\n            v = v - np.dot(v, b) * b\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            basis.append(v / norm)\n        if len(basis) == 2:\n            break\n    return [np.round(b, 4).tolist() for b in basis]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n\n    Parameters\n    ----------\n    a, b : array-like of length 3\n        Input vectors.\n\n    Returns\n    -------\n    list\n        Cross product rounded to 4 decimal places.\n    \"\"\"\n    a = np.asarray(a, dtype=float)\n    b = np.asarray(b, dtype=float)\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError('Both input vectors must contain exactly three elements.')\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations Ax = b with Cramer's Rule.\n    \n    Parameters\n    ----------\n    A : (n, n) array_like\n        Coefficient matrix.\n    b : (n,)  array_like\n        Constant terms.\n    \n    Returns\n    -------\n    list\n        Solution vector rounded to 4 decimal places,\n        or -1 if the system has no unique solution.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).flatten()\n    if A.ndim != 2 or A.shape[0] != A.shape[1] or b.size != A.shape[0]:\n        raise ValueError('A must be square and compatible with b.')\n    n = A.shape[0]\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0.0):\n        return -1\n    x = np.empty(n)\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x[i] = det_Ai / det_A\n    x_rounded = np.round(x, 4)\n    x_rounded[np.isclose(x_rounded, 0)] = 0.0\n    return x_rounded.tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Compute the Bhattacharyya distance between two discrete probability\n    distributions.\n\n    Parameters\n    ----------\n    p, q : list[float]\n        Lists of non-negative numbers representing (possibly un-normalised)\n        probability masses.\n\n    Returns\n    -------\n    float\n        Bhattacharyya distance rounded to 4 decimal places.\n        If the lists are empty, of different length, contain negatives,\n        or have zero total mass, 0.0 is returned.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p_arr = np.asarray(p, dtype=float)\n    q_arr = np.asarray(q, dtype=float)\n    if np.any(p_arr < 0) or np.any(q_arr < 0):\n        return 0.0\n    (p_sum, q_sum) = (p_arr.sum(), q_arr.sum())\n    if p_sum == 0 or q_sum == 0:\n        return 0.0\n    p_arr /= p_sum\n    q_arr /= q_sum\n    bc = np.sum(np.sqrt(p_arr * q_arr))\n    if bc == 0:\n        return float('inf')\n    distance = -np.log(bc)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "from typing import List, Union\nNumber = Union[int, float]\ndef vector_sum(a: List[Number], b: List[Number]) -> Union[List[Number], int]:\n    \"\"\"\n    Compute the element-wise sum of two vectors (lists).\n\n    Parameters\n    ----------\n    a : list[int | float]\n        First vector.\n    b : list[int | float]\n        Second vector.\n\n    Returns\n    -------\n    list[int | float]\n        Element-wise sum of `a` and `b` if they have the same length.\n    int\n        -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if not isinstance(a, list) or not isinstance(b, list) or len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Compute the REINFORCE policy-gradient estimate.\n\n    Parameters\n    ----------\n    theta     : (num_states, num_actions) 2-D numpy.array \u2013 the policy parameters.\n    episodes  : list of episodes. Each episode is a list of (state, action, reward).\n\n    Returns\n    -------\n    A (num_states, num_actions) python list containing the averaged policy-gradient,\n    rounded to 4 decimal places.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    policy = np.empty_like(theta, dtype=float)\n    for s in range(num_states):\n        logits = theta[s] - np.max(theta[s])\n        exp_logits = np.exp(logits)\n        policy[s] = exp_logits / np.sum(exp_logits)\n    grad = np.zeros_like(theta, dtype=float)\n    for ep in episodes:\n        T = len(ep)\n        returns = np.zeros(T, dtype=float)\n        G = 0.0\n        for t in reversed(range(T)):\n            (_, _, r) = ep[t]\n            G += r\n            returns[t] = G\n        for (t, (s, a, _)) in enumerate(ep):\n            G_t = returns[t]\n            grad[s] -= G_t * policy[s]\n            grad[s, a] += G_t\n    grad /= max(len(episodes), 1)\n    grad = np.round(grad, 4)\n    return grad.tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Compare the per-token (or per sample) FLOPs of\n      \u2022 a standard dense feed-forward layer (d_in \u2192 d_out) and\n      \u2022 a sparsely\u2013gated Mixture-of-Experts (MoE) layer that\n        \u2013 uses a softmax gate (linear projection d_in \u2192 n_experts)\n        \u2013 activates k_active (top-k) experts, each an identical\n          dense projection d_in \u2192 d_out\n        \u2013 linearly combines the k_active outputs.\n\n    Parameters\n    ----------\n    n_experts : int   \u2013 total number of experts in the MoE layer\n    k_active  : int   \u2013 number of experts actually executed (k in top-k)\n    d_in      : int   \u2013 input dimension\n    d_out     : int   \u2013 output dimension\n\n    Returns\n    -------\n    dict with keys\n      dense_flops     : float \u2013 FLOPs of the plain dense layer\n      moe_flops       : float \u2013 FLOPs of the MoE layer\n      savings_percent : float \u2013 100 * (1 \u2013 moe/dense)\n    All numbers are rounded to 1 decimal place.\n    \"\"\"\n    dense_flops = 2.0 * d_in * d_out\n    gate_flops = 2.0 * d_in * n_experts\n    experts_flops = k_active * 2.0 * d_in * d_out\n    combine_flops = k_active * d_out\n    moe_flops = gate_flops + experts_flops + combine_flops\n    savings_percent = 100.0 * (1.0 - moe_flops / dense_flops)\n    dense_flops = round(dense_flops, 1)\n    moe_flops = round(moe_flops, 1)\n    savings_percent = round(savings_percent, 1)\n    return {'dense_flops': dense_flops, 'moe_flops': moe_flops, 'savings_percent': savings_percent}"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the \u2018Noisy Top-K\u2019 gating from Shazeer et al., 2017.\n\n    Steps\n    -----\n    1. clean_logits  = X @ W_g\n    2. noise_std     = softplus(X @ W_noise)\n    3. noisy_logits  = clean_logits + noise_std * N\n       (only used for selecting the k experts)\n    4. For every sample pick the k indices with the largest noisy_logits.\n    5. On those k indices compute a softmax of the *clean* logits.\n       All other positions are 0.\n    6. Round the result to 4 decimal places and return a Python list.\n    \"\"\"\n    (batch, n_experts) = (X.shape[0], W_g.shape[1])\n    k = max(1, min(k, n_experts))\n    clean_logits = X @ W_g\n    noise_std = np.log1p(np.exp(X @ W_noise))\n    noisy_logits = clean_logits + noise_std * N\n    gating = np.zeros_like(clean_logits)\n    for b in range(batch):\n        topk_idx = np.argpartition(noisy_logits[b], -k)[-k:]\n        sel_logits = clean_logits[b, topk_idx]\n        sel_logits -= sel_logits.max()\n        probs = np.exp(sel_logits)\n        probs /= probs.sum()\n        gating[b, topk_idx] = probs\n    gating = np.round(gating, 4)\n    return gating.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Sparse Mixture-of-Experts layer with softmax gating and top-k routing.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input tensor of shape (batch_size, input_dim).\n    We : np.ndarray\n        Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg : np.ndarray\n        Gating weight matrix of shape (input_dim, n_experts).\n    n_experts : int\n        Number of experts (must match We.shape[0]).\n    top_k : int\n        Number of experts to route each token to (top-k).\n\n    Returns\n    -------\n    list\n        MoE output (rounded to 4 decimals) converted to a regular Python list.\n    \"\"\"\n    assert We.shape[0] == n_experts, 'We and n_experts mismatch'\n    assert Wg.shape[1] == n_experts, 'Wg and n_experts mismatch'\n    assert 1 <= top_k <= n_experts, 'top_k must be in [1, n_experts]'\n    (batch_size, input_dim) = x.shape\n    (_, _, output_dim) = We.shape\n    logits = x @ Wg\n    logits_max = np.max(logits, axis=1, keepdims=True)\n    exp_logits = np.exp(logits - logits_max)\n    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    top_idx = np.argsort(probs, axis=1)[:, -top_k:]\n    out = np.zeros((batch_size, output_dim))\n    for b in range(batch_size):\n        idxs = top_idx[b]\n        sel_probs = probs[b, idxs]\n        norm_sel_probs = sel_probs / np.sum(sel_probs)\n        for (coef, e_idx) in zip(norm_sel_probs, idxs):\n            expert_out = x[b] @ We[e_idx]\n            out[b] += coef * expert_out\n    return np.round(out, 4).tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Group Normalization for a 4-D tensor  (B, C, H, W).\n\n    Parameters\n    ----------\n    X          : input tensor with shape (B, C, H, W)\n    gamma, beta: scale and shift (either shape (C,)  or  (1,C,1,1)  or  (C,1,1))\n    num_groups : number of channel groups (must exactly divide C)\n    epsilon    : small constant for numerical stability\n\n    Returns\n    -------\n    Python list of the normalized, scaled and shifted tensor\n    with all elements rounded to 4 decimal places.\n    \"\"\"\n    if X.ndim != 4:\n        raise ValueError('X must be a 4-D tensor of shape (B,C,H,W)')\n    (B, C, H, W) = X.shape\n    if C % num_groups != 0:\n        raise ValueError('num_groups ({}) must divide the number of channels ({})'.format(num_groups, C))\n    G = num_groups\n    X_grouped = X.reshape(B, G, C // G, H, W)\n    mean = X_grouped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_grouped.var(axis=(2, 3, 4), keepdims=True)\n    X_norm = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    if gamma.ndim == 1:\n        gamma_b = gamma.reshape(1, C, 1, 1)\n    else:\n        gamma_b = gamma\n    if beta.ndim == 1:\n        beta_b = beta.reshape(1, C, 1, 1)\n    else:\n        beta_b = beta\n    out = X_norm * gamma_b + beta_b\n    out = np.round(out, 4)\n    return out.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef _grad(x: float) -> float:\n    return 4 * x ** 3 - 9 * x ** 2\ndef _hess(x: float) -> float:\n    return 12 * x ** 2 - 18 * x\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Gradient\u2013descent search for the minimum of\n        f(x) = x^4 - 3x^3 + 2\n\n    Parameters\n    ----------\n    start_x       : initial guess\n    learning_rate : step size for gradient descent\n    tolerance     : |x_{k+1} - x_k| threshold for convergence\n    max_iters     : safety cap on iterations\n\n    Returns\n    -------\n    float : the x-coordinate of the minimum, rounded to 4 dp\n    \"\"\"\n    x = float(start_x)\n    for _ in range(max_iters):\n        g = _grad(x)\n        if abs(g) < tolerance:\n            if _hess(x) > 0:\n                break\n            x += np.random.uniform(-1.0, 1.0)\n            continue\n        x_new = x - learning_rate * g\n        if abs(x_new - x) < tolerance:\n            x = x_new\n            if _hess(x) > 0:\n                break\n            else:\n                x += np.random.uniform(-1.0, 1.0)\n                continue\n        x = x_new\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) \u2013 a learnable,   normalization\u2013free substitute for LayerNorm.\n    \n    Parameters\n    ----------\n    x     : np.ndarray\n            Arbitrary-shaped input tensor.  \n            The last dimension is treated as the \u201cfeature\u201d dimension that would\n            normally be normalized in a Transformer.\n    alpha : float\n            Exponent that modulates how strongly the dynamic rescaling reacts\n            to the statistics of the current token.\n    gamma : float\n            Output scale (analogous to the gain in LayerNorm).\n    beta  : float\n            Pre-tanh scale (controls the slope of the linear region).\n    \n    Returns\n    -------\n    list[float]\n            DyT-transformed tensor, rounded to 4 d.p., returned as a Python list.\n    \"\"\"\n    eps = 1e-07\n    mean_abs_token = np.mean(np.abs(x), axis=-1, keepdims=True) + eps\n    y = gamma * np.tanh(beta * x / np.power(mean_abs_token, alpha))\n    y = np.round(y, 4)\n    return y.tolist()"}
