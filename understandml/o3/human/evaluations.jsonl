{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Compute the dot product of a matrix `a` with a vector `b`.\n    \n    Parameters\n    ----------\n    a : list[list[int | float]]\n        An n \u00d7 m matrix represented as a list of n rows, \n        each row being a list of length m.\n    b : list[int | float]\n        A vector (list) of length m.\n    \n    Returns\n    -------\n    list[int | float]\n        A list of length n, where each entry is the dot product of\n        the corresponding row of `a` with `b`.\n    int\n        Returns -1 if dimensions are incompatible.\n    \"\"\"\n    if not isinstance(a, list) or not isinstance(b, list):\n        return -1\n    if len(a) == 0:\n        return [] if len(b) == 0 else -1\n    num_cols = len(a[0])\n    if num_cols != len(b):\n        return -1\n    for row in a:\n        if len(row) != num_cols:\n            return -1\n    result: list[int | float] = []\n    for row in a:\n        dot_val = sum((row[i] * b[i] for i in range(num_cols)))\n        result.append(dot_val)\n    return result\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List, Union\nNumber = Union[int, float]\ndef transpose_matrix(a: List[List[Number]]) -> List[List[Number]]:\n    \"\"\"\n    Return the transpose of the 2-D matrix `a`.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        A rectangular list-of-lists representing the matrix.\n\n    Returns\n    -------\n    list[list[int | float]]\n        The transposed matrix.\n\n    Raises\n    ------\n    ValueError\n        If `a` is not rectangular (i.e., its rows differ in length).\n\n    Examples\n    --------\n    >>> transpose_matrix([[1, 2, 3], [4, 5, 6]])\n    [[1, 4], [2, 5], [3, 6]]\n    >>> transpose_matrix([])\n    []\n    \"\"\"\n    if not a:\n        return []\n    row_lengths = {len(row) for row in a}\n    if len(row_lengths) != 1:\n        raise ValueError('All rows must have the same length to form a proper matrix.')\n    return [list(col) for col in zip(*a)]\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape a matrix (list of lists) to the requested shape.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        Input matrix.\n    new_shape : tuple[int, int]\n        Desired shape in the form (rows, columns).\n\n    Returns\n    -------\n    list[list[int | float]]\n        The reshaped matrix as a list of lists if possible;\n        otherwise an empty list ([]).\n    \"\"\"\n    if not isinstance(new_shape, tuple) or len(new_shape) != 2 or (not all((isinstance(x, int) and x > 0 for x in new_shape))):\n        return []\n    (rows, cols) = new_shape\n    flat = [item for row in a for item in row]\n    if len(flat) != rows * cols:\n        return []\n    reshaped = np.array(flat).reshape(rows, cols).tolist()\n    return reshaped\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List\nimport statistics\ndef calculate_matrix_mean(matrix: List[List[float]], mode: str) -> List[float]:\n    \"\"\"\n    Calculate the arithmetic mean of a 2-D numeric matrix either row-wise or column-wise.\n\n    Parameters\n    ----------\n    matrix : list[list[float]]\n        A rectangular list-of-lists (all rows must have the same length for\n        column mode).\n    mode : str\n        Either \"row\" or \"column\" (case\u2013insensitive).\n\n    Returns\n    -------\n    list[float]\n        A list containing the mean of every row (for mode=\"row\") or every\n        column (for mode=\"column\").\n\n    Raises\n    ------\n    ValueError\n        If the mode is not \"row\" or \"column\", or if the matrix is jagged when\n        column means are requested.\n    \"\"\"\n    mode = mode.lower()\n    if mode not in {'row', 'column'}:\n        raise ValueError('mode must be either \"row\" or \"column\"')\n    if not matrix:\n        return []\n    if mode == 'row':\n        return [statistics.mean(row) if row else float('nan') for row in matrix]\n    num_cols = len(matrix[0])\n    if any((len(row) != num_cols for row in matrix)):\n        raise ValueError('All rows must have the same length to compute column means')\n    return [statistics.mean(col) for col in zip(*matrix)]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiply every element of a 2-D matrix by a scalar.\n\n    Parameters\n    ----------\n    matrix : list[list[int | float]]\n        A rectangular (rows of equal length) 2-D list representing the matrix.\n    scalar : int | float\n        The value by which each element of the matrix is to be multiplied.\n\n    Returns\n    -------\n    list[list[int | float]]\n        A new matrix where each element is the product of the corresponding\n        element in `matrix` and `scalar`.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport cmath\nfrom typing import Union, List\nNumber = Union[int, float, complex]\ndef calculate_eigenvalues(matrix: List[List[Number]]) -> List[Number]:\n    \"\"\"\n    Calculate (and return in descending order) the eigenvalues of a 2\u00d72 matrix.\n\n    Parameters\n    ----------\n    matrix : list[list[float | int]]\n        2\u00d72 matrix represented as nested lists\n        [[a, b],\n         [c, d]]\n\n    Returns\n    -------\n    list[Number]\n        Eigenvalues \u03bb\u2081, \u03bb\u2082 sorted from highest to lowest (by real part).\n        They can be real or complex numbers.\n    \"\"\"\n    if len(matrix) != 2 or any((len(row) != 2 for row in matrix)):\n        raise ValueError('Input must be a 2\u00d72 matrix (list of two lists, each of length two).')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_disc = cmath.sqrt(discriminant)\n    \u03bb1 = (trace + sqrt_disc) / 2\n    \u03bb2 = (trace - sqrt_disc) / 2\n    eigenvalues = sorted([\u03bb1, \u03bb2], key=lambda x: (x.real, x.imag), reverse=True)\n    return eigenvalues\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Perform the matrix transformation  T^{-1} \u00b7 A \u00b7 S\n    --------------------------------------------------\n    Parameters\n    ----------\n    A : 2-D list\n        Base matrix to be transformed.\n    T : 2-D list (square)\n        Row-basis change matrix. Must be invertible and its order\n        must equal the number of rows of A.\n    S : 2-D list (square)\n        Column-basis change matrix. Must be invertible and its order\n        must equal the number of columns of A.\n\n    Returns\n    -------\n    2-D python list (numbers rounded to 4 decimals) or -1 if\n    * matrices are not conformable, or\n    * T or S is not invertible.\n    \"\"\"\n    try:\n        A_np = np.asarray(A, dtype=float)\n        T_np = np.asarray(T, dtype=float)\n        S_np = np.asarray(S, dtype=float)\n        if T_np.ndim != 2 or S_np.ndim != 2 or A_np.ndim != 2:\n            return -1\n        (nT, mT) = T_np.shape\n        (nS, mS) = S_np.shape\n        (nA, mA) = A_np.shape\n        if nT != mT or nS != mS:\n            return -1\n        if nT != nA or nS != mA:\n            return -1\n        if abs(np.linalg.det(T_np)) < 1e-12 or abs(np.linalg.det(S_np)) < 1e-12:\n            return -1\n        T_inv = np.linalg.inv(T_np)\n        transformed = T_inv @ A_np @ S_np\n        transformed = np.round(transformed, 4)\n        return transformed.tolist()\n    except Exception:\n        return -1\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    \"\"\"\n    Return the inverse of a 2\u00d72 matrix.\n    \n    Args:\n        matrix (list[list[float]]): A 2\u00d72 matrix in row-major order\n                                    [[a, b],\n                                     [c, d]]\n    \n    Returns:\n        list[list[float]] | None: The inverse matrix if it exists,\n                                  otherwise None when the matrix\n                                  is singular (non-invertible).\n    \"\"\"\n    if not isinstance(matrix, list) or len(matrix) != 2 or any((not isinstance(row, list) or len(row) != 2 for row in matrix)):\n        raise ValueError('Input must be a 2\u00d72 matrix provided as [[a, b], [c, d]].')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if abs(det) < 1e-12:\n        return None\n    inv_det = 1.0 / det\n    return [[d * inv_det, -b * inv_det], [-c * inv_det, a * inv_det]]\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Multiply two matrices a and b using classic row-by-column multiplication.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        Left matrix  (m \u00d7 n)\n    b : list[list[int | float]]\n        Right matrix (n \u00d7 p)\n\n    Returns\n    -------\n    list[list[int | float]]\n        Resulting matrix (m \u00d7 p) if the inner dimensions match.\n    -1\n        Returned when matrices are empty, jagged, or dimensions are\n        incompatible for multiplication.\n    \"\"\"\n    if not a or not b:\n        return -1\n    a_row_len = len(a[0])\n    if any((len(row) != a_row_len for row in a)):\n        return -1\n    b_row_len = len(b[0])\n    if any((len(row) != b_row_len for row in b)):\n        return -1\n    if a_row_len != len(b):\n        return -1\n    result: list[list[int | float]] = []\n    for row_a in a:\n        new_row = []\n        for col_idx in range(b_row_len):\n            val = sum((row_a[k] * b[k][col_idx] for k in range(a_row_len)))\n            new_row.append(val)\n        result.append(new_row)\n    return result\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List\nimport math\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the (sample) covariance matrix for a list of feature-vectors.\n\n    Parameters\n    ----------\n    vectors : list[list[float]]\n        Each inner list is a feature (all observations for that feature).\n        All inner lists must be the same length (same number of observations).\n\n    Returns\n    -------\n    list[list[float]]\n        A square matrix (as a list of lists) where entry (i, j)\n        is the sample covariance between feature i and feature j.\n    \"\"\"\n    if not vectors:\n        raise ValueError(\"Input list 'vectors' is empty.\")\n    n_features = len(vectors)\n    n_observations = len(vectors[0])\n    for v in vectors:\n        if len(v) != n_observations:\n            raise ValueError('All feature vectors must have the same number of observations.')\n        if n_observations < 2:\n            raise ValueError('At least two observations are required to compute covariance.')\n    means = [sum(v) / n_observations for v in vectors]\n    cov_matrix = [[0.0 for _ in range(n_features)] for _ in range(n_features)]\n    for i in range(n_features):\n        for j in range(i, n_features):\n            cov_ij = 0.0\n            for k in range(n_observations):\n                cov_ij += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            cov_ij /= n_observations - 1\n            cov_matrix[i][j] = cov_ij\n            cov_matrix[j][i] = cov_ij\n    return cov_matrix\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve Ax = b using the Jacobi iterative method.\n\n    Parameters\n    ----------\n    A : np.ndarray\n        Square coefficient matrix.\n    b : np.ndarray\n        Right-hand-side vector (or 1-D array).\n    n : int\n        Number of Jacobi iterations to perform.\n\n    Returns\n    -------\n    list\n        Approximate solution vector (rounded to 4 decimals) as a Python list.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).flatten()\n    (rows, cols) = A.shape\n    if rows != cols:\n        raise ValueError('Matrix A must be square.')\n    if b.size != rows:\n        raise ValueError(\"Vector b must have the same length as A's dimension.\")\n    if np.any(np.isclose(np.diag(A), 0.0)):\n        raise ValueError(\"Zero detected on A's diagonal; Jacobi method cannot proceed.\")\n    D = np.diag(A)\n    R = A - np.diagflat(D)\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x = (b - R @ x) / D\n        x = np.round(x, 4)\n    return x.tolist()\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the singular values of a 2\u00d72 matrix using a single-sweep Jacobi\n    (a.k.a. Jacobi \u201cJacobian\u201d) rotation on A\u1d40A.  The algorithm is exact for a\n    2\u00d72, yet the flow mirrors what would be done for larger matrices with a\n    full Jacobi SVD.\n\n    Parameters\n    ----------\n    A : np.ndarray\n        A real 2\u00d72 matrix.\n\n    Returns\n    -------\n    tuple\n        (\u03c3\u2081, \u03c3\u2082) \u2014 the singular values sorted in descending order and rounded\n        to 4 decimal places.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input must be a 2\u00d72 matrix.')\n    B = A.T @ A\n    (b00, b01, b11) = (B[0, 0], B[0, 1], B[1, 1])\n    if abs(b01) > 1e-12:\n        tau = (b11 - b00) / (2.0 * b01)\n        t = np.sign(tau) / (abs(tau) + np.sqrt(1.0 + tau * tau))\n        c = 1.0 / np.sqrt(1.0 + t * t)\n        s = c * t\n        V = np.array([[c, -s], [s, c]])\n        Sigma2 = V.T @ B @ V\n    else:\n        Sigma2 = B\n    (sigma1, sigma2) = np.sqrt(np.diag(Sigma2))\n    if sigma1 < sigma2:\n        (sigma1, sigma2) = (sigma2, sigma1)\n    sigma1 = float(np.round(sigma1, 4))\n    sigma2 = float(np.round(sigma2, 4))\n    return (sigma1, sigma2)\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Compute the determinant of a 4\u00d74 matrix via Laplace\u2019s expansion.\n\n    Parameters\n    ----------\n    matrix : list[list[int | float]]\n        A 4\u00d74 matrix represented as a list of four lists, each of length four.\n\n    Returns\n    -------\n    float\n        Determinant of the given matrix.\n\n    Raises\n    ------\n    ValueError\n        If the supplied matrix is not 4\u00d74.\n    \"\"\"\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4\u00d74 matrix (list of four lists, each of length four).')\n\n    def det(mat: list[list[int | float]]) -> float:\n        n = len(mat)\n        if n == 1:\n            return mat[0][0]\n        if n == 2:\n            return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n        total = 0.0\n        for col in range(n):\n            sign = -1 if col % 2 else 1\n            minor = [[mat[r][c] for c in range(n) if c != col] for r in range(1, n)]\n            total += sign * mat[0][col] * det(minor)\n        return total\n    return float(det(matrix))\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Fits a linear regression model using the normal equation.\n\n    Parameters\n    ----------\n    X : list[list[float]]\n        Feature matrix (each inner list is one sample\u2019s features).\n    y : list[float]\n        Target vector.\n\n    Returns\n    -------\n    list[float]\n        The regression coefficients, including the intercept as the\n        first element.  All coefficients are rounded to 4 decimal places.\n    \"\"\"\n    X_mat = np.asarray(X, dtype=float)\n    y_vec = np.asarray(y, dtype=float).reshape(-1, 1)\n    ones = np.ones((X_mat.shape[0], 1), dtype=float)\n    X_aug = np.hstack((ones, X_mat))\n    theta = np.linalg.pinv(X_aug.T @ X_aug) @ X_aug.T @ y_vec\n    theta = np.round(theta.flatten(), 4)\n    return theta.tolist()\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Perform linear regression with gradient descent.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with a leading column of ones for the intercept term.\n        Shape (m, n) where m is the number of samples.\n    y : np.ndarray\n        Target values. Shape (m,) or (m, 1).\n    alpha : float\n        Learning-rate.\n    iterations : int\n        Number of gradient-descent steps.\n\n    Returns\n    -------\n    list\n        Model coefficients (including intercept) rounded to 4 decimals.\n    \"\"\"\n    y = y.reshape(-1, 1)\n    (m, n) = X.shape\n    theta = np.zeros((n, 1))\n    for _ in range(iterations):\n        predictions = X @ theta\n        errors = predictions - y\n        gradient = X.T @ errors / m\n        theta -= alpha * gradient\n    theta = np.round(theta, 4).reshape(-1).tolist()\n    return theta\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform column-wise feature scaling on a 2-D NumPy array.\n    \n    Returns\n    -------\n    standardized_list : list[list[float]]\n        Z-score scaled features: (x - \u03bc) / \u03c3, rounded to 4 decimals.\n    minmax_list : list[list[float]]\n        Min-max scaled features: (x - min) / (max - min), rounded to 4 decimals.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError('Input array must be 2-D (samples \u00d7 features).')\n    means = data.mean(axis=0)\n    stds = data.std(axis=0, ddof=0)\n    stds_safe = np.where(stds == 0, 1, stds)\n    standardized = (data - means) / stds_safe\n    standardized[:, stds == 0] = 0\n    standardized = np.round(standardized, 4)\n    mins = data.min(axis=0)\n    maxs = data.max(axis=0)\n    ranges = maxs - mins\n    ranges_safe = np.where(ranges == 0, 1, ranges)\n    minmax = (data - mins) / ranges_safe\n    minmax[:, ranges == 0] = 0\n    minmax = np.round(minmax, 4)\n    return (standardized.tolist(), minmax.tolist())\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Simple, fully-vectorised k-Means clustering.\n\n    Parameters\n    ----------\n    points : list[tuple[float, float]]\n        Samples to cluster (works for any dimensionality, not only 2-D).\n    k : int\n        Number of clusters.\n    initial_centroids : list[tuple[float, float]]\n        Starting positions of the centroids.\n    max_iterations : int\n        Maximum number of updates before the algorithm stops.\n\n    Returns\n    -------\n    list[tuple[float, float]]\n        Final centroid coordinates, rounded to 4 decimal places.\n    \"\"\"\n    pts = np.asarray(points, dtype=float)\n    centroids = np.asarray(initial_centroids, dtype=float)\n    if centroids.shape[0] != k:\n        raise ValueError('Length of initial_centroids must equal k.')\n    if pts.ndim != 2 or centroids.ndim != 2:\n        raise ValueError('Points and centroids must be 2-D arrays.')\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(pts[:, None, :] - centroids[None, :, :], axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.empty_like(centroids)\n        for idx in range(k):\n            cluster_pts = pts[labels == idx]\n            if len(cluster_pts):\n                new_centroids[idx] = cluster_pts.mean(axis=0)\n            else:\n                new_centroids[idx] = centroids[idx]\n        if np.allclose(new_centroids, centroids):\n            centroids = new_centroids\n            break\n        centroids = new_centroids\n    return [tuple(np.round(c, 4)) for c in centroids]\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k: int=5, shuffle: bool=True, random_seed: int | None=None):\n    \"\"\"\n    Generate train / test indices for K-Fold cross-validation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, \u2026).\n    y : np.ndarray\n        Target vector/array of shape (n_samples, \u2026).  Only the first\n        dimension is used to validate consistency with `X`.\n    k : int, default=5\n        Number of folds.\n    shuffle : bool, default=True\n        Whether to shuffle samples before splitting.\n    random_seed : int or None, default=None\n        Seed for the RNG used when `shuffle=True`.\n\n    Returns\n    -------\n    folds : list[tuple[np.ndarray, np.ndarray]]\n        List of length `k`.  Each element is a tuple\n        `(train_indices, test_indices)` where both are 1-D `np.ndarray`\n        objects containing the integer indices of samples that belong\n        to the training and test set, respectively, for that fold.\n\n    Raises\n    ------\n    ValueError\n        If `k` is less than 2 or greater than the number of samples.\n    \"\"\"\n    n_samples = len(X)\n    if n_samples != len(y):\n        raise ValueError('`X` and `y` must have the same number of samples.')\n    if k < 2:\n        raise ValueError('`k` must be at least 2.')\n    if k > n_samples:\n        raise ValueError('`k` cannot be greater than the number of samples.')\n    if shuffle:\n        rng = np.random.default_rng(random_seed)\n        indices = rng.permutation(n_samples)\n    else:\n        indices = np.arange(n_samples)\n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[:n_samples % k] += 1\n    folds = []\n    current = 0\n    for fold_size in fold_sizes:\n        (start, stop) = (current, current + fold_size)\n        test_idx = indices[start:stop]\n        train_idx = np.concatenate((indices[:start], indices[stop:]))\n        folds.append((train_idx, test_idx))\n        current = stop\n    return folds\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis on a dataset.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array with shape (n_samples, n_features).\n    k : int\n        Number of principal components to return (k <= n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        A list containing the first k principal components.\n        Each inner list is an eigenvector (length = n_features),\n        rounded to 4 decimal places.\n    \"\"\"\n    X = data.astype(float)\n    mean = X.mean(axis=0)\n    std = X.std(axis=0, ddof=1)\n    X_std = (X - mean) / std\n    cov_mat = np.cov(X_std, rowvar=False)\n    (eig_vals, eig_vecs) = np.linalg.eigh(cov_mat)\n    sorted_idx = np.argsort(eig_vals)[::-1]\n    eig_vals = eig_vals[sorted_idx]\n    eig_vecs = eig_vecs[:, sorted_idx]\n    principal_components = eig_vecs[:, :k].T\n    principal_components = np.round(principal_components, 4).tolist()\n    return principal_components\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\nfrom copy import deepcopy\nfrom typing import Any, Dict, List\ndef entropy(examples: List[dict], target_attr: str) -> float:\n    \"\"\"\n    Shannon entropy of the class label in `examples`.\n    \"\"\"\n    total = len(examples)\n    if not total:\n        return 0.0\n    counts = Counter((ex[target_attr] for ex in examples))\n    return -sum((n / total * math.log2(n / total) for n in counts.values()))\ndef information_gain(examples: List[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    Information gain obtained by splitting `examples` on `attr`.\n    \"\"\"\n    total = len(examples)\n    if not total:\n        return 0.0\n    partitions: Dict[Any, List[dict]] = {}\n    for ex in examples:\n        key = ex[attr]\n        partitions.setdefault(key, []).append(ex)\n    weighted_entropy = sum((len(part) / total * entropy(part, target_attr) for part in partitions.values()))\n    return entropy(examples, target_attr) - weighted_entropy\ndef majority_class(examples: List[dict], target_attr: str) -> Any:\n    \"\"\"\n    Most common class label in `examples`.\n    \"\"\"\n    if not examples:\n        return None\n    return Counter((ex[target_attr] for ex in examples)).most_common(1)[0][0]\ndef learn_decision_tree(examples: List[dict], attributes: List[str], target_attr: str) -> Dict[str, Any]:\n    \"\"\"\n    Learn a decision tree (ID3 style) using entropy / information gain.\n\n    Parameters\n    ----------\n    examples : list of dict\n        Each element is a sample represented as {attr_name: value, ...}.\n    attributes : list of str\n        The candidate attributes we may still split on (target excluded).\n    target_attr : str\n        Name of the class label attribute.\n\n    Returns\n    -------\n    tree : dict\n        A nested dictionary encoding the learned tree.\n        Leaf nodes are represented directly by the class label\n        (e.g., 'yes' or 0 or 'setosa').\n        Internal nodes have the form:\n            {attribute_name: {value1: subtree1, value2: subtree2, ...}}\n    \"\"\"\n    if not examples:\n        return None\n    first_label = examples[0][target_attr]\n    if all((ex[target_attr] == first_label for ex in examples)):\n        return first_label\n    if not attributes:\n        return majority_class(examples, target_attr)\n    gains = [(information_gain(examples, attr, target_attr), attr) for attr in attributes]\n    (_, best_attr) = max(gains, key=lambda t: t[0])\n    tree: Dict[str, Any] = {best_attr: {}}\n    values = {ex[best_attr] for ex in examples}\n    for v in values:\n        subset = [ex for ex in examples if ex[best_attr] == v]\n        remaining_attrs = [a for a in attributes if a != best_attr]\n        subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n        if subtree is None:\n            subtree = majority_class(examples, target_attr)\n        tree[best_attr][v] = subtree\n    return tree\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Deterministic Pegasos for kernel\u2010SVMs (binary {-1 , +1} labels).\n\n    Returns\n    -------\n    alpha : list\n        Dual coefficients (shape = n_samples) rounded to 4 decimals.\n    bias  : float\n        Intercept term rounded to 4 decimals.\n    \"\"\"\n\n    def linear_kernel(X):\n        return X @ X.T\n\n    def rbf_kernel(X):\n        sq = np.sum(X * X, 1)[:, None]\n        dist2 = sq + sq.T - 2 * (X @ X.T)\n        return np.exp(-dist2 / (2.0 * sigma ** 2))\n    X = data.astype(float)\n    y = labels.astype(float).ravel()\n    n = X.shape[0]\n    if kernel == 'linear':\n        K = linear_kernel(X)\n    elif kernel == 'rbf':\n        K = rbf_kernel(X)\n    else:\n        raise ValueError(\"kernel must be 'linear' or 'rbf'\")\n    beta = np.zeros(n)\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        beta *= 1.0 - eta * lambda_val\n        f = K @ beta\n        viol = y * f < 1\n        idx = np.where(viol)[0]\n        if idx.size:\n            beta[idx] += eta / idx.size * y[idx]\n    alpha = beta * y\n    C = 1.0 / (lambda_val * n)\n    f_final = K @ beta\n    sv_mask = (alpha > 1e-06) & (alpha < C - 1e-06)\n    if np.any(sv_mask):\n        bias = np.mean(y[sv_mask] - f_final[sv_mask])\n    else:\n        bias = np.mean(y - f_final)\n    alpha = np.round(alpha, 4).tolist()\n    bias = round(float(bias), 4)\n    return (alpha, bias)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation for the input `z`.\n\n    Parameters\n    ----------\n    z : float\n        Input value.\n\n    Returns\n    -------\n    float\n        Sigmoid output rounded to four decimal places.\n    \"\"\"\n    s = 1.0 / (1.0 + math.exp(-z))\n    return round(s, 4)\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax activation values for a list of scores.\n\n    Parameters\n    ----------\n    scores : list[float]\n        Raw scores (logits).\n\n    Returns\n    -------\n    list[float]\n        Softmax probabilities, each rounded to four decimal places.\n    \"\"\"\n    if not scores:\n        return []\n    max_score = max(scores)\n    exp_shifted = [math.exp(s - max_score) for s in scores]\n    total = sum(exp_shifted)\n    softmax_vals = [round(val / total, 4) for val in exp_shifted]\n    return softmax_vals\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\nfrom typing import List, Tuple\nimport numpy as np\ndef single_neuron_model(features: List[List[float]], labels: List[int], weights: List[float], bias: float) -> Tuple[List[float], float]:\n    \"\"\"\n    Simulates a single\u2010neuron binary classifier with a sigmoid activation.\n\n    Parameters\n    ----------\n    features : list[list[float]]\n        2-D list where each inner list contains the feature values of one sample.\n    labels   : list[int]\n        Ground-truth binary labels (0/1) for every sample.\n    weights  : list[float]\n        Weight assigned to every feature (must match feature dimension).\n    bias     : float\n        Scalar bias term for the neuron.\n\n    Returns\n    -------\n    Tuple[List[float], float]\n        \u2022 List with the predicted probabilities (rounded to 4 decimals).  \n        \u2022 Mean-squared error between predictions & labels (rounded to 4 decimals).\n    \"\"\"\n    X = np.asarray(features, dtype=float)\n    w = np.asarray(weights, dtype=float)\n    y_true = np.asarray(labels, dtype=float)\n    z = X.dot(w) + bias\n    y_pred = 1.0 / (1.0 + np.exp(-z))\n    mse = np.mean((y_pred - y_true) ** 2)\n    y_pred = np.round(y_pred, 4).tolist()\n    mse = float(np.round(mse, 4))\n    return (y_pred, mse)\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Train a single sigmoid-activated neuron with mean\u2013squared-error loss.\n\n    Parameters\n    ----------\n    features        : 2-D array of shape (n_samples, n_features)\n    labels          : 1-D array of shape (n_samples,) containing 0/1 targets\n    initial_weights : 1-D array of shape (n_features,)\n    initial_bias    : scalar bias term\n    learning_rate   : gradient\u2013descent step size\n    epochs          : number of passes over the data\n\n    Returns\n    -------\n    weights : numpy.ndarray  (rounded to 4 decimals)\n    bias    : float          (rounded to 4 decimals)\n    losses  : list[float]    (epoch-wise MSE, each rounded to 4 decimals)\n    \"\"\"\n    X = np.asarray(features, dtype=float)\n    y = np.asarray(labels, dtype=float).reshape(-1)\n    w = np.asarray(initial_weights, dtype=float).reshape(-1).copy()\n    b = float(initial_bias)\n    n_samples = X.shape[0]\n\n    def _sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    losses = []\n    for _ in range(epochs):\n        z = X @ w + b\n        y_pred = _sigmoid(z)\n        error = y_pred - y\n        mse = np.mean(error ** 2)\n        losses.append(mse)\n        factor = 2.0 / n_samples * error * y_pred * (1 - y_pred)\n        grad_w = X.T @ factor\n        grad_b = np.sum(factor)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n    w_rounded = np.round(w, 4)\n    b_rounded = round(b, 4)\n    losses_list = np.round(losses, 4).tolist()\n    return (w_rounded, b_rounded, losses_list)\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = float(data)\n        self.grad = 0.0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0.0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (0.0 if out.data == 0 else 1.0) * out.grad\n        out._backward = _backward\n        return out\n    __radd__ = __add__\n    __rmul__ = __mul__\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'\n\n    def backward(self):\n        (topo, visited) = ([], set())\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the change\u2013of\u2013basis matrix P that converts coordinates expressed in\n    basis B to coordinates expressed in basis C for R\u00b3.\n\n    Given any vector v:\n        [v]_C = P \u00b7 [v]_B\n\n    Parameters\n    ----------\n    B : list[list[int | float]]\n        Three vectors of the B\u2013basis, each given in the standard basis.\n    C : list[list[int | float]]\n        Three vectors of the C\u2013basis, each given in the standard basis.\n\n    Returns\n    -------\n    list[list[float]]\n        3\u00d73 transformation matrix rounded to 4 decimal places (row\u2013major list).\n    \"\"\"\n    B_mat = np.array(B, dtype=float).T\n    C_mat = np.array(C, dtype=float).T\n    P = np.linalg.inv(C_mat) @ B_mat\n    return np.round(P, 4).tolist()\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Singular Value Decomposition for a real 2\u00d72 matrix using nothing\n    but eigen\u2013analysis as described in\n    https://metamerist.blogspot.com/2006/10/linear-algebra-for-graphics-geeks-svd.html\n    \n    Parameters\n    ----------\n    A : np.ndarray\n        A real 2\u00d72 matrix.\n    \n    Returns\n    -------\n    (U, S, V) : tuple\n        U, S and V are python lists (obtained with .tolist()) holding the\n        matrices rounded to 4 decimals such that\n\n            A \u2248 U \u00b7 S \u00b7 V              (all 2\u00d72)\n\n        Note that V here is already the transpose of the usual right\u2013\n        singular\u2013vector matrix, so the simple triple product reproduces A.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Only 2\u00d72 matrices are supported.')\n    A = A.astype(float)\n    AtA = A.T @ A\n    (eig_vals, eig_vecs) = np.linalg.eigh(AtA)\n    idx = np.argsort(eig_vals)[::-1]\n    eig_vals = eig_vals[idx]\n    V = eig_vecs[:, idx]\n    if np.linalg.det(V) < 0:\n        V[:, -1] *= -1\n    sing = np.sqrt(np.clip(eig_vals, 0, None))\n    S = np.diag(sing)\n    eps = 1e-10\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if sing[i] > eps:\n            U[:, i] = A @ V[:, i] / sing[i]\n    if sing[0] < eps and sing[1] < eps:\n        U = np.eye(2)\n    elif sing[0] < eps or sing[1] < eps:\n        nz = 0 if sing[0] > eps else 1\n        u = U[:, nz]\n        U[:, 1 - nz] = np.array([-u[1], u[0]])\n        U[:, 1 - nz] /= np.linalg.norm(U[:, 1 - nz])\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    Vt = np.round(V.T, 4)\n    return (U.tolist(), S.tolist(), Vt.tolist())\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle two NumPy arrays in unison while keeping the correspondence\n    between samples in X and y intact.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature array of shape (n_samples, \u2026).\n    y : np.ndarray\n        Label/target array of shape (n_samples, \u2026).\n    seed : int, optional\n        Seed for reproducible shuffling.  Default is None (non-deterministic).\n\n    Returns\n    -------\n    tuple(list, list)\n        A tuple containing the shuffled X and y, both converted to native\n        Python lists via NumPy\u2019s `tolist()` method.\n    \"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y)\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples.')\n    rng = np.random.default_rng(seed)\n    perm = rng.permutation(X.shape[0])\n    X_shuffled = X[perm]\n    y_shuffled = y[perm]\n    return (X_shuffled.tolist(), y_shuffled.tolist())\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generator that returns consecutive mini-batches from X (and y, if given).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix with shape (n_samples, \u2026).\n    y : np.ndarray or None, optional\n        Target array with length n_samples. If None, only X is batched.\n    batch_size : int, default=64\n        Number of samples per batch.\n\n    Yields\n    ------\n    list\n        \u2022 If y is None  : list representation of an X batch.  \n        \u2022 If y is given : tuple (X_batch_list, y_batch_list).\n    \"\"\"\n    X = np.asarray(X)\n    if batch_size <= 0:\n        raise ValueError('batch_size must be a positive integer.')\n    if y is not None:\n        y = np.asarray(y)\n        if X.shape[0] != y.shape[0]:\n            raise ValueError('X and y must contain the same number of samples.')\n    n_samples = X.shape[0]\n    for start in range(0, n_samples, batch_size):\n        end = start + batch_size\n        X_batch = X[start:end]\n        if y is None:\n            yield X_batch.tolist()\n        else:\n            y_batch = y[start:end]\n            yield (X_batch.tolist(), y_batch.tolist())\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Split a dataset into two subsets based on a feature-value threshold.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Input dataset.\n    feature_i : int\n        Index of the feature (column) on which to split.\n    threshold : float or int\n        Threshold value to compare against the chosen feature.\n\n    Returns\n    -------\n    tuple(list, list)\n        \u2022 First list  -> samples where X[:, feature_i] >= threshold  \n        \u2022 Second list -> samples where X[:, feature_i]  < threshold\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    if not 0 <= feature_i < X.shape[1]:\n        raise IndexError('feature_i is out of bounds for the number of features in X.')\n    mask = X[:, feature_i] >= threshold\n    left = X[mask]\n    right = X[~mask]\n    return (left.tolist(), right.tolist())\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a 2-D numpy array.\n\n    Parameters\n    ----------\n    X : array_like, shape (n_samples, n_features)\n        Input data.\n    degree : int\n        Maximum polynomial degree to generate (must be >= 0).\n\n    Returns\n    -------\n    list\n        Python list (of lists) containing the new feature matrix with all\n        polynomial combinations of the input features up to the given degree,\n        including the bias (constant) term.\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D array, got shape {}.'.format(X.shape))\n    if not isinstance(degree, int) or degree < 0:\n        raise ValueError('degree must be a non-negative integer, got {}.'.format(degree))\n    (n_samples, n_features) = X.shape\n    cols = []\n    for d in range(degree + 1):\n        for comb in combinations_with_replacement(range(n_features), d):\n            if not comb:\n                col = np.ones((n_samples, 1), dtype=X.dtype)\n            else:\n                col = np.prod(X[:, comb], axis=1, keepdims=True)\n            cols.append(col)\n    poly_X = np.hstack(cols)\n    return poly_X.tolist()\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets (bootstraps/permutations) of a dataset.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Feature matrix.\n    y : array-like, shape (n_samples,)\n        Target vector.\n    n_subsets : int\n        Number of random subsets to generate.\n    replacements : bool, default=True\n        If True, sample with replacement (bootstrap); otherwise, sample\n        without replacement (permutation).\n    seed : int, default=42\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    list\n        A list of length `n_subsets`. Each element is a tuple\n        (X_subset, y_subset) where both `X_subset` and `y_subset`\n        have been converted to pure Python lists via `.tolist()`.\n    \"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y)\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D array.')\n    if y.ndim != 1:\n        raise ValueError('y must be a 1-D array.')\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must contain the same number of samples.')\n    if n_subsets <= 0:\n        raise ValueError('n_subsets must be a positive integer.')\n    rng = np.random.default_rng(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = rng.integers(low=0, high=n_samples, size=n_samples)\n        else:\n            indices = rng.choice(n_samples, size=n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    One-hot encode a 1-D array of integer class labels.\n\n    Parameters\n    ----------\n    x : array-like (1-D)\n        Integer class labels to be encoded.\n    n_col : int, optional\n        Total number of columns (i.e., distinct classes).  \n        If None, it is inferred as max(x) + 1.\n\n    Returns\n    -------\n    list\n        One-hot encoded representation as a Python list of lists.\n    \"\"\"\n    x = np.asarray(x, dtype=int).ravel()\n    if n_col is None:\n        n_col = int(x.max()) + 1\n    one_hot = np.eye(n_col, dtype=int)[x]\n    return one_hot.tolist()\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1-D numpy array into a diagonal matrix and return it as a\n    native Python list (via NumPy\u2019s `tolist()` method).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        A one-dimensional array whose elements will form the diagonal\n        of the resulting matrix.\n\n    Returns\n    -------\n    list\n        A 2-D list representing the diagonal matrix.\n    \"\"\"\n    x = np.asarray(x)\n    if x.ndim != 1:\n        raise ValueError('Input must be a 1D numpy array.')\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : array-like (1D)\n        Ground-truth labels.\n    y_pred : array-like (1D)\n        Predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy score (proportion of correct predictions).\n\n    Raises\n    ------\n    ValueError\n        If the input arrays have different lengths or are empty.\n    \"\"\"\n    y_true = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    if y_true.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.size != y_pred.size:\n        raise ValueError('y_true and y_pred must have the same length.')\n    correct = np.sum(y_true == y_pred)\n    accuracy = correct / y_true.size\n    return float(accuracy)\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate a Pearson\u2010correlation matrix.\n    \n    Parameters\n    ----------\n    X : 2-D numpy array (n_samples \u00d7 n_features_x)\n    Y : 2-D numpy array (n_samples \u00d7 n_features_y), optional\n        If omitted, Y is taken to be X.\n    \n    Returns\n    -------\n    list\n        2-D Python list containing the correlation coefficients,\n        rounded to 4 decimal places.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D array.')\n    if Y is None:\n        Y = X\n    else:\n        Y = np.asarray(Y, dtype=float)\n        if Y.ndim != 2:\n            raise ValueError('Y must be a 2-D array.')\n        if Y.shape[0] != X.shape[0]:\n            raise ValueError('X and Y must have the same number of rows (samples).')\n    X_mean = X.mean(axis=0)\n    Y_mean = Y.mean(axis=0)\n    X_std = X.std(axis=0, ddof=1)\n    Y_std = Y.std(axis=0, ddof=1)\n    X_std[X_std == 0] = np.nan\n    Y_std[Y_std == 0] = np.nan\n    Xz = (X - X_mean) / X_std\n    Yz = (Y - Y_mean) / Y_std\n    n = X.shape[0]\n    corr = Xz.T @ Yz / (n - 1)\n    corr = np.round(corr, 4)\n    return corr.tolist()\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Train an AdaBoost ensemble of decision stumps.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Training data.\n    y : ndarray of shape (n_samples,)\n        Labels. Accepts {-1, 1} or {0, 1}. The latter will be converted.\n    n_clf : int\n        Number of weak classifiers (decision stumps) to learn.\n\n    Returns\n    -------\n    clfs : list\n        A list with one dictionary per learned stump.  Each dictionary holds\n        \u2022 feature      : index of the feature used\n        \u2022 threshold    : threshold value\n        \u2022 polarity     : 1  or -1 (`1` means: predict -1 for x < thr,\n                                    -1 means: predict -1 for x > thr)\n        \u2022 alpha        : classifier weight  \n        All numeric items are rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    if set(np.unique(y)) == {0, 1}:\n        y = y * 2 - 1\n    (n_samples, n_features) = X.shape\n    w = np.full(n_samples, 1.0 / n_samples)\n    clfs = []\n    eps = 1e-10\n    for _ in range(n_clf):\n        best = {'feature': None, 'threshold': None, 'polarity': 1, 'error': float('inf'), 'prediction': None}\n        for j in range(n_features):\n            feature_values = X[:, j]\n            thresholds = np.unique(feature_values)\n            for thr in thresholds:\n                for pol in (1, -1):\n                    pred = np.ones(n_samples)\n                    if pol == 1:\n                        pred[feature_values < thr] = -1\n                    else:\n                        pred[feature_values > thr] = -1\n                    err = np.sum(w[pred != y])\n                    if err < best['error']:\n                        best.update(feature=j, threshold=thr, polarity=pol, error=err, prediction=pred.copy())\n        err = max(best['error'], eps)\n        alpha = 0.5 * math.log((1 - err) / err)\n        w *= np.exp(-alpha * y * best['prediction'])\n        w /= w.sum()\n        clf = dict(feature=best['feature'], threshold=round(float(best['threshold']), 4), polarity=int(best['polarity']), alpha=round(float(alpha), 4))\n        clfs.append(clf)\n    return clfs\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a 1-D list/array of scores.\n\n    Parameters\n    ----------\n    scores : list or np.ndarray\n        Input scores.\n\n    Returns\n    -------\n    list\n        Log-softmax values rounded to 4 decimal places.\n    \"\"\"\n    x = np.asarray(scores, dtype=np.float64)\n    x_shifted = x - np.max(x)\n    log_softmax_vals = x_shifted - np.log(np.sum(np.exp(x_shifted)))\n    return np.round(log_softmax_vals, 4).tolist()\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nimport numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        \"\"\"\n        Receives the dimensionality of the input that will be fed\n        to the layer.  (Called by the network-builder before the\n        real training starts.)\n        \"\"\"\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n    \"\"\"\n    Fully-connected (a.k.a. \u201cdense\u201d) layer.\n    --------------------------------------------------------------------\n    1) W  : weight matrix                    \u2013 shape  (n_in , n_units)\n    2) w0 : bias vector                      \u2013 shape  (n_units, )\n    3) *Opt: (deep-copies of) the optimiser  \u2013 one for W, one for w0\n    --------------------------------------------------------------------\n    All public methods return python lists and are 4-decimal rounded,\n    as demanded in the exercise statement.\n    \"\"\"\n\n    def __init__(self, n_units, input_shape=None):\n        self.n_units = n_units\n        self.input_shape = input_shape\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n        self.layer_input = None\n\n    def initialize(self, optimizer):\n        \"\"\"\n        Called once the network knows the real input dimensionality and\n        once an optimizer object is available.\n        \"\"\"\n        if self.input_shape is None:\n            raise ValueError('Input shape is undefined. Call `set_input_shape` before `initialize`.')\n        limit = 1.0 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros(self.n_units)\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        if self.W is None or self.w0 is None:\n            return 0\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        \"\"\"\n        X : ndarray (batch_size, n_in)\n        returns list \u2013 4-decimal rounded\n        \"\"\"\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return np.round(output, 4).tolist()\n\n    def backward_pass(self, accum_grad):\n        \"\"\"\n        accum_grad : ndarray/list  (batch_size, n_units)\n                     gradient arriving from the next layer\n        returns     : list \u2013 gradient wrt the layer input (n_in)\n        \"\"\"\n        accum_grad = np.asarray(accum_grad)\n        grad_W = np.dot(self.layer_input.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0)\n        if self.trainable and self.W_opt is not None:\n            self.W = self.W_opt.update(self.W, grad_W)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        grad_input = np.dot(accum_grad, self.W.T)\n        return np.round(grad_input, 4).tolist()\n\n    def output_shape(self):\n        return (self.n_units,)\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int=0, stride: int=1):\n    \"\"\"\n    A very small, framework\u2013free 2-D convolution layer (single input-,\n    single output-channel).\n\n    Parameters\n    ----------\n    input_matrix : np.ndarray\n        2-D array (H \u00d7 W) representing the input feature map.\n    kernel       : np.ndarray\n        2-D array (kH \u00d7 kW) holding the convolutional weights.\n        (No flip is performed \u2192 cross-correlation, as in most DL libs.)\n    padding      : int\n        How many zero rows / columns to add symmetrically around\n        the input.\n    stride       : int\n        Number of pixels to move the kernel each step.\n\n    Returns\n    -------\n    list\n        Convolved feature map rounded to 4 dp and converted to a\n        regular Python list.\n    \"\"\"\n    if input_matrix.ndim != 2 or kernel.ndim != 2:\n        raise ValueError('input_matrix and kernel must both be 2-D arrays.')\n    if padding < 0 or stride <= 0:\n        raise ValueError('padding must be >= 0 and stride must be > 0.')\n    padded = np.pad(input_matrix, pad_width=((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    (H_p, W_p) = padded.shape\n    (kH, kW) = kernel.shape\n    out_h = (H_p - kH) // stride + 1\n    out_w = (W_p - kW) // stride + 1\n    if out_h <= 0 or out_w <= 0:\n        raise ValueError('Kernel size / padding / stride combination produces invalid output dimension.')\n    out = np.empty((out_h, out_w), dtype=float)\n    for i in range(out_h):\n        for j in range(out_w):\n            hs = i * stride\n            ws = j * stride\n            window = padded[hs:hs + kH, ws:ws + kW]\n            out[i, j] = np.sum(window * kernel)\n    out = np.round(out, 4)\n    return out.tolist()\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef relu(z: float) -> float:\n    \"\"\"\n    Rectified Linear Unit (ReLU) activation function.\n\n    Parameters\n    ----------\n    z : float\n        Input value.\n\n    Returns\n    -------\n    float\n        z if z > 0, else 0.\n    \"\"\"\n    return z if z > 0 else 0.0\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D feature matrix of shape (n_samples, n_features).\n    w : np.ndarray\n        1-D coefficient vector of shape (n_features, ).\n    y_true : np.ndarray\n        1-D array with the true target values of shape (n_samples, ).\n    alpha : float\n        Regularisation strength (\u03bb).\n\n    Returns\n    -------\n    float\n        Ridge loss rounded to 4 decimal places.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_pred - y_true) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    total_loss = mse + reg_term\n    return float(np.round(total_loss, 4))\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Compute the Leaky ReLU activation for a single scalar input.\n\n    Parameters\n    ----------\n    z : float\n        The input value.\n    alpha : float, optional\n        Slope applied when z is negative (default is 0.01).\n\n    Returns\n    -------\n    float | int\n        The transformed value after applying the Leaky ReLU function.\n    \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel (dot product) between two input vectors.\n\n    Parameters\n    ----------\n    x1, x2 : array-like\n        Input vectors of the same length.\n\n    Returns\n    -------\n    float\n        The dot product \u27e8x1, x2\u27e9.\n    \"\"\"\n    v1 = np.asarray(x1).ravel()\n    v2 = np.asarray(x2).ravel()\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same length.')\n    return float(np.dot(v1, v2))\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Compute the precision for binary classification.\n    \n    Parameters\n    ----------\n    y_true : array-like\n        Ground\u2013truth (correct) binary labels (0 or 1).\n    y_pred : array-like\n        Predicted binary labels (0 or 1).\n    \n    Returns\n    -------\n    float\n        Precision = TP / (TP + FP).  \n        Returns 0.0 if there are no positive predictions (TP + FP == 0).\n    \"\"\"\n    y_true = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    if y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError('y_true and y_pred must have the same number of elements.')\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    denom = tp + fp\n    return tp / denom if denom != 0 else 0.0\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Gradient\u2013descent optimizer that supports\n        1. 'batch'       : Batch Gradient Descent\n        2. 'stochastic'  : Stochastic Gradient Descent\n        3. 'mini-batch'  : Mini-Batch Gradient Descent\n\n    Parameters\n    ----------\n    X : ndarray, shape (m, n)\n        Feature matrix.\n    y : ndarray, shape (m,)  or  (m, 1)\n        Target vector.\n    weights : ndarray, shape (n,)  or  (n, 1)\n        Initial weight vector.\n    learning_rate : float\n        Learning rate (\u03b1).\n    n_iterations : int\n        Number of epochs (full passes over the training data).\n    batch_size : int, default=1\n        Size of mini-batches when method='mini-batch'.\n        Ignored for 'batch'.  For 'stochastic', an internal\n        batch size of 1 is always used.\n    method : str, {'batch', 'stochastic', 'mini-batch'}, default='batch'\n        Variant of gradient descent to apply.\n\n    Returns\n    -------\n    list\n        Optimised weights rounded to 4 decimals, in native Python list\n        form (via ndarray.tolist()).\n    \"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y).reshape(-1)\n    weights = np.asarray(weights).reshape(-1)\n    (m, n) = X.shape\n\n    def _batch_grad(x_batch, y_batch, w):\n        \"\"\"\n        Computes gradient of MSE w.r.t. w for a batch.\n        \"\"\"\n        b_m = x_batch.shape[0]\n        preds = x_batch.dot(w)\n        errors = preds - y_batch\n        grad = 2.0 / b_m * x_batch.T.dot(errors)\n        return grad\n    for epoch in range(n_iterations):\n        if method == 'batch':\n            grad = _batch_grad(X, y, weights)\n            weights -= learning_rate * grad\n        elif method == 'stochastic':\n            indices = np.random.permutation(m)\n            for idx in indices:\n                x_i = X[idx:idx + 1]\n                y_i = y[idx]\n                grad = _batch_grad(x_i, y_i, weights)\n                weights -= learning_rate * grad\n        elif method == 'mini-batch':\n            indices = np.random.permutation(m)\n            for start in range(0, m, batch_size):\n                end = start + batch_size\n                batch_idx = indices[start:end]\n                x_b = X[batch_idx]\n                y_b = y[batch_idx]\n                grad = _batch_grad(x_b, y_b, weights)\n                weights -= learning_rate * grad\n        else:\n            raise ValueError(\"method must be 'batch', 'stochastic' or 'mini-batch'\")\n    weights = np.round(weights, 4)\n    return weights.reshape(-1).tolist()\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix, *, tol=1e-12):\n    \"\"\"\n    Convert the given matrix to Reduced Row Echelon Form (RREF).\n\n    Parameters\n    ----------\n    matrix : array-like\n        Input matrix (list of lists, tuple of tuples, numpy.ndarray \u2026).\n    tol : float, optional\n        Numerical tolerance used to decide when a value is effectively zero.\n        Default is 1e-12.\n\n    Returns\n    -------\n    list\n        The RREF of `matrix`, returned as (nested) Python lists\n        via NumPy\u2019s ``tolist()`` method.\n    \"\"\"\n    A = np.array(matrix, dtype=float, copy=True)\n    (m, n) = A.shape\n    pivot_row = 0\n    for col in range(n):\n        if pivot_row >= m:\n            break\n        rows_below = np.abs(A[pivot_row:, col])\n        max_idx_rel = np.argmax(rows_below)\n        max_val = rows_below[max_idx_rel]\n        if max_val < tol:\n            continue\n        pivot_idx = pivot_row + max_idx_rel\n        if pivot_idx != pivot_row:\n            A[[pivot_row, pivot_idx]] = A[[pivot_idx, pivot_row]]\n        pivot_val = A[pivot_row, col]\n        A[pivot_row] = A[pivot_row] / pivot_val\n        for r in range(m):\n            if r != pivot_row:\n                factor = A[r, col]\n                if abs(factor) > tol:\n                    A[r] -= factor * A[pivot_row]\n        pivot_row += 1\n    A[np.abs(A) < tol] = 0.0\n    return A.tolist()\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Adam optimizer implementation.\n\n    Parameters\n    ----------\n    f : callable\n        Objective function to minimize. Not directly used in the core\n        algorithm, but kept for completeness / potential monitoring.\n    grad : callable\n        Function that computes the gradient of `f`. It must accept a\n        single argument `x` and return the gradient array of the same\n        shape as `x`.\n    x0 : array-like\n        Initial parameter vector.\n    learning_rate : float, optional\n        Step size (\u03b1). Default is 1e-3.\n    beta1 : float, optional\n        Exponential decay rate for first-moment estimates (\u03b2\u2081). Default 0.9.\n    beta2 : float, optional\n        Exponential decay rate for second-moment estimates (\u03b2\u2082). Default 0.999.\n    epsilon : float, optional\n        Small constant for numerical stability. Default 1e-8.\n    num_iterations : int, optional\n        Number of update steps. Default 1000.\n\n    Returns\n    -------\n    list\n        Optimized parameters, flattened and rounded to 4 decimal places.\n    \"\"\"\n    x = np.asarray(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(1, num_iterations + 1):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1.0 - beta1) * g\n        v = beta2 * v + (1.0 - beta2) * g ** 2\n        m_hat = m / (1.0 - beta1 ** t)\n        v_hat = v / (1.0 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).reshape(-1).tolist()\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Lasso (L1-regularised) Linear Regression trained with (batch) Gradient Descent.\n\n    Parameters\n    ----------\n    X : np.array, shape = (n_samples, n_features)\n        Design / feature matrix.\n    y : np.array, shape = (n_samples,) or (n_samples, 1)\n        Target vector.\n    alpha : float, default=0.1\n        Strength of the L1 penalty term.\n    learning_rate : float, default=0.01\n        Step size for the gradient descent updates.\n    max_iter : int, default=1000\n        Maximum number of iterations.\n    tol : float, default=1e-4\n        Threshold for early stopping based on absolute change in the\n        objective function between two consecutive iterations.\n\n    Returns\n    -------\n    (weights_list, bias)\n        weights_list : python list of shape (n_features,) rounded to 4 decimals.\n        bias         : float rounded to 4 decimals.\n    \"\"\"\n    y = y.reshape(-1)\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n\n    def objective(pred, w):\n        mse_term = 0.5 / n_samples * np.sum((y - pred) ** 2)\n        l1_term = alpha * np.sum(np.abs(w))\n        return mse_term + l1_term\n    prev_cost = float('inf')\n    for _ in range(max_iter):\n        y_pred = X @ w + b\n        error = y_pred - y\n        grad_w = 1.0 / n_samples * (X.T @ error) + alpha * np.sign(w)\n        grad_b = 1.0 / n_samples * np.sum(error)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n        curr_cost = objective(y_pred, w)\n        if abs(prev_cost - curr_cost) < tol:\n            break\n        prev_cost = curr_cost\n    w = np.round(w, 4)\n    b = round(b, 4)\n    return (w.tolist(), b)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Optimal String Alignment (restricted Damerau-Levenshtein) distance\n    ---------------------------------------------------------------\n    Allowed unit-cost operations:\n        \u2022 insertion\n        \u2022 deletion\n        \u2022 substitution\n        \u2022 transposition of two adjacent characters\n    A character (or character pair) can be edited at most once \u2014 the\n    restriction that separates OSA from the full Damerau-Levenshtein metric.\n    \n    Parameters\n    ----------\n    source : str\n        String to be transformed.\n    target : str\n        Desired result string.\n    \n    Returns\n    -------\n    int\n        Minimum number of edits needed to turn `source` into `target`.\n    \"\"\"\n    if source == target:\n        return 0\n    if not source:\n        return len(target)\n    if not target:\n        return len(source)\n    (m, n) = (len(source), len(target))\n    D = np.zeros((m + 1, n + 1), dtype=int)\n    D[0, :] = np.arange(n + 1)\n    D[:, 0] = np.arange(m + 1)\n    for i in range(1, m + 1):\n        s_i = source[i - 1]\n        for j in range(1, n + 1):\n            t_j = target[j - 1]\n            cost = 0 if s_i == t_j else 1\n            deletion = D[i - 1, j] + 1\n            insertion = D[i, j - 1] + 1\n            substitution = D[i - 1, j - 1] + cost\n            D[i, j] = min(deletion, insertion, substitution)\n            if i > 1 and j > 1 and (s_i == target[j - 2]) and (source[i - 2] == t_j):\n                transposition = D[i - 2, j - 2] + 1\n                if transposition < D[i, j]:\n                    D[i, j] = transposition\n    return int(D[m, n])\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Compute the recall score for binary classification.\n\n    Parameters\n    ----------\n    y_true : list or array-like\n        Ground-truth binary labels (0 or 1).\n    y_pred : list or array-like\n        Predicted binary labels (0 or 1).\n\n    Returns\n    -------\n    float\n        Recall rounded to three decimal places. Returns 0.0 if there are\n        no actual positive samples (i.e., TP + FN == 0).\n    \"\"\"\n    y_true = np.asarray(y_true).astype(int)\n    y_pred = np.asarray(y_pred).astype(int)\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    rec = tp / denominator\n    return round(rec, 3)\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": false, "result": "failed: self_attention() takes 3 positional arguments but 4 were given", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Computes scaled-dot-product self-attention.\n    \n    Parameters\n    ----------\n    Q, K, V : np.ndarray\n        \u2022 2-D shape : (seq_len , d_k)  \n        \u2022 3-D shape : (batch , seq_len , d_k)\n        The three inputs must share the same last-dimension size (d_k).\n    \n    Returns\n    -------\n    list\n        Attention output rounded to 4 decimal places and converted to Python list.\n    \"\"\"\n    Q = np.asarray(Q, dtype=np.float64)\n    K = np.asarray(K, dtype=np.float64)\n    V = np.asarray(V, dtype=np.float64)\n    d_k = K.shape[-1]\n    scale = np.sqrt(d_k)\n    if Q.ndim == 2:\n        scores = Q @ K.T / scale\n    else:\n        scores = np.matmul(Q, K.transpose(0, 2, 1)) / scale\n    max_score = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - max_score)\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    if V.ndim == 2:\n        output = attention_weights @ V\n    else:\n        output = np.matmul(attention_weights, V)\n    output = np.round(output, 4)\n    return output.tolist()\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Runs a simple RNN forward pass over an input sequence and returns the\n    final hidden state (rounded to 4 decimal places).\n\n    Parameters\n    ----------\n    input_sequence : list of list of floats\n        A sequence of T input vectors, each of dimensionality `input_size`.\n    initial_hidden_state : list of floats\n        Initial hidden state vector of dimensionality `hidden_size`.\n    Wx : list of list of floats\n        Input-to-hidden weight matrix of shape (hidden_size, input_size).\n    Wh : list of list of floats\n        Hidden-to-hidden weight matrix of shape (hidden_size, hidden_size).\n    b : list of floats\n        Bias vector of shape (hidden_size,).\n\n    Returns\n    -------\n    list of floats\n        Final hidden state after processing the whole sequence, with every\n        element rounded to 4 decimal places.\n    \"\"\"\n    Wx = np.asarray(Wx, dtype=float)\n    Wh = np.asarray(Wh, dtype=float)\n    b = np.asarray(b, dtype=float)\n    h = np.asarray(initial_hidden_state, dtype=float)\n    for x_t in input_sequence:\n        x_t = np.asarray(x_t, dtype=float)\n        h = np.tanh(Wx @ x_t + Wh @ h + b)\n    return np.round(h, 4).tolist()\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Apply a 2-D translation to a collection of points.\n\n    Parameters\n    ----------\n    points : list[list[float]]\n        List of points, each defined as [x, y].\n    tx : float\n        Translation distance along the x-axis.\n    ty : float\n        Translation distance along the y-axis.\n\n    Returns\n    -------\n    list[list[float]]\n        Translated points, converted back to a pure Python list\n        via NumPy\u2019s ``tolist()`` method.\n    \"\"\"\n    pts = np.asarray(points, dtype=float)\n    if pts.ndim != 2 or pts.shape[1] != 2:\n        raise ValueError('`points` must be a list/array of shape (N, 2).')\n    ones = np.ones((pts.shape[0], 1), dtype=float)\n    homogeneous_pts = np.hstack((pts, ones))\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]], dtype=float)\n    translated_homogeneous = homogeneous_pts @ translation_matrix.T\n    translated_pts = translated_homogeneous[:, :2]\n    return translated_pts.tolist()\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the KL divergence   D_KL( P || Q )  between two normal distributions\n    \n        P ~ N(mu_p, sigma_p^2)\n        Q ~ N(mu_q, sigma_q^2)\n    \n    Parameters\n    ----------\n    mu_p, mu_q : float or array-like\n        Means of the two distributions.\n    sigma_p, sigma_q : float or array-like\n        Standard deviations (not variances!) of the two distributions.\n        Must be strictly positive.\n    \n    Returns\n    -------\n    float\n        KL divergence.  If the inputs are arrays, the divergence is summed\n        over all corresponding dimensions (i.e. treating each dimension as\n        independent and additive in KL).\n    \"\"\"\n    mu_p = np.asarray(mu_p, dtype=float)\n    mu_q = np.asarray(mu_q, dtype=float)\n    sigma_p = np.asarray(sigma_p, dtype=float)\n    sigma_q = np.asarray(sigma_q, dtype=float)\n    if np.any(sigma_p <= 0) or np.any(sigma_q <= 0):\n        raise ValueError('Standard deviations must be positive.')\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n    kl_element = np.log(sigma_q / sigma_p) + (var_p + (mu_p - mu_q) ** 2) / (2 * var_q) - 0.5\n    return float(np.sum(kl_element))\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the linear system Ax = b using the Gauss-Seidel iterative method.\n\n    Parameters\n    ----------\n    A : array-like (m \u00d7 m)\n        Coefficient matrix (assumed square and non-singular).\n    b : array-like (m,)\n        Right-hand-side vector.\n    n : int\n        Number of Gauss-Seidel iterations to perform.\n    x_ini : array-like, optional\n        Initial guess for the solution.  If None, a zero vector is used.\n\n    Returns\n    -------\n    list\n        Approximated solution vector (rounded to 4 decimals) as a Python list.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).reshape(-1)\n    m = A.shape[0]\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.asarray(x_ini, dtype=float).reshape(-1)\n    for _ in range(n):\n        for i in range(m):\n            sigma_left = np.dot(A[i, :i], x[:i])\n            sigma_right = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - sigma_left - sigma_right) / A[i, i]\n    return np.round(x, 4).reshape(-1).tolist()\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian Elimination\n    with partial pivoting.\n    \n    Parameters\n    ----------\n    A :   (n, n) array_like\n          Coefficient matrix.\n    b :   (n,) array_like\n          Right\u2013hand-side vector.\n    \n    Returns\n    -------\n    list\n          Solution vector rounded to 4 decimals and converted\n          to a regular Python list.\n    \"\"\"\n    A = np.array(A, dtype=float).copy()\n    b = np.array(b, dtype=float).copy().flatten()\n    n = b.size\n    for k in range(n - 1):\n        pivot_row = k + np.argmax(np.abs(A[k:, k]))\n        if np.isclose(A[pivot_row, k], 0):\n            raise ValueError('Matrix is singular or nearly singular.')\n        if pivot_row != k:\n            A[[k, pivot_row]] = A[[pivot_row, k]]\n            b[[k, pivot_row]] = b[[pivot_row, k]]\n        for i in range(k + 1, n):\n            factor = A[i, k] / A[k, k]\n            A[i, k:] -= factor * A[k, k:]\n            b[i] -= factor * b[k]\n    if np.isclose(A[-1, -1], 0):\n        raise ValueError('Matrix is singular or nearly singular.')\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        s = np.dot(A[i, i + 1:], x[i + 1:])\n        x[i] = (b[i] - s) / A[i, i]\n    return np.round(x, 4).tolist()\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        limit = np.sqrt(1.0 / (input_size + hidden_size))\n        self.Wf = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n        self.Wi = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n        self.Wc = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n        self.Wo = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Parameters\n        ----------\n        x : np.ndarray, shape (seq_len, input_size)\n            Sequence to process.  Each row is the input vector at that time-step.\n        initial_hidden_state : np.ndarray, shape (hidden_size,) or (hidden_size, 1)\n        initial_cell_state   : np.ndarray, shape (hidden_size,) or (hidden_size, 1)\n\n        Returns\n        -------\n        hidden_states : list\n            Rounded (4-dp) hidden state for every time-step, converted to python list.\n            Shape conceptually (seq_len, hidden_size)\n        final_hidden_state : list\n            Rounded (4-dp) hidden state after last step.\n        final_cell_state : list\n            Rounded (4-dp) cell state after last step.\n        \"\"\"\n        h_t = np.asarray(initial_hidden_state).reshape(self.hidden_size, 1).astype(float)\n        c_t = np.asarray(initial_cell_state).reshape(self.hidden_size, 1).astype(float)\n        seq = np.asarray(x, dtype=float)\n        seq_len = seq.shape[0]\n        hidden_seq = []\n        for t in range(seq_len):\n            x_t = seq[t].reshape(self.input_size, 1)\n            concat = np.vstack((h_t, x_t))\n            f_t = sigmoid(self.Wf @ concat + self.bf)\n            i_t = sigmoid(self.Wi @ concat + self.bi)\n            c_hat_t = np.tanh(self.Wc @ concat + self.bc)\n            o_t = sigmoid(self.Wo @ concat + self.bo)\n            c_t = f_t * c_t + i_t * c_hat_t\n            h_t = o_t * np.tanh(c_t)\n            hidden_seq.append(h_t.T)\n        hidden_seq = np.vstack(hidden_seq)\n        hidden_seq_rounded = np.round(hidden_seq, 4)\n        final_h_rounded = np.round(h_t.T, 4)\n        final_c_rounded = np.round(c_t.T, 4)\n        return (hidden_seq_rounded.tolist(), final_h_rounded.flatten().tolist(), final_c_rounded.flatten().tolist())\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom math import log\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Computes the TF-IDF matrix (docs  \u00d7 query_terms).\n\n    Parameters\n    ----------\n    corpus : List[List[str]]\n        Each sub-list represents a document (tokenised list of words).\n    query :  List[str]\n        List of words for which TF-IDF scores are needed.\n\n    Returns\n    -------\n    List[List[float]]\n        A 2-D list where element [i][j] is the TF-IDF of query[j] in\n        document i.  Values are rounded to 4 decimal places.\n    \"\"\"\n    if len(query) == 0:\n        return []\n    if len(corpus) == 0:\n        return []\n    n_docs = len(corpus)\n    df = {term: 0 for term in query}\n    for doc in corpus:\n        words_in_doc = set(doc)\n        for term in query:\n            if term in words_in_doc:\n                df[term] += 1\n    idf = {}\n    for term in query:\n        idf_val = log((n_docs + 1) / (df[term] + 1)) + 1\n        idf[term] = idf_val\n    tf_idf_matrix = np.zeros((n_docs, len(query)), dtype=float)\n    for (i, doc) in enumerate(corpus):\n        doc_len = len(doc)\n        if doc_len == 0:\n            continue\n        counts = {}\n        for word in doc:\n            counts[word] = counts.get(word, 0) + 1\n        for (j, term) in enumerate(query):\n            tf = counts.get(term, 0) / doc_len\n            tf_idf_matrix[i, j] = tf * idf[term]\n    tf_idf_matrix = np.round(tf_idf_matrix, 4)\n    return tf_idf_matrix.tolist()\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels (0/1)\n    :param y_pred: Numpy array of predicted labels (0/1)\n    :param beta: The weight of recall relative to precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    if beta < 0:\n        raise ValueError('beta has to be non-negative')\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n    tp = np.sum(y_true & y_pred)\n    fp = np.sum(~y_true & y_pred)\n    fn = np.sum(y_true & ~y_pred)\n    precision = tp / (tp + fp) if tp + fp else 0.0\n    recall = tp / (tp + fn) if tp + fn else 0.0\n    if precision == 0.0 and recall == 0.0:\n        return 0.0\n    beta_sq = beta ** 2\n    f_beta = (1 + beta_sq) * precision * recall / (beta_sq * precision + recall)\n    return round(f_beta, 3)\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: SimpleRNN.backward() missing 3 required positional arguments: 'outputs', 'last_inputs', and 'last_hiddens'", "solution": "import numpy as np\nimport numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Build a tiny RNN.\n        -------------------------------------------------\n            input_size   : dimensionality of every item x_t\n            hidden_size  : dimensionality of hidden state  h_t\n            output_size  : dimensionality of every output  y_t\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.initialize_weights(input_size, hidden_size, output_size)\n\n    def initialize_weights(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Weight matrices are drawn from \ud835\udcdd(0,1) and scaled by 0.01.\n        Biases are initialised to 0.\n        \"\"\"\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def rnn_forward(self, input_sequence):\n        \"\"\"\n        input_sequence : ndarray  (seq_len, input_size)\n\n        Returns\n        -------\n        outputs       : list of ndarray (output_size, 1) \u2013  y_1 \u2026 y_T\n        last_inputs   : list of ndarray (input_size , 1) \u2013  x_1 \u2026 x_T\n        last_hiddens  : list of ndarray (hidden_size, 1) \u2013  h_1 \u2026 h_T\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        for x_t in input_sequence:\n            x_t = x_t.reshape(-1, 1)\n            h_t = np.tanh(self.W_xh @ x_t + self.W_hh @ h_prev + self.b_h)\n            y_t = self.W_hy @ h_t + self.b_y\n            outputs.append(y_t)\n            last_inputs.append(x_t)\n            last_hiddens.append(h_t)\n            h_prev = h_t\n        return (outputs, last_inputs, last_hiddens)\n\n    def forward(self, input_sequence):\n        (outputs, _, _) = self.rnn_forward(input_sequence)\n        return outputs\n\n    def rnn_backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate=0.01):\n        \"\"\"\n        Back-propagate the error through time and update parameters.\n\n        expected_output : ndarray (seq_len, output_size)\n        Returns\n        -------\n        total_loss : float   (aggregated \u00bd\u00b7MSE over the whole sequence)\n        \"\"\"\n        seq_len = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros((self.hidden_size, 1))\n        total_loss = 0.0\n        for t in reversed(range(seq_len)):\n            y_pred = outputs[t]\n            y_true = expected_output[t].reshape(-1, 1)\n            dy = y_pred - y_true\n            total_loss += 0.5 * np.sum(dy ** 2)\n            dW_hy += dy @ last_hiddens[t].T\n            db_y += dy\n            dh = self.W_hy.T @ dy + dh_next\n            dh_raw = (1 - last_hiddens[t] ** 2) * dh\n            dW_xh += dh_raw @ last_inputs[t].T\n            prev_h = np.zeros_like(last_hiddens[0]) if t == 0 else last_hiddens[t - 1]\n            dW_hh += dh_raw @ prev_h.T\n            db_h += dh_raw\n            dh_next = self.W_hh.T @ dh_raw\n        for dparam in (dW_xh, dW_hh, dW_hy, db_h, db_y):\n            np.clip(dparam, -1, 1, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return total_loss\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate=0.01):\n        return self.rnn_backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n\n    def train_on_sequence(self, input_sequence, expected_output, learning_rate=0.01):\n        \"\"\"\n        Runs a forward pass then BPTT update.  Returns the loss.\n        \"\"\"\n        (outputs, last_inputs, last_hiddens) = self.rnn_forward(input_sequence)\n        loss = self.rnn_backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n        return loss\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol: float=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix (m \u00d7 m)\n    :param b: Right-hand side vector (m,)\n    :param n: Maximum number of CG iterations allowed\n    :param x0: Optional initial guess (defaults to the zero vector)\n    :param tol: Euclidean-norm tolerance for \u2016r_k\u2016 = \u2016b \u2212 A x_k\u2016\n    :return: Solution vector x (rounded to 8 decimals and returned as a Python list)\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float)\n    (m, m2) = A.shape\n    if m != m2 or b.shape[0] != m:\n        raise ValueError('Incompatible matrix/vector dimensions.')\n    x = np.zeros_like(b) if x0 is None else np.asarray(x0, dtype=float).copy()\n    if x.shape != b.shape:\n        raise ValueError('Initial guess x0 has incorrect shape.')\n    r = b - A @ x\n    p = r.copy()\n    rs_old = float(r @ r)\n    if np.sqrt(rs_old) < tol:\n        return np.round(x, 8).tolist()\n    for _ in range(n):\n        Ap = A @ p\n        alpha = rs_old / float(p @ Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = float(r @ r)\n        if np.sqrt(rs_new) < tol:\n            break\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n    return np.round(x, 8).tolist()\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    labels = np.asarray(y)\n    (_, counts) = np.unique(labels, return_counts=True)\n    probs = counts / counts.sum()\n    gini = 1.0 - np.sum(probs ** 2)\n    return round(float(gini), 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    Parameters\n    ----------\n    dense_matrix : list[list[Number]]\n        A 2-D (row \u2011 major) Python list representing the dense matrix.\n\n    Returns\n    -------\n    tuple[list, list, list]\n        (values, column_indices, row_pointer) where\n            values          : non\u2013zero elements in row-major order\n            column_indices  : corresponding column indices\n            row_pointer     : cumulative counts of non-zeros per row.\n                              Always starts with 0 and has length rows + 1.\n    \"\"\"\n    if not dense_matrix:\n        return ([], [], [0])\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        for (col_idx, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_idx)\n        row_pointer.append(len(values))\n    return (values, column_indices, row_pointer)\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: list/tuple of numbers \u2013 the vector to be projected\n    :param L: list/tuple of numbers \u2013 the line vector defining the direction\n    :return: list \u2013 projection of v onto L, each coordinate rounded to 3 dp\n    \"\"\"\n    if len(v) != len(L):\n        raise ValueError('Vectors v and L must have the same dimension.')\n    if not any(L):\n        raise ValueError('Line vector L must be non-zero.')\n    dot_vL = sum((vi * Li for (vi, Li) in zip(v, L)))\n    dot_LL = sum((Li * Li for Li in L))\n    scale = dot_vL / dot_LL\n    proj = [round(scale * Li, 3) for Li in L]\n    return proj\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    Parameters\n    ----------\n    dense_matrix : list[list]\n        Rectangular (m \u00d7 n) matrix stored as a list of rows.\n\n    Returns\n    -------\n    tuple\n        (values, row_indices, col_ptr) where\n            values       : list of all non-zero entries, column by column\n            row_indices  : list of row indices for each stored value\n            col_ptr      : list whose length is n_columns + 1;\n                           col_ptr[j] is the index in `values` where column j starts,\n                           and col_ptr[-1] == len(values)\n    \"\"\"\n    if not dense_matrix:\n        return ([], [], [0])\n    n_rows = len(dense_matrix)\n    n_cols = len(dense_matrix[0])\n    for (r, row) in enumerate(dense_matrix):\n        if len(row) != n_cols:\n            raise ValueError(f'Row {r} has length {len(row)}; expected {n_cols}')\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(n_cols):\n        for row in range(n_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Return a basis of the column-space (image) of the matrix A.\n\n    Parameters\n    ----------\n    A : array_like, shape (m, n)\n        The input matrix.\n\n    Returns\n    -------\n    list\n        A list\u2010of\u2010lists representing the columns of A that form a basis\n        of its column space (rounded to 8 decimals).\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    (m, n) = A.shape\n    B = A.copy()\n    pivot_cols = []\n    row = 0\n    tol = 1e-12\n    for col in range(n):\n        pivot = None\n        for r in range(row, m):\n            if abs(B[r, col]) > tol:\n                pivot = r\n                break\n        if pivot is None:\n            continue\n        if pivot != row:\n            B[[row, pivot]] = B[[pivot, row]]\n        pivot_cols.append(col)\n        for r in range(row + 1, m):\n            factor = B[r, col] / B[row, col]\n            if abs(factor) > tol:\n                B[r, col:] -= factor * B[row, col:]\n        row += 1\n        if row == m:\n            break\n    if pivot_cols:\n        basis = A[:, pivot_cols]\n        basis = np.round(basis, 8)\n        return basis.tolist()\n    else:\n        return []\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the coefficient of determination (R-squared).\n\n    Parameters\n    ----------\n    y_true : array-like\n        Actual/observed target values.\n    y_pred : array-like\n        Predicted target values from the regression model.\n\n    Returns\n    -------\n    float\n        R-squared value, rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    if ss_tot == 0:\n        return round(0.0, 3)\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Parameters\n    ----------\n    img : list[list[int or float]]\n        2-D list where each inner list represents a row of pixel values\n        ranging from 0 (black) to 255 (white).\n\n    Returns\n    -------\n    float\n        Average brightness rounded to two decimals, or\n        -1 for any invalid input:\n          \u2022 empty image / empty rows\n          \u2022 inconsistent row lengths\n          \u2022 pixel value outside 0-255 or non-numeric\n    \"\"\"\n    if not img or not isinstance(img, list):\n        return -1\n    row_len = len(img[0])\n    if row_len == 0:\n        return -1\n    total = 0.0\n    count = 0\n    for row in img:\n        if not isinstance(row, (list, tuple)) or len(row) != row_len:\n            return -1\n        for px in row:\n            try:\n                val = float(px)\n            except (TypeError, ValueError):\n                return -1\n            if val < 0 or val > 255:\n                return -1\n            total += val\n        count += row_len\n    average = total / count\n    return round(average, 2)\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Compute the Root Mean Square Error (RMSE) between two equal-shaped\n    numeric sequences.\n\n    Parameters\n    ----------\n    y_true : array-like\n        Actual/true target values.\n    y_pred : array-like\n        Predicted target values.\n\n    Returns\n    -------\n    float\n        RMSE rounded to three decimal places.\n\n    Raises\n    ------\n    ValueError\n        * The two inputs cannot be converted to numeric arrays.\n        * The arrays are empty.\n        * The arrays have mismatched shapes.\n        * The arrays contain NaN or Inf values.\n    \"\"\"\n    try:\n        y_true = np.asarray(y_true, dtype=float)\n        y_pred = np.asarray(y_pred, dtype=float)\n    except (TypeError, ValueError):\n        raise ValueError('Inputs must be array-like and contain numeric values.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if np.isnan(y_true).any() or np.isnan(y_pred).any():\n        raise ValueError('Input arrays must not contain NaN values.')\n    if np.isinf(y_true).any() or np.isinf(y_pred).any():\n        raise ValueError('Input arrays must not contain Inf values.')\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_val = np.sqrt(mse)\n    return round(float(rmse_val), 3)\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Compute the Jaccard Index for two binary label arrays.\n\n    Parameters\n    ----------\n    y_true : array-like\n        Ground-truth binary labels (0/1).\n    y_pred : array-like\n        Predicted binary labels (0/1).\n\n    Returns\n    -------\n    float\n        Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(float(jaccard), 3)\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Compute the Dice (S\u00f8rensen\u2013Dice) coefficient for two binary label arrays.\n\n    Parameters\n    ----------\n    y_true : array-like\n        Ground-truth binary labels (0/1 or False/True).\n    y_pred : array-like\n        Predicted binary labels (0/1 or False/True).\n\n    Returns\n    -------\n    float\n        Dice score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('`y_true` and `y_pred` must have the same shape.')\n    intersection = np.logical_and(y_true, y_pred).sum()\n    positives_true = y_true.sum()\n    positives_pred = y_pred.sum()\n    denominator = positives_true + positives_pred\n    if denominator == 0:\n        dice = 1.0\n    else:\n        dice = 2.0 * intersection / denominator\n    return round(float(dice), 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef _bipolar_hv(seed: int, dim: int) -> np.ndarray:\n    \"\"\"\n    Create a deterministic bipolar (+1 / \u20131) hypervector of dimensionality *dim*\n    using *seed*.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    return rng.randint(0, 2, dim, dtype=np.int8) * 2 - 1\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a dataset *row* using Hyper-dimensional\n    Computing.\n    \n    Parameters\n    ----------\n    row : dict\n        Keys are feature names, values are the corresponding data values.\n    dim : int\n        Dimensionality of the hypervectors.\n    random_seeds : dict\n        Mapping from feature name -> integer seed.  The same seed is used to\n        create both the feature-name hypervector and, in a deterministic way,\n        the value hypervector so that results are fully reproducible.\n    \n    Returns\n    -------\n    list\n        Bipolar composite hypervector representing the whole row.\n    \"\"\"\n    if not row:\n        return [0] * dim\n    bundle = np.zeros(dim, dtype=int)\n    for (feature, value) in row.items():\n        seed = random_seeds.get(feature, 0)\n        feat_hv = _bipolar_hv(seed, dim)\n        value_seed = (seed ^ hash(str(value)) & 4294967295) & 4294967295\n        value_hv = _bipolar_hv(value_seed, dim)\n        bound_hv = feat_hv * value_hv\n        bundle += bound_hv\n    composite = np.where(bundle >= 0, 1, -1)\n    return composite.tolist()\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Build a 2\\u2006\u00d7\\u20062 confusion matrix for a binary-classification task.\n\n    Parameters\n    ----------\n    data : list[list]\n        Each inner list/tuple is [y_true, y_pred].\n\n    Returns\n    -------\n    list[list]\n        A 2\\u2006\u00d7\\u20062 matrix in the same layout used by\n        sklearn.metrics.confusion_matrix:\n\n                    predicted\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502  0   \u2502  1   \u2502\n        \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n        true   0  \u2502 TN   \u2502 FP   \u2502\n               1  \u2502 FN   \u2502 TP   \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        i.e. rows correspond to the actual class, columns to the prediction.\n    \"\"\"\n    labels = sorted({true for (true, _) in data} | {pred for (_, pred) in data})\n    if len(labels) != 2:\n        raise ValueError('Exactly two distinct class labels are required.')\n    idx = {label: i for (i, label) in enumerate(labels)}\n    matrix = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        i = idx[y_true]\n        j = idx[y_pred]\n        matrix[i][j] += 1\n    return matrix\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Parameters\n    ----------\n    v1, v2 : np.ndarray\n        Input vectors of identical shape.\n\n    Returns\n    -------\n    float\n        Cosine similarity rounded to three decimal places.\n\n    Raises\n    ------\n    ValueError\n        If the vectors have different shapes, are empty,\n        or have zero magnitude.\n    \"\"\"\n    v1 = np.asarray(v1, dtype=float)\n    v2 = np.asarray(v2, dtype=float)\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    norm1 = np.linalg.norm(v1)\n    norm2 = np.linalg.norm(v2)\n    if norm1 == 0 or norm2 == 0:\n        raise ValueError('Input vectors must have non-zero magnitude.')\n    similarity = np.dot(v1.ravel(), v2.ravel()) / (norm1 * norm2)\n    return round(float(similarity), 3)\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[list[list[int]], float, float, float, float]:\n    \"\"\"\n    Calculate confusion-matrix, accuracy, F1-score, specificity and\n    negative-predictive-value for a binary-classification result.\n\n    Parameters\n    ----------\n    actual     : list[int]   \u2013 ground-truth labels (0/1)\n    predicted  : list[int]   \u2013 model predictions (0/1)\n\n    Returns\n    -------\n    (\n        [[TN, FP],\n         [FN, TP]],         # confusion matrix\n        accuracy,            # float (3 d.p.)\n        f1_score,            # float (3 d.p.)\n        specificity,         # float (3 d.p.)\n        negative_pred_value  # float (3 d.p.)\n    )\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('`actual` and `predicted` must have the same length.')\n    if not all((x in (0, 1) for x in actual + predicted)):\n        raise ValueError('Elements of `actual` and `predicted` must be 0 or 1.')\n    tn = fp = fn = tp = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            tp += 1\n        elif a == 1 and p == 0:\n            fn += 1\n        elif a == 0 and p == 1:\n            fp += 1\n        elif a == 0 and p == 0:\n            tn += 1\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    total = tn + fp + fn + tp\n    accuracy = (tp + tn) / total if total else 0\n    precision = tp / (tp + fp) if tp + fp else 0\n    recall = tp / (tp + fn) if tp + fn else 0\n    f1_score = 2 * precision * recall / (precision + recall) if precision + recall else 0\n    specificity = tn / (tn + fp) if tn + fp else 0\n    negative_predictive_value = tn / (tn + fn) if tn + fn else 0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\nfrom collections import Counter\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate several descriptive\u2013statistics metrics for a 1-D numeric\n    iterable (list, tuple, NumPy array \u2026).\n\n    Parameters\n    ----------\n    data : 1-D array-like\n        Numeric data.\n\n    Returns\n    -------\n    dict\n        Dictionary whose keys are\n        'mean', 'median', 'mode', 'variance', 'standard_deviation',\n        '25th_percentile', '50th_percentile', '75th_percentile',\n        'interquartile_range'.\n\n        Every value is rounded to four decimal places **except** the\n        mode, which is returned exactly as it appears in the data\n        (or, when several modes exist, as a list of the un-rounded\n        modal values).\n    \"\"\"\n    arr = np.asarray(data, dtype=float)\n    if arr.size == 0:\n        raise ValueError('`data` must contain at least one value.')\n    arr = arr[~np.isnan(arr)]\n    if arr.size == 0:\n        raise ValueError('`data` contains only NaNs.')\n    mean_ = round(float(np.mean(arr)), 4)\n    median_ = round(float(np.median(arr)), 4)\n    counts = Counter(arr)\n    max_freq = max(counts.values())\n    modes = [k for (k, v) in counts.items() if v == max_freq]\n    mode_ = modes[0] if len(modes) == 1 else modes\n    var_ = round(float(np.var(arr, ddof=0)), 4)\n    std_ = round(float(np.std(arr, ddof=0)), 4)\n    p25 = round(float(np.percentile(arr, 25)), 4)\n    p50 = round(float(np.percentile(arr, 50)), 4)\n    p75 = round(float(np.percentile(arr, 75)), 4)\n    iqr_ = round(float(p75 - p25), 4)\n    return {'mean': mean_, 'median': median_, 'mode': mode_, 'variance': var_, 'standard_deviation': std_, '25th_percentile': p25, '50th_percentile': p50, '75th_percentile': p75, 'interquartile_range': iqr_}\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    \n    :param n: Total number of trials (non-negative integer)\n    :param k: Number of successes (non-negative integer, 0 \u2264 k \u2264 n)\n    :param p: Probability of success on each trial (0 \u2264 p \u2264 1)\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    if not 0 <= p <= 1 or n < 0 or k < 0 or (k > n):\n        return 0.0\n    try:\n        comb = math.comb(n, k)\n    except AttributeError:\n\n        def _comb(n_, k_):\n            k_ = min(k_, n_ - k_)\n            numerator = 1\n            denominator = 1\n            for i in range(1, k_ + 1):\n                numerator *= n_ - (k_ - i)\n                denominator *= i\n            return numerator // denominator\n        comb = _comb(n, k)\n    prob = comb * p ** k * (1 - p) ** (n - k)\n    return round(prob, 5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    :param x:        The value at which the PDF is evaluated.\n    :param mean:     The mean (\u03bc) of the distribution.\n    :param std_dev:  The standard deviation (\u03c3) of the distribution.\n    :return:         PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be positive.')\n    coeff = 1.0 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coeff * math.exp(exponent)\n    return round(pdf_value, 5)\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n\n    P(X = k) = (lam ** k) * e^(\u2212lam) / k!\n\n    Parameters\n    ----------\n    k : int\n        Number of events (must be a non-negative integer).\n    lam : float or int\n        Average rate (mean) of occurrences in the given interval (must be \u2265 0).\n\n    Returns\n    -------\n    float\n        Probability rounded to 5 decimal places.\n\n    Raises\n    ------\n    ValueError\n        If k is negative or not an integer, or if lam is negative.\n    \"\"\"\n    if not (isinstance(k, int) and k >= 0):\n        raise ValueError('k must be a non-negative integer.')\n    if lam < 0:\n        raise ValueError('lam (\u03bb) must be non-negative.')\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n\n    Contrast (simple range-based definition) = max_pixel_value \u2212 min_pixel_value.\n\n    Args\n    ----\n    img : numpy.ndarray\n        2-D array (H \u00d7 W) representing a grayscale image whose pixel values are\n        expected to be in the range 0\u2013255 (integer or float).\n\n    Returns\n    -------\n    float\n        The contrast value. It is zero for a completely flat image and at most\n        255 for an 8-bit image.\n\n    Raises\n    ------\n    ValueError\n        If `img` is not a 2-D numpy array.\n    \"\"\"\n    img = np.asarray(img)\n    if img.ndim != 2:\n        raise ValueError(f'Expected a 2-D grayscale image, got array with shape {img.shape}')\n    img_min = float(np.min(img))\n    img_max = float(np.max(img))\n    contrast = img_max - img_min\n    return contrast\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1-D array representing the first vector.\n        vec2 (numpy.ndarray): 1-D array representing the second vector.\n\n    Returns\n    -------\n        float or int:\n            The dot product of `vec1` and `vec2`.\n\n    Raises\n    ------\n        ValueError: If either input is not one-dimensional or if the vectors\n                    do not have the same length.\n        TypeError:  If the inputs cannot be interpreted as NumPy arrays.\n    \"\"\"\n    try:\n        vec1 = np.asarray(vec1)\n        vec2 = np.asarray(vec2)\n    except Exception as exc:\n        raise TypeError('Both inputs must be array-like objects that can be converted to NumPy arrays.') from exc\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1-D vectors.')\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError('Vectors must be of the same length.')\n    return np.dot(vec1, vec2)\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation (polynomial feature expansion).\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The maximum degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: Nested list where each inner list contains the polynomial\n                           features of the corresponding input, rounded to 8 decimals.\n                           If `degree` is less than 0 an empty list is returned.\n    \"\"\"\n    if degree < 0:\n        return []\n    if not data:\n        return []\n    vander = np.vander(np.asarray(data, dtype=float), N=degree + 1, increasing=True)\n    return np.round(vander, 8).tolist()\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Create positional encodings as described in \u201cAttention Is All You Need\u201d.\n    \n    Parameters\n    ----------\n    position : int\n        Maximum length of the input sequence (number of positions).\n    d_model : int\n        Dimensionality of the model / embedding size.\n    \n    Returns\n    -------\n    list  |  int\n        A `position \u00d7 d_model` list (dtype=float16) containing the positional \n        encodings, or -1 for invalid inputs.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_indices = np.arange(position)[:, np.newaxis]\n    dim_indices = np.arange(d_model)[np.newaxis, :]\n    angle_rates = 1 / np.power(10000, 2 * (dim_indices // 2) / d_model)\n    angle_rads = pos_indices * angle_rates\n    pe = np.empty_like(angle_rads, dtype=np.float32)\n    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    return pe.astype(np.float16).tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine whether a model is overfitting, underfitting, or a good fit.\n    \n    Parameters\n    ----------\n    training_accuracy : float\n        Training accuracy of the model (expected range: 0.0 \u2013 1.0).\n    test_accuracy : float\n        Test/validation accuracy of the model (expected range: 0.0 \u2013 1.0).\n\n    Returns\n    -------\n    int\n        1  -> Overfitting\n        -1 -> Underfitting\n        0  -> Good fit\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    \n    Adjusts the learning rate based on the moving averages of the\n    gradient and squared gradient, with bias-correction.\n    \n    Parameters\n    ----------\n    parameter : scalar or np.ndarray\n        Current parameter value \u03b8\u209c\n    grad : scalar or np.ndarray\n        Current gradient  \u2207\u03b8\u209c\n    m : scalar or np.ndarray\n        Exponential moving average of the gradient (1\u02e2\u1d57 moment)\n    v : scalar or np.ndarray\n        Exponential moving average of the squared gradient (2\u207f\u1d48 moment)\n    t : int\n        Current timestep (starts at 1)\n    learning_rate : float, optional\n        Step size \u03b1\n    beta1 : float, optional\n        Decay rate for the 1\u02e2\u1d57 moment  (default 0.9)\n    beta2 : float, optional\n        Decay rate for the 2\u207f\u1d48 moment  (default 0.999)\n    epsilon : float, optional\n        Numerical-stability term (default 1e-8)\n    \n    Returns\n    -------\n    tuple\n        (updated_parameter, updated_m, updated_v) \u2013 each rounded to\n        5 decimals and converted to list(s).\n    \"\"\"\n    parameter = np.asarray(parameter, dtype=np.float64)\n    grad = np.asarray(grad, dtype=np.float64)\n    m = np.asarray(m, dtype=np.float64)\n    v = np.asarray(v, dtype=np.float64)\n    m = beta1 * m + (1.0 - beta1) * grad\n    v = beta2 * v + (1.0 - beta2) * np.square(grad)\n    m_hat = m / (1.0 - beta1 ** t)\n    v_hat = v / (1.0 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    param_out = np.round(parameter, 5).tolist()\n    m_out = np.round(m, 5).tolist()\n    v_out = np.round(v, 5).tolist()\n    return (param_out, m_out, v_out)\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: 'n_embd'", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\ndef layer_norm(x, g, b, eps: float=1e-05):\n    \"\"\"Applies layer normalisation over last dimension.\"\"\"\n    mu = x.mean(-1, keepdims=True)\n    var = ((x - mu) ** 2).mean(-1, keepdims=True)\n    x_hat = (x - mu) / np.sqrt(var + eps)\n    return g * x_hat + b\ndef split_heads(x, n_head):\n    \"\"\"\n    (seq, emb) -> (head, seq, head_dim)\n    \"\"\"\n    (seq_len, emb) = x.shape\n    head_dim = emb // n_head\n    x = x.reshape(seq_len, n_head, head_dim)\n    return np.transpose(x, (1, 0, 2))\ndef merge_heads(x):\n    \"\"\"\n    (head, seq, head_dim) -> (seq, emb)\n    \"\"\"\n    (n_head, seq_len, head_dim) = x.shape\n    return np.transpose(x, (1, 0, 2)).reshape(seq_len, n_head * head_dim)\ndef causal_attention(q, k, v):\n    \"\"\"\n    q, k, v : (head, seq, head_dim)\n    returns : (head, seq, head_dim)\n    \"\"\"\n    dk = q.shape[-1]\n    scores = np.matmul(q, k.transpose(0, 2, 1)) / np.sqrt(dk)\n    seq_len = scores.shape[-1]\n    mask = np.tril(np.ones((seq_len, seq_len), dtype=np.float32))\n    scores = scores * mask - 10000000000.0 * (1 - mask)\n    probs = np.exp(scores - scores.max(-1, keepdims=True))\n    probs = probs / probs.sum(-1, keepdims=True)\n    return np.matmul(probs, v)\ndef make_tiny_block(embed_dim, n_head, seed=0):\n    rng = np.random.RandomState(seed)\n    head_dim = embed_dim // n_head\n    return {'wq': rng.randn(embed_dim, embed_dim) * 0.02, 'wk': rng.randn(embed_dim, embed_dim) * 0.02, 'wv': rng.randn(embed_dim, embed_dim) * 0.02, 'wo': rng.randn(embed_dim, embed_dim) * 0.02, 'w1': rng.randn(embed_dim, 4 * embed_dim) * 0.02, 'w2': rng.randn(4 * embed_dim, embed_dim) * 0.02, 'ln1_g': np.ones(embed_dim), 'ln1_b': np.zeros(embed_dim), 'ln2_g': np.ones(embed_dim), 'ln2_b': np.zeros(embed_dim), 'n_head': n_head}\ndef transformer_block(x, block):\n    \"\"\"\n    x : (seq, emb)\n    \"\"\"\n    h = layer_norm(x, block['ln1_g'], block['ln1_b'])\n    q = h @ block['wq']\n    k = h @ block['wk']\n    v = h @ block['wv']\n    q = split_heads(q, block['n_head'])\n    k = split_heads(k, block['n_head'])\n    v = split_heads(v, block['n_head'])\n    attn_out = causal_attention(q, k, v)\n    attn_out = merge_heads(attn_out)\n    h = attn_out @ block['wo']\n    x = x + h\n    h2 = layer_norm(x, block['ln2_g'], block['ln2_b'])\n    h2 = h2 @ block['w1']\n    h2 = np.maximum(h2, 0)\n    h2 = h2 @ block['w2']\n    return x + h2\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    \"\"\"\n    Very small, didactic GPT-2-style generator.\n    \"\"\"\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    n_embd = hparams['n_embd']\n    if not params['blocks']:\n        params['blocks'].append(make_tiny_block(n_embd, n_head, seed=123))\n    tokens = encoder.encode(prompt)\n    if len(tokens) == 0:\n        raise ValueError('Prompt must contain at least one recognised token.')\n    for _ in range(n_tokens_to_generate):\n        if len(tokens) >= n_ctx:\n            break\n        seq_len = len(tokens)\n        idx = np.arange(seq_len)\n        wte = params['wte'][tokens]\n        wpe = params['wpe'][idx]\n        h = wte + wpe\n        h = transformer_block(h, params['blocks'][0])\n        h = layer_norm(h, params['ln_f']['g'], params['ln_f']['b'])\n        last_hidden = h[-1]\n        logits = last_hidden @ params['wte'].T\n        probs = np.exp(logits - logits.max())\n        probs = probs / probs.sum()\n        next_tok = int(np.random.choice(len(probs), p=probs))\n        tokens.append(next_tok)\n    return encoder.decode(tokens)\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    A minimal self-attention style calculation on a 1-D sequence of crystal\n    values.\n\n    Parameters\n    ----------\n    n : int\n        Number of crystals (must match len(crystal_values)).\n    crystal_values : list/tuple/np.ndarray\n        Numeric value for each crystal.\n    dimension : int\n        Unused in this simplified setting, kept only to respect the signature.\n\n    Returns\n    -------\n    list[float]\n        The enhanced pattern (attention-weighted value) for every crystal,\n        each rounded to four decimal places.\n    \"\"\"\n\n    def softmax(x):\n        \"\"\"\n        Stable softmax applied along the last axis.\n        \"\"\"\n        x = np.asarray(x, dtype=np.float64)\n        x -= x.max()\n        exp_x = np.exp(x)\n        return exp_x / exp_x.sum()\n    values = np.asarray(crystal_values, dtype=np.float64)\n    if values.shape[0] != n:\n        raise ValueError('`n` must match the number of provided crystal values.')\n    patterns = []\n    for i in range(n):\n        scores = values[i] * values\n        attn_weights = softmax(scores)\n        enhanced_value = np.dot(attn_weights, values)\n        patterns.append(round(float(enhanced_value), 4))\n    return patterns\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'lower'", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    corpus : list[str] \u2013 collection of documents\n    query  : str       \u2013 query string\n    k1, b  : float     \u2013 BM25 parameters\n    returns: list[float] \u2013 BM25 score for each document (rounded to 3 decimals)\n    \"\"\"\n    docs_tokens = [doc.lower().split() for doc in corpus]\n    query_terms = query.lower().split()\n    N = len(corpus)\n    doc_lens = [len(toks) for toks in docs_tokens]\n    avgdl = np.mean(doc_lens) if doc_lens else 0.0\n    df = {}\n    for tokens in docs_tokens:\n        for term in set(tokens):\n            df[term] = df.get(term, 0) + 1\n    scores = []\n    for (tokens, dl) in zip(docs_tokens, doc_lens):\n        tf = Counter(tokens)\n        score = 0.0\n        for term in query_terms:\n            if term not in df:\n                continue\n            idf = np.log((N - df[term] + 0.5) / (df[term] + 0.5) + 1)\n            freq = tf.get(term, 0)\n            denom = freq + k1 * (1 - b + b * dl / avgdl) if avgdl else 1\n            score += idf * (freq * (k1 + 1)) / denom\n        scores.append(round(score, 3))\n    return scores\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('`y_true` and `y_pred` must be of the same length.')\n    tp = fp = fn = 0\n    for (t, p) in zip(y_true, y_pred):\n        if p == 1 and t == 1:\n            tp += 1\n        elif p == 1 and t != 1:\n            fp += 1\n        elif p != 1 and t == 1:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp else 0.0\n    recall = tp / (tp + fn) if tp + fn else 0.0\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n    return round(f1, 3)\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    consumption_data : list/tuple with 10 daily consumption values\n    returns          : integer forecast for day-15, including 5 % safety margin\n    \"\"\"\n    detrended = []\n    for (idx, value) in enumerate(consumption_data):\n        day = idx + 1\n        fluct = 10 * math.sin(2 * PI * day / 10)\n        detrended.append(value - fluct)\n    n = len(detrended)\n    xs = list(range(1, n + 1))\n    sum_x = sum(xs)\n    sum_y = sum(detrended)\n    sum_x2 = sum((x * x for x in xs))\n    sum_xy = sum((x * y for (x, y) in zip(xs, detrended)))\n    denom = n * sum_x2 - sum_x ** 2\n    slope = (n * sum_xy - sum_x * sum_y) / denom\n    intercept = (sum_y - slope * sum_x) / n\n    day15_base = slope * 15 + intercept\n    day15_fluct = 10 * math.sin(2 * PI * 15 / 10)\n    day15_total = day15_base + day15_fluct\n    rounded = round(day15_total)\n    with_margin = math.ceil(rounded * 1.05)\n    return int(with_margin)\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    mae_value = np.mean(np.abs(y_true - y_pred))\n    return round(float(mae_value), 3)\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: multi_head_attention() takes 4 positional arguments but 5 were given", "solution": "import numpy as np\nimport numpy as np\ndef _softmax(x, axis=-1):\n    \"\"\"Numerically-stable softmax.\"\"\"\n    x = x - np.max(x, axis=axis, keepdims=True)\n    exp = np.exp(x)\n    return exp / np.sum(exp, axis=axis, keepdims=True)\ndef self_attention(Q, K, V):\n    \"\"\"\n    Q, K, V : (m, d)  \u2013 same length m, feature size d\n    Returns : (m, d)\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    weights = _softmax(scores, axis=-1)\n    return weights @ V\ndef _split_heads(x, n_heads):\n    \"\"\"(m, n) -> (m, n_heads, depth) where depth = n // n_heads\"\"\"\n    (m, n) = x.shape\n    assert n % n_heads == 0, 'n must be divisible by n_heads'\n    depth = n // n_heads\n    return x.reshape(m, n_heads, depth)\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Perform multi-head attention (no final linear projection for brevity).\n\n    Q, K, V : (m, n)\n    n_heads : int  \u2013 number of attention heads\n\n    Returns\n    -------\n    (m, n) \u2013 concatenated outputs from all heads\n    \"\"\"\n    Q_h = _split_heads(Q, n_heads)\n    K_h = _split_heads(K, n_heads)\n    V_h = _split_heads(V, n_heads)\n    head_outputs = [self_attention(Q_h[:, h, :], K_h[:, h, :], V_h[:, h, :]) for h in range(n_heads)]\n    return np.concatenate(head_outputs, axis=-1)\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n\n    Raises:\n    ValueError: If the two lists have different lengths, contain values\n                other than 0/1, or if the coefficient is undefined\n                because one of the variables has no variance.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length.')\n    n11 = n10 = n01 = n00 = 0\n    for (xi, yi) in zip(x, y):\n        if xi not in (0, 1) or yi not in (0, 1):\n            raise ValueError('Both input lists must contain only 0s and 1s.')\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        else:\n            n00 += 1\n    n1_ = n11 + n10\n    n0_ = n01 + n00\n    n_1 = n11 + n01\n    n_0 = n10 + n00\n    denom = (n1_ * n0_ * n_1 * n_0) ** 0.5\n    if denom == 0:\n        raise ValueError('Phi coefficient is undefined (zero variance in at least one variable).')\n    phi = (n11 * n00 - n10 * n01) / denom\n    return round(phi, 4)\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    if x >= 2.5:\n        return 1.0\n    return 0.2 * x + 0.5\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU (Exponential Linear Unit) activation function.\n\n    ELU(x) = x                              if x >= 0\n             alpha * (exp(x) - 1)           if x < 0\n\n    The result is rounded to 4 decimal places.\n\n    Args:\n        x (float): Input value.\n        alpha (float): Scaling factor applied to negative inputs (default: 1.0).\n\n    Returns:\n        float: ELU activation value rounded to 4 decimal places.\n    \"\"\"\n    elu_val = x if x >= 0 else alpha * (math.exp(x) - 1)\n    return round(elu_val, 4)\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the numerically-stable Softplus activation function.\n\n    Softplus(x) = log(1 + e**x)\n\n    Args:\n        x (float): Input value.\n\n    Returns:\n        float: Softplus value rounded to 4 decimal places.\n    \"\"\"\n    if math.isinf(x):\n        return float('inf') if x > 0 else 0.0\n    if math.isnan(x):\n        return float('nan')\n    if x > 0:\n        value = x + math.log1p(math.exp(-x))\n    else:\n        value = math.log1p(math.exp(x))\n    return round(value, 4)\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    The Softsign function is defined as:\n        f(x) = x / (1 + |x|)\n    It smoothly maps any real-valued input to the interval (-1, 1).\n\n    Args:\n        x (float): Input value.\n\n    Returns:\n        float: Softsign of the input, rounded to 4 decimal places.\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List / 1-D array of likelihood ratios \u03c1_i = \u03c0_\u03b8(a_i|s_i) / \u03c0_\u03b8_old(a_i|s_i).\n        A:    List / 1-D array of advantage estimates \ud835\udc34_i.\n        pi_theta_old: List / 1-D array of old\u2013policy probabilities \u03c0_\u03b8_old(a_i|s_i).\n        pi_theta_ref: List / 1-D array of reference\u2013policy probabilities \u03c0_ref(a_i|s_i).\n        epsilon: PPO-style clipping parameter \u03b5.\n        beta:    Coefficient on the KL(\u03c0_\u03b8 || \u03c0_ref) penalty term.\n\n    Returns:\n        GRPO objective value (float, rounded to 6 decimals).\n    \"\"\"\n    rhos = np.asarray(rhos, dtype=np.float64)\n    A = np.asarray(A, dtype=np.float64)\n    pi_theta_old = np.asarray(pi_theta_old, dtype=np.float64)\n    pi_theta_ref = np.asarray(pi_theta_ref, dtype=np.float64)\n    pi_theta = rhos * pi_theta_old\n    clipped_rhos = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon)\n    surrogate = np.minimum(rhos * A, clipped_rhos * A)\n    L_clip = np.mean(surrogate)\n    tiny = 1e-10\n    p = np.clip(pi_theta, tiny, 1.0)\n    q = np.clip(pi_theta_ref, tiny, 1.0)\n    KL = np.mean(p * (np.log(p) - np.log(q)))\n    objective = L_clip - beta * KL\n    return float(np.round(objective, 6))\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Swish(x) = x * sigmoid(x)\n    with sigmoid(x) = 1 / (1 + e^(\u2212x))\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to 4 decimal places.\n    \"\"\"\n    sigmoid_x = 1.0 / (1.0 + math.exp(-x))\n    swish_val = x * sigmoid_x\n    return round(swish_val, 4)\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value (rounded to 4 decimal places)\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        selu_val = scale * x\n    else:\n        selu_val = scale * alpha * math.expm1(x)\n    return round(selu_val, 4)\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary-classification prediction using Logistic Regression.\n\n    Args:\n        X      : Input feature matrix of shape (N, D)\n        weights: Weight vector of shape (D,)  (or anything broadcastable to (D,))\n        bias   : Scalar bias term\n\n    Returns:\n        List with N binary predictions (0 or 1)\n    \"\"\"\n    weights = np.asarray(weights).reshape(-1)\n    logits = X @ weights + bias\n    logits_clipped = np.clip(logits, -709, 709)\n    probabilities = 1.0 / (1.0 + np.exp(-logits_clipped))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient\u2013descent training algorithm for Softmax regression, optimized\n    with Cross-Entropy loss.\n\n    Parameters\n    ----------\n    X : ndarray of shape (N, D)\n        Feature matrix.\n    y : ndarray of shape (N,)\n        Integer class labels ranging from 0 \u2026 (M-1).\n    learning_rate : float\n        Step size (\u03b1) for gradient descent.\n    iterations : int\n        Number of parameter-update steps.\n\n    Returns\n    -------\n    B : list[float]\n        Learned (D+1)\u00d7M parameter matrix flattened to 1-D, rounded to 4 dp.\n        The first row corresponds to the bias term.\n    losses : list[float]\n        Cross-entropy loss values collected at each iteration, rounded to 4 dp.\n    \"\"\"\n    (N, D) = X.shape\n    Xb = np.hstack([np.ones((N, 1)), X])\n    classes = np.unique(y)\n    M = classes.size\n    Y = np.zeros((N, M))\n    Y[np.arange(N), y.astype(int)] = 1.0\n    W = np.zeros((D + 1, M))\n    losses = []\n    for _ in range(iterations):\n        logits = Xb @ W\n        logits -= logits.max(axis=1, keepdims=True)\n        exp_logits = np.exp(logits)\n        probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n        ce_loss = -np.mean(np.log(probs[np.arange(N), y.astype(int)] + 1e-15))\n        losses.append(ce_loss)\n        grad = Xb.T @ (probs - Y) / N\n        W -= learning_rate * grad\n    W_rounded = np.round(W, 4).flatten().tolist()\n    losses_rounded = np.round(losses, 4).tolist()\n    return (W_rounded, losses_rounded)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient\u2013descent training algorithm for logistic regression using\n    Binary Cross-Entropy (BCE) loss.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary target vector of shape (n_samples,).\n    learning_rate : float\n        Step size for gradient descent.\n    iterations : int\n        Number of gradient\u2013descent iterations.\n\n    Returns\n    -------\n    tuple[list[float], list[float]]\n        \u2022 List of learned coefficients (weights + bias) rounded to 4 dp.  \n        \u2022 List of BCE losses collected at every iteration rounded to 4 dp.\n    \"\"\"\n    (m, n) = X.shape\n    X_bias = np.hstack([np.ones((m, 1)), X])\n    w = np.zeros(n + 1)\n    eps = 1e-15\n    losses = []\n    for _ in range(iterations):\n        z = X_bias @ w\n        p = 1 / (1 + np.exp(-z))\n        loss = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n        losses.append(loss)\n        grad = X_bias.T @ (p - y) / m\n        w -= learning_rate * grad\n    w_rounded = np.round(w, 4).tolist()\n    losses_rounded = np.round(losses, 4).tolist()\n    return (w_rounded, losses_rounded)\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters\n    ----------\n    Q, K, V : np.ndarray\n        Query, Key, and Value tensors.  Shapes can be\n            \u2022 (seq_len, d_k)               \u2013 single sequence\n            \u2022 (batch, seq_len, d_k)        \u2013 batched sequences\n    mask : np.ndarray\n        Boolean or 0/1 mask indicating which positions can be attended to.\n        Shapes accepted:\n            \u2022 (seq_len, seq_len)\n            \u2022 (batch, seq_len, seq_len)\n        A value of 0 (False) means the position is **masked out**.\n    \n    Returns\n    -------\n    list\n        Attention output converted to a Python list.\n    \"\"\"\n    added_batch_dim = False\n    if Q.ndim == 2:\n        Q = Q[None, ...]\n        K = K[None, ...]\n        V = V[None, ...]\n        added_batch_dim = True\n    (batch_size, seq_len, d_k) = Q.shape\n    scores = np.matmul(Q, np.swapaxes(K, -2, -1)) / np.sqrt(d_k)\n    if mask.ndim == 2:\n        mask = mask[None, ...]\n    mask = mask.astype(bool)\n    scores = np.where(mask, scores, -1000000000.0)\n    scores_max = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - scores_max)\n    attn_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = np.matmul(attn_weights, V)\n    if added_batch_dim:\n        output = output[0]\n    return output.tolist()\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([1,1,0,0])", "solution": "\nfrom collections import Counter\nfrom math import log2\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n\n    The metric used is Shannon entropy (base-2) of the color distribution.\n    \u2022  Entropy is 0 when every apple has the same color.\n    \u2022  Entropy grows as the color distribution becomes more uniform across\n       a larger number of different colors.\n\n    The result is rounded to 4 decimal places, as required.\n    \"\"\"\n    if not apples:\n        return 0.0\n    n = len(apples)\n    counts = Counter(apples)\n    if len(counts) == 1:\n        return 0.0\n    entropy = 0.0\n    for c in counts.values():\n        p = c / n\n        entropy -= p * log2(p)\n    return round(entropy, 4)\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Layer-normalizes a 3-D tensor (batch, seq_len, feature_dim).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input of shape (B, L, D)\n    gamma : np.ndarray\n        Scale parameter of shape (D,) or broadcast-compatible with the last dim.\n    beta : np.ndarray\n        Shift parameter of shape (D,) or broadcast-compatible with the last dim.\n    epsilon : float, optional\n        Small constant for numerical stability.\n\n    Returns\n    -------\n    list\n        Layer-normalized tensor (same shape as X) rounded to 5 decimals and\n        converted to a Python list.\n    \"\"\"\n    X = np.asarray(X, dtype=np.float32)\n    gamma = np.asarray(gamma, dtype=np.float32)\n    beta = np.asarray(beta, dtype=np.float32)\n    mean = X.mean(axis=-1, keepdims=True)\n    var = X.var(axis=-1, keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    out = X_hat * gamma + beta\n    return np.round(out, 5).tolist()\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom collections import Counter\nimport re\nimport numpy as np\nfrom collections import Counter\ndef _tokenize(text):\n    \"\"\"\n    Very small helper: lowercase, keep only alphanumerics, split on whitespace.\n    \"\"\"\n    return re.findall('\\\\w+', text.lower())\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute a simplified METEOR score between `reference` and `candidate`.\n\n    Parameters\n    ----------\n    reference : str\n        Gold/ground-truth sentence.\n    candidate : str\n        Machine-translation output.\n    alpha, beta, gamma : float\n        Standard METEOR parameters. Defaults reproduce the common setting:\n        alpha=0.9 (recall-weighted F-mean), beta=3, gamma=0.5.\n\n    Returns\n    -------\n    float\n        METEOR score rounded to 3 decimals.\n    \"\"\"\n    ref_tokens = _tokenize(reference)\n    cand_tokens = _tokenize(candidate)\n    if not ref_tokens or not cand_tokens:\n        return 0.0\n    ref_used = [False] * len(ref_tokens)\n    aligned_ref_positions = []\n    last_pos = -1\n    for tok in cand_tokens:\n        found = -1\n        for idx in range(last_pos + 1, len(ref_tokens)):\n            if not ref_used[idx] and ref_tokens[idx] == tok:\n                found = idx\n                break\n        if found == -1:\n            for idx in range(len(ref_tokens)):\n                if not ref_used[idx] and ref_tokens[idx] == tok:\n                    found = idx\n                    break\n        if found != -1:\n            ref_used[found] = True\n            aligned_ref_positions.append(found)\n            last_pos = found\n    m = len(aligned_ref_positions)\n    if m == 0:\n        return 0.0\n    precision = m / len(cand_tokens)\n    recall = m / len(ref_tokens)\n    f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall)\n    chunks = 1\n    for i in range(1, m):\n        if aligned_ref_positions[i] != aligned_ref_positions[i - 1] + 1:\n            chunks += 1\n    frag_ratio = chunks / m\n    penalty = gamma * frag_ratio ** beta\n    score = (1 - penalty) * f_mean\n    return round(score, 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) between two events.\n\n    Parameters\n    ----------\n    joint_counts : int or float\n        Number of times the two events occurred together.\n    total_counts_x : int or float\n        Number of times event X occurred.\n    total_counts_y : int or float\n        Number of times event Y occurred.\n    total_samples : int or float\n        Total number of observations / samples.\n\n    Returns\n    -------\n    float\n        PMI rounded to 3 decimal places. If any of the required counts is zero\n        (making PMI undefined), the function returns 0.0 by convention.\n    \"\"\"\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0:\n        return 0.0\n    ratio = joint_counts * total_samples / (total_counts_x * total_counts_y)\n    pmi_value = np.log2(ratio)\n    return round(float(pmi_value), 3)\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Scale a list of integers to the range [0, 1] using Min-Max Normalization.\n\n    Parameters\n    ----------\n    x : list[int]\n        The input list of integers.\n\n    Returns\n    -------\n    list[float]\n        A list of floats, each rounded to 4 decimal places, representing\n        the normalized values.\n    \"\"\"\n    if not x:\n        return []\n    (mn, mx) = (min(x), max(x))\n    if mn == mx:\n        return [0.0 for _ in x]\n    rng = mx - mn\n    return [round((xi - mn) / rng, 4) for xi in x]\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Simple residual (ResNet-style) block.\n    \n    Parameters\n    ----------\n    x  : np.ndarray (1-D)\n        Input vector.\n    w1 : np.ndarray (2-D)\n        First weight matrix. Its shape must be (x.size, hidden_units).\n    w2 : np.ndarray (2-D)\n        Second weight matrix. Its shape must be (hidden_units, x.size).\n    \n    Returns\n    -------\n    list\n        Output vector after the residual block, rounded to 4 decimal places.\n    \"\"\"\n    relu = lambda z: np.maximum(0, z)\n    h1 = relu(x @ w1)\n    h2 = relu(h1 @ w2)\n    out = relu(h2 + x)\n    return np.round(out, 4).tolist()\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3-D NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input array of shape (height, width, channels).\n\n    Returns\n    -------\n    np.ndarray\n        1-D array of shape (channels,) where each element is the mean\n        of all spatial locations (height \u00d7 width) in the corresponding channel.\n\n    Raises\n    ------\n    ValueError\n        If `x` is not a 3-D array.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError(f'Expected a 3-D tensor of shape (H, W, C); got array with shape {x.shape}')\n    return x.mean(axis=(0, 1))\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Batch\u2013Normalization for BCHW input.\n\n    Parameters\n    ----------\n    X       : ndarray, shape (B, C, H, W)\n              Mini\u2013batch of feature-maps.\n    gamma   : ndarray, shape (C,) or broadcastable to (1,C,1,1)\n              Per\u2013channel scale factors.\n    beta    : ndarray, shape (C,) or broadcastable to (1,C,1,1)\n              Per\u2013channel shift factors.\n    epsilon : float\n              Numerical stability term.\n\n    Returns\n    -------\n    list\n        Normalized, scaled and shifted tensor rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=np.float64)\n    gamma = np.asarray(gamma, dtype=np.float64)\n    beta = np.asarray(beta, dtype=np.float64)\n    if gamma.ndim == 1:\n        gamma = gamma.reshape(1, -1, 1, 1)\n    elif gamma.ndim == 3:\n        gamma = gamma.reshape(1, *gamma.shape)\n    if beta.ndim == 1:\n        beta = beta.reshape(1, -1, 1, 1)\n    elif beta.ndim == 3:\n        beta = beta.reshape(1, *beta.shape)\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)\n    var = X.var(axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    Y = gamma * X_hat + beta\n    return np.round(Y, 4).tolist()\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the value of the derivative of a single-term polynomial c * x^n\n    at a given point x.\n\n    Parameters\n    ----------\n    c : float\n        Coefficient of the polynomial term.\n    x : float\n        Point at which the derivative is evaluated.\n    n : float\n        Exponent (power) of x in the term.\n\n    Returns\n    -------\n    float\n        The derivative value, rounded to 4 decimal places.\n    \"\"\"\n    if n == 0:\n        return 0.0\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the span of the given 2-D vectors\n    using the Gram\u2013Schmidt process.\n\n    Parameters\n    ----------\n    vectors : list[list[float]]\n        A list of 2-D column-vectors (length-2 lists or tuples).\n    tol : float, optional\n        Tolerance below which a vector is treated as the zero vector\n        (i.e. not linearly independent from the current basis).\n\n    Returns\n    -------\n    list[list[float]]\n        A list of orthonormal basis vectors rounded to 4 decimals.\n    \"\"\"\n    if not vectors:\n        return []\n    for v in vectors:\n        if len(v) != 2:\n            raise ValueError('All input vectors must be 2-dimensional.')\n    basis = []\n    for v in vectors:\n        v = np.asarray(v, dtype=float)\n        for b in basis:\n            v = v - np.dot(v, b) * b\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            basis.append(v / norm)\n        if len(basis) == 2:\n            break\n    return [np.round(b, 4).tolist() for b in basis]\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n\n    Parameters\n    ----------\n    a, b : array-like of length 3\n        Input vectors.\n\n    Returns\n    -------\n    list\n        Cross product rounded to 4 decimal places.\n    \"\"\"\n    a = np.asarray(a, dtype=float)\n    b = np.asarray(b, dtype=float)\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError('Both input vectors must contain exactly three elements.')\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations Ax = b with Cramer's Rule.\n    \n    Parameters\n    ----------\n    A : (n, n) array_like\n        Coefficient matrix.\n    b : (n,)  array_like\n        Constant terms.\n    \n    Returns\n    -------\n    list\n        Solution vector rounded to 4 decimal places,\n        or -1 if the system has no unique solution.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).flatten()\n    if A.ndim != 2 or A.shape[0] != A.shape[1] or b.size != A.shape[0]:\n        raise ValueError('A must be square and compatible with b.')\n    n = A.shape[0]\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0.0):\n        return -1\n    x = np.empty(n)\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x[i] = det_Ai / det_A\n    x_rounded = np.round(x, 4)\n    x_rounded[np.isclose(x_rounded, 0)] = 0.0\n    return x_rounded.tolist()\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Compute the Bhattacharyya distance between two discrete probability\n    distributions.\n\n    Parameters\n    ----------\n    p, q : list[float]\n        Lists of non-negative numbers representing (possibly un-normalised)\n        probability masses.\n\n    Returns\n    -------\n    float\n        Bhattacharyya distance rounded to 4 decimal places.\n        If the lists are empty, of different length, contain negatives,\n        or have zero total mass, 0.0 is returned.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p_arr = np.asarray(p, dtype=float)\n    q_arr = np.asarray(q, dtype=float)\n    if np.any(p_arr < 0) or np.any(q_arr < 0):\n        return 0.0\n    (p_sum, q_sum) = (p_arr.sum(), q_arr.sum())\n    if p_sum == 0 or q_sum == 0:\n        return 0.0\n    p_arr /= p_sum\n    q_arr /= q_sum\n    bc = np.sum(np.sqrt(p_arr * q_arr))\n    if bc == 0:\n        return float('inf')\n    distance = -np.log(bc)\n    return round(distance, 4)\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List, Union\nNumber = Union[int, float]\ndef vector_sum(a: List[Number], b: List[Number]) -> Union[List[Number], int]:\n    \"\"\"\n    Compute the element-wise sum of two vectors (lists).\n\n    Parameters\n    ----------\n    a : list[int | float]\n        First vector.\n    b : list[int | float]\n        Second vector.\n\n    Returns\n    -------\n    list[int | float]\n        Element-wise sum of `a` and `b` if they have the same length.\n    int\n        -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if not isinstance(a, list) or not isinstance(b, list) or len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Compute the REINFORCE policy-gradient estimate.\n\n    Parameters\n    ----------\n    theta     : (num_states, num_actions) 2-D numpy.array \u2013 the policy parameters.\n    episodes  : list of episodes. Each episode is a list of (state, action, reward).\n\n    Returns\n    -------\n    A (num_states, num_actions) python list containing the averaged policy-gradient,\n    rounded to 4 decimal places.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    policy = np.empty_like(theta, dtype=float)\n    for s in range(num_states):\n        logits = theta[s] - np.max(theta[s])\n        exp_logits = np.exp(logits)\n        policy[s] = exp_logits / np.sum(exp_logits)\n    grad = np.zeros_like(theta, dtype=float)\n    for ep in episodes:\n        T = len(ep)\n        returns = np.zeros(T, dtype=float)\n        G = 0.0\n        for t in reversed(range(T)):\n            (_, _, r) = ep[t]\n            G += r\n            returns[t] = G\n        for (t, (s, a, _)) in enumerate(ep):\n            G_t = returns[t]\n            grad[s] -= G_t * policy[s]\n            grad[s, a] += G_t\n    grad /= max(len(episodes), 1)\n    grad = np.round(grad, 4)\n    return grad.tolist()\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Compare the per-token (or per sample) FLOPs of\n      \u2022 a standard dense feed-forward layer (d_in \u2192 d_out) and\n      \u2022 a sparsely\u2013gated Mixture-of-Experts (MoE) layer that\n        \u2013 uses a softmax gate (linear projection d_in \u2192 n_experts)\n        \u2013 activates k_active (top-k) experts, each an identical\n          dense projection d_in \u2192 d_out\n        \u2013 linearly combines the k_active outputs.\n\n    Parameters\n    ----------\n    n_experts : int   \u2013 total number of experts in the MoE layer\n    k_active  : int   \u2013 number of experts actually executed (k in top-k)\n    d_in      : int   \u2013 input dimension\n    d_out     : int   \u2013 output dimension\n\n    Returns\n    -------\n    dict with keys\n      dense_flops     : float \u2013 FLOPs of the plain dense layer\n      moe_flops       : float \u2013 FLOPs of the MoE layer\n      savings_percent : float \u2013 100 * (1 \u2013 moe/dense)\n    All numbers are rounded to 1 decimal place.\n    \"\"\"\n    dense_flops = 2.0 * d_in * d_out\n    gate_flops = 2.0 * d_in * n_experts\n    experts_flops = k_active * 2.0 * d_in * d_out\n    combine_flops = k_active * d_out\n    moe_flops = gate_flops + experts_flops + combine_flops\n    savings_percent = 100.0 * (1.0 - moe_flops / dense_flops)\n    dense_flops = round(dense_flops, 1)\n    moe_flops = round(moe_flops, 1)\n    savings_percent = round(savings_percent, 1)\n    return {'dense_flops': dense_flops, 'moe_flops': moe_flops, 'savings_percent': savings_percent}\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the \u2018Noisy Top-K\u2019 gating from Shazeer et al., 2017.\n\n    Steps\n    -----\n    1. clean_logits  = X @ W_g\n    2. noise_std     = softplus(X @ W_noise)\n    3. noisy_logits  = clean_logits + noise_std * N\n       (only used for selecting the k experts)\n    4. For every sample pick the k indices with the largest noisy_logits.\n    5. On those k indices compute a softmax of the *clean* logits.\n       All other positions are 0.\n    6. Round the result to 4 decimal places and return a Python list.\n    \"\"\"\n    (batch, n_experts) = (X.shape[0], W_g.shape[1])\n    k = max(1, min(k, n_experts))\n    clean_logits = X @ W_g\n    noise_std = np.log1p(np.exp(X @ W_noise))\n    noisy_logits = clean_logits + noise_std * N\n    gating = np.zeros_like(clean_logits)\n    for b in range(batch):\n        topk_idx = np.argpartition(noisy_logits[b], -k)[-k:]\n        sel_logits = clean_logits[b, topk_idx]\n        sel_logits -= sel_logits.max()\n        probs = np.exp(sel_logits)\n        probs /= probs.sum()\n        gating[b, topk_idx] = probs\n    gating = np.round(gating, 4)\n    return gating.tolist()\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": false, "result": "failed: too many values to unpack (expected 2)", "solution": "import numpy as np\nimport numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Sparse Mixture-of-Experts layer with softmax gating and top-k routing.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input tensor of shape (batch_size, input_dim).\n    We : np.ndarray\n        Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg : np.ndarray\n        Gating weight matrix of shape (input_dim, n_experts).\n    n_experts : int\n        Number of experts (must match We.shape[0]).\n    top_k : int\n        Number of experts to route each token to (top-k).\n\n    Returns\n    -------\n    list\n        MoE output (rounded to 4 decimals) converted to a regular Python list.\n    \"\"\"\n    assert We.shape[0] == n_experts, 'We and n_experts mismatch'\n    assert Wg.shape[1] == n_experts, 'Wg and n_experts mismatch'\n    assert 1 <= top_k <= n_experts, 'top_k must be in [1, n_experts]'\n    (batch_size, input_dim) = x.shape\n    (_, _, output_dim) = We.shape\n    logits = x @ Wg\n    logits_max = np.max(logits, axis=1, keepdims=True)\n    exp_logits = np.exp(logits - logits_max)\n    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    top_idx = np.argsort(probs, axis=1)[:, -top_k:]\n    out = np.zeros((batch_size, output_dim))\n    for b in range(batch_size):\n        idxs = top_idx[b]\n        sel_probs = probs[b, idxs]\n        norm_sel_probs = sel_probs / np.sum(sel_probs)\n        for (coef, e_idx) in zip(norm_sel_probs, idxs):\n            expert_out = x[b] @ We[e_idx]\n            out[b] += coef * expert_out\n    return np.round(out, 4).tolist()\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Group Normalization for a 4-D tensor  (B, C, H, W).\n\n    Parameters\n    ----------\n    X          : input tensor with shape (B, C, H, W)\n    gamma, beta: scale and shift (either shape (C,)  or  (1,C,1,1)  or  (C,1,1))\n    num_groups : number of channel groups (must exactly divide C)\n    epsilon    : small constant for numerical stability\n\n    Returns\n    -------\n    Python list of the normalized, scaled and shifted tensor\n    with all elements rounded to 4 decimal places.\n    \"\"\"\n    if X.ndim != 4:\n        raise ValueError('X must be a 4-D tensor of shape (B,C,H,W)')\n    (B, C, H, W) = X.shape\n    if C % num_groups != 0:\n        raise ValueError('num_groups ({}) must divide the number of channels ({})'.format(num_groups, C))\n    G = num_groups\n    X_grouped = X.reshape(B, G, C // G, H, W)\n    mean = X_grouped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_grouped.var(axis=(2, 3, 4), keepdims=True)\n    X_norm = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    if gamma.ndim == 1:\n        gamma_b = gamma.reshape(1, C, 1, 1)\n    else:\n        gamma_b = gamma\n    if beta.ndim == 1:\n        beta_b = beta.reshape(1, C, 1, 1)\n    else:\n        beta_b = beta\n    out = X_norm * gamma_b + beta_b\n    out = np.round(out, 4)\n    return out.tolist()\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef _grad(x: float) -> float:\n    return 4 * x ** 3 - 9 * x ** 2\ndef _hess(x: float) -> float:\n    return 12 * x ** 2 - 18 * x\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Gradient\u2013descent search for the minimum of\n        f(x) = x^4 - 3x^3 + 2\n\n    Parameters\n    ----------\n    start_x       : initial guess\n    learning_rate : step size for gradient descent\n    tolerance     : |x_{k+1} - x_k| threshold for convergence\n    max_iters     : safety cap on iterations\n\n    Returns\n    -------\n    float : the x-coordinate of the minimum, rounded to 4 dp\n    \"\"\"\n    x = float(start_x)\n    for _ in range(max_iters):\n        g = _grad(x)\n        if abs(g) < tolerance:\n            if _hess(x) > 0:\n                break\n            x += np.random.uniform(-1.0, 1.0)\n            continue\n        x_new = x - learning_rate * g\n        if abs(x_new - x) < tolerance:\n            x = x_new\n            if _hess(x) > 0:\n                break\n            else:\n                x += np.random.uniform(-1.0, 1.0)\n                continue\n        x = x_new\n    return round(x, 4)\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) \u2013 a learnable,   normalization\u2013free substitute for LayerNorm.\n    \n    Parameters\n    ----------\n    x     : np.ndarray\n            Arbitrary-shaped input tensor.  \n            The last dimension is treated as the \u201cfeature\u201d dimension that would\n            normally be normalized in a Transformer.\n    alpha : float\n            Exponent that modulates how strongly the dynamic rescaling reacts\n            to the statistics of the current token.\n    gamma : float\n            Output scale (analogous to the gain in LayerNorm).\n    beta  : float\n            Pre-tanh scale (controls the slope of the linear region).\n    \n    Returns\n    -------\n    list[float]\n            DyT-transformed tensor, rounded to 4 d.p., returned as a Python list.\n    \"\"\"\n    eps = 1e-07\n    mean_abs_token = np.mean(np.abs(x), axis=-1, keepdims=True) + eps\n    y = gamma * np.tanh(beta * x / np.power(mean_abs_token, alpha))\n    y = np.round(y, 4)\n    return y.tolist()\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
