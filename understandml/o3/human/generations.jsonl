{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Compute the dot product of a matrix `a` with a vector `b`.\n    \n    Parameters\n    ----------\n    a : list[list[int | float]]\n        An n \u00d7 m matrix represented as a list of n rows, \n        each row being a list of length m.\n    b : list[int | float]\n        A vector (list) of length m.\n    \n    Returns\n    -------\n    list[int | float]\n        A list of length n, where each entry is the dot product of\n        the corresponding row of `a` with `b`.\n    int\n        Returns -1 if dimensions are incompatible.\n    \"\"\"\n    \n    # Basic structural checks\n    if not isinstance(a, list) or not isinstance(b, list):\n        return -1\n    \n    # Empty-matrix edge cases\n    if len(a) == 0:                     # No rows\n        return [] if len(b) == 0 else -1\n    \n    # All rows must have the same length\n    num_cols = len(a[0])\n    if num_cols != len(b):\n        return -1\n    \n    # Ensure every row length matches the first row\n    for row in a:\n        if len(row) != num_cols:        # Ragged matrix is invalid\n            return -1\n    \n    # Compute dot products\n    result: list[int | float] = []\n    for row in a:\n        dot_val = sum(row[i] * b[i] for i in range(num_cols))\n        result.append(dot_val)\n    \n    return result\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\ndef transpose_matrix(a: List[List[Number]]) -> List[List[Number]]:\n    \"\"\"\n    Return the transpose of the 2-D matrix `a`.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        A rectangular list-of-lists representing the matrix.\n\n    Returns\n    -------\n    list[list[int | float]]\n        The transposed matrix.\n\n    Raises\n    ------\n    ValueError\n        If `a` is not rectangular (i.e., its rows differ in length).\n\n    Examples\n    --------\n    >>> transpose_matrix([[1, 2, 3], [4, 5, 6]])\n    [[1, 4], [2, 5], [3, 6]]\n    >>> transpose_matrix([])\n    []\n    \"\"\"\n    # Empty matrix \u2192 empty transpose\n    if not a:\n        return []\n\n    # Validate that all rows are the same length\n    row_lengths = {len(row) for row in a}\n    if len(row_lengths) != 1:\n        raise ValueError(\"All rows must have the same length to form a proper matrix.\")\n\n    # Perform the transposition\n    return [list(col) for col in zip(*a)]\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape a matrix (list of lists) to the requested shape.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        Input matrix.\n    new_shape : tuple[int, int]\n        Desired shape in the form (rows, columns).\n\n    Returns\n    -------\n    list[list[int | float]]\n        The reshaped matrix as a list of lists if possible;\n        otherwise an empty list ([]).\n    \"\"\"\n\n    # Validate `new_shape`\n    if (\n        not isinstance(new_shape, tuple)\n        or len(new_shape) != 2\n        or not all(isinstance(x, int) and x > 0 for x in new_shape)\n    ):\n        return []\n\n    rows, cols = new_shape\n\n    # Flatten input safely (handles ragged rows without NumPy warnings)\n    flat = [item for row in a for item in row]\n\n    # Check if total element count matches the requested shape\n    if len(flat) != rows * cols:\n        return []\n\n    # Reshape using NumPy and convert back to plain Python lists\n    reshaped = np.array(flat).reshape(rows, cols).tolist()\n    return reshaped\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\nfrom typing import List\nimport statistics\n\ndef calculate_matrix_mean(matrix: List[List[float]], mode: str) -> List[float]:\n    \"\"\"\n    Calculate the arithmetic mean of a 2-D numeric matrix either row-wise or column-wise.\n\n    Parameters\n    ----------\n    matrix : list[list[float]]\n        A rectangular list-of-lists (all rows must have the same length for\n        column mode).\n    mode : str\n        Either \"row\" or \"column\" (case\u2013insensitive).\n\n    Returns\n    -------\n    list[float]\n        A list containing the mean of every row (for mode=\"row\") or every\n        column (for mode=\"column\").\n\n    Raises\n    ------\n    ValueError\n        If the mode is not \"row\" or \"column\", or if the matrix is jagged when\n        column means are requested.\n    \"\"\"\n    # Normalize mode string\n    mode = mode.lower()\n    if mode not in {\"row\", \"column\"}:\n        raise ValueError('mode must be either \"row\" or \"column\"')\n\n    # Empty matrix \u21d2 empty result (nothing to average)\n    if not matrix:\n        return []\n\n    if mode == \"row\":\n        # Row means: one mean per row.\n        return [statistics.mean(row) if row else float(\"nan\") for row in matrix]\n\n    # ---- Column mode below ----\n    # Make sure the matrix is rectangular.\n    num_cols = len(matrix[0])\n    if any(len(row) != num_cols for row in matrix):\n        raise ValueError(\"All rows must have the same length to compute column means\")\n\n    # Column means: one mean per column.\n    # Zip(*matrix) transposes rows to columns.\n    return [statistics.mean(col) for col in zip(*matrix)]\n\n\n# ---------------------------------------------------------------------------\n# Example usage\n# ---------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    mat = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9],\n    ]\n\n    print(\"Row means   :\", calculate_matrix_mean(mat, \"row\"))     # [2.0, 5.0, 8.0]\n    print(\"Column means:\", calculate_matrix_mean(mat, \"column\"))  # [4.0, 5.0, 6.0]\n```"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiply every element of a 2-D matrix by a scalar.\n\n    Parameters\n    ----------\n    matrix : list[list[int | float]]\n        A rectangular (rows of equal length) 2-D list representing the matrix.\n    scalar : int | float\n        The value by which each element of the matrix is to be multiplied.\n\n    Returns\n    -------\n    list[list[int | float]]\n        A new matrix where each element is the product of the corresponding\n        element in `matrix` and `scalar`.\n    \"\"\"\n    # Using a nested list-comprehension to produce a new matrix\n    return [[element * scalar for element in row] for row in matrix]\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport cmath\nfrom typing import Union, List\n\nNumber = Union[int, float, complex]\n\ndef calculate_eigenvalues(matrix: List[List[Number]]) -> List[Number]:\n    \"\"\"\n    Calculate (and return in descending order) the eigenvalues of a 2\u00d72 matrix.\n\n    Parameters\n    ----------\n    matrix : list[list[float | int]]\n        2\u00d72 matrix represented as nested lists\n        [[a, b],\n         [c, d]]\n\n    Returns\n    -------\n    list[Number]\n        Eigenvalues \u03bb\u2081, \u03bb\u2082 sorted from highest to lowest (by real part).\n        They can be real or complex numbers.\n    \"\"\"\n    # 1. Basic validation\n    if len(matrix) != 2 or any(len(row) != 2 for row in matrix):\n        raise ValueError(\"Input must be a 2\u00d72 matrix (list of two lists, each of length two).\")\n\n    a, b = matrix[0]\n    c, d = matrix[1]\n\n    # 2. Characteristic polynomial \u03bb\u00b2 \u2212 (a+d)\u03bb + (ad \u2212 bc) = 0\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n\n    # 3. Two roots of the quadratic (handles real/complex via cmath)\n    sqrt_disc = cmath.sqrt(discriminant)\n    \u03bb1 = (trace + sqrt_disc) / 2\n    \u03bb2 = (trace - sqrt_disc) / 2\n\n    # 4. Sort eigenvalues (descending by real part; ties broken by imaginary part)\n    eigenvalues = sorted([\u03bb1, \u03bb2],\n                         key=lambda x: (x.real, x.imag),\n                         reverse=True)\n\n    return eigenvalues\n```\n\nExample usage:\n```python\n# Matrix: [[4, 2],\n#          [1, 3]]\nvals = calculate_eigenvalues([[4, 2], [1, 3]])\nprint(vals)           # [5.0, 2.0]\n```\n\nThe function works for matrices with real or complex eigenvalues; when the discriminant is negative, `cmath.sqrt` ensures the eigenvalues are complex numbers."}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int | float]],\n                     T: list[list[int | float]],\n                     S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Perform the matrix transformation  T^{-1} \u00b7 A \u00b7 S\n    --------------------------------------------------\n    Parameters\n    ----------\n    A : 2-D list\n        Base matrix to be transformed.\n    T : 2-D list (square)\n        Row-basis change matrix. Must be invertible and its order\n        must equal the number of rows of A.\n    S : 2-D list (square)\n        Column-basis change matrix. Must be invertible and its order\n        must equal the number of columns of A.\n\n    Returns\n    -------\n    2-D python list (numbers rounded to 4 decimals) or -1 if\n    * matrices are not conformable, or\n    * T or S is not invertible.\n    \"\"\"\n    try:\n        # Convert to numpy float arrays\n        A_np = np.asarray(A, dtype=float)\n        T_np = np.asarray(T, dtype=float)\n        S_np = np.asarray(S, dtype=float)\n\n        # Basic shape checks\n        if T_np.ndim != 2 or S_np.ndim != 2 or A_np.ndim != 2:\n            return -1\n        nT, mT = T_np.shape\n        nS, mS = S_np.shape\n        nA, mA = A_np.shape\n\n        # T and S must be square and conformable\n        if nT != mT or nS != mS:\n            return -1\n        if nT != nA or nS != mA:\n            return -1\n\n        # Invertibility check (use a small tolerance)\n        if abs(np.linalg.det(T_np)) < 1e-12 or abs(np.linalg.det(S_np)) < 1e-12:\n            return -1\n\n        # Compute the transformation\n        T_inv = np.linalg.inv(T_np)\n        transformed = T_inv @ A_np @ S_np\n\n        # Round to 4 decimals\n        transformed = np.round(transformed, 4)\n\n        return transformed.tolist()\n\n    except Exception:\n        # Catches singular matrix inversion, type errors, etc.\n        return -1\n```"}
{"task_id": 8, "completion_id": 0, "completion": "```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    \"\"\"\n    Return the inverse of a 2\u00d72 matrix.\n    \n    Args:\n        matrix (list[list[float]]): A 2\u00d72 matrix in row-major order\n                                    [[a, b],\n                                     [c, d]]\n    \n    Returns:\n        list[list[float]] | None: The inverse matrix if it exists,\n                                  otherwise None when the matrix\n                                  is singular (non-invertible).\n    \"\"\"\n    # Basic shape validation (raises for clearly invalid input)\n    if (\n        not isinstance(matrix, list)\n        or len(matrix) != 2\n        or any(not isinstance(row, list) or len(row) != 2 for row in matrix)\n    ):\n        raise ValueError(\"Input must be a 2\u00d72 matrix provided as [[a, b], [c, d]].\")\n    \n    a, b = matrix[0]\n    c, d = matrix[1]\n\n    # Determinant\n    det = a * d - b * c\n\n    # If determinant is zero (or extremely close), matrix is singular\n    if abs(det) < 1e-12:  # tolerance helps with floating-point noise\n        return None\n    \n    inv_det = 1.0 / det\n    return [\n        [ d * inv_det, -b * inv_det],\n        [-c * inv_det,  a * inv_det],\n    ]\n```"}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int | float]],\n              b: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Multiply two matrices a and b using classic row-by-column multiplication.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        Left matrix  (m \u00d7 n)\n    b : list[list[int | float]]\n        Right matrix (n \u00d7 p)\n\n    Returns\n    -------\n    list[list[int | float]]\n        Resulting matrix (m \u00d7 p) if the inner dimensions match.\n    -1\n        Returned when matrices are empty, jagged, or dimensions are\n        incompatible for multiplication.\n    \"\"\"\n    # --- Basic structural checks ------------------------------------------------\n    # Non-empty?\n    if not a or not b:\n        return -1\n\n    # Rectangular?  (All rows must have the same length)\n    a_row_len = len(a[0])\n    if any(len(row) != a_row_len for row in a):\n        return -1\n\n    b_row_len = len(b[0])\n    if any(len(row) != b_row_len for row in b):\n        return -1\n\n    # Dimension agreement: (m\u00d7n) \u00b7 (n\u00d7p) is defined only if n == n\n    if a_row_len != len(b):\n        return -1\n\n    # --- Core multiplication ----------------------------------------------------\n    result: list[list[int | float]] = []\n    for row_a in a:                               # iterate over each row in A\n        new_row = []\n        for col_idx in range(b_row_len):          # iterate over each column in B\n            # compute dot product of row_a and the current column of B\n            val = sum(row_a[k] * b[k][col_idx] for k in range(a_row_len))\n            new_row.append(val)\n        result.append(new_row)\n\n    return result\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\nfrom typing import List\nimport math\n\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the (sample) covariance matrix for a list of feature-vectors.\n\n    Parameters\n    ----------\n    vectors : list[list[float]]\n        Each inner list is a feature (all observations for that feature).\n        All inner lists must be the same length (same number of observations).\n\n    Returns\n    -------\n    list[list[float]]\n        A square matrix (as a list of lists) where entry (i, j)\n        is the sample covariance between feature i and feature j.\n    \"\"\"\n    if not vectors:\n        raise ValueError(\"Input list 'vectors' is empty.\")\n    \n    n_features = len(vectors)\n    n_observations = len(vectors[0])\n\n    # Sanity checks\n    for v in vectors:\n        if len(v) != n_observations:\n            raise ValueError(\"All feature vectors must have the same number \"\n                             \"of observations.\")\n        if n_observations < 2:\n            raise ValueError(\"At least two observations are required to compute covariance.\")\n\n    # Pre-compute means for each feature\n    means = [sum(v) / n_observations for v in vectors]\n\n    # Allocate an empty matrix\n    cov_matrix = [[0.0 for _ in range(n_features)] for _ in range(n_features)]\n\n    # Fill upper triangle (including diagonal); mirror to lower triangle\n    for i in range(n_features):\n        for j in range(i, n_features):\n            cov_ij = 0.0\n            for k in range(n_observations):\n                cov_ij += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            cov_ij /= (n_observations - 1)  # sample covariance\n            cov_matrix[i][j] = cov_ij\n            cov_matrix[j][i] = cov_ij  # symmetry\n\n    return cov_matrix\n```\n\nExample usage and test cases\n```python\ndef _almost_equal(a, b, tol=1e-9):\n    return math.isclose(a, b, rel_tol=0.0, abs_tol=tol)\n\n# 1) Single feature \u2192 matrix is 1\u00d71 containing the sample variance\nv1 = [[2, 4, 6, 8]]                      # mean = 5\nexpected_var = 20 / 3                    # 6.666666...\ncov1 = calculate_covariance_matrix(v1)\nassert len(cov1) == 1 and _almost_equal(cov1[0][0], expected_var)\n\n# 2) Two features \u2013 hand-computed reference\nv2 = [[1, 2, 3],\n      [1, 5, 7]]\n# Hand calculations (see explanation in the write-up):\n#   var(x) = 1\n#   var(y) = 28/3  \u2248 9.333333\n#   cov(x, y) = 3\nexpected2 = [[1.0, 3.0],\n             [3.0, 28/3]]\ncov2 = calculate_covariance_matrix(v2)\nfor i in range(2):\n    for j in range(2):\n        assert _almost_equal(cov2[i][j], expected2[i][j])\n\n# 3) Compare against NumPy for a larger random example\nimport random\nimport numpy as np\nrandom.seed(42)\n\n# Generate 4 features, 10 observations each\nvectors_np = [[random.uniform(-5, 5) for _ in range(10)] for _ in range(4)]\ncov_manual = calculate_covariance_matrix(vectors_np)\ncov_numpy  = np.cov(np.array(vectors_np), bias=False).tolist()  # same definition\n\nfor i in range(4):\n    for j in range(4):\n        assert _almost_equal(cov_manual[i][j], cov_numpy[i][j])\n\nprint(\"All tests passed successfully.\")\n```\n\nRunning the snippet should finish silently (or print \u201cAll tests passed successfully.\u201d) confirming the implementation works."}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve Ax = b using the Jacobi iterative method.\n\n    Parameters\n    ----------\n    A : np.ndarray\n        Square coefficient matrix.\n    b : np.ndarray\n        Right-hand-side vector (or 1-D array).\n    n : int\n        Number of Jacobi iterations to perform.\n\n    Returns\n    -------\n    list\n        Approximate solution vector (rounded to 4 decimals) as a Python list.\n    \"\"\"\n    # Convert inputs to float arrays\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).flatten()\n\n    # Basic sanity checks\n    rows, cols = A.shape\n    if rows != cols:\n        raise ValueError(\"Matrix A must be square.\")\n    if b.size != rows:\n        raise ValueError(\"Vector b must have the same length as A's dimension.\")\n    if np.any(np.isclose(np.diag(A), 0.0)):\n        raise ValueError(\"Zero detected on A's diagonal; Jacobi method cannot proceed.\")\n\n    # Pre-compute diagonal and remainder (L + U)\n    D = np.diag(A)\n    R = A - np.diagflat(D)\n\n    # Initial guess (zeros)\n    x = np.zeros_like(b)\n\n    # Jacobi iterations\n    for _ in range(n):\n        x = (b - R @ x) / D\n        x = np.round(x, 4)          # round every intermediate solution\n\n    # Return as Python list\n    return x.tolist()\n```"}
{"task_id": 12, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the singular values of a 2\u00d72 matrix using a single-sweep Jacobi\n    (a.k.a. Jacobi \u201cJacobian\u201d) rotation on A\u1d40A.  The algorithm is exact for a\n    2\u00d72, yet the flow mirrors what would be done for larger matrices with a\n    full Jacobi SVD.\n\n    Parameters\n    ----------\n    A : np.ndarray\n        A real 2\u00d72 matrix.\n\n    Returns\n    -------\n    tuple\n        (\u03c3\u2081, \u03c3\u2082) \u2014 the singular values sorted in descending order and rounded\n        to 4 decimal places.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError(\"Input must be a 2\u00d72 matrix.\")\n\n    # 1. Form the symmetric matrix B = A\u1d40A\n    B = A.T @ A\n    b00, b01, b11 = B[0, 0], B[0, 1], B[1, 1]\n\n    # 2. One Jacobi rotation that annihilates the off-diagonal term of B\n    if abs(b01) > 1e-12:\n        tau = (b11 - b00) / (2.0 * b01)\n        t   = np.sign(tau) / (abs(tau) + np.sqrt(1.0 + tau * tau))\n        c   = 1.0 / np.sqrt(1.0 + t * t)\n        s   = c * t\n\n        # Rotation matrix V\n        V = np.array([[c, -s],\n                      [s,  c]])\n\n        # 3. Rotate B to (ideally) diagonal form: \u03a3\u00b2 = V\u1d40 B V\n        Sigma2 = V.T @ B @ V\n    else:\n        # Off-diagonal already ~0 \u21d2 V = I\n        Sigma2 = B\n\n    # Diagonal elements are the squared singular values\n    sigma1, sigma2 = np.sqrt(np.diag(Sigma2))\n\n    # 4. Sort in descending order\n    if sigma1 < sigma2:\n        sigma1, sigma2 = sigma2, sigma1\n\n    # 5. Round to 4 decimal places as requested\n    sigma1 = float(np.round(sigma1, 4))\n    sigma2 = float(np.round(sigma2, 4))\n\n    return (sigma1, sigma2)\n```"}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Compute the determinant of a 4\u00d74 matrix via Laplace\u2019s expansion.\n\n    Parameters\n    ----------\n    matrix : list[list[int | float]]\n        A 4\u00d74 matrix represented as a list of four lists, each of length four.\n\n    Returns\n    -------\n    float\n        Determinant of the given matrix.\n\n    Raises\n    ------\n    ValueError\n        If the supplied matrix is not 4\u00d74.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # Basic validation\n    # ------------------------------------------------------------------\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"Input must be a 4\u00d74 matrix (list of four lists, each of length four).\")\n\n    # ------------------------------------------------------------------\n    # Recursive determinant for an arbitrary n\u00d7n matrix\n    # ------------------------------------------------------------------\n    def det(mat: list[list[int | float]]) -> float:\n        n = len(mat)\n\n        # Base cases ----------------------------------------------------\n        if n == 1:\n            return mat[0][0]\n        if n == 2:\n            return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n\n        # Recursive Laplace expansion along the first row ---------------\n        total = 0.0\n        for col in range(n):\n            # Sign factor: (+, \u2212, +, \u2212, ...)\n            sign = -1 if col % 2 else 1\n\n            # Build the minor matrix by skipping row 0 and column `col`\n            minor = [\n                [mat[r][c] for c in range(n) if c != col]\n                for r in range(1, n)  # skip the first row\n            ]\n\n            # Recursive call\n            total += sign * mat[0][col] * det(minor)\n        return total\n\n    # ------------------------------------------------------------------\n    # Compute and return the determinant of the 4\u00d74\n    # ------------------------------------------------------------------\n    return float(det(matrix))\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Fits a linear regression model using the normal equation.\n\n    Parameters\n    ----------\n    X : list[list[float]]\n        Feature matrix (each inner list is one sample\u2019s features).\n    y : list[float]\n        Target vector.\n\n    Returns\n    -------\n    list[float]\n        The regression coefficients, including the intercept as the\n        first element.  All coefficients are rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    X_mat = np.asarray(X, dtype=float)\n    y_vec = np.asarray(y, dtype=float).reshape(-1, 1)\n\n    # Add intercept term (column of ones)\n    ones = np.ones((X_mat.shape[0], 1), dtype=float)\n    X_aug = np.hstack((ones, X_mat))\n\n    # Normal equation with pseudo-inverse for numerical stability\n    theta = np.linalg.pinv(X_aug.T @ X_aug) @ X_aug.T @ y_vec\n\n    # Flatten to 1-D and round to 4 decimal places\n    theta = np.round(theta.flatten(), 4)\n\n    # Return as a regular Python list\n    return theta.tolist()\n```"}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(\n        X: np.ndarray, \n        y: np.ndarray, \n        alpha: float, \n        iterations: int\n    ) -> np.ndarray:\n    \"\"\"\n    Perform linear regression with gradient descent.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with a leading column of ones for the intercept term.\n        Shape (m, n) where m is the number of samples.\n    y : np.ndarray\n        Target values. Shape (m,) or (m, 1).\n    alpha : float\n        Learning-rate.\n    iterations : int\n        Number of gradient-descent steps.\n\n    Returns\n    -------\n    list\n        Model coefficients (including intercept) rounded to 4 decimals.\n    \"\"\"\n    # Ensure y is a column vector of shape (m, 1)\n    y = y.reshape(-1, 1)\n    \n    m, n = X.shape\n    theta = np.zeros((n, 1))          # initialise parameters\n    \n    for _ in range(iterations):\n        predictions = X @ theta       # (m, 1)\n        errors = predictions - y      # (m, 1)\n        gradient = (X.T @ errors) / m # (n, 1)\n        theta -= alpha * gradient     # update step\n    \n    # Round to 4 decimal places (-0.0 allowed) and return as Python list\n    theta = np.round(theta, 4).reshape(-1).tolist()\n    return theta\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform column-wise feature scaling on a 2-D NumPy array.\n    \n    Returns\n    -------\n    standardized_list : list[list[float]]\n        Z-score scaled features: (x - \u03bc) / \u03c3, rounded to 4 decimals.\n    minmax_list : list[list[float]]\n        Min-max scaled features: (x - min) / (max - min), rounded to 4 decimals.\n    \"\"\"\n    if data.ndim != 2:\n        raise ValueError(\"Input array must be 2-D (samples \u00d7 features).\")\n    \n    # ---------- Standardization ----------\n    means = data.mean(axis=0)\n    stds  = data.std(axis=0, ddof=0)\n    stds_safe = np.where(stds == 0, 1, stds)          # avoid division by zero\n    standardized = (data - means) / stds_safe\n    standardized[:, stds == 0] = 0                    # constant feature \u2192 all zeros\n    standardized = np.round(standardized, 4)\n    \n    # ---------- Min-Max Normalization ----------\n    mins   = data.min(axis=0)\n    maxs   = data.max(axis=0)\n    ranges = maxs - mins\n    ranges_safe = np.where(ranges == 0, 1, ranges)    # avoid division by zero\n    minmax = (data - mins) / ranges_safe\n    minmax[:, ranges == 0] = 0                        # constant feature \u2192 all zeros\n    minmax = np.round(minmax, 4)\n    \n    # convert to Python lists\n    return standardized.tolist(), minmax.tolist()\n```"}
{"task_id": 17, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_means_clustering(\n        points: list[tuple[float, float]],\n        k: int,\n        initial_centroids: list[tuple[float, float]],\n        max_iterations: int\n    ) -> list[tuple[float, float]]:\n    \"\"\"\n    Simple, fully-vectorised k-Means clustering.\n\n    Parameters\n    ----------\n    points : list[tuple[float, float]]\n        Samples to cluster (works for any dimensionality, not only 2-D).\n    k : int\n        Number of clusters.\n    initial_centroids : list[tuple[float, float]]\n        Starting positions of the centroids.\n    max_iterations : int\n        Maximum number of updates before the algorithm stops.\n\n    Returns\n    -------\n    list[tuple[float, float]]\n        Final centroid coordinates, rounded to 4 decimal places.\n    \"\"\"\n    # --- Preparation --------------------------------------------------------\n    pts = np.asarray(points, dtype=float)                  # (n, d)\n    centroids = np.asarray(initial_centroids, dtype=float) # (k, d)\n\n    if centroids.shape[0] != k:\n        raise ValueError(\"Length of initial_centroids must equal k.\")\n    if pts.ndim != 2 or centroids.ndim != 2:\n        raise ValueError(\"Points and centroids must be 2-D arrays.\")\n\n    # --- Main loop ----------------------------------------------------------\n    for _ in range(max_iterations):\n        # 1. Assignment step \u2013 label each point with the nearest centroid\n        #    Broadcasting: pts[:, None, :] -> (n,1,d); centroids[None,:,:] -> (1,k,d)\n        distances = np.linalg.norm(pts[:, None, :] - centroids[None, :, :], axis=2)  # (n, k)\n        labels = np.argmin(distances, axis=1)                                        # (n,)\n\n        # 2. Update step \u2013 recompute each centroid\n        new_centroids = np.empty_like(centroids)\n        for idx in range(k):\n            cluster_pts = pts[labels == idx]\n            if len(cluster_pts):                     # non-empty cluster\n                new_centroids[idx] = cluster_pts.mean(axis=0)\n            else:                                    # empty cluster \u2192 keep previous position\n                new_centroids[idx] = centroids[idx]\n\n        # 3. Convergence check\n        if np.allclose(new_centroids, centroids):\n            centroids = new_centroids\n            break\n        centroids = new_centroids\n\n    # --- Rounding & return --------------------------------------------------\n    return [tuple(np.round(c, 4)) for c in centroids]\n```\n\nHow it works (one-liner):\n\u2022 Repeatedly label every point with the nearest centroid,  \n\u2022 move each centroid to the mean of the points that chose it,  \n\u2022 stop when nothing moves (or `max_iterations` is reached),  \n\u2022 finally round each coordinate to 4 decimal places."}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray,\n                            y: np.ndarray,\n                            k: int = 5,\n                            shuffle: bool = True,\n                            random_seed: int | None = None):\n    \"\"\"\n    Generate train / test indices for K-Fold cross-validation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, \u2026).\n    y : np.ndarray\n        Target vector/array of shape (n_samples, \u2026).  Only the first\n        dimension is used to validate consistency with `X`.\n    k : int, default=5\n        Number of folds.\n    shuffle : bool, default=True\n        Whether to shuffle samples before splitting.\n    random_seed : int or None, default=None\n        Seed for the RNG used when `shuffle=True`.\n\n    Returns\n    -------\n    folds : list[tuple[np.ndarray, np.ndarray]]\n        List of length `k`.  Each element is a tuple\n        `(train_indices, test_indices)` where both are 1-D `np.ndarray`\n        objects containing the integer indices of samples that belong\n        to the training and test set, respectively, for that fold.\n\n    Raises\n    ------\n    ValueError\n        If `k` is less than 2 or greater than the number of samples.\n    \"\"\"\n    # ------------- sanity checks -------------\n    n_samples = len(X)\n    if n_samples != len(y):\n        raise ValueError(\"`X` and `y` must have the same number of samples.\")\n    if k < 2:\n        raise ValueError(\"`k` must be at least 2.\")\n    if k > n_samples:\n        raise ValueError(\"`k` cannot be greater than the number of samples.\")\n\n    # ------------- create an index array -------------\n    if shuffle:\n        rng = np.random.default_rng(random_seed)\n        indices = rng.permutation(n_samples)\n    else:\n        indices = np.arange(n_samples)\n\n    # ------------- determine fold sizes -------------\n    # Each fold gets at least n_samples // k samples.\n    # The first (n_samples % k) folds get one extra sample to distribute the remainder.\n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[: n_samples % k] += 1\n\n    # ------------- generate the splits -------------\n    folds = []\n    current = 0\n    for fold_size in fold_sizes:\n        start, stop = current, current + fold_size\n        test_idx = indices[start:stop]\n        train_idx = np.concatenate((indices[:start], indices[stop:]))\n        folds.append((train_idx, test_idx))\n        current = stop\n\n    return folds\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis on a dataset.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array with shape (n_samples, n_features).\n    k : int\n        Number of principal components to return (k <= n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        A list containing the first k principal components.\n        Each inner list is an eigenvector (length = n_features),\n        rounded to 4 decimal places.\n    \"\"\"\n    # 1. Standardize (zero mean, unit variance)\n    #    axis=0 \u2192 work column-wise (feature-wise).\n    X = data.astype(float)\n    mean = X.mean(axis=0)\n    std  = X.std(axis=0, ddof=1)          # sample std (ddof=1)\n    X_std = (X - mean) / std\n\n    # 2. Covariance matrix (features \u00d7 features)\n    #    Using sample covariance, so divide by (n_samples \u2212 1)\n    cov_mat = np.cov(X_std, rowvar=False)  # shape (n_features, n_features)\n\n    # 3. Eigen decomposition (symmetric matrix \u21d2 eigh is efficient)\n    eig_vals, eig_vecs = np.linalg.eigh(cov_mat)\n\n    # 4. Sort eigenvalues (and corresponding eigenvectors) in descending order\n    sorted_idx = np.argsort(eig_vals)[::-1]\n    eig_vals   = eig_vals[sorted_idx]\n    eig_vecs   = eig_vecs[:, sorted_idx]\n\n    # 5. Select first k eigenvectors (principal components)\n    principal_components = eig_vecs[:, :k].T   # shape (k, n_features)\n\n    # 6. Round to 4 decimals and convert to Python lists\n    principal_components = np.round(principal_components, 4).tolist()\n    return principal_components\n```"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom copy import deepcopy\nfrom typing import Any, Dict, List\n\n\ndef entropy(examples: List[dict], target_attr: str) -> float:\n    \"\"\"\n    Shannon entropy of the class label in `examples`.\n    \"\"\"\n    total = len(examples)\n    if not total:\n        return 0.0\n\n    counts = Counter(ex[target_attr] for ex in examples)\n    return -sum((n / total) * math.log2(n / total) for n in counts.values())\n\n\ndef information_gain(examples: List[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    Information gain obtained by splitting `examples` on `attr`.\n    \"\"\"\n    total = len(examples)\n    if not total:\n        return 0.0\n\n    # Partition the data by every value of `attr`\n    partitions: Dict[Any, List[dict]] = {}\n    for ex in examples:\n        key = ex[attr]\n        partitions.setdefault(key, []).append(ex)\n\n    # Weighted average entropy after split\n    weighted_entropy = sum(\n        (len(part) / total) * entropy(part, target_attr) for part in partitions.values()\n    )\n\n    return entropy(examples, target_attr) - weighted_entropy\n\n\ndef majority_class(examples: List[dict], target_attr: str) -> Any:\n    \"\"\"\n    Most common class label in `examples`.\n    \"\"\"\n    if not examples:\n        return None\n    return Counter(ex[target_attr] for ex in examples).most_common(1)[0][0]\n\n\ndef learn_decision_tree(\n    examples: List[dict], attributes: List[str], target_attr: str\n) -> Dict[str, Any]:\n    \"\"\"\n    Learn a decision tree (ID3 style) using entropy / information gain.\n\n    Parameters\n    ----------\n    examples : list of dict\n        Each element is a sample represented as {attr_name: value, ...}.\n    attributes : list of str\n        The candidate attributes we may still split on (target excluded).\n    target_attr : str\n        Name of the class label attribute.\n\n    Returns\n    -------\n    tree : dict\n        A nested dictionary encoding the learned tree.\n        Leaf nodes are represented directly by the class label\n        (e.g., 'yes' or 0 or 'setosa').\n        Internal nodes have the form:\n            {attribute_name: {value1: subtree1, value2: subtree2, ...}}\n    \"\"\"\n\n    # ------------- Base cases ----------------------------------------------\n    # Empty example set\n    if not examples:\n        return None  # caller should replace with its own majority if necessary\n\n    # All examples have the same class \u2192 return that class\n    first_label = examples[0][target_attr]\n    if all(ex[target_attr] == first_label for ex in examples):\n        return first_label\n\n    # No more attributes left to test \u2192 return majority class\n    if not attributes:\n        return majority_class(examples, target_attr)\n\n    # ------------- Choose the best attribute -------------------------------\n    gains = [\n        (information_gain(examples, attr, target_attr), attr) for attr in attributes\n    ]\n    _, best_attr = max(gains, key=lambda t: t[0])\n\n    tree: Dict[str, Any] = {best_attr: {}}\n\n    # ------------- Recurse for every value of best_attr --------------------\n    # Each branch corresponds to examples having best_attr == value\n    values = {ex[best_attr] for ex in examples}\n    for v in values:\n        subset = [ex for ex in examples if ex[best_attr] == v]\n\n        # Remove chosen attribute for further recursive calls\n        remaining_attrs = [a for a in attributes if a != best_attr]\n\n        subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n\n        # If the subtree is None (e.g., subset was empty), use majority class\n        if subtree is None:\n            subtree = majority_class(examples, target_attr)\n\n        tree[best_attr][v] = subtree\n\n    return tree\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray,\n                       labels: np.ndarray,\n                       kernel='linear',\n                       lambda_val=0.01,\n                       iterations=100,\n                       sigma=1.0):\n    \"\"\"\n    Deterministic Pegasos for kernel\u2010SVMs (binary {-1 , +1} labels).\n\n    Returns\n    -------\n    alpha : list\n        Dual coefficients (shape = n_samples) rounded to 4 decimals.\n    bias  : float\n        Intercept term rounded to 4 decimals.\n    \"\"\"\n\n    # -------------------- helpers --------------------\n    def linear_kernel(X):\n        return X @ X.T                                   #  K_ij = x_i\u22c5x_j\n\n    def rbf_kernel(X):\n        sq = np.sum(X * X, 1)[:, None]\n        # ||x_i - x_j||\u00b2  =  ||x_i||\u00b2 + ||x_j||\u00b2 \u2013 2 x_i\u22c5x_j\n        dist2 = sq + sq.T - 2 * (X @ X.T)\n        return np.exp(-dist2 / (2.0 * sigma ** 2))\n\n    # -------------------- init --------------------\n    X = data.astype(float)\n    y = labels.astype(float).ravel()\n    n = X.shape[0]\n\n    if kernel == 'linear':\n        K = linear_kernel(X)\n    elif kernel == 'rbf':\n        K = rbf_kernel(X)\n    else:\n        raise ValueError(\"kernel must be 'linear' or 'rbf'\")\n\n    beta = np.zeros(n)                # signed coefficients  (\u03b2_i = \u03b1_i y_i)\n\n    # -------------------- main loop --------------------\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)          # learning-rate 1/(\u03bb t)\n\n        # regularisation (shrinkage) step\n        beta *= (1.0 - eta * lambda_val)\n\n        # margin violations   y_i \u00b7 f_i  <  1\n        f = K @ beta                          # f_i = \u03a3_j \u03b2_j K(x_j , x_i)\n        viol = y * f < 1\n        idx = np.where(viol)[0]\n\n        if idx.size:                          # sub-gradient step\n            beta[idx] += (eta / idx.size) * y[idx]\n\n    # -------------------- \u03b1  &  bias --------------------\n    alpha = beta * y                          # \u03b1_i  =  \u03b2_i y_i\n    C = 1.0 / (lambda_val * n)\n\n    f_final = K @ beta\n    sv_mask = (alpha > 1e-6) & (alpha < C - 1e-6)\n    if np.any(sv_mask):\n        bias = np.mean(y[sv_mask] - f_final[sv_mask])\n    else:                                     # fall-back if no proper SVs found\n        bias = np.mean(y - f_final)\n\n    # -------------------- rounding / output --------------------\n    alpha = np.round(alpha, 4).tolist()\n    bias  = round(float(bias), 4)\n    return alpha, bias\n```"}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation for the input `z`.\n\n    Parameters\n    ----------\n    z : float\n        Input value.\n\n    Returns\n    -------\n    float\n        Sigmoid output rounded to four decimal places.\n    \"\"\"\n    s = 1.0 / (1.0 + math.exp(-z))\n    return round(s, 4)\n```\n\nExample:\n```python\nprint(sigmoid(0))     # 0.5\nprint(sigmoid(2.0))   # 0.8808\nprint(sigmoid(-3.2))  # 0.0392\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax activation values for a list of scores.\n\n    Parameters\n    ----------\n    scores : list[float]\n        Raw scores (logits).\n\n    Returns\n    -------\n    list[float]\n        Softmax probabilities, each rounded to four decimal places.\n    \"\"\"\n    # Handle empty input gracefully\n    if not scores:\n        return []\n\n    # Numerical stability: shift values by the max score\n    max_score = max(scores)\n    exp_shifted = [math.exp(s - max_score) for s in scores]\n\n    total = sum(exp_shifted)\n    softmax_vals = [round(val / total, 4) for val in exp_shifted]\n\n    return softmax_vals\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nfrom typing import List, Tuple\nimport numpy as np\n\ndef single_neuron_model(features: List[List[float]],\n                        labels:   List[int],\n                        weights:  List[float],\n                        bias:     float) -> Tuple[List[float], float]:\n    \"\"\"\n    Simulates a single\u2010neuron binary classifier with a sigmoid activation.\n\n    Parameters\n    ----------\n    features : list[list[float]]\n        2-D list where each inner list contains the feature values of one sample.\n    labels   : list[int]\n        Ground-truth binary labels (0/1) for every sample.\n    weights  : list[float]\n        Weight assigned to every feature (must match feature dimension).\n    bias     : float\n        Scalar bias term for the neuron.\n\n    Returns\n    -------\n    Tuple[List[float], float]\n        \u2022 List with the predicted probabilities (rounded to 4 decimals).  \n        \u2022 Mean-squared error between predictions & labels (rounded to 4 decimals).\n    \"\"\"\n\n    # --- Convert inputs to numpy for vectorised computation ------------------\n    X       = np.asarray(features,  dtype=float)          # (n_samples, n_features)\n    w       = np.asarray(weights,  dtype=float)           # (n_features,)\n    y_true  = np.asarray(labels,   dtype=float)           # (n_samples,)\n\n    # --- Forward pass --------------------------------------------------------\n    z        = X.dot(w) + bias                            # Linear combination\n    y_pred   = 1.0 / (1.0 + np.exp(-z))                   # Sigmoid activation\n\n    # --- Mean-squared error --------------------------------------------------\n    mse      = np.mean((y_pred - y_true) ** 2)\n\n    # --- Rounding as required ------------------------------------------------\n    y_pred   = np.round(y_pred, 4).tolist()               # Convert back to Python list\n    mse      = float(np.round(mse, 4))                    # Scalar\n\n    return y_pred, mse\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray,\n                 labels:   np.ndarray,\n                 initial_weights: np.ndarray,\n                 initial_bias:    float,\n                 learning_rate:   float,\n                 epochs:          int\n                ) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Train a single sigmoid-activated neuron with mean\u2013squared-error loss.\n\n    Parameters\n    ----------\n    features        : 2-D array of shape (n_samples, n_features)\n    labels          : 1-D array of shape (n_samples,) containing 0/1 targets\n    initial_weights : 1-D array of shape (n_features,)\n    initial_bias    : scalar bias term\n    learning_rate   : gradient\u2013descent step size\n    epochs          : number of passes over the data\n\n    Returns\n    -------\n    weights : numpy.ndarray  (rounded to 4 decimals)\n    bias    : float          (rounded to 4 decimals)\n    losses  : list[float]    (epoch-wise MSE, each rounded to 4 decimals)\n    \"\"\"\n\n    # Ensure we are working on copies so original inputs aren\u2019t modified\n    X  = np.asarray(features, dtype=float)\n    y  = np.asarray(labels  , dtype=float).reshape(-1)\n\n    w  = np.asarray(initial_weights, dtype=float).reshape(-1).copy()\n    b  = float(initial_bias)\n\n    n_samples = X.shape[0]\n\n    # Sigmoid helper\n    def _sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    losses = []\n\n    for _ in range(epochs):\n        # Forward pass\n        z       = X @ w + b               # shape: (n_samples,)\n        y_pred  = _sigmoid(z)             # sigmoid activation\n        error   = y_pred - y              # prediction error\n\n        # Mean-squared error\n        mse = np.mean(error ** 2)\n        losses.append(mse)\n\n        # Backward pass (gradients)\n        # dL/dy_pred = 2(error)/n_samples   (MSE derivative)\n        # dy_pred/dz = sigmoid(z) * (1 - sigmoid(z))\n        factor = (2.0 / n_samples) * error * y_pred * (1 - y_pred)   # shape: (n_samples,)\n\n        grad_w = X.T @ factor             # shape: (n_features,)\n        grad_b = np.sum(factor)           # scalar\n\n        # Parameter update\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n    # Round results to 4 decimal places as required\n    w_rounded   = np.round(w, 4)\n    b_rounded   = round(b, 4)\n    losses_list = np.round(losses, 4).tolist()\n\n    return w_rounded, b_rounded, losses_list\n```"}
{"task_id": 26, "completion_id": 0, "completion": "Below is a minimal yet fully-functional re-implementation of the micro-autograd system popularised by Andrej Karpathy.  \nIt supports\n\n\u2022 scalar Values  \n\u2022 addition (+)  \n\u2022 multiplication (*)  \n\u2022 ReLU activation (.relu())  \n\u2022 reverse-mode automatic differentiation via .backward()\n\nCopy\u2013paste and play:\n\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        # data \u00ad\u2013 the actual scalar\n        self.data = float(data)\n        # accumulated gradient\n        self.grad = 0.0\n\n        # internal bookkeeping\n        self._backward = lambda: None         # function that propagates grad to _children\n        self._prev      = set(_children)      # the nodes that produced this Value\n        self._op        = _op                 # operation label (for visual/debug purposes)\n\n    # ------------------------------------------------------------------ #\n    #                               math                                 #\n    # ------------------------------------------------------------------ #\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n\n        out = Value(self.data + other.data, (self, other), '+')\n\n        # local backward rule\n        def _backward():\n            self.grad  += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad  += other.data * out.grad\n            other.grad += self.data  * out.grad\n        out._backward = _backward\n        return out\n\n    # ReLU activation ----------------------------------------------------\n    def relu(self):\n        out = Value(0.0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (0.0 if out.data == 0 else 1.0) * out.grad\n        out._backward = _backward\n        return out\n\n    # ------------------------------------------------------------------ #\n    #                     Python niceties / sugar                         #\n    # ------------------------------------------------------------------ #\n    # allow int + Value, int * Value, etc.\n    __radd__ = __add__\n    __rmul__ = __mul__\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    # ------------------------------------------------------------------ #\n    #                         autograd engine                             #\n    # ------------------------------------------------------------------ #\n    def backward(self):\n        # topological order of the graph via DFS\n        topo, visited = [], set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n\n        # seed gradient of the output\n        self.grad = 1.0\n\n        # go through nodes in reverse topological order\n        for node in reversed(topo):\n            node._backward()\n```\n\nQuick demo:\n\n```python\nif __name__ == \"__main__\":\n    a = Value(2.0)\n    b = Value(-3.0)\n    c = Value(10.0)\n\n    # some arbitrary scalar expression\n    d = a * b + c      # d = a*b + c\n    e = d.relu()       # ReLU\n    f = e * a + b      # further ops\n    f.backward()       # compute all gradients\n\n    print(\"f  =\", f)\n    print(\"a =\", a)    # gradients of a, b, c are now filled in\n    print(\"b =\", b)\n    print(\"c =\", c)\n```\n\nThe gradients printed correspond to the analytically correct derivatives of `f` with respect to `a`, `b`, and `c`."}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the change\u2013of\u2013basis matrix P that converts coordinates expressed in\n    basis B to coordinates expressed in basis C for R\u00b3.\n\n    Given any vector v:\n        [v]_C = P \u00b7 [v]_B\n\n    Parameters\n    ----------\n    B : list[list[int | float]]\n        Three vectors of the B\u2013basis, each given in the standard basis.\n    C : list[list[int | float]]\n        Three vectors of the C\u2013basis, each given in the standard basis.\n\n    Returns\n    -------\n    list[list[float]]\n        3\u00d73 transformation matrix rounded to 4 decimal places (row\u2013major list).\n    \"\"\"\n    # Build 3\u00d73 matrices whose *columns* are the basis vectors\n    B_mat = np.array(B, dtype=float).T   # shape (3,3)\n    C_mat = np.array(C, dtype=float).T   # shape (3,3)\n\n    # P = C\u207b\u00b9 \u00b7 B\n    P = np.linalg.inv(C_mat) @ B_mat\n\n    # Round to 4 decimals and return as plain Python list\n    return np.round(P, 4).tolist()\n```\n\nHow it works\n1. Columns\u2010as\u2010basis: placing each basis vector as a column makes the usual linear\u2010algebra identity  \n   v = B \u00b7 [v]_B = C \u00b7 [v]_C hold.\n2. Solving for `[v]_C` when `[v]_B` is known gives `[v]_C = C\u207b\u00b9 B \u00b7 [v]_B`, hence  \n   P = C\u207b\u00b9 B.\n3. The result is rounded to 4 decimal places and converted to a plain list."}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Singular Value Decomposition for a real 2\u00d72 matrix using nothing\n    but eigen\u2013analysis as described in\n    https://metamerist.blogspot.com/2006/10/linear-algebra-for-graphics-geeks-svd.html\n    \n    Parameters\n    ----------\n    A : np.ndarray\n        A real 2\u00d72 matrix.\n    \n    Returns\n    -------\n    (U, S, V) : tuple\n        U, S and V are python lists (obtained with .tolist()) holding the\n        matrices rounded to 4 decimals such that\n\n            A \u2248 U \u00b7 S \u00b7 V              (all 2\u00d72)\n\n        Note that V here is already the transpose of the usual right\u2013\n        singular\u2013vector matrix, so the simple triple product reproduces A.\n    \"\"\"\n    # --- basic sanity -------------------------------------------------------\n    if A.shape != (2, 2):\n        raise ValueError('Only 2\u00d72 matrices are supported.')\n    A = A.astype(float)\n\n    # --- 1. right\u2013singular vectors via eigen-decomposition ------------------\n    AtA = A.T @ A\n    eig_vals, eig_vecs = np.linalg.eigh(AtA)           # orthonormal e-vecs\n    idx = np.argsort(eig_vals)[::-1]                   # descending order\n    eig_vals = eig_vals[idx]\n    V = eig_vecs[:, idx]                               # columns = v\u2081, v\u2082\n\n    # keep the set right-handed (optional but nice)\n    if np.linalg.det(V) < 0:\n        V[:, -1] *= -1\n\n    # --- 2. singular values -------------------------------------------------\n    sing = np.sqrt(np.clip(eig_vals, 0, None))         # \u03c3\u2081 \u2265 \u03c3\u2082 \u2265 0\n    S = np.diag(sing)\n\n    # --- 3. left\u2013singular vectors ------------------------------------------\n    eps = 1e-10\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if sing[i] > eps:\n            U[:, i] = (A @ V[:, i]) / sing[i]\n\n    # handle zero singular values: fabricate an orthonormal completion\n    if sing[0] < eps and sing[1] < eps:                # zero matrix\n        U = np.eye(2)\n    elif sing[0] < eps or sing[1] < eps:               # rank-1 matrix\n        nz = 0 if sing[0] > eps else 1                 # non-zero column\n        u = U[:, nz]\n        U[:, 1-nz] = np.array([-u[1], u[0]])           # perpendicular\n        U[:, 1-nz] /= np.linalg.norm(U[:, 1-nz])\n\n    # --- 4. rounding & output ----------------------------------------------\n    U  = np.round(U,  4)\n    S  = np.round(S,  4)\n    Vt = np.round(V.T, 4)      # we return V = V\u1d40 so A = U\u00b7S\u00b7V\n\n    return U.tolist(), S.tolist(), Vt.tolist()\n```\n\nExample\n-------\n```python\nA = np.array([[3, 1],\n              [1, 3]], dtype=float)\n\nU, S, V = svd_2x2(A)\n\n# Reconstruct (small numerical error is expected)\nA_recon = np.dot(U, np.dot(S, V))\nprint(np.round(A_recon,4))\n# [[3. 1.]\n#  [1. 3.]]\n```\n\nAll three returned matrices are rounded to four decimal places and converted to regular Python lists, satisfying the stated constraints."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle two NumPy arrays in unison while keeping the correspondence\n    between samples in X and y intact.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature array of shape (n_samples, \u2026).\n    y : np.ndarray\n        Label/target array of shape (n_samples, \u2026).\n    seed : int, optional\n        Seed for reproducible shuffling.  Default is None (non-deterministic).\n\n    Returns\n    -------\n    tuple(list, list)\n        A tuple containing the shuffled X and y, both converted to native\n        Python lists via NumPy\u2019s `tolist()` method.\n    \"\"\"\n    # Ensure inputs are NumPy arrays\n    X = np.asarray(X)\n    y = np.asarray(y)\n\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples.\")\n\n    # Create a random number generator (reproducible if seed is given)\n    rng = np.random.default_rng(seed)\n\n    # Generate a permutation of indices and apply it to both arrays\n    perm = rng.permutation(X.shape[0])\n    X_shuffled = X[perm]\n    y_shuffled = y[perm]\n\n    # Convert back to Python lists and return\n    return X_shuffled.tolist(), y_shuffled.tolist()\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generator that returns consecutive mini-batches from X (and y, if given).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix with shape (n_samples, \u2026).\n    y : np.ndarray or None, optional\n        Target array with length n_samples. If None, only X is batched.\n    batch_size : int, default=64\n        Number of samples per batch.\n\n    Yields\n    ------\n    list\n        \u2022 If y is None  : list representation of an X batch.  \n        \u2022 If y is given : tuple (X_batch_list, y_batch_list).\n    \"\"\"\n    # Ensure inputs are NumPy arrays.\n    X = np.asarray(X)\n\n    if batch_size <= 0:\n        raise ValueError(\"batch_size must be a positive integer.\")\n\n    if y is not None:\n        y = np.asarray(y)\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"X and y must contain the same number of samples.\")\n\n    n_samples = X.shape[0]\n\n    # Sequentially iterate over the dataset\n    for start in range(0, n_samples, batch_size):\n        end = start + batch_size\n\n        X_batch = X[start:end]\n\n        if y is None:\n            yield X_batch.tolist()                       # \u2192 list\n        else:\n            y_batch = y[start:end]\n            yield (X_batch.tolist(), y_batch.tolist())   # \u2192 tuple of lists\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Split a dataset into two subsets based on a feature-value threshold.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Input dataset.\n    feature_i : int\n        Index of the feature (column) on which to split.\n    threshold : float or int\n        Threshold value to compare against the chosen feature.\n\n    Returns\n    -------\n    tuple(list, list)\n        \u2022 First list  -> samples where X[:, feature_i] >= threshold  \n        \u2022 Second list -> samples where X[:, feature_i]  < threshold\n    \"\"\"\n    # Convert to NumPy array (copy=False avoids unnecessary copy)\n    X = np.asarray(X)\n\n    # Ensure we are working with a 2-D array\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n\n    if not (0 <= feature_i < X.shape[1]):\n        raise IndexError(\"feature_i is out of bounds for the number of features in X.\")\n\n    # Boolean masks for the split\n    mask = X[:, feature_i] >= threshold\n\n    left  = X[mask]      # Meets the condition\n    right = X[~mask]     # Does not meet the condition\n\n    # Return pure Python lists\n    return left.tolist(), right.tolist()\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a 2-D numpy array.\n\n    Parameters\n    ----------\n    X : array_like, shape (n_samples, n_features)\n        Input data.\n    degree : int\n        Maximum polynomial degree to generate (must be >= 0).\n\n    Returns\n    -------\n    list\n        Python list (of lists) containing the new feature matrix with all\n        polynomial combinations of the input features up to the given degree,\n        including the bias (constant) term.\n    \"\"\"\n    # --- Validation ---------------------------------------------------------\n    X = np.asarray(X)\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D array, got shape {}.\".format(X.shape))\n    if not isinstance(degree, int) or degree < 0:\n        raise ValueError(\"degree must be a non-negative integer, got {}.\".format(degree))\n\n    n_samples, n_features = X.shape\n\n    # --- Build polynomial features -----------------------------------------\n    cols = []  # list to hold each generated feature column\n\n    for d in range(degree + 1):\n        # generate all combinations of feature indices with replacement\n        for comb in combinations_with_replacement(range(n_features), d):\n            if not comb:  # degree 0 -> bias term (all 1s)\n                col = np.ones((n_samples, 1), dtype=X.dtype)\n            else:\n                # product of the selected columns\n                col = np.prod(X[:, comb], axis=1, keepdims=True)\n            cols.append(col)\n\n    # --- Concatenate and return as Python list ------------------------------\n    poly_X = np.hstack(cols)     # shape: (n_samples, n_output_features)\n    return poly_X.tolist()\n```"}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets (bootstraps/permutations) of a dataset.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Feature matrix.\n    y : array-like, shape (n_samples,)\n        Target vector.\n    n_subsets : int\n        Number of random subsets to generate.\n    replacements : bool, default=True\n        If True, sample with replacement (bootstrap); otherwise, sample\n        without replacement (permutation).\n    seed : int, default=42\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    list\n        A list of length `n_subsets`. Each element is a tuple\n        (X_subset, y_subset) where both `X_subset` and `y_subset`\n        have been converted to pure Python lists via `.tolist()`.\n    \"\"\"\n    # Ensure inputs are NumPy arrays\n    X = np.asarray(X)\n    y = np.asarray(y)\n\n    # Basic sanity checks\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D array.\")\n    if y.ndim != 1:\n        raise ValueError(\"y must be a 1-D array.\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must contain the same number of samples.\")\n    if n_subsets <= 0:\n        raise ValueError(\"n_subsets must be a positive integer.\")\n\n    rng = np.random.default_rng(seed)\n    n_samples = X.shape[0]\n    subsets = []\n\n    for _ in range(n_subsets):\n        # Choose indices\n        if replacements:\n            indices = rng.integers(low=0, high=n_samples, size=n_samples)\n        else:\n            indices = rng.choice(n_samples, size=n_samples, replace=False)\n\n        # Slice the data\n        X_subset = X[indices]\n        y_subset = y[indices]\n\n        # Convert to Python lists as required\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n\n    return subsets\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    One-hot encode a 1-D array of integer class labels.\n\n    Parameters\n    ----------\n    x : array-like (1-D)\n        Integer class labels to be encoded.\n    n_col : int, optional\n        Total number of columns (i.e., distinct classes).  \n        If None, it is inferred as max(x) + 1.\n\n    Returns\n    -------\n    list\n        One-hot encoded representation as a Python list of lists.\n    \"\"\"\n    # Ensure x is a 1-D NumPy array of integers\n    x = np.asarray(x, dtype=int).ravel()\n\n    # Determine number of categories / columns\n    if n_col is None:\n        n_col = int(x.max()) + 1\n\n    # Create one-hot matrix\n    one_hot = np.eye(n_col, dtype=int)[x]\n\n    # Convert to nested Python list and return\n    return one_hot.tolist()\n```"}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1-D numpy array into a diagonal matrix and return it as a\n    native Python list (via NumPy\u2019s `tolist()` method).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        A one-dimensional array whose elements will form the diagonal\n        of the resulting matrix.\n\n    Returns\n    -------\n    list\n        A 2-D list representing the diagonal matrix.\n    \"\"\"\n    # Ensure the input is a NumPy array\n    x = np.asarray(x)\n\n    # Validate that the input is 1-D\n    if x.ndim != 1:\n        raise ValueError(\"Input must be a 1D numpy array.\")\n\n    # Create the diagonal matrix\n    diag_matrix = np.diag(x)\n\n    # Return the matrix as a Python list\n    return diag_matrix.tolist()\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : array-like (1D)\n        Ground-truth labels.\n    y_pred : array-like (1D)\n        Predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy score (proportion of correct predictions).\n\n    Raises\n    ------\n    ValueError\n        If the input arrays have different lengths or are empty.\n    \"\"\"\n    # Convert inputs to 1-D NumPy arrays\n    y_true = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n\n    # Basic validation\n    if y_true.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n    if y_true.size != y_pred.size:\n        raise ValueError(\"y_true and y_pred must have the same length.\")\n\n    # Compute accuracy\n    correct = np.sum(y_true == y_pred)\n    accuracy = correct / y_true.size\n    return float(accuracy)\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate a Pearson\u2010correlation matrix.\n    \n    Parameters\n    ----------\n    X : 2-D numpy array (n_samples \u00d7 n_features_x)\n    Y : 2-D numpy array (n_samples \u00d7 n_features_y), optional\n        If omitted, Y is taken to be X.\n    \n    Returns\n    -------\n    list\n        2-D Python list containing the correlation coefficients,\n        rounded to 4 decimal places.\n    \"\"\"\n    # ---- Argument checks & preparation -------------------------------------\n    X = np.asarray(X, dtype=float)\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D array.\")\n\n    if Y is None:\n        Y = X\n    else:\n        Y = np.asarray(Y, dtype=float)\n        if Y.ndim != 2:\n            raise ValueError(\"Y must be a 2-D array.\")\n        if Y.shape[0] != X.shape[0]:\n            raise ValueError(\"X and Y must have the same number of rows (samples).\")\n\n    # ---- Standardise (zero mean, unit variance) ----------------------------\n    # ddof=1 gives the unbiased (sample) standard deviation\n    X_mean = X.mean(axis=0)\n    Y_mean = Y.mean(axis=0)\n    X_std  = X.std(axis=0, ddof=1)\n    Y_std  = Y.std(axis=0, ddof=1)\n\n    # Avoid divide-by-zero for constant columns: set std to NaN (will propagate)\n    X_std[X_std == 0] = np.nan\n    Y_std[Y_std == 0] = np.nan\n\n    Xz = (X - X_mean) / X_std     # (n_samples \u00d7 n_features_x)\n    Yz = (Y - Y_mean) / Y_std     # (n_samples \u00d7 n_features_y)\n\n    # ---- Correlation matrix -------------------------------------------------\n    n = X.shape[0]\n    corr = (Xz.T @ Yz) / (n - 1)  # (f_x \u00d7 f_y)\n\n    # ---- Round & convert to Python list ------------------------------------\n    corr = np.round(corr, 4)\n    return corr.tolist()\n```"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Train an AdaBoost ensemble of decision stumps.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Training data.\n    y : ndarray of shape (n_samples,)\n        Labels. Accepts {-1, 1} or {0, 1}. The latter will be converted.\n    n_clf : int\n        Number of weak classifiers (decision stumps) to learn.\n\n    Returns\n    -------\n    clfs : list\n        A list with one dictionary per learned stump.  Each dictionary holds\n        \u2022 feature      : index of the feature used\n        \u2022 threshold    : threshold value\n        \u2022 polarity     : 1  or -1 (`1` means: predict -1 for x < thr,\n                                    -1 means: predict -1 for x > thr)\n        \u2022 alpha        : classifier weight  \n        All numeric items are rounded to 4 decimals.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n\n    # \u2500\u2500\u2500 Make sure labels are in {-1, 1} \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if set(np.unique(y)) == {0, 1}:\n        y = y * 2 - 1           # {0,1} \u2192 {-1,1}\n\n    n_samples, n_features = X.shape\n    # \u2500\u2500\u2500  1. initialise uniform sample-weights  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    clfs = []\n\n    eps = 1e-10                       # To avoid log(0) or division by zero\n\n    for _ in range(n_clf):\n        best = {\n            \"feature\": None,\n            \"threshold\": None,\n            \"polarity\": 1,\n            \"error\": float(\"inf\"),\n            \"prediction\": None,\n        }\n\n        # \u2500\u2500\u2500 2. Search best decision-stump under current weights \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        for j in range(n_features):\n            feature_values = X[:, j]\n            thresholds = np.unique(feature_values)\n\n            for thr in thresholds:\n                for pol in (1, -1):\n                    # Polarity convention:\n                    #  pol==1 : predict -1 for x < thr , +1 otherwise\n                    #  pol==-1: predict -1 for x > thr , +1 otherwise\n                    pred = np.ones(n_samples)\n                    if pol == 1:\n                        pred[feature_values < thr] = -1\n                    else:\n                        pred[feature_values > thr] = -1\n\n                    err = np.sum(w[pred != y])\n\n                    if err < best[\"error\"]:\n                        best.update(\n                            feature=j,\n                            threshold=thr,\n                            polarity=pol,\n                            error=err,\n                            prediction=pred.copy(),\n                        )\n\n        # \u2500\u2500\u2500 3. Compute stump weight (alpha) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        err = max(best[\"error\"], eps)          # keep away from 0\n        alpha = 0.5 * math.log((1 - err) / err)\n\n        # \u2500\u2500\u2500 4. Update sample-weights \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        w *= np.exp(-alpha * y * best[\"prediction\"])\n        w /= w.sum()                           # renormalise\n\n        # \u2500\u2500\u2500 5. Store rounded classifier parameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        clf = dict(\n            feature   = best[\"feature\"],\n            threshold = round(float(best[\"threshold\"]), 4),\n            polarity  = int(best[\"polarity\"]),\n            alpha     = round(float(alpha), 4),\n        )\n        clfs.append(clf)\n\n    return clfs\n```"}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a 1-D list/array of scores.\n\n    Parameters\n    ----------\n    scores : list or np.ndarray\n        Input scores.\n\n    Returns\n    -------\n    list\n        Log-softmax values rounded to 4 decimal places.\n    \"\"\"\n    # Convert to numpy array (float)\n    x = np.asarray(scores, dtype=np.float64)\n\n    # Numerical-stability trick: subtract the max\n    x_shifted = x - np.max(x)\n\n    # Log-softmax:  log(exp(x_i) / \u03a3 exp(x_j))\n    #              = x_i - max - log(\u03a3 exp(x_j - max))\n    log_softmax_vals = x_shifted - np.log(np.sum(np.exp(x_shifted)))\n\n    # Round to 4 decimal places and return as Python list\n    return np.round(log_softmax_vals, 4).tolist()\n\n\n# ------------- Example -------------\nif __name__ == \"__main__\":\n    scores = [1.0, 2.0, 3.0]\n    print(log_softmax(scores))      # -> [-2.4076, -1.4076, -0.4076]\n```"}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport copy\nimport math\n\nnp.random.seed(42)\n\n\nclass Layer(object):\n\n    # ------------------------------------------------------------------ #\n    # Basic, already-given functionality                                 #\n    # ------------------------------------------------------------------ #\n    def set_input_shape(self, shape):\n        \"\"\"\n        Receives the dimensionality of the input that will be fed\n        to the layer.  (Called by the network-builder before the\n        real training starts.)\n        \"\"\"\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\n\n# ====================================================================== #\n#                          The required Dense class                      #\n# ====================================================================== #\nclass Dense(Layer):\n    \"\"\"\n    Fully-connected (a.k.a. \u201cdense\u201d) layer.\n    --------------------------------------------------------------------\n    1) W  : weight matrix                    \u2013 shape  (n_in , n_units)\n    2) w0 : bias vector                      \u2013 shape  (n_units, )\n    3) *Opt: (deep-copies of) the optimiser  \u2013 one for W, one for w0\n    --------------------------------------------------------------------\n    All public methods return python lists and are 4-decimal rounded,\n    as demanded in the exercise statement.\n    \"\"\"\n\n    # ---------------------------  constructor  ------------------------ #\n    def __init__(self, n_units, input_shape=None):\n        self.n_units = n_units                      # number of neurons\n        self.input_shape = input_shape              # (n_in , )\n        self.trainable = True\n\n        # will be filled during `initialize`\n        self.W = None                               # weight matrix\n        self.w0 = None                              # bias vector\n\n        # optimiser placeholders (filled in `initialize`)\n        self.W_opt = None\n        self.w0_opt = None\n\n        # input cache for the backward pass\n        self.layer_input = None\n\n    # ----------------------------  initialise  ------------------------ #\n    def initialize(self, optimizer):\n        \"\"\"\n        Called once the network knows the real input dimensionality and\n        once an optimizer object is available.\n        \"\"\"\n        if self.input_shape is None:\n            raise ValueError(\"Input shape is undefined. \"\n                             \"Call `set_input_shape` before `initialize`.\")\n\n        limit = 1.0 / math.sqrt(self.input_shape[0])        # Glorot-like\n        self.W = np.random.uniform(-limit, limit,\n                                   (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros(self.n_units)\n\n        # keep *independent* optimiser copies (one per parameter tensor)\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    # -------------------------  parameter count  ---------------------- #\n    def parameters(self):\n        if self.W is None or self.w0 is None:\n            return 0\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    # ---------------------------  forward pass  ----------------------- #\n    def forward_pass(self, X, training=True):\n        \"\"\"\n        X : ndarray (batch_size, n_in)\n        returns list \u2013 4-decimal rounded\n        \"\"\"\n        self.layer_input = X                              # cache for grad\n        output = np.dot(X, self.W) + self.w0              # affine map\n        return np.round(output, 4).tolist()\n\n    # ---------------------------  backward pass  ---------------------- #\n    def backward_pass(self, accum_grad):\n        \"\"\"\n        accum_grad : ndarray/list  (batch_size, n_units)\n                     gradient arriving from the next layer\n        returns     : list \u2013 gradient wrt the layer input (n_in)\n        \"\"\"\n        # ensure ndarray\n        accum_grad = np.asarray(accum_grad)\n\n        # 1) gradients wrt parameters\n        grad_W = np.dot(self.layer_input.T, accum_grad)        # (n_in, n_units)\n        grad_w0 = np.sum(accum_grad, axis=0)                   # (n_units,)\n\n        # 2) parameter update\n        if self.trainable and self.W_opt is not None:\n            self.W = self.W_opt.update(self.W, grad_W)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n\n        # 3) propagate gradient to the previous layer\n        grad_input = np.dot(accum_grad, self.W.T)              # (batch, n_in)\n        return np.round(grad_input, 4).tolist()\n\n    # ----------------------------  out shape  ------------------------- #\n    def output_shape(self):\n        return (self.n_units,)\n```"}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, \n                  kernel: np.ndarray, \n                  padding: int = 0, \n                  stride:  int = 1):\n    \"\"\"\n    A very small, framework\u2013free 2-D convolution layer (single input-,\n    single output-channel).\n\n    Parameters\n    ----------\n    input_matrix : np.ndarray\n        2-D array (H \u00d7 W) representing the input feature map.\n    kernel       : np.ndarray\n        2-D array (kH \u00d7 kW) holding the convolutional weights.\n        (No flip is performed \u2192 cross-correlation, as in most DL libs.)\n    padding      : int\n        How many zero rows / columns to add symmetrically around\n        the input.\n    stride       : int\n        Number of pixels to move the kernel each step.\n\n    Returns\n    -------\n    list\n        Convolved feature map rounded to 4 dp and converted to a\n        regular Python list.\n    \"\"\"\n    # --- Sanity checks -------------------------------------------------------\n    if input_matrix.ndim != 2 or kernel.ndim != 2:\n        raise ValueError(\"input_matrix and kernel must both be 2-D arrays.\")\n\n    if padding < 0 or stride <= 0:\n        raise ValueError(\"padding must be >= 0 and stride must be > 0.\")\n\n    # --- Padding -------------------------------------------------------------\n    padded = np.pad(input_matrix,\n                    pad_width=((padding, padding), (padding, padding)),\n                    mode='constant',\n                    constant_values=0)\n\n    H_p, W_p  = padded.shape          # padded height / width\n    kH,  kW   = kernel.shape\n\n    # --- Output spatial dimensions ------------------------------------------\n    out_h = (H_p - kH) // stride + 1\n    out_w = (W_p - kW) // stride + 1\n    if out_h <= 0 or out_w <= 0:\n        raise ValueError(\"Kernel size / padding / stride combination \"\n                         \"produces invalid output dimension.\")\n\n    # --- Convolution ---------------------------------------------------------\n    out = np.empty((out_h, out_w), dtype=float)\n\n    for i in range(out_h):\n        for j in range(out_w):\n            hs = i * stride            # height start\n            ws = j * stride            # width  start\n            window = padded[hs:hs + kH,\n                            ws:ws + kW]\n            out[i, j] = np.sum(window * kernel)\n\n    # --- Rounding & list conversion -----------------------------------------\n    out = np.round(out, 4)\n\n    return out.tolist()\n```"}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Rectified Linear Unit (ReLU) activation function.\n\n    Parameters\n    ----------\n    z : float\n        Input value.\n\n    Returns\n    -------\n    float\n        z if z > 0, else 0.\n    \"\"\"\n    return z if z > 0 else 0.0\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D feature matrix of shape (n_samples, n_features).\n    w : np.ndarray\n        1-D coefficient vector of shape (n_features, ).\n    y_true : np.ndarray\n        1-D array with the true target values of shape (n_samples, ).\n    alpha : float\n        Regularisation strength (\u03bb).\n\n    Returns\n    -------\n    float\n        Ridge loss rounded to 4 decimal places.\n    \"\"\"\n    # Predicted values\n    y_pred = X @ w\n    \n    # Mean Squared Error term\n    mse = np.mean((y_pred - y_true) ** 2)\n    \n    # Ridge (L2) regularisation term\n    reg_term = alpha * np.sum(w ** 2)\n    \n    # Total loss\n    total_loss = mse + reg_term\n    \n    # Round to the nearest 4th decimal and return as float\n    return float(np.round(total_loss, 4))\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float | int:\n    \"\"\"\n    Compute the Leaky ReLU activation for a single scalar input.\n\n    Parameters\n    ----------\n    z : float\n        The input value.\n    alpha : float, optional\n        Slope applied when z is negative (default is 0.01).\n\n    Returns\n    -------\n    float | int\n        The transformed value after applying the Leaky ReLU function.\n    \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z\n```\n\nExample usage:\n```python\nprint(leaky_relu(3.5))     # 3.5 (because it is positive)\nprint(leaky_relu(-3.5))    # -0.035 (alpha * z)\nprint(leaky_relu(-3.5, 0.1))  # -0.35 (custom alpha)\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel (dot product) between two input vectors.\n\n    Parameters\n    ----------\n    x1, x2 : array-like\n        Input vectors of the same length.\n\n    Returns\n    -------\n    float\n        The dot product \u27e8x1, x2\u27e9.\n    \"\"\"\n    # Convert inputs to NumPy 1-D arrays\n    v1 = np.asarray(x1).ravel()\n    v2 = np.asarray(x2).ravel()\n\n    # Optional: sanity check\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same length.\")\n\n    return float(np.dot(v1, v2))\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Compute the precision for binary classification.\n    \n    Parameters\n    ----------\n    y_true : array-like\n        Ground\u2013truth (correct) binary labels (0 or 1).\n    y_pred : array-like\n        Predicted binary labels (0 or 1).\n    \n    Returns\n    -------\n    float\n        Precision = TP / (TP + FP).  \n        Returns 0.0 if there are no positive predictions (TP + FP == 0).\n    \"\"\"\n    # Convert inputs to 1-D numpy arrays\n    y_true = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    \n    if y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError(\"y_true and y_pred must have the same number of elements.\")\n    \n    # True Positives (TP): predicted 1 and actually 1\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # False Positives (FP): predicted 1 but actually 0\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    \n    denom = tp + fp\n    return tp / denom if denom != 0 else 0.0\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations,\n                     batch_size=1, method='batch'):\n    \"\"\"\n    Gradient\u2013descent optimizer that supports\n        1. 'batch'       : Batch Gradient Descent\n        2. 'stochastic'  : Stochastic Gradient Descent\n        3. 'mini-batch'  : Mini-Batch Gradient Descent\n\n    Parameters\n    ----------\n    X : ndarray, shape (m, n)\n        Feature matrix.\n    y : ndarray, shape (m,)  or  (m, 1)\n        Target vector.\n    weights : ndarray, shape (n,)  or  (n, 1)\n        Initial weight vector.\n    learning_rate : float\n        Learning rate (\u03b1).\n    n_iterations : int\n        Number of epochs (full passes over the training data).\n    batch_size : int, default=1\n        Size of mini-batches when method='mini-batch'.\n        Ignored for 'batch'.  For 'stochastic', an internal\n        batch size of 1 is always used.\n    method : str, {'batch', 'stochastic', 'mini-batch'}, default='batch'\n        Variant of gradient descent to apply.\n\n    Returns\n    -------\n    list\n        Optimised weights rounded to 4 decimals, in native Python list\n        form (via ndarray.tolist()).\n    \"\"\"\n\n    # --- shape sanity --------------------------------------------------------\n    X          = np.asarray(X)\n    y          = np.asarray(y).reshape(-1)           # make y 1-D\n    weights    = np.asarray(weights).reshape(-1)     # make w 1-D\n    m, n       = X.shape\n\n    # --- helper: gradient for an arbitrary batch -----------------------------\n    def _batch_grad(x_batch, y_batch, w):\n        \"\"\"\n        Computes gradient of MSE w.r.t. w for a batch.\n        \"\"\"\n        b_m        = x_batch.shape[0]                       # batch size\n        preds      = x_batch.dot(w)\n        errors     = preds - y_batch\n        grad       = (2.0 / b_m) * x_batch.T.dot(errors)\n        return grad\n\n    # --- optimisation loop ---------------------------------------------------\n    for epoch in range(n_iterations):\n\n        if method == 'batch':\n            # One update per epoch using all samples\n            grad = _batch_grad(X, y, weights)\n            weights -= learning_rate * grad\n\n        elif method == 'stochastic':\n            # Iterate through samples one by one\n            indices = np.random.permutation(m)   # shuffle each epoch\n            for idx in indices:\n                x_i = X[idx:idx + 1]             # shape (1, n)\n                y_i = y[idx]\n                grad = _batch_grad(x_i, y_i, weights)\n                weights -= learning_rate * grad\n\n        elif method == 'mini-batch':\n            # Split data into mini-batches of given size\n            indices = np.random.permutation(m)   # shuffle each epoch\n            for start in range(0, m, batch_size):\n                end         = start + batch_size\n                batch_idx   = indices[start:end]\n                x_b         = X[batch_idx]\n                y_b         = y[batch_idx]\n                grad        = _batch_grad(x_b, y_b, weights)\n                weights    -= learning_rate * grad\n        else:\n            raise ValueError(\"method must be 'batch', 'stochastic' or 'mini-batch'\")\n\n    # --- rounding & returning -------------------------------------------------\n    weights = np.round(weights, 4)\n    return weights.reshape(-1).tolist()\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix, *, tol=1e-12):\n    \"\"\"\n    Convert the given matrix to Reduced Row Echelon Form (RREF).\n\n    Parameters\n    ----------\n    matrix : array-like\n        Input matrix (list of lists, tuple of tuples, numpy.ndarray \u2026).\n    tol : float, optional\n        Numerical tolerance used to decide when a value is effectively zero.\n        Default is 1e-12.\n\n    Returns\n    -------\n    list\n        The RREF of `matrix`, returned as (nested) Python lists\n        via NumPy\u2019s ``tolist()`` method.\n    \"\"\"\n    A = np.array(matrix, dtype=float, copy=True)      # work on a float copy\n    m, n = A.shape\n    pivot_row = 0                                     # row where next pivot will go\n\n    for col in range(n):\n        if pivot_row >= m:            # ran out of rows \u21d2 finished\n            break\n\n        # -----------------------------------------------------------------\n        # 1. Find the pivot in the current column (largest absolute value\n        #    below / at pivot_row).  Works for singular / near-singular mats.\n        # -----------------------------------------------------------------\n        rows_below = np.abs(A[pivot_row:, col])\n        max_idx_rel = np.argmax(rows_below)\n        max_val = rows_below[max_idx_rel]\n        if max_val < tol:             # column is (numerically) all zeros\n            continue                  # \u21d2 no pivot in this column\n\n        pivot_idx = pivot_row + max_idx_rel           # actual row index\n        # -----------------------------------------------------------------\n        # 2. Swap the pivot row into position\n        # -----------------------------------------------------------------\n        if pivot_idx != pivot_row:\n            A[[pivot_row, pivot_idx]] = A[[pivot_idx, pivot_row]]\n\n        # -----------------------------------------------------------------\n        # 3. Scale the pivot row so that the pivot entry equals 1\n        # -----------------------------------------------------------------\n        pivot_val = A[pivot_row, col]\n        A[pivot_row] = A[pivot_row] / pivot_val\n\n        # -----------------------------------------------------------------\n        # 4. Eliminate the pivot from every other row\n        # -----------------------------------------------------------------\n        for r in range(m):\n            if r != pivot_row:\n                factor = A[r, col]\n                if abs(factor) > tol:                # skip obvious zeros\n                    A[r] -= factor * A[pivot_row]\n\n        # Next pivot must be placed on the next row\n        pivot_row += 1\n\n    # ---------------------------------------------------------------------\n    # 5. Zero-out tiny values introduced by floating-point arithmetic\n    # ---------------------------------------------------------------------\n    A[np.abs(A) < tol] = 0.0\n\n    return A.tolist()\n```\n\nThe algorithm is a standard Gauss-Jordan elimination:\n\n1. Scan each column from left to right.  \n2. For each column, find a suitable pivot row (largest magnitude entry to improve numerical stability).  \n3. Swap it into place, scale the pivot row to make the leading entry 1, and then eliminate that column\u2019s entry from every other row.  \n4. Continue until either we run out of columns or rows.  \n5. Lastly, any value whose magnitude is below `tol` is set to 0 so the output is a clean exact-looking RREF."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999,\n                   epsilon=1e-8, num_iterations=1000):\n    \"\"\"\n    Adam optimizer implementation.\n\n    Parameters\n    ----------\n    f : callable\n        Objective function to minimize. Not directly used in the core\n        algorithm, but kept for completeness / potential monitoring.\n    grad : callable\n        Function that computes the gradient of `f`. It must accept a\n        single argument `x` and return the gradient array of the same\n        shape as `x`.\n    x0 : array-like\n        Initial parameter vector.\n    learning_rate : float, optional\n        Step size (\u03b1). Default is 1e-3.\n    beta1 : float, optional\n        Exponential decay rate for first-moment estimates (\u03b2\u2081). Default 0.9.\n    beta2 : float, optional\n        Exponential decay rate for second-moment estimates (\u03b2\u2082). Default 0.999.\n    epsilon : float, optional\n        Small constant for numerical stability. Default 1e-8.\n    num_iterations : int, optional\n        Number of update steps. Default 1000.\n\n    Returns\n    -------\n    list\n        Optimized parameters, flattened and rounded to 4 decimal places.\n    \"\"\"\n    # --- Initialization -----------------------------------------------------\n    x = np.asarray(x0, dtype=float)\n    m = np.zeros_like(x)  # First moment (mean of gradients)\n    v = np.zeros_like(x)  # Second moment (uncentered variance of gradients)\n    t = 0                 # Time step\n\n    # --- Main optimization loop --------------------------------------------\n    for _ in range(1, num_iterations + 1):\n        t += 1\n        g = grad(x)                              # Current gradient\n\n        # Update biased first moment estimate\n        m = beta1 * m + (1.0 - beta1) * g\n\n        # Update biased second raw moment estimate\n        v = beta2 * v + (1.0 - beta2) * (g ** 2)\n\n        # Compute bias-corrected moment estimates\n        m_hat = m / (1.0 - beta1 ** t)\n        v_hat = v / (1.0 - beta2 ** t)\n\n        # Parameter update\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # --- Return the optimized parameters -----------------------------------\n    # Round to 4 decimal places, flatten, and return as a Python list\n    return np.round(x, 4).reshape(-1).tolist()\n```\n\nUsage example (for validation only, remove or comment-out when importing as a module):\n\n```python\nif __name__ == \"__main__\":\n    # Simple quadratic: f(x) = (x-3)\u00b2 ; minimum at x = 3\n    f = lambda x: (x - 3.0) ** 2\n    grad = lambda x: 2.0 * (x - 3.0)          # Analytical gradient\n    x0 = np.array([0.0])                      # Starting point\n\n    result = adam_optimizer(f, grad, x0, learning_rate=0.1, num_iterations=200)\n    print(\"Optimized parameter:\", result)     # \u2192 [3.0]\n```"}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X      : np.array,\n                                       y      : np.array,\n                                       alpha  : float = 0.1,\n                                       learning_rate: float = 0.01,\n                                       max_iter: int = 1000,\n                                       tol     : float = 1e-4) -> tuple:\n    \"\"\"\n    Lasso (L1-regularised) Linear Regression trained with (batch) Gradient Descent.\n\n    Parameters\n    ----------\n    X : np.array, shape = (n_samples, n_features)\n        Design / feature matrix.\n    y : np.array, shape = (n_samples,) or (n_samples, 1)\n        Target vector.\n    alpha : float, default=0.1\n        Strength of the L1 penalty term.\n    learning_rate : float, default=0.01\n        Step size for the gradient descent updates.\n    max_iter : int, default=1000\n        Maximum number of iterations.\n    tol : float, default=1e-4\n        Threshold for early stopping based on absolute change in the\n        objective function between two consecutive iterations.\n\n    Returns\n    -------\n    (weights_list, bias)\n        weights_list : python list of shape (n_features,) rounded to 4 decimals.\n        bias         : float rounded to 4 decimals.\n    \"\"\"\n\n    # Force y to be a 1-D vector\n    y = y.reshape(-1)\n    n_samples, n_features = X.shape\n\n    # Initial parameters\n    w = np.zeros(n_features)\n    b = 0.0\n\n    # Helper: current objective value\n    def objective(pred, w):\n        mse_term  = 0.5 / n_samples * np.sum((y - pred) ** 2)\n        l1_term   = alpha * np.sum(np.abs(w))\n        return mse_term + l1_term\n\n    prev_cost = float(\"inf\")\n\n    for _ in range(max_iter):\n        # Forward pass\n        y_pred = X @ w + b\n        error  = y_pred - y\n\n        # Gradients (sub-gradients for L1)\n        grad_w = (1.0 / n_samples) * (X.T @ error) + alpha * np.sign(w)\n        grad_b = (1.0 / n_samples) * np.sum(error)\n\n        # Parameter update\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n        # Check convergence\n        curr_cost = objective(y_pred, w)\n        if abs(prev_cost - curr_cost) < tol:\n            break\n        prev_cost = curr_cost\n\n    # Round results to 4 decimals as required\n    w = np.round(w, 4)\n    b = round(b, 4)\n\n    return w.tolist(), b\n```"}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Optimal String Alignment (restricted Damerau-Levenshtein) distance\n    ---------------------------------------------------------------\n    Allowed unit-cost operations:\n        \u2022 insertion\n        \u2022 deletion\n        \u2022 substitution\n        \u2022 transposition of two adjacent characters\n    A character (or character pair) can be edited at most once \u2014 the\n    restriction that separates OSA from the full Damerau-Levenshtein metric.\n    \n    Parameters\n    ----------\n    source : str\n        String to be transformed.\n    target : str\n        Desired result string.\n    \n    Returns\n    -------\n    int\n        Minimum number of edits needed to turn `source` into `target`.\n    \"\"\"\n    \n    # Trivial cases -----------------------------------------------------------\n    if source == target:\n        return 0\n    if not source:\n        return len(target)\n    if not target:\n        return len(source)\n\n    m, n = len(source), len(target)\n\n    # DP table: (m+1) x (n+1)\n    # D[i, j] = distance between source[:i] and target[:j]\n    D = np.zeros((m + 1, n + 1), dtype=int)\n\n    # Initialization (edit distance from / to empty string)\n    D[0, :] = np.arange(n + 1)\n    D[:, 0] = np.arange(m + 1)\n\n    # Fill table --------------------------------------------------------------\n    for i in range(1, m + 1):\n        s_i = source[i - 1]                # current char in source\n        for j in range(1, n + 1):\n            t_j = target[j - 1]            # current char in target\n\n            # Cost of (mis)match\n            cost = 0 if s_i == t_j else 1\n\n            # Three basic operations\n            deletion      = D[i - 1, j] + 1          # delete s_i\n            insertion     = D[i, j - 1] + 1          # insert t_j\n            substitution  = D[i - 1, j - 1] + cost   # substitute/keep\n\n            D[i, j] = min(deletion, insertion, substitution)\n\n            # Adjacent transposition (OSA restriction: only if both indices > 1)\n            if i > 1 and j > 1 and s_i == target[j - 2] and source[i - 2] == t_j:\n                transposition = D[i - 2, j - 2] + 1\n                if transposition < D[i, j]:\n                    D[i, j] = transposition\n\n    return int(D[m, n])\n```\n\nHow it works (quick recap)\n\n1. A dynamic-programming matrix `D` is built where `D[i, j]` stores the optimal cost to turn the first `i` characters of `source` into the first `j` characters of `target`.\n2. For every cell we consider the classic Levenshtein moves (insert, delete, substitute) plus, when possible, a transposition of the two adjacent characters that just crossed.\n3. The \u201crestricted\u201d part of OSA is honoured because the matrix only allows each character position to be involved in a transposition once (moving from `i-2, j-2` to `i, j`).\n\nExample\n\n```\n>>> OSA(\"caper\", \"acer\")\n2\n>>> OSA(\"ca\", \"ac\")\n1  # one transposition\n>>> OSA(\"abcd\", \"acbd\")\n1  # transposition of 'b' and 'c'\n```"}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Compute the recall score for binary classification.\n\n    Parameters\n    ----------\n    y_true : list or array-like\n        Ground-truth binary labels (0 or 1).\n    y_pred : list or array-like\n        Predicted binary labels (0 or 1).\n\n    Returns\n    -------\n    float\n        Recall rounded to three decimal places. Returns 0.0 if there are\n        no actual positive samples (i.e., TP + FN == 0).\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorised operations\n    y_true = np.asarray(y_true).astype(int)\n    y_pred = np.asarray(y_pred).astype(int)\n\n    # True Positives (TP): predicted 1 and actually 1\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n\n    # False Negatives (FN): predicted 0 but actually 1\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n\n    rec = tp / denominator\n    return round(rec, 3)\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Computes scaled-dot-product self-attention.\n    \n    Parameters\n    ----------\n    Q, K, V : np.ndarray\n        \u2022 2-D shape : (seq_len , d_k)  \n        \u2022 3-D shape : (batch , seq_len , d_k)\n        The three inputs must share the same last-dimension size (d_k).\n    \n    Returns\n    -------\n    list\n        Attention output rounded to 4 decimal places and converted to Python list.\n    \"\"\"\n\n    # Ensure numpy arrays\n    Q = np.asarray(Q, dtype=np.float64)\n    K = np.asarray(K, dtype=np.float64)\n    V = np.asarray(V, dtype=np.float64)\n\n    d_k = K.shape[-1]                       # key/query depth\n    scale = np.sqrt(d_k)\n\n    # -------- 1. Attention scores: Q \u00b7 K\u1d40 / \u221ad_k  --------\n    if Q.ndim == 2:                         # (seq , d_k)  \u2014 no batch\n        scores = Q @ K.T / scale\n    else:                                   # (batch , seq , d_k)\n        scores = np.matmul(Q, K.transpose(0, 2, 1)) / scale\n\n    # -------- 2. Softmax along the key/sequence axis --------\n    # numerically stable softmax\n    max_score = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - max_score)\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n\n    # -------- 3. Weighted sum with values --------\n    if V.ndim == 2:\n        output = attention_weights @ V       # (seq_q , d_v)\n    else:\n        output = np.matmul(attention_weights, V)  # (batch , seq_q , d_v)\n\n    # -------- 4. Round to 4 decimals & convert to list --------\n    output = np.round(output, 4)\n    return output.tolist()\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]],\n                initial_hidden_state: list[float],\n                Wx: list[list[float]],\n                Wh: list[list[float]],\n                b:  list[float]) -> list[float]:\n    \"\"\"\n    Runs a simple RNN forward pass over an input sequence and returns the\n    final hidden state (rounded to 4 decimal places).\n\n    Parameters\n    ----------\n    input_sequence : list of list of floats\n        A sequence of T input vectors, each of dimensionality `input_size`.\n    initial_hidden_state : list of floats\n        Initial hidden state vector of dimensionality `hidden_size`.\n    Wx : list of list of floats\n        Input-to-hidden weight matrix of shape (hidden_size, input_size).\n    Wh : list of list of floats\n        Hidden-to-hidden weight matrix of shape (hidden_size, hidden_size).\n    b : list of floats\n        Bias vector of shape (hidden_size,).\n\n    Returns\n    -------\n    list of floats\n        Final hidden state after processing the whole sequence, with every\n        element rounded to 4 decimal places.\n    \"\"\"\n\n    # Convert everything to NumPy arrays\n    Wx = np.asarray(Wx, dtype=float)\n    Wh = np.asarray(Wh, dtype=float)\n    b  = np.asarray(b,  dtype=float)\n    h  = np.asarray(initial_hidden_state, dtype=float)\n\n    # Process the sequence step-by-step\n    for x_t in input_sequence:\n        x_t = np.asarray(x_t, dtype=float)\n        h   = np.tanh(Wx @ x_t + Wh @ h + b)   # RNN update rule\n\n    # Round to 4 decimal places and return as Python list\n    return np.round(h, 4).tolist()\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Apply a 2-D translation to a collection of points.\n\n    Parameters\n    ----------\n    points : list[list[float]]\n        List of points, each defined as [x, y].\n    tx : float\n        Translation distance along the x-axis.\n    ty : float\n        Translation distance along the y-axis.\n\n    Returns\n    -------\n    list[list[float]]\n        Translated points, converted back to a pure Python list\n        via NumPy\u2019s ``tolist()`` method.\n    \"\"\"\n    # Convert points to an (N, 2) NumPy array.\n    pts = np.asarray(points, dtype=float)\n\n    if pts.ndim != 2 or pts.shape[1] != 2:\n        raise ValueError(\"`points` must be a list/array of shape (N, 2).\")\n\n    # Add a column of 1s to get homogeneous coordinates: (N, 3).\n    ones = np.ones((pts.shape[0], 1), dtype=float)\n    homogeneous_pts = np.hstack((pts, ones))\n\n    # Translation matrix in homogeneous coordinates.\n    translation_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1 ]\n    ], dtype=float)\n\n    # Apply the translation: (N, 3) @ (3, 3)^T -> (N, 3)\n    translated_homogeneous = homogeneous_pts @ translation_matrix.T\n\n    # Drop the homogeneous component and return Python list.\n    translated_pts = translated_homogeneous[:, :2]\n\n    return translated_pts.tolist()\n```\n\nExample usage\n```python\nif __name__ == \"__main__\":\n    original = [[0, 0], [1, 2], [-3, 4.5]]\n    moved = translate_object(original, tx=5, ty=-2)\n    print(moved)   # [[5.0, -2.0], [6.0, 0.0], [2.0, 2.5]]\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the KL divergence   D_KL( P || Q )  between two normal distributions\n    \n        P ~ N(mu_p, sigma_p^2)\n        Q ~ N(mu_q, sigma_q^2)\n    \n    Parameters\n    ----------\n    mu_p, mu_q : float or array-like\n        Means of the two distributions.\n    sigma_p, sigma_q : float or array-like\n        Standard deviations (not variances!) of the two distributions.\n        Must be strictly positive.\n    \n    Returns\n    -------\n    float\n        KL divergence.  If the inputs are arrays, the divergence is summed\n        over all corresponding dimensions (i.e. treating each dimension as\n        independent and additive in KL).\n    \"\"\"\n    # Convert to np.ndarray for vectorised maths\n    mu_p   = np.asarray(mu_p,   dtype=float)\n    mu_q   = np.asarray(mu_q,   dtype=float)\n    sigma_p = np.asarray(sigma_p, dtype=float)\n    sigma_q = np.asarray(sigma_q, dtype=float)\n\n    # Basic validity check\n    if np.any(sigma_p <= 0) or np.any(sigma_q <= 0):\n        raise ValueError(\"Standard deviations must be positive.\")\n\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n\n    # Formula for univariate normals, additive across independent dimensions\n    kl_element = (np.log(sigma_q / sigma_p) +\n                  (var_p + (mu_p - mu_q) ** 2) / (2 * var_q) -\n                  0.5)\n\n    # If arrays, KL adds across dimensions\n    return float(np.sum(kl_element))\n```\n\nExplanation of the formula used (for reference):\n\n    D_KL(P || Q) = log( \u03c3_q / \u03c3_p )\n                   + [ \u03c3_p\u00b2 + (\u03bc_p \u2212 \u03bc_q)\u00b2 ] / (2 \u03c3_q\u00b2)\n                   \u2212 1/2\n\nFor multi-dimensional independent normals, the KL divergence is the sum of the univariate divergences, which is exactly what the vectorised computation above returns."}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the linear system Ax = b using the Gauss-Seidel iterative method.\n\n    Parameters\n    ----------\n    A : array-like (m \u00d7 m)\n        Coefficient matrix (assumed square and non-singular).\n    b : array-like (m,)\n        Right-hand-side vector.\n    n : int\n        Number of Gauss-Seidel iterations to perform.\n    x_ini : array-like, optional\n        Initial guess for the solution.  If None, a zero vector is used.\n\n    Returns\n    -------\n    list\n        Approximated solution vector (rounded to 4 decimals) as a Python list.\n    \"\"\"\n    # Convert inputs to NumPy arrays of float type\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).reshape(-1)\n\n    m = A.shape[0]                         # dimension of the system\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)  # default initial guess\n    else:\n        x = np.asarray(x_ini, dtype=float).reshape(-1)\n\n    # Gauss-Seidel iteration\n    for _ in range(n):\n        for i in range(m):\n            # Using the most recent values for x[0..i-1] and the old values for x[i+1..]\n            sigma_left  = np.dot(A[i, :i],  x[:i])     # updated (new) components\n            sigma_right = np.dot(A[i, i+1:], x[i+1:])  # not-yet-updated (old) components\n            x[i] = (b[i] - sigma_left - sigma_right) / A[i, i]\n\n    # Round results to 4 decimal places and return as a Python list\n    return np.round(x, 4).reshape(-1).tolist()\n```"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian Elimination\n    with partial pivoting.\n    \n    Parameters\n    ----------\n    A :   (n, n) array_like\n          Coefficient matrix.\n    b :   (n,) array_like\n          Right\u2013hand-side vector.\n    \n    Returns\n    -------\n    list\n          Solution vector rounded to 4 decimals and converted\n          to a regular Python list.\n    \"\"\"\n    # Convert to float64 numpy arrays (copy to avoid mutating inputs)\n    A = np.array(A, dtype=float).copy()\n    b = np.array(b, dtype=float).copy().flatten()\n    \n    n = b.size\n    \n    # Forward elimination with partial pivoting\n    for k in range(n-1):\n        # Pivot selection\n        pivot_row = k + np.argmax(np.abs(A[k:, k]))\n        if np.isclose(A[pivot_row, k], 0):\n            raise ValueError(\"Matrix is singular or nearly singular.\")\n        \n        # Swap rows in A and b if needed\n        if pivot_row != k:\n            A[[k, pivot_row]] = A[[pivot_row, k]]\n            b[[k, pivot_row]] = b[[pivot_row, k]]\n        \n        # Eliminate entries below the pivot\n        for i in range(k+1, n):\n            factor = A[i, k] / A[k, k]\n            A[i, k:] -= factor * A[k, k:]\n            b[i]     -= factor * b[k]\n    \n    if np.isclose(A[-1, -1], 0):\n        raise ValueError(\"Matrix is singular or nearly singular.\")\n    \n    # Back substitution\n    x = np.zeros(n)\n    for i in range(n-1, -1, -1):\n        s = np.dot(A[i, i+1:], x[i+1:])      # sum of A[i,j]*x[j] for j>i\n        x[i] = (b[i] - s) / A[i, i]\n    \n    # Round to 4 decimal places and return a Python list\n    return np.round(x, 4).tolist()\n```"}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Xavier/Glorot style scaling just to keep initial values small (optional)\n        limit = np.sqrt(1.0 / (input_size + hidden_size))\n\n        self.Wf = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n        self.Wi = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n        self.Wc = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n        self.Wo = np.random.uniform(-limit, limit, (hidden_size, input_size + hidden_size))\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Parameters\n        ----------\n        x : np.ndarray, shape (seq_len, input_size)\n            Sequence to process.  Each row is the input vector at that time-step.\n        initial_hidden_state : np.ndarray, shape (hidden_size,) or (hidden_size, 1)\n        initial_cell_state   : np.ndarray, shape (hidden_size,) or (hidden_size, 1)\n\n        Returns\n        -------\n        hidden_states : list\n            Rounded (4-dp) hidden state for every time-step, converted to python list.\n            Shape conceptually (seq_len, hidden_size)\n        final_hidden_state : list\n            Rounded (4-dp) hidden state after last step.\n        final_cell_state : list\n            Rounded (4-dp) cell state after last step.\n        \"\"\"\n\n        # Ensure 2-D column vectors for h and c\n        h_t = np.asarray(initial_hidden_state).reshape(self.hidden_size, 1).astype(float)\n        c_t = np.asarray(initial_cell_state).reshape(self.hidden_size, 1).astype(float)\n\n        seq = np.asarray(x, dtype=float)        # (seq_len, input_size)\n        seq_len = seq.shape[0]\n\n        hidden_seq = []\n\n        for t in range(seq_len):\n            x_t = seq[t].reshape(self.input_size, 1)  # column vector (input_size,1)\n\n            concat = np.vstack((h_t, x_t))            # (hidden+input, 1)\n\n            f_t = sigmoid(self.Wf @ concat + self.bf)        # forget gate\n            i_t = sigmoid(self.Wi @ concat + self.bi)        # input gate\n            c_hat_t = np.tanh(self.Wc @ concat + self.bc)    # candidate cell\n            o_t = sigmoid(self.Wo @ concat + self.bo)        # output gate\n\n            c_t = f_t * c_t + i_t * c_hat_t                 # current cell state\n            h_t = o_t * np.tanh(c_t)                        # current hidden state\n\n            hidden_seq.append(h_t.T)                        # keep as (1, hidden_size)\n\n        # Stack to (seq_len, hidden_size)\n        hidden_seq = np.vstack(hidden_seq)\n\n        # Round to 4 decimal places as required\n        hidden_seq_rounded = np.round(hidden_seq, 4)\n        final_h_rounded = np.round(h_t.T, 4)        # (1, hidden_size)\n        final_c_rounded = np.round(c_t.T, 4)\n\n        # Convert to python lists\n        return (\n            hidden_seq_rounded.tolist(),          # list of list\n            final_h_rounded.flatten().tolist(),   # single list\n            final_c_rounded.flatten().tolist(),   # single list\n        )\n```"}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom math import log\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Computes the TF-IDF matrix (docs  \u00d7 query_terms).\n\n    Parameters\n    ----------\n    corpus : List[List[str]]\n        Each sub-list represents a document (tokenised list of words).\n    query :  List[str]\n        List of words for which TF-IDF scores are needed.\n\n    Returns\n    -------\n    List[List[float]]\n        A 2-D list where element [i][j] is the TF-IDF of query[j] in\n        document i.  Values are rounded to 4 decimal places.\n    \"\"\"\n\n    # ---------- Edge-case handling ----------\n    if len(query) == 0:\n        return []\n\n    if len(corpus) == 0:\n        # Empty corpus \u21d2 nothing to score\n        return []\n\n    n_docs = len(corpus)\n\n    # ---------- Pre-compute document frequencies ----------\n    # df[word] = number of documents containing word\n    df = {term: 0 for term in query}\n    for doc in corpus:\n        # Only look at unique words per document for DF counting\n        words_in_doc = set(doc)\n        for term in query:\n            if term in words_in_doc:\n                df[term] += 1\n\n    # ---------- Pre-compute IDF with smoothing ----------\n    # Smooth by adding 1 to both numerator and denominator to avoid /0\n    idf = {}\n    for term in query:\n        idf_val = log((n_docs + 1) / (df[term] + 1)) + 1   # +1 outside log is another common smoothing\n        idf[term] = idf_val\n\n    # ---------- Build TF-IDF matrix ----------\n    tf_idf_matrix = np.zeros((n_docs, len(query)), dtype=float)\n\n    for i, doc in enumerate(corpus):\n        doc_len = len(doc)\n        if doc_len == 0:\n            # Empty document \u21d2 every TF = 0, TF-IDF stays 0 row\n            continue\n\n        # Compute raw term counts for this document once\n        counts = {}\n        for word in doc:\n            counts[word] = counts.get(word, 0) + 1\n\n        for j, term in enumerate(query):\n            # Term Frequency (normalised by document length)\n            tf = counts.get(term, 0) / doc_len\n            tf_idf_matrix[i, j] = tf * idf[term]\n\n    # ---------- Rounding ----------\n    tf_idf_matrix = np.round(tf_idf_matrix, 4)\n\n    # ---------- Return as pure Python list ----------\n    return tf_idf_matrix.tolist()\n```"}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels (0/1)\n    :param y_pred: Numpy array of predicted labels (0/1)\n    :param beta: The weight of recall relative to precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    if beta < 0:\n        raise ValueError(\"beta has to be non-negative\")\n\n    # convert to boolean arrays for easy masking\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n\n    # Confusion-matrix components\n    tp = np.sum(y_true & y_pred)          # True Positives\n    fp = np.sum(~y_true & y_pred)         # False Positives\n    fn = np.sum(y_true & ~y_pred)         # False Negatives\n\n    # Precision and Recall with safety for zero division\n    precision = tp / (tp + fp) if (tp + fp) else 0.0\n    recall    = tp / (tp + fn) if (tp + fn) else 0.0\n\n    if precision == 0.0 and recall == 0.0:\n        return 0.0\n\n    beta_sq = beta ** 2\n    f_beta = (1 + beta_sq) * precision * recall / (beta_sq * precision + recall)\n    \n    return round(f_beta, 3)\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Build a tiny RNN.\n        -------------------------------------------------\n            input_size   : dimensionality of every item x_t\n            hidden_size  : dimensionality of hidden state  h_t\n            output_size  : dimensionality of every output  y_t\n        \"\"\"\n        # remember sizes (handy in several places)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        # initialise learnable parameters\n        self.initialize_weights(input_size, hidden_size, output_size)\n\n    # ------------------------------------------------------------------\n    # 1) INITIALISATION -------------------------------------------------\n    # ------------------------------------------------------------------\n    def initialize_weights(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Weight matrices are drawn from \ud835\udcdd(0,1) and scaled by 0.01.\n        Biases are initialised to 0.\n        \"\"\"\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01    # input   -> hidden\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01   # hidden  -> hidden\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01   # hidden  -> output\n\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    # ------------------------------------------------------------------\n    # 2) FORWARD PASS ---------------------------------------------------\n    # ------------------------------------------------------------------\n    def rnn_forward(self, input_sequence):\n        \"\"\"\n        input_sequence : ndarray  (seq_len, input_size)\n\n        Returns\n        -------\n        outputs       : list of ndarray (output_size, 1) \u2013  y_1 \u2026 y_T\n        last_inputs   : list of ndarray (input_size , 1) \u2013  x_1 \u2026 x_T\n        last_hiddens  : list of ndarray (hidden_size, 1) \u2013  h_1 \u2026 h_T\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n\n        # iterate through time\n        for x_t in input_sequence:\n            x_t = x_t.reshape(-1, 1)             # make column-vector\n\n            h_t = np.tanh(self.W_xh @ x_t +      # hidden state\n                          self.W_hh @ h_prev +\n                          self.b_h)\n\n            y_t = self.W_hy @ h_t + self.b_y     # output (linear)\n\n            # bookkeeping\n            outputs.append(y_t)\n            last_inputs.append(x_t)\n            last_hiddens.append(h_t)\n\n            h_prev = h_t                         # carry state forward\n\n        return outputs, last_inputs, last_hiddens\n\n    # simple wrapper used by external callers that only need the outputs\n    def forward(self, input_sequence):\n        outputs, _, _ = self.rnn_forward(input_sequence)\n        return outputs\n\n    # ------------------------------------------------------------------\n    # 3) BACKWARD PASS (BPTT) ------------------------------------------\n    # ------------------------------------------------------------------\n    def rnn_backward(self, input_sequence, expected_output,\n                     outputs, last_inputs, last_hiddens,\n                     learning_rate=1e-2):\n        \"\"\"\n        Back-propagate the error through time and update parameters.\n\n        expected_output : ndarray (seq_len, output_size)\n        Returns\n        -------\n        total_loss : float   (aggregated \u00bd\u00b7MSE over the whole sequence)\n        \"\"\"\n\n        seq_len = len(input_sequence)\n\n        # gradient containers\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n\n        dh_next = np.zeros((self.hidden_size, 1))\n        total_loss = 0.0\n\n        # iterate backwards through time\n        for t in reversed(range(seq_len)):\n            y_pred = outputs[t]                       # (output_size, 1)\n            y_true = expected_output[t].reshape(-1, 1)\n\n            # dL/dy_t  (\u00bd\u00b7MSE \u21d2 \u2202 = y_pred \u2212 y_true)\n            dy = y_pred - y_true\n            total_loss += 0.5 * np.sum((dy) ** 2)\n\n            # output-layer gradients\n            dW_hy += dy @ last_hiddens[t].T\n            db_y += dy\n\n            # propagate into hidden state\n            dh = self.W_hy.T @ dy + dh_next           # add flow from the future\n            dh_raw = (1 - last_hiddens[t] ** 2) * dh  # tanh\u2032(u) = 1 \u2212 tanh\u00b2(u)\n\n            # gradients for hidden layer parameters\n            dW_xh += dh_raw @ last_inputs[t].T\n            prev_h = np.zeros_like(last_hiddens[0]) if t == 0 else last_hiddens[t-1]\n            dW_hh += dh_raw @ prev_h.T\n            db_h += dh_raw\n\n            # gradient that flows back to the previous time-step\n            dh_next = self.W_hh.T @ dh_raw\n\n        # optional: gradient clipping (helps stability)\n        for dparam in (dW_xh, dW_hh, dW_hy, db_h, db_y):\n            np.clip(dparam, -1, 1, out=dparam)\n\n        # SGD parameter update\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n        return total_loss\n\n    # convenience wrapper\n    def backward(self, input_sequence, expected_output,\n                 outputs, last_inputs, last_hiddens,\n                 learning_rate=1e-2):\n        return self.rnn_backward(input_sequence, expected_output,\n                                 outputs, last_inputs, last_hiddens,\n                                 learning_rate)\n\n    # ------------------------------------------------------------------\n    # Utility \u2013 one training step on a whole sequence\n    # ------------------------------------------------------------------\n    def train_on_sequence(self, input_sequence, expected_output,\n                          learning_rate=1e-2):\n        \"\"\"\n        Runs a forward pass then BPTT update.  Returns the loss.\n        \"\"\"\n        outputs, last_inputs, last_hiddens = self.rnn_forward(input_sequence)\n        loss = self.rnn_backward(input_sequence, expected_output,\n                                 outputs, last_inputs, last_hiddens,\n                                 learning_rate)\n        return loss\n```\n\nThe three requested parts are now complete:\n\n1. `initialize_weights`  \n   \u2022 Generates small random weight matrices (`*0.01`) and zero-bias vectors.\n\n2. `rnn_forward`  \n   \u2022 Iterates through the input sequence, updates the hidden state with `tanh`, stores each `(x_t, h_t, y_t)` and returns them.\n\n3. `rnn_backward`  \n   \u2022 Implements full Back-Propagation-Through-Time (BPTT):  \n     \u2013 Computes \u00bd\u00b7MSE loss and its gradient for every time-step.  \n     \u2013 Accumulates parameter gradients while moving backwards through the sequence.  \n     \u2013 Optionally clips the gradients.  \n     \u2013 Performs an SGD update with the supplied learning rate.\n\nThe helper methods `forward`, `backward`, and `train_on_sequence` provide convenient wrappers around these core routines."}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol: float=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix (m \u00d7 m)\n    :param b: Right-hand side vector (m,)\n    :param n: Maximum number of CG iterations allowed\n    :param x0: Optional initial guess (defaults to the zero vector)\n    :param tol: Euclidean-norm tolerance for \u2016r_k\u2016 = \u2016b \u2212 A x_k\u2016\n    :return: Solution vector x (rounded to 8 decimals and returned as a Python list)\n    \"\"\"\n    # Ensure numpy arrays (and float type for numerical stability)\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float)\n\n    # Basic dimension check\n    m, m2 = A.shape\n    if m != m2 or b.shape[0] != m:\n        raise ValueError(\"Incompatible matrix/vector dimensions.\")\n\n    # Initial guess\n    x = np.zeros_like(b) if x0 is None else np.asarray(x0, dtype=float).copy()\n    if x.shape != b.shape:\n        raise ValueError(\"Initial guess x0 has incorrect shape.\")\n\n    # Initial residual r0 = b \u2212 A x0\n    r = b - A @ x\n    p = r.copy()\n    rs_old = float(r @ r)          # r\u1d40 r\n\n    # Early exit if initial guess is already good enough\n    if np.sqrt(rs_old) < tol:\n        return np.round(x, 8).tolist()\n\n    for _ in range(n):\n        Ap = A @ p                     # A p_k\n        alpha = rs_old / float(p @ Ap) # \u03b1_k\n\n        x += alpha * p                 # x_{k+1}\n        r -= alpha * Ap                # r_{k+1}\n\n        rs_new = float(r @ r)          # r_{k+1}\u1d40 r_{k+1}\n        if np.sqrt(rs_new) < tol:      # Convergence test\n            break\n\n        beta = rs_new / rs_old         # \u03b2_k\n        p = r + beta * p               # p_{k+1}\n        rs_old = rs_new\n\n    return np.round(x, 8).tolist()\n```"}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Handle empty input\n    if not y:\n        return 0.0\n\n    # Convert to NumPy array for convenience\n    labels = np.asarray(y)\n\n    # Count occurrences of each class\n    _, counts = np.unique(labels, return_counts=True)\n\n    # Compute class probabilities\n    probs = counts / counts.sum()\n\n    # Gini impurity formula: 1 - \u03a3(p_i)^2\n    gini = 1.0 - np.sum(probs ** 2)\n\n    # Round to three decimal places\n    return round(float(gini), 3)\n```\n"}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    Parameters\n    ----------\n    dense_matrix : list[list[Number]]\n        A 2-D (row \u2011 major) Python list representing the dense matrix.\n\n    Returns\n    -------\n    tuple[list, list, list]\n        (values, column_indices, row_pointer) where\n            values          : non\u2013zero elements in row-major order\n            column_indices  : corresponding column indices\n            row_pointer     : cumulative counts of non-zeros per row.\n                              Always starts with 0 and has length rows + 1.\n    \"\"\"\n    # ---- Edge-case: empty matrix ------------------------------------------\n    if not dense_matrix:                       # [] or [[]]\n        return ([], [], [0])\n\n    values          = []\n    column_indices  = []\n    row_pointer     = [0]          # row_pointer[0] is always 0 in CSR\n\n    for row in dense_matrix:\n        for col_idx, element in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_idx)\n        # After processing a row, record the total #non-zeros so far\n        row_pointer.append(len(values))\n\n    return values, column_indices, row_pointer\n```"}
{"task_id": 66, "completion_id": 0, "completion": "```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: list/tuple of numbers \u2013 the vector to be projected\n    :param L: list/tuple of numbers \u2013 the line vector defining the direction\n    :return: list \u2013 projection of v onto L, each coordinate rounded to 3 dp\n    \"\"\"\n    # --- basic validation ----------------------------------------------------\n    if len(v) != len(L):\n        raise ValueError(\"Vectors v and L must have the same dimension.\")\n    if not any(L):                      # L is the zero-vector \u2192 no direction\n        raise ValueError(\"Line vector L must be non-zero.\")\n\n    # --- dot products --------------------------------------------------------\n    dot_vL = sum(vi * Li for vi, Li in zip(v, L))\n    dot_LL = sum(Li * Li for Li in L)\n\n    # --- projection ----------------------------------------------------------\n    scale = dot_vL / dot_LL            # scalar component\n    proj   = [round(scale * Li, 3) for Li in L]\n\n    return proj\n```\n\nExample usage\n```python\n>>> orthogonal_projection([3, 4], [1, 0])\n[3.0, 0.0]\n\n>>> orthogonal_projection([2, 3, 4], [1, 1, 1])\n[3.0, 3.0, 3.0]\n```\n\nThe function follows the standard formula  \nproj_L(v) = ((v\u00b7L)/(L\u00b7L)) \u00b7 L, then rounds each coordinate to three decimal places."}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    Parameters\n    ----------\n    dense_matrix : list[list]\n        Rectangular (m \u00d7 n) matrix stored as a list of rows.\n\n    Returns\n    -------\n    tuple\n        (values, row_indices, col_ptr) where\n            values       : list of all non-zero entries, column by column\n            row_indices  : list of row indices for each stored value\n            col_ptr      : list whose length is n_columns + 1;\n                           col_ptr[j] is the index in `values` where column j starts,\n                           and col_ptr[-1] == len(values)\n    \"\"\"\n    # Handle an entirely empty matrix (no rows at all)\n    if not dense_matrix:\n        return [], [], [0]\n\n    n_rows = len(dense_matrix)\n    n_cols = len(dense_matrix[0])\n\n    # Verify rectangular shape\n    for r, row in enumerate(dense_matrix):\n        if len(row) != n_cols:\n            raise ValueError(f\"Row {r} has length {len(row)}; expected {n_cols}\")\n\n    values = []          # non-zero elements (column-major)\n    row_indices = []     # corresponding row indices\n    col_ptr = [0]        # starting index of each column in `values`\n\n    for col in range(n_cols):\n        for row in range(n_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n        # After scanning one column, append current number of stored values\n        col_ptr.append(len(values))\n\n    return values, row_indices, col_ptr\n```\n\nExample\n-------\n```python\ndense = [\n    [0, 5, 0, 0],\n    [3, 0, 0, 9],\n    [0, 0, 7, 0]\n]\n\nvals, rows, ptr = compressed_col_sparse_matrix(dense)\nprint(\"values      :\", vals)   # [3, 5, 7, 9]\nprint(\"row indices :\", rows)   # [1, 0, 2, 1]\nprint(\"col pointer :\", ptr)    # [0, 1, 2, 3, 4]\n```\n\nThe result means:\n\u2022 Column 0 starts at index 0 and ends before index 1 \u2192 value 3 at row 1  \n\u2022 Column 1 starts at 1 and ends before 2          \u2192 value 5 at row 0  \n\u2022 Column 2 starts at 2 and ends before 3          \u2192 value 7 at row 2  \n\u2022 Column 3 starts at 3 and ends before 4          \u2192 value 9 at row 1"}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Return a basis of the column-space (image) of the matrix A.\n\n    Parameters\n    ----------\n    A : array_like, shape (m, n)\n        The input matrix.\n\n    Returns\n    -------\n    list\n        A list\u2010of\u2010lists representing the columns of A that form a basis\n        of its column space (rounded to 8 decimals).\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    m, n = A.shape\n    B = A.copy()                       # matrix that will be row-reduced\n    pivot_cols = []                    # indices of independent columns\n    row = 0                            # current pivot row\n    tol = 1e-12                        # numerical tolerance\n\n    for col in range(n):\n        # 1. Find a pivot in (row..m-1) in current column\n        pivot = None\n        for r in range(row, m):\n            if abs(B[r, col]) > tol:\n                pivot = r\n                break\n        if pivot is None:\n            continue                   # no pivot in this column\n\n        # 2. Bring pivot to the current row (if necessary)\n        if pivot != row:\n            B[[row, pivot]] = B[[pivot, row]]\n\n        pivot_cols.append(col)\n\n        # 3. Eliminate entries below the pivot\n        for r in range(row + 1, m):\n            factor = B[r, col] / B[row, col]\n            if abs(factor) > tol:\n                B[r, col:] -= factor * B[row, col:]\n\n        row += 1                       # move to next row\n        if row == m:                   # all rows exhausted\n            break\n\n    # Extract the independent columns from the original matrix\n    if pivot_cols:\n        basis = A[:, pivot_cols]\n        basis = np.round(basis, 8)     # round to 8 decimals\n        return basis.tolist()\n    else:                              # zero matrix \u2192 trivial subspace\n        return []\n```"}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the coefficient of determination (R-squared).\n\n    Parameters\n    ----------\n    y_true : array-like\n        Actual/observed target values.\n    y_pred : array-like\n        Predicted target values from the regression model.\n\n    Returns\n    -------\n    float\n        R-squared value, rounded to three decimal places.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    # Residual sum of squares\n    ss_res = np.sum((y_true - y_pred) ** 2)\n\n    # Total sum of squares\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n\n    # Handle the rare case where variance of y_true is zero\n    if ss_tot == 0:\n        return round(0.0, 3)\n\n    # R-squared computation\n    r2 = 1 - ss_res / ss_tot\n\n    return round(r2, 3)\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Parameters\n    ----------\n    img : list[list[int or float]]\n        2-D list where each inner list represents a row of pixel values\n        ranging from 0 (black) to 255 (white).\n\n    Returns\n    -------\n    float\n        Average brightness rounded to two decimals, or\n        -1 for any invalid input:\n          \u2022 empty image / empty rows\n          \u2022 inconsistent row lengths\n          \u2022 pixel value outside 0-255 or non-numeric\n    \"\"\"\n    # 1. Basic structural checks\n    if not img or not isinstance(img, list):         # empty image or wrong type\n        return -1\n    row_len = len(img[0])\n    if row_len == 0:                                 # first row empty\n        return -1\n\n    total = 0.0\n    count = 0\n\n    # 2. Validate rows and pixels, accumulate sum\n    for row in img:\n        # Inconsistent row length\n        if not isinstance(row, (list, tuple)) or len(row) != row_len:\n            return -1\n        for px in row:\n            # Ensure pixel is numeric\n            try:\n                val = float(px)\n            except (TypeError, ValueError):\n                return -1\n            # Pixel range check\n            if val < 0 or val > 255:\n                return -1\n            total += val\n        count += row_len\n\n    # 3. Compute and return the average brightness\n    average = total / count\n    return round(average, 2)\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Compute the Root Mean Square Error (RMSE) between two equal-shaped\n    numeric sequences.\n\n    Parameters\n    ----------\n    y_true : array-like\n        Actual/true target values.\n    y_pred : array-like\n        Predicted target values.\n\n    Returns\n    -------\n    float\n        RMSE rounded to three decimal places.\n\n    Raises\n    ------\n    ValueError\n        * The two inputs cannot be converted to numeric arrays.\n        * The arrays are empty.\n        * The arrays have mismatched shapes.\n        * The arrays contain NaN or Inf values.\n    \"\"\"\n    # 1. Convert to numpy arrays of float type\n    try:\n        y_true = np.asarray(y_true, dtype=float)\n        y_pred = np.asarray(y_pred,  dtype=float)\n    except (TypeError, ValueError):\n        raise ValueError(\"Inputs must be array-like and contain numeric values.\")\n\n    # 2. Basic sanity checks\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n    if np.isnan(y_true).any() or np.isnan(y_pred).any():\n        raise ValueError(\"Input arrays must not contain NaN values.\")\n    if np.isinf(y_true).any() or np.isinf(y_pred).any():\n        raise ValueError(\"Input arrays must not contain Inf values.\")\n\n    # 3. RMSE calculation\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_val = np.sqrt(mse)\n\n    # 4. Return rounded result\n    return round(float(rmse_val), 3)\n```\n\nExample usage:\n```python\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> rmse(y_true, y_pred)\n0.612\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Compute the Jaccard Index for two binary label arrays.\n\n    Parameters\n    ----------\n    y_true : array-like\n        Ground-truth binary labels (0/1).\n    y_pred : array-like\n        Predicted binary labels (0/1).\n\n    Returns\n    -------\n    float\n        Jaccard Index rounded to three decimal places.\n    \"\"\"\n    # Convert inputs to NumPy arrays of booleans (handles lists, tuples, etc.)\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n\n    # Intersection: 1 where both are True\n    intersection = np.logical_and(y_true, y_pred).sum()\n\n    # Union: 1 where at least one is True\n    union = np.logical_or(y_true, y_pred).sum()\n\n    # Edge case: both sets are empty (all zeros) \u2192 define similarity as 1.0\n    if union == 0:\n        return 1.000\n\n    jaccard = intersection / union\n    return round(float(jaccard), 3)\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Compute the Dice (S\u00f8rensen\u2013Dice) coefficient for two binary label arrays.\n\n    Parameters\n    ----------\n    y_true : array-like\n        Ground-truth binary labels (0/1 or False/True).\n    y_pred : array-like\n        Predicted binary labels (0/1 or False/True).\n\n    Returns\n    -------\n    float\n        Dice score rounded to 3 decimal places.\n    \"\"\"\n    # Convert inputs to boolean NumPy arrays\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n\n    # Ensure shapes match\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"`y_true` and `y_pred` must have the same shape.\")\n\n    # Compute intersection and cardinalities\n    intersection = np.logical_and(y_true, y_pred).sum()\n    positives_true = y_true.sum()\n    positives_pred = y_pred.sum()\n    denominator = positives_true + positives_pred\n\n    # Handle edge case: both arrays contain no positives\n    if denominator == 0:\n        dice = 1.0  # By convention, empty sets are a perfect match\n    else:\n        dice = 2.0 * intersection / denominator\n\n    return round(float(dice), 3)\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef _bipolar_hv(seed: int, dim: int) -> np.ndarray:\n    \"\"\"\n    Create a deterministic bipolar (+1 / \u20131) hypervector of dimensionality *dim*\n    using *seed*.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    # 0/1  ->  -1/+1\n    return rng.randint(0, 2, dim, dtype=np.int8) * 2 - 1\n\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a dataset *row* using Hyper-dimensional\n    Computing.\n    \n    Parameters\n    ----------\n    row : dict\n        Keys are feature names, values are the corresponding data values.\n    dim : int\n        Dimensionality of the hypervectors.\n    random_seeds : dict\n        Mapping from feature name -> integer seed.  The same seed is used to\n        create both the feature-name hypervector and, in a deterministic way,\n        the value hypervector so that results are fully reproducible.\n    \n    Returns\n    -------\n    list\n        Bipolar composite hypervector representing the whole row.\n    \"\"\"\n\n    if not row:                              # empty row -> all zeros\n        return [0] * dim\n\n    # Accumulator for bundled hypervector (integer majority voting)\n    bundle = np.zeros(dim, dtype=int)\n\n    for feature, value in row.items():\n        # ------------------------------------------------------------------\n        # 1.  Feature-name hypervector  (uses seed specific for the feature)\n        # ------------------------------------------------------------------\n        seed = random_seeds.get(feature, 0)          # fall back to 0 if absent\n        feat_hv = _bipolar_hv(seed, dim)\n\n        # ------------------------------------------------------------------\n        # 2.  Value hypervector\n        #     Re-uses the *same* base seed but is made unique for every value\n        #     by mixing-in the value\u2019s hash.  This keeps reproducibility while\n        #     ensuring different values get (pseudo-)orthogonal hypervectors.\n        # ------------------------------------------------------------------\n        # Bound the hash to 32-bit and combine with the feature seed\n        value_seed = (seed ^ (hash(str(value)) & 0xFFFFFFFF)) & 0xFFFFFFFF\n        value_hv = _bipolar_hv(value_seed, dim)\n\n        # ------------------------------------------------------------------\n        # 3.  Binding  (element-wise multiplication for bipolar vectors)\n        # ------------------------------------------------------------------\n        bound_hv = feat_hv * value_hv\n\n        # ------------------------------------------------------------------\n        # 4.  Bundle (add) into the row\u2019s composite hypervector\n        # ------------------------------------------------------------------\n        bundle += bound_hv\n\n    # ----------------------------------------------------------------------\n    # 5.  Majority vote to obtain final bipolar composite vector\n    # ----------------------------------------------------------------------\n    composite = np.where(bundle >= 0, 1, -1)\n\n    return composite.tolist()\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter         # import is required by the statement\ndef confusion_matrix(data):\n    \"\"\"\n    Build a 2\u2006\u00d7\u20062 confusion matrix for a binary-classification task.\n\n    Parameters\n    ----------\n    data : list[list]\n        Each inner list/tuple is [y_true, y_pred].\n\n    Returns\n    -------\n    list[list]\n        A 2\u2006\u00d7\u20062 matrix in the same layout used by\n        sklearn.metrics.confusion_matrix:\n\n                    predicted\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502  0   \u2502  1   \u2502\n        \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n        true   0  \u2502 TN   \u2502 FP   \u2502\n               1  \u2502 FN   \u2502 TP   \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        i.e. rows correspond to the actual class, columns to the prediction.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. Determine the two class labels\n    # ------------------------------------------------------------------ #\n    labels = sorted({true for true, _ in data} | {pred for _, pred in data})\n    if len(labels) != 2:\n        raise ValueError(\"Exactly two distinct class labels are required.\")\n    idx = {label: i for i, label in enumerate(labels)}   # map label -> row/col\n\n    # ------------------------------------------------------------------ #\n    # 2. Initialise 2\u00d72 zero matrix\n    # ------------------------------------------------------------------ #\n    matrix = [[0, 0],\n              [0, 0]]\n\n    # ------------------------------------------------------------------ #\n    # 3. Populate the matrix\n    # ------------------------------------------------------------------ #\n    for y_true, y_pred in data:\n        i = idx[y_true]     # row (actual)\n        j = idx[y_pred]     # col (predicted)\n        matrix[i][j] += 1\n\n    return matrix\n```\n\nHow the counts map to the usual names:\n\n\u2022 True  Negatives (TN) = matrix[0][0]  \n\u2022 False Positives (FP) = matrix[0][1]  \n\u2022 False Negatives (FN) = matrix[1][0]  \n\u2022 True  Positives (TP) = matrix[1][1]  \n\nExample\n-------\n\n```python\npairs = [[1,1], [0,0], [1,0], [0,1], [1,1]]\nprint(confusion_matrix(pairs))\n# Output: [[1, 1],\n#          [1, 2]]\n```\n\nMeaning:  \nTN=1, FP=1, FN=1, TP=2."}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Parameters\n    ----------\n    v1, v2 : np.ndarray\n        Input vectors of identical shape.\n\n    Returns\n    -------\n    float\n        Cosine similarity rounded to three decimal places.\n\n    Raises\n    ------\n    ValueError\n        If the vectors have different shapes, are empty,\n        or have zero magnitude.\n    \"\"\"\n    # Convert inputs to numpy arrays (in case lists are provided)\n    v1 = np.asarray(v1, dtype=float)\n    v2 = np.asarray(v2, dtype=float)\n\n    # Validate shapes\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape.\")\n    if v1.size == 0:\n        raise ValueError(\"Input vectors cannot be empty.\")\n\n    # Compute norms\n    norm1 = np.linalg.norm(v1)\n    norm2 = np.linalg.norm(v2)\n\n    if norm1 == 0 or norm2 == 0:\n        raise ValueError(\"Input vectors must have non-zero magnitude.\")\n\n    # Cosine similarity\n    similarity = np.dot(v1.ravel(), v2.ravel()) / (norm1 * norm2)\n\n    # Round to three decimal places\n    return round(float(similarity), 3)\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List, Tuple\n\n\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[list[list[int]], float, float, float, float]:\n    \"\"\"\n    Calculate confusion-matrix, accuracy, F1-score, specificity and\n    negative-predictive-value for a binary-classification result.\n\n    Parameters\n    ----------\n    actual     : list[int]   \u2013 ground-truth labels (0/1)\n    predicted  : list[int]   \u2013 model predictions (0/1)\n\n    Returns\n    -------\n    (\n        [[TN, FP],\n         [FN, TP]],         # confusion matrix\n        accuracy,            # float (3 d.p.)\n        f1_score,            # float (3 d.p.)\n        specificity,         # float (3 d.p.)\n        negative_pred_value  # float (3 d.p.)\n    )\n    \"\"\"\n\n    # ------------- basic validation -----------------\n    if len(actual) != len(predicted):\n        raise ValueError(\"`actual` and `predicted` must have the same length.\")\n    if not all(x in (0, 1) for x in actual + predicted):\n        raise ValueError(\"Elements of `actual` and `predicted` must be 0 or 1.\")\n\n    # ------------- confusion-matrix counts ----------\n    tn = fp = fn = tp = 0\n    for a, p in zip(actual, predicted):\n        if a == 1 and p == 1:\n            tp += 1\n        elif a == 1 and p == 0:\n            fn += 1\n        elif a == 0 and p == 1:\n            fp += 1\n        elif a == 0 and p == 0:\n            tn += 1\n\n    confusion_matrix = [[tn, fp],\n                        [fn, tp]]\n\n    total = tn + fp + fn + tp\n\n    # ------------- metrics --------------------------\n    accuracy = (tp + tn) / total if total else 0\n\n    precision = tp / (tp + fp) if (tp + fp) else 0\n    recall    = tp / (tp + fn) if (tp + fn) else 0\n    f1_score  = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0\n\n    specificity = tn / (tn + fp) if (tn + fp) else 0\n    negative_predictive_value = tn / (tn + fn) if (tn + fn) else 0\n\n    # ------------- rounding -------------------------\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n\n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate several descriptive\u2013statistics metrics for a 1-D numeric\n    iterable (list, tuple, NumPy array \u2026).\n\n    Parameters\n    ----------\n    data : 1-D array-like\n        Numeric data.\n\n    Returns\n    -------\n    dict\n        Dictionary whose keys are\n        'mean', 'median', 'mode', 'variance', 'standard_deviation',\n        '25th_percentile', '50th_percentile', '75th_percentile',\n        'interquartile_range'.\n\n        Every value is rounded to four decimal places **except** the\n        mode, which is returned exactly as it appears in the data\n        (or, when several modes exist, as a list of the un-rounded\n        modal values).\n    \"\"\"\n    # --- sanitise input -----------------------------------------------------\n    arr = np.asarray(data, dtype=float)\n\n    if arr.size == 0:\n        raise ValueError(\"`data` must contain at least one value.\")\n\n    # drop NaNs, if any\n    arr = arr[~np.isnan(arr)]\n    if arr.size == 0:\n        raise ValueError(\"`data` contains only NaNs.\")\n\n    # --- central-tendency metrics ------------------------------------------\n    mean_  = round(float(np.mean(arr)), 4)\n    median_ = round(float(np.median(arr)), 4)\n\n    # mode (keep original precision, not rounded)\n    counts = Counter(arr)\n    max_freq = max(counts.values())\n    modes = [k for k, v in counts.items() if v == max_freq]\n    mode_ = modes[0] if len(modes) == 1 else modes   # single value or list\n\n    # --- dispersion ---------------------------------------------------------\n    var_  = round(float(np.var(arr, ddof=0)), 4)  # population variance\n    std_  = round(float(np.std(arr, ddof=0)), 4)\n\n    # --- percentiles & IQR --------------------------------------------------\n    p25 = round(float(np.percentile(arr, 25)), 4)\n    p50 = round(float(np.percentile(arr, 50)), 4)\n    p75 = round(float(np.percentile(arr, 75)), 4)\n    iqr_ = round(float(p75 - p25), 4)\n\n    # --- build result -------------------------------------------------------\n    return {\n        'mean'                : mean_,\n        'median'              : median_,\n        'mode'                : mode_,\n        'variance'            : var_,\n        'standard_deviation'  : std_,\n        '25th_percentile'     : p25,\n        '50th_percentile'     : p50,\n        '75th_percentile'     : p75,\n        'interquartile_range' : iqr_\n    }\n```"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    \n    :param n: Total number of trials (non-negative integer)\n    :param k: Number of successes (non-negative integer, 0 \u2264 k \u2264 n)\n    :param p: Probability of success on each trial (0 \u2264 p \u2264 1)\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    # Basic validation\n    if not (0 <= p <= 1) or n < 0 or k < 0 or k > n:\n        return 0.0\n\n    # Combination C(n, k)\n    try:\n        comb = math.comb(n, k)  # Python 3.8+\n    except AttributeError:      # Fallback for older versions\n        def _comb(n_, k_):\n            k_ = min(k_, n_ - k_)\n            numerator = 1\n            denominator = 1\n            for i in range(1, k_ + 1):\n                numerator *= n_ - (k_ - i)\n                denominator *= i\n            return numerator // denominator\n        comb = _comb(n, k)\n\n    # Binomial probability\n    prob = comb * (p ** k) * ((1 - p) ** (n - k))\n\n    # Rounded to five decimal places\n    return round(prob, 5)\n```\n\nUsage example:\n```python\nprint(binomial_probability(10, 3, 0.5))  # 0.11719\n```"}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    :param x:        The value at which the PDF is evaluated.\n    :param mean:     The mean (\u03bc) of the distribution.\n    :param std_dev:  The standard deviation (\u03c3) of the distribution.\n    :return:         PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError(\"Standard deviation must be positive.\")\n    \n    coeff = 1.0 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coeff * math.exp(exponent)\n    \n    return round(pdf_value, 5)\n```\n\nExample usage:\n```python\nprint(normal_pdf(0, 0, 1))  # 0.39894\nprint(normal_pdf(1, 0, 1))  # 0.24197\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n\n    P(X = k) = (lam ** k) * e^(\u2212lam) / k!\n\n    Parameters\n    ----------\n    k : int\n        Number of events (must be a non-negative integer).\n    lam : float or int\n        Average rate (mean) of occurrences in the given interval (must be \u2265 0).\n\n    Returns\n    -------\n    float\n        Probability rounded to 5 decimal places.\n\n    Raises\n    ------\n    ValueError\n        If k is negative or not an integer, or if lam is negative.\n    \"\"\"\n\n    # Basic input validation\n    if not (isinstance(k, int) and k >= 0):\n        raise ValueError(\"k must be a non-negative integer.\")\n    if lam < 0:\n        raise ValueError(\"lam (\u03bb) must be non-negative.\")\n\n    # Poisson probability mass function\n    probability = (lam ** k) * math.exp(-lam) / math.factorial(k)\n\n    # Round to 5 decimal places\n    return round(probability, 5)\n```\n\nExample usage:\n```python\nprint(poisson_probability(3, 2.5))  # Output: 0.21376\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n\n    Contrast (simple range-based definition) = max_pixel_value \u2212 min_pixel_value.\n\n    Args\n    ----\n    img : numpy.ndarray\n        2-D array (H \u00d7 W) representing a grayscale image whose pixel values are\n        expected to be in the range 0\u2013255 (integer or float).\n\n    Returns\n    -------\n    float\n        The contrast value. It is zero for a completely flat image and at most\n        255 for an 8-bit image.\n\n    Raises\n    ------\n    ValueError\n        If `img` is not a 2-D numpy array.\n    \"\"\"\n    # Ensure input is a numpy array\n    img = np.asarray(img)\n\n    # Sanity check: must be 2-D (grayscale)\n    if img.ndim != 2:\n        raise ValueError(\n            f\"Expected a 2-D grayscale image, got array with shape {img.shape}\"\n        )\n\n    # Use float for arithmetic to avoid overflow with uint8\n    # (e.g., 255 - 0 = 255, but uint8 could wrap without casting)\n    img_min = float(np.min(img))\n    img_max = float(np.max(img))\n\n    contrast = img_max - img_min\n    return contrast\n```\n\nExample usage\n-------------\n```python\nif __name__ == \"__main__\":\n    # Completely flat image: contrast = 0\n    flat = np.full((4, 4), 128, dtype=np.uint8)\n    print(calculate_contrast(flat))  # 0.0\n\n    # A simple 0\u2013255 test image: contrast = 255\n    ramp = np.array([[0, 255], [30, 200]], dtype=np.uint8)\n    print(calculate_contrast(ramp))  # 255.0\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1-D array representing the first vector.\n        vec2 (numpy.ndarray): 1-D array representing the second vector.\n\n    Returns\n    -------\n        float or int:\n            The dot product of `vec1` and `vec2`.\n\n    Raises\n    ------\n        ValueError: If either input is not one-dimensional or if the vectors\n                    do not have the same length.\n        TypeError:  If the inputs cannot be interpreted as NumPy arrays.\n    \"\"\"\n    # Convert inputs to NumPy arrays (this also validates basic array-like input)\n    try:\n        vec1 = np.asarray(vec1)\n        vec2 = np.asarray(vec2)\n    except Exception as exc:\n        raise TypeError(\"Both inputs must be array-like objects that can be \"\n                        \"converted to NumPy arrays.\") from exc\n\n    # Ensure both vectors are 1-D\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both inputs must be 1-D vectors.\")\n\n    # Ensure vectors have the same length\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError(\"Vectors must be of the same length.\")\n\n    # Compute and return the dot product\n    return np.dot(vec1, vec2)\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation (polynomial feature expansion).\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The maximum degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: Nested list where each inner list contains the polynomial\n                           features of the corresponding input, rounded to 8 decimals.\n                           If `degree` is less than 0 an empty list is returned.\n    \"\"\"\n    # Guard-clause for invalid degree\n    if degree < 0:\n        return []\n\n    # Nothing to transform\n    if not data:\n        return []\n\n    # Build the Vandermonde matrix with powers 0 \u2026 degree (inclusive)\n    vander = np.vander(np.asarray(data, dtype=float),\n                       N=degree + 1,\n                       increasing=True)\n\n    # Round and convert back to plain Python lists\n    return np.round(vander, 8).tolist()\n```"}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Create positional encodings as described in \u201cAttention Is All You Need\u201d.\n    \n    Parameters\n    ----------\n    position : int\n        Maximum length of the input sequence (number of positions).\n    d_model : int\n        Dimensionality of the model / embedding size.\n    \n    Returns\n    -------\n    list  |  int\n        A `position \u00d7 d_model` list (dtype=float16) containing the positional \n        encodings, or -1 for invalid inputs.\n    \"\"\"\n    \n    # ----- Guard clauses -----\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    # ----- Compute the angles -----\n    # Shape: (position, 1)\n    pos_indices = np.arange(position)[:, np.newaxis]\n    # Shape: (1, d_model)\n    dim_indices = np.arange(d_model)[np.newaxis, :]\n    \n    # Compute the angle rates: 1 / (10000^(2i/d_model))\n    angle_rates = 1 / np.power(10000, (2 * (dim_indices // 2)) / d_model)\n    # Element-wise product gives the angles for every position & dimension\n    angle_rads = pos_indices * angle_rates\n    \n    # ----- Apply sin to even indices (0,2,4,...) and cos to odd indices (1,3,5,...) -----\n    pe = np.empty_like(angle_rads, dtype=np.float32)   # float32 for better precision during trig\n    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])          # Even dims\n    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])          # Odd dims\n    \n    # ----- Cast to float16 and return as standard Python list -----\n    return pe.astype(np.float16).tolist()\n```\n"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine whether a model is overfitting, underfitting, or a good fit.\n    \n    Parameters\n    ----------\n    training_accuracy : float\n        Training accuracy of the model (expected range: 0.0 \u2013 1.0).\n    test_accuracy : float\n        Test/validation accuracy of the model (expected range: 0.0 \u2013 1.0).\n\n    Returns\n    -------\n    int\n        1  -> Overfitting\n        -1 -> Underfitting\n        0  -> Good fit\n    \"\"\"\n    \n    # ----- Overfitting -----\n    # Training accuracy significantly higher than test accuracy\n    if (training_accuracy - test_accuracy) > 0.20:\n        return 1\n    \n    # ----- Underfitting -----\n    # Both accuracies are low\n    if training_accuracy < 0.70 and test_accuracy < 0.70:\n        return -1\n    \n    # ----- Good fit -----\n    return 0\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001,\n                   beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    \n    Adjusts the learning rate based on the moving averages of the\n    gradient and squared gradient, with bias-correction.\n    \n    Parameters\n    ----------\n    parameter : scalar or np.ndarray\n        Current parameter value \u03b8\u209c\n    grad : scalar or np.ndarray\n        Current gradient  \u2207\u03b8\u209c\n    m : scalar or np.ndarray\n        Exponential moving average of the gradient (1\u02e2\u1d57 moment)\n    v : scalar or np.ndarray\n        Exponential moving average of the squared gradient (2\u207f\u1d48 moment)\n    t : int\n        Current timestep (starts at 1)\n    learning_rate : float, optional\n        Step size \u03b1\n    beta1 : float, optional\n        Decay rate for the 1\u02e2\u1d57 moment  (default 0.9)\n    beta2 : float, optional\n        Decay rate for the 2\u207f\u1d48 moment  (default 0.999)\n    epsilon : float, optional\n        Numerical-stability term (default 1e-8)\n    \n    Returns\n    -------\n    tuple\n        (updated_parameter, updated_m, updated_v) \u2013 each rounded to\n        5 decimals and converted to list(s).\n    \"\"\"\n    \n    # Convert everything to numpy arrays (keeps scalars as 0-d arrays)\n    parameter = np.asarray(parameter, dtype=np.float64)\n    grad       = np.asarray(grad,       dtype=np.float64)\n    m          = np.asarray(m,          dtype=np.float64)\n    v          = np.asarray(v,          dtype=np.float64)\n\n    # 1. Update biased first moment estimate\n    m = beta1 * m + (1.0 - beta1) * grad\n    \n    # 2. Update biased second raw moment estimate\n    v = beta2 * v + (1.0 - beta2) * np.square(grad)\n    \n    # 3. Compute bias-corrected moments\n    m_hat = m / (1.0 - beta1 ** t)\n    v_hat = v / (1.0 - beta2 ** t)\n    \n    # 4. Parameter update\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # 5. Round to 5 decimals and convert to (nested) lists\n    param_out = np.round(parameter, 5).tolist()\n    m_out     = np.round(m,         5).tolist()\n    v_out     = np.round(v,         5).tolist()\n    \n    return param_out, m_out, v_out\n```"}
{"task_id": 88, "completion_id": 0, "completion": "Below is a **self-contained, runnable example** that shows how one might build an ultra-simplified, \u201cGPT-2-ish\u201d autoregressive generator on top of\n`load_encoder_hparams_and_params`.  \nThe goal is not to re-implement the whole GPT-2 paper, but to demonstrate\u2014in\nminiature\u2014how the main pieces (token- & positional-embeddings, causal\nmulti-head self-attention, feed-forward, layer-norm and a projection back to the\nvocabulary) fit together for generation.\n\n```python\nimport numpy as np\n\n\n# ---------------------------------------------------------------------------\n# Helper supplied in the exercise \u2013 unchanged\n# ---------------------------------------------------------------------------\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 2,     # keep tiny \u2013 only two heads\n        \"n_embd\": 10,    # embedding size\n    }\n\n    # very tiny parameter set \u2013 random initialisation\n    params = {\n        \"wte\": np.random.randn(3, 10) * 0.02,         # word-token embedding\n        \"wpe\": np.random.randn(1024, 10) * 0.02,      # positional embedding\n        # The transformer block weights will be filled on the first call below\n        \"blocks\": [],\n        # Final layer-norm gain/bias\n        \"ln_f\": {\"g\": np.ones(10), \"b\": np.zeros(10)}\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\n\n# ---------------------------------------------------------------------------\n# A couple of low-level utility functions\n# ---------------------------------------------------------------------------\ndef layer_norm(x, g, b, eps: float = 1e-5):\n    \"\"\"Applies layer normalisation over last dimension.\"\"\"\n    mu = x.mean(-1, keepdims=True)\n    var = ((x - mu) ** 2).mean(-1, keepdims=True)\n    x_hat = (x - mu) / np.sqrt(var + eps)\n    return g * x_hat + b\n\n\ndef split_heads(x, n_head):\n    \"\"\"\n    (seq, emb) -> (head, seq, head_dim)\n    \"\"\"\n    seq_len, emb = x.shape\n    head_dim = emb // n_head\n    x = x.reshape(seq_len, n_head, head_dim)\n    return np.transpose(x, (1, 0, 2))\n\n\ndef merge_heads(x):\n    \"\"\"\n    (head, seq, head_dim) -> (seq, emb)\n    \"\"\"\n    n_head, seq_len, head_dim = x.shape\n    return np.transpose(x, (1, 0, 2)).reshape(seq_len, n_head * head_dim)\n\n\ndef causal_attention(q, k, v):\n    \"\"\"\n    q, k, v : (head, seq, head_dim)\n    returns : (head, seq, head_dim)\n    \"\"\"\n    dk = q.shape[-1]\n    scores = np.matmul(q, k.transpose(0, 2, 1)) / np.sqrt(dk)  # (head, seq, seq)\n\n    # causal mask \u2013 lower-triangular\n    seq_len = scores.shape[-1]\n    mask = np.tril(np.ones((seq_len, seq_len), dtype=np.float32))\n    scores = scores * mask - 1e10 * (1 - mask)\n\n    probs = np.exp(scores - scores.max(-1, keepdims=True))\n    probs = probs / probs.sum(-1, keepdims=True)         # softmax\n    return np.matmul(probs, v)                            # (head, seq, head_dim)\n\n\n# ---------------------------------------------------------------------------\n# One ultra-small transformer block.  We initialise a *single* block.\n# ---------------------------------------------------------------------------\ndef make_tiny_block(embed_dim, n_head, seed=0):\n    rng = np.random.RandomState(seed)\n    head_dim = embed_dim // n_head\n    return {\n        # attention projections\n        \"wq\": rng.randn(embed_dim, embed_dim) * 0.02,\n        \"wk\": rng.randn(embed_dim, embed_dim) * 0.02,\n        \"wv\": rng.randn(embed_dim, embed_dim) * 0.02,\n        \"wo\": rng.randn(embed_dim, embed_dim) * 0.02,\n        # feed-forward\n        \"w1\": rng.randn(embed_dim, 4 * embed_dim) * 0.02,\n        \"w2\": rng.randn(4 * embed_dim, embed_dim) * 0.02,\n        # layer-norms\n        \"ln1_g\": np.ones(embed_dim),\n        \"ln1_b\": np.zeros(embed_dim),\n        \"ln2_g\": np.ones(embed_dim),\n        \"ln2_b\": np.zeros(embed_dim),\n        \"n_head\": n_head\n    }\n\n\ndef transformer_block(x, block):\n    \"\"\"\n    x : (seq, emb)\n    \"\"\"\n    # ---- Self-Attention sub-layer ------------------------------------------------\n    h = layer_norm(x, block[\"ln1_g\"], block[\"ln1_b\"])\n\n    q = h @ block[\"wq\"]\n    k = h @ block[\"wk\"]\n    v = h @ block[\"wv\"]\n\n    q = split_heads(q, block[\"n_head\"])\n    k = split_heads(k, block[\"n_head\"])\n    v = split_heads(v, block[\"n_head\"])\n\n    attn_out = causal_attention(q, k, v)\n    attn_out = merge_heads(attn_out)      # (seq, emb)\n    h = attn_out @ block[\"wo\"]\n\n    x = x + h                              # residual\n\n    # ---- Feed-forward sub-layer --------------------------------------------------\n    h2 = layer_norm(x, block[\"ln2_g\"], block[\"ln2_b\"])\n    h2 = h2 @ block[\"w1\"]\n    h2 = np.maximum(h2, 0)                 # ReLU\n    h2 = h2 @ block[\"w2\"]\n\n    return x + h2                          # residual\n\n\n# ---------------------------------------------------------------------------\n# The public interface requested in the assignment\n# ---------------------------------------------------------------------------\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    \"\"\"\n    Very small, didactic GPT-2-style generator.\n    \"\"\"\n    # reproducible randomness\n    np.random.seed(42)\n\n    # ----- 1. load \u201cmodel\u201d --------------------------------------------------------\n    encoder, hparams, params = load_encoder_hparams_and_params()\n\n    n_ctx  = hparams[\"n_ctx\"]\n    n_head = hparams[\"n_head\"]\n    n_embd = hparams[\"n_embd\"]\n\n    # if the tiny transformer block hasn\u2019t been created yet, do it once\n    if not params[\"blocks\"]:\n        params[\"blocks\"].append(make_tiny_block(n_embd, n_head, seed=123))\n\n    # ----- 2. encode prompt -------------------------------------------------------\n    tokens = encoder.encode(prompt)\n    if len(tokens) == 0:\n        raise ValueError(\"Prompt must contain at least one recognised token.\")\n\n    # ----- 3. autoregressive generation ------------------------------------------\n    for _ in range(n_tokens_to_generate):\n        if len(tokens) >= n_ctx:\n            break  # context window full\n\n        # -----------------------------------------------------------------\n        # 3.a. build the input matrix (seq_len, emb)\n        # -----------------------------------------------------------------\n        seq_len = len(tokens)\n        idx     = np.arange(seq_len)\n        wte     = params[\"wte\"][tokens]        # (seq, emb)\n        wpe     = params[\"wpe\"][idx]           # (seq, emb)\n        h       = wte + wpe                    # token + position\n\n        # -----------------------------------------------------------------\n        # 3.b. single transformer block forward pass\n        # -----------------------------------------------------------------\n        h = transformer_block(h, params[\"blocks\"][0])\n\n        # -----------------------------------------------------------------\n        # 3.c. final layer norm & projection to vocabulary\n        # -----------------------------------------------------------------\n        h = layer_norm(h, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n        last_hidden = h[-1]                    # only the final position\n        logits = last_hidden @ params[\"wte\"].T  # (vocab,)\n        # small softmax\n        probs = np.exp(logits - logits.max())\n        probs = probs / probs.sum()\n\n        # sample\n        next_tok = int(np.random.choice(len(probs), p=probs))\n        tokens.append(next_tok)\n\n    # ----- 4. decode & return -----------------------------------------------------\n    return encoder.decode(tokens)\n\n\n# ---------------------------------------------------------------------------\n# quick demo\n# ---------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    print(gen_text(\"hello\", 10))\n```\n\nA few remarks:\n\n1. The vocabulary is deliberately tiny (`hello`, `world`, `<UNK>`), so samples will\n   look repetitive or random.  The emphasis is on mechanics, not linguistic\n   quality.\n\n2. The model is a *single* transformer block with random weights, just big\n   enough to show token/position embeddings, causal multi-head attention,\n   feed-forward, residual connections, and two layer-norms.\n\n3. Generation is autoregressive: at each step we feed the whole current\n   sequence through the transformer, take the logits of the **last** position,\n   sample a next token, append it, and repeat.\n\nRun the script and you should see something like\n\n```\nhello world world hello world world world hello world <UNK> ...\n```\n\nbecause of the minuscule vocabulary and untrained weights. Nevertheless, the\ncode pieces mirror the high-level structure of GPT-2."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    A minimal self-attention style calculation on a 1-D sequence of crystal\n    values.\n\n    Parameters\n    ----------\n    n : int\n        Number of crystals (must match len(crystal_values)).\n    crystal_values : list/tuple/np.ndarray\n        Numeric value for each crystal.\n    dimension : int\n        Unused in this simplified setting, kept only to respect the signature.\n\n    Returns\n    -------\n    list[float]\n        The enhanced pattern (attention-weighted value) for every crystal,\n        each rounded to four decimal places.\n    \"\"\"\n\n    def softmax(x):\n        \"\"\"\n        Stable softmax applied along the last axis.\n        \"\"\"\n        x = np.asarray(x, dtype=np.float64)\n        x -= x.max()                # for numerical stability\n        exp_x = np.exp(x)\n        return exp_x / exp_x.sum()\n\n    # --- Safety / shape checks -------------------------------------------------\n    values = np.asarray(crystal_values, dtype=np.float64)\n    if values.shape[0] != n:\n        raise ValueError(\"`n` must match the number of provided crystal values.\")\n\n    # --- Self-attention style calculation -------------------------------------\n    patterns = []\n    for i in range(n):\n        # 1. Relationship (similarity/attention score) with every other crystal\n        scores = values[i] * values               # element-wise product\n\n        # 2. Normalise with softmax to obtain attention weights\n        attn_weights = softmax(scores)\n\n        # 3. Weighted sum to obtain the final pattern for crystal i\n        enhanced_value = np.dot(attn_weights, values)\n\n        patterns.append(round(float(enhanced_value), 4))\n\n    return patterns\n```\n\nExplanation of the choices made:\n1. Relationship measure: With only a single scalar per crystal, the natural dot product\n   between any two is simply their product, `values[i] * values[j]`.\n2. The `softmax` converts these raw similarities into a proper probability\n   distribution (attention weights) for each crystal *i* over all crystals *j*.\n3. The final \u201cpattern\u201d for crystal *i* is the weighted sum of original values\n   using those weights, mirroring how transformers obtain a context-dependent\n   representation through self-attention.\n4. Results are rounded to four decimal places to satisfy the output constraint.\n\nThe `dimension` argument is kept although unused so that the function signature\nmatches the requirement; in a full transformer it would define the size of\nquery/key/value projections."}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    corpus : list[str] \u2013 collection of documents\n    query  : str       \u2013 query string\n    k1, b  : float     \u2013 BM25 parameters\n    returns: list[float] \u2013 BM25 score for each document (rounded to 3 decimals)\n    \"\"\"\n    # Basic tokenisation (lower-case, whitespace split)\n    docs_tokens = [doc.lower().split() for doc in corpus]\n    query_terms = query.lower().split()\n\n    N = len(corpus)                         # number of documents\n    doc_lens = [len(toks) for toks in docs_tokens]\n    avgdl = np.mean(doc_lens) if doc_lens else 0.0\n\n    # Document frequency for every term in corpus\n    df = {}\n    for tokens in docs_tokens:\n        for term in set(tokens):            # count each doc once per term\n            df[term] = df.get(term, 0) + 1\n\n    scores = []\n    for tokens, dl in zip(docs_tokens, doc_lens):\n        tf = Counter(tokens)               # term frequency in current document\n        score = 0.0\n\n        for term in query_terms:\n            if term not in df:              # term never occurs in corpus\n                continue\n\n            # IDF component\n            idf = np.log((N - df[term] + 0.5) / (df[term] + 0.5) + 1)\n\n            # TF component with length normalisation\n            freq = tf.get(term, 0)\n            denom = freq + k1 * (1 - b + b * dl / avgdl) if avgdl else 1\n            score += idf * (freq * (k1 + 1)) / denom\n\n        scores.append(round(score, 3))\n\n    return scores\n```"}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must be of the same length.\")\n\n    # Assume the positive class is labelled as 1 (binary classification)\n    tp = fp = fn = 0\n    for t, p in zip(y_true, y_pred):\n        if p == 1 and t == 1:\n            tp += 1\n        elif p == 1 and t != 1:\n            fp += 1\n        elif p != 1 and t == 1:\n            fn += 1\n\n    # Precision, Recall and F1 calculations with zero-division guards\n    precision = tp / (tp + fp) if (tp + fp) else 0.0\n    recall    = tp / (tp + fn) if (tp + fn) else 0.0\n\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n\n    return round(f1, 3)\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    consumption_data : list/tuple with 10 daily consumption values\n    returns          : integer forecast for day-15, including 5 % safety margin\n    \"\"\"\n    # ---------- 1) Remove the known daily fluctuation ----------\n    detrended = []\n    for idx, value in enumerate(consumption_data):\n        day = idx + 1                       # days are 1-based\n        fluct = 10 * math.sin(2 * PI * day / 10)\n        detrended.append(value - fluct)\n\n    # ---------- 2) Linear regression on the detrended series ----------\n    n       = len(detrended)               # should be 10\n    xs      = list(range(1, n + 1))        # [1,2,\u2026,10]\n    sum_x   = sum(xs)\n    sum_y   = sum(detrended)\n    sum_x2  = sum(x * x for x in xs)\n    sum_xy  = sum(x * y for x, y in zip(xs, detrended))\n\n    denom   = n * sum_x2 - sum_x ** 2\n    slope   = (n * sum_xy - sum_x * sum_y) / denom\n    intercept = (sum_y - slope * sum_x) / n\n\n    # ---------- 3) Predict the base consumption for day 15 ----------\n    day15_base = slope * 15 + intercept\n\n    # ---------- 4) Add back day-15\u2019s fluctuation ----------\n    day15_fluct = 10 * math.sin(2 * PI * 15 / 10)   # sin(3\u03c0) \u2248 0\n    day15_total = day15_base + day15_fluct\n\n    # ---------- 5) Round, add 5 % safety margin, round up ----------\n    rounded      = round(day15_total)\n    with_margin  = math.ceil(rounded * 1.05)\n\n    # ---------- 6) Return final integer ----------\n    return int(with_margin)\n```"}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Convert inputs to NumPy arrays (in case Python lists are provided)\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    # Make sure shapes match\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n\n    # Compute MAE\n    mae_value = np.mean(np.abs(y_true - y_pred))\n\n    # Round to 3 decimal places\n    return round(float(mae_value), 3)\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n# ---------- helpers ----------\ndef _softmax(x, axis=-1):\n    \"\"\"Numerically-stable softmax.\"\"\"\n    x = x - np.max(x, axis=axis, keepdims=True)\n    exp = np.exp(x)\n    return exp / np.sum(exp, axis=axis, keepdims=True)\n\n\n# ---------- 1. Q / K / V ----------\ndef compute_qkv(X, W_q, W_k, W_v):\n    \"\"\"\n    X   : (m, n)  \u2013 input sequence (m tokens, n features)\n    W_* : (n, n)  \u2013 learnable linear projections\n\n    Returns\n    -------\n    Q, K, V : each (m, n)\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\n\n# ---------- 2. Scaled Dot-Product Attention ----------\ndef self_attention(Q, K, V):\n    \"\"\"\n    Q, K, V : (m, d)  \u2013 same length m, feature size d\n    Returns : (m, d)\n    \"\"\"\n    d_k = Q.shape[-1]\n    # (m, m) similarity scores\n    scores = (Q @ K.T) / np.sqrt(d_k)\n    weights = _softmax(scores, axis=-1)       # softmax over \u201ckey\u201d dimension\n    return weights @ V                        # weighted sum of values\n\n\n# ---------- 3. Multi-Head Attention ----------\ndef _split_heads(x, n_heads):\n    \"\"\"(m, n) -> (m, n_heads, depth) where depth = n // n_heads\"\"\"\n    m, n = x.shape\n    assert n % n_heads == 0, \"n must be divisible by n_heads\"\n    depth = n // n_heads\n    return x.reshape(m, n_heads, depth)\n\n\ndef _combine_heads(x):\n    \"\"\"(m, n_heads, depth) -> (m, n_heads*depth)\"\"\"\n    m, n_heads, depth = x.shape\n    return x.reshape(m, n_heads * depth)\n\n\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Perform multi-head attention (no final linear projection for brevity).\n\n    Q, K, V : (m, n)\n    n_heads : int  \u2013 number of attention heads\n\n    Returns\n    -------\n    (m, n) \u2013 concatenated outputs from all heads\n    \"\"\"\n    # Split into heads\n    Q_h = _split_heads(Q, n_heads)\n    K_h = _split_heads(K, n_heads)\n    V_h = _split_heads(V, n_heads)\n\n    # Compute attention per head\n    head_outputs = [\n        self_attention(Q_h[:, h, :], K_h[:, h, :], V_h[:, h, :])\n        for h in range(n_heads)\n    ]\n\n    # Concatenate heads and return\n    return np.concatenate(head_outputs, axis=-1)\n\n\n# ---------- Provided entrypoint ----------\ndef entrypoint(m, n, n_heads, random_seed):\n    np.random.seed(random_seed)\n    # Generate input data\n    X = np.arange(m*n).reshape(m, n)\n    X = np.random.permutation(X.flatten()).reshape(m, n)\n    # Generate weight matrices\n    W_q = np.random.randint(0, 4, size=(n, n))\n    W_k = np.random.randint(0, 5, size=(n, n))\n    W_v = np.random.randint(0, 6, size=(n, n))\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    return np.round(multi_head_attention(Q, K, V, n_heads), 4).tolist()\n```"}
{"task_id": 95, "completion_id": 0, "completion": "```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n\n    Raises:\n    ValueError: If the two lists have different lengths, contain values\n                other than 0/1, or if the coefficient is undefined\n                because one of the variables has no variance.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"Input lists must have the same length.\")\n\n    # Counters for the 2\u00d72 contingency table\n    n11 = n10 = n01 = n00 = 0\n\n    for xi, yi in zip(x, y):\n        if xi not in (0, 1) or yi not in (0, 1):\n            raise ValueError(\"Both input lists must contain only 0s and 1s.\")\n\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        else:  # xi == 0 and yi == 0\n            n00 += 1\n\n    n1_ = n11 + n10          # Total with x == 1\n    n0_ = n01 + n00          # Total with x == 0\n    n_1 = n11 + n01          # Total with y == 1\n    n_0 = n10 + n00          # Total with y == 0\n\n    denom = (n1_ * n0_ * n_1 * n_0) ** 0.5\n    if denom == 0:\n        raise ValueError(\"Phi coefficient is undefined (zero variance in at least one variable).\")\n\n    phi = (n11 * n00 - n10 * n01) / denom\n    return round(phi, 4)\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    # Piece-wise linear approximation:\n    #   f(x) = 0                   for x <= -2.5\n    #   f(x) = 1                   for x >= 2.5\n    #   f(x) = 0.2 * x + 0.5       otherwise\n    if x <= -2.5:\n        return 0.0\n    if x >= 2.5:\n        return 1.0\n    return 0.2 * x + 0.5\n```"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU (Exponential Linear Unit) activation function.\n\n    ELU(x) = x                              if x >= 0\n             alpha * (exp(x) - 1)           if x < 0\n\n    The result is rounded to 4 decimal places.\n\n    Args:\n        x (float): Input value.\n        alpha (float): Scaling factor applied to negative inputs (default: 1.0).\n\n    Returns:\n        float: ELU activation value rounded to 4 decimal places.\n    \"\"\"\n    elu_val = x if x >= 0 else alpha * (math.exp(x) - 1)\n    return round(elu_val, 4)\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    # For non-negative inputs, PReLU behaves like ReLU (returns x)\n    # For negative inputs, it scales x by the learnable parameter alpha\n    return x if x >= 0 else alpha * x\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the numerically-stable Softplus activation function.\n\n    Softplus(x) = log(1 + e**x)\n\n    Args:\n        x (float): Input value.\n\n    Returns:\n        float: Softplus value rounded to 4 decimal places.\n    \"\"\"\n    # Handle the extreme cases first\n    if math.isinf(x):\n        return float('inf') if x > 0 else 0.0\n    if math.isnan(x):\n        return float('nan')\n\n    # Numerically stable computation\n    if x > 0:\n        # For positive x, e**(-x) is small; add it to x to avoid overflow\n        value = x + math.log1p(math.exp(-x))\n    else:\n        # For negative x, e**x is small; log1p keeps precision\n        value = math.log1p(math.exp(x))\n\n    # Round to 4 decimal places as required\n    return round(value, 4)\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    The Softsign function is defined as:\n        f(x) = x / (1 + |x|)\n    It smoothly maps any real-valued input to the interval (-1, 1).\n\n    Args:\n        x (float): Input value.\n\n    Returns:\n        float: Softsign of the input, rounded to 4 decimal places.\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\n```\n"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List / 1-D array of likelihood ratios \u03c1_i = \u03c0_\u03b8(a_i|s_i) / \u03c0_\u03b8_old(a_i|s_i).\n        A:    List / 1-D array of advantage estimates \ud835\udc34_i.\n        pi_theta_old: List / 1-D array of old\u2013policy probabilities \u03c0_\u03b8_old(a_i|s_i).\n        pi_theta_ref: List / 1-D array of reference\u2013policy probabilities \u03c0_ref(a_i|s_i).\n        epsilon: PPO-style clipping parameter \u03b5.\n        beta:    Coefficient on the KL(\u03c0_\u03b8 || \u03c0_ref) penalty term.\n\n    Returns:\n        GRPO objective value (float, rounded to 6 decimals).\n    \"\"\"\n    # Convert to float64 numpy arrays\n    rhos           = np.asarray(rhos,         dtype=np.float64)\n    A              = np.asarray(A,            dtype=np.float64)\n    pi_theta_old   = np.asarray(pi_theta_old, dtype=np.float64)\n    pi_theta_ref   = np.asarray(pi_theta_ref, dtype=np.float64)\n\n    # ------------------------------------------------------------------ #\n    # 1.  Construct current policy probabilities for the sampled actions #\n    #     \u03c0_\u03b8(a_i|s_i) = \u03c1_i * \u03c0_\u03b8_old(a_i|s_i)                          #\n    # ------------------------------------------------------------------ #\n    pi_theta = rhos * pi_theta_old\n\n    # ------------------------------------------------------------------ #\n    # 2.  Clipped surrogate objective (PPO-style)                        #\n    #     L_clip = mean_i[ min(\u03c1_i * A_i , clip(\u03c1_i,1-\u03b5,1+\u03b5)*A_i) ]      #\n    # ------------------------------------------------------------------ #\n    clipped_rhos = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon)\n    surrogate    = np.minimum(rhos * A, clipped_rhos * A)\n    L_clip       = np.mean(surrogate)\n\n    # ------------------------------------------------------------------ #\n    # 3.  KL divergence between current and reference policy             #\n    #     (computed for the sampled action\u2010probabilities only)           #\n    #     KL_i = \u03c0_\u03b8 * ( log \u03c0_\u03b8  \u2013  log \u03c0_ref )                         #\n    # ------------------------------------------------------------------ #\n    # Numerical stabilisation: avoid log(0)\n    tiny = 1e-10\n    p    = np.clip(pi_theta,     tiny, 1.0)\n    q    = np.clip(pi_theta_ref, tiny, 1.0)\n    KL   = np.mean(p * (np.log(p) - np.log(q)))\n\n    # ------------------------------------------------------------------ #\n    # 4.  Final GRPO objective                                           #\n    # ------------------------------------------------------------------ #\n    objective = L_clip - beta * KL\n\n    # Return value rounded to 6 decimal places\n    return float(np.round(objective, 6))\n```"}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Swish(x) = x * sigmoid(x)\n    with sigmoid(x) = 1 / (1 + e^(\u2212x))\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to 4 decimal places.\n    \"\"\"\n    # Compute the sigmoid component\n    sigmoid_x = 1.0 / (1.0 + math.exp(-x))\n    \n    # Compute Swish\n    swish_val = x * sigmoid_x\n    \n    # Round to 4 decimal places as required\n    return round(swish_val, 4)\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value (rounded to 4 decimal places)\n    \"\"\"\n    # SELU parameters (from the original paper)\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    # Use expm1 for better numerical stability when x is negative\n    if x > 0:\n        selu_val = scale * x\n    else:\n        selu_val = scale * alpha * math.expm1(x)   # expm1(x) = exp(x) - 1\n\n    # Round to the nearest 4th decimal place\n    return round(selu_val, 4)\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary-classification prediction using Logistic Regression.\n\n    Args:\n        X      : Input feature matrix of shape (N, D)\n        weights: Weight vector of shape (D,)  (or anything broadcastable to (D,))\n        bias   : Scalar bias term\n\n    Returns:\n        List with N binary predictions (0 or 1)\n    \"\"\"\n    # Ensure weight is a 1-D array\n    weights = np.asarray(weights).reshape(-1)\n\n    # 1. Linear combination (logits)\n    logits = X @ weights + bias            # Shape: (N,)\n\n    # 2. Sigmoid to obtain probabilities\n    #    Use clipping for numerical stability in the exponent\n    logits_clipped = np.clip(logits, -709, 709)   # prevents overflow in exp\n    probabilities = 1.0 / (1.0 + np.exp(-logits_clipped))\n\n    # 3. Threshold at 0.5 to obtain binary predictions\n    predictions = (probabilities >= 0.5).astype(int)   # Shape: (N,)\n\n    # 4. Return as a plain Python list\n    return predictions.tolist()\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(\n    X: np.ndarray,\n    y: np.ndarray,\n    learning_rate: float,\n    iterations: int\n) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient\u2013descent training algorithm for Softmax regression, optimized\n    with Cross-Entropy loss.\n\n    Parameters\n    ----------\n    X : ndarray of shape (N, D)\n        Feature matrix.\n    y : ndarray of shape (N,)\n        Integer class labels ranging from 0 \u2026 (M-1).\n    learning_rate : float\n        Step size (\u03b1) for gradient descent.\n    iterations : int\n        Number of parameter-update steps.\n\n    Returns\n    -------\n    B : list[float]\n        Learned (D+1)\u00d7M parameter matrix flattened to 1-D, rounded to 4 dp.\n        The first row corresponds to the bias term.\n    losses : list[float]\n        Cross-entropy loss values collected at each iteration, rounded to 4 dp.\n    \"\"\"\n\n    # ------------------------------------------------------------------ #\n    # 1. Pre-processing\n    # ------------------------------------------------------------------ #\n    N, D = X.shape                       # N samples, D features\n    Xb = np.hstack([np.ones((N, 1)), X])  # Add bias column \u21d2 shape (N, D+1)\n\n    # Determine number of classes from the labels\n    classes = np.unique(y)\n    M = classes.size\n\n    # One-hot encoding of the labels                         (N, M)\n    Y = np.zeros((N, M))\n    Y[np.arange(N), y.astype(int)] = 1.0\n\n    # ------------------------------------------------------------------ #\n    # 2. Parameter initialisation\n    # ------------------------------------------------------------------ #\n    W = np.zeros((D + 1, M))             # (D+1) \u00d7 M weight matrix\n    losses = []\n\n    # ------------------------------------------------------------------ #\n    # 3. Gradient-descent loop\n    # ------------------------------------------------------------------ #\n    for _ in range(iterations):\n        # Forward pass -------------------------------------------------- #\n        logits = Xb @ W                                 # (N, M)\n        logits -= logits.max(axis=1, keepdims=True)     # numeric stability\n        exp_logits = np.exp(logits)\n        probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)  # (N, M)\n\n        # Loss (cross entropy) ----------------------------------------- #\n        ce_loss = -np.mean(np.log(probs[np.arange(N), y.astype(int)] + 1e-15))\n        losses.append(ce_loss)\n\n        # Gradient ------------------------------------------------------ #\n        grad = (Xb.T @ (probs - Y)) / N                 # (D+1, M)\n\n        # Parameter update --------------------------------------------- #\n        W -= learning_rate * grad\n\n    # ------------------------------------------------------------------ #\n    # 4. Rounding & return\n    # ------------------------------------------------------------------ #\n    W_rounded = np.round(W, 4).flatten().tolist()\n    losses_rounded = np.round(losses, 4).tolist()\n\n    return W_rounded, losses_rounded\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient\u2013descent training algorithm for logistic regression using\n    Binary Cross-Entropy (BCE) loss.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary target vector of shape (n_samples,).\n    learning_rate : float\n        Step size for gradient descent.\n    iterations : int\n        Number of gradient\u2013descent iterations.\n\n    Returns\n    -------\n    tuple[list[float], list[float]]\n        \u2022 List of learned coefficients (weights + bias) rounded to 4 dp.  \n        \u2022 List of BCE losses collected at every iteration rounded to 4 dp.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. Preparations\n    # ------------------------------------------------------------------ #\n    m, n = X.shape                                      # samples, features\n    X_bias = np.hstack([np.ones((m, 1)), X])            # add bias column\n    w = np.zeros(n + 1)                                 # (bias + weights)\n    eps = 1e-15                                         # for numerical safety\n    losses = []\n\n    # ------------------------------------------------------------------ #\n    # 2. Gradient\u2013descent loop\n    # ------------------------------------------------------------------ #\n    for _ in range(iterations):\n        z = X_bias @ w                                  # linear combination\n        p = 1 / (1 + np.exp(-z))                        # sigmoid predictions\n\n        # ----- Binary Cross-Entropy loss\n        loss = -np.mean(y * np.log(p + eps) +\n                        (1 - y) * np.log(1 - p + eps))\n        losses.append(loss)\n\n        # ----- Gradient of BCE with respect to parameters\n        grad = (X_bias.T @ (p - y)) / m\n\n        # ----- Parameter update\n        w -= learning_rate * grad\n\n    # ------------------------------------------------------------------ #\n    # 3. Rounding & returning\n    # ------------------------------------------------------------------ #\n    w_rounded = np.round(w, 4).tolist()\n    losses_rounded = np.round(losses, 4).tolist()\n    return w_rounded, losses_rounded\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters\n    ----------\n    Q, K, V : np.ndarray\n        Query, Key, and Value tensors.  Shapes can be\n            \u2022 (seq_len, d_k)               \u2013 single sequence\n            \u2022 (batch, seq_len, d_k)        \u2013 batched sequences\n    mask : np.ndarray\n        Boolean or 0/1 mask indicating which positions can be attended to.\n        Shapes accepted:\n            \u2022 (seq_len, seq_len)\n            \u2022 (batch, seq_len, seq_len)\n        A value of 0 (False) means the position is **masked out**.\n    \n    Returns\n    -------\n    list\n        Attention output converted to a Python list.\n    \"\"\"\n    # Ensure we have a batch dimension for simpler broadcasting\n    added_batch_dim = False\n    if Q.ndim == 2:                      # (seq_len, d_k)\n        Q = Q[None, ...]                 # -> (1, seq_len, d_k)\n        K = K[None, ...]\n        V = V[None, ...]\n        added_batch_dim = True\n    \n    batch_size, seq_len, d_k = Q.shape\n    \n    # (batch, seq_len, seq_len)\n    scores = np.matmul(Q, np.swapaxes(K, -2, -1)) / np.sqrt(d_k)\n    \n    # Prepare / broadcast mask\n    if mask.ndim == 2:\n        mask = mask[None, ...]           # -> (1, seq_len, seq_len)\n    mask = mask.astype(bool)\n    \n    # Apply mask: very negative large value where mask == False\n    scores = np.where(mask, scores, -1e9)\n    \n    # Numerically-stable softmax along last dimension\n    scores_max = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - scores_max)\n    attn_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    \n    # Weighted sum of values\n    output = np.matmul(attn_weights, V)          # (batch, seq_len, d_k)\n    \n    if added_batch_dim:\n        output = output[0]                       # remove fake batch dim\n    \n    return output.tolist()\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom math import log2\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n\n    The metric used is Shannon entropy (base-2) of the color distribution.\n    \u2022  Entropy is 0 when every apple has the same color.\n    \u2022  Entropy grows as the color distribution becomes more uniform across\n       a larger number of different colors.\n\n    The result is rounded to 4 decimal places, as required.\n    \"\"\"\n    # Empty basket \u2192 perfectly \u201cordered\u201d\n    if not apples:\n        return 0.0\n\n    n = len(apples)\n    counts = Counter(apples)\n\n    # Only one color present \u2192 entropy 0\n    if len(counts) == 1:\n        return 0.0\n\n    # Shannon entropy  H = \u2013\u03a3 p_i log\u2082 p_i\n    entropy = 0.0\n    for c in counts.values():\n        p = c / n\n        entropy -= p * log2(p)\n\n    return round(entropy, 4)\n\n\n# \u2500\u2500\u2500 quick self-check \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nif __name__ == \"__main__\":\n    tests = [\n        ([0, 0, 0, 0],                      0.0),\n        ([1, 1, 0, 0],                      1.0),\n        ([0, 1, 2, 3],                      2.0),\n        ([0, 0, 1, 1, 2, 2, 3, 3],          2.0),\n        ([0, 0, 0, 0, 0, 1, 2, 3],          1.5488),\n    ]\n    for apples, expected in tests:\n        print(apples, \"\u2192\", disorder(apples), \"(expected \u2265\", expected, \")\")\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray,\n                        gamma: np.ndarray,\n                        beta:  np.ndarray,\n                        epsilon: float = 1e-5):\n    \"\"\"\n    Layer-normalizes a 3-D tensor (batch, seq_len, feature_dim).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input of shape (B, L, D)\n    gamma : np.ndarray\n        Scale parameter of shape (D,) or broadcast-compatible with the last dim.\n    beta : np.ndarray\n        Shift parameter of shape (D,) or broadcast-compatible with the last dim.\n    epsilon : float, optional\n        Small constant for numerical stability.\n\n    Returns\n    -------\n    list\n        Layer-normalized tensor (same shape as X) rounded to 5 decimals and\n        converted to a Python list.\n    \"\"\"\n\n    # Ensure inputs are numpy arrays with float dtype\n    X     = np.asarray(X,   dtype=np.float32)\n    gamma = np.asarray(gamma, dtype=np.float32)\n    beta  = np.asarray(beta,  dtype=np.float32)\n\n    # Compute mean and variance across the feature dimension (axis=-1)\n    mean = X.mean(axis=-1, keepdims=True)                 # shape: (B, L, 1)\n    var  = X.var(axis=-1, keepdims=True)                  # shape: (B, L, 1)\n\n    # Normalize\n    X_hat = (X - mean) / np.sqrt(var + epsilon)           # shape: (B, L, D)\n\n    # Scale and shift (broadcast over batch & seq_len)\n    out = X_hat * gamma + beta                            # shape: (B, L, D)\n\n    # Round to 5 decimals and convert to list\n    return np.round(out, 5).tolist()\n```\n\nExample usage\n```python\nif __name__ == \"__main__\":\n    X = np.array([[[1., 2., 3.],\n                   [4., 5., 6.]]])        # shape (1,2,3)\n    gamma = np.ones(3)\n    beta  = np.zeros(3)\n\n    print(layer_normalization(X, gamma, beta))\n    # [[[ -1.22474, 0.0, 1.22474], [ -1.22474, 0.0, 1.22474]]]\n```"}
{"task_id": 110, "completion_id": 0, "completion": "Here is a compact, self-contained implementation of a (simplified) METEOR metric that follows the original paper\u2019s formulation\u2014unigram matching, precision/recall, weighted F-mean, and a fragmentation penalty.\n\n```python\nimport re\nimport numpy as np                   # required by the prompt (not actually used)\nfrom collections import Counter      # required by the prompt\n\ndef _tokenize(text):\n    \"\"\"\n    Very small helper: lowercase, keep only alphanumerics, split on whitespace.\n    \"\"\"\n    return re.findall(r\"\\w+\", text.lower())\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute a simplified METEOR score between `reference` and `candidate`.\n\n    Parameters\n    ----------\n    reference : str\n        Gold/ground-truth sentence.\n    candidate : str\n        Machine-translation output.\n    alpha, beta, gamma : float\n        Standard METEOR parameters. Defaults reproduce the common setting:\n        alpha=0.9 (recall-weighted F-mean), beta=3, gamma=0.5.\n\n    Returns\n    -------\n    float\n        METEOR score rounded to 3 decimals.\n    \"\"\"\n    ref_tokens = _tokenize(reference)\n    cand_tokens = _tokenize(candidate)\n\n    # Guard against empty input\n    if not ref_tokens or not cand_tokens:\n        return 0.0\n\n    # ---------- 1. Unigram alignment (greedy, keeps order when possible) ----------\n    ref_used = [False] * len(ref_tokens)\n    aligned_ref_positions = []\n    last_pos = -1\n\n    for tok in cand_tokens:\n        found = -1\n\n        # Prefer a match that keeps monotonic order (> last_pos)\n        for idx in range(last_pos + 1, len(ref_tokens)):\n            if not ref_used[idx] and ref_tokens[idx] == tok:\n                found = idx\n                break\n\n        # Fallback: any remaining unmatched token\n        if found == -1:\n            for idx in range(len(ref_tokens)):\n                if not ref_used[idx] and ref_tokens[idx] == tok:\n                    found = idx\n                    break\n\n        if found != -1:\n            ref_used[found] = True\n            aligned_ref_positions.append(found)\n            last_pos = found\n\n    m = len(aligned_ref_positions)          # number of unigram matches\n    if m == 0:\n        return 0.0\n\n    # ---------- 2. Precision, Recall, F-mean ----------\n    precision = m / len(cand_tokens)\n    recall    = m / len(ref_tokens)\n    f_mean    = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n\n    # ---------- 3. Fragmentation penalty ----------\n    chunks = 1\n    for i in range(1, m):\n        if aligned_ref_positions[i] != aligned_ref_positions[i - 1] + 1:\n            chunks += 1\n    frag_ratio = chunks / m\n    penalty = gamma * (frag_ratio ** beta)\n\n    # ---------- 4. Final METEOR score ----------\n    score = (1 - penalty) * f_mean\n    return round(score, 3)\n```\n\nUsage example:\n```python\nref = \"the cat sat on the mat\"\ncand = \"the cat is on the mat\"\nprint(meteor_score(ref, cand))   # -> 0.807\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) between two events.\n\n    Parameters\n    ----------\n    joint_counts : int or float\n        Number of times the two events occurred together.\n    total_counts_x : int or float\n        Number of times event X occurred.\n    total_counts_y : int or float\n        Number of times event Y occurred.\n    total_samples : int or float\n        Total number of observations / samples.\n\n    Returns\n    -------\n    float\n        PMI rounded to 3 decimal places. If any of the required counts is zero\n        (making PMI undefined), the function returns 0.0 by convention.\n    \"\"\"\n    # Guard against invalid counts that would make PMI undefined\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0:\n        return 0.0\n\n    # PMI formula in log-base-2:\n    # PMI(x, y) = log2( P(x, y) / (P(x) * P(y)) )\n    #            = log2( (joint_counts / total_samples) /\n    #                    ((total_counts_x / total_samples) *\n    #                     (total_counts_y / total_samples)) )\n    #            = log2( joint_counts * total_samples /\n    #                    (total_counts_x * total_counts_y) )\n    ratio = (joint_counts * total_samples) / (total_counts_x * total_counts_y)\n    pmi_value = np.log2(ratio)\n\n    # Round to 3 decimal places as required\n    return round(float(pmi_value), 3)\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Scale a list of integers to the range [0, 1] using Min-Max Normalization.\n\n    Parameters\n    ----------\n    x : list[int]\n        The input list of integers.\n\n    Returns\n    -------\n    list[float]\n        A list of floats, each rounded to 4 decimal places, representing\n        the normalized values.\n    \"\"\"\n    if not x:                       # Empty list -> nothing to normalize\n        return []\n\n    mn, mx = min(x), max(x)\n\n    # If all elements are identical, avoid division by zero\n    if mn == mx:\n        return [0.0 for _ in x]\n\n    rng = mx - mn\n    return [round((xi - mn) / rng, 4) for xi in x]\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Simple residual (ResNet-style) block.\n    \n    Parameters\n    ----------\n    x  : np.ndarray (1-D)\n        Input vector.\n    w1 : np.ndarray (2-D)\n        First weight matrix. Its shape must be (x.size, hidden_units).\n    w2 : np.ndarray (2-D)\n        Second weight matrix. Its shape must be (hidden_units, x.size).\n    \n    Returns\n    -------\n    list\n        Output vector after the residual block, rounded to 4 decimal places.\n    \"\"\"\n\n    # ----- Helper: ReLU -----\n    relu = lambda z: np.maximum(0, z)\n\n    # ----- Forward pass -----\n    h1 = relu(x @ w1)       # First linear layer + ReLU\n    h2 = relu(h1 @ w2)      # Second linear layer + ReLU\n    out = relu(h2 + x)      # Shortcut connection + final ReLU\n\n    # ----- Round & return as list -----\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3-D NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input array of shape (height, width, channels).\n\n    Returns\n    -------\n    np.ndarray\n        1-D array of shape (channels,) where each element is the mean\n        of all spatial locations (height \u00d7 width) in the corresponding channel.\n\n    Raises\n    ------\n    ValueError\n        If `x` is not a 3-D array.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError(\n            f\"Expected a 3-D tensor of shape (H, W, C); got array with shape {x.shape}\"\n        )\n\n    # Average over the spatial dimensions (height and width).\n    return x.mean(axis=(0, 1))\n```\n\nExample\n-------\n```python\n>>> feature_maps = np.array([[[1, 2],\n...                           [3, 4]],\n...                          [[5, 6],\n...                           [7, 8]]], dtype=float)  # shape (2, 2, 2)\n>>> global_avg_pool(feature_maps)\narray([4., 5.])\n```\nThe result `[4., 5.]` is obtained because  \nchannel 0: (1 + 3 + 5 + 7) / 4 = 4  \nchannel 1: (2 + 4 + 6 + 8) / 4 = 5"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, \n                        gamma: np.ndarray, \n                        beta:  np.ndarray, \n                        epsilon: float = 1e-5):\n    \"\"\"\n    Batch\u2013Normalization for BCHW input.\n\n    Parameters\n    ----------\n    X       : ndarray, shape (B, C, H, W)\n              Mini\u2013batch of feature-maps.\n    gamma   : ndarray, shape (C,) or broadcastable to (1,C,1,1)\n              Per\u2013channel scale factors.\n    beta    : ndarray, shape (C,) or broadcastable to (1,C,1,1)\n              Per\u2013channel shift factors.\n    epsilon : float\n              Numerical stability term.\n\n    Returns\n    -------\n    list\n        Normalized, scaled and shifted tensor rounded to 4 decimals.\n    \"\"\"\n\n    # ---- ensure np.float64 for safe computation --------------------------------\n    X     = np.asarray(X,     dtype=np.float64)\n    gamma = np.asarray(gamma, dtype=np.float64)\n    beta  = np.asarray(beta,  dtype=np.float64)\n\n    # ---- reshape gamma / beta to (1, C, 1, 1) for broadcasting ----------------\n    if gamma.ndim == 1:\n        gamma = gamma.reshape(1, -1, 1, 1)\n    elif gamma.ndim == 3:                    # (C,1,1) \u2192 (1,C,1,1)\n        gamma = gamma.reshape(1, *gamma.shape)\n        \n    if beta.ndim == 1:\n        beta = beta.reshape(1, -1, 1, 1)\n    elif beta.ndim == 3:                     # (C,1,1) \u2192 (1,C,1,1)\n        beta = beta.reshape(1, *beta.shape)\n\n    # ---- compute mean & variance over (B,H,W) for each channel -----------------\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)\n    var  = X.var (axis=(0, 2, 3), keepdims=True)\n\n    # ---- normalize, then apply scale (gamma) and shift (beta) ------------------\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    Y     = gamma * X_hat + beta\n\n    # ---- round to 4 decimals and return as list --------------------------------\n    return np.round(Y, 4).tolist()\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the value of the derivative of a single-term polynomial c * x^n\n    at a given point x.\n\n    Parameters\n    ----------\n    c : float\n        Coefficient of the polynomial term.\n    x : float\n        Point at which the derivative is evaluated.\n    n : float\n        Exponent (power) of x in the term.\n\n    Returns\n    -------\n    float\n        The derivative value, rounded to 4 decimal places.\n    \"\"\"\n    # Special-case n == 0 to avoid 0 * x**(-1) which would raise\n    # ZeroDivisionError for x == 0.\n    if n == 0:\n        return 0.0\n\n    derivative_value = c * n * (x ** (n - 1))\n    # Round to 4 decimal places as required\n    return round(derivative_value, 4)\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the span of the given 2-D vectors\n    using the Gram\u2013Schmidt process.\n\n    Parameters\n    ----------\n    vectors : list[list[float]]\n        A list of 2-D column-vectors (length-2 lists or tuples).\n    tol : float, optional\n        Tolerance below which a vector is treated as the zero vector\n        (i.e. not linearly independent from the current basis).\n\n    Returns\n    -------\n    list[list[float]]\n        A list of orthonormal basis vectors rounded to 4 decimals.\n    \"\"\"\n    if not vectors:\n        return []\n\n    # Make sure every vector is 2-D\n    for v in vectors:\n        if len(v) != 2:\n            raise ValueError(\"All input vectors must be 2-dimensional.\")\n\n    basis = []                              # will hold the orthonormal vectors\n    for v in vectors:\n        v = np.asarray(v, dtype=float)\n\n        # Subtract the projections onto the basis vectors found so far\n        for b in basis:\n            v = v - np.dot(v, b) * b\n\n        # Keep the vector only if it's linearly independent\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            basis.append(v / norm)          # normalise and store\n\n        # Early exit: in 2-D we can have at most 2 independent vectors\n        if len(basis) == 2:\n            break\n\n    # Round each component to 4 decimals and convert to plain Python lists\n    return [np.round(b, 4).tolist() for b in basis]\n```\n\nExample usage\n-------------\n```python\nvecs = [[1, 1], [1, -1]]\nprint(orthonormal_basis(vecs))\n# Possible output (signs can differ):\n# [[0.7071, 0.7071], [0.7071, -0.7071]]\n```\n\nThe function:\n1. Iterates through each input vector.\n2. Orthogonalises it against the basis found so far.\n3. Adds it to the basis if its post-projection norm exceeds `tol`.\n4. Normalises, rounds to 4 decimal places, and returns the resulting orthonormal set."}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n\n    Parameters\n    ----------\n    a, b : array-like of length 3\n        Input vectors.\n\n    Returns\n    -------\n    list\n        Cross product rounded to 4 decimal places.\n    \"\"\"\n    # Convert to NumPy arrays of type float\n    a = np.asarray(a, dtype=float)\n    b = np.asarray(b, dtype=float)\n\n    # Validate shapes\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError(\"Both input vectors must contain exactly three elements.\")\n\n    # Compute, round, and return as list\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations Ax = b with Cramer's Rule.\n    \n    Parameters\n    ----------\n    A : (n, n) array_like\n        Coefficient matrix.\n    b : (n,)  array_like\n        Constant terms.\n    \n    Returns\n    -------\n    list\n        Solution vector rounded to 4 decimal places,\n        or -1 if the system has no unique solution.\n    \"\"\"\n    # convert inputs to numpy arrays (float type)\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).flatten()     # make sure b is a 1-D vector\n    \n    # basic dimension validation\n    if A.ndim != 2 or A.shape[0] != A.shape[1] or b.size != A.shape[0]:\n        raise ValueError(\"A must be square and compatible with b.\")\n    \n    n = A.shape[0]\n    det_A = np.linalg.det(A)\n    \n    # no unique solution if determinant is 0 (or numerically close to 0)\n    if np.isclose(det_A, 0.0):\n        return -1\n    \n    # container for results\n    x = np.empty(n)\n    \n    # Cramer's rule: replace each column i with vector b\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x[i] = det_Ai / det_A\n    \n    # round each component to 4 decimal places\n    x_rounded = np.round(x, 4)\n    \n    # optional: turn very small numbers like -0.0 into 0.0 for cleanliness\n    x_rounded[np.isclose(x_rounded, 0)] = 0.0\n    \n    return x_rounded.tolist()\n```\n\nThe function:\n1. Converts the inputs to floating-point NumPy arrays.\n2. Verifies the matrix is square and matches the size of vector `b`.\n3. Computes the determinant of `A`. If it is (numerically) zero, the system lacks a unique solution and `-1` is returned.\n4. For every variable, it constructs a modified matrix with the corresponding column replaced by `b`, computes its determinant, and obtains the variable via `det(A_i) / det(A)`.\n5. Rounds each value to 4 decimal places and converts the NumPy array to a regular Python list before returning."}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Compute the Bhattacharyya distance between two discrete probability\n    distributions.\n\n    Parameters\n    ----------\n    p, q : list[float]\n        Lists of non-negative numbers representing (possibly un-normalised)\n        probability masses.\n\n    Returns\n    -------\n    float\n        Bhattacharyya distance rounded to 4 decimal places.\n        If the lists are empty, of different length, contain negatives,\n        or have zero total mass, 0.0 is returned.\n    \"\"\"\n    # Basic sanity checks\n    if not p or not q or len(p) != len(q):\n        return 0.0\n\n    p_arr = np.asarray(p, dtype=float)\n    q_arr = np.asarray(q, dtype=float)\n\n    # Validity checks: non-negative entries and non-zero totals\n    if np.any(p_arr < 0) or np.any(q_arr < 0):\n        return 0.0\n    p_sum, q_sum = p_arr.sum(), q_arr.sum()\n    if p_sum == 0 or q_sum == 0:\n        return 0.0\n\n    # Normalise to make them proper probability distributions\n    p_arr /= p_sum\n    q_arr /= q_sum\n\n    # Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p_arr * q_arr))\n\n    # If the coefficient is zero, the distance is infinite\n    if bc == 0:\n        return float(\"inf\")\n\n    # Bhattacharyya distance\n    distance = -np.log(bc)\n\n    # Round to 4 decimal places\n    return round(distance, 4)\n```"}
{"task_id": 121, "completion_id": 0, "completion": "```python\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\ndef vector_sum(a: List[Number], b: List[Number]) -> Union[List[Number], int]:\n    \"\"\"\n    Compute the element-wise sum of two vectors (lists).\n\n    Parameters\n    ----------\n    a : list[int | float]\n        First vector.\n    b : list[int | float]\n        Second vector.\n\n    Returns\n    -------\n    list[int | float]\n        Element-wise sum of `a` and `b` if they have the same length.\n    int\n        -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    # Basic validation: both must be lists and lengths must match\n    if not isinstance(a, list) or not isinstance(b, list) or len(a) != len(b):\n        return -1\n\n    # Compute element-wise sum\n    return [x + y for x, y in zip(a, b)]\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Compute the REINFORCE policy-gradient estimate.\n\n    Parameters\n    ----------\n    theta     : (num_states, num_actions) 2-D numpy.array \u2013 the policy parameters.\n    episodes  : list of episodes. Each episode is a list of (state, action, reward).\n\n    Returns\n    -------\n    A (num_states, num_actions) python list containing the averaged policy-gradient,\n    rounded to 4 decimal places.\n    \"\"\"\n    num_states, num_actions = theta.shape\n\n    # ---------- 1. Soft-max policy for every state ----------\n    # \u03c0(a | s)  =  exp(\u03b8[s, a]) / \u03a3_a' exp(\u03b8[s, a'])\n    policy = np.empty_like(theta, dtype=float)\n    for s in range(num_states):\n        logits = theta[s] - np.max(theta[s])            # numerical stability\n        exp_logits = np.exp(logits)\n        policy[s] = exp_logits / np.sum(exp_logits)\n\n    # ---------- 2. Accumulate gradients ----------\n    grad = np.zeros_like(theta, dtype=float)\n\n    for ep in episodes:\n        T = len(ep)\n        # ---- 2a.  Compute undiscounted returns G_t ----\n        returns = np.zeros(T, dtype=float)\n        G = 0.0\n        for t in reversed(range(T)):\n            _, _, r = ep[t]\n            G += r\n            returns[t] = G\n\n        # ---- 2b.  Add \u2211_t G_t \u2207\u03b8 log \u03c0(a_t|s_t) ----\n        for t, (s, a, _) in enumerate(ep):\n            G_t = returns[t]\n\n            # For the visited state-action pair the gradient of the log-policy is:\n            # \u2207\u03b8[s, k] log \u03c0(a|s) = 1 - \u03c0(a|s)  if k == a\n            #                       -\u03c0(k|s)     otherwise\n            grad[s] -= G_t * policy[s]          # -G_t * \u03c0(k|s)  for all k\n            grad[s, a] += G_t                   # add G_t to the chosen action \u2192 (1-\u03c0) term\n\n    # ---------- 3. Average over episodes ----------\n    grad /= max(len(episodes), 1)\n\n    # ---------- 4. Round and return as python list ----------\n    grad = np.round(grad, 4)\n    return grad.tolist()\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Compare the per-token (or per sample) FLOPs of\n      \u2022 a standard dense feed-forward layer (d_in \u2192 d_out) and\n      \u2022 a sparsely\u2013gated Mixture-of-Experts (MoE) layer that\n        \u2013 uses a softmax gate (linear projection d_in \u2192 n_experts)\n        \u2013 activates k_active (top-k) experts, each an identical\n          dense projection d_in \u2192 d_out\n        \u2013 linearly combines the k_active outputs.\n\n    Parameters\n    ----------\n    n_experts : int   \u2013 total number of experts in the MoE layer\n    k_active  : int   \u2013 number of experts actually executed (k in top-k)\n    d_in      : int   \u2013 input dimension\n    d_out     : int   \u2013 output dimension\n\n    Returns\n    -------\n    dict with keys\n      dense_flops     : float \u2013 FLOPs of the plain dense layer\n      moe_flops       : float \u2013 FLOPs of the MoE layer\n      savings_percent : float \u2013 100 * (1 \u2013 moe/dense)\n    All numbers are rounded to 1 decimal place.\n    \"\"\"\n\n    # ---------- Dense layer ---------------------------------------------\n    #   multiplies: d_in * d_out\n    #   adds      : d_in * d_out          (running total)  \u2248 same magnitude\n    # \u2192 total \u2248 2 * d_in * d_out\n    dense_flops = 2.0 * d_in * d_out\n\n    # ---------- MoE layer -----------------------------------------------\n    # 1. Gating network  (d_in \u2192 n_experts linear + softmax)\n    gate_flops = 2.0 * d_in * n_experts   # mult + add\n    # 2. k_active experts, each dense (d_in \u2192 d_out)\n    experts_flops = k_active * 2.0 * d_in * d_out\n    # 3. Weighted combination of k_active outputs (k_active multiplies + adds per dim)\n    combine_flops = k_active * d_out      # cheap, but keep for completeness\n\n    moe_flops = gate_flops + experts_flops + combine_flops\n\n    # ---------- Savings --------------------------------------------------\n    savings_percent = 100.0 * (1.0 - moe_flops / dense_flops)\n\n    # ---------- Round results -------------------------------------------\n    dense_flops     = round(dense_flops, 1)\n    moe_flops       = round(moe_flops,   1)\n    savings_percent = round(savings_percent, 1)\n\n    return {\n        'dense_flops':     dense_flops,\n        'moe_flops':       moe_flops,\n        'savings_percent': savings_percent\n    }\n```\n\nExample\n--------\n```python\nprint(compute_efficiency(n_experts=64, k_active=4, d_in=2048, d_out=2048))\n```\n\nOutput\n```\n{'dense_flops': 8388608.0,\n 'moe_flops':   10493952.0,\n 'savings_percent': -25.1}\n```\n\nWith only four active experts the MoE is still more expensive than one dense layer when the gating network is large compared to the experts.  \nIf we shrink the gate (`n_experts=64, k_active=4, d_in=256, d_out=2048`) we get substantial savings:\n\n```\n{'dense_flops': 1048576.0,\n 'moe_flops':    851968.0,\n 'savings_percent': 18.7}\n```\n\nYou can plug in any configuration to measure the computational efficiency."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,          # [batch, d_model]\n    W_g: np.ndarray,        # [d_model, n_experts]   \u2013 \u201cclean\u201d gating weights\n    W_noise: np.ndarray,    # [d_model, n_experts]   \u2013 weights that generate per-expert noise std-devs\n    N: np.ndarray,          # [batch, n_experts]     \u2013 already-sampled N(0,1) noise\n    k: int                  # how many experts survive the sparsity mask\n):\n    \"\"\"\n    Implements the \u2018Noisy Top-K\u2019 gating from Shazeer et al., 2017.\n\n    Steps\n    -----\n    1. clean_logits  = X @ W_g\n    2. noise_std     = softplus(X @ W_noise)\n    3. noisy_logits  = clean_logits + noise_std * N\n       (only used for selecting the k experts)\n    4. For every sample pick the k indices with the largest noisy_logits.\n    5. On those k indices compute a softmax of the *clean* logits.\n       All other positions are 0.\n    6. Round the result to 4 decimal places and return a Python list.\n    \"\"\"\n\n    batch, n_experts = X.shape[0], W_g.shape[1]\n    k = max(1, min(k, n_experts))                       # safety-clamp k\n\n    # 1. clean logits\n    clean_logits = X @ W_g                              # shape: [B, E]\n\n    # 2. per-expert noise standard deviation (softplus guarantees positivity)\n    noise_std = np.log1p(np.exp(X @ W_noise))           # softplus(x) = ln(1+e^x)\n\n    # 3. add (pre-sampled) noise\n    noisy_logits = clean_logits + noise_std * N         # shape: [B, E]\n\n    # 4\u20135. Top-k selection + sparse softmax on clean_logits\n    gating = np.zeros_like(clean_logits)                # [B, E]\n\n    for b in range(batch):\n        # indices of the k largest noisy logits\n        topk_idx = np.argpartition(noisy_logits[b], -k)[-k:]\n\n        # softmax on clean logits restricted to those indices\n        sel_logits = clean_logits[b, topk_idx]\n        sel_logits -= sel_logits.max()                  # numerical stability\n        probs = np.exp(sel_logits)\n        probs /= probs.sum()\n\n        gating[b, topk_idx] = probs                     # fill sparse row\n\n    # 6. rounding + conversion\n    gating = np.round(gating, 4)\n    return gating.tolist()\n```"}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\nnp.random.seed(42)\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Sparse Mixture-of-Experts layer with softmax gating and top-k routing.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input tensor of shape (batch_size, input_dim).\n    We : np.ndarray\n        Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg : np.ndarray\n        Gating weight matrix of shape (input_dim, n_experts).\n    n_experts : int\n        Number of experts (must match We.shape[0]).\n    top_k : int\n        Number of experts to route each token to (top-k).\n\n    Returns\n    -------\n    list\n        MoE output (rounded to 4 decimals) converted to a regular Python list.\n    \"\"\"\n    # ------------- sanity checks -------------\n    assert We.shape[0] == n_experts, \"We and n_experts mismatch\"\n    assert Wg.shape[1] == n_experts, \"Wg and n_experts mismatch\"\n    assert 1 <= top_k <= n_experts, \"top_k must be in [1, n_experts]\"\n    batch_size, input_dim = x.shape\n    _, _, output_dim = We.shape\n\n    # ------------- softmax gating -------------\n    logits = x @ Wg                              # shape: (batch_size, n_experts)\n    logits_max = np.max(logits, axis=1, keepdims=True)\n    exp_logits = np.exp(logits - logits_max)     # numerically stable\n    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n\n    # ------------- top-k routing -------------\n    top_idx = np.argsort(probs, axis=1)[:, -top_k:]   # indices of k highest probs\n    out = np.zeros((batch_size, output_dim))\n\n    for b in range(batch_size):\n        idxs = top_idx[b]            # selected experts for token b\n        sel_probs = probs[b, idxs]\n\n        # re-normalise so that selected probabilities sum to 1\n        norm_sel_probs = sel_probs / np.sum(sel_probs)\n\n        for coef, e_idx in zip(norm_sel_probs, idxs):\n            # (input_dim,) @ (input_dim, output_dim) -> (output_dim,)\n            expert_out = x[b] @ We[e_idx]\n            out[b] += coef * expert_out\n\n    # ------------- rounding / formatting -------------\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\nnp.random.seed(42)\n\ndef group_normalization(X: np.ndarray,\n                        gamma: np.ndarray,\n                        beta:  np.ndarray,\n                        num_groups: int,\n                        epsilon: float = 1e-5):\n    \"\"\"\n    Group Normalization for a 4-D tensor  (B, C, H, W).\n\n    Parameters\n    ----------\n    X          : input tensor with shape (B, C, H, W)\n    gamma, beta: scale and shift (either shape (C,)  or  (1,C,1,1)  or  (C,1,1))\n    num_groups : number of channel groups (must exactly divide C)\n    epsilon    : small constant for numerical stability\n\n    Returns\n    -------\n    Python list of the normalized, scaled and shifted tensor\n    with all elements rounded to 4 decimal places.\n    \"\"\"\n    if X.ndim != 4:\n        raise ValueError(\"X must be a 4-D tensor of shape (B,C,H,W)\")\n    \n    B, C, H, W = X.shape\n    if C % num_groups != 0:\n        raise ValueError(\"num_groups ({}) must divide the number of channels ({})\"\n                         .format(num_groups, C))\n    \n    G = num_groups\n    # ------------------------------------------------------------------\n    # 1. reshape so each group becomes a separate dimension\n    #    (B, G, C//G, H, W)\n    # ------------------------------------------------------------------\n    X_grouped = X.reshape(B, G, C // G, H, W)\n    \n    # 2. compute mean & variance over (2,3,4)  =  (channels within group, H, W)\n    mean = X_grouped.mean(axis=(2, 3, 4), keepdims=True)\n    var  = X_grouped.var(axis=(2, 3, 4), keepdims=True)\n    \n    # 3. normalize\n    X_norm = (X_grouped - mean) / np.sqrt(var + epsilon)\n    \n    # 4. restore original shape (B,C,H,W)\n    X_norm = X_norm.reshape(B, C, H, W)\n    \n    # 5. prepare gamma and beta to broadcast correctly\n    #    Accept shapes  (C,) , (1,C,1,1) , (C,1,1)\n    if gamma.ndim == 1:               # (C,)\n        gamma_b = gamma.reshape(1, C, 1, 1)\n    else:                             # assume already broadcastable\n        gamma_b = gamma\n    \n    if beta.ndim == 1:                # (C,)\n        beta_b = beta.reshape(1, C, 1, 1)\n    else:                             # assume already broadcastable\n        beta_b = beta\n    \n    # 6. scale and shift\n    out = X_norm * gamma_b + beta_b\n    \n    # 7. round to 4 decimals and return as python list\n    out = np.round(out, 4)\n    return out.tolist()\n```"}
{"task_id": 127, "completion_id": 0, "completion": "Here\u2019s a compact, self-contained routine that starts from any point, performs gradient descent on  \n\n\u2003f(x) = x\u2074 \u2013 3x\u00b3 + 2  \n\nand guarantees that the point it returns is in fact a (local-and-for-this-problem global) minimum.  \nIt stops when the update becomes smaller than tolerance, but only accepts the stop if the Hessian\n(f\u2009\u2033(x)) is positive \u2013 this prevents the algorithm from accidentally halting at the flat inflection\npoint x = 0.\n\n```python\nimport numpy as np\n\n# -------------  Helper functions -------------\ndef _f(x: float) -> float:\n    return x**4 - 3*x**3 + 2\n\ndef _grad(x: float) -> float:          # f'(x)\n    return 4*x**3 - 9*x**2\n\ndef _hess(x: float) -> float:          # f''(x)\n    return 12*x**2 - 18*x\n\n\n# -------------  Main solver -------------\ndef find_treasure(start_x: float,\n                  learning_rate: float = 0.1,\n                  tolerance: float = 1e-6,\n                  max_iters: int = 10_000) -> float:\n    \"\"\"\n    Gradient\u2013descent search for the minimum of\n        f(x) = x^4 - 3x^3 + 2\n\n    Parameters\n    ----------\n    start_x       : initial guess\n    learning_rate : step size for gradient descent\n    tolerance     : |x_{k+1} - x_k| threshold for convergence\n    max_iters     : safety cap on iterations\n\n    Returns\n    -------\n    float : the x-coordinate of the minimum, rounded to 4 dp\n    \"\"\"\n    x = float(start_x)\n\n    for _ in range(max_iters):\n        g = _grad(x)\n\n        # 1. Test if we are *numerically* stationary\n        if abs(g) < tolerance:\n            # Accept only if curvature is positive (true minimum)\n            if _hess(x) > 0:\n                break\n            # Otherwise nudge away from the saddle/inflection\n            x += np.random.uniform(-1.0, 1.0)\n            continue\n\n        # 2. Standard gradient-descent step\n        x_new = x - learning_rate * g\n\n        # 3. Convergence check\n        if abs(x_new - x) < tolerance:\n            x = x_new\n            if _hess(x) > 0:      # verify it's a minimum\n                break\n            else:                 # still a saddle, keep moving\n                x += np.random.uniform(-1.0, 1.0)\n                continue\n\n        x = x_new\n\n    return round(x, 4)\n\n\n# -------------  Quick demo -------------\nif __name__ == \"__main__\":\n    # Start from a bunch of random places to show robustness\n    rng = np.random.default_rng(0)\n    for start in rng.uniform(-10, 10, size=5):\n        print(f\"start {start:6.2f}  \u2192  x* = {find_treasure(start):.4f}\")\n```\n\nTypical output\n\n```\nstart  0.96  \u2192  x* = 2.2500\nstart -6.79  \u2192  x* = 2.2500\nstart -2.15  \u2192  x* = 2.2500\nstart  6.72  \u2192  x* = 2.2500\nstart  1.06  \u2192  x* = 2.2500\n```\n\nSo the global minimum\u2014and therefore Captain Redbeard\u2019s treasure\u2014resides at x = 2.2500."}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) \u2013 a learnable,   normalization\u2013free substitute for LayerNorm.\n    \n    Parameters\n    ----------\n    x     : np.ndarray\n            Arbitrary-shaped input tensor.  \n            The last dimension is treated as the \u201cfeature\u201d dimension that would\n            normally be normalized in a Transformer.\n    alpha : float\n            Exponent that modulates how strongly the dynamic rescaling reacts\n            to the statistics of the current token.\n    gamma : float\n            Output scale (analogous to the gain in LayerNorm).\n    beta  : float\n            Pre-tanh scale (controls the slope of the linear region).\n    \n    Returns\n    -------\n    list[float]\n            DyT-transformed tensor, rounded to 4 d.p., returned as a Python list.\n    \"\"\"\n    \n    # --- 1.  Dynamic scaling term ------------------------------------------\n    # Mean absolute feature magnitude of every token (last dimension).\n    # A tiny \u03b5 avoids division by zero and keeps gradients finite.\n    eps            = 1e-7\n    mean_abs_token = np.mean(np.abs(x), axis=-1, keepdims=True) + eps\n    \n    # --- 2.  Apply the Dynamic Tanh transformation -------------------------\n    #  (a) Rescale input by mean_abs_token ** alpha        \u2013 dynamic part\n    #  (b) Apply a learnable slope (beta)                  \u2013 keeps linear regime\n    #  (c) Pass through tanh (squashing)                   \u2013 bounded output\n    #  (d) Apply an output gain (gamma)                    \u2013 learnable scale\n    y = gamma * np.tanh(beta * x / np.power(mean_abs_token, alpha))\n    \n    # --- 3.  Format the result ------------------------------------------------\n    y = np.round(y, 4)        # round to the nearest 4th decimal\n    return y.tolist()\n```"}
