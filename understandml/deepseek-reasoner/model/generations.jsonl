{"task_id": 0, "completion_id": 0, "completion": "To solve this problem, we need to implement Ridge Regression using its closed-form solution. Ridge Regression is a linear regression technique that includes L2 regularization to prevent overfitting by penalizing large coefficients. The closed-form solution involves matrix operations and a regularization term to ensure the matrix is invertible.\n\n### Approach\n1. **Data Preparation**: Split the input data into features (X) and target (y). The last column of the input array is the target, while the preceding columns are the features.\n2. **Bias Term Addition**: Add a column of ones to the feature matrix to account for the intercept term (bias) in the linear regression model.\n3. **Matrix Construction**: Construct the matrix \\( \\mathbf{X}^\\top \\mathbf{X} + \\alpha \\mathbf{I} \\), where \\( \\mathbf{I} \\) is the identity matrix. This matrix ensures invertibility through the regularization term \\( \\alpha \\).\n4. **Invertibility Check**: Check if the constructed matrix is invertible by computing its determinant. If the determinant is numerically close to zero (using a threshold), return -1 as the matrix is not invertible.\n5. **Weight Calculation**: If the matrix is invertible, compute the weight vector using the closed-form solution and return it rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"Compute the closed-form solution for Ridge Regression.\"\"\"\n    X = data[:, :-1]\n    y = data[:, -1]\n    \n    # Add bias term (intercept column)\n    X_b = np.hstack([np.ones((X.shape[0], 1)), X])\n    \n    n_features = X_b.shape[1]\n    A = X_b.T @ X_b + alpha * np.eye(n_features)\n    \n    det = np.linalg.det(A)\n    if np.abs(det) < 1e-12:\n        return -1\n    \n    w = np.linalg.inv(A) @ (X_b.T @ y)\n    return np.round(w, 4).tolist()\n```\n\n### Explanation\n1. **Data Splitting**: The input data is split into features (X) and target (y). This is done by slicing the input array such that the last column is the target and the rest are features.\n2. **Bias Term**: A column of ones is added to the feature matrix to account for the intercept term, which allows the model to learn a bias parameter.\n3. **Matrix Construction**: The matrix \\( \\mathbf{X}^\\top \\mathbf{X} + \\alpha \\mathbf{I} \\) is constructed. The regularization term \\( \\alpha \\mathbf{I} \\) ensures that the matrix is positive definite and invertible, especially when \\( \\alpha > 0 \\).\n4. **Determinant Check**: The determinant of the matrix is computed to check for invertibility. If the determinant is below a small threshold (1e-12), the matrix is considered singular, and -1 is returned.\n5. **Weight Calculation**: If the matrix is invertible, the weight vector is computed using the closed-form solution \\( \\hat{\\mathbf{w}} = (\\mathbf{X}^\\top \\mathbf{X} + \\alpha \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\). The weights are then rounded to four decimal places and returned as a list.\n\nThis approach efficiently combines linear algebra operations with numerical checks to ensure robustness and correctness in the Ridge Regression implementation."}
{"task_id": 1, "completion_id": 0, "completion": "To solve this problem, we need to implement the dual-form perceptron learning algorithm. This algorithm learns a linear classifier by expressing the weight vector as a linear combination of training samples using dual parameters. The key steps involve initializing these parameters, iterating through the training samples, updating the parameters when misclassifications occur, and stopping when either no updates are needed or a maximum number of updates is reached.\n\n### Approach\n1. **Initialization**: Start with dual parameters (alpha) and bias (b) set to zero.\n2. **Gram Matrix**: Precompute the Gram matrix to efficiently calculate inner products between training samples.\n3. **Iterative Update**: Scan through each sample, compute the activation using the Gram matrix, and check if the sample is misclassified. If misclassified, update the dual parameters and bias, then restart the scan from the beginning.\n4. **Stopping Conditions**: Stop when an entire pass through the data results in no updates or the maximum number of updates is reached.\n5. **Weight Vector Calculation**: Compute the primal weight vector from the dual parameters after training.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\"\"\"\n    n_samples, n_features = X_train.shape\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    updates = 0\n    gram = X_train @ X_train.T  # Precompute Gram matrix\n    i = 0\n    \n    while updates < n_iter:\n        if i >= n_samples:\n            break  # Full pass completed with no updates\n        # Compute activation for sample i\n        activation = np.dot(alpha * y_train, gram[:, i])\n        margin = y_train[i] * (activation + b)\n        if margin <= 0:\n            # Update alpha and b\n            alpha[i] += eta\n            b += eta * y_train[i]\n            updates += 1\n            i = 0  # Restart scan\n        else:\n            i += 1\n    \n    # Compute the primal weight vector w\n    w = np.dot(alpha * y_train, X_train)\n    # Round to four decimal places\n    w_rounded = [round(x, 4) for x in w]\n    b_rounded = round(b, 4)\n    \n    return (w_rounded, b_rounded)\n```\n\n### Explanation\n1. **Initialization**: The dual parameters `alpha` and bias `b` are initialized to zero. The number of updates is tracked to ensure we do not exceed the maximum allowed iterations.\n2. **Gram Matrix**: The Gram matrix is precomputed once to store all inner products of training samples, which speeds up activation calculations during each iteration.\n3. **Iterative Update**: For each sample, the activation is computed using the precomputed Gram matrix. If the sample is misclassified (margin \u2264 0), the dual parameter `alpha` and bias `b` are updated, and the scan restarts from the first sample. This ensures that updates are immediately reflected in subsequent checks.\n4. **Stopping Conditions**: The loop exits if a full pass through all samples is completed without any updates or the maximum number of updates is reached.\n5. **Weight Vector Calculation**: After training, the primal weight vector `w` is derived from the dual parameters and training data, then rounded to four decimal places along with the bias `b`.\n\nThis approach efficiently handles both linearly separable and non-separable data, ensuring convergence within a finite number of iterations for separable cases and stopping after a specified number of updates for non-separable cases."}
{"task_id": 2, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that returns a dictionary containing the link functions, their inverses, and their first derivatives for three common Generalized Linear Models (GLMs): identity, log, and logit. These functions must work with both numpy scalars and arrays, performing element-wise operations.\n\n### Approach\nThe task involves creating three dictionaries (one for each link function) where each dictionary contains three callable functions: the link function, its inverse, and its derivative. The key steps are:\n\n1. **Identity Link**: The link function is the identity function, its inverse is the same, and the derivative is always 1.\n2. **Log Link**: The link function is the natural logarithm, its inverse is the exponential function, and the derivative is the reciprocal of the input.\n3. **Logit Link**: The link function is the log-odds function, its inverse is the logistic function, and the derivative is the reciprocal of the product of the input and its complement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    return {\n        'identity': {\n            'link': lambda mu: mu,\n            'inv_link': lambda eta: eta,\n            'deriv': lambda mu: np.ones_like(mu)\n        },\n        'log': {\n            'link': np.log,\n            'inv_link': np.exp,\n            'deriv': lambda mu: 1.0 / mu\n        },\n        'logit': {\n            'link': lambda mu: np.log(mu / (1 - mu)),\n            'inv_link': lambda eta: 1 / (1 + np.exp(-eta)),\n            'deriv': lambda mu: 1 / (mu * (1 - mu))\n        }\n    }\n```\n\n### Explanation\n1. **Identity Link**:\n   - **Link Function**: Directly returns the input.\n   - **Inverse Link Function**: Directly returns the input.\n   - **Derivative**: Always returns 1, regardless of the input, using `np.ones_like` to handle array shapes.\n\n2. **Log Link**:\n   - **Link Function**: Uses `np.log` to compute the natural logarithm element-wise.\n   - **Inverse Link Function**: Uses `np.exp` to compute the exponential element-wise.\n   - **Derivative**: Computes the reciprocal of the input element-wise.\n\n3. **Logit Link**:\n   - **Link Function**: Computes the log-odds using element-wise division and logarithm.\n   - **Inverse Link Function**: Applies the logistic function using element-wise operations to map the linear predictor back to the mean.\n   - **Derivative**: Computes the reciprocal of the product of the input and its complement element-wise.\n\nThese functions are designed to handle numpy arrays and scalars efficiently, ensuring that all operations are performed element-wise, which is essential for compatibility with numpy's broadcasting and array operations."}
{"task_id": 3, "completion_id": 0, "completion": "To solve this problem, we need to compute the depth of the leaf node each sample reaches in an isolation tree. The isolation tree is constructed by recursively partitioning the data using random splits. Each split is determined by selecting a feature and a split value within the range of the selected feature's values. The depth of a sample is the number of edges from the root to the leaf node where the sample ends up.\n\n### Approach\n1. **Set Random Seed**: To ensure reproducibility, we set the random seed to 0 at the beginning of the function.\n2. **Recursive Tree Construction**: We recursively build the tree by splitting nodes into left and right children based on a randomly selected feature and split value. Each node processes the indices of the samples that reach it.\n3. **Leaf Node Condition**: If a node receives fewer than three samples, it becomes a leaf, and the depth of those samples is recorded.\n4. **Splitting Process**: For nodes with three or more samples, we split the data using a randomly selected feature and a uniformly random split value. If a split results in one of the children being empty, the current node is treated as a leaf to avoid infinite recursion.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\"\"\"\n    np.random.seed(0)\n    n_samples = data.shape[0]\n    depths = [0] * n_samples\n    indices = list(range(n_samples))\n    \n    def process_node(current_indices, current_depth):\n        if len(current_indices) < 3:\n            for i in current_indices:\n                depths[i] = current_depth\n            return\n        # Randomly select feature\n        f = np.random.randint(data.shape[1])\n        feature_values = data[current_indices, f]\n        down = np.min(feature_values)\n        up = np.max(feature_values)\n        v = np.random.uniform(down, up)\n        # Split indices into left and right\n        left = []\n        right = []\n        for i in current_indices:\n            if data[i, f] <= v:\n                left.append(i)\n            else:\n                right.append(i)\n        # Check if split is non-trivial\n        if not left or not right:\n            # Treat as leaf if split doesn't divide the data\n            for i in current_indices:\n                depths[i] = current_depth\n            return\n        # Recursively process children\n        process_node(left, current_depth + 1)\n        process_node(right, current_depth + 1)\n    \n    process_node(indices, 0)\n    return depths\n```\n\n### Explanation\n1. **Setting the Random Seed**: The random seed is set to 0 to ensure that the results are reproducible.\n2. **Initialization**: We initialize the depths list to store the depth of each sample and start with all sample indices at the root node.\n3. **Recursive Processing**: The `process_node` function handles each node. If a node has fewer than three samples, it becomes a leaf, and the current depth is recorded for those samples.\n4. **Feature and Split Selection**: For nodes with three or more samples, a feature is randomly selected, and a split value is chosen uniformly between the minimum and maximum values of the selected feature.\n5. **Splitting Check**: After splitting, if either child is empty, the current node is treated as a leaf to avoid infinite recursion. Otherwise, the function recursively processes the left and right children with incremented depth.\n\nThis approach ensures that the isolation tree is built correctly according to the problem's specifications while handling edge cases to avoid infinite recursion and ensuring termination."}
{"task_id": 7, "completion_id": 0, "completion": "To solve this problem, we need to identify all frequent itemsets in a list of transactions given a minimum support threshold. A frequent itemset is one that appears in a sufficient number of transactions, determined by the threshold. The solution uses the Apriori algorithm, which efficiently generates candidate itemsets and prunes them based on support counts.\n\n### Approach\n1. **Calculate Minimum Support**: Convert the given minimum support threshold from a fraction to an absolute count using the ceiling of the product of the threshold and the number of transactions.\n2. **Preprocess Transactions**: Convert each transaction into a set to remove duplicates and store them for efficient subset checks.\n3. **Generate Frequent 1-Itemsets**: Count the occurrence of each item across all transactions and retain those that meet the minimum support.\n4. **Generate Candidate Itemsets**: For each subsequent level (k \u2265 2), generate candidate itemsets by joining frequent itemsets from the previous level, ensuring they are sorted and valid (all subsets are frequent).\n5. **Prune Candidates**: Check if all subsets of each candidate itemset are frequent. If not, discard the candidate.\n6. **Count Support**: For each valid candidate, count how many transactions contain the itemset and retain those meeting the minimum support.\n7. **Aggregate Results**: Collect all frequent itemsets from all levels and return them as a dictionary.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\"\"\"\n    n_trans = len(transactions)\n    if n_trans == 0 or minsup <= 0:\n        return {}\n    min_support = math.ceil(minsup * n_trans)\n    if min_support <= 0:\n        return {}\n    \n    preprocessed = [set(t) for t in transactions]\n    \n    # Generate frequent 1-itemsets\n    item_counter = Counter()\n    for t in preprocessed:\n        for item in t:\n            item_counter[item] += 1\n    F_prev = { (item,): count for item, count in item_counter.items() if count >= min_support }\n    \n    frequent_itemsets = dict(F_prev)\n    k = 2\n    \n    while F_prev:\n        itemsets = sorted(F_prev.keys())\n        Ck = []\n        # Generate candidates by joining\n        for i in range(len(itemsets)):\n            itemset1 = itemsets[i]\n            for j in range(i + 1, len(itemsets)):\n                itemset2 = itemsets[j]\n                if itemset1[:-1] == itemset2[:-1] and itemset1[-1] < itemset2[-1]:\n                    new_candidate = itemset1 + (itemset2[-1],)\n                    # Check all (k-1)-subsets\n                    valid = True\n                    for subset in combinations(new_candidate, k - 1):\n                        if subset not in F_prev:\n                            valid = False\n                            break\n                    if valid:\n                        Ck.append(new_candidate)\n        # Deduplicate\n        Ck = list(set(Ck))\n        # Compute support for candidates\n        Fk = {}\n        for candidate in Ck:\n            count = 0\n            for t in preprocessed:\n                if all(item in t for item in candidate):\n                    count += 1\n            if count >= min_support:\n                Fk[candidate] = count\n        if not Fk:\n            break\n        frequent_itemsets.update(Fk)\n        F_prev = Fk\n        k += 1\n    \n    return frequent_itemsets\n```\n\n### Explanation\n1. **Minimum Support Calculation**: Converts the relative threshold to an absolute count using the total number of transactions.\n2. **Transaction Preprocessing**: Converts each transaction into a set to handle duplicates efficiently.\n3. **Frequent 1-Itemsets**: Uses a counter to tally item occurrences and filters those meeting the minimum support.\n4. **Candidate Generation**: For each level, generates candidates by joining previous frequent itemsets, ensuring they are sorted and valid.\n5. **Pruning**: Removes candidates if any of their subsets are not frequent, ensuring only valid candidates proceed.\n6. **Support Counting**: Checks each candidate against all transactions to determine if they meet the support threshold.\n7. **Result Aggregation**: Collects all valid frequent itemsets across all levels and returns them in a sorted tuple format.\n\nThis approach efficiently narrows down candidates using the Apriori principle, ensuring that only potentially frequent itemsets are considered, thus optimizing both time and space complexity."}
{"task_id": 11, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm from scratch without relying on any external machine-learning libraries. The algorithm must follow specific steps to initialize cluster centers, assign samples to the nearest center, recompute centers, and handle edge cases such as empty clusters.\n\n### Approach\n1. **Initialization**: Start with the first `k` samples from the input data as the initial cluster centers.\n2. **Assignment Step**: For each sample, compute the squared Euclidean distance to each cluster center and assign the sample to the nearest cluster.\n3. **Update Centers**: Recompute each cluster's center as the mean of all samples assigned to it. If a cluster becomes empty, reinitialize its center with a random sample from the dataset.\n4. **Convergence Check**: Stop iterating when the maximum change in any cluster center's position between consecutive iterations is less than `epsilon` or when the maximum number of iterations `max_iter` is reached.\n5. **Rounding and Final Assignment**: After convergence, round each coordinate of the cluster centers to 4 decimal places and reassign each sample to the nearest cluster based on the rounded centers.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\"\"\"\n    if data.size == 0 or k == 0:\n        return [], []\n    \n    # Initialize centers with the first k samples\n    centers = data[:k].copy()\n    \n    for _ in range(max_iter):\n        # Calculate squared distances from each sample to each center\n        distances = np.sum((data[:, np.newaxis] - centers) ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        new_centers = np.zeros_like(centers)\n        for j in range(k):\n            mask = labels == j\n            if np.sum(mask) == 0:\n                # Re-initialize empty cluster with a random sample\n                new_centers[j] = data[np.random.randint(data.shape[0])]\n            else:\n                new_centers[j] = data[mask].mean(axis=0)\n        \n        # Check for convergence\n        delta = np.max(np.linalg.norm(new_centers - centers, axis=1))\n        if delta < epsilon:\n            centers = new_centers.copy()\n            break\n        centers = new_centers.copy()\n    \n    # Round the final centers to 4 decimal places\n    centers = np.round(centers, 4)\n    \n    # Reassign labels based on the rounded centers\n    distances = np.sum((data[:, np.newaxis] - centers) ** 2, axis=2)\n    labels = np.argmin(distances, axis=1).tolist()\n    \n    # Convert centers to a list of lists\n    centers_list = [center.tolist() for center in centers]\n    \n    return (centers_list, labels)\n```\n\n### Explanation\n1. **Initialization**: The initial cluster centers are selected as the first `k` samples from the input data.\n2. **Assignment Step**: For each sample, the squared Euclidean distance to each cluster center is computed using vectorized operations for efficiency. Each sample is assigned to the nearest cluster based on these distances.\n3. **Update Centers**: After assigning all samples, new cluster centers are computed as the mean of the samples in each cluster. If a cluster has no samples, its center is reinitialized with a random sample from the dataset to avoid empty clusters.\n4. **Convergence Check**: The algorithm checks if the maximum change in cluster centers between iterations is below the threshold `epsilon`. If so, it stops early; otherwise, it continues until `max_iter` is reached.\n5. **Rounding and Final Assignment**: After convergence, the cluster centers are rounded to 4 decimal places. Each sample is then reassigned to the nearest cluster based on the rounded centers to ensure the final labels correspond to the rounded centers.\n\nThis approach ensures that the algorithm efficiently clusters the data while handling edge cases such as empty clusters and providing results that meet the specified precision requirements."}
{"task_id": 13, "completion_id": 0, "completion": "To solve this problem, we need to implement an item-based k-NN collaborative filtering recommender system. The goal is to recommend up to `k` items that a user has not yet rated based on their predicted attractiveness using item-item similarity.\n\n### Approach\n1. **Build Item-Item Similarity Matrix**: Compute the similarity between each pair of items using either cosine similarity or Pearson correlation. This matrix will be symmetric and of shape (n_items, n_items).\n2. **Identify Rated Items**: Determine which items the active user has already rated.\n3. **Predict Attractiveness Scores**: For each unrated item, compute a score based on the similarity matrix and the user's ratings of other items.\n4. **Sort and Recommend**: Sort the unrated items by their predicted scores in descending order, and return the top `k` items, ensuring stability by using item indices for ties.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\"\"\"\n    n_items = data.shape[1]\n    S = np.zeros((n_items, n_items))\n    \n    # Step 1: Build item-item similarity matrix S\n    for i in range(n_items):\n        for j in range(i + 1, n_items):\n            common_rated = (data[:, i] > 0) & (data[:, j] > 0)\n            if not np.any(common_rated):\n                S[i, j] = S[j, i] = 0.0\n                continue\n            \n            v1 = data[common_rated, i]\n            v2 = data[common_rated, j]\n            \n            if criterion == 'cosine':\n                std1 = np.std(v1, ddof=1)\n                if std1 > 1e-3:\n                    v1 = v1 - np.mean(v1)\n                std2 = np.std(v2, ddof=1)\n                if std2 > 1e-3:\n                    v2 = v2 - np.mean(v2)\n                \n                norm1 = np.linalg.norm(v1)\n                norm2 = np.linalg.norm(v2)\n                if norm1 == 0 or norm2 == 0:\n                    sim = 0.0\n                else:\n                    sim = np.dot(v1, v2) / (norm1 * norm2)\n            elif criterion == 'pearson':\n                if len(v1) < 2:\n                    sim = 0.0\n                else:\n                    corr = np.corrcoef(v1, v2)[0, 1]\n                    sim = 0.0 if np.isnan(corr) else corr\n            else:\n                sim = 0.0  # Invalid criterion\n            \n            S[i, j] = S[j, i] = sim\n    \n    # Step 2: Collect rated items for the active user\n    user_ratings = data[user_ind]\n    rated_items = np.where(user_ratings > 0)[0].tolist()\n    \n    if len(rated_items) == n_items:\n        return []\n    \n    # Step 3: Compute scores for unrated items\n    unrated = [t for t in range(n_items) if user_ratings[t] == 0]\n    scores = []\n    \n    for t in unrated:\n        sum_num = 0.0\n        sum_den = 0.0\n        for i in rated_items:\n            sim = S[t, i]\n            sum_num += user_ratings[i] * sim\n            sum_den += abs(sim)\n        \n        score = sum_num / sum_den if sum_den != 0 else 0.0\n        scores.append((score, t))\n    \n    # Step 4: Sort and select top k\n    sorted_scores = sorted(scores, key=lambda x: (-x[0], x[1]))\n    result = [t for (score, t) in sorted_scores[:k]]\n    \n    return result\n```\n\n### Explanation\n1. **Similarity Matrix Construction**: The code first constructs the item-item similarity matrix. For each pair of items, it finds users who rated both items. Depending on the similarity criterion (cosine or Pearson), it computes the similarity and stores it in the matrix.\n2. **Rated Items Identification**: The active user's rated items are identified using the user's rating vector.\n3. **Score Calculation**: For each unrated item, the score is calculated using the sum of the product of the user's ratings and the similarities, normalized by the sum of absolute similarities.\n4. **Sorting and Recommendation**: The unrated items are sorted by their scores in descending order, and the top `k` items are returned. If scores are tied, items with smaller indices are prioritized.\n\nThis approach ensures that the recommendations are based on the most similar items the user has already rated, providing personalized and relevant suggestions."}
{"task_id": 19, "completion_id": 0, "completion": "To solve this problem, we need to determine the best split for a gradient-boosting tree node that maximizes the reduction in the regularized loss function. The split must leave at least two training instances on each side, and if no valid split yields a positive gain, we return `None`.\n\n### Approach\n1. **Iterate Over Features**: For each feature in the feature matrix, sort the feature values along with their corresponding gradients (g) and Hessians (h).\n2. **Cumulative Sums**: Compute cumulative sums of the sorted gradients and Hessians to efficiently calculate sums for potential left and right splits.\n3. **Parent Loss Calculation**: Calculate the loss for the current node (parent) using the cumulative sums of the entire dataset.\n4. **Unique Thresholds**: For each unique value of the sorted feature, consider it as a potential threshold for splitting.\n5. **Split Validity Check**: For each threshold, determine the split index and check if both left and right splits have at least two instances.\n6. **Gain Calculation**: Compute the loss for the left and right splits, then calculate the gain. Track the maximum gain and corresponding feature and threshold.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    n_samples, n_features = X.shape\n    best_gain = -np.inf\n    best_feature = None\n    best_threshold = None\n\n    for feature in range(n_features):\n        X_feature = X[:, feature]\n        sorted_indices = np.argsort(X_feature)\n        X_sorted = X_feature[sorted_indices]\n        g_sorted = g[sorted_indices]\n        h_sorted = h[sorted_indices]\n\n        cum_g = np.concatenate([[0], np.cumsum(g_sorted)])\n        cum_h = np.concatenate([[0], np.cumsum(h_sorted)])\n        total_g = cum_g[-1]\n        total_h = cum_h[-1]\n\n        denominator_parent = total_h + lam\n        if denominator_parent == 0:\n            continue  # Avoid division by zero\n        parent_loss = -(total_g ** 2) / (2 * denominator_parent) + gamma\n\n        unique_vals = np.unique(X_sorted)\n\n        for t in unique_vals:\n            split_idx = np.searchsorted(X_sorted, t, side='right')\n            left_count = split_idx\n            right_count = n_samples - split_idx\n\n            if left_count < 2 or right_count < 2:\n                continue\n\n            sum_g_left = cum_g[split_idx]\n            sum_h_left = cum_h[split_idx]\n            sum_g_right = total_g - sum_g_left\n            sum_h_right = total_h - sum_h_left\n\n            denominator_left = sum_h_left + lam\n            denominator_right = sum_h_right + lam\n\n            if denominator_left == 0 or denominator_right == 0:\n                continue  # Avoid division by zero\n\n            left_loss = -(sum_g_left ** 2) / (2 * denominator_left) + gamma\n            right_loss = -(sum_g_right ** 2) / (2 * denominator_right) + gamma\n\n            current_gain = parent_loss - (left_loss + right_loss)\n\n            if current_gain > best_gain:\n                best_gain = current_gain\n                best_feature = feature\n                best_threshold = t\n\n    if best_gain > 0:\n        return (best_feature, best_threshold)\n    else:\n        return None\n```\n\n### Explanation\n1. **Feature Sorting**: Each feature column is sorted, and corresponding gradients and Hessians are reordered to maintain consistency.\n2. **Cumulative Sums**: Cumulative sums of gradients and Hessians allow efficient calculation of sums for potential splits.\n3. **Parent Loss**: The loss for the current node is computed once per feature using the total sums of gradients and Hessians.\n4. **Threshold Evaluation**: For each unique threshold, the split index is determined, and the validity of the split is checked.\n5. **Gain Calculation**: The gain from splitting is calculated by comparing the parent loss with the combined losses of the left and right children. The best gain and corresponding feature and threshold are tracked.\n\nThis approach ensures that we efficiently evaluate all possible splits while adhering to the constraints, resulting in the optimal split that maximizes the gain."}
{"task_id": 20, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that computes both the sigmoid activation function and its gradient for a given input, which can be a scalar, a list, or a NumPy array. The function should return the results rounded to 4 decimal places, formatted appropriately based on the input type.\n\n### Approach\n1. **Input Handling**: The function first checks if the input is a scalar (integer or float). If it is, the sigmoid and gradient are computed directly and returned as rounded floats.\n2. **Array Conversion**: If the input is a list, it is converted to a NumPy array for uniform processing. If the input is already a NumPy array, it is used as-is.\n3. **Scalar Check for Arrays**: If the input is a 0-dimensional NumPy array (scalar), it is treated as a scalar, and the results are returned as rounded floats.\n4. **Array Processing**: For 1-D or 2-D arrays, the sigmoid and gradient are computed element-wise. The results are rounded to 4 decimal places and converted to Python lists before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)):\n        sig = 1 / (1 + np.exp(-x))\n        grad = sig * (1 - sig)\n        return (round(sig, 4), round(grad, 4))\n    else:\n        if isinstance(x, list):\n            arr = np.array(x)\n        else:\n            arr = x  # Assume it's a numpy array\n\n        if arr.ndim == 0:\n            sig = 1 / (1 + np.exp(-arr))\n            grad = sig * (1 - sig)\n            return (round(sig.item(), 4), round(grad.item(), 4))\n        else:\n            sig = 1 / (1 + np.exp(-arr))\n            grad = sig * (1 - sig)\n            sig_rounded = np.round(sig, 4).tolist()\n            grad_rounded = np.round(grad, 4).tolist()\n            return (sig_rounded, grad_rounded)\n```\n\n### Explanation\n- **Scalar Handling**: If the input is a scalar (integer or float), the sigmoid and gradient are computed directly using the formula and returned as rounded floats.\n- **List Conversion**: Lists are converted to NumPy arrays to leverage vectorized operations for efficient computation.\n- **0-D Array Handling**: If a NumPy array is 0-dimensional (scalar), it is processed similarly to a scalar input, and results are returned as rounded floats.\n- **Array Processing**: For 1-D or 2-D arrays, the sigmoid and gradient are computed element-wise. The results are rounded and converted to nested Python lists to maintain the input shape structure.\n\nThis approach ensures that the function works efficiently for all specified input types and returns results in the required format, adhering to the problem constraints."}
{"task_id": 21, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost algorithm using one-dimensional decision stumps as weak learners. The goal is to create an ensemble of stumps that minimizes the training error until it is less than or equal to a specified threshold `epsilon`.\n\n### Approach\n1. **Initialization**: Start with uniform weights for all training samples.\n2. **Generate Thresholds**: Compute potential thresholds for decision stumps by taking midpoints between consecutive sorted training points.\n3. **Weak Learner Selection**: For each threshold and direction (left or right of the threshold), compute the weighted error and select the stump with the minimum error.\n4. **Update Weights**: Adjust the sample weights based on the selected stump's performance, increasing weights for misclassified samples and decreasing weights for correctly classified samples.\n5. **Compute Coefficient**: Calculate the coefficient (alpha) for the selected stump using the formula derived from the weighted error.\n6. **Check Stopping Condition**: After updating the ensemble, compute the training error. If it is less than or equal to `epsilon`, stop training.\n7. **Prediction**: Use the ensemble of stumps to predict labels for the test samples by taking a weighted majority vote.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\"\"\"\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    N = len(x_train)\n    \n    if not np.all(np.isin(y_train, [-1, 1])):\n        raise ValueError(\"y_train must contain only 1 or -1\")\n    \n    sorted_x = np.sort(x_train)\n    if len(sorted_x) == 1:\n        thetas = [sorted_x[0]]\n    else:\n        thetas = [(sorted_x[i] + sorted_x[i+1]) / 2 for i in range(len(sorted_x)-1)]\n    thetas = np.array(thetas)\n    \n    D = np.ones(N) / N\n    stumps = []\n    \n    while True:\n        best_err = np.inf\n        best_d = None\n        best_theta = None\n        best_preds = None\n        \n        for theta in thetas:\n            for d in [0, 1]:\n                if d == 0:\n                    preds = np.where(x_train <= theta, 1, -1)\n                else:\n                    preds = np.where(x_train > theta, 1, -1)\n                err = np.sum(D * (preds != y_train))\n                if err < best_err:\n                    best_err = err\n                    best_d = d\n                    best_theta = theta\n                    best_preds = preds\n        \n        if best_err == 0:\n            alpha = np.inf\n        else:\n            alpha = 0.5 * np.log((1 - best_err) / best_err)\n        \n        stumps.append((alpha, best_d, best_theta))\n        \n        correct = (best_preds == y_train)\n        factors = np.where(correct, np.exp(-alpha), np.exp(alpha))\n        D *= factors\n        D /= D.sum()\n        \n        current_preds = []\n        for x in x_train:\n            sum_alpha = 0.0\n            for a, d_stump, theta_stump in stumps:\n                if d_stump == 0:\n                    h = 1 if x <= theta_stump else -1\n                else:\n                    h = 1 if x > theta_stump else -1\n                sum_alpha += a * h\n            current_pred = 1 if sum_alpha >= 0 else -1\n            current_preds.append(current_pred)\n        \n        training_error = np.mean(np.array(current_preds) != y_train)\n        if training_error <= epsilon:\n            break\n    \n    y_pred = []\n    for x in x_test:\n        sum_alpha = 0.0\n        for a, d, theta in stumps:\n            if d == 0:\n                h = 1 if x <= theta else -1\n            else:\n                h = 1 if x > theta else -1\n            sum_alpha += a * h\n        pred = 1 if sum_alpha >= 0 else -1\n        y_pred.append(pred)\n    \n    return y_pred\n```\n\n### Explanation\n1. **Initialization**: The algorithm starts with uniform weights for all training samples.\n2. **Threshold Generation**: Midpoints between consecutive sorted training points are computed to generate potential thresholds for decision stumps.\n3. **Stump Selection**: For each threshold and direction, the algorithm computes the weighted error and selects the stump with the lowest error.\n4. **Weight Update**: After selecting the best stump, sample weights are updated to focus more on misclassified samples, and the weights are normalized.\n5. **Coefficient Calculation**: The coefficient (alpha) for the selected stump is calculated based on its weighted error.\n6. **Stopping Condition**: The training error of the current ensemble is checked after each iteration. Training stops once this error is less than or equal to `epsilon`.\n7. **Prediction**: The ensemble of stumps is used to predict labels for test samples by taking a weighted majority vote of their predictions.\n\nThis approach ensures that the model iteratively improves by focusing on difficult samples, leading to a robust ensemble classifier."}
{"task_id": 25, "completion_id": 0, "completion": "To solve this problem, we need to predict the class labels for test samples using a pre-trained Gaussian Kernel Support Vector Machine (SVM). The SVM uses a Radial Basis Function (RBF) kernel, and the prediction is based on the decision function derived from the training data, Lagrange multipliers, bias term, and kernel hyper-parameter.\n\n### Approach\nThe key steps to compute the decision function for each test sample are as follows:\n1. **Compute Squared Euclidean Distances**: For each test sample, compute the squared Euclidean distance to all training samples. This is done efficiently using vectorized operations to avoid explicit loops.\n2. **Compute RBF Kernel Matrix**: Transform the squared distances into the RBF kernel values using the given gamma hyper-parameter.\n3. **Decision Function Calculation**: Use the Lagrange multipliers, training labels, and the kernel matrix to compute the decision values for each test sample.\n4. **Class Prediction**: Convert the decision values into class labels (1 or -1) based on the sign of the decision value. A non-negative decision value results in a class label of 1, otherwise -1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    # Compute squared norms of training and test samples\n    X_train_sq = np.sum(X_train**2, axis=1)\n    X_test_sq = np.sum(X_test**2, axis=1)\n    \n    # Compute the cross term between training and test samples\n    cross_term = X_train @ X_test.T\n    \n    # Compute squared Euclidean distances using broadcasting\n    squared_distances = X_train_sq[:, np.newaxis] + X_test_sq[np.newaxis, :] - 2 * cross_term\n    \n    # Calculate the Gaussian kernel matrix\n    K = np.exp(-gamma * squared_distances)\n    \n    # Compute the decision values for all test samples\n    decision_values = np.dot(alpha * y_train, K) + b\n    \n    # Determine predicted labels (1 if decision value >= 0, else -1)\n    predicted_labels = np.where(decision_values >= 0, 1, -1).astype(int).tolist()\n    \n    return predicted_labels\n```\n\n### Explanation\n1. **Squared Euclidean Distances**: The squared distances between training and test samples are computed using vectorized operations. This avoids explicit loops and leverages NumPy's efficient matrix operations. The formula used is:\n   \\[\n   \\text{squared\\_distance}(x_i, z_j) = \\|x_i\\|^2 + \\|z_j\\|^2 - 2 \\cdot x_i \\cdot z_j^T\n   \\]\n   where \\(x_i\\) is a training sample and \\(z_j\\) is a test sample.\n\n2. **RBF Kernel Matrix**: The RBF kernel values are computed by exponentiating the negative product of gamma and the squared distances matrix. This transforms distances into similarity measures.\n\n3. **Decision Values**: The decision function for each test sample is computed by taking the dot product of the Lagrange multipliers (adjusted by training labels) with the kernel matrix, then adding the bias term. This step efficiently aggregates contributions from all training samples.\n\n4. **Class Prediction**: The predicted class labels are determined by the sign of the decision values. Non-negative values are assigned class 1, and negative values are assigned class -1, ensuring the output adheres to the required format.\n\nThis approach ensures the solution is both efficient and scalable, handling arbitrary numbers of training and test samples using fully vectorized operations."}
{"task_id": 28, "completion_id": 0, "completion": "To solve this problem, we need to compute the optimal rank-k reconstruction of a given matrix using a truncated Singular Value Decomposition (SVD). This approach is equivalent to using the best linear autoencoder with k latent units. The solution involves checking the validity of k, performing the SVD, truncating it to k components, reconstructing the matrix, and calculating the mean squared error (MSE).\n\n### Approach\n1. **Input Validation**: Check if k is within the valid range (1 \u2264 k \u2264 min(m, n)), where m and n are the dimensions of the input matrix. If not, return -1.\n2. **Matrix Conversion**: Convert the input list of lists into a NumPy array for efficient matrix operations.\n3. **Singular Value Decomposition (SVD)**: Compute the SVD of the matrix. This decomposition breaks the matrix into three components: U, \u03a3 (a vector of singular values), and Vt (the transpose of V).\n4. **Truncation**: Truncate the SVD components to retain only the top k singular values and corresponding vectors.\n5. **Reconstruction**: Use the truncated components to reconstruct the matrix approximation.\n6. **Mean Squared Error (MSE) Calculation**: Compute the MSE using the sum of the squares of the singular values that were excluded from the truncation, divided by the total number of elements in the matrix.\n7. **Rounding and Conversion**: Round the reconstructed matrix and MSE to four decimal places and convert the matrix back to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    X_np = np.array(X)\n    m, n = X_np.shape\n    \n    if k < 1 or k > min(m, n):\n        return -1\n    \n    U, S, Vt = np.linalg.svd(X_np, full_matrices=False)\n    \n    # Truncate to rank k\n    U_k = U[:, :k]\n    S_k = S[:k]\n    Vt_k = Vt[:k, :]\n    \n    # Reconstruct X_hat\n    X_hat = U_k @ np.diag(S_k) @ Vt_k\n    \n    # Compute MSE using the sum of squares of the remaining singular values\n    mse = np.sum(S[k:] ** 2) / (m * n)\n    \n    # Rounding and converting to list of lists\n    X_hat_rounded = np.round(X_hat, 4)\n    X_hat_list = X_hat_rounded.tolist()\n    mse_rounded = round(mse, 4)\n    \n    return (X_hat_list, mse_rounded)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if k is within the valid range. If not, it returns -1.\n2. **Matrix Conversion**: The input matrix is converted to a NumPy array for efficient computation.\n3. **SVD Computation**: Using NumPy's `svd` function with `full_matrices=False`, we get the economy-sized SVD, which is more efficient for non-square matrices.\n4. **Truncation**: The matrices U, \u03a3, and Vt are truncated to retain only the top k components, which are the most significant in terms of variance.\n5. **Reconstruction**: The matrix is reconstructed using the truncated components. This is done by matrix multiplication of the truncated U, diagonal matrix of singular values, and truncated Vt.\n6. **MSE Calculation**: The MSE is computed using the sum of squares of the excluded singular values, providing an efficient and accurate measure of reconstruction error without explicitly constructing the error matrix.\n7. **Rounding and Conversion**: The reconstructed matrix and MSE are rounded to four decimal places and converted back to the required list format for output.\n\nThis approach ensures that the solution is both efficient and accurate, leveraging the mathematical properties of SVD to minimize the reconstruction error."}
{"task_id": 29, "completion_id": 0, "completion": "To solve this problem, we need to implement a simple one-dimensional gradient boosting regressor using decision stumps. The algorithm involves iteratively adding stumps (simple decision trees with one split) to reduce the residual sum of squares (RSS) until it falls below a specified tolerance. Each stump is chosen to minimize the RSS of the current residuals, and the final prediction is the sum of contributions from all stumps.\n\n### Approach\n1. **Sort Training Data**: The training data is sorted based on the feature values to facilitate the calculation of midpoints between consecutive values.\n2. **Generate Split Candidates**: Midpoints between consecutive sorted feature values are computed as potential split points.\n3. **Filter Valid Splits**: Only splits that divide the data into non-empty subsets are considered valid.\n4. **Train the Ensemble**: Iteratively select the best stump (split point) that minimizes the RSS of the residuals, update the residuals, and repeat until the RSS is below the tolerance.\n5. **Predict**: For a query point, sum the contributions from all stumps based on their split points.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                          y_train: list[float] | np.ndarray,\n                          x_query: float,\n                          epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\"\"\"\n    x_train = np.asarray(x_train)\n    y_train = np.asarray(y_train)\n    sorted_indices = np.argsort(x_train)\n    x_sorted = x_train[sorted_indices]\n    y_sorted = y_train[sorted_indices]\n    \n    split_candidates = []\n    n = len(x_sorted)\n    for i in range(n - 1):\n        split = (x_sorted[i] + x_sorted[i + 1]) / 2\n        split_candidates.append(split)\n    \n    valid_splits = []\n    for s in split_candidates:\n        mask_left = x_sorted <= s\n        mask_right = x_sorted > s\n        if np.any(mask_left) and np.any(mask_right):\n            valid_splits.append(s)\n    \n    ensemble = []\n    residuals = y_sorted.copy()\n    current_rss = np.sum(residuals ** 2)\n    \n    if valid_splits:\n        while current_rss > epsilon:\n            best_split = None\n            best_c1 = None\n            best_c2 = None\n            best_new_rss = np.inf\n            \n            for s in valid_splits:\n                mask_left = x_sorted <= s\n                mask_right = ~mask_left\n                c1 = np.mean(residuals[mask_left])\n                c2 = np.mean(residuals[mask_right])\n                predictions = np.where(mask_left, c1, c2)\n                new_residuals = residuals - predictions\n                new_rss = np.sum(new_residuals ** 2)\n                \n                if new_rss < best_new_rss:\n                    best_new_rss = new_rss\n                    best_split = s\n                    best_c1 = c1\n                    best_c2 = c2\n            \n            if best_split is None:\n                break\n            \n            ensemble.append((best_split, best_c1, best_c2))\n            mask_left = x_sorted <= best_split\n            predictions = np.where(mask_left, best_c1, best_c2)\n            residuals -= predictions\n            current_rss = best_new_rss\n            \n            if current_rss <= epsilon:\n                break\n    \n    prediction = 0.0\n    for stump in ensemble:\n        s, c1, c2 = stump\n        if x_query <= s:\n            prediction += c1\n        else:\n            prediction += c2\n    \n    return round(prediction, 4)\n```\n\n### Explanation\n1. **Sorting Data**: The input data is sorted to facilitate the calculation of midpoints between consecutive feature values.\n2. **Split Candidates**: Midpoints between consecutive sorted feature values are generated as potential split points.\n3. **Valid Splits**: Only splits that result in non-empty subsets are considered valid to avoid undefined means.\n4. **Training Loop**: The algorithm iteratively selects the best stump that minimizes the RSS of the current residuals. The residuals are updated by subtracting the predictions from the current model, and the process continues until the RSS is below the specified tolerance.\n5. **Prediction**: The prediction for a query point is the sum of contributions from all stumps, where each stump's contribution is determined based on the query point's position relative to the stump's split point.\n\nThis approach efficiently builds an ensemble of stumps to model the target variable, ensuring that each iteration reduces the residual error until the desired tolerance is met."}
{"task_id": 34, "completion_id": 0, "completion": "To solve this problem, we need to generate a NumPy tensor filled with 0.0s and 1.0s based on a specified shape and sparsity value. The sparsity value represents the probability of generating a 1.0. Additionally, the solution must handle reproducibility using a seed and validate the input sparsity value.\n\n### Approach\n1. **Input Validation**: Check if the sparsity value is within the valid range [0, 1]. If not, return -1.\n2. **Seed Initialization**: If a seed is provided, initialize the NumPy random number generator with this seed to ensure reproducibility.\n3. **Generate Random Values**: Create a tensor of random values uniformly distributed between 0 and 1 using the specified shape.\n4. **Threshold Comparison**: Convert the random values to binary values (0.0 or 1.0) based on the threshold derived from the sparsity value. The threshold is calculated as \\(1 - \\text{sparsity}\\), where values greater than or equal to the threshold become 1.0, and others become 0.0.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None) -> np.ndarray | int:\n    \"\"\"Generates a binary tensor with specified shape and sparsity.\n\n    The tensor is filled with 0.0s and 1.0s, where each element has a probability\n    equal to 'sparsity' of being 1.0. If a seed is provided, the results are reproducible.\n\n    Args:\n        shape: The dimensions of the output tensor.\n        sparsity: Probability of an element being 1.0; must be in [0, 1].\n        seed: Optional seed for the random number generator.\n\n    Returns:\n        A numpy.ndarray with elements 0.0 and 1.0, or -1 if sparsity is invalid.\n    \"\"\"\n    if not (0.0 <= sparsity <= 1.0):\n        return -1\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    threshold = 1.0 - sparsity\n    random_tensor = np.random.rand(*shape)\n    binary_tensor = np.where(random_tensor >= threshold, 1.0, 0.0)\n    \n    return binary_tensor\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the sparsity value is within the valid range [0, 1]. If not, it returns -1 as specified.\n2. **Seed Initialization**: If a seed is provided, the NumPy random number generator is initialized with this seed to ensure that the same sequence of random numbers is generated each time, leading to reproducible results.\n3. **Random Tensor Generation**: Using `np.random.rand`, a tensor of the specified shape is generated with values uniformly distributed between 0 (inclusive) and 1 (exclusive).\n4. **Threshold Comparison**: The threshold is calculated as \\(1 - \\text{sparsity}\\). Each element in the random tensor is compared against this threshold. Elements greater than or equal to the threshold are set to 1.0, and those below are set to 0.0 using `np.where`.\n\nThis approach efficiently generates the desired binary tensor with the specified sparsity and shape while ensuring input validity and reproducibility through seed initialization."}
{"task_id": 39, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs a median split on a dataset along a specified column, which is a fundamental step in constructing a KD-Tree. The goal is to efficiently find the median element in the specified column and partition the dataset into elements strictly smaller and larger than this median value.\n\n### Approach\n1. **Identify the Median**: Use the `numpy.argpartition` function to find the median element in O(n) time. This function helps in partitioning the array such that the k-th element is in its sorted position, with all smaller elements before it and all larger elements after it.\n2. **Determine Pivot**: The pivot is the row index of the median element found using `argpartition`.\n3. **Partition Data**: Using the median value, create masks to identify rows with values strictly smaller and larger than the median. Convert these masks into lists of row indices, ensuring they are sorted for deterministic output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\"\"\"\n    n_samples = data.shape[0]\n    k = n_samples // 2\n    col = data[:, d]\n    partitioned_indices = np.argpartition(col, k)\n    pivot_idx = partitioned_indices[k]\n    med_val = col[pivot_idx]\n    \n    # Get left and right indices\n    left_mask = col < med_val\n    right_mask = col > med_val\n    \n    left_indices = np.flatnonzero(left_mask).tolist()\n    right_indices = np.flatnonzero(right_mask).tolist()\n    \n    # Sort the indices to ensure deterministic output\n    left_indices.sort()\n    right_indices.sort()\n    \n    return (pivot_idx, left_indices, right_indices)\n```\n\n### Explanation\n1. **Finding the Median**: The median is determined by partitioning the column using `np.argpartition`, which ensures that the k-th element (where k is the middle index) is in its correct sorted position. This allows us to find the median in O(n) time.\n2. **Pivot Identification**: The pivot index is the row index of the median value in the original dataset.\n3. **Partitioning**: Masks are created to filter rows with values strictly smaller and larger than the median. These masks are converted to lists of indices, which are then sorted to ensure the output is deterministic.\n\nThis approach efficiently partitions the dataset while adhering to the O(n) time complexity requirement for the median split, making it suitable for constructing KD-Trees."}
{"task_id": 40, "completion_id": 0, "completion": "To solve this problem, we need to convert a one-dimensional signal into overlapping frames using NumPy's stride manipulation techniques. This approach ensures that the resulting array is a view of the original data, not a copy, which is efficient in terms of memory usage. The solution must handle edge cases and enforce specific constraints on the input parameters.\n\n### Approach\n1. **Input Validation**: Check that the input array is one-dimensional, the stride is a positive integer, and the length of the input array is at least the frame width.\n2. **Calculate Number of Frames**: Determine the number of frames using the formula `(len(x) - frame_width) // stride + 1`.\n3. **Stride Calculation**: Compute the strides for the new array such that consecutive rows (frames) are separated by the specified hop length (stride).\n4. **Create Strided View**: Use NumPy's `as_strided` function to create a view of the original array with the calculated shape and strides.\n5. **Set Writeability**: Adjust the writeability of the resulting array based on the provided `writeable` flag.\n\n### Solution Code\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Validate input conditions\n    assert x.ndim == 1, \"x must be one-dimensional\"\n    assert stride >= 1, \"stride must be >= 1\"\n    assert len(x) >= frame_width, \"x is shorter than frame_width\"\n    \n    # Calculate the number of frames\n    n_frames = (len(x) - frame_width) // stride + 1\n    \n    # Determine the new shape and strides for the strided view\n    new_shape = (n_frames, frame_width)\n    new_strides = (stride * x.strides[0], x.strides[0])\n    \n    # Create the strided view\n    framed = as_strided(x, shape=new_shape, strides=new_strides)\n    \n    # Set the writeable flag as specified\n    if not writeable:\n        framed.flags.writeable = False\n    \n    return framed\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input array `x` is one-dimensional, the stride is at least 1, and the length of `x` is sufficient to form at least one frame. These checks ensure that the input parameters meet the necessary constraints.\n2. **Frame Calculation**: The number of frames is computed using integer division to determine how many complete frames can be extracted from the input array given the specified frame width and stride.\n3. **Stride Manipulation**: By adjusting the strides, we create a view where each row starts `stride` elements apart from the previous row, allowing efficient overlap without copying data.\n4. **Writeability Control**: The resulting array's writeability is set based on the `writeable` parameter, ensuring that the view can be made read-only if required.\n\nThis approach efficiently handles the conversion of the input signal into overlapping frames using NumPy's advanced stride manipulation, ensuring both memory efficiency and compliance with the specified constraints."}
{"task_id": 48, "completion_id": 0, "completion": "To solve this problem, we need to verify whether a given matrix is row-stochastic. A row-stochastic matrix is defined as a two-dimensional matrix where each element is a valid probability (between 0 and 1 inclusive) and each row sums to 1 within a small numerical tolerance.\n\n### Approach\n1. **Input Handling**: The function accepts either a list of lists or a numpy array. If the input is a list of lists, we first check that all elements are lists and that all rows have the same length to ensure it is a valid 2D structure. If the input is a numpy array, we directly use it.\n2. **Conversion to Numpy Array**: Convert the input to a numpy array if it is a list of lists. This allows us to leverage numpy's efficient array operations for subsequent checks.\n3. **Dimensionality Check**: Ensure the array is two-dimensional.\n4. **Numeric Check**: Verify that the numpy array has a numeric data type.\n5. **Element Range Check**: Ensure all elements are between 0 and 1 inclusive.\n6. **Row Sum Check**: Verify that each row sums to 1 within a tolerance of 1e-8 using numpy's `allclose` function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Handle list input\n    if isinstance(X, list):\n        # Check all elements are lists (ensuring 2D structure)\n        for row in X:\n            if not isinstance(row, list):\n                return False\n        # Check all rows have the same length\n        if X:\n            first_len = len(X[0])\n            for row in X:\n                if len(row) != first_len:\n                    return False\n        # Convert to numpy array\n        try:\n            arr = np.array(X)\n        except:\n            return False\n    elif isinstance(X, np.ndarray):\n        arr = X\n    else:\n        return False  # Invalid input type\n\n    # Check array is 2D\n    if arr.ndim != 2:\n        return False\n\n    # Check array has numeric data type\n    if not np.issubdtype(arr.dtype, np.number):\n        return False\n\n    # Check all elements are between 0 and 1 inclusive\n    if (arr < 0).any() or (arr > 1).any():\n        return False\n\n    # Check each row sums to 1 within tolerance\n    row_sums = arr.sum(axis=1)\n    if not np.allclose(row_sums, 1.0, atol=1e-8):\n        return False\n\n    return True\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input is a list of lists. If so, it ensures each element is a list and all rows have the same length. If the input is a numpy array, it proceeds directly.\n2. **Conversion and Checks**: The input is converted to a numpy array if necessary. The function then checks if the array is two-dimensional and has a numeric data type.\n3. **Element Range Check**: Using numpy operations, it verifies that all elements are within the range [0, 1].\n4. **Row Sum Check**: The function calculates the sum of each row and checks if all rows sum to 1 within a tolerance of 1e-8 using numpy's `allclose` function, which handles floating-point precision issues effectively.\n\nThis approach ensures that the function efficiently and accurately verifies all conditions required for a matrix to be row-stochastic."}
{"task_id": 55, "completion_id": 0, "completion": "To solve this problem, we need to implement the RMSprop optimization algorithm for ordinary least-squares (OLS) linear regression. The goal is to find the weight vector that minimizes the mean-squared error using mini-batches of data, while stopping early if the gradient's norm falls below a threshold or the maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Start with an all-zero weight vector and initialize the running average of squared gradients to zero.\n2. **Mini-batch Processing**: If the number of samples is less than the batch size, use the full dataset for each iteration. Otherwise, process the data in successive blocks (mini-batches) and wrap around when the end of the dataset is reached.\n3. **Gradient Calculation**: For each mini-batch, compute the gradient of the loss function.\n4. **Parameter Update**: Update the running average of squared gradients and adjust the weights using the RMSprop rule.\n5. **Stopping Condition**: After each parameter update, compute the full gradient to check if its norm falls below the threshold. If it does, stop early; otherwise, continue until the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\"\"\"\n    n, d = X.shape[0], X.shape[1]\n    w = np.zeros(d)\n    s = np.zeros_like(w)\n    start = 0\n\n    for _ in range(max_iter):\n        # Select the current batch\n        if n < batch_size:\n            X_batch = X\n            y_batch = y\n            m = n\n        else:\n            indices = (start + np.arange(batch_size)) % n\n            X_batch = X[indices]\n            y_batch = y[indices]\n            m = batch_size\n            start = (start + batch_size) % n\n\n        # Compute gradient for the current batch\n        residual_batch = X_batch @ w - y_batch\n        g_batch = X_batch.T @ residual_batch / m\n\n        # Update running average of squared gradients\n        s = rho * s + (1 - rho) * (g_batch ** 2)\n\n        # Update weights using RMSprop rule\n        w -= eta * g_batch / (np.sqrt(s) + eps_station)\n\n        # Compute full gradient for early stopping check\n        residual_full = X @ w - y\n        g_full = X.T @ residual_full / n\n        g_norm = np.linalg.norm(g_full)\n\n        if g_norm < epsilon:\n            break\n\n    # Round and convert to list\n    return np.round(w, 4).tolist()\n```\n\n### Explanation\n1. **Initialization**: The weight vector `w` and the running average `s` are initialized to zeros.\n2. **Mini-batch Selection**: Depending on whether the number of samples is less than the batch size, either the entire dataset is used or successive mini-batches are selected, wrapping around the dataset as needed.\n3. **Gradient Calculation**: For each mini-batch, compute the residual error and the gradient of the loss function using the mini-batch data.\n4. **Parameter Update**: The running average of squared gradients (`s`) is updated using the decay rate `rho`, and the weights (`w`) are adjusted using the RMSprop update rule.\n5. **Stopping Condition**: After each update, the full gradient is computed and its norm is checked against the threshold `epsilon`. If the norm is below the threshold, the optimization stops early; otherwise, it continues until the maximum number of iterations is reached.\n\nThis approach efficiently combines mini-batch processing for updates with full gradient checks for early stopping, ensuring both computational efficiency and convergence accuracy."}
{"task_id": 56, "completion_id": 0, "completion": "To solve this problem, we need to implement the SoftPlus activation function and its gradient (the sigmoid function) in a numerically stable manner. The SoftPlus function is defined as \\( \\text{SoftPlus}(x) = \\log(1 + e^x) \\), and its gradient is the sigmoid function \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). The challenge is to handle large values of \\( x \\) without causing numerical overflow or underflow.\n\n### Approach\n1. **Numerical Stability**: \n   - For the SoftPlus function, when \\( x \\) is large and positive, we compute it as \\( x + \\log(1 + e^{-x}) \\). This avoids overflow by using the identity \\( \\log(1 + e^x) = x + \\log(1 + e^{-x}) \\).\n   - When \\( x \\) is negative, we use \\( \\log(1 + e^x) \\) directly, which is computed accurately using the `log1p` function for small values of \\( e^x \\).\n   - For the sigmoid function, we use a piece-wise approach: \\( 1 / (1 + e^{-x}) \\) for non-negative \\( x \\) and \\( e^x / (1 + e^x) \\) for negative \\( x \\) to avoid overflow.\n\n2. **Input Handling**: \n   - The function accepts scalars, lists, or NumPy arrays. Convert the input to a NumPy array for uniform processing and then convert back to the original type after computation.\n\n3. **Rounding**: \n   - Results are rounded to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"Compute the SoftPlus activation function or its gradient (sigmoid) with numerical stability.\"\"\"\n    is_scalar = not isinstance(x, (list, np.ndarray))\n    x_np = np.array([x]) if is_scalar else np.asarray(x)\n    \n    if deriv:\n        # Compute the sigmoid (derivative) with stable computation\n        sigmoid = np.where(x_np >= 0,\n                           1.0 / (1.0 + np.exp(-x_np)),\n                           np.exp(x_np) / (1.0 + np.exp(x_np)))\n        output = sigmoid\n    else:\n        # Compute SoftPlus with stable computation\n        mask = x_np > 0\n        softplus_vals = np.zeros_like(x_np)\n        x_pos = x_np[mask]\n        softplus_vals[mask] = x_pos + np.log1p(np.exp(-x_pos))\n        x_neg = x_np[~mask]\n        softplus_vals[~mask] = np.log1p(np.exp(x_neg))\n        output = softplus_vals\n    \n    output_rounded = np.round(output, 4)\n    return output_rounded.item() if is_scalar else output_rounded.tolist()\n```\n\n### Explanation\n- **Numerical Stability**: The code uses piece-wise calculations to avoid overflow. For SoftPlus, when \\( x \\) is positive, it leverages the identity \\( \\log(1 + e^x) = x + \\log(1 + e^{-x}) \\). For the sigmoid function, it splits the computation into non-negative and negative regions to avoid overflow.\n- **Input Handling**: The input is converted to a NumPy array for processing, ensuring that both scalar and array inputs are handled uniformly.\n- **Rounding**: The results are rounded to four decimal places using NumPy's `round` function, ensuring the output meets the specified precision requirements.\n\nThis approach ensures that the function is both efficient and numerically stable, handling extreme values of \\( x \\) without overflow or underflow issues."}
{"task_id": 58, "completion_id": 0, "completion": "To solve this problem, we need to compute the log-probability of a given sequence using an unsmoothed Maximum-Likelihood Estimation (MLE) N-gram language model trained on a corpus. The solution involves building count tables for N-grams and their prefixes, then using these counts to compute the probabilities of each N-gram in the sequence.\n\n### Approach\n1. **Check Sequence Length**: If the sequence length is less than the order N, return 0.0 as there are no N-grams to process.\n2. **Generate N-gram Counts**: From the corpus, generate all possible N-grams and count their occurrences.\n3. **Generate Prefix Counts**: If N > 1, generate (N-1)-grams (prefixes) from the corpus and count their occurrences.\n4. **Compute Log-Probability**: For each N-gram in the sequence:\n   - Check if the N-gram exists in the corpus. If not, return negative infinity.\n   - For N > 1, check if the prefix (first N-1 elements of the N-gram) exists in the prefix counts. If not, return negative infinity.\n   - Compute the probability using the counts and sum the log-probabilities. If any count is zero, return negative infinity immediately.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    if len(sequence) < N:\n        return round(0.0, 4)\n    \n    # Generate N-gram counts from the corpus\n    corpus_ngrams = [tuple(corpus[i:i+N]) for i in range(len(corpus) - N + 1)]\n    count_ngrams = Counter(corpus_ngrams)\n    \n    # Generate (N-1)-gram counts if N > 1\n    count_prefix = None\n    if N > 1:\n        corpus_prefix_ngrams = [tuple(corpus[i:i+(N-1)]) for i in range(len(corpus) - (N-1) + 1)]\n        count_prefix = Counter(corpus_prefix_ngrams)\n    \n    total_log = 0.0\n    for i in range(len(sequence) - N + 1):\n        current_ngram = tuple(sequence[i:i+N])\n        numerator = count_ngrams.get(current_ngram, 0)\n        if numerator == 0:\n            return float('-inf')\n        \n        if N == 1:\n            denominator = len(corpus)\n        else:\n            prefix = current_ngram[:-1]\n            denominator = count_prefix.get(prefix, 0)\n        \n        if denominator == 0:\n            return float('-inf')\n        \n        prob = numerator / denominator\n        total_log += np.log(prob)\n    \n    return round(total_log, 4)\n```\n\n### Explanation\n1. **Check Sequence Length**: If the sequence is shorter than N, return 0.0 immediately as there are no N-grams to process.\n2. **Count N-grams**: Generate all possible N-grams from the corpus and count their occurrences using a `Counter`.\n3. **Count Prefixes**: For N > 1, generate all possible (N-1)-grams (prefixes) from the corpus and count their occurrences.\n4. **Iterate Over Sequence**: For each N-gram in the sequence:\n   - Check if the N-gram exists in the corpus. If not, return negative infinity.\n   - For N > 1, check if the prefix exists in the prefix counts. If not, return negative infinity.\n   - Compute the probability as the ratio of the N-gram count to the prefix count (or corpus length for unigrams) and accumulate the log-probability.\n5. **Return Result**: Sum all log-probabilities, round to 4 decimal places, and return the result. If any count check fails, return negative infinity immediately."}
{"task_id": 62, "completion_id": 0, "completion": "To solve this problem, we need to implement a univariate Lasso regression model using coordinate descent to minimize the Lasso objective function with polynomial feature expansion. The model should handle both regularization and ordinary least squares (OLS) solutions when the regularization strength is zero.\n\n### Approach\n1. **Polynomial Feature Expansion**: Construct a design matrix that includes polynomial features up to the specified degree. This matrix starts with a column of ones (for the bias term) followed by the original features raised to increasing powers up to the given degree.\n2. **Coordinate Descent Optimization**: Use coordinate descent to iteratively update each coefficient. The bias term (intercept) is updated first in each iteration using the mean of the residuals. Subsequent coefficients are updated using a soft thresholding operator to enforce sparsity.\n3. **Residual Update**: Efficiently update the residuals after each coefficient adjustment to ensure the correct residuals are used for subsequent updates.\n4. **Early Stopping**: Terminate the optimization process early if the maximum change in coefficients between iterations falls below a specified tolerance.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef lasso_regression(X: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\", degree: int, reg_factor: float, n_iterations: int = 1000, tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model with polynomial features using coordinate descent.\"\"\"\n    X = np.asarray(X).flatten()\n    y = np.asarray(y).flatten()\n    m = X.shape[0]\n    if m == 0:\n        return [0.0] * (degree + 1)\n    \n    # Create polynomial design matrix\n    X_poly = np.vander(X, degree + 1, increasing=True)\n    \n    # Initialize weights and residual\n    w = np.zeros(degree + 1)\n    residual = y - X_poly @ w  # Initial residual is y\n    \n    for _ in range(n_iterations):\n        max_delta = 0.0\n        \n        # Update the bias term (w0)\n        sum_other_terms = X_poly[:, 1:] @ w[1:]\n        w0_new = np.mean(y - sum_other_terms)\n        delta_w0 = abs(w0_new - w[0])\n        if delta_w0 > max_delta:\n            max_delta = delta_w0\n        residual -= (w0_new - w[0])  # Adjust residual for new w0\n        w[0] = w0_new\n        \n        # Update each weight w_j for j >= 1 using coordinate descent\n        for j in range(1, degree + 1):\n            Xj = X_poly[:, j]\n            Xj_squared_sum = Xj @ Xj\n            \n            if Xj_squared_sum == 0:\n                new_wj = 0.0\n            else:\n                rho_j = Xj @ residual + w[j] * Xj_squared_sum\n                # Soft thresholding\n                if abs(rho_j) <= reg_factor:\n                    new_wj = 0.0\n                else:\n                    new_wj = (np.sign(rho_j) * (abs(rho_j) - reg_factor)) / Xj_squared_sum\n            \n            delta = abs(new_wj - w[j])\n            if delta > max_delta:\n                max_delta = delta\n            \n            # Update residual\n            residual += (w[j] - new_wj) * Xj\n            w[j] = new_wj\n        \n        # Check for early stopping\n        if max_delta < tol:\n            break\n    \n    # Round coefficients to 4 decimal places\n    return [round(coef, 4) for coef in w]\n```\n\n### Explanation\n1. **Polynomial Feature Expansion**: The design matrix is created using `np.vander` with the `increasing=True` parameter to generate polynomial features up to the specified degree.\n2. **Coordinate Descent**: The algorithm iterates over each coefficient, updating the bias term first by computing the mean of the residuals. For each subsequent coefficient, the update involves computing the soft thresholding operator to apply L1 regularization.\n3. **Residual Adjustment**: After each coefficient update, the residuals are adjusted to reflect the new coefficient value, ensuring that subsequent updates use the most recent residuals.\n4. **Early Stopping**: The algorithm stops early if the maximum change in coefficients between iterations is below the specified tolerance, improving efficiency.\n\nThis approach efficiently handles both regularization and OLS solutions, providing a robust implementation of Lasso regression with polynomial features."}
{"task_id": 63, "completion_id": 0, "completion": "To solve this problem, we need to compute the backward probability vector \\(\\beta_t\\) for a given time index \\(t\\) in a Hidden Markov Model (HMM). The backward probability vector \\(\\beta_t(i)\\) represents the probability of observing the remaining sequence from time \\(t+1\\) to the end, given that the system is in state \\(i\\) at time \\(t\\).\n\n### Approach\n1. **Initialization**: Start with the base case where the backward probabilities for the last time step \\(T-1\\) are all 1.0, as there are no future observations beyond this point.\n2. **Iterative Calculation**: For each time step from \\(T-2\\) down to the given time \\(t\\), compute the backward probabilities using the formula:\n   \\[\n   \\beta_t(i) = \\sum_{j=1}^{N} a_{ij} \\cdot b_j(o_{t+1}) \\cdot \\beta_{t+1}(j)\n   \\]\n   Here, \\(a_{ij}\\) is the transition probability from state \\(i\\) to \\(j\\), \\(b_j(o_{t+1})\\) is the emission probability of observing \\(o_{t+1}\\) in state \\(j\\), and \\(\\beta_{t+1}(j)\\) is the backward probability for state \\(j\\) at time \\(t+1\\).\n\n3. **Backward Recursion**: Iterate from the second last time step backward to the given time \\(t\\), updating the backward probabilities at each step using the previously computed values from the next time step.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"Compute the backward probability vector beta_t for the given HMM parameters and observation sequence.\"\"\"\n    N = len(A)\n    T = len(obs)\n    \n    if t == T - 1:\n        return [round(1.0, 4) for _ in range(N)]\n    \n    beta_prev = [1.0 for _ in range(N)]\n    \n    for s in range(T-2, t-1, -1):\n        o = obs[s+1]\n        beta_current = []\n        for i in range(N):\n            total = 0.0\n            for j in range(N):\n                total += A[i][j] * B[j][o] * beta_prev[j]\n            beta_current.append(total)\n        beta_prev = beta_current\n    \n    return [round(x, 4) for x in beta_prev]\n```\n\n### Explanation\n1. **Initialization**: The function first checks if the given time index \\(t\\) is the last time step (i.e., \\(T-1\\)). If so, it returns a list of 1.0s rounded to four decimal places, as there are no future observations to consider.\n2. **Backward Iteration**: Starting from the second last time step \\(T-2\\), the function iterates backward to the given time \\(t\\). For each time step \\(s\\), it calculates the backward probabilities for each state \\(i\\) by summing over all possible transitions to future states \\(j\\), considering the transition probabilities, emission probabilities, and the previously computed backward probabilities.\n3. **Updating Probabilities**: The backward probabilities for each time step are computed iteratively and stored in a list, which is updated at each step until the desired time \\(t\\) is reached. The final result is rounded to four decimal places and returned.\n\nThis approach efficiently computes the backward probabilities using dynamic programming, ensuring that each step builds on the previous computations, leading to an optimal solution."}
{"task_id": 65, "completion_id": 0, "completion": "To solve this problem, we need to compute the probability of a given observation sequence being generated by a Hidden Markov Model (HMM) using the backward algorithm. The backward algorithm efficiently calculates this probability by recursively computing backward variables starting from the last observation and moving backward in time.\n\n### Approach\n1. **Input Validation**: Check if any of the input lists (transition matrix `A`, emission matrix `B`, initial distribution `pi`, or observation sequence `obs`) are empty. If any of them are empty, return 0.0 immediately as no valid probability can be computed.\n2. **Convert Inputs to Numpy Arrays**: Convert the input lists to numpy arrays for efficient matrix operations.\n3. **Initialize Backward Variables**: Start with the backward variables for the last time step, which are all initialized to 1.\n4. **Recursive Calculation**: Compute the backward variables for each preceding time step using the formula that involves the transition probabilities, emission probabilities, and the previously computed backward variables.\n5. **Compute Total Probability**: Combine the initial state probabilities, emission probabilities of the first observation, and the computed backward variables to get the total probability of the observation sequence.\n6. **Round Result**: Return the result rounded to six decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    # Check for empty inputs\n    if not A or not B or not pi or not obs:\n        return 0.0\n    \n    # Convert lists to numpy arrays for efficient computation\n    A = np.array(A, dtype=np.float64)\n    B = np.array(B, dtype=np.float64)\n    pi = np.array(pi, dtype=np.float64)\n    obs = np.array(obs, dtype=int)\n    \n    T = len(obs)\n    N = len(pi)\n    \n    # If there are no observations, return 0.0\n    if T == 0:\n        return 0.0\n    \n    # Initialize the backward variable for the last time step (T-1)\n    beta_prev = np.ones(N, dtype=np.float64)\n    \n    # Iterate from T-2 down to 0\n    for t in reversed(range(T - 1)):\n        # Get the observation index at t+1\n        o = obs[t + 1]\n        # Compute the product of emission probabilities and beta_prev\n        temp = B[:, o] * beta_prev\n        # Update beta_prev by multiplying with transition matrix\n        beta_prev = A.dot(temp)\n    \n    # Calculate the total probability using the initial distribution, first observation, and beta_0\n    total = np.sum(pi * B[:, obs[0]] * beta_prev)\n    \n    # Round to six decimal places\n    return round(total, 6)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if any of the input lists are empty. If so, it returns 0.0 immediately.\n2. **Conversion to Numpy Arrays**: The input lists are converted to numpy arrays to leverage efficient matrix operations.\n3. **Initialization**: The backward variables for the last time step are initialized to 1 since there are no future observations beyond the last time step.\n4. **Recursive Calculation**: For each time step from the second last to the first, the backward variables are updated using the transition probabilities, emission probabilities of the next observation, and the previously computed backward variables.\n5. **Total Probability Calculation**: The total probability is computed by combining the initial state probabilities, emission probabilities of the first observation, and the backward variables from the first time step.\n6. **Rounding**: The result is rounded to six decimal places to meet the specified precision requirement.\n\nThis approach efficiently computes the probability using dynamic programming and matrix operations, ensuring both correctness and performance."}
{"task_id": 69, "completion_id": 0, "completion": "To solve this problem, we need to compute the likelihood of an observation sequence given a Hidden Markov Model (HMM) using the forward algorithm. The HMM is defined by an initial state probability vector, a state transition matrix, and an emission matrix. The solution involves validating the input dimensions and observation indices, then applying the forward algorithm to compute the probability of the observation sequence.\n\n### Approach\n1. **Input Validation**: \n   - Check the dimensions of the initial state probabilities, state transition matrix, and emission matrix.\n   - Ensure all observation indices are within the valid range defined by the emission matrix.\n   \n2. **Forward Algorithm**:\n   - **Initialization**: Compute the initial forward probabilities using the initial state probabilities and the emission probabilities for the first observation.\n   - **Iteration**: For each subsequent observation, update the forward probabilities by summing over all possible previous states, multiplying by the transition probabilities, and then by the emission probabilities for the current observation.\n   - **Termination**: Sum the final forward probabilities to get the total likelihood of the observation sequence.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Validate input dimensions\n    n = len(S)\n    if n == 0:\n        return -1.0\n    \n    # Check state-transition matrix A is n x n\n    if len(A) != n:\n        return -1.0\n    for row in A:\n        if len(row) != n:\n            return -1.0\n    \n    # Check emission matrix B is n x m\n    if len(B) != n:\n        return -1.0\n    if n == 0:\n        return -1.0  # redundant check as n is already non-zero\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1.0\n    \n    # Check observations are valid\n    if not observations:\n        return -1.0\n    for obs in observations:\n        if not isinstance(obs, int) or obs < 0 or obs >= m:\n            return -1.0\n    \n    # Initialize forward probabilities\n    T = len(observations)\n    alpha_prev = [S[i] * B[i][observations[0]] for i in range(n)]\n    \n    # Iterate through the rest of the observations\n    for t in range(1, T):\n        current_obs = observations[t]\n        alpha_current = []\n        for j in range(n):\n            total = 0.0\n            for i in range(n):\n                total += alpha_prev[i] * A[i][j]\n            alpha_current.append(total * B[j][current_obs])\n        alpha_prev = alpha_current\n    \n    # Compute the final probability and round to 4 decimal places\n    final_prob = sum(alpha_prev)\n    return round(final_prob, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input matrices (S, A, B) have the correct dimensions. It also ensures that all observation indices are valid and within the range defined by the emission matrix.\n2. **Initialization**: The initial forward probabilities are calculated by multiplying the initial state probabilities by the emission probabilities corresponding to the first observation.\n3. **Iteration**: For each subsequent observation, the forward probabilities are updated by considering all possible transitions from previous states, summing these probabilities, and then multiplying by the emission probability for the current observation.\n4. **Termination**: The final probability is obtained by summing the forward probabilities at the last time step, which gives the total likelihood of the observation sequence. This result is then rounded to four decimal places for the final output."}
{"task_id": 70, "completion_id": 0, "completion": "To solve this problem, we need to implement the FP-Growth algorithm to discover all frequent item-sets in a transactional database. The FP-Growth algorithm efficiently finds frequent item-sets without generating candidate item-sets explicitly, using a prefix tree (FP-tree) structure and recursive mining of conditional FP-trees.\n\n### Approach\n1. **Count Item Frequencies**: First, count the frequency of each item in the transactions and filter out items that do not meet the minimum support threshold.\n2. **Build the FP-Tree**: Construct an FP-tree where each node represents an item and its count in the transactions. The tree is built by processing each transaction, filtering out infrequent items, and sorting the remaining items by their frequency.\n3. **Mine the FP-Tree**: Recursively mine the FP-tree starting from the least frequent items. For each item, generate its conditional pattern base, construct a conditional FP-tree, and recursively mine this tree to discover larger item-sets.\n\n### Solution Code\n```python\nfrom collections import Counter, defaultdict\n\nclass TreeNode:\n    def __init__(self, item, count, parent):\n        self.item = item\n        self.count = count\n        self.parent = parent\n        self.children = {}\n        self.node_link = None\n\ndef build_fp_tree(transactions_with_counts, min_support):\n    item_counts = defaultdict(int)\n    for transaction, count in transactions_with_counts:\n        for item in transaction:\n            item_counts[item] += count\n\n    frequent_items = {item for item, cnt in item_counts.items() if cnt >= min_support}\n    if not frequent_items:\n        return None, None\n\n    sorted_items = sorted(frequent_items, key=lambda x: (-item_counts[x], x))\n\n    root = TreeNode(None, 0, None)\n    header_table = {}\n    for item in sorted_items:\n        header_table[item] = [item_counts[item], None]\n\n    for transaction, count in transactions_with_counts:\n        sorted_transaction = [item for item in sorted_items if item in transaction]\n        if not sorted_transaction:\n            continue\n        current_node = root\n        for item in sorted_transaction:\n            if item in current_node.children:\n                current_node = current_node.children[item]\n                current_node.count += count\n            else:\n                new_node = TreeNode(item, count, current_node)\n                current_node.children[item] = new_node\n                current_node = new_node\n                if header_table[item][1] is None:\n                    header_table[item][1] = new_node\n                else:\n                    last_node = header_table[item][1]\n                    while last_node.node_link is not None:\n                        last_node = last_node.node_link\n                    last_node.node_link = new_node\n\n    return root, header_table\n\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    if not transactions or min_support < 1:\n        return []\n\n    item_counts = defaultdict(int)\n    for transaction in transactions:\n        unique_items = set(transaction)\n        for item in unique_items:\n            item_counts[item] += 1\n\n    frequent_items = {item: count for item, count in item_counts.items() if count >= min_support}\n    if not frequent_items:\n        return []\n\n    sorted_items = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n\n    filtered_transactions = []\n    for trans in transactions:\n        unique_trans = list(set(trans))\n        filtered = [item for item in unique_trans if item in frequent_items]\n        if filtered:\n            filtered.sort(key=lambda x: sorted_items.index(x))\n            filtered_transactions.append(filtered)\n\n    if not filtered_transactions:\n        return []\n\n    transactions_with_counts = [(t, 1) for t in filtered_transactions]\n    root, header_table = build_fp_tree(transactions_with_counts, min_support)\n    if not header_table:\n        return []\n\n    header_list = sorted(header_table.items(), key=lambda x: (-x[1][0], x[0]))\n\n    frequent_itemsets = []\n\n    def mine_tree(header_list, prefix, frequent_itemsets):\n        for item, (support, head_node) in reversed(header_list):\n            new_prefix = prefix.copy()\n            new_prefix.append(item)\n            new_prefix_sorted = sorted(new_prefix)\n            frequent_itemsets.append(tuple(new_prefix_sorted))\n\n            conditional_patterns = []\n            node = head_node\n            while node is not None:\n                path = []\n                current = node.parent\n                while current.parent is not None:\n                    path.append(current.item)\n                    current = current.parent\n                path.reverse()\n                if path:\n                    conditional_patterns.append((path, node.count))\n                node = node.node_link\n\n            if conditional_patterns:\n                cond_root, cond_header_table = build_fp_tree(conditional_patterns, min_support)\n                if cond_header_table is not None:\n                    cond_header_list = sorted(cond_header_table.items(), key=lambda x: (-x[1][0], x[0]))\n                    mine_tree(cond_header_list, new_prefix_sorted, frequent_itemsets)\n\n    mine_tree(header_list, [], frequent_itemsets)\n\n    unique_itemsets = set(frequent_itemsets)\n    sorted_itemsets = [list(items) for items in unique_itemsets]\n    sorted_itemsets.sort(key=lambda x: (len(x), x))\n    return sorted_itemsets\n```\n\n### Explanation\n1. **Counting and Filtering Items**: The algorithm starts by counting the frequency of each item in the transactions and filtering out those that do not meet the minimum support threshold.\n2. **Building the FP-Tree**: Transactions are processed to include only frequent items, sorted by their frequency. The FP-tree is constructed by inserting each transaction into the tree, incrementing counts for existing nodes or creating new nodes as needed.\n3. **Mining the FP-Tree**: The tree is mined recursively starting from the least frequent items. For each item, conditional pattern bases are generated, and conditional FP-trees are built. These trees are recursively mined to discover all frequent item-sets, which are then deduplicated and sorted according to the specified criteria.\n\nThis approach efficiently discovers all frequent item-sets using the FP-Growth algorithm, leveraging the FP-tree structure to avoid the expensive candidate generation step of traditional methods."}
{"task_id": 75, "completion_id": 0, "completion": "To solve this problem, we need to implement a k-Nearest Neighbor (k-NN) classifier that assigns class labels to test samples based on the majority vote of their nearest neighbors in the training set. The solution must handle invalid values of k appropriately and efficiently compute distances using the Euclidean metric.\n\n### Approach\n1. **Check Validity of k**: Ensure that k is a positive integer and does not exceed the number of training samples. If invalid, return a list of -1s for each test sample.\n2. **Convert Data to NumPy Arrays**: Convert the training and test data into NumPy arrays for efficient computation of distances.\n3. **Compute Distances**: For each test sample, compute the squared Euclidean distances to all training samples to avoid unnecessary square root operations.\n4. **Find Nearest Neighbors**: Identify the k nearest neighbors by sorting the distances and selecting the smallest k indices.\n5. **Majority Vote**: Count the class labels of the k nearest neighbors and determine the majority class. In case of a tie, return the smallest class label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\"\"\"\n    n_train = len(X_train)\n    n_test = len(X_test)\n    \n    # Check if k is invalid\n    if not isinstance(k, int) or k <= 0 or k > n_train:\n        return [-1] * n_test\n    \n    # Convert to numpy arrays for efficient computation\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n    \n    predictions = []\n    for x in X_test_np:\n        # Calculate squared Euclidean distances\n        distances = np.sum((X_train_np - x) ** 2, axis=1)\n        \n        # Get indices of k smallest distances\n        nearest_indices = np.argsort(distances)[:k]\n        \n        # Get corresponding labels\n        nearest_labels = y_train_np[nearest_indices]\n        \n        # Count occurrences of each label\n        label_counts = {}\n        for label in nearest_labels:\n            label_counts[label] = label_counts.get(label, 0) + 1\n        \n        # Determine the label with the maximum count, breaking ties by smallest label\n        max_count = max(label_counts.values())\n        candidates = [label for label, count in label_counts.items() if count == max_count]\n        predicted_label = min(candidates)\n        predictions.append(predicted_label)\n    \n    return predictions\n```\n\n### Explanation\n1. **Validity Check**: The function first checks if k is a valid positive integer within the range of the number of training samples. If not, it returns a list of -1s.\n2. **Data Conversion**: Training and test data are converted to NumPy arrays to leverage vectorized operations for efficient distance calculations.\n3. **Distance Calculation**: Squared Euclidean distances are computed for each test sample relative to all training samples, avoiding the computationally expensive square root operation.\n4. **Nearest Neighbors Identification**: Using argsort, the indices of the k smallest distances are identified, and corresponding class labels are retrieved.\n5. **Majority Voting**: The class labels of the nearest neighbors are counted, and the label with the highest count is selected. In case of a tie, the smallest class label is chosen.\n\nThis approach ensures efficient computation and correct handling of edge cases, providing accurate predictions based on the k-NN algorithm."}
{"task_id": 76, "completion_id": 0, "completion": "To solve this problem, we need to compute the categorical cross-entropy loss for a batch of one-hot encoded targets. This loss function is commonly used in classification tasks to measure the difference between predicted probabilities and true labels. The key steps involve converting inputs to numpy arrays for efficient computation, ensuring numerical stability by adding a small epsilon value, and correctly applying the cross-entropy formula.\n\n### Approach\n1. **Input Conversion**: Convert the input labels (y) and predicted probabilities (y_pred) into numpy arrays to leverage efficient element-wise operations.\n2. **Numerical Stability**: Add a small value (machine epsilon) to the predicted probabilities to avoid taking the logarithm of zero, which would be numerically unstable.\n3. **Logarithm Calculation**: Compute the natural logarithm of the adjusted predicted probabilities.\n4. **Element-wise Multiplication**: Multiply the true labels with the computed logarithms. Since the true labels are one-hot encoded, this step effectively isolates the log probability of the true class for each sample.\n5. **Summation and Negation**: Sum all the resulting values and take the negative to obtain the loss.\n6. **Rounding**: Round the computed loss to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | np.ndarray, y_pred: list | np.ndarray) -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y)\n    y_pred = np.asarray(y_pred)\n    eps = np.finfo(y_pred.dtype).eps\n    log_probs = np.log(y_pred + eps)\n    loss = -np.sum(y * log_probs)\n    return round(loss.item(), 4)\n```\n\n### Explanation\n1. **Input Conversion**: The inputs `y` and `y_pred` are converted to numpy arrays using `np.asarray` to handle both list and numpy array inputs uniformly.\n2. **Numerical Stability**: Machine epsilon, the smallest positive number such that `1.0 + eps != 1.0` for the data type of `y_pred`, is added to each element of `y_pred` to prevent taking the logarithm of zero.\n3. **Logarithm Calculation**: The logarithm of the adjusted predicted probabilities is computed using `np.log`.\n4. **Element-wise Multiplication**: The product of the true labels `y` and the log probabilities is computed. Since `y` is one-hot encoded, this operation effectively selects the log probability of the true class for each sample.\n5. **Summation and Negation**: The sum of all elements in the resulting matrix is computed and negated to obtain the loss value.\n6. **Rounding**: The final loss value is rounded to four decimal places using Python's built-in `round` function to meet the specified precision requirement.\n\nThis approach efficiently computes the cross-entropy loss while ensuring numerical stability and correctness, leveraging numpy's optimized operations for performance."}
{"task_id": 77, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward propagation for an L-layer neural network using ReLU activation for the hidden layers and sigmoid activation for the output layer. The goal is to compute the final activation matrix and collect necessary intermediate values (caches) for potential use in backpropagation.\n\n### Approach\n1. **Initialization**: Determine the number of layers (L) from the parameters dictionary. Initialize the activation from the input data (X) and an empty list to store caches.\n2. **Layer Iteration**: Loop through each layer from 1 to L:\n   - **Linear Step**: Compute the linear transformation using the weights (W) and biases (b) from the parameters dictionary.\n   - **Activation Step**: Apply ReLU activation for hidden layers (1 to L-1) and sigmoid activation for the output layer (L).\n   - **Cache Storage**: Store the intermediate values (input activations, weights, biases, and linear output) needed for backpropagation in the caches list.\n3. **Final Output**: After processing all layers, round the final activation matrix (AL) to 4 decimal places and convert it to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> tuple[list[list[float]], list]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\"\"\"\n\n    L = len(parameters) // 2\n    caches = []\n    A_prev = X\n\n    for l in range(1, L + 1):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        Z = np.dot(W, A_prev) + b\n        if l == L:\n            A = 1 / (1 + np.exp(-Z))  # Sigmoid\n        else:\n            A = np.maximum(0, Z)       # ReLU\n\n        linear_cache = (A_prev, W, b)\n        activation_cache = Z\n        caches.append((linear_cache, activation_cache))\n        A_prev = A\n\n    AL_rounded = np.round(A_prev, 4)\n    return AL_rounded.tolist(), caches\n```\n\n### Explanation\n1. **Initialization**: The number of layers (L) is determined by dividing the length of the parameters dictionary by 2 (since each layer has a weight matrix and a bias vector).\n2. **Layer Processing**:\n   - For each layer, the linear transformation is computed using the current weights and biases.\n   - ReLU activation is applied for hidden layers, which introduces non-linearity by outputting the positive part of its input.\n   - Sigmoid activation is applied to the output layer, squashing the values between 0 and 1, suitable for binary classification.\n3. **Caching**: Intermediate values (input activations, weights, biases, and linear outputs) are stored in caches to facilitate efficient gradient computation during backpropagation.\n4. **Output Processing**: The final activation matrix (AL) is rounded to 4 decimal places and converted to a Python list to meet the specified output format.\n\nThis approach ensures that the neural network processes each layer correctly, applies the appropriate activation functions, and prepares necessary intermediate values for potential backpropagation, all while returning the final activations in the required format."}
{"task_id": 81, "completion_id": 0, "completion": "To solve this problem, we need to compute the binary cross-entropy loss (log-loss) between the predicted probabilities from a binary classifier and the ground-truth labels. This loss function is crucial for evaluating the performance of the classifier, as it penalizes incorrect predictions more heavily the more confident they are.\n\n### Approach\n1. **Clipping Predictions**: Since taking the logarithm of zero is undefined, we first clip the predicted probabilities to a small interval around [0, 1]. Specifically, we use \u03b5 = 1e-15 to avoid numerical instability.\n2. **Logarithmic Terms Calculation**: For each sample, compute two terms: one involving the logarithm of the predicted probability (for actual label 1) and another involving the logarithm of one minus the predicted probability (for actual label 0).\n3. **Summation and Averaging**: Sum these terms across all samples, then average and negate the result to get the final cost.\n4. **Rounding**: The computed cost is rounded to six decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    m = A2.size\n    log_probs = Y * np.log(A2_clipped) + (1 - Y) * np.log(1 - A2_clipped)\n    cost = -np.sum(log_probs) / m\n    return round(cost.item(), 6)\n```\n\n### Explanation\n1. **Clipping**: The `np.clip` function ensures that all elements of `A2` are within [\u03b5, 1-\u03b5], preventing division by zero or taking the logarithm of zero.\n2. **Logarithmic Terms**: The terms `Y * np.log(A2_clipped)` and `(1 - Y) * np.log(1 - A2_clipped)` compute the contributions of each sample to the loss, depending on whether the true label is 1 or 0.\n3. **Summation and Averaging**: By summing these terms and dividing by the number of samples `m`, we average the loss over all samples. Multiplying by -1 converts the sum of negative log probabilities into a positive loss value.\n4. **Rounding**: The result is rounded to six decimal places using Python's `round` function to meet the specified precision requirement.\n\nThis approach efficiently computes the binary cross-entropy loss using vectorized operations, ensuring both numerical stability and computational efficiency."}
{"task_id": 82, "completion_id": 0, "completion": "To solve this problem, we need to implement the \u03b5-soft exploration policy in reinforcement learning. This policy ensures that every action has a non-zero probability of being selected while still favoring the greedy (best) action. The goal is to compute the probabilities for each action given a set of Q-values and an exploration parameter \u03b5.\n\n### Approach\n1. **Identify the Greedy Action**: Determine the action with the highest Q-value. In case of ties, select the first occurrence (smallest index).\n2. **Calculate Base Probability**: Compute the base probability for non-greedy actions as \u03b5 divided by the number of actions.\n3. **Adjust Probability for Greedy Action**: The greedy action's probability is adjusted to be higher than the base probability by adding (1 - \u03b5) to the base probability of the greedy action.\n4. **Ensure Numerical Stability**: Convert the resulting probabilities to a list, round each to four decimal places, and ensure the sum of the probabilities is 1 within a numerical tolerance of 1e-12.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef epsilon_soft(Q: list[float] | np.ndarray, epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list must contain the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    Q_array = np.asarray(Q)\n    n = Q_array.size\n    if n == 0:\n        return []\n    a_star = np.argmax(Q_array)\n    base = epsilon / n\n    probs = np.full(n, base, dtype=np.float64)\n    probs[a_star] += 1 - epsilon\n    # Round to 4 decimal places using numpy's around to ensure consistent rounding\n    rounded_probs = np.round(probs, 4)\n    # Convert to list of Python floats (to match the required output type)\n    return rounded_probs.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input Q-values are converted to a NumPy array for easier manipulation.\n2. **Determine Number of Actions**: The number of actions `n` is determined from the size of the Q-values array.\n3. **Identify Greedy Action**: Using `np.argmax`, the index of the first occurrence of the maximum Q-value is found.\n4. **Compute Base Probability**: The base probability for non-greedy actions is calculated as \u03b5/n.\n5. **Adjust Greedy Action Probability**: The probability of the greedy action is increased by (1 - \u03b5) to ensure it has a higher chance of being selected.\n6. **Rounding**: Each probability is rounded to four decimal places to meet the problem's requirements.\n7. **Return as List**: The probabilities are converted back to a list of floats and returned.\n\nThis approach ensures that the probabilities are computed efficiently and meet the numerical stability requirements, while adhering to the \u03b5-soft exploration policy constraints."}
{"task_id": 86, "completion_id": 0, "completion": "To solve this problem, we need to aggregate the predictions from multiple decision trees in a Random Forest classifier using majority voting. The final prediction for each sample is determined by the most frequent class label among the trees, with ties broken by selecting the smallest label.\n\n### Approach\n1. **Transpose the Predictions**: Convert the input list of lists (where each inner list represents predictions from a single tree) into a structure where each element corresponds to all predictions for a single sample. This is done using the `zip` function.\n2. **Count Votes**: For each sample, count the occurrences of each class label using the `Counter` class from the `collections` module.\n3. **Determine Majority Vote**: Identify the class label(s) with the highest count. If there is a tie (multiple labels with the same highest count), select the smallest label by sorting the candidates and choosing the first element.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    samples = zip(*predictions)\n    aggregated = []\n    for sample_votes in samples:\n        counts = Counter(sample_votes)\n        max_count = max(counts.values())\n        candidates = [k for k, v in counts.items() if v == max_count]\n        candidates_sorted = sorted(candidates)\n        aggregated.append(candidates_sorted[0])\n    return aggregated\n```\n\n### Explanation\n1. **Transposing the Predictions**: The `zip(*predictions)` operation converts rows of tree predictions into columns of sample predictions, allowing us to process each sample's votes individually.\n2. **Counting Votes**: Using `Counter`, we efficiently count the occurrences of each class label for every sample.\n3. **Handling Ties**: After identifying the class labels with the highest votes, we sort these labels and select the smallest one to break any ties. This ensures consistent results for both numeric and string labels, following natural ordering rules.\n\nThis approach efficiently aggregates predictions using majority voting, ensuring correctness and handling ties appropriately by leveraging sorting and counting mechanisms."}
{"task_id": 88, "completion_id": 0, "completion": "To solve this problem, we need to implement the softplus activation function in a numerically stable manner using NumPy's `logaddexp` function. The solution should handle scalar, list, and NumPy array inputs, returning the results rounded to 4 decimal places and converted to the appropriate Python data type.\n\n### Approach\n1. **Understanding the Softplus Function**: The softplus function is defined as \\( \\text{softplus}(z) = \\ln(1 + e^z) \\). Direct computation using exponentials can lead to overflow or underflow for extreme values of \\( z \\). Using `np.logaddexp(0.0, z)` ensures numerical stability by computing \\( \\ln(e^0 + e^z) \\) accurately.\n2. **Handling Different Input Types**: The function should accept scalars, Python lists, or NumPy arrays. For scalar inputs, the result is returned as a float. For array-like inputs, the result is converted to a Python list using NumPy's `tolist()` method.\n3. **Rounding**: The results are rounded to 4 decimal places to meet the problem's precision requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        s = np.logaddexp(0.0, z)\n        rounded = np.round(s, 4)\n        return float(rounded)\n    else:\n        z_array = np.asarray(z)\n        s = np.logaddexp(0.0, z_array)\n        s_rounded = np.round(s, 4)\n        return s_rounded.tolist()\n```\n\n### Explanation\n1. **Numerical Stability**: By using `np.logaddexp(0.0, z)`, we avoid the potential overflow issues with direct computation of \\( e^z \\) for large values of \\( z \\), and underflow issues for large negative values of \\( z \\).\n2. **Input Handling**: The function checks if the input is a scalar (int or float). If so, it computes the softplus value, rounds it, and returns it as a float. For array-like inputs, the input is converted to a NumPy array, processed element-wise, rounded, and then converted back to a Python list using `tolist()`.\n3. **Rounding**: The results are rounded to 4 decimal places using `np.round` to ensure precision, maintaining the required format for both scalar and array-like outputs.\n\nThis approach efficiently handles different input types and ensures numerical stability while providing accurate and formatted results."}
{"task_id": 90, "completion_id": 0, "completion": "To solve this problem, we need to evaluate the performance of a policy in a multi-armed bandit problem by computing the mean-squared error (MSE) between the true expected payouts of each arm and the policy's current estimates of those payouts. The MSE is a measure of how close the policy's estimates are to the true values.\n\n### Approach\n1. **Check for Valid Estimates**: First, we need to ensure that the policy has valid estimates. If the policy does not have any estimates (either missing or an empty dictionary), we return `numpy.nan`.\n2. **Retrieve True Expected Values**: Retrieve the true expected values of the bandit's arms. If these values are missing or not in the correct format, return `numpy.nan`.\n3. **Check Arm Consistency**: Ensure that the policy's estimates cover all the arms present in the bandit. If any arm is missing from the policy's estimates, return `numpy.nan`.\n4. **Compute Squared Errors**: For each arm, compute the squared error between the true expected value and the policy's estimate.\n5. **Calculate MSE**: Average the squared errors and round the result to four decimal places to get the final MSE.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\"\"\"\n    # Check if policy has ev_estimates and it's a non-empty dictionary\n    ev_estimates = None\n    if hasattr(policy, 'ev_estimates'):\n        ev_estimates = policy.ev_estimates\n    elif isinstance(policy, dict) and 'ev_estimates' in policy:\n        ev_estimates = policy['ev_estimates']\n    else:\n        return np.nan\n    \n    if not isinstance(ev_estimates, dict) or len(ev_estimates) == 0:\n        return np.nan\n    \n    # Retrieve arm_evs from bandit and validate\n    arm_evs = None\n    if hasattr(bandit, 'arm_evs'):\n        arm_evs = bandit.arm_evs\n    elif isinstance(bandit, dict) and 'arm_evs' in bandit:\n        arm_evs = bandit['arm_evs']\n    else:\n        return np.nan\n    \n    if not isinstance(arm_evs, (list, tuple)):\n        return np.nan\n    \n    n_arms = len(arm_evs)\n    \n    # Ensure all arm indices are present in ev_estimates\n    for i in range(n_arms):\n        if i not in ev_estimates:\n            return np.nan\n    \n    squared_errors = []\n    for i in range(n_arms):\n        true_val = arm_evs[i]\n        estimate_val = ev_estimates[i]\n        squared_error = (estimate_val - true_val) ** 2\n        squared_errors.append(squared_error)\n    \n    average_mse = sum(squared_errors) / n_arms\n    return round(average_mse, 4)\n```\n\n### Explanation\n1. **Checking Policy Estimates**: The code first checks if the policy's estimates (`ev_estimates`) exist and are in the correct format (a non-empty dictionary). If not, it returns `numpy.nan`.\n2. **Retrieving True Values**: The true expected values (`arm_evs`) are retrieved from the bandit. If these values are missing or not a list/tuple, the function returns `numpy.nan`.\n3. **Consistency Check**: The code ensures that the policy's estimates cover all arms present in the bandit. If any arm is missing, it returns `numpy.nan`.\n4. **Computing Errors**: For each arm, the squared difference between the true value and the policy's estimate is computed and stored.\n5. **Averaging and Rounding**: The average of these squared errors is computed, rounded to four decimal places, and returned as the final MSE.\n\nThis approach ensures that all edge cases are handled, and the MSE is computed accurately based on the policy's current estimates."}
{"task_id": 96, "completion_id": 0, "completion": "To solve this problem, we need to compute the additive-smoothed log-probability of a given sentence using an N-gram language model. The solution involves building frequency tables from a training corpus, handling unseen words, and applying additive smoothing to estimate probabilities.\n\n### Approach\n1. **Build Vocabulary and N-gram Counts**: \n   - Extract all unique words from the training corpus to form the vocabulary.\n   - Generate n-gram frequency tables for n from 1 to N using the training corpus.\n\n2. **Preprocess Target Sentence**:\n   - Replace any word in the target sentence that is not present in the training corpus vocabulary with the \"<UNK>\" token.\n\n3. **Check Sentence Length**:\n   - If the length of the preprocessed target sentence is less than N, return 0.0 as no valid N-grams can be formed.\n\n4. **Compute Probabilities with Smoothing**:\n   - For each N-gram in the target sentence, compute the probability using additive smoothing. The formula used is:\n     \\[\n     P(w_i | \\text{context}) = \\frac{\\text{count}(\\text{context} \\circ w_i) + K}{\\text{count}(\\text{context}) + K \\cdot |V|}\n     \\]\n   - Here, \\( |V| \\) is the size of the vocabulary including the \"<UNK>\" token.\n\n5. **Sum Log-Probabilities**:\n   - Convert each probability to its natural logarithm and sum these values. Round the result to four decimal places.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter, defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    # Step 1: Collect all words in the corpus to build the training vocabulary\n    training_vocab = set()\n    for sentence in corpus:\n        tokens = sentence.split()\n        training_vocab.update(tokens)\n    \n    # Vocabulary size including <UNK>\n    V_size = len(training_vocab) + 1\n    \n    # Step 2: Build n-gram frequency tables for n from 1 to N\n    ngram_counts = defaultdict(Counter)\n    for sentence in corpus:\n        tokens = sentence.split()\n        for n in range(1, N + 1):\n            for i in range(len(tokens) - n + 1):\n                ngram = tuple(tokens[i:i+n])\n                ngram_counts[n][ngram] += 1\n    \n    # Step 3: Process the target sequence, replacing OOV words with <UNK>\n    sequence_tokens = sequence.split()\n    processed_tokens = [word if word in training_vocab else '<UNK>' for word in sequence_tokens]\n    \n    # Check if the processed sequence is shorter than N\n    if len(processed_tokens) < N:\n        return 0.0\n    \n    total_log_prob = 0.0\n    # Step 4: Iterate over each N-gram in the processed sequence\n    for i in range(len(processed_tokens) - N + 1):\n        current_ngram = tuple(processed_tokens[i:i+N])\n        context = current_ngram[:-1]\n        \n        # Get the count of the current n-gram\n        count_ngram = ngram_counts[N].get(current_ngram, 0)\n        \n        # Get the count of the context (n-1 gram)\n        if N == 1:\n            # For unigrams, context is empty, use sum of all unigrams in corpus\n            count_context = sum(ngram_counts[1].values())\n        else:\n            count_context = ngram_counts[N-1].get(context, 0)\n        \n        # Compute the probability with additive smoothing\n        numerator = count_ngram + K\n        denominator = count_context + K * V_size\n        prob = numerator / denominator\n        \n        # Add the log probability to the total\n        total_log_prob += math.log(prob)\n    \n    # Round to 4 decimal places\n    return round(total_log_prob, 4)\n```\n\n### Explanation\n1. **Vocabulary Construction**: The code first extracts all unique words from the training corpus to build the vocabulary. This helps in identifying out-of-vocabulary (OOV) words in the target sentence, which are then replaced with the \"<UNK>\" token.\n\n2. **N-gram Frequency Tables**: The code constructs frequency tables for all n-grams (from 1 to N) using the training corpus. These tables are used to count occurrences of n-grams and their contexts.\n\n3. **Target Sentence Preprocessing**: The target sentence is processed to replace OOV words with \"<UNK>\", ensuring that all words are part of the known vocabulary.\n\n4. **Probability Calculation**: For each N-gram in the preprocessed target sentence, the code calculates the probability using additive smoothing. This involves adjusting counts with a smoothing constant \\( K \\) to handle unseen n-grams and contexts.\n\n5. **Log-Probability Summation**: The logarithm of each probability is computed and summed to get the final result, which is then rounded to four decimal places for the output. This approach ensures numerical stability and handles underflow issues common in probability multiplications."}
{"task_id": 108, "completion_id": 0, "completion": "To solve this problem, we need to factorize a real-valued matrix using the regularized Alternating Least Squares (ALS) algorithm. The goal is to find two low-rank matrices such that the regularized Frobenius norm of the reconstruction error is minimized. The algorithm alternates between updating these matrices while keeping one fixed at each step until the loss converges or a maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Initialize the factor matrices \\( W \\) and \\( H \\) with a fixed random seed to ensure reproducibility.\n2. **Alternating Updates**: Update \\( W \\) and \\( H \\) alternately using closed-form solutions derived from regularized least squares. Specifically, update \\( W \\) while keeping \\( H \\) fixed and vice versa.\n3. **Loss Calculation**: After each update, compute the combined loss (reconstruction error plus regularization terms). Track the best (lowest) loss encountered during the iterations.\n4. **Convergence Check**: Stop iterating if the loss drops below the specified tolerance or the maximum number of iterations is reached.\n5. **Return Result**: Return the best reconstruction matrix rounded to four decimal places and converted to a standard Python list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\"\"\"\n    np.random.seed(0)\n    N, M = X.shape\n    W = np.random.randn(N, K)\n    H = np.random.randn(K, M)\n    \n    best_loss = float('inf')\n    best_X_hat = None\n    \n    for _ in range(max_iter):\n        # Update W\n        H_HT = H @ H.T\n        reg_W = H_HT + alpha * np.eye(K)\n        inv_reg_W = np.linalg.inv(reg_W)\n        W = (X @ H.T) @ inv_reg_W\n        \n        # Update H\n        WT_W = W.T @ W\n        reg_H = WT_W + alpha * np.eye(K)\n        inv_reg_H = np.linalg.inv(reg_H)\n        H = inv_reg_H @ (W.T @ X)\n        \n        # Compute current reconstruction and loss\n        X_hat = W @ H\n        frobenius_loss = np.linalg.norm(X - X_hat, 'fro') ** 2\n        reg_term = alpha * (np.linalg.norm(W, 'fro')**2 + np.linalg.norm(H, 'fro')**2)\n        current_loss = frobenius_loss + reg_term\n        \n        # Update best X_hat if current loss is lower\n        if current_loss < best_loss:\n            best_loss = current_loss\n            best_X_hat = X_hat.copy()\n        \n        # Check for convergence\n        if best_loss <= tol:\n            break\n    \n    return best_X_hat.round(4).tolist()\n```\n\n### Explanation\n1. **Initialization**: The matrices \\( W \\) and \\( H \\) are initialized using a fixed random seed to ensure reproducibility. This ensures that the same initial matrices are generated every time the function is run.\n2. **Matrix Updates**: In each iteration, \\( W \\) is updated by solving a regularized least squares problem with \\( H \\) fixed, and then \\( H \\) is updated similarly with \\( W \\) fixed. The updates use matrix inverses to solve the closed-form solutions efficiently.\n3. **Loss Tracking**: After each iteration, the loss is calculated as the sum of the reconstruction error and regularization terms. The best reconstruction matrix is tracked to ensure that the solution returned is the one with the lowest loss encountered, even if subsequent iterations might increase the loss due to numerical instability.\n4. **Convergence Check**: The loop stops early if the loss falls below the specified tolerance, ensuring efficient computation. If the maximum number of iterations is reached without convergence, the best solution found so far is returned.\n5. **Result Formatting**: The final reconstructed matrix is rounded to four decimal places and converted to a Python list of lists for compatibility with standard Python data structures.\n\nThis approach efficiently balances computational complexity and accuracy, leveraging the structure of the ALS algorithm to iteratively improve the solution while ensuring regularization to prevent overfitting."}
{"task_id": 109, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm with K-Means++ initialization. The algorithm must be reproducible by setting both Python's `random` and NumPy's random generator with a provided `random_state` value. The solution involves initializing centroids using the K-Means++ method, performing Lloyd's iterations until convergence or maximum iterations are reached, and returning the sorted centroids rounded to four decimal places.\n\n### Approach\n\n1. **Set Random Seeds**: To ensure reproducibility, both Python's `random` and NumPy's random generator are seeded with the provided `random_state`.\n2. **K-Means++ Initialization**: \n   - The first centroid is chosen uniformly at random from the data points.\n   - Subsequent centroids are chosen based on the squared distances from the nearest existing centroid, ensuring a more optimal initial placement.\n3. **Lloyd's Iterations**:\n   - **Assignment Step**: Assign each data point to the nearest centroid.\n   - **Update Step**: Compute new centroids as the mean of the assigned points. If a cluster becomes empty, its centroid remains unchanged.\n   - **Stopping Condition**: The loop stops when cluster assignments no longer change or the maximum number of iterations is reached.\n4. **Post-Processing**: The final centroids are rounded to four decimal places and sorted by their first coordinate, using the entire centroid tuple as a secondary key for tie-breaking.\n\n### Solution Code\n\n```python\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\"\"\"\n    \n    # Set the seeds for reproducibility\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n    \n    m, n = X.shape\n    \n    # K-Means++ initialisation\n    centroids = []\n    indices = np.arange(m)\n    \n    # Select the first centroid randomly\n    first_idx = np.random.choice(indices)\n    centroids.append(X[first_idx].copy())\n    \n    for _ in range(1, K):\n        current_centroids = np.array(centroids)\n        # Compute squared distances from each point to the nearest centroid\n        dist_sq = np.sum((X[:, np.newaxis, :] - current_centroids) ** 2, axis=2)\n        min_dist_sq = np.min(dist_sq, axis=1)\n        sum_D = min_dist_sq.sum()\n        \n        if sum_D == 0:\n            probabilities = np.ones(m) / m\n        else:\n            probabilities = min_dist_sq / sum_D\n        \n        # Select the next centroid\n        next_idx = np.random.choice(indices, p=probabilities)\n        centroids.append(X[next_idx].copy())\n    \n    centroids = np.array(centroids)\n    \n    # Lloyd's algorithm iterations\n    prev_labels = None\n    for _ in range(max_iter):\n        # Assign each point to the nearest centroid\n        dist_sq = ((X[:, np.newaxis, :] - centroids) ** 2).sum(axis=2)\n        labels = np.argmin(dist_sq, axis=1)\n        \n        # Check for convergence\n        if prev_labels is not None and np.array_equal(labels, prev_labels):\n            break\n        \n        # Update centroids\n        new_centroids = []\n        for i in range(K):\n            mask = (labels == i)\n            if np.sum(mask) == 0:\n                # Keep the centroid unchanged if cluster is empty\n                new_centroid = centroids[i].copy()\n            else:\n                new_centroid = X[mask].mean(axis=0)\n            new_centroids.append(new_centroid)\n        \n        centroids = np.array(new_centroids)\n        prev_labels = labels.copy()\n    \n    # Round and sort the centroids\n    centroids_rounded = np.round(centroids, decimals=4)\n    centroids_list = centroids_rounded.tolist()\n    centroids_list.sort(key=lambda x: (x[0], x))\n    \n    return centroids_list\n```\n\n### Explanation\n\n1. **Random Seed Initialization**: Ensures that both Python's `random` and NumPy's random number generator produce reproducible results.\n2. **K-Means++ Initialization**: This method selects initial centroids in a way that spreads them out, leading to faster convergence and better results compared to random initialization.\n3. **Lloyd's Algorithm**: The core of the K-Means algorithm, iteratively improving centroid positions by alternating between assigning points to the nearest centroid and updating centroids based on assigned points.\n4. **Handling Empty Clusters**: If a cluster becomes empty during iterations, its centroid is kept unchanged to maintain stability.\n5. **Sorting and Rounding**: The final centroids are rounded to four decimal places and sorted to ensure a consistent output format.\n\nThis approach efficiently balances computational performance with the need for accurate and reproducible clustering results."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Encodes a string into a fixed-length list of integer token ids.\"\"\"\n    # Truncate the text to the first 'time_steps' characters if necessary\n    truncated_text = text[:time_steps]\n    # Convert each character to its corresponding token id, using 0 for unknown characters\n    encoded = [vocabulary.get(c, 0) for c in truncated_text]\n    # Pad the encoded list with zeros to reach the desired length\n    pad_length = time_steps - len(encoded)\n    encoded += [0] * pad_length\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer token ids back to a string, ignoring padding tokens (0).\"\"\"\n    # Filter out padding tokens (0) and map the rest to characters\n    return ''.join([inverse_vocab[idx] for idx in indices if idx != 0])\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encodes the input text, runs model prediction, and decodes the output.\"\"\"\n    # Encode the input text into a fixed-length integer sequence\n    encoded_sequence = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # Convert to a batch of size 1 for the model\n    input_batch = np.array([encoded_sequence])\n    # Obtain the model's prediction (assuming output is a probability distribution over classes)\n    prediction = model.predict(input_batch)\n    # Convert predictions to token ids by selecting the most probable class at each time step\n    predicted_ids = np.argmax(prediction, axis=-1).squeeze()\n    # Decode the predicted token ids into a string, omitting padding tokens\n    return int_to_string(predicted_ids, inv_output_vocabulary)\n```"}
{"task_id": 113, "completion_id": 0, "completion": "To solve this problem, we need to create a helper function that automates running a sequence-to-sequence model on multiple input examples and collects the predictions. The function should handle cases where the examples are not provided by using a predefined global constant. The predictions should be printed in a readable format and returned in a list.\n\n### Approach\n1. **Handle Optional Examples**: The function should check if the `examples` argument is provided. If not, it should fall back to using a global constant `EXAMPLES`.\n2. **Process Each Example**: For each input example, call another helper function `run_example` to get the model's prediction as a list of characters.\n3. **Format and Print Output**: Convert the list of characters into a string, print both the input and the predicted output in a user-friendly format.\n4. **Collect Predictions**: Collect all predicted strings in a list and return them in the same order as the input examples.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str]\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        output_str = ''.join(output_chars)\n        print(f'input:  {example}')\n        print(f'output: {output_str}')\n        predictions.append(output_str)\n    return predictions\n```\n\n### Explanation\n- **Handling Optional Examples**: The function checks if the `examples` parameter is `None`. If it is, the global `EXAMPLES` constant is used. This allows the function to work even when the user does not explicitly provide examples.\n- **Processing Each Example**: For each input string in the examples, the helper function `run_example` is called. This helper function is assumed to return the model's prediction as a list of characters.\n- **Formatting and Printing**: The list of characters from `run_example` is joined into a single string. The input and predicted output are printed in a structured format for easy readability.\n- **Collecting Predictions**: Each predicted string is collected into a list, which is returned at the end. This ensures that the predictions are in the same order as the input examples, maintaining consistency.\n\nThis approach efficiently automates the process of running a model on multiple inputs, providing clear output, and collecting results, making it a valuable tool for quick model evaluation."}
{"task_id": 115, "completion_id": 0, "completion": "To solve this problem, we need to compute the average binary cross-entropy loss and its gradient for logistic regression given a feature matrix, a binary target vector, and a weight vector. The solution involves calculating the predicted probabilities using the logistic function, clipping these probabilities to avoid numerical instability, and then using these values to compute the loss and gradient as specified.\n\n### Approach\n1. **Compute Logits and Probabilities**: First, compute the logits (linear combination of features and weights) and apply the sigmoid function to get the predicted probabilities.\n2. **Clip Probabilities**: To avoid numerical issues when taking the logarithm of probabilities that are exactly 0 or 1, clip these probabilities to a small interval around [0, 1].\n3. **Compute Loss**: Use the clipped probabilities to compute the binary cross-entropy loss.\n4. **Compute Gradient**: Use the original (unclipped) probabilities to compute the gradient of the loss with respect to the weights, following the given formula.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\"\"\"\n    m = X.shape[0]\n    y = y.ravel()\n    w = w.ravel()\n    \n    z = X.dot(w)\n    p = 1 / (1 + np.exp(-z))\n    \n    # Clip probabilities to avoid numerical instability in log\n    epsilon = 1e-20\n    p_clipped = np.clip(p, epsilon, 1 - epsilon)\n    \n    # Compute loss using clipped probabilities\n    loss_terms = y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped)\n    loss = -np.sum(loss_terms) / m\n    loss_rounded = np.round(loss, 4).item()\n    \n    # Compute gradient using original probabilities\n    diff = p - y\n    gradient = X.T.dot(diff) / m\n    gradient_rounded = np.round(gradient, 4)\n    \n    # Convert gradient to a nested list of lists\n    gradient_list = gradient_rounded.reshape(-1, 1).tolist()\n    \n    return (loss_rounded, gradient_list)\n```\n\n### Explanation\n1. **Logits and Probabilities**: The logits (z) are computed as the dot product of the feature matrix (X) and the weight vector (w). The probabilities (p) are obtained by applying the sigmoid function to these logits.\n2. **Clipping Probabilities**: To prevent numerical instability, we clip the probabilities to a small interval [\u03b5, 1-\u03b5], where \u03b5 is 1e-20. This ensures that the logarithm operations in the loss calculation do not encounter invalid values.\n3. **Loss Calculation**: The loss is computed using the clipped probabilities. This involves calculating the cross-entropy loss for each sample and averaging it over all samples.\n4. **Gradient Calculation**: The gradient is derived using the original (unclipped) probabilities. The formula for the gradient involves the transpose of the feature matrix multiplied by the difference between the predicted probabilities and the actual target values, scaled by the number of samples.\n\nThis approach ensures numerical stability while maintaining the correct computation of both the loss and gradient as per the problem specifications. The results are then rounded to four decimal places and formatted into the required output format."}
{"task_id": 118, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. AdaBoost is an ensemble method that combines multiple weak classifiers to create a strong classifier. Each weak classifier (decision stump in this case) is trained iteratively, focusing more on the misclassified samples from the previous iteration. The final prediction is a weighted sum of all weak classifiers' predictions.\n\n### Approach\n1. **Initialize Weights**: Start with equal weights for all training samples.\n2. **Iterative Training**: For each iteration, find the best decision stump that minimizes the weighted classification error.\n3. **Compute Stump Weight**: Use the weighted error to compute the weight (alpha) for the current stump.\n4. **Update Weights**: Adjust the weights of the training samples, increasing the weights of misclassified samples and decreasing those of correctly classified samples.\n5. **Aggregate Predictions**: Combine the predictions of all stumps using their respective weights to make the final prediction on the test data.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\"\"\"\n    m, n = X_train.shape\n    n_clf = max(n_clf, 1)\n    D = np.ones(m) / m  # Initial weights\n    stumps = []\n\n    for _ in range(n_clf):\n        best_error = np.inf\n        best_j = -1\n        best_threshold = 0.0\n        best_sign = 1\n\n        # Iterate over each feature to find the best split\n        for j in range(n):\n            sorted_indices = np.argsort(X_train[:, j])\n            X_sorted_j = X_train[sorted_indices, j]\n            y_sorted = y_train[sorted_indices]\n            weights_sorted = D[sorted_indices]\n\n            # Precompute cumulative sums for positive and negative classes\n            pos_mask = (y_sorted == 1)\n            neg_mask = (y_sorted == -1)\n            cum_pos = np.cumsum(pos_mask * weights_sorted)\n            cum_neg = np.cumsum(neg_mask * weights_sorted)\n\n            total_pos = cum_pos[-1] if len(cum_pos) > 0 else 0\n            total_neg = cum_neg[-1] if len(cum_neg) > 0 else 0\n\n            # Check all possible split points\n            for k in range(len(X_sorted_j) - 1):\n                if X_sorted_j[k] == X_sorted_j[k + 1]:\n                    continue  # Skip if values are the same\n\n                sum_pos_left = cum_pos[k]\n                sum_neg_left = cum_neg[k]\n                sum_pos_right = total_pos - sum_pos_left\n                sum_neg_right = total_neg - sum_neg_left\n\n                error1 = sum_neg_left + sum_pos_right\n                error2 = sum_pos_left + sum_neg_right\n\n                current_error = min(error1, error2)\n                if current_error < best_error:\n                    best_error = current_error\n                    best_j = j\n                    best_threshold = (X_sorted_j[k] + X_sorted_j[k + 1]) / 2\n                    best_sign = 1 if error1 < error2 else -1\n\n        # Compute alpha for the best stump\n        eps = 1e-15\n        error_t = max(eps, min(best_error, 1 - eps))\n        alpha = 0.5 * np.log((1 - error_t) / error_t)\n        stumps.append((alpha, best_j, best_threshold, best_sign))\n\n        # Update weights\n        h_train = np.where(X_train[:, best_j] <= best_threshold, best_sign, -best_sign)\n        D *= np.exp(-alpha * y_train * h_train)\n        D /= D.sum()\n\n    # Predict on X_test\n    predictions = []\n    for x in X_test:\n        sum_alpha_h = 0.0\n        for alpha, j, threshold, sign in stumps:\n            if x[j] <= threshold:\n                h = sign\n            else:\n                h = -sign\n            sum_alpha_h += alpha * h\n        predictions.append(1 if sum_alpha_h >= 0 else -1)\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: Each sample is initially given equal weight.\n2. **Finding the Best Stump**: For each feature, we sort the samples and evaluate potential thresholds. The best threshold and direction (sign) are chosen based on the weighted error.\n3. **Weight Update**: After selecting the best stump, the weights of misclassified samples are increased, and those of correctly classified samples are decreased, ensuring the next stump focuses more on the errors.\n4. **Prediction**: The final prediction is a weighted vote of all stumps, where each stump's influence is determined by its alpha value. The sign of the weighted sum determines the class label.\n\nThis approach efficiently combines weak learners to form a strong classifier, leveraging iterative re-weighting to focus on difficult samples, thereby improving overall model performance."}
{"task_id": 128, "completion_id": 0, "completion": "To solve this problem, we need to implement a multi-class Linear Discriminant Analysis (LDA) to project data onto a lower-dimensional space that maximizes the separation between different classes. This involves computing scatter matrices, performing eigen decomposition, and projecting the data onto the most significant eigenvectors.\n\n### Approach\n1. **Compute Within-Class Scatter Matrix (S_W)**: This matrix measures the spread of data within each class. It is computed by summing the covariance matrices of each class.\n2. **Compute Between-Class Scatter Matrix (S_B)**: This matrix measures the spread between different classes. It is computed by summing the outer products of the difference between each class mean and the global mean, weighted by the number of samples in each class.\n3. **Form Matrix A**: Compute the matrix A as the product of the pseudo-inverse of S_W and S_B to handle potential singularity issues in S_W.\n4. **Eigen Decomposition**: Perform eigen decomposition on matrix A to find the eigenvectors and eigenvalues. Sort the eigenvectors by descending eigenvalues.\n5. **Sign Determinism**: Ensure the sign of each eigenvector is deterministic by flipping it if the first non-zero element is negative.\n6. **Project Data**: Project the data onto the top `n_components` eigenvectors and round the results to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\"\"\"\n    if n_components == 0:\n        return []\n    \n    m, d = X.shape\n    classes = np.unique(y)\n    global_mean = np.mean(X, axis=0)\n    \n    # Compute within-class scatter matrix S_W\n    S_W = np.zeros((d, d))\n    for c in classes:\n        X_c = X[y == c]\n        class_mean = np.mean(X_c, axis=0)\n        X_c_centered = X_c - class_mean\n        S_W += X_c_centered.T @ X_c_centered\n    \n    # Compute between-class scatter matrix S_B\n    S_B = np.zeros((d, d))\n    for c in classes:\n        X_c = X[y == c]\n        class_mean = np.mean(X_c, axis=0)\n        n_c = X_c.shape[0]\n        diff = class_mean - global_mean\n        S_B += n_c * np.outer(diff, diff)\n    \n    # Compute matrix A = pinv(S_W) @ S_B\n    A = np.linalg.pinv(S_W) @ S_B\n    \n    # Eigen decomposition and sorting\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Deterministic sign adjustment\n    for i in range(sorted_eigenvectors.shape[1]):\n        eigenvector = sorted_eigenvectors[:, i]\n        for j in range(eigenvector.shape[0]):\n            if not np.isclose(eigenvector[j], 0, atol=1e-8):\n                if eigenvector[j] < 0:\n                    sorted_eigenvectors[:, i] = -eigenvector\n                break\n    \n    # Select the top n_components eigenvectors\n    W = sorted_eigenvectors[:, :n_components]\n    \n    # Project data and round\n    X_proj = X @ W\n    X_proj_rounded = np.round(X_proj, 4)\n    \n    return X_proj_rounded.tolist()\n```\n\n### Explanation\n1. **Within-Class Scatter Matrix (S_W)**: This matrix aggregates the covariance of each class, capturing the spread within each class. It is computed by centering the data around the class mean and summing the outer products.\n2. **Between-Class Scatter Matrix (S_B)**: This matrix captures the separation between different classes by measuring how each class mean deviates from the global mean, weighted by the number of samples in each class.\n3. **Matrix A**: The product of the pseudo-inverse of S_W and S_B ensures numerical stability even if S_W is singular.\n4. **Eigen Decomposition**: Eigenvalues and eigenvectors of matrix A are computed, sorted in descending order to prioritize directions of maximum variance.\n5. **Sign Adjustment**: Eigenvectors are adjusted to ensure a deterministic sign, enhancing consistency across different runs and platforms.\n6. **Projection**: The data is projected onto the selected eigenvectors, and the results are rounded to four decimal places for consistency.\n\nThis approach efficiently reduces the dimensionality while preserving class separability, making it suitable for multi-class classification tasks."}
{"task_id": 140, "completion_id": 0, "completion": "To solve this problem, we need to determine the minimum time for an agent to escape a maze while avoiding spreading fire. The fire spreads first each minute, followed by the agent's movement. The agent can only move into a cell if it is not on fire at the time of arrival.\n\n### Approach\n1. **Precompute Fire Spread Times**: Using a multi-source BFS, compute the earliest time each cell catches fire. This helps us determine which cells are safe for the agent at any given time.\n2. **Initial Checks**: Immediately return -1 if the start or end cell is on fire at time 0. If the grid is 1x1 and safe, return 0.\n3. **Agent BFS**: Use BFS to explore the agent's possible movements, ensuring each move is to a cell that is not a wall and not on fire at the time of arrival. Track the earliest time each cell is visited to avoid redundant checks.\n\n### Solution Code\n```python\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    n = len(grid)\n    if n == 0:\n        return -1\n    \n    # Precompute the earliest time each cell catches fire\n    fire_time = [[math.inf] * n for _ in range(n)]\n    q = deque()\n    \n    # Initialize fire sources\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_time[i][j] = 0\n                q.append((i, j))\n    \n    # Directions for BFS (up, down, left, right)\n    dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    # BFS to compute fire spread times\n    while q:\n        i, j = q.popleft()\n        for di, dj in dirs:\n            ni, nj = i + di, j + dj\n            if 0 <= ni < n and 0 <= nj < n:\n                if grid[ni][nj] != '#' and fire_time[ni][nj] > fire_time[i][j] + 1:\n                    fire_time[ni][nj] = fire_time[i][j] + 1\n                    q.append((ni, nj))\n    \n    # Check if start or end is on fire at t=0\n    if fire_time[0][0] == 0 or fire_time[n-1][n-1] == 0:\n        return -1\n    \n    # Handle 1x1 grid case\n    if n == 1:\n        return 0\n    \n    # BFS for agent's movement\n    visited = [[math.inf] * n for _ in range(n)]\n    q_agent = deque()\n    q_agent.append((0, 0, 0))\n    visited[0][0] = 0\n    \n    while q_agent:\n        i, j, t = q_agent.popleft()\n        for di, dj in dirs:\n            ni, nj = i + di, j + dj\n            if 0 <= ni < n and 0 <= nj < n:\n                if grid[ni][nj] == '#':\n                    continue  # Skip walls\n                new_time = t + 1\n                # Check if the cell is safe to move into at new_time\n                if fire_time[ni][nj] > new_time:\n                    if ni == n-1 and nj == n-1:\n                        return new_time\n                    if visited[ni][nj] > new_time:\n                        visited[ni][nj] = new_time\n                        q_agent.append((ni, nj, new_time))\n    \n    return -1\n```\n\n### Explanation\n1. **Fire Spread Calculation**: Using BFS from all initial fire cells, we determine the earliest time each cell catches fire. This helps in checking if a cell is safe for the agent at any given time.\n2. **Initial Checks**: Ensure the start and end cells are not on fire at time 0. Handle the trivial case of a 1x1 grid.\n3. **Agent Movement**: Using BFS, explore all possible paths for the agent, moving to cells only if they are safe (not on fire and not a wall) at the next time step. Track visited cells to avoid redundant checks and ensure the shortest path is found.\n\nThis approach efficiently combines BFS for both fire spread and agent movement, ensuring optimal pathfinding while considering dynamic obstacles (spreading fire)."}
{"task_id": 141, "completion_id": 0, "completion": "To solve this problem, we need to implement a k-Nearest Neighbors (k-NN) classifier that predicts the labels for an unlabeled test set based on a labeled training set. The classifier uses one of three distance metrics (Euclidean, Manhattan, or Cosine) to determine the nearest neighbors and then applies a majority vote to predict the class, with a tie-breaker that selects the smallest label value in case of a tie.\n\n### Approach\n1. **Compute Distance Matrix**: Calculate the distance between each test sample and every training sample using the specified metric (Euclidean, Manhattan, or Cosine). If an unknown metric is provided, default to Euclidean.\n2. **Find Nearest Neighbors**: For each test sample, identify the k training samples with the smallest distances.\n3. **Majority Vote**: Predict the class label by majority vote among the k nearest neighbors. In case of a tie, return the smallest label value.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"Implement k-Nearest Neighbors classifier.\"\"\"\n    \n    # Handle unknown metric by falling back to Euclidean\n    if metric not in ['euclidean', 'manhattan', 'cosine']:\n        metric = 'euclidean'\n    \n    # Compute pairwise distances between test and training samples\n    if metric == 'euclidean':\n        distances = np.sqrt(((X_test[:, np.newaxis] - X_train) ** 2).sum(axis=2))\n    elif metric == 'manhattan':\n        distances = np.abs(X_test[:, np.newaxis] - X_train).sum(axis=2)\n    elif metric == 'cosine':\n        dot_product = np.dot(X_test, X_train.T)\n        norm_test = np.linalg.norm(X_test, axis=1, keepdims=True)\n        norm_train = np.linalg.norm(X_train, axis=1)\n        cosine_sim = dot_product / (norm_test * norm_train)\n        distances = 1 - cosine_sim\n    \n    # Find indices of k nearest neighbors for each test sample\n    nearest_indices = np.argpartition(distances, k, axis=1)[:, :k]\n    \n    # Collect the labels of the nearest neighbors\n    y_neighbors = y_train[nearest_indices]\n    \n    # Determine the majority vote with tie-breaker (smallest label in case of tie)\n    y_pred = []\n    for neighbors in y_neighbors:\n        labels, counts = np.unique(neighbors, return_counts=True)\n        max_count = np.max(counts)\n        candidates = labels[counts == max_count]\n        y_pred.append(np.min(candidates))\n    \n    return np.array(y_pred)\n```\n\n### Explanation\n1. **Distance Calculation**:\n   - **Euclidean Distance**: Computed as the square root of the sum of squared differences between each feature.\n   - **Manhattan Distance**: Sum of absolute differences between each feature.\n   - **Cosine Distance**: Derived from 1 minus the cosine similarity, which is the dot product of vectors divided by the product of their norms.\n\n2. **Nearest Neighbor Identification**:\n   - Using `np.argpartition`, we efficiently find the indices of the k smallest distances for each test sample without fully sorting the array, which optimizes performance.\n\n3. **Majority Vote with Tie-Breaker**:\n   - For each test sample, we collect the labels of the k nearest neighbors and use `np.unique` to count occurrences. The label with the highest count is selected, and in case of a tie, the smallest label is chosen.\n\nThis approach ensures that the k-NN classifier efficiently computes distances, identifies neighbors, and accurately predicts labels while handling ties appropriately."}
{"task_id": 146, "completion_id": 0, "completion": "To solve this problem, we need to implement the k-Nearest Neighbors (k-NN) classifier from scratch. The k-NN algorithm is a simple, instance-based learning algorithm used for classification tasks. The core idea is to predict the label of a test sample based on the majority vote of its k nearest neighbors in the training set, determined by a specified distance metric.\n\n### Approach\n1. **Distance Metrics**: Compute the distance between each test sample and all training samples using one of three metrics: Euclidean, Manhattan, or Cosine distance.\n2. **Neighbor Selection**: For each test sample, identify the k nearest training samples based on the computed distances.\n3. **Majority Vote**: Determine the predicted label by majority vote among the k nearest neighbors. In case of a tie, select the smallest label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\"\"\"\n    predictions = []\n    n_train = X.shape[0]\n    k_adj = min(k, n_train)\n    \n    for x in X_test:\n        if metric == 'euclidean':\n            distances = np.sqrt(np.sum((X - x) ** 2, axis=1))\n        elif metric == 'manhattan':\n            distances = np.sum(np.abs(X - x), axis=1)\n        elif metric == 'cosine':\n            dot_product = np.dot(X, x)\n            norm_x = np.linalg.norm(x)\n            norm_X = np.linalg.norm(X, axis=1)\n            denominator = norm_X * norm_x + 1e-12\n            cosine_sim = dot_product / denominator\n            distances = 1 - cosine_sim\n        else:\n            raise ValueError(f\"Invalid metric '{metric}'. Choose 'euclidean', 'manhattan', or 'cosine'.\")\n        \n        sorted_indices = np.argsort(distances)\n        k_nearest_indices = sorted_indices[:k_adj]\n        k_nearest_labels = y[k_nearest_indices]\n        \n        unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)\n        max_count = np.max(counts)\n        candidates = unique_labels[counts == max_count]\n        predicted_label = np.min(candidates)\n        predictions.append(predicted_label)\n    \n    return np.array(predictions).tolist()\n```\n\n### Explanation\n1. **Distance Calculation**:\n   - **Euclidean Distance**: Computed as the square root of the sum of squared differences between corresponding features.\n   - **Manhattan Distance**: Computed as the sum of absolute differences between corresponding features.\n   - **Cosine Distance**: Computed as 1 minus the cosine similarity, which measures the cosine of the angle between two vectors. An epsilon (1e-12) is added to avoid division by zero.\n\n2. **Neighbor Selection**:\n   - For each test sample, compute distances to all training samples using the chosen metric.\n   - Sort the distances and select the indices of the k smallest distances (or all training samples if k exceeds the number of training samples).\n\n3. **Majority Vote**:\n   - Collect the labels of the k nearest neighbors.\n   - Determine the most frequent label. In case of a tie, select the smallest label using `np.min`.\n\nThis approach ensures efficient computation of distances using vectorized operations and handles edge cases such as ties in the majority vote by selecting the smallest label. The solution is implemented using basic NumPy operations to ensure efficiency and clarity."}
{"task_id": 155, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward pass of an actor network commonly used in reinforcement learning. This network processes an environment state through a series of linear transformations and activation functions to produce an action vector within the range [-1, 1]. The network architecture consists of three linear layers with ReLU activation functions after the first two layers and a tanh activation function after the final layer.\n\n### Approach\n1. **Convert Input State**: Convert the input state into a NumPy array to facilitate matrix operations.\n2. **First Linear Layer**: Apply the first linear transformation using weights W1 and biases b1, followed by a ReLU activation.\n3. **Second Linear Layer**: Apply the second linear transformation using weights W2 and biases b2, followed by another ReLU activation.\n4. **Third Linear Layer**: Apply the third linear transformation using weights W3 and biases b3.\n5. **Tanh Activation**: Apply the tanh activation function to the output of the third linear layer to ensure the action values are within the range [-1, 1].\n6. **Rounding and Conversion**: Round the resulting action values to four decimal places and convert the NumPy array to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    x = np.asarray(state)\n    # First layer: Linear + ReLU\n    x = np.dot(x, weights['W1']) + weights['b1']\n    x = np.maximum(x, 0)\n    # Second layer: Linear + ReLU\n    x = np.dot(x, weights['W2']) + weights['b2']\n    x = np.maximum(x, 0)\n    # Third layer: Linear + tanh\n    x = np.dot(x, weights['W3']) + weights['b3']\n    x = np.tanh(x)\n    # Round to 4 decimal places and convert to list\n    return np.round(x, 4).tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input state is converted to a NumPy array to handle both list and array inputs uniformly.\n2. **Linear Transformations**: Each linear layer applies a matrix multiplication (dot product) followed by the addition of a bias vector. This is done using the respective weights (W1, W2, W3) and biases (b1, b2, b3) from the provided dictionary.\n3. **ReLU Activation**: The ReLU function (max(0, x)) is applied element-wise after the first and second linear layers to introduce non-linearity.\n4. **Tanh Activation**: The tanh function is applied to the output of the third linear layer to squash the values into the range [-1, 1], which is suitable for action outputs in many reinforcement learning tasks.\n5. **Rounding and Conversion**: The final action values are rounded to four decimal places for precision and converted to a Python list to meet the output requirements.\n\nThis approach efficiently processes the input state through the defined network architecture using NumPy operations, ensuring both correctness and performance."}
{"task_id": 160, "completion_id": 0, "completion": "To solve this problem, we need to implement Lasso regression using coordinate descent. Lasso regression is a linear regression technique that includes L1 regularization to prevent overfitting and encourage sparsity in the model coefficients. The coordinate descent algorithm iteratively updates each coefficient while keeping others fixed, which is efficient and converges well for Lasso.\n\n### Approach\n1. **Add Intercept Column**: Depending on whether an intercept is needed, we add a column of ones (if `fit_intercept` is True) or zeros (if False) to the design matrix. This intercept term is not regularized.\n2. **Initialize Weights**: All weights, including the intercept, are initialized to zero.\n3. **Coordinate Descent Loop**: For each iteration up to `max_iters`:\n   - **Update Intercept**: If an intercept is being fitted, it is recomputed as the mean of the residuals from the current model.\n   - **Iterate Over Features**: For each feature (excluding the intercept), temporarily set its weight to zero, compute the partial residual, update the weight using soft-thresholding to apply L1 regularization, and then update the weight in the model.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\"\"\"\n    m, n_original = X.shape\n    \n    # Add intercept column (all ones or zeros)\n    if fit_intercept:\n        intercept_col = np.ones((m, 1))\n    else:\n        intercept_col = np.zeros((m, 1))\n    X_new = np.hstack((intercept_col, X))\n    \n    n_new = X_new.shape[1]\n    w = np.zeros(n_new)\n    m = X_new.shape[0]  # Number of samples\n    \n    for _ in range(max_iters):\n        if fit_intercept:\n            # Compute residuals without intercept contribution and update intercept\n            residuals = y - X_new[:, 1:] @ w[1:]\n            w[0] = np.mean(residuals)\n        \n        # Coordinate descent over each feature weight (excluding intercept)\n        for j in range(1, n_new):\n            old_wj = w[j]\n            w[j] = 0.0  # Temporarily set current weight to zero\n            r_j = y - X_new @ w  # Residual with current weight zeroed\n            xj = X_new[:, j]\n            rho_j = xj.dot(r_j)\n            sum_xj_sq = xj.dot(xj)\n            \n            if sum_xj_sq == 0.0:\n                w_j_new = 0.0\n            else:\n                tau = lambda_param * m\n                soft_val = np.sign(rho_j) * np.maximum(np.abs(rho_j) - tau, 0.0)\n                w_j_new = soft_val / sum_xj_sq\n            \n            w[j] = w_j_new  # Update weight\n    \n    # Extract intercept and weights, rounding to four decimals\n    bias = round(float(w[0]), 4)\n    weights = [round(float(weight), 4) for weight in w[1:]]\n    \n    return (weights, bias)\n```\n\n### Explanation\n1. **Intercept Handling**: The intercept is handled by adding a column of ones (or zeros) to the design matrix. This allows the intercept to be treated as part of the weight vector but excluded from regularization.\n2. **Weight Initialization**: All weights are initialized to zero to start the optimization process from a simple baseline.\n3. **Coordinate Descent**: For each iteration, the intercept is updated first (if needed) by computing the mean residual. Each feature weight is then updated by temporarily setting it to zero, computing the residual without that feature's contribution, and applying the soft-thresholding operator to incorporate L1 regularization. This process ensures that each weight update considers the current model's residuals, leading to a sparse solution.\n\nThis approach efficiently optimizes the Lasso objective function using coordinate descent, ensuring both accuracy and sparsity in the resulting model."}
{"task_id": 165, "completion_id": 0, "completion": "To solve this problem, we need to compute the normalized Hamming distance between two 1-D integer vectors. The Hamming distance is the proportion of indices at which the corresponding elements of the vectors differ. If the vectors have different lengths, the function should return -1. Otherwise, the result is rounded to 4 decimal places.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the two vectors are the same. If not, return -1 immediately.\n2. **Handle Empty Vectors**: If both vectors are empty (length 0), return 0.0 since there are no elements to compare.\n3. **Convert to NumPy Arrays**: Convert both input vectors to NumPy arrays to facilitate efficient element-wise comparison.\n4. **Compute Differences**: Use NumPy to compute the element-wise differences between the two arrays and sum these differences.\n5. **Normalize and Round**: Normalize the sum of differences by the length of the vectors and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | np.ndarray, y: list[int] | np.ndarray) -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    \n    N = len(x)\n    if N == 0:\n        return 0.0\n    \n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    \n    sum_diff = np.sum(x_arr != y_arr)\n    normalized = sum_diff / N\n    \n    return round(normalized, 4)\n```\n\n### Explanation\n- **Length Check**: The function first checks if the input vectors have the same length. If not, it returns -1 immediately.\n- **Empty Vectors**: If both vectors are empty (length 0), the function returns 0.0 as there are no elements to compare.\n- **Conversion to NumPy Arrays**: Converting both vectors to NumPy arrays allows for efficient element-wise comparison using NumPy's vectorized operations.\n- **Element-wise Comparison**: Using NumPy's element-wise comparison (`x_arr != y_arr`), we generate a boolean array where `True` indicates differing elements. Summing this array gives the count of differing elements.\n- **Normalization and Rounding**: The count of differing elements is divided by the length of the vectors to get the normalized Hamming distance, which is then rounded to 4 decimal places for the final result.\n\nThis approach efficiently handles both Python lists and NumPy arrays, ensuring correctness and performance for various input sizes."}
{"task_id": 169, "completion_id": 0, "completion": "To solve this problem, we need to generate a list of window coefficients for various symmetric window functions commonly used in digital signal processing. The goal is to implement a function that can generate these coefficients for specified window types, ensuring correct handling of edge cases and proper rounding of results.\n\n### Approach\n1. **Validation of Inputs**: \n   - Ensure the number of coefficients `N` is a positive integer.\n   - Handle the special case where `N = 1`, which always returns a single coefficient `[1.0]`.\n   - Validate the window type to ensure it is one of the supported types: \"hamming\", \"hann\", \"blackman_harris\", or \"generalized_cosine\".\n\n2. **Generate Coefficients**:\n   - For each window type, compute the coefficients using their respective mathematical formulas.\n   - Use vectorized operations with NumPy for efficient computation.\n   - For the \"generalized_cosine\" window, ensure that coefficients are provided and compute the sum of cosine terms as specified.\n\n3. **Rounding and Conversion**:\n   - Round each coefficient to 4 decimal places using NumPy's rounding function.\n   - Convert the result from a NumPy array to a Python list before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\"\"\"\n\n    # Check if N is a positive integer\n    if not isinstance(N, int) or N < 1:\n        raise ValueError(\"N must be a positive integer\")\n    \n    # Handle the special case where N is 1\n    if N == 1:\n        return [1.0]\n    \n    # Validate the window type\n    valid_windows = [\"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\"]\n    if window not in valid_windows:\n        raise ValueError(\"Unknown window type\")\n    \n    n = np.arange(N)\n    if window == \"hamming\":\n        arg = 2 * np.pi * n / (N - 1)\n        w = 0.54 - 0.46 * np.cos(arg)\n    elif window == \"hann\":\n        arg = 2 * np.pi * n / (N - 1)\n        w = 0.5 - 0.5 * np.cos(arg)\n    elif window == \"blackman_harris\":\n        a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n        arg = 2 * np.pi * n / (N - 1)\n        term1 = a1 * np.cos(arg)\n        term2 = a2 * np.cos(2 * arg)\n        term3 = a3 * np.cos(3 * arg)\n        w = a0 - term1 + term2 - term3\n    elif window == \"generalized_cosine\":\n        if coefficients is None:\n            raise ValueError(\"coefficients must be provided for generalized_cosine window\")\n        arg = 2 * np.pi * n / (N - 1)\n        w = np.zeros(N)\n        for k, a in enumerate(coefficients):\n            w += a * np.cos(k * arg)\n    \n    # Round to 4 decimal places and convert to Python list\n    rounded = np.round(w, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if `N` is a valid positive integer and handles the special case where `N = 1` directly. It then ensures the provided window type is valid.\n\n2. **Coefficient Calculation**: Depending on the window type, the function calculates the coefficients using the respective formula. For the \"generalized_cosine\" window, it checks if coefficients are provided and computes the sum of cosine terms as specified by the coefficients list.\n\n3. **Rounding and Conversion**: After computing the coefficients using efficient vectorized operations, the results are rounded to 4 decimal places and converted to a Python list to meet the output requirements.\n\nThis approach ensures that the function is both efficient and correct, handling all edge cases and providing accurate results as specified."}
{"task_id": 171, "completion_id": 0, "completion": "To solve this problem, we need to implement a binary Logistic Regression classifier using mini-batch Gradient Descent from scratch. The classifier should handle any two distinct numeric labels, add an intercept term to the data, and return predictions for the test set using a decision threshold of 0.5. If all training labels are the same, the classifier should predict that label for all test samples.\n\n### Approach\n1. **Handle Special Case**: Check if all training labels are the same. If so, return that label for all test samples.\n2. **Label Mapping**: Convert the original labels to {0, 1} for training and map predictions back to the original labels.\n3. **Data Preprocessing**: Add an intercept term (bias) to both training and test datasets.\n4. **Model Training**: Initialize weights and use mini-batch Gradient Descent to minimize the cross-entropy loss. Update weights iteratively using gradients computed from each mini-batch.\n5. **Prediction**: Compute probabilities for the test set using the trained weights, apply the decision threshold, and map predictions back to the original labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n    \"\"\"\n    # Handle the case where all training labels are identical\n    unique_labels = list(set(y_train))\n    if len(unique_labels) == 1:\n        return [unique_labels[0]] * len(X_test)\n    \n    # Prepare label mapping to {0, 1}\n    unique_labels = sorted(unique_labels)\n    label0, label1 = unique_labels\n    label_map = {label0: 0, label1: 1}\n    inverse_map = {0: label0, 1: label1}\n    y_internal = np.array([label_map[y] for y in y_train])\n    \n    # Add intercept term to training and test data\n    X_train_np = np.hstack([np.ones((len(X_train), 1)), np.array(X_train)])\n    X_test_np = np.hstack([np.ones((len(X_test), 1)), np.array(X_test)])\n    \n    # Initialize weights\n    weights = np.zeros(X_train_np.shape[1])\n    m = X_train_np.shape[0]\n    \n    # Mini-batch Gradient Descent\n    for epoch in range(epochs):\n        # Shuffle the training data\n        indices = np.random.permutation(m)\n        X_shuffled = X_train_np[indices]\n        y_shuffled = y_internal[indices]\n        \n        # Iterate over mini-batches\n        for i in range(0, m, batch_size):\n            X_batch = X_shuffled[i:i + batch_size]\n            y_batch = y_shuffled[i:i + batch_size]\n            \n            # Compute predictions and gradient\n            z = X_batch.dot(weights)\n            h = 1 / (1 + np.exp(-z))\n            error = h - y_batch\n            gradient = X_batch.T.dot(error) / len(X_batch)\n            \n            # Update weights\n            weights -= learning_rate * gradient\n    \n    # Predict on test data\n    z_test = X_test_np.dot(weights)\n    probabilities = 1 / (1 + np.exp(-z_test))\n    predictions_internal = (probabilities >= 0.5).astype(int)\n    \n    # Map predictions back to original labels\n    return [inverse_map[pred] for pred in predictions_internal.tolist()]\n```\n\n### Explanation\n1. **Special Case Handling**: If all training labels are identical, the function immediately returns that label for all test samples.\n2. **Label Conversion**: The original labels are mapped to {0, 1} for training. After predictions, results are mapped back to the original labels.\n3. **Intercept Addition**: A column of ones is added to the feature matrices to account for the bias term in the logistic regression model.\n4. **Model Training**: Weights are initialized and updated iteratively using mini-batch Gradient Descent. Each epoch involves shuffling the data and processing it in mini-batches to compute gradients and update weights.\n5. **Prediction**: The logistic function (sigmoid) is applied to the dot product of test features and weights. Predictions are thresholded at 0.5 and converted back to the original labels.\n\nThis approach ensures that the model efficiently learns the decision boundary using mini-batch Gradient Descent, handles various label sets, and provides accurate predictions for linearly separable data."}
{"task_id": 176, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. The goal is to train an ensemble of decision stumps on a training dataset and use them to predict class labels for a test dataset.\n\n### Approach\n1. **Convert Labels**: Convert the training labels from {0, 1} to {-1, 1} to align with the AdaBoost formulation.\n2. **Initialize Weights**: Start with uniform weights for all training samples.\n3. **Train Decision Stumps**: For each boosting round, find the best decision stump (feature, threshold, polarity) that minimizes the weighted classification error. The best stump is chosen based on the lowest error, with ties broken by feature index, threshold, and polarity.\n4. **Update Weights**: Adjust the sample weights based on the stump's performance, increasing weights for misclassified samples and decreasing weights for correctly classified samples.\n5. **Early Stopping**: If a perfect stump (zero weighted error) is found, stop training early.\n6. **Predict**: Aggregate the weighted votes of all stumps on the test data and convert the aggregated result to class labels {0, 1}.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    # Convert input to numpy arrays\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    n_samples, n_features = X_train.shape\n    # Convert labels from {0, 1} to {-1, 1}\n    y_converted = np.where(y_train == 1, 1, -1)\n    # Initialize sample weights\n    weights = np.ones(n_samples) / n_samples\n    stumps = []  # To store each stump's parameters and alpha\n    \n    for _ in range(n_estimators):\n        best_error = float('inf')\n        best_j = -1\n        best_threshold = None\n        best_polarity = None\n        best_h = None\n        \n        # Iterate over each feature to find the best stump\n        for j in range(n_features):\n            unique_values = np.unique(X_train[:, j])\n            for threshold in unique_values:\n                for polarity in [1, -1]:\n                    # Compute predictions for current stump\n                    if polarity == 1:\n                        h = np.where(X_train[:, j] < threshold, 1, -1)\n                    else:\n                        h = np.where(X_train[:, j] >= threshold, 1, -1)\n                    # Calculate weighted error\n                    error = np.sum(weights * (h != y_converted))\n                    # Check if current candidate is better\n                    current_candidate = (error, j, threshold, polarity)\n                    if current_candidate < (best_error, best_j, best_threshold, best_polarity):\n                        best_error, best_j, best_threshold, best_polarity = current_candidate\n                        best_h = h.copy()\n        \n        # If perfect stump found, add and break early\n        if best_error < 1e-10:\n            alpha = 0.5 * np.log((1.0 - best_error) / (best_error + 1e-10))\n            stumps.append((best_j, best_threshold, best_polarity, alpha))\n            break\n        \n        # Compute alpha for the best stump\n        alpha = 0.5 * np.log((1.0 - best_error) / (best_error + 1e-10))\n        # Update weights with the best_h\n        exponent = -alpha * y_converted * best_h\n        weights *= np.exp(exponent)\n        weights /= np.sum(weights)  # Normalize\n        # Save the stump parameters\n        stumps.append((best_j, best_threshold, best_polarity, alpha))\n        \n        # Check for early stopping after weight update\n        if best_error < 1e-10:\n            break\n    \n    # Generate predictions for X_test\n    predictions = []\n    for x in X_test:\n        sum_votes = 0.0\n        for j, threshold, polarity, alpha_t in stumps:\n            feature_val = x[j]\n            if polarity == 1:\n                h = 1 if (feature_val < threshold) else -1\n            else:\n                h = 1 if (feature_val >= threshold) else -1\n            sum_votes += alpha_t * h\n        pred = 1 if sum_votes > 0 else 0\n        predictions.append(pred)\n    \n    return predictions\n```\n\n### Explanation\n1. **Label Conversion**: Training labels are converted from {0, 1} to {-1, 1} to align with the AdaBoost formulation which uses -1 and 1 for weight updates.\n2. **Weight Initialization**: Each sample initially has equal weight, ensuring that all samples contribute equally to the first weak learner.\n3. **Stump Training**: For each boosting round, the algorithm iterates through each feature and possible threshold to find the best decision stump. The best stump is selected based on the lowest weighted error, with tie-breaking rules.\n4. **Weight Update**: After selecting the best stump, weights are updated to focus more on misclassified samples, allowing subsequent stumps to correct previous mistakes.\n5. **Early Stopping**: If a stump achieves zero training error, training stops early to avoid unnecessary computations.\n6. **Prediction**: The final prediction is made by aggregating the weighted votes of all stumps. The sign of the aggregated votes determines the class label (0 or 1).\n\nThis approach efficiently combines multiple weak learners to form a strong classifier, leveraging the AdaBoost algorithm's ability to focus on difficult samples through iterative weight updates."}
{"task_id": 178, "completion_id": 0, "completion": "To solve this problem, we need to implement the Particle Swarm Optimization (PSO) algorithm to minimize the Sphere function. The Sphere function is defined as the sum of the squares of each component of a vector, and we aim to find the vector that minimizes this function within the bounds of -1 to 1 for each component. The PSO algorithm uses a swarm of particles that iteratively update their positions and velocities based on their own best-known positions and the best-known position of the entire swarm.\n\n### Approach\n1. **Input Validation**: Check if the input dimensions, number of particles, and number of iterations are positive integers. If any of them are non-positive, return -1.\n2. **Initialization**: Use a random number generator (RNG) with a given seed to initialize the positions of particles uniformly within the bounds [-1, 1]. Initialize velocities to zero.\n3. **Personal and Global Best Tracking**: Track each particle's best-known position (personal best) and the best-known position across all particles (global best).\n4. **Iterative Updates**: For each iteration, update each particle's velocity and position. The velocity update considers the particle's inertia, cognitive component (movement towards its own best position), and social component (movement towards the swarm's best position). The position is clipped to stay within the bounds [-1, 1].\n5. **Evaluation and Update**: After each position update, evaluate the new position. Update the personal best if the new position is better. Update the global best if any particle's new position is better than the current global best.\n6. **Result**: Return the best value found, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\"\"\"\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1.0\n    \n    w = 0.5\n    c1 = 1.5\n    c2 = 1.5\n    \n    rng = np.random.default_rng(seed)\n    \n    # Initialize positions and velocities\n    positions = rng.uniform(low=-1, high=1, size=(num_particles, n_dims))\n    velocities = np.zeros((num_particles, n_dims))\n    \n    # Personal bests and global best\n    pbest_positions = positions.copy()\n    current_values = np.sum(positions ** 2, axis=1)\n    gbest_idx = np.argmin(current_values)\n    gbest_value = current_values[gbest_idx]\n    gbest_position = pbest_positions[gbest_idx].copy()\n    \n    for _ in range(num_iterations):\n        for i in range(num_particles):\n            # Generate random numbers for each dimension\n            r1 = rng.uniform(0, 1, size=n_dims)\n            r2 = rng.uniform(0, 1, size=n_dims)\n            \n            # Update velocity\n            velocities[i] = (w * velocities[i] \n                             + c1 * r1 * (pbest_positions[i] - positions[i]) \n                             + c2 * r2 * (gbest_position - positions[i]))\n            \n            # Update position with clipping\n            new_pos = np.clip(positions[i] + velocities[i], -1, 1)\n            new_val = np.sum(new_pos ** 2)\n            \n            # Update current position\n            positions[i] = new_pos\n            \n            # Update personal best and global best if improved\n            if new_val < current_values[i]:\n                current_values[i] = new_val\n                pbest_positions[i] = new_pos.copy()\n                \n                if new_val < gbest_value:\n                    gbest_value = new_val\n                    gbest_position = new_pos.copy()\n    \n    return round(gbest_value, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input parameters are valid (positive integers). If not, it returns -1.\n2. **Initialization**: The positions of particles are initialized using a uniform distribution within the bounds [-1, 1], and velocities are initialized to zero.\n3. **Tracking Bests**: Each particle's initial position is its personal best. The global best is determined by evaluating the initial positions.\n4. **Iterative Updates**: For each iteration, each particle's velocity is updated based on its inertia, cognitive component, and social component. The position is updated and clipped to stay within bounds.\n5. **Evaluation**: The new position is evaluated, and the personal and global bests are updated if the new position yields a better result.\n6. **Result**: The best value found during the iterations is returned, rounded to four decimal places.\n\nThis approach ensures that the algorithm efficiently explores the search space and converges towards the optimal solution using the principles of swarm intelligence."}
{"task_id": 180, "completion_id": 0, "completion": "To solve this problem, we need to implement the k-Nearest Neighbours (k-NN) algorithm that can function both as a classifier and a regressor. The algorithm should use Euclidean distance, handle edge cases for the value of k, and return predictions based on majority vote for classification or average for regression.\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Convert the training and test feature matrices into numpy arrays for efficient computation.\n2. **Determine Effective k**: Adjust the value of k based on the number of training samples. If k is 0 or exceeds the number of training samples, use all training samples.\n3. **Compute Distances**: For each test sample, compute the Euclidean distances to all training samples.\n4. **Sort Distances**: Sort the distances to find the k-nearest neighbors.\n5. **Neighbor Indices**: Extract the indices of the k-nearest neighbors.\n6. **Prediction**:\n   - **Classification**: Use majority vote to determine the most frequent label among the neighbors. In case of a tie, return the smallest label.\n   - **Regression**: Compute the average of the neighbors' target values and round to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\"\"\"\n\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n    \n    n_train = X_train_np.shape[0]\n    \n    if k == 0 or k > n_train:\n        effective_k = n_train\n    else:\n        effective_k = k\n    \n    predictions = []\n    \n    for test_point in X_test_np:\n        # Compute Euclidean distances\n        distances = np.sqrt(np.sum((X_train_np - test_point) ** 2, axis=1))\n        # Get sorted indices by distance\n        sorted_indices = np.argsort(distances)\n        # Get neighbor indices\n        neighbors_indices = sorted_indices[:effective_k]\n        # Get corresponding labels\n        neighbor_labels = y_train_np[neighbors_indices]\n        \n        if task == \"classification\":\n            # Find most frequent label, resolving ties by smallest label\n            counts = Counter(neighbor_labels)\n            if not counts:\n                # Edge case when there are no neighbors (unlikely given effective_k)\n                predictions.append(None)  # though per problem constraints, this shouldn't happen\n                continue\n            max_count = max(counts.values())\n            candidates = [label for label, cnt in counts.items() if cnt == max_count]\n            prediction = min(candidates)\n        else:\n            # Regression: compute mean and round\n            mean_value = np.mean(neighbor_labels)\n            prediction = round(mean_value, 4)\n        \n        predictions.append(prediction)\n    \n    return predictions\n```\n\n### Explanation\n1. **Input Conversion**: The training and test data are converted to numpy arrays to leverage efficient vector operations.\n2. **Effective k Calculation**: Adjust k to ensure it is within valid bounds based on the number of training samples.\n3. **Distance Calculation**: For each test point, compute the Euclidean distance to all training points using vectorized operations.\n4. **Sorting and Neighbor Selection**: Distances are sorted, and the indices of the nearest neighbors are selected.\n5. **Prediction Logic**:\n   - **Classification**: Uses a counter to tally the labels of the nearest neighbors, resolving ties by selecting the smallest label.\n   - **Regression**: Computes the mean of the neighbors' targets and rounds the result to four decimal places.\n\nThis approach ensures that the k-NN algorithm efficiently handles both classification and regression tasks while adhering to the specified constraints."}
{"task_id": 184, "completion_id": 0, "completion": "To solve this problem, we need to implement a binary decision-tree classifier using the CART algorithm from scratch. The classifier should use Gini impurity for splitting nodes and handle stopping criteria such as identical labels, maximum depth, and empty child nodes.\n\n### Approach\n1. **Tree Structure**: Represent each node in the decision tree using a `TreeNode` class, which can be either a leaf node (containing a class label) or a decision node (containing a feature index, threshold, and left/right children).\n2. **Gini Impurity**: Calculate the Gini impurity to measure the quality of a split. The Gini impurity for a set of labels is given by \\(1 - \\sum (p_i^2)\\), where \\(p_i\\) is the proportion of samples of class \\(i\\).\n3. **Splitting Criteria**: For each feature, evaluate midpoints between consecutive unique values as potential thresholds. Select the split that maximizes the Gini gain (difference between parent Gini impurity and weighted children Gini impurities).\n4. **Stopping Criteria**: Stop splitting and create a leaf node if all labels are identical, the maximum depth is reached, or a split results in an empty child.\n5. **Recursive Tree Building**: Recursively build the tree by splitting nodes based on the best split found, and handle stopping criteria at each step.\n6. **Prediction**: Traverse the built tree for each test sample to determine its class label based on the decision nodes' thresholds and features.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\"\"\"\n\n    class TreeNode:\n        def __init__(self, value=None, feature=None, threshold=None, left=None, right=None):\n            self.value = value\n            self.feature = feature\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n\n        def is_leaf(self):\n            return self.value is not None\n\n    def gini(y):\n        counts = np.bincount(y)\n        if len(counts) == 0:\n            return 0.0\n        probs = counts / len(y)\n        return 1.0 - np.sum(probs ** 2)\n\n    def get_majority_class(y):\n        counts = np.bincount(y)\n        return np.argmax(counts)\n\n    def build_tree(X, y, current_depth, max_depth):\n        # All samples have the same label\n        unique_classes = np.unique(y)\n        if len(unique_classes) == 1:\n            return TreeNode(value=int(unique_classes[0]))\n\n        # Check if max_depth is reached\n        if max_depth is not None and current_depth >= max_depth:\n            return TreeNode(value=int(get_majority_class(y)))\n\n        parent_gini = gini(y)\n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n        best_left_mask = None\n        best_right_mask = None\n\n        n_samples, n_features = X.shape\n\n        for feature_idx in range(n_features):\n            feature_values = X[:, feature_idx]\n            sorted_indices = np.argsort(feature_values)\n            sorted_features = feature_values[sorted_indices]\n            unique_values = np.unique(sorted_features)\n\n            if len(unique_values) < 2:\n                continue  # No split possible\n\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n\n            for thresh in thresholds:\n                left_mask = feature_values <= thresh\n                right_mask = ~left_mask\n\n                y_left = y[left_mask]\n                y_right = y[right_mask]\n\n                if len(y_left) == 0 or len(y_right) == 0:\n                    continue\n\n                # Calculate weighted Gini impurity\n                g_left = gini(y_left)\n                g_right = gini(y_right)\n                n_left, n_right = len(y_left), len(y_right)\n                weighted_gini = (n_left * g_left + n_right * g_right) / (n_left + n_right)\n                gain = parent_gini - weighted_gini\n\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature_idx\n                    best_threshold = thresh\n                    best_left_mask = left_mask\n                    best_right_mask = right_mask\n\n        # If no valid split found or no gain\n        if best_gain <= 0 or best_feature is None:\n            return TreeNode(value=int(get_majority_class(y)))\n\n        # Split the data\n        X_left, y_left = X[best_left_mask], y[best_left_mask]\n        X_right, y_right = X[best_right_mask], y[best_right_mask]\n\n        # Recursively build children\n        left_child = build_tree(X_left, y_left, current_depth + 1, max_depth)\n        right_child = build_tree(X_right, y_right, current_depth + 1, max_depth)\n\n        return TreeNode(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n\n    # Handle edge case with no training data\n    if X_train.size == 0:\n        return [0] * len(X_test)\n\n    # Build the decision tree\n    root = build_tree(X_train, y_train, current_depth=0, max_depth=max_depth)\n\n    # Predict each test sample\n    predictions = []\n    for sample in X_test:\n        node = root\n        while True:\n            if node.is_leaf():\n                predictions.append(node.value)\n                break\n            feature_val = sample[node.feature]\n            if feature_val <= node.threshold:\n                node = node.left\n            else:\n                node = node.right\n\n    return predictions\n```\n\n### Explanation\n1. **Tree Node Structure**: The `TreeNode` class represents each node in the decision tree, which can be a leaf node (with a class label) or a decision node (with a feature index, threshold, and child nodes).\n2. **Gini Impurity Calculation**: The `gini` function computes the impurity of a set of labels, which helps in evaluating the quality of splits.\n3. **Majority Class Calculation**: The `get_majority_class` function determines the most frequent class label in a node, used when creating leaf nodes due to stopping criteria.\n4. **Tree Construction**: The `build_tree` function recursively constructs the tree by evaluating potential splits, applying stopping criteria, and creating child nodes.\n5. **Prediction**: Each test sample is classified by traversing the tree from the root to a leaf node, applying the decision rules at each node.\n\nThis approach ensures that the decision tree is built efficiently, adhering to the specified stopping criteria and splitting rules, and provides deterministic predictions for test samples."}
{"task_id": 190, "completion_id": 0, "completion": "To solve this problem, we need to find the best binary split of a dataset based on the Gini impurity, which is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. The goal is to minimize the weighted Gini impurity of the split.\n\n### Approach\n1. **Convert Inputs to Arrays**: Convert the feature matrix `X` and labels `y` to NumPy arrays for efficient computation.\n2. **Compute Gini Impurity of the Whole Dataset**: Calculate the Gini impurity for the entire dataset to use as a fallback if no valid splits are found.\n3. **Check for Constant Features**: Determine if all features are constant (i.e., they have only one unique value). If so, return the fallback result immediately.\n4. **Iterate Over Features and Thresholds**: For each feature, consider each unique value as a potential threshold. For each threshold, split the dataset into left and right subsets.\n5. **Validate Splits**: Skip splits where either subset is empty.\n6. **Compute Gini Impurity for Subsets**: Calculate the Gini impurity for both left and right subsets and compute the weighted Gini impurity for the split.\n7. **Track Best Split**: Keep track of the best split based on the lowest Gini impurity, using tie-breaking rules if necessary.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\"\"\"\n\n    X = np.asarray(X)\n    y = np.asarray(y)\n    n_samples, n_features = X.shape\n\n    # Compute Gini impurity for the entire dataset\n    classes, counts = np.unique(y, return_counts=True)\n    total = len(y)\n    if total == 0:\n        return (-1, None, 0.0)  # Edge case with no samples\n    p = counts / total\n    g_whole = 1.0 - np.sum(p ** 2)\n    g_whole_rounded = round(g_whole, 4)\n\n    # Check if all features are constant\n    all_constant = True\n    for j in range(n_features):\n        if np.unique(X[:, j]).size > 1:\n            all_constant = False\n            break\n    if all_constant:\n        return (-1, None, g_whole_rounded)\n\n    best_feature = -1\n    best_threshold = None\n    best_gini = float('inf')\n\n    for j in range(n_features):\n        feature = X[:, j]\n        unique_values = np.unique(feature)\n        for v in unique_values:\n            left_mask = feature <= v\n            n_left = np.sum(left_mask)\n            n_right = n_samples - n_left\n\n            if n_left == 0 or n_right == 0:\n                continue\n\n            # Compute Gini impurity for left and right\n            y_left = y[left_mask]\n            y_right = y[~left_mask]\n\n            # Left node\n            left_counts = np.unique(y_left, return_counts=True)[1]\n            p_left = left_counts / n_left\n            g_left = 1.0 - np.sum(p_left ** 2)\n\n            # Right node\n            right_counts = np.unique(y_right, return_counts=True)[1]\n            p_right = right_counts / n_right\n            g_right = 1.0 - np.sum(p_right ** 2)\n\n            # Weighted Gini impurity\n            g_split = (n_left / n_samples) * g_left + (n_right / n_samples) * g_right\n\n            # Update best split considering tie-breakers\n            if g_split < best_gini - 1e-12:\n                best_gini = g_split\n                best_feature = j\n                best_threshold = v\n            elif abs(g_split - best_gini) < 1e-12:\n                if (best_feature == -1) or (j < best_feature) or (j == best_feature and v < best_threshold):\n                    best_gini = g_split\n                    best_feature = j\n                    best_threshold = v\n\n    if best_feature == -1:\n        return (-1, None, g_whole_rounded)\n    else:\n        return (best_feature, best_threshold, round(best_gini, 4))\n```\n\n### Explanation\n1. **Input Conversion**: The feature matrix `X` and labels `y` are converted to NumPy arrays for efficient manipulation.\n2. **Gini Impurity Calculation**: The Gini impurity for the entire dataset is computed to handle cases where no valid splits exist.\n3. **Constant Feature Check**: If all features are constant, the function immediately returns the fallback result.\n4. **Feature and Threshold Iteration**: For each feature, each unique value is considered as a potential threshold to split the dataset.\n5. **Split Validation**: Splits resulting in empty subsets are skipped.\n6. **Gini Computation**: For valid splits, the Gini impurity of the left and right subsets is calculated, and the weighted Gini impurity is determined.\n7. **Best Split Tracking**: The best split is tracked using the minimum Gini impurity and tie-breaking rules for equal impurities.\n\nThis approach ensures that the best possible split is found efficiently, adhering to the constraints and requirements specified."}
{"task_id": 191, "completion_id": 0, "completion": "To solve this problem, we need to create a factory function that returns a callable distance metric based on the specified name. The callable should validate the input vectors and compute the distance using the specified metric. If validation fails or the metric name is unsupported, the callable should return -1.\n\n### Approach\n1. **Factory Function**: The `metric` function takes a metric name and returns a callable. If the name is unsupported, the returned callable always returns -1.\n2. **Validation**: The callable checks if the input vectors are valid (lists, tuples, or NumPy arrays, 1-dimensional, non-empty, same length). For the cosine metric, it also checks that neither vector has a zero norm.\n3. **Distance Calculation**: The callable computes the distance using the specified metric (Euclidean, Manhattan, Chebyshev, or Cosine) and rounds the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n    valid_metrics = {'euclidean', 'manhattan', 'chebyshev', 'cosine'}\n    if name not in valid_metrics:\n        def invalid_metric(x, y):\n            return -1\n        return invalid_metric\n    \n    def validated_metric(x, y):\n        try:\n            # Check if inputs are of allowed types\n            if not isinstance(x, (list, tuple, np.ndarray)) or not isinstance(y, (list, tuple, np.ndarray)):\n                return -1\n            \n            # Convert to numpy arrays of float; this handles numeric conversion checks\n            x_arr = np.array(x, dtype=float)\n            y_arr = np.array(y, dtype=float)\n            \n            # Check if arrays are 1-dimensional\n            if x_arr.ndim != 1 or y_arr.ndim != 1:\n                return -1\n            \n            # Check non-empty and same length\n            if x_arr.size == 0 or y_arr.size == 0 or x_arr.shape != y_arr.shape:\n                return -1\n            \n            # Additional checks for cosine distance\n            if name == 'cosine':\n                norm_x = np.linalg.norm(x_arr)\n                norm_y = np.linalg.norm(y_arr)\n                if norm_x == 0.0 or norm_y == 0.0:\n                    return -1\n            \n            # Compute the distance based on the metric name\n            if name == 'euclidean':\n                distance = np.sqrt(np.sum((x_arr - y_arr) ** 2))\n            elif name == 'manhattan':\n                distance = np.sum(np.abs(x_arr - y_arr))\n            elif name == 'chebyshev':\n                distance = np.max(np.abs(x_arr - y_arr))\n            elif name == 'cosine':\n                dot_product = np.dot(x_arr, y_arr)\n                norm_product = np.linalg.norm(x_arr) * np.linalg.norm(y_arr)\n                distance = 1 - (dot_product / norm_product)\n            else:\n                return -1  # This is unreachable due to earlier check\n            \n            # Round to four decimal places as per requirement\n            return round(distance, 4)\n        except:\n            return -1\n    \n    return validated_metric\n```\n\n### Explanation\n1. **Factory Function**: The `metric` function checks if the provided name is valid. If not, it returns a callable that always returns -1.\n2. **Validation**: The returned callable first checks if the inputs are valid types (list, tuple, or NumPy array). It converts them to NumPy arrays and checks if they are 1-dimensional, non-empty, and of the same length.\n3. **Cosine Specific Check**: For the cosine metric, it ensures neither vector has a zero norm to avoid division by zero.\n4. **Distance Calculation**: Depending on the metric name, it computes the distance using the appropriate formula and rounds the result to four decimal places.\n5. **Error Handling**: Any errors during validation or computation are caught, and the callable returns -1 in such cases."}
{"task_id": 197, "completion_id": 0, "completion": "To solve this problem, we need to analyze the action space of a reinforcement learning (RL) environment and return statistics about its structure. The action space can be either discrete or continuous, and it can be single- or multi-dimensional. The goal is to provide the number of actions per dimension, a list of all possible discrete actions (if applicable), and the total number of action dimensions.\n\n### Approach\n1. **Determine the Structure of the Action Space**:\n   - Check if the action space is multi-dimensional by looking for a `spaces` attribute. If present, process each subspace individually.\n   - For each subspace, determine if it is discrete (has an `n` attribute) or continuous (has a `shape` attribute).\n   - For single-dimensional spaces, check if it is discrete or continuous by looking for `n` or `shape` attributes respectively.\n\n2. **Generate Statistics**:\n   - **n_actions_per_dim**: For each dimension, store the number of discrete actions or `inf` for continuous dimensions.\n   - **action_ids**: Generate all possible discrete actions using the Cartesian product if all dimensions are discrete. If any dimension is continuous, set this to `None`.\n   - **action_dim**: Calculate the total number of dimensions by summing up the dimensions from all subspaces.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\"\"\"\n    n_actions_per_dim = []\n    action_dim = 0\n    action_ids = None\n\n    if hasattr(env.action_space, 'spaces'):\n        # Handle multi-dimensional action space with subspaces\n        for subspace in env.action_space.spaces:\n            if hasattr(subspace, 'n'):\n                # Discrete subspace\n                n_actions_per_dim.append(subspace.n)\n                action_dim += 1\n            elif hasattr(subspace, 'shape'):\n                # Continuous subspace\n                dims = int(np.prod(subspace.shape))\n                n_actions_per_dim.extend([np.inf] * dims)\n                action_dim += dims\n            else:\n                raise ValueError(\"Unsupported subspace type in multi-dimensional action space.\")\n    else:\n        # Handle single-dimensional action space\n        if hasattr(env.action_space, 'n'):\n            # Discrete action space\n            n_actions_per_dim.append(env.action_space.n)\n            action_dim = 1\n        elif hasattr(env.action_space, 'shape'):\n            # Continuous action space\n            dims = int(np.prod(env.action_space.shape))\n            n_actions_per_dim.extend([np.inf] * dims)\n            action_dim = dims\n        else:\n            raise ValueError(\"Unsupported action space type.\")\n\n    # Determine if any dimension is continuous\n    has_continuous = any(np.isinf(n) for n in n_actions_per_dim)\n    if has_continuous:\n        action_ids = None\n    else:\n        # Generate all possible discrete action tuples\n        ranges = [range(n) for n in n_actions_per_dim]\n        action_ids = list(product(*ranges))\n\n    return (n_actions_per_dim, action_ids, action_dim)\n```\n\n### Explanation\n1. **Multi-dimensional Action Space**:\n   - If the environment's action space has a `spaces` attribute, iterate through each subspace. For each discrete subspace (with `n`), add the number of actions to `n_actions_per_dim` and increment the dimension count. For continuous subspaces (with `shape`), calculate the number of dimensions from the shape and add `inf` for each dimension.\n\n2. **Single-dimensional Action Space**:\n   - Check if the action space is discrete (using `n`) or continuous (using `shape`). For discrete, store the number of actions. For continuous, calculate the number of dimensions from the shape and add `inf` for each.\n\n3. **Generate Action IDs**:\n   - If any dimension is continuous, set `action_ids` to `None`. Otherwise, generate all possible discrete actions using the Cartesian product of ranges for each dimension.\n\nThis approach ensures that we correctly handle both single- and multi-dimensional action spaces, distinguish between discrete and continuous dimensions, and efficiently generate the required statistics."}
{"task_id": 198, "completion_id": 0, "completion": "To solve this problem, we need to update the word\u2013topic matrix \u03b2 in Latent Dirichlet Allocation (LDA) during the variational M-step. The goal is to compute the maximum-likelihood estimate of \u03b2 using the current variational parameters \u03c6 (phi) and the given corpus, then normalize the columns of \u03b2 so that each sums to 1. \n\n### Approach\n1. **Initialization**: Determine the number of topics (T) from the shape of the phi matrix. Initialize a zero matrix \u03b2 with dimensions V (vocabulary size) x T (number of topics).\n2. **Accumulate Counts**: For each document and each token in the document, update the \u03b2 matrix using the corresponding phi values. This involves adding the phi values for each token to the corresponding row in \u03b2 based on the word index from the corpus.\n3. **Normalization**: Compute the sum of each column in \u03b2 and normalize each column so that the sum equals 1. This ensures that each column represents a valid probability distribution over the vocabulary for the corresponding topic.\n4. **Rounding and Conversion**: Round each entry in the normalized \u03b2 matrix to 4 decimal places and convert the result to a list of lists for the required output format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    # Determine the number of topics T\n    T = phi[0].shape[1] if phi else 0\n    \n    # Initialize beta with zeros\n    beta = np.zeros((V, T), dtype=np.float64)\n    \n    # Iterate over each document and its tokens to accumulate counts\n    for d in range(len(phi)):\n        doc_phi = phi[d]  # Shape (N_d, T)\n        doc_words = corpus[d]  # List of word indices for document d\n        N_d = len(doc_words)\n        \n        for n in range(N_d):\n            v = doc_words[n]\n            beta[v, :] += doc_phi[n, :]\n    \n    # Compute the sum for each column (topic)\n    sums = beta.sum(axis=0)\n    \n    # Avoid division by zero by setting zero sums to 1 (though such columns would remain zero)\n    sums = np.where(sums == 0, 1.0, sums)\n    \n    # Normalize each column to sum to 1\n    beta_normalized = beta / sums.reshape(1, -1)\n    \n    # Round to 4 decimal places\n    beta_rounded = np.round(beta_normalized, 4)\n    \n    # Convert to a list of lists, ensuring all elements are standard Python floats\n    result = [\n        [float(round(val, 4)) for val in row]\n        for row in beta_rounded\n    ]\n    \n    return result\n```\n\n### Explanation\n- **Initialization**: The matrix \u03b2 is initialized with zeros to ensure that any word not present in the corpus will have a row of zeros.\n- **Accumulate Counts**: For each token in each document, we update the corresponding row in \u03b2 using the phi values. This step aggregates the contributions of each token to each topic based on the variational parameters.\n- **Normalization**: Each column of \u03b2 is normalized by dividing by the sum of its entries. This ensures that each column represents a valid probability distribution. If a column sum is zero (indicating no tokens were assigned to that topic), it is handled by setting the sum to 1 to avoid division by zero, resulting in a column of zeros.\n- **Rounding and Conversion**: The normalized \u03b2 matrix is rounded to 4 decimal places and converted to a list of lists, ensuring the output format meets the specified requirements.\n\nThis approach efficiently computes the updated \u03b2 matrix while adhering to the constraints and requirements of the problem."}
{"task_id": 202, "completion_id": 0, "completion": "To solve this problem, we need to implement the Partitioning Around Medoids (PAM) clustering algorithm. This algorithm groups data points into clusters by iteratively improving a set of representative points called medoids. The goal is to minimize the total clustering cost, defined as the sum of the Euclidean distances between every sample and the medoid of the cluster it belongs to.\n\n### Approach\n1. **Initialization**: Start with the first `k` samples as the initial medoids.\n2. **Iterative Improvement**: Repeatedly attempt to reduce the total clustering cost by swapping any current medoid with a non-medoid sample. Each swap is evaluated based on the reduction in the total cost, and the swap that provides the largest cost reduction is accepted. This process continues until no further cost reduction is possible.\n3. **Convergence Check**: Stop when no swap can further decrease the cost.\n4. **Cluster Assignment**: After convergence, assign each sample to the cluster of the closest medoid. If two medoids are equidistant, choose the one that appears first in the medoid list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\"\"\"\n    n_samples = X.shape[0]\n    current_medoids = list(range(k))  # Initial medoids are the first k samples\n    \n    # Compute initial total cost\n    current_cost = 0.0\n    for x in X:\n        min_dist = np.min([np.linalg.norm(x - X[med]) for med in current_medoids])\n        current_cost += min_dist\n    \n    while True:\n        best_gain = 0.0\n        best_new_medoids = current_medoids.copy()\n        \n        # Evaluate all possible swaps\n        for m in current_medoids:\n            non_medoids = [s for s in range(n_samples) if s not in current_medoids]\n            for s in non_medoids:\n                # Create new_medoids by replacing m with s\n                new_medoids = current_medoids.copy()\n                idx = new_medoids.index(m)\n                new_medoids[idx] = s\n                \n                # Calculate new cost for this swap\n                new_cost = 0.0\n                for x in X:\n                    min_dist = np.inf\n                    for med in new_medoids:\n                        dist = np.linalg.norm(x - X[med])\n                        if dist < min_dist:\n                            min_dist = dist\n                    new_cost += min_dist\n                \n                gain = current_cost - new_cost\n                if gain > best_gain:\n                    best_gain = gain\n                    best_new_medoids = new_medoids\n        \n        # Check if any swap improved the cost\n        if best_gain > 0:\n            current_medoids = best_new_medoids\n            current_cost -= best_gain\n        else:\n            break  # Converged\n    \n    # Assign each sample to the closest medoid\n    labels = []\n    for x in X:\n        min_dist = np.inf\n        best_label = 0\n        for label_idx, med in enumerate(current_medoids):\n            dist = np.linalg.norm(x - X[med])\n            if dist < min_dist:\n                min_dist = dist\n                best_label = label_idx\n            elif dist == min_dist and label_idx < best_label:\n                best_label = label_idx\n        labels.append(best_label)\n    \n    return labels\n```\n\n### Explanation\n1. **Initialization**: The initial medoids are the first `k` samples of the input matrix `X`.\n2. **Cost Calculation**: The initial cost is calculated by summing the minimum Euclidean distances from each sample to the nearest medoid.\n3. **Iterative Improvement**: For each medoid and each non-medoid sample, compute the new cost if they were swapped. Track the swap that results in the maximum cost reduction. If such a swap exists, update the medoids and repeat the process.\n4. **Convergence Check**: The loop terminates when no further cost-reducing swaps are possible.\n5. **Cluster Assignment**: Each sample is assigned to the cluster of the closest medoid, with ties resolved by the order of medoids in the list.\n\nThis approach ensures that the algorithm follows a deterministic path, making the results reproducible and testable. The use of Euclidean distance and iterative improvement ensures that the total clustering cost is minimized effectively."}
{"task_id": 216, "completion_id": 0, "completion": "To solve this problem, we need to implement Elastic Net linear regression from scratch using batch gradient descent. The model should generate polynomial features up to a specified degree, standardize these features, and use a combination of L1 and L2 regularization to learn the weights. The solution must also handle input validation and return predictions for unseen data.\n\n### Approach\n1. **Input Validation**: Check if the input matrices `x_train` and `x_test` have the same number of features and if the number of iterations is valid.\n2. **Polynomial Feature Expansion**: Generate polynomial features for each original feature up to the specified degree, including a bias term (column of ones).\n3. **Standardization**: Standardize the polynomial features (excluding the bias term) using the mean and standard deviation from the training data.\n4. **Model Training**: Use batch gradient descent to minimize the mean squared error with Elastic Net regularization. The gradient includes contributions from both L1 and L2 penalties.\n5. **Prediction**: Apply the learned weights to the standardized test data and return the predictions rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_test = np.array(x_test)\n    \n    # Reshape if input is 1D\n    if x_train.ndim == 1:\n        x_train = x_train.reshape(-1, 1)\n    if x_test.ndim == 1:\n        x_test = x_test.reshape(-1, 1)\n    \n    # Check input compatibility\n    if x_train.shape[1] != x_test.shape[1] or n_iterations < 1:\n        return -1\n    \n    # Function to generate polynomial features including bias term\n    def generate_poly(x, degree):\n        n_samples, n_features = x.shape\n        poly = np.ones((n_samples, 1))  # Start with bias term\n        for feature_idx in range(n_features):\n            feature_col = x[:, feature_idx]\n            for power in range(1, degree + 1):\n                new_col = (feature_col ** power).reshape(-1, 1)\n                poly = np.hstack((poly, new_col))\n        return poly\n    \n    # Generate polynomial features for train and test\n    x_train_poly = generate_poly(x_train, degree)\n    x_test_poly = generate_poly(x_test, degree)\n    \n    # Standardize non-bias features using training data's mean and std\n    x_train_non_bias = x_train_poly[:, 1:]\n    if x_train_non_bias.size == 0:\n        x_train_std = x_train_poly\n        x_test_std = x_test_poly\n    else:\n        means = np.mean(x_train_non_bias, axis=0)\n        stds = np.std(x_train_non_bias, axis=0, ddof=0)\n        stds[stds == 0] = 1.0  # Avoid division by zero\n        x_train_std = x_train_poly.copy()\n        x_train_std[:, 1:] = (x_train_non_bias - means) / stds\n        # Apply same transformation to test data\n        x_test_non_bias = x_test_poly[:, 1:]\n        x_test_std = x_test_poly.copy()\n        x_test_std[:, 1:] = (x_test_non_bias - means) / stds\n    \n    # Initialize weights\n    n_samples_train, n_features = x_train_std.shape\n    w = np.zeros(n_features)\n    \n    # Perform batch gradient descent\n    for _ in range(n_iterations):\n        y_pred = x_train_std @ w\n        error = y_pred - y_train\n        gradient_mse = (x_train_std.T @ error) / n_samples_train\n        # Compute regularization gradient for non-bias terms\n        reg_grad = np.zeros_like(w)\n        if n_features > 1:\n            mask = np.arange(n_features) >= 1\n            w_non_bias = w[mask]\n            sign_w = np.sign(w_non_bias)\n            l1_part = l1_ratio * sign_w\n            l2_part = 2 * (1 - l1_ratio) * w_non_bias\n            reg_grad[mask] = reg_factor * (l1_part + l2_part)\n        total_gradient = gradient_mse + reg_grad\n        w -= learning_rate * total_gradient\n    \n    # Predict on test data\n    y_pred_test = x_test_std @ w\n    return np.round(y_pred_test, 4).tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input matrices `x_train` and `x_test` have compatible shapes and if the number of iterations is valid.\n2. **Polynomial Feature Expansion**: Polynomial features are generated for each original feature up to the specified degree. This includes a bias term (column of ones) added as the first column.\n3. **Standardization**: The non-bias features are standardized using the mean and standard deviation from the training data to ensure zero mean and unit variance. The same transformation is applied to the test data.\n4. **Model Training**: The model uses batch gradient descent to update the weights. The gradient includes contributions from both the mean squared error and the Elastic Net regularization term, which combines L1 and L2 penalties.\n5. **Prediction**: The trained model is applied to the standardized test data to generate predictions, which are then rounded to four decimal places and returned as a list."}
{"task_id": 217, "completion_id": 0, "completion": "To solve this problem, we need to compute three core components of the logistic (sigmoid) loss for binary classification tasks. These components are the gradient of the logistic loss, the Hessian (second derivative), and the probability obtained from the sigmoid transformation of predicted scores. The solution must efficiently handle vectorized operations and ensure the results are rounded to six decimal places.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: This allows for efficient vectorized operations, which are crucial for performance when dealing with large datasets.\n2. **Compute Sigmoid Values**: The sigmoid function is applied to the predicted scores to obtain probabilities. This is also used in calculating the Hessian.\n3. **Calculate Gradient**: The gradient for each observation is derived using the sigmoid of the negative product of actual and predicted values.\n4. **Calculate Hessian**: The Hessian is computed using the product of the sigmoid values and their complements.\n5. **Round Results**: Each computed value is rounded to six decimal places and converted back to Python lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    \n    # Compute probabilities and Hessian components\n    s_predicted = 1 / (1 + np.exp(-predicted))\n    hessian = s_predicted * (1 - s_predicted)\n    prob = s_predicted\n    \n    # Compute gradient components\n    s_grad = 1 / (1 + np.exp(actual * predicted))\n    grad = actual * s_grad\n    \n    # Round to 6 decimal places and convert to lists\n    grad_list = np.round(grad, 6).tolist()\n    hessian_list = np.round(hessian, 6).tolist()\n    prob_list = np.round(prob, 6).tolist()\n    \n    return (grad_list, hessian_list, prob_list)\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The inputs are converted to NumPy arrays to leverage vectorized operations, which are more efficient than iterating through each element individually.\n2. **Sigmoid Calculation**: The sigmoid function is applied to the predicted values to get probabilities. This is done using the formula \\( \\text{sigmoid}(z) = 1 / (1 + e^{-z}) \\).\n3. **Gradient Calculation**: The gradient for each observation is computed using the formula \\( \\text{grad}_i = \\text{actual}_i \\times \\text{sigmoid}(-\\text{actual}_i \\times \\text{predicted}_i) \\). This is efficiently computed using vectorized operations.\n4. **Hessian Calculation**: The Hessian is derived from the product of the sigmoid values and their complements, which is the derivative of the sigmoid function.\n5. **Rounding and Conversion**: The results are rounded to six decimal places using NumPy's rounding function and then converted to Python lists to meet the output requirements.\n\nThis approach ensures that the computations are efficient, accurate, and meet the specified formatting requirements."}
{"task_id": 218, "completion_id": 0, "completion": "To solve this problem, we need to determine the outcome of a Blackjack game based on the final hands of the player and the dealer. The solution must adhere to the specific rules of Blackjack regarding hand values, busts, and natural blackjacks to compute the correct reward for the player.\n\n### Approach\n1. **Calculate Hand Scores**: For both the player and the dealer, compute the score of their hands. Each Ace (1) can be counted as 11 if it helps achieve a score of 21 or less. The score is the highest possible value without exceeding 21.\n2. **Check for Busts**: If either the player or the dealer has a score over 21, they bust. The player loses immediately if they bust, and wins if the dealer busts.\n3. **Check for Natural Blackjacks**: A natural blackjack is a two-card hand that sums to 21 (one Ace and one 10-value card). If the player has a natural blackjack and the dealer does not, the player gets a higher reward (1.5 instead of 1). If both have natural blackjacks, it's a draw.\n4. **Compare Scores**: If neither busts and there are no natural blackjacks, compare the scores. The higher score wins, with equal scores resulting in a draw.\n\n### Solution Code\n```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\"\"\"\n\n    # Calculate player's score\n    player_sum = sum(player)\n    player_aces = player.count(1)\n    if player_aces > 0 and (player_sum + 10) <= 21:\n        player_score = player_sum + 10\n    else:\n        player_score = player_sum\n\n    if player_score > 21:\n        return -1.0\n\n    # Calculate dealer's score\n    dealer_sum = sum(dealer)\n    dealer_aces = dealer.count(1)\n    if dealer_aces > 0 and (dealer_sum + 10) <= 21:\n        dealer_score = dealer_sum + 10\n    else:\n        dealer_score = dealer_sum\n\n    if dealer_score > 21:\n        return 1.0\n\n    # Check for naturals\n    player_natural = len(player) == 2 and player_score == 21\n    dealer_natural = len(dealer) == 2 and dealer_score == 21\n\n    if player_natural:\n        if dealer_natural:\n            return 0.0\n        else:\n            return 1.5 if dealer_score != 21 else 0.0\n    elif dealer_natural:\n        return 0.0 if player_score == 21 else -1.0\n    else:\n        if player_score > dealer_score:\n            return 1.0\n        elif player_score < dealer_score:\n            return -1.0\n        else:\n            return 0.0\n```\n\n### Explanation\n1. **Hand Score Calculation**: The score is calculated by summing the card values. If there are Aces and adding 10 to the sum does not exceed 21, one Ace is counted as 11.\n2. **Bust Check**: If the player's score exceeds 21, they lose immediately. If the dealer's score exceeds 21, the player wins.\n3. **Natural Blackjack Check**: A natural blackjack is identified by a two-card hand summing to 21. If both have natural blackjacks, it's a draw. If only the player has one, they win with a reward of 1.5.\n4. **Score Comparison**: If neither has a natural blackjack and neither busts, the scores are compared directly to determine the winner.\n\nThis approach ensures that all Blackjack rules are followed accurately, providing the correct reward based on the game's outcome."}
{"task_id": 221, "completion_id": 0, "completion": "To solve this problem, we need to implement the Mean Squared Logarithmic Error (MSLE) metric, which is commonly used to evaluate regression models. The MSLE is defined as the average of the squared differences between the logarithms of the actual and predicted values, adjusted by adding 1 to each value before taking the logarithm. \n\n### Approach\n1. **Input Validation**: Ensure that the actual and predicted values have the same shape and that all values are non-negative. If any check fails, return -1.\n2. **Helper Function**: Compute the element-wise squared logarithmic error between actual and predicted values using the formula \\((\\log(1 + \\text{actual}_i) - \\log(1 + \\text{predicted}_i))^2\\) for each element.\n3. **Main Function**: Use the helper function to compute the squared logarithmic errors, then take the mean of these errors and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"Return element-wise squared log errors.\"\"\"\n    return (np.log1p(actual) - np.log1p(predicted)) ** 2\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    \n    # Check if the shapes are the same\n    if actual.shape != predicted.shape:\n        return -1.0\n    \n    # Check all elements are non-negative\n    if (actual < 0).any() or (predicted < 0).any():\n        return -1.0\n    \n    # Calculate squared log errors and their mean\n    sle = squared_log_error(actual, predicted)\n    msle = np.mean(sle)\n    \n    # Round to 4 decimal places\n    return round(msle, 4)\n```\n\n### Explanation\n1. **Input Validation**: The code first converts the input sequences (lists, tuples, or arrays) into NumPy arrays. It then checks if these arrays have the same shape. If not, it returns -1. Next, it checks if all elements in both arrays are non-negative. If any element is negative, it returns -1.\n2. **Helper Function**: The helper function `squared_log_error` computes the element-wise squared logarithmic errors using NumPy's `log1p` function, which efficiently computes \\(\\log(1 + x)\\).\n3. **Main Function**: The main function `mean_squared_log_error` uses the helper function to get the squared errors, computes their mean, and rounds the result to 4 decimal places before returning it as a float.\n\nThis approach ensures that the implementation is efficient, handles edge cases through input validation, and adheres to the mathematical definition of MSLE."}
{"task_id": 222, "completion_id": 0, "completion": "To solve this problem, we need to implement the sigmoid activation function that works element-wise on scalar values, Python lists, and NumPy arrays. The solution must be numerically stable for very large positive or negative values and return the results in the appropriate format based on the input type.\n\n### Approach\nThe sigmoid function is defined as:\n\\[ \\text{sigmoid}(z) = \\frac{1}{1 + \\exp(-z)} \\]\n\nTo ensure numerical stability, especially for very large positive or negative values, we split the computation into two cases:\n1. For \\( z \\geq 0 \\), we use the standard formula \\( 1 / (1 + \\exp(-z)) \\).\n2. For \\( z < 0 \\), we use the alternative formula \\( \\exp(z) / (1 + \\exp(z)) \\).\n\nThis approach avoids overflow issues by preventing the exponentiation of large positive numbers.\n\nThe solution handles different input types as follows:\n1. **Scalar Input**: Convert to a float, compute the sigmoid value using the stable formula, and return a float rounded to 4 decimal places.\n2. **List or NumPy Array Input**: Convert the input to a NumPy array, apply the sigmoid function element-wise using vectorized operations for efficiency, and convert the result back to a Python list with the same structure, rounding each element to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        # Handle scalar input (int, float) or scalar numpy array\n        if isinstance(z, np.ndarray):\n            z_scalar = z.item()\n        else:\n            z_scalar = z\n        if z_scalar >= 0:\n            s = 1.0 / (1.0 + np.exp(-z_scalar))\n        else:\n            s = np.exp(z_scalar) / (1.0 + np.exp(z_scalar))\n        return round(s, 4)\n    else:\n        # Handle array-like input (list or numpy array)\n        arr = np.array(z, dtype=np.float64)\n        # Compute the sigmoid with numerical stability\n        result = np.where(arr >= 0, 1.0 / (1.0 + np.exp(-arr)), np.exp(arr) / (1.0 + np.exp(arr)))\n        rounded = np.round(result, 4)\n        return rounded.tolist()\n```\n\n### Explanation\n1. **Scalar Handling**: The function first checks if the input is a scalar (either a Python int/float or a scalar NumPy array). If so, it computes the sigmoid using the stable formula, rounds the result to 4 decimal places, and returns it as a float.\n2. **Array Handling**: For non-scalar inputs (lists or NumPy arrays), the input is converted to a NumPy array. The sigmoid function is applied element-wise using `np.where` to handle positive and negative values appropriately. The results are rounded to 4 decimal places and converted back to a Python list to maintain the input structure.\n\nThis approach ensures numerical stability and efficient computation across all input types, adhering to the specified return format."}
{"task_id": 224, "completion_id": 0, "completion": "To solve this problem, we need to implement the Leaky ReLU activation function, which is a common activation function used in neural networks to avoid the \"dying neuron\" problem. The Leaky ReLU allows a small, non-zero gradient when the input is negative. The task requires the function to handle various input types, including scalars, nested lists, and numpy arrays, and return the result as a Python list with the same structure as the input.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Check Input Type**: Determine if the input is a numpy array, a list (possibly nested), or a scalar.\n2. **Process Numpy Arrays**: Use vectorized operations for efficient computation on numpy arrays, then convert the result to a Python list.\n3. **Process Lists Recursively**: Handle nested lists by recursively applying the Leaky ReLU function to each element.\n4. **Handle Scalars Directly**: Apply the Leaky ReLU function directly to scalar values.\n\nThis approach ensures that the function works efficiently for numpy arrays using vectorized operations while correctly handling nested lists through recursion and scalars through direct computation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(element, a) for element in z]\n    else:\n        return z if z >= 0 else a * z\n```\n\n### Explanation\n- **Numpy Array Handling**: When the input is a numpy array, the function uses `np.where` to apply the Leaky ReLU element-wise. This is efficient and leverages numpy's optimized operations. The result is then converted to a Python list using `.tolist()`.\n- **List Handling**: If the input is a list (including nested lists), the function processes each element recursively. Each element is checked and processed according to its type, ensuring the nested structure is preserved.\n- **Scalar Handling**: For scalar values (integers or floats), the function directly applies the Leaky ReLU formula, returning the value itself if non-negative or `a` times the value if negative.\n\nThis solution efficiently handles all specified input types and ensures the output maintains the same structure as the input, converted to a Python list where necessary."}
{"task_id": 226, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\"\"\"\n    n_samples, n_features = X_train.shape\n    weights = np.ones(n_samples) / n_samples\n    ensemble = []\n\n    for _ in range(n_clf):\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n        best_error = np.inf\n\n        # Search for the best stump\n        for feature in range(n_features):\n            values = X_train[:, feature]\n            unique_values = np.unique(values)\n            if len(unique_values) < 2:\n                continue  # Not enough unique values to split\n\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n            for threshold in thresholds:\n                # Calculate errors for both polarities\n                pred_p1 = np.where(values <= threshold, -1, 1)\n                error_p1 = np.sum(weights * (pred_p1 != y_train))\n                pred_pm1 = np.where(values <= threshold, 1, -1)\n                error_pm1 = np.sum(weights * (pred_pm1 != y_train))\n\n                if error_p1 < error_pm1:\n                    current_error, current_polarity = error_p1, 1\n                else:\n                    current_error, current_polarity = error_pm1, -1\n\n                if current_error < best_error:\n                    best_error = current_error\n                    best_feature = feature\n                    best_threshold = threshold\n                    best_polarity = current_polarity\n\n        # Compute learner weight\n        error = best_error\n        alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n        # Update sample weights\n        pred = np.where(X_train[:, best_feature] <= best_threshold, -best_polarity, best_polarity)\n        misclassified = (pred != y_train)\n        weights *= np.exp(alpha * (2 * misclassified - 1))  # Increase if misclassified\n        weights /= np.sum(weights)  # Normalize\n\n        # Save the stump parameters\n        ensemble.append((alpha, best_feature, best_threshold, best_polarity))\n\n    # Predict on test data\n    y_pred = np.zeros(X_test.shape[0])\n    for alpha, feature, threshold, polarity in ensemble:\n        x_feature = X_test[:, feature]\n        stump_pred = np.where(x_feature <= threshold, -polarity, polarity)\n        y_pred += alpha * stump_pred\n\n    # Convert to final predictions\n    y_pred = np.sign(y_pred)\n    y_pred[y_pred == 0] = 1  # Handle zero predictions as 1\n    return y_pred.astype(int).tolist()\n```"}
{"task_id": 241, "completion_id": 0, "completion": "To solve this problem, we need to initialize a tensor with an orthogonal matrix using the algorithm described by Saxe et al. (2014). The orthogonal matrix helps in maintaining the stability of neural network activations during the initial stages of training.\n\n### Approach\nThe algorithm involves the following steps:\n1. **Check Input Validity**: Ensure the input shape has at least two dimensions. If not, return -1.\n2. **Reshape Input**: Convert the input shape into a 2D matrix where the first dimension is `d0` (the first element of the shape) and the second dimension is the product of the remaining elements.\n3. **Generate Random Matrix**: Create a matrix filled with samples from a standard normal distribution.\n4. **Compute SVD**: Perform Singular Value Decomposition (SVD) on the generated matrix to get orthogonal matrices U and V\u1d40.\n5. **Select Orthogonal Factor**: Choose either U or V\u1d40 based on which one matches the shape of the generated matrix.\n6. **Reshape and Scale**: Reshape the selected orthogonal factor back to the original input shape and scale it by the given factor.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n    \n    d0 = shape[0]\n    remaining_dims = int(np.prod(shape[1:]))\n    # Create the 2D matrix A with shape (d0, remaining_dims)\n    A = np.random.normal(size=(d0, remaining_dims))\n    \n    # Compute the SVD of A\n    U, S, Vh = np.linalg.svd(A, full_matrices=False)\n    \n    # Determine which factor to use (U or Vh)\n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = Vh\n    \n    # Reshape Q to the desired shape and apply scaling\n    tensor = Q.reshape(shape) * scale\n    return tensor.tolist()\n```\n\n### Explanation\n1. **Input Validity Check**: The function first checks if the input shape has at least two dimensions. If not, it returns -1 as specified.\n2. **Reshape Input**: The input shape is split into the first dimension `d0` and the product of the remaining dimensions to form a 2D matrix `A`.\n3. **Generate Random Matrix**: Using NumPy's `np.random.normal`, a matrix `A` of the specified shape is filled with random values from a standard normal distribution.\n4. **Compute SVD**: The SVD of matrix `A` is computed to obtain matrices U, S, and Vh. The `full_matrices=False` parameter ensures the decomposition is economical.\n5. **Select Orthogonal Factor**: Depending on whether U matches the shape of `A`, either U or Vh (transpose of V) is selected. This ensures the selected matrix maintains the orthogonality required.\n6. **Reshape and Scale**: The selected orthogonal matrix is reshaped back to the original input shape and scaled by the given factor to produce the final tensor.\n\nThis approach ensures that the resulting tensor meets the orthogonality condition required, either through the rows or columns being orthonormal up to the specified scaling factor."}
{"task_id": 243, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward pass of a small fully-connected Actor-Critic network with shared feature extractor layers and separate output heads for the actor and critic. The network uses fixed weights and biases, all set to 1.0 and 0.0 respectively, leading to deterministic outputs.\n\n### Approach\n1. **Input Processing**: The input state is a list of three floating-point numbers. We first compute the sum of these three elements.\n2. **Feature Extraction**:\n   - **Dense-1 Layer**: The sum of the state elements is passed through a ReLU activation function. If the sum is negative, it becomes zero; otherwise, it remains the same.\n   - **Dense-2 and Dense-3 Layers**: These layers process the output of the previous layer. Given the fixed weights and biases, each subsequent layer's output is a scaled version of the previous layer's output.\n3. **Actor Head**: The actor head processes the output from the last feature extraction layer. Since all weights are 1.0, the logits (pre-softmax values) are identical, leading to equal probabilities for each action when passed through a softmax activation.\n4. **Critic Head**: The critic head produces a single state value by linearly combining the outputs of the last feature extraction layer.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\"\"\"\n    sum_s = sum(state)\n    z1 = max(0.0, sum_s)\n    state_value = 64.0 * z1\n    action_probs = [round(1/3, 4)] * 3\n    return (action_probs, round(state_value, 4))\n```\n\n### Explanation\n1. **Sum Calculation**: The sum of the input state elements is computed to determine the initial feature extraction value.\n2. **ReLU Activation**: The sum is passed through a ReLU function to ensure non-negative output.\n3. **State Value Calculation**: The state value is derived by scaling the ReLU output by a factor of 64, which is a result of the fixed weights through the network layers.\n4. **Action Probabilities**: Since all logits are identical due to uniform weights, the softmax activation results in equal probabilities for each action, each rounded to four decimal places.\n\nThis approach efficiently computes the forward pass using the deterministic nature of the fixed weights and biases, ensuring accurate and efficient results."}
{"task_id": 249, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward pass of an Actor-Critic neural network using NumPy. The network consists of three shared fully-connected layers with ReLU activations, followed by separate actor and critic heads. The actor head outputs action probabilities using a softmax function, while the critic head outputs a scalar state value. The results are rounded to four decimal places, with special handling for exact 0 or 1 values.\n\n### Approach\n1. **Convert Input State**: Convert the input state from a list to a NumPy array for matrix operations.\n2. **Shared Layers Processing**: Apply three affine transformations followed by ReLU activations using the provided weights and biases.\n3. **Actor Head**: Compute action scores using the final hidden layer output, then apply softmax to get probabilities.\n4. **Critic Head**: Compute the state value using the same hidden layer output.\n5. **Rounding**: Round the probabilities and state value to four decimal places, converting exact 0.0 or 1.0 values to one decimal place.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\"\"\"\n    x = np.array(state, dtype=np.float32)\n    \n    # Shared layers processing\n    for layer in ['1', '2', '3']:\n        W = np.array(params[f'W{layer}'], dtype=np.float32)\n        b = np.array(params[f'b{layer}'], dtype=np.float32)\n        x = x @ W + b\n        x = np.maximum(x, 0)  # ReLU activation\n    \n    # Actor head\n    Wa = np.array(params['Wa'], dtype=np.float32)\n    ba = np.array(params['ba'], dtype=np.float32)\n    actor_scores = x @ Wa + ba\n    exp_scores = np.exp(actor_scores - np.max(actor_scores))  # Numerical stability\n    probabilities = exp_scores / exp_scores.sum()\n    prob_list = probabilities.tolist()\n    \n    # Critic head\n    Wc = np.array(params['Wc'], dtype=np.float32)\n    bc = np.array(params['bc'], dtype=np.float32)\n    critic_value = (x @ Wc + bc).item()\n    \n    # Rounding probabilities\n    rounded_probs = []\n    for p in prob_list:\n        p_rounded = round(p, 4)\n        if p_rounded in (0.0, 1.0):\n            rounded_probs.append(round(p_rounded, 1))\n        else:\n            rounded_probs.append(round(p_rounded, 4))\n    \n    # Rounding critic value\n    value_rounded = round(critic_value, 4)\n    if value_rounded in (0.0, 1.0):\n        value_final = round(value_rounded, 1)\n    else:\n        value_final = round(value_rounded, 4)\n    \n    return (rounded_probs, value_final)\n```\n\n### Explanation\n1. **Shared Layers**: The input state is processed through three dense layers with ReLU activations. Each layer's output is computed as a linear transformation followed by an element-wise ReLU activation.\n2. **Actor Head**: The final hidden layer output is transformed into action scores, which are then converted to probabilities using the softmax function to ensure they sum to 1.\n3. **Critic Head**: The same hidden layer output is used to compute the state value through a linear transformation.\n4. **Rounding**: Probabilities and the state value are rounded to four decimal places. If the result is exactly 0.0 or 1.0 after rounding, it is further rounded to one decimal place for consistency with the problem's requirements. This ensures the output meets the specified precision and formatting constraints."}
{"task_id": 253, "completion_id": 0, "completion": "To solve this problem, we need to compute either the Elastic-Net penalty value or its analytical gradient for a given weight vector. The Elastic-Net penalty is a combination of L1 and L2 regularization terms, which helps in reducing model complexity and preventing overfitting. The solution involves calculating the penalty or its gradient based on the provided parameters and returning the result rounded to four decimal places.\n\n### Approach\n1. **Problem Analysis**:\n   - The Elastic-Net penalty combines L1 and L2 regularization terms. The penalty is calculated using a convex combination of these terms, controlled by the L1 ratio (\u03bb).\n   - The gradient of the penalty is derived from the sum of the gradients of the L1 and L2 terms.\n\n2. **Key Insights**:\n   - The L1 term is the sum of the absolute values of the weight vector elements.\n   - The L2 term is half the sum of the squares of the weight vector elements.\n   - The gradient of the L1 term is the sign of each weight element, and the gradient of the L2 term is the weight element itself.\n\n3. **Algorithm Design**:\n   - **Penalty Calculation**:\n     - Convert the weight vector to a NumPy array for efficient computation.\n     - Compute the L1 and L2 terms using the given parameters.\n     - Combine the terms and multiply by the regularization strength (\u03b1) to get the penalty value.\n   - **Gradient Calculation**:\n     - Compute the gradient for each weight element by combining the gradients of the L1 and L2 terms.\n     - Round each element of the gradient vector to four decimal places and return as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w_np = np.asarray(w)\n    if gradient:\n        grad = alpha * (l1_ratio * np.sign(w_np) + (1 - l1_ratio) * w_np)\n        grad_rounded = np.round(grad, 4)\n        return grad_rounded.tolist()\n    else:\n        l1_term = l1_ratio * np.sum(np.abs(w_np))\n        l2_sq_term = 0.5 * (1 - l1_ratio) * np.sum(w_np ** 2)\n        penalty = alpha * (l1_term + l2_sq_term)\n        return round(penalty, 4)\n```\n\n### Explanation\n- **Penalty Calculation**:\n  - The penalty is computed by combining the L1 norm (sum of absolute values) and the squared L2 norm (sum of squares) of the weight vector. Each term is scaled by its respective coefficient (\u03bb and 1-\u03bb) and then combined with the overall regularization strength (\u03b1).\n\n- **Gradient Calculation**:\n  - The gradient is derived by taking the sign of each element for the L1 term and the element itself for the L2 term. These gradients are combined using the respective coefficients and scaled by \u03b1. Each element of the resulting gradient vector is rounded to four decimal places.\n\nThis approach ensures that the function efficiently computes either the penalty or its gradient based on the input parameters, providing accurate results rounded to the required precision."}
{"task_id": 256, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\"\"\"\n\n    if param_name in {\"X\", \"y\"}:\n        return None\n\n    # Handle Ba and Bx special cases\n    if param_name == \"Ba\":\n        key = \"ba\"\n    elif param_name == \"Bx\":\n        key = \"bx\"\n    else:\n        key = param_name\n\n    # Check if the key exists in the parameters (assuming model.parameters is accessible)\n    if key not in model.parameters:\n        return None  # Or raise an error, but problem statement implies existence\n\n    param = model.parameters[key]\n    original_param = param.copy()  # Save original parameter to restore later\n\n    # Convert the parameter to float64 for accurate computations\n    param_float64 = param.astype(np.float64)\n    model.parameters[key] = param_float64  # Temporarily replace with float64 version\n\n    # Initialize gradients array\n    grads = np.zeros_like(param_float64, dtype=np.float64)\n\n    # Iterate over each element of the parameter\n    for idx in np.ndindex(param_float64.shape):\n        original_val = param_float64[idx].item()  # Ensure scalar value\n\n        # Compute L(\u03b8_i + \u03b5)\n        param_float64[idx] = original_val + epsilon\n        model.flush_gradients()  # Reset any internal state/gradients\n        predictions_plus = []\n        for t in range(n_t):\n            X_t = X[:, :, t].astype(np.float64)  # Ensure input is float64\n            pred = model.forward(X_t)\n            predictions_plus.append(pred)\n        L_plus = loss_func(predictions_plus)\n\n        # Compute L(\u03b8_i - \u03b5)\n        param_float64[idx] = original_val - epsilon\n        model.flush_gradients()\n        predictions_minus = []\n        for t in range(n_t):\n            X_t = X[:, :, t].astype(np.float64)\n            pred = model.forward(X_t)\n            predictions_minus.append(pred)\n        L_minus = loss_func(predictions_minus)\n\n        # Restore original value\n        param_float64[idx] = original_val\n\n        # Centered difference\n        grad = (L_plus - L_minus) / (2 * epsilon)\n        grads[idx] = grad\n\n    # Restore the original parameter (dtype and value)\n    model.parameters[key] = original_param\n\n    return grads.T  # Return the transposed numerical gradient\n```"}
{"task_id": 257, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. AdaBoost works by iteratively training weak classifiers, adjusting the weights of misclassified samples, and combining the weak classifiers into a strong classifier. Each decision stump (one-level decision tree) is trained to minimize the weighted classification error, and the contribution of each stump is weighted based on its accuracy.\n\n### Approach\n1. **Initialize Weights**: Start with uniform weights for all training samples.\n2. **Iterate for Each Weak Classifier**: For each iteration, find the best decision stump (feature, threshold, and direction) that minimizes the weighted classification error.\n3. **Compute Classifier Weight**: Use the weighted error to compute the classifier's weight (alpha) using the formula: \n   \\[\n   \\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\varepsilon_t}{\\varepsilon_t}\\right)\n   \\]\n   where \\(\\varepsilon_t\\) is the weighted error of the stump.\n4. **Update Sample Weights**: Increase the weights of misclassified samples and decrease the weights of correctly classified samples, then normalize the weights.\n5. **Predict on Test Data**: Combine the predictions of all weak classifiers weighted by their alpha values to form the final prediction for each test sample.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\"\"\"\n    \n    m, n = X_train.shape\n    n_clf = max(n_clf, 1)\n    weights = np.ones(m) / m\n    stumps = []\n    \n    for _ in range(n_clf):\n        best_error = np.inf\n        best_feature = None\n        best_threshold = None\n        best_direction = None\n        \n        for j in range(n):\n            feature_values = X_train[:, j]\n            sorted_indices = np.argsort(feature_values)\n            sorted_feature = feature_values[sorted_indices]\n            sorted_y = y_train[sorted_indices]\n            sorted_weights = weights[sorted_indices]\n            \n            # Check thresholds between consecutive values\n            for i in range(len(sorted_feature) - 1):\n                th = (sorted_feature[i] + sorted_feature[i+1]) / 2\n                # Direction +1: left predicts -1, right predicts +1\n                error_plus = np.sum(sorted_weights[:i+1][sorted_y[:i+1] != -1]) + np.sum(sorted_weights[i+1:][sorted_y[i+1:] != 1])\n                # Direction -1: left predicts +1, right predicts -1\n                error_minus = np.sum(sorted_weights[:i+1][sorted_y[:i+1] != 1]) + np.sum(sorted_weights[i+1:][sorted_y[i+1:] != -1])\n                \n                current_error = min(error_plus, error_minus)\n                current_direction = 1 if error_plus < error_minus else -1\n                \n                if current_error < best_error:\n                    best_error = current_error\n                    best_feature = j\n                    best_threshold = th\n                    best_direction = current_direction\n            \n            # Check threshold before the first element\n            th = sorted_feature[0] - 1e-5\n            error_plus = np.sum(sorted_weights[sorted_y != 1])\n            error_minus = np.sum(sorted_weights[sorted_y != -1])\n            current_error = min(error_plus, error_minus)\n            current_direction = 1 if error_plus < error_minus else -1\n            if current_error < best_error:\n                best_error = current_error\n                best_feature = j\n                best_threshold = th\n                best_direction = current_direction\n            \n            # Check threshold after the last element\n            th = sorted_feature[-1] + 1e-5\n            error_plus = np.sum(sorted_weights[sorted_y != -1])\n            error_minus = np.sum(sorted_weights[sorted_y != 1])\n            current_error = min(error_plus, error_minus)\n            current_direction = 1 if error_plus < error_minus else -1\n            if current_error < best_error:\n                best_error = current_error\n                best_feature = j\n                best_threshold = th\n                best_direction = current_direction\n        \n        epsilon = max(best_error, 1e-10)\n        epsilon = min(epsilon, 1 - 1e-10)\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        \n        # Update weights\n        pred = np.where(X_train[:, best_feature] >= best_threshold, best_direction, -best_direction)\n        incorrect = (pred != y_train)\n        weights *= np.exp(alpha * incorrect)\n        weights /= np.sum(weights)\n        \n        stumps.append((best_feature, best_threshold, best_direction, alpha))\n    \n    # Predict on X_test\n    predictions = []\n    for x in X_test:\n        sum_alpha = 0.0\n        for j, th, d, a in stumps:\n            if x[j] >= th:\n                h = d\n            else:\n                h = -d\n            sum_alpha += a * h\n        predictions.append(1 if sum_alpha >= 0 else -1)\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: We start by initializing the weights of all training samples to be equal.\n2. **Weak Classifier Training**: For each weak classifier, we iterate over each feature and potential threshold to find the best split that minimizes the weighted error. This involves checking all possible thresholds and both directions (predicting +1 or -1 for values above the threshold).\n3. **Error Calculation**: For each potential split, we calculate the weighted error for both possible directions and select the one with the lower error.\n4. **Alpha Calculation**: The weight (alpha) of each classifier is computed based on its error, ensuring numerical stability by clipping error values.\n5. **Weight Update**: The weights of misclassified samples are increased, and the weights of correctly classified samples are decreased, followed by normalization.\n6. **Prediction**: For each test sample, the predictions of all weak classifiers are combined using their respective alpha values, and the final prediction is determined by the sign of the weighted sum.\n\nThis approach efficiently combines multiple weak classifiers to form a strong classifier, leveraging the iterative re-weighting mechanism of AdaBoost to focus on difficult samples in subsequent iterations."}
{"task_id": 261, "completion_id": 0, "completion": "To solve this problem, we need to implement the Glorot (Xavier) normal weight-initialization function for neural networks. This function initializes the weights of a neural network layer such that the variance of the activations is consistent across layers, which helps in faster convergence during training.\n\n### Approach\nThe Glorot normal initialization involves three main steps:\n1. **Calculate fan_in and fan_out**: These values depend on the dimensions of the input and output layers. For a 2-D tensor, `fan_in` and `fan_out` are the dimensions of the input and output layers respectively. For higher-dimensional tensors (e.g., convolutional layers), `fan_in` and `fan_out` are computed considering the product of the kernel dimensions (receptive field size) along with the input and output channels.\n2. **Compute the standard deviation**: Using the formula \\( s = \\sqrt{\\frac{2}{fan\\_in + fan\\_out}} \\), we determine the standard deviation for the normal distribution.\n3. **Generate the weight matrix**: Using NumPy's random number generator, we create a tensor with the specified shape, where each element is drawn from a normal distribution with mean 0 and the computed standard deviation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    if len(shape) == 2:\n        fan_in, fan_out = shape[0], shape[1]\n    else:\n        receptive_field_size = np.prod(shape[2:])\n        fan_in = shape[1] * receptive_field_size\n        fan_out = shape[0] * receptive_field_size\n    \n    std = np.sqrt(2.0 / (fan_in + fan_out))\n    return np.random.normal(0.0, std, shape)\n```\n\n### Explanation\n1. **Fan_in and Fan_out Calculation**:\n   - For a 2-D tensor (e.g., a fully-connected layer), `fan_in` is the number of input units, and `fan_out` is the number of output units.\n   - For higher-dimensional tensors (e.g., convolutional layers), the first two dimensions are the number of output and input channels, respectively. The product of the remaining dimensions (kernel size) gives the receptive field size. `fan_in` is the product of input channels and receptive field size, while `fan_out` is the product of output channels and receptive field size.\n\n2. **Standard Deviation Calculation**:\n   - The standard deviation is computed using the formula derived from the Glorot initialization theory, which balances the variance of the activations across layers.\n\n3. **Weight Matrix Generation**:\n   - Using NumPy's `random.normal` function, we generate a tensor with the specified shape, ensuring the elements are sampled from a normal distribution with the calculated standard deviation and mean 0.\n\nThis approach ensures that the initial weights are set in a way that promotes efficient training by maintaining consistent variance across network layers."}
{"task_id": 266, "completion_id": 0, "completion": "To solve this problem, we need to convert an undirected graph given by its vertex list `V` and edge list `E` into an adjacency list representation. The adjacency list must maintain the order of vertices as they appear in `V`, ensure no duplicate neighbors, and sort each neighbor list according to their appearance in `V`.\n\n### Approach\n1. **Vertex Set Creation**: Create a set of vertices from `V` for quick membership checks.\n2. **Adjacency Dictionary Initialization**: Initialize a dictionary where each vertex maps to a set of its adjacent vertices. This helps in efficiently avoiding duplicate edges.\n3. **Edge Processing**: For each edge in `E`, add each vertex to the other's adjacency set if both vertices are present in `V`.\n4. **Vertex Index Mapping**: Create a dictionary to map each vertex to its index in `V` for sorting purposes.\n5. **Result Construction**: For each vertex in `V`, convert its adjacency set to a list sorted by the vertices' indices in `V`.\n\n### Solution Code\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\"\"\"\n    vertices_set = set(V)\n    adj_dict = {v: set() for v in V}\n    \n    for u, v in E:\n        if u in vertices_set and v in vertices_set:\n            adj_dict[u].add(v)\n            adj_dict[v].add(u)\n    \n    vertex_to_index = {v: idx for idx, v in enumerate(V)}\n    result = []\n    \n    for v in V:\n        adj_vertices = list(adj_dict[v])\n        adj_vertices.sort(key=lambda x: vertex_to_index[x])\n        result.append(adj_vertices)\n    \n    return result\n```\n\n### Explanation\n1. **Vertex Set Creation**: Using a set of vertices from `V` allows O(1) time complexity for checking if a vertex is present in `V`.\n2. **Adjacency Dictionary**: Each vertex starts with an empty set to store its neighbors. This ensures that each neighbor is added exactly once.\n3. **Edge Processing**: For each edge `(u, v)`, both vertices are added to each other's adjacency sets if they exist in `V`. This handles the undirected nature of the graph.\n4. **Index Mapping**: A dictionary maps each vertex to its index in `V`, allowing efficient sorting of neighbors based on their order in `V`.\n5. **Result Construction**: Each adjacency set is converted to a list, sorted by the vertices' indices in `V`, ensuring the required order in the output.\n\nThis approach efficiently constructs the adjacency list while maintaining the required constraints and order, ensuring correctness and optimal performance."}
{"task_id": 267, "completion_id": 0, "completion": "To solve this problem, we need to find the optimal weighted decision stump for a binary classification task. A decision stump is a simple model that makes predictions based on a single feature and a threshold, with a polarity parameter that determines the direction of the comparison. The goal is to minimize the weighted classification error given the sample weights.\n\n### Approach\n1. **Iterate Over Each Feature**: For each feature in the dataset, we consider all unique values of that feature as potential thresholds.\n2. **Sort and Preprocess**: For each feature, sort the feature values along with their corresponding labels and weights. This allows efficient computation of errors for different thresholds.\n3. **Compute Cumulative Sums**: Precompute cumulative sums of errors for both possible polarities (1 and -1) to efficiently calculate the error for each threshold.\n4. **Evaluate Thresholds**: For each unique threshold value in the sorted feature values, compute the error for both polarities. Track the threshold, feature, and polarity that yield the minimum error.\n5. **Return Best Parameters**: After evaluating all features and thresholds, return the parameters of the decision stump with the lowest weighted error.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\"\"\"\n    best_error = np.inf\n    best_feature = 0\n    best_threshold = 0.0\n    best_polarity = 1\n\n    n_samples, n_features = X.shape\n\n    for j in range(n_features):\n        x_j = X[:, j]\n        sorted_indices = np.argsort(x_j)\n        sorted_x = x_j[sorted_indices]\n        sorted_y = y[sorted_indices]\n        sorted_weights = sample_weights[sorted_indices]\n\n        unique_thetas, split_indices = np.unique(sorted_x, return_index=True)\n\n        n = len(sorted_x)\n        if n == 0:\n            continue\n\n        mask_neq_1 = (sorted_y != 1)\n        mask_neq_minus1 = (sorted_y != -1)\n\n        contrib_neq_1 = mask_neq_1 * sorted_weights\n        contrib_neq_minus1 = mask_neq_minus1 * sorted_weights\n\n        cum_neq_1 = np.zeros(n + 1)\n        cum_neq_1[1:] = np.cumsum(contrib_neq_1)\n        cum_neq_minus1 = np.zeros(n + 1)\n        cum_neq_minus1[1:] = np.cumsum(contrib_neq_minus1)\n\n        total_neq_1 = cum_neq_1[-1]\n        total_neq_minus1 = cum_neq_minus1[-1]\n\n        for theta, split_idx in zip(unique_thetas, split_indices):\n            error_p1 = cum_neq_1[split_idx] + (total_neq_minus1 - cum_neq_minus1[split_idx])\n            error_pm1 = cum_neq_minus1[split_idx] + (total_neq_1 - cum_neq_1[split_idx])\n\n            if error_p1 < best_error:\n                best_error = error_p1\n                best_feature = j\n                best_threshold = theta\n                best_polarity = 1\n            if error_pm1 < best_error:\n                best_error = error_pm1\n                best_feature = j\n                best_threshold = theta\n                best_polarity = -1\n\n    best_threshold = round(best_threshold, 4)\n    best_error = round(best_error, 4)\n\n    return {\n        \"feature_index\": best_feature,\n        \"threshold\": best_threshold,\n        \"polarity\": best_polarity,\n        \"weighted_error\": best_error\n    }\n```\n\n### Explanation\n1. **Feature Iteration**: The algorithm iterates over each feature in the dataset to evaluate all possible decision stumps based on that feature.\n2. **Sorting and Unique Thresholds**: For each feature, the data is sorted, and unique values are extracted as potential thresholds. This ensures that all possible split points are considered.\n3. **Cumulative Sums**: Cumulative sums of errors for both polarities are precomputed. This allows efficient calculation of the error for each threshold by leveraging these precomputed sums.\n4. **Error Calculation**: For each threshold, the error is calculated for both polarities (1 and -1) by considering the cumulative sums of misclassified samples. The algorithm keeps track of the threshold, feature, and polarity that result in the minimum error.\n5. **Result**: The best parameters (feature, threshold, polarity) and the minimal error are returned, with values rounded to four decimal places as required.\n\nThis approach ensures that we efficiently evaluate all possible decision stumps while minimizing the weighted classification error, making it suitable for integration into boosting algorithms like AdaBoost."}
{"task_id": 273, "completion_id": 0, "completion": "To solve this problem, we need to determine the optimal numerical threshold for splitting a decision tree based on information gain. The goal is to find the threshold that maximizes the information gain when splitting the dataset into left and right subsets.\n\n### Approach\n1. **Sort the Features and Targets**: Combine the feature and target values into pairs and sort them by the feature values. This helps in efficiently generating candidate thresholds and splitting the dataset.\n2. **Generate Candidate Thresholds**: Identify midpoints between consecutive distinct values in the sorted feature list. These midpoints are the candidate thresholds for splitting the data.\n3. **Compute Parent Entropy**: Calculate the entropy of the entire target set, which serves as the baseline entropy before any split.\n4. **Evaluate Each Threshold**: For each candidate threshold, split the dataset into left and right subsets, compute their entropies, and calculate the information gain.\n5. **Select the Best Threshold**: Choose the threshold that maximizes the information gain. If multiple thresholds yield the same gain, select the smallest threshold. If no threshold provides a positive gain, return `(None, 0.0)`.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\nimport bisect\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\"\"\"\n    # Sort the features and corresponding targets\n    sorted_pairs = sorted(zip(feature, target), key=lambda x: x[0])\n    sorted_features = [x[0] for x in sorted_pairs]\n    sorted_targets = [x[1] for x in sorted_pairs]\n    \n    # Generate candidate thresholds between consecutive distinct sorted features\n    unique_sorted = []\n    prev = None\n    for val in sorted_features:\n        if val != prev:\n            unique_sorted.append(val)\n            prev = val\n    candidate_thresholds = []\n    for i in range(len(unique_sorted) - 1):\n        mid = (unique_sorted[i] + unique_sorted[i+1]) / 2\n        candidate_thresholds.append(mid)\n    \n    # Check if there are no legal thresholds\n    if not candidate_thresholds:\n        return (None, 0.0)\n    \n    # Compute parent entropy\n    parent_counts = Counter(sorted_targets)\n    total_parent = len(sorted_targets)\n    parent_entropy = 0.0\n    for count in parent_counts.values():\n        p = count / total_parent\n        if p > 0:\n            parent_entropy -= p * math.log2(p)\n    \n    # If parent entropy is zero, no split will improve\n    if parent_entropy == 0.0:\n        return (None, 0.0)\n    \n    max_gain = -float('inf')\n    best_threshold = None\n    \n    # Evaluate each candidate threshold\n    for t in candidate_thresholds:\n        split_idx = bisect.bisect_left(sorted_features, t)\n        left_targets = sorted_targets[:split_idx]\n        right_targets = sorted_targets[split_idx:]\n        \n        # Calculate left entropy\n        left_total = len(left_targets)\n        left_entropy = 0.0\n        if left_total > 0:\n            left_counts = Counter(left_targets)\n            for count in left_counts.values():\n                p = count / left_total\n                if p > 0:\n                    left_entropy -= p * math.log2(p)\n        \n        # Calculate right entropy\n        right_total = len(right_targets)\n        right_entropy = 0.0\n        if right_total > 0:\n            right_counts = Counter(right_targets)\n            for count in right_counts.values():\n                p = count / right_total\n                if p > 0:\n                    right_entropy -= p * math.log2(p)\n        \n        # Compute information gain\n        n = total_parent\n        weighted_avg = (left_total / n) * left_entropy + (right_total / n) * right_entropy\n        gain = parent_entropy - weighted_avg\n        \n        # Update best threshold based on maximum gain and smallest threshold in case of ties\n        if gain > max_gain or (gain == max_gain and t < best_threshold):\n            max_gain = gain\n            best_threshold = t\n    \n    # Check if maximum gain is positive\n    if max_gain <= 0.0:\n        return (None, 0.0)\n    else:\n        return (round(best_threshold, 4), round(max_gain, 4))\n```\n\n### Explanation\n1. **Sorting and Generating Thresholds**: The feature and target values are sorted together to maintain correspondence. Candidate thresholds are generated as midpoints between consecutive distinct values in the sorted feature list.\n2. **Entropy Calculation**: Entropy is computed using the Shannon entropy formula, which measures the impurity of the target labels. The entropy of the parent node (entire dataset) is calculated once, and then the entropies of the left and right subsets are computed for each threshold.\n3. **Information Gain**: For each threshold, the information gain is calculated by subtracting the weighted average entropy of the left and right subsets from the parent entropy. The threshold with the highest gain is selected, with ties resolved by choosing the smallest threshold.\n4. **Edge Cases**: The solution handles cases where all feature values are identical or no split provides a positive information gain by returning `(None, 0.0)`.\n\nThis approach efficiently evaluates all possible splits and selects the optimal threshold, ensuring the decision tree split maximizes the information gain."}
{"task_id": 286, "completion_id": 0, "completion": "To solve this problem, we need to create a utility function that retrieves a weight initializer function based on a given name. The utility should check if the name corresponds to a callable function in the global namespace and return it, or raise an error if it does not exist or is not callable.\n\n### Approach\n1. **Retrieve Global Symbols**: Use Python's `globals()` function to get all global symbols in the current module.\n2. **Check Existence and Callability**: Verify if the given name exists in the global symbols and if the corresponding object is callable.\n3. **Return or Raise Error**: Return the callable if valid; otherwise, raise a `ValueError` with a specific message.\n\n### Solution Code\n```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[1.0 for _ in range(cols)] for _ in range(rows)]\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    random.seed(seed)\n    rows, cols = shape\n    return [[random.uniform(low, high) for _ in range(cols)] for _ in range(rows)]\n\ndef get_initializer(name: str):\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    global_symbols = globals()\n    initializer = global_symbols.get(name)\n    if initializer is not None and callable(initializer):\n        return initializer\n    raise ValueError(\"Invalid initialization function.\")\n```\n\n### Explanation\n- **zeros_init**: This function takes a shape tuple and returns a matrix filled with zeros.\n- **ones_init**: Similar to `zeros_init`, but fills the matrix with ones.\n- **random_uniform_init**: Generates a matrix with uniformly distributed random numbers within a specified range, using a seed for reproducibility.\n- **get_initializer**: This utility function checks the global namespace for a callable object with the given name. If found, it returns the callable; otherwise, it raises a `ValueError`.\n\nThis approach ensures that we dynamically retrieve the correct initializer function based on the provided name, adhering to the specified requirements and handling invalid cases appropriately."}
{"task_id": 287, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    s_h, s_w = stride\n    \n    # Compute output dimensions\n    H_out = (H - p_h) // s_h + 1\n    W_out = (W - p_w) // s_w + 1\n    \n    # Forward pass: create windowed view and compute mean\n    X_strides = X.strides\n    new_strides = (X_strides[0], X_strides[1], X_strides[2] * s_h, X_strides[3] * s_w, X_strides[2], X_strides[3])\n    X_windows = as_strided(X, shape=(N, C, H_out, W_out, p_h, p_w), strides=new_strides)\n    pooled_output = np.mean(X_windows, axis=(-2, -1))\n    \n    # Backward pass: distribute gradients to input windows\n    grad_input = np.zeros_like(X)\n    grad_windows = as_strided(grad_input, shape=(N, C, H_out, W_out, p_h, p_w), strides=new_strides)\n    scaled_grad = accum_grad[..., np.newaxis, np.newaxis] / (p_h * p_w)\n    np.add(grad_windows, scaled_grad, out=grad_windows)\n    \n    # Round and convert to lists\n    pooled_output_rounded = np.round(pooled_output, 4)\n    grad_input_rounded = np.round(grad_input, 4)\n    \n    return (pooled_output_rounded.tolist(), grad_input_rounded.tolist())\n```"}
{"task_id": 290, "completion_id": 0, "completion": "To solve this problem, we need to determine if two binary decision trees are equivalent based on their structure and values. The equivalence criteria include having the same shape, matching feature indices at internal nodes, and thresholds and leaf values being numerically close within a specified tolerance.\n\n### Approach\nThe solution involves a recursive comparison of the two trees. The key steps are as follows:\n1. **Check Node Types**: If one node is a `Leaf` and the other is a `Node`, they are not equivalent.\n2. **Leaf Comparison**: If both nodes are `Leaf` nodes, compare their values using `numpy.allclose` with an absolute tolerance of 1e-8.\n3. **Node Comparison**: If both nodes are `Node` types, check if their feature indices and thresholds match. The thresholds must be within the specified tolerance using `numpy.isclose`.\n4. **Recursive Check**: Recursively compare the left and right children of the nodes to ensure the entire subtree structure and values match.\n\n### Solution Code\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    value: object\n\n@dataclass\nclass Node:\n    feature: int\n    threshold: float\n    left: object\n    right: object\n\ndef compare_trees(tree_a, tree_b):\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-8, rtol=0.0)\n    elif isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8, rtol=0.0):\n            return False\n        return compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    else:\n        return False\n```\n\n### Explanation\n- **Node Type Check**: The function first checks if both nodes are either `Leaf` or `Node` instances. If there is a mismatch in types, the trees are not equivalent.\n- **Leaf Value Comparison**: For `Leaf` nodes, their values are compared using `numpy.allclose`, which handles various data types (int, float, list, numpy array) and ensures numerical closeness within the specified tolerance.\n- **Node Feature and Threshold Check**: For `Node` instances, the feature indices must be identical, and thresholds must be within 1e-8 absolute tolerance using `numpy.isclose`.\n- **Recursive Subtree Check**: The function recursively checks the left and right subtrees to ensure structural and value equivalence throughout the entire tree.\n\nThis approach ensures that both the structure and values of the trees are thoroughly checked, adhering to the problem constraints and requirements."}
{"task_id": 292, "completion_id": 0, "completion": "To solve this problem, we need to perform a single-point column crossover on two parent weight matrices to generate two offspring matrices. This crossover involves swapping columns from a specified cutoff point onwards between the two parents. The solution must validate the input matrices to ensure they have the same shape and then efficiently perform the column swap using NumPy for optimal performance.\n\n### Approach\n1. **Validation**: Check if both parent matrices have the same shape. This involves ensuring they have the same number of rows and columns. If they do not, return -1.\n2. **Edge Case Handling**: If the matrices are empty, return empty matrices as children.\n3. **Conversion to NumPy Arrays**: Convert the parent matrices to NumPy arrays for efficient slicing and manipulation.\n4. **Column Crossover**: Use NumPy slicing to create the children matrices by combining columns from both parents based on the specified cutoff point.\n5. **Conversion to Python Lists**: Convert the resulting NumPy arrays back to nested Python lists before returning them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                           parent2: list[list[int | float]],\n                           cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\"\"\"\n    # Check if both parents have the same number of rows\n    parent1_rows = len(parent1)\n    parent2_rows = len(parent2)\n    if parent1_rows != parent2_rows:\n        return -1\n    \n    # Handle empty matrices\n    if parent1_rows == 0:\n        return ([], [])\n    \n    # Check all rows in parent1 have the same number of columns\n    parent1_cols = len(parent1[0])\n    for row in parent1:\n        if len(row) != parent1_cols:\n            return -1\n    \n    # Check all rows in parent2 have the same number of columns\n    parent2_cols = len(parent2[0])\n    for row in parent2:\n        if len(row) != parent2_cols:\n            return -1\n    \n    # Check if the number of columns matches between parents\n    if parent1_cols != parent2_cols:\n        return -1\n    \n    # Convert parents to numpy arrays\n    np_parent1 = np.array(parent1)\n    np_parent2 = np.array(parent2)\n    \n    # Perform the crossover\n    child1 = np.hstack((np_parent1[:, :cutoff], np_parent2[:, cutoff:]))\n    child2 = np.hstack((np_parent2[:, :cutoff], np_parent1[:, cutoff:]))\n    \n    # Convert back to Python lists and return\n    return (child1.tolist(), child2.tolist())\n```\n\n### Explanation\n1. **Validation**: The function first checks if the number of rows in both parent matrices is the same. If not, it returns -1. It then checks if the matrices are non-empty and if each row within each matrix has the same number of columns. If any of these checks fail, it returns -1.\n2. **Edge Case Handling**: If both matrices are empty, the function immediately returns empty matrices as children.\n3. **Conversion to NumPy Arrays**: The parent matrices are converted to NumPy arrays to leverage efficient array operations.\n4. **Column Crossover**: Using NumPy's `hstack` function, the columns from both parents are combined based on the cutoff point. Columns before the cutoff are taken from the original parent, and columns from the cutoff onwards are taken from the other parent.\n5. **Conversion to Python Lists**: The resulting NumPy arrays are converted back to nested Python lists using the `tolist()` method to meet the output requirements.\n\nThis approach ensures that the solution is both efficient and correct, handling all edge cases and validating inputs appropriately."}
{"task_id": 294, "completion_id": 0, "completion": "To solve this problem, we need to convert a custom graph representation into an adjacency dictionary. The adjacency dictionary must meet specific requirements, especially ensuring that undirected edges are represented in both directions and that neighbor lists are sorted lexicographically by neighbor labels.\n\n### Approach\n1. **Initialize Adjacency Dictionary**: Create an empty dictionary where each key is a vertex label from the input graph, initialized with an empty list to store edges.\n2. **Process Edges**: Iterate through each edge in the input graph. For each edge, add it to the adjacency list of the source vertex. If the graph is undirected, also add the reverse edge to ensure both directions are present.\n3. **Sort Neighbor Lists**: After processing all edges, sort each vertex's list of neighbors lexicographically by the neighbor's label to ensure deterministic output.\n\n### Solution Code\n```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\"\"\"\n    adj = {}\n    v2i = G['_V2I']\n    # Initialize adjacency dict with all vertices\n    for vertex in v2i:\n        adj[vertex] = []\n    # Process each edge in the graph\n    for edges in G['_G']:\n        for edge in edges:\n            s, t, w = edge\n            adj[s].append((t, w))\n            # Add reverse edge for undirected graphs\n            if not G['is_directed']:\n                adj[t].append((s, w))\n    # Sort each adjacency list lexicographically by neighbor label\n    for u in adj:\n        adj[u].sort(key=lambda x: x[0])\n    return adj\n```\n\n### Explanation\n1. **Initialization**: The adjacency dictionary `adj` is initialized with all vertex labels from `G['_V2I']`, each mapped to an empty list. This ensures that even vertices with no edges are present in the result.\n2. **Edge Processing**: Each edge from the input graph's `_G` list is processed. For directed graphs, edges are added as-is. For undirected graphs, each edge is added in both directions (once from source to target and once from target to source).\n3. **Sorting**: After all edges are processed, each vertex's list of neighbors is sorted lexicographically by neighbor labels to ensure the output is deterministic and meets the problem's requirements.\n\nThis approach efficiently converts the custom graph structure into the required adjacency dictionary, handling both directed and undirected graphs appropriately while ensuring the output format is correct."}
{"task_id": 296, "completion_id": 0, "completion": "To solve this problem, we need to verify if all elements in a given NumPy array are binary, meaning each element is either 0 or 1. The solution should handle arrays of any shape and data type, including integers, floats, and booleans.\n\n### Approach\nThe approach involves the following steps:\n1. **Convert Input to NumPy Array**: Ensure the input is converted to a NumPy array to handle array-like inputs uniformly.\n2. **Create a Boolean Mask**: Generate a boolean mask where each element is `True` if it is either 0 or 1, and `False` otherwise. This mask is created by checking each element against 0 and 1 using element-wise comparison.\n3. **Check All Elements**: Use NumPy's `np.all` function to verify if all elements in the boolean mask are `True`. This function efficiently checks the entire array for the condition.\n4. **Return Result as Python Boolean**: Convert the result from a NumPy boolean type to a Python built-in boolean type to meet the output constraints.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    x = np.asarray(x)\n    return bool(np.all((x == 0) | (x == 1)))\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `x` is converted to a NumPy array using `np.asarray(x)`. This ensures that even if the input is a list or another array-like structure, it is handled as a NumPy array.\n2. **Boolean Mask Creation**: The mask `(x == 0) | (x == 1)` checks each element in the array to see if it is either 0 or 1. This works for all numeric types and booleans, as boolean values `True` and `False` are treated as 1 and 0, respectively.\n3. **Check All Elements**: The `np.all` function checks if every element in the boolean mask is `True`, indicating all elements in the original array are 0 or 1.\n4. **Result Conversion**: The result from `np.all` is converted to a Python boolean using `bool()` to ensure the return type is a built-in boolean, as required.\n\nThis approach efficiently handles all specified data types and array shapes, ensuring correctness and adherence to the problem constraints."}
{"task_id": 298, "completion_id": 0, "completion": "To solve this problem, we need to calculate the total log-probability of a query sentence using a Maximum-Likelihood-Estimation (MLE) N-gram language model trained on a given corpus. The solution involves processing both the corpus and the query to build the model, then using it to compute the probabilities of the query's N-grams.\n\n### Approach\n\n1. **Corpus Processing**:\n   - **Tokenization**: Split each sentence into tokens based on whitespace.\n   - **Padding**: For N-grams where N > 1, prepend each token sequence with N-1 special tokens `<bol>` and append a `<eol>` token. No padding is needed for unigrams (N=1).\n   - **N-gram and Prefix Counting**: For each sentence in the corpus, generate all N-grams and their (N-1)-gram prefixes. Count the occurrences of these N-grams and prefixes to build the model.\n\n2. **Query Processing**:\n   - **Tokenization and Padding**: Similar to the corpus processing, split the query into tokens and pad it appropriately based on the value of N.\n   - **N-gram Generation**: Generate all possible N-grams from the padded query tokens.\n   - **Probability Calculation**: For each N-gram in the query, compute its probability using the counts from the corpus. If any N-gram or its prefix is unseen (count is zero), return negative infinity. Otherwise, sum the natural logarithms of these probabilities and round the result to four decimal places.\n\n### Solution Code\n\n```python\nimport math\nfrom collections import Counter\nfrom typing import List\n\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\"\"\"\n    if N < 1:\n        raise ValueError(\"N must be \u2265 1\")\n\n    # Process the corpus to build the model\n    if N == 1:\n        unigram_counts = Counter()\n        for sentence in corpus:\n            tokens = sentence.split()\n            unigram_counts.update(tokens)\n        total_tokens = sum(unigram_counts.values())\n    else:\n        ngram_counts = Counter()\n        prefix_counts = Counter()\n        for sentence in corpus:\n            original_tokens = sentence.split()\n            padded_tokens = ['<bol>'] * (N - 1) + original_tokens + ['<eol>']\n            # Generate all N-grams for the current sentence\n            for i in range(len(padded_tokens) - N + 1):\n                ngram = tuple(padded_tokens[i:i + N])\n                ngram_counts[ngram] += 1\n            # Generate all (N-1)-grams for the current sentence\n            for i in range(len(padded_tokens) - (N - 1) + 1):\n                prefix = tuple(padded_tokens[i:i + (N - 1)])\n                prefix_counts[prefix] += 1\n\n    # Process the query\n    query_tokens = query.split()\n    if N > 1:\n        padded_query = ['<bol>'] * (N - 1) + query_tokens + ['<eol>']\n    else:\n        padded_query = query_tokens\n\n    # Check if the padded query is long enough\n    if len(padded_query) < N:\n        return float('-inf')\n\n    # Generate all N-grams from the padded query\n    query_ngrams = [tuple(padded_query[i:i + N]) for i in range(len(padded_query) - N + 1)]\n\n    log_sum = 0.0\n    for ngram in query_ngrams:\n        if N == 1:\n            unigram = ngram[0]\n            count = unigram_counts.get(unigram, 0)\n            if count == 0 or total_tokens == 0:\n                return float('-inf')\n            prob = count / total_tokens\n        else:\n            # Check if the ngram exists\n            count_ngram = ngram_counts.get(ngram, 0)\n            if count_ngram == 0:\n                return float('-inf')\n            # Check the prefix\n            prefix = ngram[:-1]\n            count_prefix = prefix_counts.get(prefix, 0)\n            if count_prefix == 0:\n                return float('-inf')\n            prob = count_ngram / count_prefix\n        log_sum += math.log(prob)\n\n    return round(log_sum, 4)\n```\n\n### Explanation\n\n- **Corpus Processing**: The corpus is processed to count N-grams and their prefixes. For N=1, we simply count the occurrences of each token. For N>1, we pad each sentence, then count each N-gram and its (N-1)-gram prefix.\n- **Query Processing**: The query is tokenized and padded similarly. We check if the padded query is long enough to form at least one N-gram. If not, we return negative infinity.\n- **Probability Calculation**: For each N-gram in the query, we check if it and its prefix exist in the corpus counts. If either is missing, we return negative infinity. Otherwise, we compute the probability and accumulate the log-probability sum, which is then rounded to four decimal places.\n\nThis approach ensures that we accurately model the N-gram probabilities and handle edge cases such as unseen N-grams or insufficient query length appropriately."}
{"task_id": 302, "completion_id": 0, "completion": "To solve this problem, we need to implement the Spectral Clustering algorithm from scratch. Spectral clustering is a technique that uses the eigenvalues of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The steps involved are constructing a similarity matrix, computing the graph Laplacian, finding eigenvalues and eigenvectors, and then applying k-means clustering on the eigenvectors.\n\n### Approach\n1. **Construct the Weighted Adjacency Matrix (W):** Compute the pairwise distances between data points and convert these distances into weights using the formula \\( w_{ij} = 1 / (1 + \\text{Euclidean distance between } x_i \\text{ and } x_j) \\). The diagonal elements of this matrix are set to zero to avoid self-connections.\n\n2. **Compute the Graph Laplacian (L):** The Laplacian matrix is computed as \\( L = D - W \\), where \\( D \\) is the diagonal degree matrix whose elements are the sum of the weights for each node.\n\n3. **Eigenvalue Decomposition:** Compute the eigenvalues and eigenvectors of the Laplacian matrix \\( L \\). The first \\( k \\) eigenvectors corresponding to the smallest eigenvalues are selected to form a matrix \\( E \\).\n\n4. **K-means Clustering:** Apply the k-means clustering algorithm on the rows of the matrix \\( E \\). The initial centroids for k-means are the first \\( k \\) rows of \\( E \\). If a cluster becomes empty during iterations, its centroid is reinitialized with a random row from \\( E \\).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n = X.shape[0]\n    \n    # Step 1: Build the weighted adjacency matrix W\n    diff = X[:, np.newaxis, :] - X[np.newaxis, :, :]\n    distance_matrix = np.sqrt((diff ** 2).sum(axis=2))\n    W = 1 / (1 + distance_matrix)\n    np.fill_diagonal(W, 0)\n    \n    # Step 2: Compute the unnormalized graph Laplacian L = D - W\n    D = np.diag(W.sum(axis=1))\n    L = D - W\n    \n    # Step 3: Compute the first k eigenvectors of L\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    E = eigenvectors[:, :k]\n    \n    # Step 4: Run k-means on the rows of E\n    centroids = E[:k, :].copy()\n    prev_labels = None\n    \n    for _ in range(100):\n        # Compute distances from each point to each centroid\n        distances = np.sqrt(((E[:, np.newaxis, :] - centroids) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=1)\n        \n        # Check for convergence\n        if prev_labels is not None and np.array_equal(labels, prev_labels):\n            break\n        prev_labels = labels.copy()\n        \n        # Update centroids\n        new_centroids = np.zeros_like(centroids)\n        counts = np.zeros(k, dtype=int)\n        \n        # Sum all points in each cluster\n        for i in range(n):\n            cluster = labels[i]\n            new_centroids[cluster] += E[i]\n            counts[cluster] += 1\n        \n        # Handle empty clusters and compute new centroids\n        for c in range(k):\n            if counts[c] == 0:\n                # Re-initialize with a random row from E\n                random_idx = np.random.randint(n)\n                new_centroids[c] = E[random_idx]\n            else:\n                new_centroids[c] = new_centroids[c] / counts[c]\n        \n        centroids = new_centroids\n    \n    return labels.tolist()\n```\n\n### Explanation\n1. **Weighted Adjacency Matrix (W):** This matrix captures the similarity between each pair of data points using the inverse of the Euclidean distance. The diagonal elements are zero to prevent self-connections.\n\n2. **Graph Laplacian (L):** The Laplacian matrix is derived from the degree matrix \\( D \\) (sum of weights for each node) and the adjacency matrix \\( W \\). It helps in studying the properties of the graph.\n\n3. **Eigenvalue Decomposition:** The eigenvectors corresponding to the smallest eigenvalues of the Laplacian matrix provide a low-dimensional embedding of the data, which is easier to cluster.\n\n4. **K-means Clustering:** The low-dimensional embedding from the eigenvectors is clustered using k-means. The initial centroids are deterministic (first \\( k \\) rows of the eigenvector matrix), and empty clusters are reinitialized with random points to avoid convergence issues.\n\nThis approach efficiently combines spectral techniques with traditional clustering to handle non-convex data distributions, which are challenging for standard k-means."}
{"task_id": 303, "completion_id": 0, "completion": "To solve this problem, we need to generate a formatted debugging string that compares the current and previous outputs of a program with their corresponding gold-standard outputs. The output must follow a specific structure with precise formatting, including dashes, line breaks, and labels.\n\n### Approach\n1. **Determine Previous and Current Indices**: Calculate the previous index as `max(ix-1, 0)` to handle the case where `ix` is 0. This ensures we always have valid indices for both previous and current elements.\n2. **Extract Values**: Retrieve the previous and current outputs (both program-generated and gold-standard) using their respective indices and labels.\n3. **Construct the Debug String**: Build the debug string using the specified format, including the required number of dashes, labels, and outputs. Append any provided warning message directly to the current gold-standard output.\n\n### Solution Code\n```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    prev_index = max(ix - 1, 0)\n    prev_mine, prev_label = params[prev_index]\n    curr_mine, curr_label = params[ix]\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    \n    debug_lines = [\n        '-' * 25 + ' DEBUG ' + '-' * 25,\n        f\"Mine (prev) [{prev_label}]:\",\n        prev_mine,\n        f\"Theirs (prev) [{prev_label}]:\",\n        prev_gold,\n        f\"Mine [{curr_label}]:\",\n        curr_mine,\n        f\"Theirs [{curr_label}]:\",\n        curr_gold + warn_str,\n        '-' * 23 + ' END DEBUG ' + '-' * 23\n    ]\n    return '\\n'.join(debug_lines)\n```\n\n### Explanation\n- **Previous Index Calculation**: The previous index is determined to ensure it is within valid bounds. If `ix` is 0, the previous index is also 0.\n- **Extracting Values**: The previous and current outputs (both program-generated and gold-standard) are extracted using their respective indices and labels from the input parameters.\n- **String Construction**: The debug string is constructed line by line, adhering to the specified format. Each section (previous and current outputs) is formatted with appropriate labels and values. The warning message is appended directly to the current gold-standard output to maintain the exact structure required.\n\nThis approach ensures that the output is correctly formatted with the required number of dashes, line breaks, and labels, providing a clear and concise comparison for debugging purposes."}
{"task_id": 304, "completion_id": 0, "completion": "To solve this problem, we need to compute the posterior mean (Maximum-A-Posteriori estimate) of the regression coefficients in Bayesian linear regression with an unknown noise variance. The model uses a normal-inverse-gamma conjugate prior on the coefficients and noise variance. The key steps involve handling the prior parameters and computing the posterior mean using closed-form formulas.\n\n### Approach\n1. **Add Intercept**: If `fit_intercept` is True, prepend a column of ones to the design matrix X to account for the bias term.\n2. **Process Prior Mean (mu)**: Convert scalar input to a vector of appropriate length. Ensure the vector length matches the number of features (including the intercept if added).\n3. **Process Prior Scale (V)**: Convert different input forms (scalar, list, matrix) into a diagonal or full matrix as required. Compute the inverse of this matrix.\n4. **Compute Posterior Parameters**: Use the closed-form formulas to compute the posterior covariance (up to noise variance) and mean of the coefficients. The posterior mean is derived using matrix operations involving the prior parameters and the data.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu=0.0,\n    V=None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility.\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    # Add intercept column if required\n    if fit_intercept:\n        X_augmented = np.hstack([np.ones((X.shape[0], 1), dtype=X.dtype), X])\n    else:\n        X_augmented = X.copy()\n    \n    n_samples, n_features = X_augmented.shape\n    \n    # Process mu to ensure it's a vector of correct length\n    if np.isscalar(mu):\n        mu_prior = np.full(n_features, mu, dtype=np.float64)\n    else:\n        mu_prior = np.asarray(mu, dtype=np.float64)\n        if mu_prior.shape != (n_features,):\n            raise ValueError(\n                \"mu must be a scalar or have length equal to the number of features.\"\n            )\n    \n    # Process V to create the prior covariance matrix and compute its inverse\n    if V is None:\n        V_prior = np.eye(n_features)\n    elif np.isscalar(V):\n        V_prior = V * np.eye(n_features)\n    elif isinstance(V, (list, tuple)):\n        if len(V) != n_features:\n            raise ValueError(\n                \"V as list/tuple must have length equal to the number of features.\"\n            )\n        V_prior = np.diag(V)\n    elif isinstance(V, np.ndarray):\n        if V.ndim == 1:\n            if V.size != n_features:\n                raise ValueError(\n                    \"V as 1D array must have length equal to the number of features.\"\n                )\n            V_prior = np.diag(V)\n        elif V.ndim == 2:\n            if V.shape != (n_features, n_features):\n                raise ValueError(\n                    \"V as 2D array must be a square matrix matching feature count.\"\n                )\n            V_prior = V\n        else:\n            raise ValueError(\"V must be 1D or 2D numpy array.\")\n    else:\n        raise TypeError(\"V must be None, scalar, list/tuple, or numpy array.\")\n    \n    V_inv = np.linalg.inv(V_prior)\n    \n    # Compute components for the posterior mean\n    XTX = X_augmented.T @ X_augmented\n    XTy = X_augmented.T @ y\n    Sigma_b = np.linalg.inv(V_inv + XTX)\n    mu_b = Sigma_b @ (V_inv @ mu_prior + XTy)\n    \n    # Round to four decimal places and return as list\n    return [round(coef, 4) for coef in mu_b.tolist()]\n```\n\n### Explanation\n1. **Intercept Handling**: The code checks if an intercept should be added and modifies the design matrix accordingly.\n2. **Prior Mean (mu)**: Converts scalar inputs to vectors and ensures existing vectors match the feature count.\n3. **Prior Scale (V)**: Converts different input forms into a matrix, handling scalars as identity matrices scaled by the scalar, lists as diagonal matrices, and checks the dimensions for 2D arrays.\n4. **Matrix Operations**: Computes the posterior covariance and mean using matrix inversions and multiplications as per the derived formulas, ensuring numerical stability and efficiency.\n5. **Result Formatting**: Rounds the results to four decimal places and returns them as a list for consistency with the problem requirements."}
{"task_id": 308, "completion_id": 0, "completion": "To solve this problem, we need to construct a Mel filterbank matrix that converts a power spectrum vector into Mel band energies using triangular filters defined on the Mel scale. The Mel scale is a perceptual scale that emphasizes lower frequencies more than higher frequencies, which is crucial in speech and audio processing.\n\n### Approach\n1. **Convert Frequency Limits to Mel Scale**: Convert the given minimum and maximum frequencies (in Hertz) to the Mel scale using the formula `mel = 2595 * log10(1 + f/700)`.\n2. **Generate Equally Spaced Mel Points**: Generate `n_filters + 2` points equally spaced in the Mel scale between the converted minimum and maximum Mel values.\n3. **Convert Mel Points Back to Hertz**: Convert these Mel points back to Hertz using the inverse formula `f = 700 * (10^(mel/2595) - 1)`.\n4. **Compute Triangular Filters**: For each filter, compute the weights for each DFT bin based on the triangular response defined by three consecutive points from the converted Hertz values.\n5. **Normalization**: If required, normalize each filter so that its area in Mel space equals 1.\n6. **Return Filterbank Matrix**: Return the resulting filterbank as a list of lists, with each element rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\"\"\"\n    \n    if max_freq is None:\n        max_freq = fs / 2\n    \n    # Convert min and max frequencies to Mel scale\n    min_mel = 2595 * np.log10(1 + min_freq / 700)\n    max_mel = 2595 * np.log10(1 + max_freq / 700)\n    \n    # Generate n_filters + 2 points equally spaced in Mel\n    mels = np.linspace(min_mel, max_mel, num=n_filters + 2)\n    \n    # Convert Mel points back to Hertz\n    f_Hz = 700 * (10 ** (mels / 2595) - 1)\n    \n    # Compute bin frequencies\n    bin_freqs = np.arange(N // 2 + 1) * fs / N\n    \n    filterbank = []\n    \n    for i in range(n_filters):\n        f_left = f_Hz[i]\n        f_center = f_Hz[i + 1]\n        f_right = f_Hz[i + 2]\n        \n        # Calculate the mask for frequencies within [f_left, f_right]\n        mask = (bin_freqs >= f_left) & (bin_freqs <= f_right)\n        \n        # Compute slopes\n        left_slope = (bin_freqs - f_left) / (f_center - f_left)\n        right_slope = (f_right - bin_freqs) / (f_right - f_center)\n        \n        # Compute weights\n        weights = np.minimum(left_slope, right_slope)\n        weights = np.clip(weights, 0.0, None)\n        weights = np.where(mask, weights, 0.0)\n        \n        # Apply normalization\n        if normalize:\n            scale = 2.0 / (f_right - f_left)\n            weights *= scale\n        \n        # Round to 4 decimal places and append as list\n        rounded_weights = [round(w, 4) for w in weights.tolist()]\n        filterbank.append(rounded_weights)\n    \n    return filterbank\n```\n\n### Explanation\n1. **Frequency Conversion**: The given minimum and maximum frequencies are converted to the Mel scale to create a non-linear spacing that better reflects human hearing sensitivity.\n2. **Equally Spaced Mel Points**: These points are then used to generate the edges of the triangular filters. By converting back to Hertz, we ensure the filters are spaced appropriately on the frequency axis.\n3. **Triangular Filter Calculation**: Each filter is defined by three consecutive points in the converted Hertz values. The weight for each DFT bin is determined by its position relative to the triangular filter's edges.\n4. **Normalization**: Each filter is normalized to ensure that the area under the filter in Mel space is 1, which helps in maintaining consistent energy levels across different filters.\n5. **Efficiency**: The use of vectorized operations with NumPy ensures efficient computation of weights for all bins simultaneously, making the solution both concise and efficient.\n\nThis approach ensures that the resulting Mel filterbank matrix accurately transforms the power spectrum into Mel band energies, suitable for further processing in speech and audio applications."}
{"task_id": 312, "completion_id": 0, "completion": "To solve this problem, we need to generate a Blackman-Harris window of arbitrary length, which can be either symmetric or periodic. The symmetric form is typically used for FIR filter design, while the periodic form is suitable for FFT-based spectral analysis. The periodic form ensures that the window tiles seamlessly when wrapped for an FFT.\n\n### Approach\n1. **Special Cases Handling**: If the window length is less than or equal to 0, return an empty list. If the window length is 1, return a list containing a single 1.0.\n2. **Coefficients Definition**: Use the predefined coefficients for the Blackman-Harris window.\n3. **Determine Parameters**:\n   - For the symmetric window, use `N = window_len - 1` and generate `window_len` samples.\n   - For the periodic window, generate `window_len + 1` samples using `N = window_len` and then discard the last sample to get the desired length.\n4. **Compute Window Values**: Apply the Blackman-Harris formula using vectorized operations for efficiency.\n5. **Rounding**: Round the computed values to 4 decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n    \n    if symmetric:\n        N = window_len - 1\n        n = np.arange(window_len)\n    else:\n        # Generate window_len + 1 samples, then discard the last\n        N = window_len\n        n = np.arange(window_len + 1)\n    \n    window = (\n        a0 \n        - a1 * np.cos(2 * np.pi * n / N) \n        + a2 * np.cos(4 * np.pi * n / N) \n        - a3 * np.cos(6 * np.pi * n / N)\n    )\n    \n    if not symmetric:\n        window = window[:-1]  # Discard the last sample for periodic\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(window, 4).tolist()\n```\n\n### Explanation\n- **Special Cases Handling**: Directly handle edge cases where the window length is non-positive or exactly 1.\n- **Coefficients Definition**: The coefficients for the Blackman-Harris window are fixed and provided in the problem statement.\n- **Parameter Determination**: For the symmetric window, the length parameter `N` is set to `window_len - 1`, while for the periodic window, `N` is set to `window_len` and an extra sample is generated and then discarded.\n- **Window Computation**: Using NumPy's vectorized operations ensures efficient computation of the window values based on the derived formula.\n- **Rounding**: The results are rounded to four decimal places to meet the output requirements and converted to a Python list for the final output.\n\nThis approach ensures that both symmetric and periodic forms of the Blackman-Harris window are generated efficiently and correctly, adhering to the problem constraints and requirements."}
{"task_id": 313, "completion_id": 0, "completion": "To solve this problem, we need to compute the expected value of the logarithm of a component of a Dirichlet-distributed random variable. The key challenge is implementing the digamma function without relying on external libraries, using a combination of recursion and asymptotic expansion for accurate approximation.\n\n### Approach\n1. **Digamma Function Implementation**: The digamma function, \u03c8(x), is computed using a recursive approach for small values of x (x < 6) and an asymptotic expansion for larger values. The recursion \u03c8(x) = \u03c8(x+1) - 1/x is used to shift small x values up to a moderate value (6). For x \u2265 6, the asymptotic expansion \u03c8(x) \u2248 ln(x) - 1/(2x) - 1/(12x\u00b2) + 1/(120x\u2074) - 1/(252x\u2076) is applied.\n\n2. **Extract Parameters**: Given a 2-D array (or list) of Dirichlet parameters, extract the relevant row and column values based on the provided indices.\n\n3. **Compute the Expected Value**: Using the digamma function, compute the expected value of the logarithm of the specified Dirichlet component and the sum of the components in the specified row. Subtract these two values and round the result to four decimal places.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    def digamma(x):\n        shift = 0.0\n        x = float(x)\n        while x < 6:\n            shift -= 1.0 / x\n            x += 1.0\n        ln_x = math.log(x)\n        inv_x = 1.0 / x\n        inv_x2 = inv_x * inv_x\n        inv_x4 = inv_x2 * inv_x2\n        inv_x6 = inv_x4 * inv_x2\n        result = ln_x - 0.5 * inv_x - (1.0 / 12.0) * inv_x2 + (1.0 / 120.0) * inv_x4 - (1.0 / 252.0) * inv_x6\n        return result + shift\n    \n    gamma_row = gamma[d]\n    sum_gamma = sum(gamma_row)\n    gamma_dt = gamma_row[t]\n    \n    term1 = digamma(gamma_dt)\n    term2 = digamma(sum_gamma)\n    \n    result = term1 - term2\n    return round(result, 4)\n```\n\n### Explanation\n1. **Digamma Function**: The function `digamma(x)` handles values of x by recursively shifting values less than 6 to higher values using the relation \u03c8(x) = \u03c8(x+1) - 1/x. For x \u2265 6, it uses an asymptotic expansion to approximate \u03c8(x) accurately.\n\n2. **Parameter Extraction**: The relevant row (gamma[d]) and column (gamma[d][t]) are extracted from the input matrix. The sum of the row elements is computed to get the denominator for the digamma function.\n\n3. **Result Calculation**: The difference between the digamma values of the specified component and the sum of the row elements is computed, then rounded to four decimal places for the final result.\n\nThis approach ensures that we accurately compute the expected value using mathematical approximations and recursion, adhering to the constraints of not using external libraries."}
{"task_id": 317, "completion_id": 0, "completion": "To solve this problem, we need to generate a formatted debug error report that compares the predictions made by a program against the expected gold values. The report should include both the previous and current predictions and their corresponding gold values, formatted in a specific structure.\n\n### Approach\n1. **Determine Previous and Current Indices**: The previous index is calculated as the maximum of the given index minus one and zero to ensure it's within valid bounds. The current index is the given index.\n2. **Retrieve Predictions and Labels**: Extract the prediction and label for both the previous and current indices from the input parameters.\n3. **Fetch Gold Values**: Use the labels to fetch the corresponding gold values from the provided dictionary.\n4. **Construct the Report**: Format the header, previous predictions, current predictions, and footer into the specified structure, including handling optional warnings and ensuring the correct number of dashes and newlines.\n\n### Solution Code\n```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\"\"\"\n\n    prev_ix = max(ix - 1, 0)\n    prev_pred, prev_label = params[prev_ix]\n    prev_gold = golds[prev_label]\n\n    curr_pred, curr_label = params[ix]\n    curr_gold = golds[curr_label]\n\n    header = '-' * 25 + ' DEBUG ' + '-' * 25 + '\\n'\n    prev_mine = f\"Mine (prev) [{prev_label}]:\\n{prev_pred}\\n\"\n    prev_theirs = f\"Theirs (prev) [{prev_label}]:\\n{prev_gold}\\n\"\n    two_blanks = '\\n\\n'\n    current_mine = f\"Mine [{curr_label}]:\\n{curr_pred}\\n\"\n    current_theirs = f\"Theirs [{curr_label}]:\\n{curr_gold}{warn_str}\\n\"\n    footer = '-' * 23 + ' END DEBUG ' + '-' * 23 + '\\n'\n\n    return (header + prev_mine + prev_theirs + two_blanks +\n            current_mine + current_theirs + footer)\n```\n\n### Explanation\n1. **Header and Footer**: The header and footer lines are constructed with a specific number of dashes and the words \"DEBUG\" and \"END DEBUG\" respectively to meet the formatting requirements.\n2. **Previous Block**: The previous prediction and gold value are formatted with their corresponding labels. This block includes both the prediction made by the program and the expected gold value.\n3. **Current Block**: The current prediction and gold value are formatted similarly, with an optional warning string appended to the gold value line.\n4. **Blank Lines**: Two blank lines are inserted between the previous block and the current block to enhance readability.\n5. **String Construction**: All parts of the report are concatenated in the correct order to form the final output string, ensuring all newlines and dashes are placed exactly as specified.\n\nThis approach ensures that the generated report is both accurate and meets the exact formatting requirements specified in the problem statement."}
{"task_id": 318, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost.M1 algorithm from scratch using decision stumps as weak learners. The AdaBoost algorithm iteratively trains weak learners, adjusting the weights of training instances based on classification errors, and combines these learners into a strong classifier. \n\n### Approach\n1. **Initialization**: Start with uniform weights for all training samples.\n2. **Weak Learner Training**: For each boosting round, find the best decision stump (feature, threshold, and polarity) that minimizes the weighted classification error.\n3. **Error Calculation**: Compute the weighted error for the selected stump.\n4. **Weight Update**: Adjust the weights of misclassified samples to focus on harder cases in subsequent iterations.\n5. **Combine Weak Learners**: After training, combine the weak learners using weighted majority voting to form the final classifier.\n6. **Prediction**: Use the final ensemble of weak learners to predict the labels of the test data.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    n_samples, n_features = X_train.shape\n    D = np.ones(n_samples) / n_samples  # Initialize weights\n    stumps = []\n\n    for _ in range(n_estimators):\n        best_error = np.inf\n        best_j = -1\n        best_t = None\n        best_p = None\n\n        # Search for the best stump\n        for j in range(n_features):\n            sorted_indices = np.argsort(X_train[:, j])\n            X_j_sorted = X_train[sorted_indices, j]\n            y_sorted = y_train[sorted_indices]\n            D_sorted = D[sorted_indices]\n\n            mask_pos = (y_sorted == 1)\n            mask_neg = ~mask_pos\n\n            # Cumulative sums for left side\n            left_pos = np.cumsum(D_sorted * mask_pos)\n            left_neg = np.cumsum(D_sorted * mask_neg)\n\n            total_pos = left_pos[-1] if len(left_pos) > 0 else 0\n            total_neg = left_neg[-1] if len(left_neg) > 0 else 0\n\n            for k in range(len(X_j_sorted) - 1):\n                if X_j_sorted[k] == X_j_sorted[k + 1]:\n                    continue\n                t_candidate = (X_j_sorted[k] + X_j_sorted[k + 1]) / 2\n\n                current_left_pos = left_pos[k]\n                current_left_neg = left_neg[k]\n                current_right_pos = total_pos - current_left_pos\n                current_right_neg = total_neg - current_left_neg\n\n                error_p1 = current_left_pos + current_right_neg\n                error_m1 = current_left_neg + current_right_pos\n\n                if error_p1 < error_m1:\n                    current_error = error_p1\n                    current_p = 1\n                else:\n                    current_error = error_m1\n                    current_p = -1\n\n                if current_error < best_error:\n                    best_error = current_error\n                    best_j = j\n                    best_t = t_candidate\n                    best_p = current_p\n\n        # Handle case where no split was found (all features are constant)\n        if best_j == -1:\n            weighted_error_p1 = np.sum(D[y_train == -1])\n            weighted_error_m1 = np.sum(D[y_train == 1])\n            if weighted_error_p1 < weighted_error_m1:\n                best_p = 1\n                best_error = weighted_error_p1\n            else:\n                best_p = -1\n                best_error = weighted_error_m1\n            best_j = 0\n            best_t = -np.inf  # Ensure x_j >= t is always True\n\n        # Compute epsilon and alpha\n        epsilon_m = best_error\n        epsilon_m = max(epsilon_m, 1e-15)\n        epsilon_m = min(epsilon_m, 1 - 1e-15)\n        alpha_m = 0.5 * math.log((1 - epsilon_m) / epsilon_m)\n\n        # Update weights\n        h_m = best_p * np.where(X_train[:, best_j] >= best_t, 1, -1)\n        D *= np.exp(-alpha_m * y_train * h_m)\n        D_sum = D.sum()\n        if D_sum > 0:\n            D /= D_sum\n        else:\n            D = np.ones_like(D) / n_samples  # Reset weights if sum is zero (unlikely)\n\n        stumps.append((best_j, best_t, best_p, alpha_m))\n\n    # Predict on X_test\n    sum_alpha_h = np.zeros(X_test.shape[0])\n    for j, t, p, alpha in stumps:\n        h = p * np.where(X_test[:, j] >= t, 1, -1)\n        sum_alpha_h += alpha * h\n\n    y_pred = [1 if s >= 0 else -1 for s in sum_alpha_h]\n    return y_pred\n```\n\n### Explanation\n1. **Initialization**: The initial weights for all samples are set uniformly.\n2. **Weak Learner Training**: For each feature, sort the values and evaluate possible thresholds. Compute the weighted error for each potential split and select the best one.\n3. **Error Handling**: If no valid split is found (e.g., all features are constant), default to predicting the majority class based on current weights.\n4. **Weight Update**: Adjust weights to focus on misclassified samples using exponential updates, normalized to maintain a valid distribution.\n5. **Prediction**: Combine predictions from all weak learners using their respective weights to form the final prediction for test data.\n\nThis approach ensures that each weak learner corrects the errors of the previous ones, leading to a robust ensemble model."}
{"task_id": 329, "completion_id": 0, "completion": "To solve this problem, we need to compute the center frequencies of DFT bins for a given number of DFT coefficients and sampling frequency. The solution must handle both cases where only non-negative frequencies are required and where all frequencies (including negative ones) are needed, arranged in a specific order similar to NumPy's `fftfreq`.\n\n### Approach\n1. **Input Validation**: Check if the number of DFT coefficients (N) or the sampling frequency (fs) is non-positive. If either is non-positive, return an empty list.\n2. **Non-negative Frequencies (positive_only=True)**:\n   - Compute the frequency spacing (delta) as the ratio of the sampling frequency (fs) to the number of coefficients (N).\n   - Generate frequencies for indices from 0 to floor(N/2) using the formula \\( f_k = k \\cdot \\Delta f \\), where \\( \\Delta f = \\frac{fs}{N} \\).\n3. **All Frequencies (positive_only=False)**:\n   - Use NumPy's `fftfreq` to generate the frequency bins, then scale these frequencies by the sampling frequency (fs) to convert them to Hz.\n4. **Rounding**: Round all computed frequencies to 4 decimal places and return them as a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    if N <= 0 or fs <= 0:\n        return []\n    \n    if positive_only:\n        max_k = N // 2\n        k_values = np.arange(max_k + 1)\n        freqs = k_values * (fs / N)\n    else:\n        freqs = np.fft.fftfreq(N) * fs\n    \n    rounded_freqs = np.round(freqs, 4)\n    return rounded_freqs.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if N or fs is non-positive, returning an empty list if so.\n2. **Non-negative Frequencies**: When `positive_only` is True, the function calculates the frequency for each index from 0 to floor(N/2). This is done by generating an array of indices up to the midpoint of N and multiplying each by the frequency spacing (fs/N).\n3. **All Frequencies**: When `positive_only` is False, the function uses NumPy's `fftfreq` to generate normalized frequency bins, which are then scaled by the sampling frequency (fs) to convert them to Hz. This ensures the correct order of positive followed by negative frequencies.\n4. **Rounding**: The computed frequencies are rounded to 4 decimal places using NumPy's `round` function and converted to a Python list using `tolist()` for the required output format.\n\nThis approach efficiently handles both modes of operation and ensures accurate frequency bin computation with proper rounding and formatting."}
{"task_id": 331, "completion_id": 0, "completion": "To solve this problem, we need to compute the Out-of-Bag Mean Squared Error (OOB-MSE) for a Random Forest regressor. This involves using predictions from trees that did not include specific samples during their training phase to evaluate the model's performance.\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Ensure that all input parameters (y_true, predictions, oob_mask) are converted to numpy arrays for efficient numerical operations.\n2. **Compute OOB Counts and Valid Samples**: For each sample, determine how many trees considered it out-of-bag (OOB). Identify samples that have at least one OOB prediction.\n3. **Handle Edge Case**: If no samples have OOB predictions, return -1 as specified.\n4. **Compute OOB Predictions Sum**: Calculate the sum of OOB predictions for each sample using element-wise multiplication with the OOB mask.\n5. **Calculate Averages**: For valid samples (those with OOB predictions), compute the average of their OOB predictions.\n6. **Compute Squared Errors**: Determine the squared error between the true values and the averaged OOB predictions for each valid sample.\n7. **Calculate MSE**: Compute the mean of these squared errors and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\"\"\"\n    y_true = np.asarray(y_true)\n    predictions = np.asarray(predictions)\n    oob_mask = np.asarray(oob_mask).astype(bool)\n    \n    oob_counts = oob_mask.sum(axis=1)\n    valid_samples = oob_counts > 0\n    \n    if not np.any(valid_samples):\n        return -1.0\n    \n    oob_sums = (predictions * oob_mask).sum(axis=1)\n    averages = oob_sums[valid_samples] / oob_counts[valid_samples]\n    \n    squared_errors = (y_true[valid_samples] - averages) ** 2\n    mse = squared_errors.mean()\n    \n    return round(mse, 4)\n```\n\n### Explanation\n1. **Input Conversion**: The inputs are converted to numpy arrays to leverage numpy's efficient array operations.\n2. **OOB Counts**: By summing the OOB mask along the tree axis (axis=1), we determine how many OOB predictions each sample has.\n3. **Valid Samples Identification**: Samples with at least one OOB prediction are identified using a boolean mask.\n4. **Edge Case Handling**: If no valid samples are found, the function returns -1 immediately.\n5. **Sum and Average Calculation**: The sum of OOB predictions for each sample is computed using element-wise multiplication with the OOB mask. The average prediction for valid samples is then derived by dividing the sum by the count of OOB predictions.\n6. **Squared Error Calculation**: The squared error for each valid sample is computed using the true values and the averaged predictions.\n7. **MSE Calculation**: The mean of these squared errors is computed and rounded to four decimal places to produce the final result.\n\nThis approach efficiently handles the computation using vectorized operations, ensuring both accuracy and performance."}
{"task_id": 332, "completion_id": 0, "completion": "To solve this problem, we need to construct a Frequent-Pattern Tree (FP-Tree) from a list of transactions without using any classes. The FP-Tree will be represented as a nested dictionary where each node contains a support count and a dictionary of its child nodes. The construction of the tree follows specific steps to ensure efficiency and correctness.\n\n### Approach\n1. **Count Item Supports**: First, we count the occurrence of each item in the transactions to determine their support. This is done by iterating through each transaction and counting each unique item once per transaction.\n2. **Filter and Sort Items**: Items that do not meet the minimum support threshold are discarded. The remaining items are sorted first by decreasing support and then alphabetically to establish a global order.\n3. **Process Transactions**: Each transaction is processed to remove duplicates, filter out infrequent items, and sort the remaining items according to the global order.\n4. **Build the FP-Tree**: Using the processed transactions, we construct the FP-Tree. Starting from the root node, we increment the support count of existing nodes or create new nodes as we traverse through each transaction.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\"\"\"\n    # Step 1: Compute the support for each item\n    item_counter = Counter()\n    for transaction in transactions:\n        unique_items = set(transaction)\n        item_counter.update(unique_items)\n    \n    # Step 2: Filter items by min_support and create global ordering\n    frequent_items = {item: count for item, count in item_counter.items() if count >= min_support}\n    if not frequent_items:\n        return {\"support\": 0, \"children\": {}}\n    \n    # Create the global order sorted by decreasing support, then alphabetically\n    sorted_items = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n    order_dict = {item: idx for idx, item in enumerate(sorted_items)}\n    \n    # Initialize the root of the FP-Tree\n    root = {\"support\": 0, \"children\": {}}\n    \n    # Step 3: Process each transaction to build the tree\n    for transaction in transactions:\n        # Remove duplicates and filter infrequent items\n        unique_trans = set(transaction)\n        filtered = [item for item in unique_trans if item in frequent_items]\n        if not filtered:\n            continue\n        \n        # Sort the transaction items according to the global order\n        filtered_sorted = sorted(filtered, key=lambda x: order_dict[x])\n        \n        # Insert into the FP-Tree\n        current_node = root\n        for item in filtered_sorted:\n            children = current_node[\"children\"]\n            if item in children:\n                children[item][\"support\"] += 1\n                current_node = children[item]\n            else:\n                new_node = {\"support\": 1, \"children\": {}}\n                children[item] = new_node\n                current_node = new_node\n    \n    return root\n```\n\n### Explanation\n1. **Counting Item Supports**: We use a `Counter` to track how many times each item appears across all transactions. Each item is counted once per transaction, regardless of duplicates within the same transaction.\n2. **Filtering and Sorting**: Items that do not meet the minimum support threshold are excluded. The remaining items are sorted to create a global order, which ensures that items with higher support come first, and alphabetical order breaks ties.\n3. **Processing Transactions**: Each transaction is converted to a set to remove duplicates, filtered to retain only frequent items, and sorted according to the global order. This processed transaction is then used to update the FP-Tree.\n4. **Building the FP-Tree**: Starting from the root node, each item in the processed transaction is either added as a new node or increments the support count of an existing node. This ensures that the tree structure efficiently represents frequent item sets while maintaining their support counts.\n\nThis approach efficiently constructs the FP-Tree by leveraging sorting and dictionary operations, ensuring that the tree is both compact and representative of the frequent patterns in the transactions."}
{"task_id": 336, "completion_id": 0, "completion": "To solve this problem, we need to compute the linear kernel matrix between two collections of input vectors, X and Y. The kernel function is defined as the dot product of two vectors plus an optional constant. The solution must handle various edge cases, including empty inputs and dimension mismatches.\n\n### Approach\n1. **Handle Empty Input for X**: If X is an empty list, return an empty list immediately.\n2. **Convert to NumPy Arrays**: Convert X and Y to NumPy arrays to handle both lists and arrays uniformly.\n3. **Reshape 1D Arrays**: Ensure that 1D arrays are reshaped to 2D arrays to treat each input as a collection of vectors.\n4. **Check for Empty X**: If X has zero rows after conversion, return an empty list.\n5. **Handle Y**: If Y is not provided, set Y to X. Convert Y to a NumPy array and reshape if necessary.\n6. **Column Compatibility Check**: Ensure X and Y have the same number of columns; if not, return -1.\n7. **Compute Kernel Matrix**: Use matrix multiplication to compute the dot product efficiently, add the constant term, and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_kernel(X: list[list[int | float]] | \"np.ndarray\", Y: list[list[int | float]] | \"np.ndarray\" | None = None, c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\"\"\"\n    \n    # Handle empty X list case\n    if isinstance(X, list) and len(X) == 0:\n        return []\n    \n    # Convert X to numpy array and ensure it's 2D\n    x = np.array(X)\n    if x.ndim == 1:\n        x = x.reshape(1, -1)\n    # Check if X has zero rows after conversion\n    if x.shape[0] == 0:\n        return []\n    \n    # Handle Y\n    if Y is None:\n        y = x.copy()\n    else:\n        y = np.array(Y)\n        if y.ndim == 1:\n            y = y.reshape(1, -1)\n    \n    # Check if columns match\n    if x.shape[1] != y.shape[1]:\n        return -1\n    \n    # Compute kernel matrix\n    dot_product = np.dot(x, y.T)\n    k_matrix = dot_product + c0\n    # Round to 4 decimal places\n    k_rounded = np.round(k_matrix, 4)\n    # Convert to list of lists\n    return k_rounded.tolist()\n```\n\n### Explanation\n1. **Handling Empty Input**: The function first checks if X is an empty list and returns an empty list immediately if true. This ensures compliance with the problem's requirement for empty inputs.\n2. **Conversion and Reshaping**: X and Y are converted to NumPy arrays. If they are 1D, they are reshaped to 2D arrays to ensure each input is treated as a collection of vectors.\n3. **Dimension Checks**: After conversion, the function checks if X has zero rows (indicating an empty collection) and returns an empty list. It also checks if X and Y have the same number of columns, returning -1 if they do not.\n4. **Matrix Computation**: The dot product of X and Y (transposed) is computed using NumPy's optimized matrix multiplication. The constant term is added, and the result is rounded to four decimal places before converting to a list of lists for the final output.\n\nThis approach efficiently handles various edge cases and ensures compatibility between different input types while adhering to the problem's constraints and requirements."}
{"task_id": 340, "completion_id": 0, "completion": "To solve this problem, we need to implement a single optimization step of the Adam (Adaptive Moment Estimation) algorithm. The Adam algorithm is an extension to stochastic gradient descent that has been widely adopted for training deep learning models due to its computational efficiency and effectiveness in practice. The algorithm computes adaptive learning rates for each parameter by estimating the first and second moments of the gradients.\n\n### Approach\nThe Adam optimization algorithm involves the following steps for each parameter update:\n1. **Update the first moment estimate (mean)**: This is done by combining the previous first moment estimate with the current gradient.\n2. **Update the second moment estimate (uncentered variance)**: This is done by combining the previous second moment estimate with the square of the current gradient.\n3. **Bias correction**: Since the initial estimates of the moments are biased towards zero, bias-corrected versions of the moments are computed.\n4. **Parameter update**: The parameters are updated using the bias-corrected moments and the learning rate. If the gradient is exactly zero, the parameter remains unchanged, but the moments are still updated.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\"\"\"\n\n    # Compute new moments\n    new_m = beta1 * m + (1 - beta1) * grad\n    new_v = beta2 * v + (1 - beta2) * (grad ** 2)\n\n    # Compute bias-corrected estimates\n    m_hat = new_m / (1 - beta1 ** t)\n    v_hat = new_v / (1 - beta2 ** t)\n\n    # Compute the denominator for the parameter update\n    denominator = np.sqrt(v_hat) + eps\n\n    # Calculate the delta for the parameter update\n    delta = lr * m_hat / denominator\n\n    # Check if all elements of grad are zero to decide parameter update\n    if np.all(grad == 0):\n        updated_param = param.copy()\n    else:\n        updated_param = param - delta\n\n    # Round and convert to lists with 8 decimal places\n    updated_param_list = np.round(updated_param, 8).tolist()\n    new_m_list = np.round(new_m, 8).tolist()\n    new_v_list = np.round(new_v, 8).tolist()\n\n    return (updated_param_list, new_m_list, new_v_list)\n```\n\n### Explanation\n1. **Moment Updates**: The first and second moments (`m` and `v`) are updated using exponential moving averages of the gradient and squared gradient, respectively. This helps in smoothing out the noisy gradients.\n2. **Bias Correction**: The moments are adjusted to account for their initialization bias towards zero. This correction is crucial in the initial steps when the estimates are more biased.\n3. **Parameter Update**: The parameters are updated using the bias-corrected moments. The learning rate is scaled by the square root of the second moment estimate, adjusted by a small constant `eps` to avoid division by zero.\n4. **Handling Zero Gradient**: If the gradient is exactly zero, the parameters remain unchanged, but the moments are still updated to ensure they reflect the current state accurately.\n\nThis implementation efficiently handles multi-dimensional parameters using NumPy's array operations, ensuring that the solution is both vectorized and efficient. The results are returned as Python lists with elements rounded to 8 decimal places for consistency and precision."}
{"task_id": 343, "completion_id": 0, "completion": "To solve this problem, we need to implement the core numerical step of the Cross-Entropy Method (CEM) to update the parameters of a sampling distribution based on the performance of sampled parameter vectors. The goal is to retain the top-performing samples and compute their mean and variance to update the distribution parameters.\n\n### Approach\n1. **Determine the Number of Samples to Retain**: Calculate the number of samples to retain using the given fraction. If the product of the fraction and the total number of samples is not an integer, we take the floor of this product.\n2. **Sort Samples by Performance**: Sort the parameter vectors (rows of the input array) based on their corresponding rewards in descending order to identify the top-performing samples.\n3. **Retain Top Samples**: Select the top samples based on the calculated number of samples to retain.\n4. **Compute Mean and Variance**: Calculate the mean and variance for each dimension (column) of the retained samples.\n5. **Round and Convert to Lists**: Round the computed mean and variance values to four decimal places and convert them to Python lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\"\"\"\n    N = theta_samples.shape[0]\n    k = int(retain_prcnt * N)\n    \n    rewards_arr = np.array(rewards)\n    sorted_indices = np.argsort(-rewards_arr)\n    elite_indices = sorted_indices[:k]\n    elite_samples = theta_samples[elite_indices]\n    \n    mean = np.mean(elite_samples, axis=0)\n    variance = np.var(elite_samples, axis=0)\n    \n    rounded_mean = np.round(mean, 4)\n    rounded_variance = np.round(variance, 4)\n    \n    return (rounded_mean.tolist(), rounded_variance.tolist())\n```\n\n### Explanation\n1. **Determine Number of Samples to Retain**: The number of samples to retain (`k`) is computed by multiplying the total number of samples (`N`) by the given fraction (`retain_prcnt`) and converting the result to an integer using the floor function.\n2. **Sort Samples**: The indices of the rewards are sorted in descending order to prioritize higher rewards. This allows us to select the top `k` samples.\n3. **Retain Top Samples**: Using the sorted indices, we extract the top `k` rows from the parameter vectors array (`theta_samples`).\n4. **Compute Mean and Variance**: The mean and variance are computed along each dimension (columns) of the retained samples. This gives us the new distribution parameters.\n5. **Round and Convert**: The computed mean and variance values are rounded to four decimal places and converted to Python lists to meet the output requirements.\n\nThis approach ensures that we efficiently update the distribution parameters based on the best-performing samples, which is crucial for optimizing policies in reinforcement learning using the Cross-Entropy Method."}
{"task_id": 353, "completion_id": 0, "completion": "To solve this problem, we need to compute the Shannon entropy of a set of class labels. Shannon entropy is a measure of the impurity or disorder of the labels and is calculated using the probabilities of each unique label's occurrence.\n\n### Approach\n1. **Handle Empty Input**: If the input sequence is empty, return 0.0 immediately as there is no entropy to compute.\n2. **Count Occurrences**: Use a counter to tally the occurrences of each unique label in the input sequence.\n3. **Calculate Probabilities**: For each unique label, compute its probability by dividing its count by the total number of labels.\n4. **Compute Entropy**: Using the probabilities, compute the entropy using the formula \\(H = -\\sum_i p_i \\log_2 p_i\\). Sum the contributions of each probability and then take the negative of the sum to get the entropy.\n5. **Round Result**: Round the computed entropy to 5 decimal places before returning.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    if not labels:\n        return 0.0\n    counts = Counter(labels)\n    total = len(labels)\n    entropy_val = 0.0\n    for cnt in counts.values():\n        p = cnt / total\n        entropy_val += p * math.log2(p)\n    entropy_val = -entropy_val\n    return round(entropy_val, 5)\n```\n\n### Explanation\n- **Empty Input Check**: The function first checks if the input is empty. If it is, the entropy is 0.0.\n- **Counting Labels**: Using `collections.Counter`, we count how many times each label appears in the input sequence.\n- **Probability Calculation**: For each unique label, we determine its probability by dividing its count by the total number of labels.\n- **Entropy Calculation**: For each probability, we compute the term \\( p \\log_2 p \\) and sum these terms. The negative of this sum gives the Shannon entropy.\n- **Rounding**: The result is rounded to 5 decimal places to meet the specified precision requirement.\n\nThis approach efficiently handles all edge cases, including empty input and uniform labels, ensuring accurate and precise entropy calculation."}
{"task_id": 354, "completion_id": 0, "completion": "To solve this problem, we need to implement a high-performance 2D convolution using the im2col + GEMM (General Matrix Multiply) strategy. This method efficiently converts the convolution operation into matrix multiplication, which can be optimized using NumPy's linear algebra routines.\n\n### Approach\n1. **Padding Handling**: Depending on the padding specification, compute the necessary padding for each side (top, bottom, left, right) of the input images. The padding can be an integer, a tuple, or 'same' to maintain the spatial dimensions of the output.\n2. **Dilation Handling**: Adjust the kernel's effective size by considering the dilation factor, which dictates the spacing between kernel elements.\n3. **im2col Transformation**: Convert the input images into columns (patches) that the kernel will slide over. This transformation considers both the stride and dilation parameters to generate the appropriate patches.\n4. **Matrix Multiplication**: Reshape the kernel into a matrix and perform matrix multiplication with the im2col-transformed input to compute the convolution result efficiently.\n5. **Reshaping and Conversion**: Reshape the result into the desired output format and convert it to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    n_ex, in_rows, in_cols, in_ch = X.shape\n    kr, kc, in_ch_W, out_ch = W.shape\n    if in_ch != in_ch_W:\n        raise ValueError(\"Mismatch in input channels between X and W\")\n    \n    kr_eff = (kr - 1) * dilation + 1\n    kc_eff = (kc - 1) * dilation + 1\n    \n    if pad == 'same':\n        pad_row_total = max(0, (in_rows - 1) * stride + kr_eff - in_rows)\n        pad_col_total = max(0, (in_cols - 1) * stride + kc_eff - in_cols)\n        pr1 = pad_row_total // 2\n        pr2 = pad_row_total - pr1\n        pc1 = pad_col_total // 2\n        pc2 = pad_col_total - pc1\n    elif isinstance(pad, int):\n        pr1 = pr2 = pad\n        pc1 = pc2 = pad\n    elif isinstance(pad, tuple):\n        if len(pad) == 2:\n            pr, pc = pad\n            pr1 = pr2 = pr\n            pc1 = pc2 = pc\n        elif len(pad) == 4:\n            pr1, pr2, pc1, pc2 = pad\n        else:\n            pr1 = pr2 = pc1 = pc2 = 0\n    else:\n        pr1 = pr2 = pc1 = pc2 = 0\n    \n    X_pad = np.pad(X, ((0, 0), (pr1, pr2), (pc1, pc2), (0, 0)), mode='constant')\n    H_pad, W_pad = X_pad.shape[1], X_pad.shape[2]\n    \n    H_out = (H_pad - kr_eff) // stride + 1\n    W_out = (W_pad - kc_eff) // stride + 1\n    \n    if H_out <= 0 or W_out <= 0:\n        return np.zeros((n_ex, H_out, W_out, out_ch)).tolist()\n    \n    s_batch, s_H, s_W, s_ch = X_pad.strides\n    new_shape = (n_ex, H_out, W_out, kr, kc, in_ch)\n    new_strides = (\n        s_batch,\n        s_H * stride,\n        s_W * stride,\n        s_H * (dilation + 1),\n        s_W * (dilation + 1),\n        s_ch\n    )\n    \n    patches = np.lib.stride_tricks.as_strided(\n        X_pad,\n        shape=new_shape,\n        strides=new_strides,\n        writeable=False\n    )\n    \n    patches_reshaped = patches.reshape(-1, kr * kc * in_ch)\n    W_reshaped = W.reshape(-1, out_ch)\n    result = np.matmul(patches_reshaped, W_reshaped)\n    output = result.reshape(n_ex, H_out, W_out, out_ch)\n    \n    return output.tolist()\n```\n\n### Explanation\n1. **Padding Calculation**: The padding is computed based on the input parameters. For 'same' padding, it ensures the output dimensions match the input dimensions by calculating the necessary symmetric padding.\n2. **Dilation Handling**: The effective kernel size is adjusted using the dilation factor to account for the spacing between kernel elements.\n3. **im2col Transformation**: Using NumPy's `as_strided` function, the input image is transformed into columns (patches) that the kernel will convolve over. This step efficiently handles both stride and dilation.\n4. **Matrix Multiplication**: The reshaped kernel and im2col-transformed input are multiplied using matrix multiplication, which is highly optimized in NumPy.\n5. **Result Reshaping**: The resulting matrix is reshaped to match the desired output dimensions and converted to a Python list for the final output.\n\nThis approach leverages NumPy's efficiency in matrix operations and strided views to perform the convolution operation in a highly optimized manner."}
{"task_id": 355, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\"\"\"\n\n    # Step 1: Mean centering\n    centered_data = data - np.mean(data, axis=0)\n\n    # Step 2: Obtain directions based on the solver\n    if solver == \"svd\":\n        U, s, Vt = np.linalg.svd(centered_data, full_matrices=False)\n        directions = Vt  # Rows are principal directions\n    elif solver == \"eigen\":\n        cov_matrix = np.cov(centered_data, rowvar=False, ddof=1)\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        sorted_idx = np.argsort(eigenvalues)[::-1]\n        sorted_eigenvalues = eigenvalues[sorted_idx]\n        sorted_eigenvectors = eigenvectors[:, sorted_idx]\n        directions = sorted_eigenvectors.T  # Transpose to get rows as directions\n    else:\n        raise ValueError(\"solver must be 'svd' or 'eigen'.\")\n\n    # Step 3: Keep top n_components directions\n    n_features = data.shape[1]\n    n_components = min(n_components, n_features)\n    directions = directions[:n_components]\n\n    # Step 4: Fix the sign determinism\n    for i in range(directions.shape[0]):\n        direction = directions[i]\n        non_zero_indices = np.flatnonzero(direction)\n        if non_zero_indices.size > 0:\n            first_non_zero = non_zero_indices[0]\n            if direction[first_non_zero] < 0:\n                directions[i] *= -1\n\n    # Step 5: Project data to obtain scores\n    scores = centered_data @ directions.T\n\n    # Step 6: Compute explained variance ratios\n    if solver == \"svd\":\n        total_variance = (s ** 2).sum()\n        explained_var_ratios = (s[:n_components] ** 2) / total_variance\n    else:  # eigen solver\n        total_variance = sorted_eigenvalues.sum()\n        explained_var_ratios = sorted_eigenvalues[:n_components] / total_variance\n\n    # Round and convert to list of lists\n    scores_rounded = np.round(scores, 4).tolist()\n    explained_var_ratios_rounded = np.round(explained_var_ratios, 4).tolist()\n\n    return (scores_rounded, explained_var_ratios_rounded)\n```"}
{"task_id": 356, "completion_id": 0, "completion": "To solve this problem, we need to create a function that extracts the correct prediction from a tree leaf node based on whether the tree is a classification tree or a regression tree. \n\n### Approach\n1. **Classification Tree**: For a classification tree, the leaf node stores a list or tuple of class probabilities. The prediction is the index of the maximum probability in this list. If there are multiple maximum probabilities, the index of the first occurrence is chosen, consistent with the behavior of `numpy.argmax`.\n2. **Regression Tree**: For a regression tree, the leaf node stores a scalar value representing the mean of the target values. The prediction is simply this scalar value.\n\n### Solution Code\n```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        probs = leaf.value\n        max_prob = max(probs)\n        return probs.index(max_prob)\n    else:\n        return leaf.value\n```\n\n### Explanation\n1. **Classification Handling**: When `classifier` is `True`, the function retrieves the list of probabilities from the leaf node. It then finds the maximum probability and returns the index of the first occurrence of this maximum value using `list.index()`.\n2. **Regression Handling**: When `classifier` is `False`, the function directly returns the scalar value stored in the leaf node, which is the mean target value for that region in a regression tree.\n\nThis approach ensures that the function efficiently determines the correct prediction based on the type of tree, adhering to the specified behavior for both classification and regression scenarios."}
{"task_id": 357, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm from scratch. The goal is to partition samples into K clusters based on the nearest Euclidean centroid. The algorithm is deterministic, initializing centroids as the first K samples, and iteratively refining them until convergence or a maximum number of iterations.\n\n### Approach\n1. **Initialization**: Start with the first K samples as initial centroids.\n2. **Iteration**: Repeat the following steps until convergence or maximum iterations:\n   - **Assignment**: Assign each sample to the nearest centroid using Euclidean distance.\n   - **Update**: Recompute centroids as the mean of assigned samples. If a centroid has no samples, it remains unchanged.\n   - **Convergence Check**: Stop if centroids change by less than a tolerance (1e-6).\n3. **Sorting and Rounding**: Sort centroids lexicographically and round each coordinate to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    centroids = data[:K].copy()\n    for _ in range(max_iters):\n        # Calculate distances from each data point to each centroid\n        distances = np.sqrt(((data[:, np.newaxis] - centroids) ** 2).sum(axis=2))\n        closest = np.argmin(distances, axis=1)\n        \n        # Update centroids\n        new_centroids = centroids.copy()\n        for j in range(K):\n            mask = (closest == j)\n            if mask.any():\n                new_centroids[j] = data[mask].mean(axis=0)\n        \n        # Check for convergence\n        if np.allclose(centroids, new_centroids, atol=1e-6):\n            break\n        centroids = new_centroids.copy()\n    \n    # Sort centroids lexicographically and round to four decimal places\n    sorted_centroids = sorted(centroids.tolist())\n    rounded_centroids = [[round(val, 4) for val in centroid] for centroid in sorted_centroids]\n    return rounded_centroids\n```\n\n### Explanation\n1. **Initialization**: The centroids are initialized using the first K samples from the data matrix.\n2. **Assignment Step**: For each sample, the Euclidean distance to each centroid is computed, and the sample is assigned to the nearest centroid.\n3. **Update Step**: Each centroid is updated to the mean of the samples assigned to it. If no samples are assigned to a centroid, it remains unchanged.\n4. **Convergence Check**: The algorithm stops early if the centroids do not change significantly (within a tolerance of 1e-6).\n5. **Sorting and Rounding**: The final centroids are sorted lexicographically and each coordinate is rounded to four decimal places for the result.\n\nThis approach ensures that the algorithm efficiently converges to stable centroids while adhering to the deterministic initialization and termination conditions specified."}
{"task_id": 362, "completion_id": 0, "completion": "To solve this problem, we need to convert a given 2-D list or NumPy array of non-negative numbers into a row-stochastic matrix. A row-stochastic matrix is one where each row sums to 1. If any row in the input matrix sums to zero, normalization is impossible, and we should return -1 in such cases.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input is converted to a NumPy array of floats to handle both lists and NumPy arrays uniformly and to facilitate numerical operations.\n2. **Compute Row Sums**: Calculate the sum of each row. If any row sums to zero, return -1 immediately as normalization is impossible.\n3. **Normalize Rows**: For each row, divide every element by the row's sum. This ensures that each row sums to 1.\n4. **Round Values**: Round each element of the resulting matrix to 4 decimal places for precision.\n5. **Convert to Python List**: Convert the normalized and rounded NumPy array back to a Python list of lists before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    arr = np.array(data, dtype=float)\n    row_sums = arr.sum(axis=1)\n    if np.any(row_sums == 0):\n        return -1\n    normalized = arr / row_sums[:, np.newaxis]\n    rounded = np.round(normalized, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input data is converted to a float NumPy array to ensure all elements are treated as floating-point numbers, which is necessary for division operations.\n2. **Row Sum Calculation**: Using `arr.sum(axis=1)`, we compute the sum of each row. This results in a 1-D array where each element corresponds to the sum of a row in the input matrix.\n3. **Check for Zero Sums**: If any element in the row sums array is zero, normalization is impossible, so we return -1 immediately.\n4. **Row Normalization**: Each element in a row is divided by the row's sum using broadcasting. `row_sums[:, np.newaxis]` reshapes the row sums into a column vector, allowing division across all columns of each row.\n5. **Rounding and Conversion**: The normalized matrix is rounded to 4 decimal places using `np.round`, and the result is converted back to a Python list of lists using `tolist()` to meet the output requirements.\n\nThis approach efficiently handles the conversion, normalization, and rounding, ensuring the solution is both correct and optimized for performance."}
{"task_id": 363, "completion_id": 0, "completion": "To solve this problem, we need to determine whether the action and observation spaces of a reinforcement learning (RL) environment are continuous. The continuity of these spaces is determined based on their structure and the types of their subspaces. \n\n### Approach\nThe approach involves checking the structure of the action and observation spaces using flags provided as input. The flags indicate whether the respective space is a composite type (Tuple or Dict). For composite spaces, we need to check all subspaces to ensure they are continuous (i.e., all are Box instances). For non-composite spaces, we simply check if the space itself is a Box.\n\n1. **Check Composite Spaces**: If the flag for a space (action or observation) is set to `True`, indicating it is a Tuple or Dict, we recursively check all subspaces to ensure they are all Box instances.\n2. **Check Non-Composite Spaces**: If the flag is `False`, we directly check if the space is an instance of Box.\n\n### Solution Code\n```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    def all_subspaces_are_box(space):\n        if isinstance(space, Tuple):\n            return all(isinstance(s, Box) for s in space.spaces)\n        elif isinstance(space, Dict):\n            return all(isinstance(s, Box) for s in space.spaces.values())\n        else:\n            return False\n    \n    cont_action = all_subspaces_are_box(env.action_space) if tuple_action else isinstance(env.action_space, Box)\n    cont_obs = all_subspaces_are_box(env.observation_space) if tuple_obs else isinstance(env.observation_space, Box)\n    \n    return (cont_action, cont_obs)\n```\n\n### Explanation\n- **Helper Function `all_subspaces_are_box`**: This function checks if all subspaces of a given composite space (either Tuple or Dict) are Box instances. For a Tuple, it iterates through all subspaces, and for a Dict, it checks all values in the dictionary of subspaces.\n- **Continuity Check**: The main function uses the helper function to determine the continuity of the action and observation spaces based on the provided flags. If the flag indicates a composite space, the helper function is used; otherwise, it checks if the space itself is a Box.\n\nThis approach ensures that we correctly identify continuous spaces both for simple and composite structures, adhering to the problem constraints and requirements."}
{"task_id": 369, "completion_id": 0, "completion": "To solve this problem, we need to aggregate the outputs of multiple weak learners (decision trees) in a Gradient Boosting model for both regression and classification tasks. The aggregation involves starting with a zero-initialized running prediction and iteratively updating it by subtracting the scaled outputs of each tree. The final prediction is derived differently for regression and classification tasks.\n\n### Approach\n1. **Sum Updates**: Sum the outputs of all trees. Each tree's output is scaled by a learning rate and subtracted from the running prediction.\n2. **Running Prediction**: Compute the running prediction by scaling the summed updates with the learning rate and negating the result.\n3. **Regression Handling**: For regression tasks, return the running prediction rounded to four decimal places.\n4. **Classification Handling**: For classification tasks, convert the running prediction (logits) to probabilities using the softmax function and then determine the class labels using the argmax function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\"\"\"\n    sum_updates = sum(updates)\n    running_pred = -learning_rate * sum_updates\n\n    if regression:\n        return np.round(running_pred, 4)\n    else:\n        # Compute softmax probabilities\n        logits = running_pred\n        max_logits = np.max(logits, axis=1, keepdims=True)\n        shifted_logits = logits - max_logits\n        exp_logits = np.exp(shifted_logits)\n        softmax = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        return np.argmax(softmax, axis=1).astype(int)\n```\n\n### Explanation\n1. **Summing Updates**: The sum of all tree outputs is computed using `sum(updates)`, which efficiently combines the outputs element-wise.\n2. **Running Prediction Calculation**: The running prediction is obtained by scaling the summed updates with the learning rate and negating the result. This is done using vectorized operations for efficiency.\n3. **Regression Handling**: For regression, the running prediction is directly returned after rounding to four decimal places using `np.round`.\n4. **Classification Handling**: For classification, the running prediction (logits) is converted to probabilities using the softmax function. To ensure numerical stability, the maximum logit value is subtracted from each logit before exponentiation. The softmax probabilities are then used to determine the class labels using `argmax`.\n\nThis approach ensures that the solution is fully vectorized, efficient, and adheres to the mathematical principles of Gradient Boosting for both regression and classification tasks."}
{"task_id": 371, "completion_id": 0, "completion": "To solve this problem, we need to implement a 1-D cross-correlation function that supports batches of examples, multiple input and output channels, strides, padding (as an integer, tuple, or \"same\"), and dilation. The goal is to produce the resulting 3-D volume as a Python list using numpy for efficient array operations.\n\n### Approach\n1. **Padding Calculation**: Determine the padding values (left and right) based on the input parameters. The padding can be specified as an integer, a tuple, or \"same\" to mimic TensorFlow's \"SAME\" padding.\n2. **Input Padding**: Apply the calculated padding to the input signal.\n3. **Effective Kernel Size**: Compute the effective kernel size considering dilation, which affects the spacing between kernel elements.\n4. **Output Length Calculation**: Determine the length of the output signal based on the padded input length, effective kernel size, and stride.\n5. **Indices Generation**: Generate indices for the sliding window positions considering the stride and dilation.\n6. **Window Extraction and Convolution**: For each example in the batch, extract windows from the padded input and perform the cross-correlation with the kernel using vectorized operations.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\"\"\"\n\n    n_examples, l_in, in_channels = X.shape\n    kernel_width = W.shape[0]\n    out_channels = W.shape[2]\n\n    # Calculate padding\n    if pad == 'same':\n        out_len = np.ceil(l_in / stride).astype(int)\n        effective_kernel = (kernel_width - 1) * (dilation + 1) + 1\n        total_pad = max(0, (out_len - 1) * stride + effective_kernel - l_in)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n    elif isinstance(pad, int):\n        p_left = pad\n        p_right = pad\n    elif isinstance(pad, tuple) and len(pad) == 2:\n        p_left, p_right = pad\n    else:\n        raise ValueError(\"Invalid pad argument. Must be 'same', int, or 2-tuple.\")\n\n    # Pad the input\n    X_padded = np.pad(X, [(0, 0), (p_left, p_right), (0, 0)], mode='constant')\n    padded_length = X_padded.shape[1]\n\n    # Compute effective kernel size\n    effective_kernel_size = (kernel_width - 1) * (dilation + 1) + 1\n\n    # Calculate output length\n    output_length = (padded_length - effective_kernel_size) // stride + 1\n    output_length = max(0, output_length)\n\n    if output_length == 0:\n        return np.zeros((n_examples, 0, out_channels)).tolist()\n\n    # Generate indices for sliding window\n    output_positions = np.arange(output_length) * stride\n    kernel_offsets = np.arange(kernel_width) * (dilation + 1)\n    indices = output_positions[:, None] + kernel_offsets[None, :]  # (output_length, kernel_width)\n\n    # Check if indices are within bounds\n    valid_indices = (indices >= 0) & (indices < padded_length)\n    indices = np.where(valid_indices, indices, 0)  # Use 0 as dummy index, values will be multiplied by 0 later\n\n    # Prepare output array\n    output = np.zeros((n_examples, output_length, out_channels))\n\n    for ex_idx in range(n_examples):\n        example = X_padded[ex_idx]  # (padded_length, in_channels)\n        window = example[indices]  # (output_length, kernel_width, in_channels)\n        # Apply mask for out-of-bound indices\n        window = window * valid_indices[..., None]\n        # Compute cross-correlation\n        example_output = np.einsum('okc,kco->oc', window, W)\n        output[ex_idx] = example_output\n\n    return output.tolist()\n```\n\n### Explanation\n1. **Padding Calculation**: Depending on the input `pad`, the function calculates the left and right padding. For \"same\" padding, it ensures the output length matches the input length divided by stride, rounded up.\n2. **Input Padding**: The input signal is padded with zeros on both ends based on the calculated padding values.\n3. **Effective Kernel Size**: The kernel size is adjusted for dilation, which determines the spacing between kernel elements.\n4. **Output Length Calculation**: The output length is determined by the padded input length, effective kernel size, and stride.\n5. **Indices Generation**: Indices for the sliding window positions are generated to efficiently extract the required windows from the padded input.\n6. **Window Extraction and Convolution**: For each example, windows are extracted from the padded input using the generated indices. The cross-correlation is performed using matrix multiplication to sum over the kernel and input channels efficiently.\n\nThis approach ensures the solution handles various padding strategies, dilation, and stride efficiently using numpy's vectorized operations, making it both correct and performant."}
{"task_id": 373, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Gini impurity of a sequence of integer class labels. The Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. \n\n### Approach\n1. **Check for Empty Input**: If the input sequence is empty, return 0.0 immediately as per the problem's convention.\n2. **Count Unique Labels**: Use NumPy's `unique` function to count the occurrences of each unique class label in the input sequence. This function efficiently returns both the unique elements and their respective counts.\n3. **Calculate Probabilities**: For each unique class label, compute the probability by dividing the count of the class by the total number of samples.\n4. **Sum of Squares of Probabilities**: Compute the sum of the squares of these probabilities. This sum is used to determine the Gini impurity.\n5. **Compute Gini Impurity**: Subtract the sum of the squared probabilities from 1 to get the Gini impurity.\n6. **Round the Result**: Round the computed Gini impurity to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    if len(y) == 0:\n        return 0.0\n    _, counts = np.unique(y, return_counts=True)\n    probs = counts / len(y)\n    sum_squares = np.sum(probs ** 2)\n    return round(1.0 - sum_squares, 4)\n```\n\n### Explanation\n- **Empty Input Handling**: The function first checks if the input is empty and returns 0.0 if true. This handles edge cases where there are no labels to process.\n- **Unique Labels and Counts**: Using `np.unique` with `return_counts=True` efficiently retrieves the unique class labels and their counts. This is more efficient than manually iterating and counting, especially for large datasets.\n- **Probability Calculation**: Each count is divided by the total number of samples to get the probability of each class. This step ensures we are working with relative frequencies.\n- **Sum of Squares**: Squaring each probability and summing them gives a measure of the purity of the dataset. A higher sum of squares indicates lower impurity.\n- **Gini Impurity Calculation**: Subtracting the sum of squared probabilities from 1 gives the Gini impurity. This value is then rounded to four decimal places to meet the specified precision requirement.\n\nThis approach efficiently computes the Gini impurity using vectorized operations provided by NumPy, ensuring both clarity and performance."}
{"task_id": 374, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that trains a multiple linear regression model using batch gradient descent. The goal is to minimize the mean-squared error (MSE) by iteratively updating the model parameters (weights and bias) using the gradients of the loss function.\n\n### Approach\n1. **Check Data Consistency**: Verify that the number of samples in the feature matrix `X` matches the number of targets in vector `y`. If not, return -1.\n2. **Initialize Parameters**: Start with weights (\u03b8) as a vector of zeros and bias (b) as zero.\n3. **Iterative Gradient Descent**: For a specified number of iterations, compute the predicted values, calculate the error, and update the weights and bias using the gradients derived from the error.\n4. **Return Results**: After completing the iterations, return the weights and bias rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\"\"\"\n    \n    # Check if the number of samples in X and y are the same\n    if X.shape[0] != y.shape[0]:\n        return -1\n    \n    m, n = X.shape  # m is number of samples, n is number of features\n    y = y.ravel()   # Ensure y is a 1D array\n    \n    # Initialize parameters\n    theta = np.zeros(n)\n    b = 0.0\n    \n    # Perform gradient descent for n_iterations\n    for _ in range(n_iterations):\n        y_hat = X.dot(theta) + b  # Compute predictions\n        error = y_hat - y         # Compute error\n        \n        # Compute gradients\n        dw = (2 / m) * X.T.dot(error)\n        db = (2 / m) * np.sum(error)\n        \n        # Update parameters\n        theta -= learning_rate * dw\n        b -= learning_rate * db\n    \n    # Round the results to 4 decimal places\n    theta_rounded = [round(val, 4) for val in theta]\n    b_rounded = round(b, 4)\n    \n    return (theta_rounded, b_rounded)\n```\n\n### Explanation\n1. **Data Consistency Check**: The function first checks if the number of rows in `X` (samples) matches the length of `y`. If not, it returns -1 to indicate inconsistent data.\n2. **Parameter Initialization**: The weights (\u03b8) are initialized as a zero vector with the same number of features as `X`, and the bias (b) is initialized to zero.\n3. **Gradient Descent Iterations**:\n   - **Prediction Calculation**: For each iteration, compute the predicted values `y_hat` using the current weights and bias.\n   - **Error Calculation**: Determine the error between the predicted values and the actual targets.\n   - **Gradient Calculation**: Compute the gradients of the loss with respect to the weights (dw) and bias (db).\n   - **Parameter Update**: Adjust the weights and bias using the computed gradients and the learning rate.\n4. **Result Rounding**: After completing all iterations, the weights and bias are rounded to four decimal places to meet the specified precision requirement.\n\nThis approach efficiently minimizes the MSE using batch gradient descent, ensuring that the model parameters are iteratively adjusted to find the optimal fit for the given data."}
{"task_id": 376, "completion_id": 0, "completion": "To solve this problem, we need to determine if a given 2-D numeric array is symmetric with respect to its main diagonal. A matrix is symmetric if it is square (same number of rows and columns) and the element at row i and column j is equal to the element at row j and column i for all valid indices (i, j). Additionally, due to potential floating-point rounding errors, we must treat two numbers as equal if their absolute difference does not exceed 1 \u00d7 10\u207b\u2078.\n\n### Approach\n1. **Check if the Matrix is Square**: First, verify that the input matrix is square. This means the number of rows must be equal to the number of columns. If any row has a different length than the number of rows, the matrix is not square, and we return False.\n2. **Convert to NumPy Array**: Convert the input list of lists into a NumPy array for efficient matrix operations.\n3. **Check Symmetry with Tolerance**: Use NumPy's `allclose` function to compare the matrix with its transpose. This function checks if all corresponding elements are close within a specified absolute tolerance (1e-8), effectively handling floating-point precision issues.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    n = len(X)\n    # Check if all rows have the same length as the number of rows (n)\n    for row in X:\n        if len(row) != n:\n            return False\n    # Convert to numpy array and check symmetry with tolerance\n    x_np = np.array(X)\n    return np.allclose(x_np, x_np.T, rtol=0, atol=1e-8)\n```\n\n### Explanation\n1. **Square Matrix Check**: The function first checks if the matrix is square by ensuring each row's length matches the number of rows. If any row fails this check, the function returns False immediately.\n2. **Conversion to NumPy Array**: Converting the input matrix to a NumPy array allows efficient matrix operations, including transposition and element-wise comparison.\n3. **Symmetry Check with Tolerance**: Using `np.allclose` with an absolute tolerance of 1e-8 ensures that minor floating-point discrepancies are ignored, providing a robust check for symmetry even in the presence of rounding errors.\n\nThis approach efficiently combines structural checks with numerical tolerance handling to accurately determine if the matrix is symmetric."}
{"task_id": 377, "completion_id": 0, "completion": "To solve this problem, we need to compute the negative gradient for the logistic loss function used in Gradient Boosting for binary classification. The goal is to transform the given binary labels and current prediction scores into the required negative gradients, ensuring all inputs are valid.\n\n### Approach\n1. **Input Validation**: Check if the provided labels (y) contain only 0s and 1s. If any other value is found, return -1.\n2. **Label Transformation**: Convert the binary labels from {0, 1} to {-1, 1} using the formula \\( y' = 2y - 1 \\).\n3. **Gradient Calculation**: Compute the negative gradient for each sample using the formula \\( g_i = \\frac{y'_i}{1 + \\exp(y'_i f_i)} \\), where \\( y'_i \\) is the transformed label and \\( f_i \\) is the current prediction score.\n4. **Rounding**: Round each computed gradient to four decimal places and return the result as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"Compute the negative gradient for logistic loss in binary classification.\"\"\"\n    y_arr = np.array(y)\n    if np.any((y_arr != 0) & (y_arr != 1)):\n        return -1\n    f_arr = np.array(f)\n    y_prime = 2 * y_arr - 1\n    denominator = 1 + np.exp(y_prime * f_arr)\n    g = y_prime / denominator\n    g_rounded = np.round(g, 4)\n    return g_rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The input labels are converted to a numpy array, and we check if all elements are either 0 or 1. If not, the function returns -1 immediately.\n2. **Label Transformation**: The labels are transformed using \\( y' = 2y - 1 \\), converting 0 to -1 and 1 to 1.\n3. **Gradient Calculation**: The denominator is computed using the exponential of the product of transformed labels and prediction scores, plus 1. Each gradient value is then calculated by dividing the transformed label by this denominator.\n4. **Rounding**: The results are rounded to four decimal places using numpy's rounding function and converted back to a list for the final output.\n\nThis approach ensures that all edge cases are handled efficiently, and the computations are vectorized for performance using numpy operations."}
{"task_id": 380, "completion_id": 0, "completion": "To solve this problem, we need to implement the Scaled Exponential Linear Unit (SELU) activation function along with its first and second derivatives. The SELU function is a scaled version of the Exponential Linear Unit (ELU) function, which helps in self-normalizing neural networks. The implementation must handle scalar values, 1-D arrays, and multi-dimensional arrays, preserving the input shape and returning results rounded to four decimal places.\n\n### Approach\n1. **Understanding SELU Function**: The SELU function is defined as `scale * ELU(x, alpha)`, where ELU is `x` if `x > 0` and `alpha * (exp(x) - 1)` otherwise. The constants `alpha` and `scale` are provided.\n2. **Derivatives**:\n   - **First Derivative**: For `x > 0`, the derivative is `scale`. For `x <= 0`, it is `scale * alpha * exp(x)`.\n   - **Second Derivative**: For `x > 0`, the second derivative is `0`. For `x <= 0`, it remains `scale * alpha * exp(x)`.\n3. **Handling Input Types**: Convert the input to a NumPy array to handle scalars, lists, and multi-dimensional arrays uniformly. Use NumPy's element-wise operations to compute results efficiently.\n4. **Rounding and Conversion**: After computing the results, round them to four decimal places and convert to Python lists or scalars using NumPy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n\n    x_np = np.asarray(x)\n\n    if order == 0:\n        elu = np.where(x_np > 0, x_np, alpha * (np.exp(x_np) - 1))\n        result = scale * elu\n    elif order == 1:\n        derivative = np.where(x_np > 0, 1, alpha * np.exp(x_np))\n        result = scale * derivative\n    elif order == 2:\n        result = np.where(x_np > 0, 0, scale * alpha * np.exp(x_np))\n    else:\n        raise ValueError(\"Order must be 0, 1, or 2.\")\n\n    rounded_result = np.round(result, 4)\n    return rounded_result.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `x` is converted to a NumPy array to handle both scalar and array inputs uniformly.\n2. **Element-wise Operations**: Using `np.where`, we apply the SELU function and its derivatives element-wise across the input array. This ensures efficient computation for any input shape.\n3. **Rounding and Conversion**: The results are rounded to four decimal places using `np.round` and converted to Python lists or scalars with `tolist()`, ensuring the output matches the required format.\n\nThis approach efficiently handles all input types and dimensions, computes the necessary values using vectorized operations for performance, and ensures the results are correctly formatted and rounded."}
{"task_id": 387, "completion_id": 0, "completion": "To solve this problem, we need to implement a gradient boosting regressor that works on one-dimensional data using decision stumps. The regressor will iteratively improve its predictions by focusing on the residuals (errors) of the previous predictions, building a sequence of simple models (stumps) to correct these errors.\n\n### Approach\n1. **Initial Prediction**: Start with the mean of the target values as the initial prediction for all samples.\n2. **Iterative Boosting**: For each boosting round:\n   - Compute the residuals (differences between actual and predicted values).\n   - Sort the data points based on the feature values.\n   - Generate candidate split points between consecutive different feature values.\n   - For each candidate split, calculate the mean residuals on both sides of the split and compute the sum of squared errors.\n   - Select the split that minimizes the sum of squared errors.\n   - Update the predictions using the selected split, scaled by a learning rate.\n3. **Edge Cases**: Handle cases where all feature values are identical by using the global mean of residuals for updates.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    x = np.array(x)\n    y = np.array(y)\n    y_hat = np.full_like(y, np.mean(y), dtype=np.float64)\n    \n    for _ in range(n_estimators):\n        residuals = y - y_hat\n        sorted_indices = np.argsort(x)\n        x_sorted = x[sorted_indices]\n        r_sorted = residuals[sorted_indices]\n        \n        split_candidates = []\n        for i in range(len(x_sorted) - 1):\n            if x_sorted[i] != x_sorted[i+1]:\n                split = (x_sorted[i] + x_sorted[i+1]) / 2\n                split_candidates.append(split)\n        \n        if not split_candidates:\n            global_mean = np.mean(residuals)\n            y_hat += learning_rate * global_mean\n            continue\n        \n        best_t = None\n        best_loss = float('inf')\n        best_left_mean = 0.0\n        best_right_mean = 0.0\n        \n        for t in split_candidates:\n            left_mask = x_sorted <= t\n            right_mask = ~left_mask\n            \n            left_mean = np.mean(r_sorted[left_mask])\n            right_mean = np.mean(r_sorted[right_mask])\n            \n            pred = np.where(left_mask, left_mean, right_mean)\n            loss = np.sum((r_sorted - pred) ** 2)\n            \n            if loss < best_loss:\n                best_loss = loss\n                best_t = t\n                best_left_mean = left_mean\n                best_right_mean = right_mean\n        \n        mask = x <= best_t\n        r_hat = np.where(mask, best_left_mean, best_right_mean)\n        y_hat += learning_rate * r_hat\n    \n    return np.round(y_hat, 4).tolist()\n```\n\n### Explanation\n- **Initial Prediction**: The initial prediction for all samples is the mean of the target values.\n- **Residual Calculation**: For each iteration, residuals are computed as the difference between the actual values and the current predictions.\n- **Sorting and Splitting**: The data points are sorted by the feature values. Potential split points are identified between consecutive different feature values.\n- **Split Evaluation**: For each candidate split, the mean residuals on both sides of the split are computed. The split that minimizes the sum of squared errors is selected.\n- **Prediction Update**: The predictions are updated by adding the contribution of the current stump's predictions, scaled by the learning rate.\n- **Edge Handling**: If all feature values are identical, the global mean of residuals is used to update predictions, ensuring the model can still adjust even with no valid splits.\n\nThis approach efficiently builds a sequence of simple models to correct errors iteratively, leading to a robust regressor for one-dimensional data."}
{"task_id": 394, "completion_id": 0, "completion": "To solve this problem, we need to implement the RMSprop optimization algorithm, which is an adaptive learning rate method used for training neural networks. The algorithm adjusts the learning rate of each parameter based on the magnitude of its gradients, leading to more efficient and stable convergence.\n\n### Approach\nThe RMSprop algorithm maintains an exponentially decaying average of the squared gradients to scale the learning rate for each parameter. The key steps involved in the implementation are:\n\n1. **Initialization of Running Average**: If the running average of squared gradients (Eg) is not provided (None), it is initialized to a zero array of the same shape as the gradient array.\n\n2. **Update Running Average**: The running average is updated using a decay rate (rho) to combine the previous average with the current squared gradient. This helps in smoothing out the variability in gradient values over time.\n\n3. **Compute Parameter Update**: The parameters are updated by scaling the learning rate with the square root of the running average (plus a small constant for numerical stability). This scaling ensures that parameters with larger gradients have smaller effective learning rates and vice versa.\n\n4. **Rounding and Conversion**: Both the updated parameters and the new running average are rounded to four decimal places and converted to Python lists before returning them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n    # Update the running average of squared gradients\n    Eg_next = rho * Eg + (1 - rho) * (grad ** 2)\n    # Compute the denominator with epsilon for numerical stability\n    denominator = np.sqrt(Eg_next + 1e-8)\n    # Update parameters\n    w_next = w - (learning_rate * grad) / denominator\n    # Round to four decimal places and convert to Python lists\n    return (np.round(w_next, 4).tolist(), np.round(Eg_next, 4).tolist())\n```\n\n### Explanation\n1. **Initialization**: If the running average `Eg` is not provided, it is initialized to a zero array matching the shape of the gradient array. This is done using `np.zeros_like(grad)`.\n\n2. **Running Average Update**: The new running average `Eg_next` is computed using the decay rate `rho` to blend the previous average with the current squared gradient. This is achieved with the formula `Eg_next = rho * Eg + (1 - rho) * (grad ** 2)`.\n\n3. **Parameter Update**: The parameters `w` are updated by subtracting the product of the learning rate and the gradient, divided by the square root of the running average (with a small epsilon added for numerical stability). This step ensures that each parameter's update is scaled appropriately based on its gradient history.\n\n4. **Rounding and Conversion**: Both the updated parameters and the new running average are rounded to four decimal places using `np.round` and converted to Python lists with `tolist()` to meet the output requirements.\n\nThis approach efficiently implements the RMSprop optimization algorithm, ensuring numerical stability and proper scaling of learning rates for each parameter based on historical gradient information."}
{"task_id": 398, "completion_id": 0, "completion": "To solve this problem, we need to implement the Exponential Linear Unit (ELU) activation function and its gradient. The ELU function is widely used in deep learning models due to its ability to help networks converge faster and mitigate the vanishing gradient problem. The task requires handling both the activation function and its derivative efficiently.\n\n### Approach\nThe ELU activation function is defined as:\n- ELU(x, \u03b1) = x for x \u2265 0\n- ELU(x, \u03b1) = \u03b1( e\u02e3 \u2212 1 ) for x < 0\n\nThe derivative of ELU is:\n- ELU\u2032(x, \u03b1) = 1 for x \u2265 0\n- ELU\u2032(x, \u03b1) = \u03b1e\u02e3 for x < 0\n\nThe approach involves:\n1. Converting the input list or array to a NumPy array for efficient element-wise operations.\n2. Using NumPy's `where` function to apply the ELU activation or its derivative based on the value of each element.\n3. Rounding the results to four decimal places and returning them as a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    x_arr = np.asarray(x)\n    if derivative:\n        result = np.where(x_arr >= 0, 1.0, alpha * np.exp(x_arr))\n    else:\n        result = np.where(x_arr >= 0, x_arr, alpha * (np.exp(x_arr) - 1))\n    return [round(val, 4) for val in result.tolist()]\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `x` is converted to a NumPy array to leverage efficient vectorized operations.\n2. **Activation Function Calculation**: Using `np.where`, the activation function is computed by applying `x` directly for non-negative values and `\u03b1( e\u02e3 \u2212 1 )` for negative values.\n3. **Derivative Calculation**: Similarly, the derivative is computed using `np.where` where the result is 1 for non-negative values and `\u03b1e\u02e3` for negative values.\n4. **Rounding and Conversion**: The results are rounded to four decimal places using Python's `round` function and converted back to a list to meet the problem's output requirements.\n\nThis approach ensures efficiency and clarity by utilizing NumPy's optimized operations for handling element-wise computations, making the solution both concise and effective."}
{"task_id": 411, "completion_id": 0, "completion": "To solve this problem, we need to compute various statistics about actions and observations collected in a reinforcement learning environment. These statistics help in understanding the structure and nature of the data, such as whether the data is multi-dimensional, discrete, or continuous, and the number of unique values in each dimension.\n\n### Approach\n1. **Check for Iterables**: Determine if the elements in the actions or observations lists are iterables (lists, tuples, or numpy arrays).\n2. **Dimensionality**: Compute the dimensionality of the actions and observations. If the elements are iterables, the dimensionality is the length of the first element. Otherwise, the dimensionality is 1.\n3. **Multi-dimensional Check**: Check if the dimensionality is greater than 1.\n4. **Continuous Check**: Flatten the lists and check if any element is a non-integer float, indicating continuous data.\n5. **Unique Values per Dimension**: Collect values for each dimension, compute the number of unique values, and sort these unique values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\"\"\"\n\n    def to_python_scalar(value):\n        if isinstance(value, np.generic):\n            return value.item()\n        return value\n\n    def get_tuple_flag(lst):\n        if not lst:\n            return False\n        return isinstance(lst[0], (list, tuple, np.ndarray))\n\n    def is_continuous(value):\n        if isinstance(value, np.generic):\n            value = value.item()\n        if isinstance(value, float):\n            return not value.is_integer()\n        if isinstance(value, int):\n            return False\n        return False\n\n    def process_dimensions(lst, is_tuple):\n        if not lst:\n            return [], []\n        dimensions = []\n        if is_tuple:\n            num_dim = len(lst[0])\n            dimensions = [[] for _ in range(num_dim)]\n            for elem in lst:\n                for i in range(num_dim):\n                    val = elem[i]\n                    py_val = to_python_scalar(val)\n                    dimensions[i].append(py_val)\n        else:\n            dimensions = [[to_python_scalar(elem) for elem in lst]]\n        n_per_dim = []\n        ids_per_dim = []\n        for dim in dimensions:\n            unique = sorted(set(dim))\n            n_per_dim.append(len(unique))\n            ids_per_dim.append(list(unique))\n        return n_per_dim, ids_per_dim\n\n    # Process actions\n    tuple_actions = get_tuple_flag(actions)\n    if tuple_actions:\n        action_dim = len(actions[0]) if actions else 0\n    else:\n        action_dim = 1 if actions else 0\n    multidim_actions = action_dim > 1\n\n    flattened_actions = []\n    if tuple_actions and actions:\n        for elem in actions:\n            flattened_actions.extend(elem)\n    else:\n        flattened_actions = list(actions)\n    continuous_actions = any(is_continuous(val) for val in flattened_actions)\n\n    n_actions_per_dim, action_ids = process_dimensions(actions, tuple_actions)\n\n    # Process observations\n    tuple_observations = get_tuple_flag(observations)\n    if tuple_observations:\n        obs_dim = len(observations[0]) if observations else 0\n    else:\n        obs_dim = 1 if observations else 0\n    multidim_observations = obs_dim > 1\n\n    flattened_observations = []\n    if tuple_observations and observations:\n        for elem in observations:\n            flattened_observations.extend(elem)\n    else:\n        flattened_observations = list(observations)\n    continuous_observations = any(is_continuous(val) for val in flattened_observations)\n\n    n_obs_per_dim, obs_ids = process_dimensions(observations, tuple_observations)\n\n    return {\n        'tuple_actions': tuple_actions,\n        'tuple_observations': tuple_observations,\n        'multidim_actions': multidim_actions,\n        'multidim_observations': multidim_observations,\n        'continuous_actions': continuous_actions,\n        'continuous_observations': continuous_observations,\n        'n_actions_per_dim': n_actions_per_dim,\n        'action_dim': action_dim,\n        'n_obs_per_dim': n_obs_per_dim,\n        'obs_dim': obs_dim,\n        'action_ids': action_ids,\n        'obs_ids': obs_ids\n    }\n```\n\n### Explanation\n1. **Check for Iterables**: The function `get_tuple_flag` checks if the elements of the list are iterables by inspecting the first element (if the list is non-empty).\n2. **Dimensionality**: For iterables, the dimensionality is the length of the first element. For scalars, the dimensionality is 1.\n3. **Multi-dimensional Check**: If the dimensionality is greater than 1, the data is considered multi-dimensional.\n4. **Continuous Check**: The list is flattened, and each element is checked to see if it is a non-integer float using the `is_continuous` function.\n5. **Unique Values per Dimension**: The `process_dimensions` function collects values for each dimension, computes the number of unique values, and sorts these values to provide the required statistics.\n\nThis approach efficiently computes all required statistics by leveraging Python's built-in data structures and functions, ensuring clarity and correctness in handling both scalar and multi-dimensional data."}
{"task_id": 413, "completion_id": 0, "completion": "To solve this problem, we need to implement a simplified Gradient Boosting Decision Tree (GBDT) regressor from scratch. The goal is to predict the target values for test data using an ensemble of regression trees, each built to correct the residuals of the previous predictions.\n\n### Approach\n1. **Initial Prediction**: Start with an initial prediction as the mean of the training targets.\n2. **Residual Calculation**: For each boosting iteration, compute the residuals (differences between the actual and predicted values).\n3. **Tree Construction**: Fit a regression tree to these residuals using the CART algorithm, which splits nodes to minimize the mean squared error (MSE) and stops growing when the maximum depth is reached or no further error reduction is possible.\n4. **Prediction Update**: Update the predictions by adding the tree's predictions multiplied by a learning rate.\n5. **Iterative Process**: Repeat the residual calculation, tree construction, and prediction update for a specified number of iterations (n_estimators).\n6. **Final Prediction**: Combine the initial prediction with all subsequent tree predictions to form the final prediction for test data.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 10, learning_rate: float = 0.1, max_depth: int = 3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\"\"\"\n\n    def build_tree(X, y, depth, max_depth):\n        if depth >= max_depth or len(y) <= 1:\n            return {'value': np.mean(y)}\n        if np.var(y) == 0:\n            return {'value': np.mean(y)}\n        \n        best_feature = None\n        best_threshold = None\n        best_mse = np.inf\n        n_samples, n_features = X.shape\n        \n        for feature in range(n_features):\n            sorted_indices = np.argsort(X[:, feature])\n            sorted_X_feature = X[sorted_indices, feature]\n            sorted_y = y[sorted_indices]\n            \n            sum_total = sorted_y.sum()\n            sum_sq_total = (sorted_y ** 2).sum()\n            sum_left = 0.0\n            sum_sq_left = 0.0\n            \n            for i in range(1, len(sorted_y)):\n                sum_left += sorted_y[i-1]\n                sum_sq_left += sorted_y[i-1] ** 2\n                sum_right = sum_total - sum_left\n                sum_sq_right = sum_sq_total - sum_sq_left\n                n_left = i\n                n_right = len(sorted_y) - i\n                \n                if n_left == 0 or n_right == 0:\n                    continue\n                \n                mse_left = sum_sq_left - (sum_left ** 2) / n_left\n                mse_right = sum_sq_right - (sum_right ** 2) / n_right\n                total_mse = mse_left + mse_right\n                \n                if total_mse < best_mse:\n                    best_mse = total_mse\n                    best_feature = feature\n                    best_threshold = (sorted_X_feature[i-1] + sorted_X_feature[i]) / 2.0\n        \n        if best_feature is None:\n            return {'value': np.mean(y)}\n        \n        left_mask = X[:, best_feature] <= best_threshold\n        right_mask = ~left_mask\n        \n        if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n            return {'value': np.mean(y)}\n        \n        left_subtree = build_tree(X[left_mask], y[left_mask], depth + 1, max_depth)\n        right_subtree = build_tree(X[right_mask], y[right_mask], depth + 1, max_depth)\n        \n        return {\n            'feature': best_feature,\n            'threshold': best_threshold,\n            'left': left_subtree,\n            'right': right_subtree\n        }\n    \n    def predict_tree(node, X):\n        predictions = np.zeros(X.shape[0])\n        for i in range(X.shape[0]):\n            current_node = node\n            while True:\n                if 'value' in current_node:\n                    predictions[i] = current_node['value']\n                    break\n                else:\n                    feat = current_node['feature']\n                    thr = current_node['threshold']\n                    if X[i, feat] <= thr:\n                        current_node = current_node['left']\n                    else:\n                        current_node = current_node['right']\n        return predictions\n    \n    initial_pred = np.mean(y_train)\n    current_train_pred = np.full_like(y_train, initial_pred, dtype=np.float64)\n    test_pred = np.full(X_test.shape[0], initial_pred, dtype=np.float64)\n    \n    for _ in range(n_estimators):\n        residuals = y_train - current_train_pred\n        tree = build_tree(X_train, residuals, 0, max_depth)\n        \n        # Update training predictions\n        pred_train = predict_tree(tree, X_train)\n        current_train_pred += learning_rate * pred_train\n        \n        # Update test predictions\n        pred_test = predict_tree(tree, X_test)\n        test_pred += learning_rate * pred_test\n    \n    test_pred_rounded = np.round(test_pred, 4)\n    return test_pred_rounded.tolist()\n```\n\n### Explanation\n1. **Initial Prediction**: The initial prediction for all samples is the mean of the training targets.\n2. **Residual Calculation**: For each iteration, residuals are computed as the difference between the actual target values and the current predictions.\n3. **Tree Construction**: A regression tree is built using the residuals. Each split in the tree is determined by minimizing the MSE. The tree stops growing when the maximum depth is reached or no valid split reduces the error.\n4. **Prediction Update**: The predictions are updated by adding the tree's predictions scaled by the learning rate. This process corrects the residuals iteratively.\n5. **Final Prediction**: After all iterations, the final prediction for test data is the sum of the initial prediction and all subsequent tree predictions, each scaled by the learning rate. The results are rounded to four decimal places and returned as a list."}
{"task_id": 416, "completion_id": 0, "completion": "To solve this problem, we need to implement the probability density function (PDF) of a multivariate Gaussian distribution without using any third-party libraries except NumPy. The solution involves calculating the PDF for each data point in the input array using the given mean vector and covariance matrix.\n\n### Approach\n1. **Input Handling**: Ensure the input data array `X` is reshaped to a 2D array if it is 1D. This ensures consistent handling of data points.\n2. **Inverse and Determinant**: Compute the inverse and determinant of the covariance matrix using NumPy's linear algebra functions.\n3. **Normalization Term**: Calculate the normalization constant using the determinant of the covariance matrix and the dimensionality of the data.\n4. **Quadratic Form**: For each data point, compute the quadratic form using the inverse of the covariance matrix. This involves vectorized operations to efficiently compute the necessary matrix multiplications.\n5. **Exponential Term**: Compute the exponential part of the PDF using the quadratic form results.\n6. **PDF Calculation**: Combine the normalization term with the exponential term to get the PDF values for each data point.\n7. **Rounding**: Round each PDF value to four decimal places and return the results as a list.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"Compute the PDF of a multivariate Gaussian distribution for given data points.\n    \n    Args:\n        X (np.ndarray): Data points, shape (n_samples, n_features) or (n_samples,) for 1D.\n        mean (np.ndarray): Mean vector of the distribution, shape (n_features,).\n        cov (np.ndarray): Covariance matrix, shape (n_features, n_features).\n        \n    Returns:\n        list: PDF values for each data point, rounded to four decimal places.\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    \n    d = len(mean)\n    cov_inv = np.linalg.inv(cov)\n    cov_det = np.linalg.det(cov)\n    \n    # Compute the normalization constant\n    denominator = (2 * np.pi) ** (d / 2) * np.sqrt(cov_det)\n    norm = 1.0 / denominator\n    \n    # Compute the quadratic term (x - mean)^T @ cov_inv @ (x - mean) for each x in X\n    diff = X - mean\n    temp = diff @ cov_inv\n    quadratic = np.sum(temp * diff, axis=1)\n    exponents = -0.5 * quadratic\n    \n    # Compute PDF values and round to 4 decimal places\n    pdf_values = norm * np.exp(exponents)\n    return [round(val, 4) for val in pdf_values]\n```\n\n### Explanation\n1. **Input Handling**: The input `X` is converted to a NumPy array and reshaped to ensure it is 2D. This allows consistent processing whether the input is 1D or 2D.\n2. **Covariance Matrix Operations**: Using NumPy's `linalg.inv` and `linalg.det`, we compute the inverse and determinant of the covariance matrix, which are essential for the PDF formula.\n3. **Normalization Constant**: The normalization term is derived from the determinant and the dimensionality of the data, ensuring the PDF integrates to 1 over the entire space.\n4. **Quadratic Form Calculation**: By vectorizing operations, we efficiently compute the quadratic form for all data points, which measures the squared Mahalanobis distance from the mean.\n5. **Exponential and PDF Calculation**: The exponential term adjusts the normalization by how far each data point is from the mean, scaled by the covariance structure. This is combined to get the final PDF values, which are then rounded to four decimal places for the output list.\n\nThis approach ensures efficient and accurate computation of the multivariate Gaussian PDF using vectorized operations and proper handling of matrix inverses and determinants."}
{"task_id": 419, "completion_id": 0, "completion": "To solve this problem, we need to implement Bayesian linear regression with a conjugate Gaussian prior and known observation variance to compute the MAP (Maximum A Posteriori) predictive mean for new data points. The solution involves several steps, including preprocessing the data, handling prior parameters, computing the posterior distribution, and making predictions.\n\n### Approach\n1. **Intercept Handling**: If required, add an intercept column of ones to both the training data `X` and the new data `X_new`.\n2. **Prior Parameter Handling**: Process the prior mean `mu` and prior covariance matrix `V` to ensure they are in the correct form.\n3. **Posterior Computation**: Compute the posterior distribution parameters using the given formulas, which involve matrix inversions and multiplications.\n4. **Prediction**: Use the posterior mean to predict the target values for the new data points and round the results to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    # Add intercept column if required\n    if fit_intercept:\n        X = np.concatenate([np.ones((X.shape[0], 1), X], axis=1)\n        X_new = np.concatenate([np.ones((X_new.shape[0], 1), dtype=X.dtype), X_new], axis=1)\n    \n    n_features = X.shape[1]\n    \n    # Process mu into a vector\n    if np.isscalar(mu):\n        mu = np.full(n_features, mu)\n    else:\n        mu = np.asarray(mu).flatten()\n    \n    # Process V into a matrix\n    if V is None:\n        V_mat = np.eye(n_features)\n    elif np.isscalar(V):\n        V_mat = V * np.eye(n_features)\n    else:\n        V_arr = np.asarray(V)\n        if V_arr.ndim == 1:\n            V_mat = np.diag(V_arr)\n        else:\n            V_mat = V_arr\n    \n    # Compute V inverse\n    V_inv = np.linalg.inv(V_mat)\n    \n    # Compute X^T X and X^T y\n    X_T = X.T\n    X_T_X = X_T @ X\n    X_T_y = X_T @ y\n    \n    # Compute posterior covariance and mean\n    term = V_inv + X_T_X\n    Sigma_p = np.linalg.inv(term)\n    mu_p = Sigma_p @ (V_inv @ mu + X_T_y)\n    \n    # Compute predictions for X_new\n    y_pred = X_new @ mu_p\n    \n    # Round to 4 decimal places and convert to list\n    return [round(float(val), 4) for val in y_pred]\n```\n\n### Explanation\n1. **Intercept Handling**: If `fit_intercept` is `True`, a column of ones is added to both the training data `X` and the new data `X_new` to account for the intercept term in the linear regression model.\n2. **Prior Mean (`mu`)**: The prior mean `mu` is converted to a vector of appropriate length. If `mu` is a scalar, it is replicated to form a vector with the same number of features as the input data.\n3. **Prior Covariance (`V`)**: The prior covariance matrix `V` is constructed based on its input type. It can be a scalar (forming a diagonal matrix), a 1-D array (forming a diagonal matrix), or a 2-D matrix.\n4. **Posterior Computation**: The posterior covariance matrix `Sigma_p` and mean `mu_p` are computed using the given formulas. This involves matrix inversion and multiplication to combine the prior information with the data.\n5. **Prediction**: The MAP predictions for `X_new` are computed using the posterior mean `mu_p`, and the results are rounded to four decimal places before being returned as a list.\n\nThis approach efficiently combines prior knowledge with observed data to make predictions, leveraging Bayesian inference to provide a robust solution."}
{"task_id": 423, "completion_id": 0, "completion": "To solve this problem, we need to compute the Root Mean Squared Logarithmic Error (RMSLE) between two sequences of non-negative numbers. RMSLE is a metric used to evaluate regression models, emphasizing the ratio of predicted to actual values rather than their absolute differences.\n\n### Approach\n1. **Input Validation**: Ensure both input sequences are of the same length and contain only non-negative numbers. If any of these conditions are not met, return -1.\n2. **Logarithmic Transformation**: Compute the natural logarithm of each element incremented by 1 for both actual and predicted values. This helps in handling zero values gracefully and emphasizes relative differences.\n3. **Difference Calculation**: Calculate the difference between the logarithmic values of predicted and actual values.\n4. **Squaring and Averaging**: Square each of these differences, compute their mean, and then take the square root of the mean to get the RMSLE.\n5. **Rounding**: Round the result to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    # Validate input lengths\n    if len(actual) != len(predicted):\n        return -1\n    \n    # Check all elements are non-negative\n    if not (all(x >= 0 for x in actual) and all(x >= 0 for x in predicted)):\n        return -1\n    \n    # Convert lists to numpy arrays for vectorized operations\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    \n    # Compute log differences\n    log_diff = np.log1p(predicted_np) - np.log1p(actual_np)\n    \n    # Square the differences, compute mean, then take square root\n    squared_diff = np.square(log_diff)\n    mean_squared_diff = np.mean(squared_diff)\n    rmsle = np.sqrt(mean_squared_diff)\n    \n    # Round to four decimal places\n    return round(rmsle, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of the input lists are equal. If not, it returns -1. It then checks if all elements in both lists are non-negative. Any negative value in either list results in returning -1.\n2. **Logarithmic Transformation**: Using `np.log1p`, we compute the natural logarithm of each element after adding 1 to it. This handles zero values without causing domain errors.\n3. **Difference Calculation**: The difference between the log-transformed predicted and actual values is computed element-wise.\n4. **Squaring and Averaging**: The differences are squared, averaged, and then the square root of the average is taken to compute the RMSLE.\n5. **Rounding**: The result is rounded to four decimal places using Python's `round` function to meet the specified precision requirement.\n\nThis approach ensures that the function efficiently handles valid inputs and computes the RMSLE accurately while adhering to the problem constraints."}
{"task_id": 428, "completion_id": 0, "completion": "To solve this problem, we need to generate a Hann window (also known as the Hanning window) of a specified length. The Hann window is used in digital signal processing to reduce spectral leakage by ensuring the endpoints smoothly reach zero. The window can be generated in two modes: symmetric and periodic, depending on the use case.\n\n### Approach\n1. **Special Cases Handling**: \n   - If the window length is less than or equal to 0, return an empty list.\n   - If the window length is exactly 1, return a list containing a single element [1.0].\n\n2. **Determine Window Length**:\n   - For a symmetric window, use the specified window length directly.\n   - For a periodic window, generate a symmetric window of length `window_len + 1` and then discard the last sample to ensure periodicity.\n\n3. **Generate the Hann Window**:\n   - Use the mathematical formula for the Hann window to compute each sample.\n   - For each sample index `n`, the value is calculated using the formula: \n     \\[\n     \\text{hann}(n) = 0.5 - 0.5 \\times \\cos\\left(\\frac{2 \\pi n}{N-1}\\right)\n     \\]\n     where \\(N\\) is the determined window length.\n\n4. **Adjust for Periodicity**:\n   - If generating a periodic window, truncate the last sample of the symmetric window.\n\n5. **Rounding and Conversion**:\n   - Round each value to 4 decimal places.\n   - Convert the resulting NumPy array to a Python list and return it.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    N = window_len if symmetric else window_len + 1\n    samples = np.arange(N)\n    hann_values = 0.5 - 0.5 * np.cos(2 * math.pi * samples / (N - 1))\n    \n    if not symmetric:\n        hann_values = hann_values[:window_len]\n    \n    hann_rounded = np.round(hann_values, 4)\n    return hann_rounded.tolist()\n```\n\n### Explanation\n- **Special Cases**: The function first checks for invalid or trivial inputs (window length \u2264 0 or exactly 1) and handles them directly.\n- **Window Length Adjustment**: Depending on whether a symmetric or periodic window is needed, the effective window length is adjusted. For periodic windows, the length is increased by 1 before generation and then truncated.\n- **Mathematical Calculation**: The Hann window values are computed using vectorized operations for efficiency, leveraging NumPy's array operations.\n- **Truncation and Rounding**: The periodic window is truncated to the desired length, and all values are rounded to 4 decimal places for precision, then converted to a Python list for the final output.\n\nThis approach ensures that the generated Hann window meets the requirements for both symmetric and periodic use cases efficiently and accurately."}
{"task_id": 433, "completion_id": 0, "completion": "To solve this problem, we need to compute the fan-in and fan-out values for a given weight-tensor shape according to specific rules. These values are essential for initializing neural network parameters using schemes like Xavier/Glorot. The solution involves distinguishing between 2D convolutional layers and other layer types based on the shape of the weight tensor.\n\n### Approach\n1. **Identify Convolutional Layer**: If the input shape has exactly 4 dimensions, it is treated as a 2D convolutional kernel. The layout for such a kernel is `(out_channels, in_channels, kernel_height, kernel_width)`.\n   - Compute the receptive field size as the product of the last two dimensions (kernel height and width).\n   - Calculate fan-in as `in_channels * receptive_field_size`.\n   - Calculate fan-out as `out_channels * receptive_field_size`.\n\n2. **Handle Other Layers**: For any other layer type (e.g., dense layers, embedding matrices), directly use the first two dimensions of the shape for fan-in and fan-out respectively.\n\n### Solution Code\n```python\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    if len(shape) == 4:\n        out_channels, in_channels, kernel_height, kernel_width = shape\n        receptive_field_size = kernel_height * kernel_width\n        fan_in = in_channels * receptive_field_size\n        fan_out = out_channels * receptive_field_size\n    else:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    return (float(fan_in), float(fan_out))\n```\n\n### Explanation\n- **Convolutional Layer Handling**: When the input shape has 4 dimensions, the code extracts `out_channels`, `in_channels`, `kernel_height`, and `kernel_width`. The receptive field size is computed as the product of `kernel_height` and `kernel_width`. Fan-in and fan-out are then calculated using the respective channels multiplied by the receptive field size.\n- **Other Layers Handling**: For any other input shape, the first two elements are directly used as fan-in and fan-out. This applies to dense layers, embedding matrices, and higher-dimensional tensors where the first two dimensions represent input and output features respectively.\n\nThis approach ensures that the correct fan-in and fan-out values are computed efficiently based on the given shape, adhering to the rules specified for different types of neural network layers."}
{"task_id": 435, "completion_id": 0, "completion": "To solve this problem, we need to implement a simplified version of a Gradient Boosting Regressor using ordinary least-squares (OLS) linear regression as the weak learner and the squared error as the loss function. The algorithm iteratively improves predictions by fitting residual errors from previous iterations.\n\n### Approach\n1. **Convert Inputs**: Convert the input features `X` and target `y` into NumPy arrays for efficient computation.\n2. **Handle Special Cases**: If `n_estimators` is non-positive or `learning_rate` is zero, return predictions as the mean of `y` for all samples.\n3. **Initialize Predictions**: Start with initial predictions set to the mean of `y`.\n4. **Iterative Boosting**: For each boosting iteration:\n   - Compute residuals as the difference between actual values and current predictions.\n   - Fit an OLS regression model on the residuals, including an intercept term.\n   - Update predictions using the weak learner's predictions scaled by the learning rate.\n5. **Final Output**: Round the final predictions to four decimal places and convert them to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    X = np.array(X, dtype=np.float64)\n    y = np.array(y, dtype=np.float64)\n    m = X.shape[0]\n    mean_val = y.mean()\n    if n_estimators <= 0 or learning_rate == 0:\n        return [round(mean_val, 4)] * m\n    y_hat = np.full(m, mean_val, dtype=np.float64)\n    for _ in range(n_estimators):\n        r = y - y_hat\n        X_aug = np.hstack([np.ones((m, 1)), X])\n        theta = np.linalg.lstsq(X_aug, r, rcond=None)[0]\n        h = X_aug @ theta\n        y_hat += learning_rate * h\n    y_hat_rounded = np.round(y_hat, 4)\n    return y_hat_rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The input features `X` and target `y` are converted to NumPy arrays to leverage efficient numerical operations.\n2. **Special Case Handling**: If the number of estimators is non-positive or the learning rate is zero, the model returns the mean of the target values for all predictions.\n3. **Initialization**: The initial prediction for each sample is set to the mean of the target values.\n4. **Boosting Loop**: For each iteration:\n   - **Residual Calculation**: Compute the residuals as the difference between the actual target values and the current predictions.\n   - **OLS Model Fitting**: Fit an OLS regression model on the residuals. This involves augmenting the feature matrix with a column of ones to account for the intercept term.\n   - **Prediction Update**: Update the predictions using the weak learner's predictions scaled by the learning rate. This step gradually improves the model by correcting the residuals from previous iterations.\n5. **Final Predictions**: The final predictions are rounded to four decimal places and converted to a list to meet the output requirements.\n\nThis approach ensures that the model iteratively improves its predictions by focusing on the errors made in previous iterations, leading to a robust and accurate regressor even with a small number of estimators."}
{"task_id": 437, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that evaluates the logistic sigmoid activation function and its first or second derivatives based on the specified order. The function should handle scalar, list, and NumPy array inputs, returning the result in the appropriate type (float for scalar inputs and list for others) rounded to four decimal places. If an invalid order is specified, the function should return -1.\n\n### Approach\n1. **Input Handling**: The function first checks if the input order is valid (0, 1, or 2). If not, it returns -1 immediately.\n2. **Conversion to NumPy Array**: Convert the input to a NumPy array to handle both scalar and non-scalar inputs uniformly.\n3. **Sigmoid Calculation**: Compute the sigmoid function using vectorized operations for efficiency.\n4. **Derivative Calculation**: Depending on the order, compute the first or second derivative using the sigmoid values.\n5. **Rounding**: Round the results to four decimal places.\n6. **Return Type Handling**: Convert the result back to a float if the input was a scalar, otherwise return a list using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in {0, 1, 2}:\n        return -1\n    \n    original_is_scalar = np.isscalar(x)\n    x_arr = np.asarray(x)\n    s = 1 / (1 + np.exp(-x_arr))\n    \n    if order == 0:\n        res = s\n    elif order == 1:\n        res = s * (1 - s)\n    else:  # order == 2\n        res = s * (1 - s) * (1 - 2 * s)\n    \n    res_rounded = np.round(res, 4)\n    \n    if original_is_scalar:\n        return float(res_rounded)\n    else:\n        return res_rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function checks if the order is 0, 1, or 2. If not, it returns -1 immediately.\n2. **Conversion to Array**: The input is converted to a NumPy array to handle both scalar and non-scalar inputs uniformly.\n3. **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n4. **Derivative Calculation**: \n   - First derivative is calculated using \\( \\sigma(x)(1 - \\sigma(x)) \\).\n   - Second derivative uses \\( \\sigma(x)(1 - \\sigma(x))(1 - 2\\sigma(x)) \\).\n5. **Rounding**: Results are rounded to four decimal places using NumPy's `round` function.\n6. **Return Type**: The result is returned as a float for scalar inputs and as a list for non-scalar inputs using `tolist()` to preserve the original structure.\n\nThis approach ensures efficient computation using vectorized operations and handles different input types appropriately, providing the correct return type and structure."}
{"task_id": 438, "completion_id": 0, "completion": "To solve this problem, we need to convert a vector of class labels into a one-hot encoded matrix. The function should handle both list and NumPy array inputs and allow for an optional number of classes. If the number of classes is not provided, it should be inferred from the data. If the provided number of classes is insufficient, the function should return -1.\n\n### Approach\n1. **Input Handling**: Convert the input to a NumPy array if it's a list to ensure consistent processing.\n2. **Edge Case Handling**: If the input is an empty list, return an empty list.\n3. **Determine Number of Classes**: If the number of classes (`num_classes`) is not provided, calculate it as the maximum label in the input plus one. If `num_classes` is provided but is smaller than the required number of classes (determined by the maximum label plus one), return -1.\n4. **Matrix Creation**: Create a zero matrix of appropriate dimensions and set the corresponding positions to 1 based on the input labels.\n5. **Conversion to List**: Convert the resulting NumPy matrix to a list of lists before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    y_np = np.asarray(y)\n    if y_np.size == 0:\n        return []\n    \n    max_label = y_np.max()\n    \n    if num_classes is not None:\n        if num_classes < max_label + 1:\n            return -1\n        num_classes_ = num_classes\n    else:\n        num_classes_ = max_label + 1\n    \n    n_samples = y_np.shape[0]\n    matrix = np.zeros((n_samples, num_classes_), dtype=int)\n    rows = np.arange(n_samples)\n    matrix[rows, y_np] = 1\n    \n    return matrix.tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input `y` is converted to a NumPy array for uniform processing, whether it's a list or a NumPy array.\n2. **Empty Input Handling**: If the input is empty, an empty list is returned immediately.\n3. **Class Number Determination**: The maximum label in the input is found. If `num_classes` is not provided, it is set to this maximum label plus one. If `num_classes` is provided but too small, the function returns -1.\n4. **Matrix Construction**: A zero matrix is created with dimensions based on the number of samples and the determined number of classes. The appropriate positions in the matrix are set to 1 using vectorized indexing for efficiency.\n5. **Conversion to List**: The resulting matrix is converted to a list of lists using the `tolist()` method to meet the output requirements.\n\nThis approach ensures efficient handling of both list and array inputs, correctly infers or validates the number of classes, and efficiently constructs the one-hot encoded matrix using NumPy operations."}
{"task_id": 439, "completion_id": 0, "completion": "To solve this problem, we need to implement the Apriori algorithm to discover all frequent itemsets in a transactional database. The Apriori algorithm follows a breadth-first strategy to efficiently find itemsets that meet a minimum support threshold. \n\n### Approach\n1. **Generate Frequent 1-Itemsets**: Start by identifying all single-item itemsets that meet the minimum support threshold.\n2. **Generate Candidates**: For each subsequent level (k-itemsets), generate candidate itemsets by self-joining the frequent itemsets from the previous level.\n3. **Prune Candidates**: Remove candidates that have any subset which is not frequent in the previous level.\n4. **Check Support**: Calculate the support for each candidate and retain those that meet the minimum support threshold.\n5. **Repeat**: Continue generating and pruning candidates until no new frequent itemsets are found.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\nimport itertools\n\ndef apriori_frequent_itemsets(transactions: list[list], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\"\"\"\n    if not transactions:\n        return []\n    \n    n_transactions = len(transactions)\n    required_count = math.ceil(min_sup * n_transactions - 1e-12)\n    transaction_sets = [frozenset(trans) for trans in transactions]\n    \n    # Generate frequent 1-itemsets\n    item_counts = defaultdict(int)\n    for trans in transactions:\n        unique_items = set(trans)\n        for item in unique_items:\n            item_counts[item] += 1\n    \n    frequent_items = []\n    frequent_1 = [tuple([item]) for item in sorted(item for item, cnt in item_counts.items() if cnt >= required_count)]\n    frequent_items.extend(frequent_1)\n    prev_frequent = frequent_1\n    \n    k = 2\n    while True:\n        # Generate candidates via self-join\n        candidates = []\n        prev_sorted = sorted(prev_frequent)\n        len_prev = len(prev_sorted)\n        for i in range(len_prev):\n            itemset_i = prev_sorted[i]\n            for j in range(i + 1, len_prev):\n                itemset_j = prev_sorted[j]\n                if itemset_i[:-1] == itemset_j[:-1]:\n                    if itemset_i[-1] < itemset_j[-1]:\n                        new_candidate = itemset_i + (itemset_j[-1],)\n                        candidates.append(new_candidate)\n                else:\n                    break  # Since sorted, no more matches\n        \n        # Prune candidates with infrequent subsets\n        prev_frequent_set = set(prev_sorted)\n        pruned_candidates = []\n        for candidate in candidates:\n            valid = True\n            for i in range(len(candidate)):\n                subset = candidate[:i] + candidate[i+1:]\n                if subset not in prev_frequent_set:\n                    valid = False\n                    break\n            if valid:\n                pruned_candidates.append(candidate)\n        \n        # Compute support for pruned candidates\n        current_frequent = []\n        for candidate in pruned_candidates:\n            count = 0\n            for t_set in transaction_sets:\n                all_in = True\n                for item in candidate:\n                    if item not in t_set:\n                        all_in = False\n                        break\n                if all_in:\n                    count += 1\n            if count >= required_count:\n                current_frequent.append(candidate)\n        \n        if not current_frequent:\n            break\n        frequent_items.extend(current_frequent)\n        prev_frequent = current_frequent\n        k += 1\n    \n    # Sort by length and lex order\n    frequent_items.sort(key=lambda x: (len(x), x))\n    return frequent_items\n```\n\n### Explanation\n1. **Preprocessing**: Convert each transaction to a set to efficiently check for item presence. Calculate the required support count based on the minimum support threshold and the number of transactions.\n2. **Frequent 1-Itemsets**: Count occurrences of each item across all transactions and filter those that meet the support threshold.\n3. **Candidate Generation**: For each level `k`, generate candidate itemsets by joining frequent itemsets from the previous level that share the first `k-2` items.\n4. **Pruning**: Remove candidates that have any subset which is not frequent in the previous level.\n5. **Support Calculation**: For each candidate, count how many transactions contain all items in the candidate and retain those that meet the support threshold.\n6. **Sorting**: The final list of frequent itemsets is sorted by the length of the itemsets and lexicographically within each length group.\n\nThis approach efficiently explores potential frequent itemsets using the Apriori principle, ensuring that only viable candidates are considered at each step, thus reducing the search space and computational complexity."}
{"task_id": 440, "completion_id": 0, "completion": "To solve this problem, we need to aggregate the probability predictions from an ensemble of estimators (such as a Random Forest) by averaging the probabilities for each class across all estimators, then determining the class with the highest average probability for each sample. The solution involves converting the input list to a NumPy array for efficient computation, averaging the probabilities, rounding them, and then determining the predicted class indices.\n\n### Approach\n1. **Convert Input to NumPy Array**: Convert the nested list of predictions into a NumPy array to leverage efficient array operations.\n2. **Average Probabilities**: Compute the mean of the probabilities across all estimators for each sample and class.\n3. **Round Probabilities**: Round the averaged probabilities to four decimal places.\n4. **Determine Predicted Classes**: Use the argmax function to find the class index with the highest probability for each sample, breaking ties by selecting the smallest index in case of equality.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    np_preds = np.array(predictions)\n    avg_probs = np.mean(np_preds, axis=0)\n    rounded_probs = np.round(avg_probs, 4)\n    predicted_classes = np.argmax(rounded_probs, axis=1)\n    return (rounded_probs.tolist(), predicted_classes.tolist())\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input list is converted into a NumPy array, which allows efficient numerical operations. The shape of the array is `(n_estimators, n_samples, n_classes)`.\n2. **Averaging Probabilities**: Using `np.mean` with `axis=0`, we average the probabilities across all estimators, resulting in an array of shape `(n_samples, n_classes)`.\n3. **Rounding**: The averaged probabilities are rounded to four decimal places using `np.round`, ensuring each value is formatted correctly.\n4. **Argmax for Class Indices**: The `np.argmax` function is applied along the class axis (axis=1) to find the index of the highest probability for each sample. This function inherently handles ties by selecting the first occurrence of the maximum value, which aligns with the problem's requirements.\n\nThis approach efficiently handles the aggregation and computation using NumPy, ensuring both correctness and performance. The final results are converted back to Python lists to meet the output constraints."}
{"task_id": 444, "completion_id": 0, "completion": "To solve this problem, we need to compute the Radial Basis Function (RBF) kernel matrix between two collections of vectors. The RBF kernel is a popular choice in various machine learning algorithms, including Gaussian Processes and Support Vector Machines. The kernel measures the similarity between pairs of vectors using a scale parameter, which can be isotropic (single value) or anisotropic (different values for each feature).\n\n### Approach\n1. **Input Validation**: \n   - Convert input vectors into NumPy arrays for efficient computation.\n   - Ensure both input matrices have the same number of features (columns).\n   - Check the validity of the scale parameter (\u03c3). If \u03c3 is a list, ensure its length matches the number of features and all values are positive. If \u03c3 is a single value, ensure it is positive. If \u03c3 is None, compute it as the square root of half the number of features.\n\n2. **Kernel Computation**:\n   - Compute the pairwise differences between all vectors in the two matrices.\n   - Scale these differences by the \u03c3 values.\n   - Square the scaled differences, sum them up for each vector pair, and apply the exponential function to the negative half of these sums.\n\n3. **Result Formatting**:\n   - Round the resulting kernel matrix values to four decimal places and convert them to a nested Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\"\"\"\n    # Convert X and Y to numpy arrays\n    X_np = np.array(X)\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.array(Y)\n    \n    # Check that X and Y are 2D arrays with matching feature dimensions\n    if X_np.ndim != 2 or Y_np.ndim != 2:\n        return -1\n    if X_np.shape[1] != Y_np.shape[1]:\n        return -1\n    \n    C = X_np.shape[1]  # Number of features\n    \n    # Validate and process sigma\n    if sigma is None:\n        sigma_val = np.sqrt(C / 2.0)\n    elif isinstance(sigma, (float, int)):\n        if sigma <= 0:\n            return -1\n        sigma_val = sigma\n    elif isinstance(sigma, list):\n        if len(sigma) != C:\n            return -1\n        try:\n            sigma_arr = np.array(sigma, dtype=np.float64)\n        except (ValueError, TypeError):\n            return -1\n        if (sigma_arr <= 0).any():\n            return -1\n        sigma_val = sigma_arr\n    else:\n        # Sigma is of invalid type\n        return -1\n    \n    # Compute pairwise differences\n    diff = X_np[:, np.newaxis, :] - Y_np[np.newaxis, :, :]\n    \n    # Scale differences by sigma_val and compute squared sum\n    scaled = diff / sigma_val\n    squared = scaled ** 2\n    sum_sq = squared.sum(axis=2)\n    \n    # Compute RBF kernel and round to 4 decimal places\n    k = np.exp(-0.5 * sum_sq)\n    rounded = np.round(k, decimals=4)\n    \n    return rounded.tolist()\n```\n\n### Explanation\n1. **Input Handling**: The input vectors are converted to NumPy arrays to leverage efficient array operations. If the second matrix Y is not provided, it defaults to the first matrix X.\n\n2. **Validation Checks**: \n   - Ensure both matrices are 2D and have the same number of features.\n   - Validate the scale parameter \u03c3 to ensure it meets the specified criteria (positive values, correct dimensions).\n\n3. **Kernel Calculation**:\n   - Compute the pairwise differences between vectors using broadcasting.\n   - Scale the differences by \u03c3, square them, sum over features, and apply the exponential function to get the kernel values.\n\n4. **Result Formatting**: The kernel matrix values are rounded to four decimal places and converted to a nested list to meet the output requirements.\n\nThis approach efficiently handles both isotropic and anisotropic scale parameters, ensuring accurate and efficient computation of the RBF kernel matrix."}
{"task_id": 446, "completion_id": 0, "completion": "To solve this problem, we need to implement a function called `decision_stump` that finds the best single-level split (decision stump) for a given feature matrix and label vector based on information gain. The goal is to split the data such that the decrease in entropy (information gain) is maximized.\n\n### Approach\n1. **Entropy Calculation**: Compute the entropy of a set of labels, which measures the impurity or uncertainty. The entropy is calculated using the formula \\( H = -\\sum (p_i \\log_2 p_i) \\), where \\( p_i \\) is the proportion of class \\( i \\) in the set.\n\n2. **Majority Label Determination**: Determine the majority label in a subset of labels. If there is a tie, the smallest label is chosen.\n\n3. **Parent Entropy Check**: Calculate the entropy of the entire dataset (parent node). If the entropy is zero (all labels are the same), return the no-split case.\n\n4. **Feature and Threshold Evaluation**: For each feature, generate possible thresholds as midpoints between consecutive unique values. For each threshold, split the data and compute the information gain. Track the best split based on the highest information gain, resolving ties by feature index and threshold value.\n\n5. **Result Determination**: If no split improves information gain, return the no-split case. Otherwise, return the best split details.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    n_samples, n_features = X.shape\n\n    def entropy(labels):\n        if len(labels) == 0:\n            return 0.0\n        _, counts = np.unique(labels, return_counts=True)\n        probs = counts / counts.sum()\n        return -np.sum(probs * np.log2(probs))\n\n    def majority_label(labels):\n        if len(labels) == 0:\n            return 0  # Should not occur in valid splits\n        values, counts = np.unique(labels, return_counts=True)\n        max_count = np.max(counts)\n        candidates = values[counts == max_count]\n        return np.min(candidates)\n\n    parent_entropy = entropy(y)\n    if parent_entropy == 0.0:\n        majority = majority_label(y)\n        return (-1, None, majority, majority)\n\n    overall_majority = majority_label(y)\n    current_max_gain = -np.inf\n    best_feature_index = -1\n    best_threshold = None\n    left_maj = None\n    right_maj = None\n\n    for j in range(n_features):\n        feature_values = X[:, j]\n        unique_values = np.unique(feature_values)\n        if len(unique_values) < 2:\n            continue\n        sorted_values = np.sort(unique_values)\n        thresholds = (sorted_values[:-1] + sorted_values[1:]) / 2\n        for t in thresholds:\n            left_mask = feature_values <= t\n            n_left = np.sum(left_mask)\n            if n_left == 0 or n_left == n_samples:\n                continue\n            left_y = y[left_mask]\n            right_y = y[~left_mask]\n            h_left = entropy(left_y)\n            h_right = entropy(right_y)\n            p_left = n_left / n_samples\n            p_right = (n_samples - n_left) / n_samples\n            gain = parent_entropy - (p_left * h_left + p_right * h_right)\n\n            if gain > current_max_gain:\n                current_max_gain = gain\n                best_feature_index = j\n                best_threshold = t\n                left_maj = majority_label(left_y)\n                right_maj = majority_label(right_y)\n            elif gain == current_max_gain:\n                if j < best_feature_index:\n                    best_feature_index = j\n                    best_threshold = t\n                    left_maj = majority_label(left_y)\n                    right_maj = majority_label(right_y)\n                elif j == best_feature_index and t < best_threshold:\n                    best_threshold = t\n                    left_maj = majority_label(left_y)\n                    right_maj = majority_label(right_y)\n\n    if current_max_gain <= 0.0:\n        return (-1, None, overall_majority, overall_majority)\n    else:\n        rounded_threshold = round(best_threshold, 4)\n        return (best_feature_index, rounded_threshold, left_maj, right_maj)\n```\n\n### Explanation\n1. **Entropy Calculation**: The entropy function calculates the entropy of a given set of labels, which helps measure the impurity. Lower entropy indicates a more homogeneous set.\n\n2. **Majority Label**: This function determines the most frequent label in a subset, breaking ties by choosing the smallest label.\n\n3. **Parent Entropy Check**: If all labels are the same, the entropy is zero, and we return the no-split case immediately.\n\n4. **Feature and Threshold Evaluation**: For each feature, possible thresholds are generated as midpoints between consecutive unique values. Each threshold is evaluated by splitting the data and calculating the information gain. The best split is tracked based on the highest gain, with tiebreakers for feature index and threshold.\n\n5. **Result Determination**: If no valid split improves information gain, the no-split case is returned. Otherwise, the best split details are returned with the threshold rounded to four decimal places."}
{"task_id": 452, "completion_id": 0, "completion": "To solve this problem, we need to split a given dataset into two subsets based on a specified feature column and a threshold value. The split criteria depend on whether the threshold is numeric (int or float) or non-numeric (categorical). For numeric thresholds, samples with values greater than or equal to the threshold form the first subset, while for non-numeric thresholds, samples with values exactly equal to the threshold form the first subset. The order of samples in each subset must be preserved, and both subsets should be returned as Python lists.\n\n### Approach\n1. **Input Handling**: Convert the input dataset to a NumPy array if it is provided as a list of lists. This allows uniform handling of both list and array inputs.\n2. **Threshold Type Check**: Determine if the threshold is numeric (exactly int or float) or non-numeric. This check is crucial as it dictates the split criteria.\n3. **Splitting the Dataset**: Iterate through each sample in the dataset. For each sample, check the value at the specified feature column. Depending on the threshold type, apply the appropriate condition (>= for numeric, == for non-numeric) to split the samples into two groups.\n4. **Conversion to Python Lists**: Convert the resulting subsets from NumPy arrays to Python lists while preserving the order of samples.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import List\n\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\"\"\"\n    X_array = np.array(X) if isinstance(X, list) else X\n    left = []\n    right = []\n    is_numeric = type(threshold) in (int, float)\n    \n    for row in X_array:\n        value = row[feature_i]\n        if is_numeric:\n            if value >= threshold:\n                left.append(row)\n            else:\n                right.append(row)\n        else:\n            if value == threshold:\n                left.append(row)\n            else:\n                right.append(row)\n    \n    left_list = [r.tolist() for r in left]\n    right_list = [r.tolist() for r in right]\n    return [left_list, right_list]\n```\n\n### Explanation\n1. **Input Conversion**: The dataset `X` is converted to a NumPy array if it is provided as a list. This ensures consistent handling of rows and columns.\n2. **Threshold Check**: The type of the threshold is checked to determine if it is numeric (int or float). This check uses the exact type rather than `isinstance` to avoid treating boolean values as numeric.\n3. **Iterating Through Samples**: Each row of the dataset is examined. The value at the specified feature column is compared against the threshold using the appropriate condition (>= for numeric, == for non-numeric) to split the data into two lists, `left` and `right`.\n4. **Conversion to Lists**: The resulting subsets, stored as lists of NumPy arrays, are converted to Python lists using `tolist()` to meet the output requirements.\n\nThis approach ensures that the dataset is split correctly according to the given threshold while preserving the original order of samples and converting the results to the required format."}
{"task_id": 453, "completion_id": 0, "completion": "To solve this problem, we need to compute the target Q-values for training a Deep Q-Network (DQN). The targets are derived from the current Q-values, the next state Q-values, the rewards received, and whether the next state is terminal. The solution involves validating the input parameters and then updating the Q-values for the actions taken in each state.\n\n### Approach\n1. **Input Validation**: Check if the discount factor `gamma` is within the valid range [0, 1]. Verify that the shapes of the Q-value matrices `Q` and `Q_next` are consistent. Ensure that the lengths of the lists `actions`, `rewards`, and `dones` match the batch size inferred from the Q-value matrices.\n2. **Target Calculation**: For each state in the batch, compute the target Q-value. If the next state is terminal (`dones[i]` is True), the target is the immediate reward. Otherwise, the target is the immediate reward plus the discounted maximum Q-value of the next state.\n3. **Update Q-values**: Copy the current Q-values and update only the entries corresponding to the actions taken with the computed targets.\n4. **Rounding and Conversion**: Round the updated Q-values to four decimal places and convert the result to a nested list structure.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # Check if gamma is within the valid range\n    if gamma < 0 or gamma > 1:\n        return -1\n    \n    # Check Q and Q_next are 2D arrays with the same shape\n    if Q.ndim != 2 or Q_next.ndim != 2 or Q.shape != Q_next.shape:\n        return -1\n    \n    batch_size = Q.shape[0]\n    \n    # Check actions, rewards, dones have length equal to batch_size\n    if len(actions) != batch_size or len(rewards) != batch_size or len(dones) != batch_size:\n        return -1\n    \n    # Create a copy of Q to modify\n    y = Q.copy()\n    \n    for i in range(batch_size):\n        done = dones[i]\n        reward = rewards[i]\n        action = actions[i]\n        \n        if done:\n            target = reward\n        else:\n            next_max = np.max(Q_next[i])\n            target = reward + gamma * next_max\n        \n        # Update the corresponding action's Q-value\n        y[i, action] = target\n    \n    # Round to 4 decimal places and convert to list of lists\n    y_rounded = np.round(y, 4)\n    return y_rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the discount factor `gamma` is within the valid range. It then ensures that the Q-value matrices `Q` and `Q_next` have the same shape and that the lengths of `actions`, `rewards`, and `dones` match the batch size.\n2. **Target Calculation**: For each state in the batch, the target Q-value is computed based on whether the next state is terminal. If it is terminal, the target is just the reward; otherwise, it includes the discounted maximum Q-value of the next state.\n3. **Update Q-values**: The function updates only the Q-value corresponding to the action taken in each state, leaving other Q-values unchanged.\n4. **Rounding and Conversion**: The updated Q-values are rounded to four decimal places and converted to a nested list structure to meet the output requirements.\n\nThis approach ensures that the Q-values are correctly updated for training the DQN while adhering to the constraints and requirements specified."}
{"task_id": 458, "completion_id": 0, "completion": "To solve this problem, we need to convert a one-dimensional array-like object of non-negative integer labels into a two-dimensional one-hot encoded matrix. The one-hot encoding is a binary matrix where each row has a single `1` at the index corresponding to the original class label, and `0` elsewhere. \n\n### Approach\n1. **Input Handling**: Convert the input labels into a numpy array for easier manipulation, regardless of whether the input is a list, tuple, or numpy array.\n2. **Check for Negative Labels**: If any label is negative, return -1 immediately as negative labels are invalid.\n3. **Determine Number of Classes**: If the number of classes (`num_classes`) is not provided, infer it as the maximum label value plus one. If `num_classes` is provided but is insufficient (smaller than the maximum label value plus one), return -1.\n4. **Generate One-Hot Encoded Matrix**: Create a list of lists where each row corresponds to a label, filled with `0`s except for a `1` at the index of the label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    labels_np = np.array(labels)\n    \n    # Handle empty labels case\n    if labels_np.size == 0:\n        return []\n    \n    # Check for any negative labels\n    if (labels_np < 0).any():\n        return -1\n    \n    max_label = labels_np.max()\n    \n    # Determine num_classes and validate if provided\n    if num_classes is not None:\n        if num_classes < max_label + 1:\n            return -1\n    else:\n        num_classes = max_label + 1\n    \n    # Generate one-hot encoded lists\n    result = []\n    for label in labels_np:\n        row = [0] * num_classes\n        row[label] = 1\n        result.append(row)\n    \n    return result\n```\n\n### Explanation\n1. **Input Conversion**: The input labels are converted to a numpy array to handle all input types uniformly.\n2. **Negative Label Check**: Using numpy operations, we efficiently check for any negative values in the labels.\n3. **Number of Classes Calculation**: The number of classes is either inferred from the maximum label value or validated against the provided value. If the provided value is too small, the function returns -1.\n4. **Matrix Construction**: For each label, a row of zeros is created with a `1` at the index corresponding to the label. This row is then added to the result list.\n\nThis approach ensures that the function efficiently handles all edge cases, including empty input and invalid class numbers, while converting the labels into the required one-hot encoded format."}
{"task_id": 461, "completion_id": 0, "completion": "To solve this problem, we need to implement a gradient boosting regressor using decision stumps (depth-1 regression trees) for one-dimensional data. The algorithm uses square loss as the objective function and updates the predictions iteratively by fitting decision stumps to the residuals of the current predictions.\n\n### Approach\n1. **Initial Prediction**: Start with the mean of the target values as the initial prediction for all samples.\n2. **Iterative Boosting**: For each boosting iteration (up to `n_estimators` times):\n   - **Residual Calculation**: Compute the residuals as the difference between the true target values and the current predictions.\n   - **Early Stopping**: If all residuals are zero, stop the iterations early.\n   - **Split Candidates**: Generate potential split points from the unique values of the feature, excluding the maximum value to avoid empty splits.\n   - **Best Split Selection**: For each candidate split, compute the sum of squared errors (SSE) and select the split that minimizes SSE. If no valid splits are found, use the mean residual for the entire dataset.\n   - **Update Predictions**: Adjust the current predictions using the decision stump predictions scaled by the learning rate.\n3. **Final Predictions**: Return the final predictions rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    x = np.array(x)\n    y = np.array(y)\n    n_samples = len(x)\n    if n_samples == 0:\n        return []\n    y_pred = np.full(n_samples, np.mean(y))\n    if n_estimators <= 0:\n        return np.round(y_pred, 4).tolist()\n    for _ in range(n_estimators):\n        r = y - y_pred\n        if np.all(r == 0):\n            break\n        unique_x = np.unique(x)\n        split_candidates = unique_x[:-1] if len(unique_x) > 1 else []\n        best_sse = float('inf')\n        best_t = None\n        best_vL = None\n        best_vR = None\n        sum_r = np.sum(r)\n        sum_r_sq = np.sum(r ** 2)\n        if len(split_candidates) == 0:\n            v = np.mean(r)\n            stump_pred = np.full(n_samples, v)\n        else:\n            for t in split_candidates:\n                left_mask = x <= t\n                count_left = np.sum(left_mask)\n                if count_left == 0 or count_left == n_samples:\n                    continue\n                sum_left = np.sum(r[left_mask])\n                sum_left_sq = np.sum(r[left_mask] ** 2)\n                count_right = n_samples - count_left\n                sum_right = sum_r - sum_left\n                sum_right_sq = sum_r_sq - sum_left_sq\n                sse_left = sum_left_sq - (sum_left ** 2) / count_left\n                sse_right = sum_right_sq - (sum_right ** 2) / count_right\n                total_sse = sse_left + sse_right\n                if (total_sse < best_sse) or (total_sse == best_sse and (best_t is None or t < best_t)):\n                    best_sse = total_sse\n                    best_t = t\n                    best_vL = sum_left / count_left\n                    best_vR = sum_right / count_right\n            if best_t is not None:\n                stump_pred = np.where(x <= best_t, best_vL, best_vR)\n            else:\n                v = np.mean(r)\n                stump_pred = np.full(n_samples, v)\n        y_pred += learning_rate * stump_pred\n    return np.round(y_pred, 4).tolist()\n```\n\n### Explanation\n1. **Initial Prediction**: The initial prediction for all samples is set to the mean of the target values.\n2. **Residual Calculation**: For each iteration, residuals are computed as the difference between the actual target values and the current predictions.\n3. **Split Candidates**: Potential split points are derived from the unique values of the feature, excluding the maximum value to ensure non-empty splits.\n4. **Best Split Selection**: For each candidate split, the sum of squared errors (SSE) is calculated. The split with the minimum SSE is chosen. If no valid splits are found, the mean residual is used.\n5. **Prediction Update**: The predictions are updated using the decision stump predictions scaled by the learning rate.\n6. **Early Stopping**: If all residuals become zero, the iterations stop early to save computation.\n\nThis approach efficiently builds an ensemble of decision stumps to iteratively improve predictions, leveraging gradient boosting principles for regression tasks."}
{"task_id": 471, "completion_id": 0, "completion": "To solve this problem, we need to implement the Expected SARSA TD(0) algorithm for updating a Q-table in a tabular setting. The algorithm involves computing the expected value of the next state using an \u03b5-soft policy derived from the current Q-table and updating the Q-value of the current state-action pair accordingly.\n\n### Approach\n1. **Copy the Q-table**: Create a deep copy of the Q-table to avoid modifying the original input directly.\n2. **Compute the target value**:\n   - If the next state is terminal (i.e., `next_state` is `None`), the target is just the immediate reward.\n   - If the next state is non-terminal, compute the expected value of the Q-values for the next state using the \u03b5-soft policy. This involves determining the greedy action (the first action with the maximum Q-value) and calculating the probabilities for each action based on the \u03b5-soft policy.\n3. **Update the Q-value**: Adjust the Q-value for the current state-action pair using the learning rate and the computed target.\n4. **Round the values**: Ensure all values in the updated Q-table are rounded to four decimal places for consistency.\n\n### Solution Code\n```python\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    # Make a copy of the Q-table to modify\n    new_q = [row.copy() for row in q_table]\n    \n    # Compute the target\n    if next_state is None:\n        target = reward\n    else:\n        s_prime = next_state\n        q_s_prime = new_q[s_prime]\n        num_actions = len(q_s_prime)\n        max_q = max(q_s_prime)\n        a_greedy = q_s_prime.index(max_q)  # first occurrence of max_q\n        \n        expectation = 0.0\n        for a_prime in range(num_actions):\n            if a_prime == a_greedy:\n                prob = (1 - epsilon) + (epsilon / num_actions)\n            else:\n                prob = epsilon / num_actions\n            expectation += prob * q_s_prime[a_prime]\n        \n        target = reward + gamma * expectation\n    \n    # Update the Q-value\n    new_q[state][action] += lr * (target - new_q[state][action])\n    \n    # Round all entries to four decimal places\n    for row in new_q:\n        for i in range(len(row)):\n            row[i] = round(row[i], 4)\n    \n    return new_q\n```\n\n### Explanation\n1. **Copying the Q-table**: This step ensures that we do not modify the original Q-table, allowing the function to return a new updated table without side effects.\n2. **Handling terminal state**: If the next state is terminal, the future value is zero, so the target is just the immediate reward.\n3. **Non-terminal state processing**: For non-terminal states, the algorithm calculates the expected value using the \u03b5-soft policy. The greedy action is identified as the first action with the highest Q-value, and probabilities for each action are computed based on whether they are greedy or not.\n4. **Updating the Q-value**: The Q-value for the current state-action pair is adjusted using the learning rate and the difference between the target and the current Q-value.\n5. **Rounding**: All values in the Q-table are rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the necessary updates while adhering to the constraints and requirements of the Expected SARSA algorithm."}
{"task_id": 474, "completion_id": 0, "completion": "To solve this problem, we need to generate a 2-D toy dataset suitable for a binary or multi-class classification problem. The dataset should be perfectly separable with each class arranged in a rectangular cluster. The solution must address the issues of hard-coded parameters and missing validation labels in the original code, while ensuring reproducibility and proper formatting.\n\n### Approach\n1. **Parameter Handling**: The function should accept parameters for the number of classes, training and validation samples per class, vertical interval between clusters, and an optional random seed.\n2. **Data Generation**: For each class, generate training and validation samples with specific ranges for x and y coordinates based on the class index. The labels are determined using a formula derived from the class index.\n3. **Reproducibility**: Use the provided seed to initialize the random number generator for reproducibility.\n4. **Rounding**: Round all coordinates to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\"\"\"\n\n    if seed is not None:\n        random.seed(seed)\n\n    X_train = []\n    X_val = []\n    Y_train = []\n    Y_val = []\n\n    for i in range(m):\n        label = (i - 0.5) * 2\n        x_floor = i // 2\n        x_low = (x_floor + 0.1) * 10\n        x_high = (x_floor + 0.9) * 10\n\n        y_mod = i % 2\n        y_low = (y_mod * interval + 0.1) * 10\n        y_high = (y_mod * interval + 0.9) * 10\n\n        # Generate training samples\n        for _ in range(n_train):\n            x = random.uniform(x_low, x_high)\n            y = random.uniform(y_low, y_high)\n            X_train.append([x, y])\n            Y_train.append(label)\n\n        # Generate validation samples\n        for _ in range(n_val):\n            x = random.uniform(x_low, x_high)\n            y = random.uniform(y_low, y_high)\n            X_val.append([x, y])\n            Y_val.append(label)\n\n    # Convert to numpy arrays and round coordinates\n    X_train = np.round(np.array(X_train), 4)\n    X_val = np.round(np.array(X_val), 4)\n    Y_train = np.array(Y_train)\n    Y_val = np.array(Y_val)\n\n    return X_train, X_val, Y_train, Y_val\n```\n\n### Explanation\n1. **Parameter Handling**: The function accepts parameters for the number of classes (`m`), training and validation samples per class (`n_train`, `n_val`), vertical interval (`interval`), and an optional seed for reproducibility.\n2. **Random Seed Initialization**: If a seed is provided, it initializes the random number generator using `random.seed(seed)` to ensure reproducible results.\n3. **Class Loop**: For each class index `i`, the function calculates the x and y coordinate ranges based on the class index and the given interval. The label for each class is determined using the formula `(i - 0.5) * 2`.\n4. **Sample Generation**: For each class, training and validation samples are generated using uniform distribution within the calculated ranges. These samples are collected into lists.\n5. **Array Conversion and Rounding**: The collected samples are converted to NumPy arrays and rounded to four decimal places to meet the output constraints.\n\nThis approach ensures that the generated data is reproducible, correctly formatted, and suitable for training and validating classification models."}
{"task_id": 475, "completion_id": 0, "completion": "To solve this problem, we need to implement the Adam optimization algorithm for a single update step. Adam is an optimization technique that adapts the learning rate for each parameter using estimates of the first and second moments of the gradients. This approach combines the advantages of two other extensions of stochastic gradient descent: AdaGrad and RMSProp.\n\n### Approach\n1. **Initialization of Moments**: If the previous first moment (m_prev) or the previous second moment (v_prev) is None, initialize them as zeros with the same shape as the gradient (grad).\n2. **Update Moments**: Compute the new first and second moments using exponential moving averages. The first moment is a weighted average of the gradient, and the second moment is a weighted average of the squared gradient.\n3. **Bias Correction**: Adjust the moments to account for their initialization bias. This correction is necessary because the initial moments (especially at early time steps) are biased towards zero.\n4. **Parameter Update**: Use the bias-corrected moments to update the parameters. The learning rate is adjusted by the ratio of the bias-corrected first moment to the square root of the bias-corrected second moment, with a small constant added for numerical stability.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    # Handle None values for previous moments\n    m_prev = np.zeros_like(grad) if m_prev is None else m_prev\n    v_prev = np.zeros_like(grad) if v_prev is None else v_prev\n    \n    # Update first and second moments\n    m_new = b1 * m_prev + (1 - b1) * grad\n    v_new = b2 * v_prev + (1 - b2) * (grad ** 2)\n    \n    # Compute bias-corrected moments\n    m_hat = m_new / (1 - b1 ** t)\n    v_hat = v_new / (1 - b2 ** t)\n    \n    # Update parameters\n    w_new = w - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n    \n    return w_new, m_new, v_new\n```\n\n### Explanation\n1. **Initialization**: The moments (m_prev and v_prev) are initialized to zero if they are None, ensuring they match the shape of the gradient.\n2. **Moment Updates**: The first moment (m_new) is updated using a weighted average of the previous first moment and the current gradient. The second moment (v_new) is updated similarly using the squared gradient.\n3. **Bias Correction**: The moments are adjusted to counteract their initialization bias, especially important in the initial steps. The correction factor is derived from the exponential decay rates (b1 and b2) raised to the power of the current time step (t).\n4. **Parameter Update**: The parameters are updated by adjusting them in the direction of the first moment, scaled by the learning rate and normalized by the square root of the second moment. A small constant (eps) is added to the denominator to prevent division by zero and ensure numerical stability.\n\nThis approach efficiently combines adaptive learning rates with momentum, making Adam a robust choice for optimization in various machine learning tasks."}
{"task_id": 479, "completion_id": 0, "completion": "To solve this problem, we need to simulate an \u03b5-greedy algorithm for the stationary multi-armed bandit problem. The goal is to determine the final estimated expected rewards for each arm after following the \u03b5-greedy strategy, which balances exploration and exploitation.\n\n### Approach\n1. **Initialization**: \n   - Seed the random number generator if a seed is provided to ensure reproducibility.\n   - Initialize the estimated values (V) for each arm to a given prior value (ev_prior).\n   - Initialize a counter (C) for each arm to track the number of times each arm has been pulled.\n\n2. **Iterate through Time Steps**:\n   - For each time step, decide whether to explore (choose a random arm) with probability \u03b5 or exploit (choose the arm with the highest current estimated value).\n   - Retrieve the reward for the chosen arm at the current time step.\n   - Update the counter for the chosen arm.\n   - Adjust the estimated value of the chosen arm using the incremental sample mean formula to incorporate the new reward.\n\n3. **Final Output**:\n   - After processing all time steps, return the estimated values for each arm, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\"\"\"\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    if not rewards:\n        return []\n    \n    n_arms = len(rewards[0])\n    V = np.full(n_arms, ev_prior, dtype=np.float64)\n    C = np.zeros(n_arms, dtype=np.int64)\n    \n    for t in range(len(rewards)):\n        current_rewards = rewards[t]\n        \n        if np.random.rand() < epsilon:\n            chosen_arm = np.random.randint(n_arms)\n        else:\n            chosen_arm = np.argmax(V)\n        \n        reward = current_rewards[chosen_arm]\n        \n        C[chosen_arm] += 1\n        V[chosen_arm] += (reward - V[chosen_arm]) / C[chosen_arm]\n    \n    return [round(val, 4) for val in V.tolist()]\n```\n\n### Explanation\n- **Initialization**: The estimated values (V) start with a prior value, and the counters (C) are initialized to zero. This setup ensures that each arm begins with the same initial estimate and no prior pulls.\n- **Exploration vs. Exploitation**: At each time step, the algorithm either explores by randomly selecting an arm (with probability \u03b5) or exploits by selecting the arm with the highest current estimate (with probability 1-\u03b5). This balance allows the algorithm to discover potentially better arms while maximizing rewards based on current knowledge.\n- **Incremental Update**: After selecting an arm, the reward is observed, the counter for that arm is incremented, and the estimated value is updated using the incremental mean formula. This ensures that the estimate adapts dynamically as more rewards are observed.\n- **Final Output**: The estimated values are rounded to four decimal places to meet the problem's precision requirements, providing a clear and concise result.\n\nThis approach efficiently balances exploration and exploitation, adapting to the observed rewards to provide accurate estimates for each arm's expected reward."}
{"task_id": 481, "completion_id": 0, "completion": "To solve this problem, we need to implement the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm from scratch. The goal is to cluster points based on their density, marking noise points that do not belong to any cluster.\n\n### Approach\nThe DBSCAN algorithm works by identifying core points, which have at least a specified number of neighbors within a given radius (eps). These core points form the basis of clusters, which are expanded to include all reachable points (both core and border points). Border points are non-core points that are within the neighborhood of a core point. Noise points are those that are neither core nor border points.\n\nThe steps to implement DBSCAN are as follows:\n1. **Initialization**: Start with all points unprocessed and no clusters.\n2. **Core Point Identification**: For each point, check if it has enough neighbors within the radius to be a core point.\n3. **Cluster Expansion**: For each core point, expand the cluster by adding all reachable points (core and border) using a breadth-first search (BFS) approach.\n4. **Labeling**: Assign cluster labels to core and border points, and label noise points as -1.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if len(data) == 0:\n        return []\n    \n    labels = [-2] * len(data)\n    cluster_id = 0\n    \n    for i in range(len(data)):\n        if labels[i] != -2:\n            continue\n        \n        # Calculate all points within eps distance of data[i]\n        distances = np.linalg.norm(data - data[i], axis=1)\n        neighbors = np.where(distances <= eps)[0]\n        \n        if len(neighbors) < min_samples:\n            labels[i] = -1\n            continue\n        \n        # Start a new cluster\n        labels[i] = cluster_id\n        queue = deque(neighbors.tolist())\n        \n        while queue:\n            j = queue.popleft()\n            \n            if labels[j] == -1:\n                labels[j] = cluster_id\n            elif labels[j] == -2:\n                labels[j] = cluster_id\n                # Check if j is a core point\n                j_distances = np.linalg.norm(data - data[j], axis=1)\n                j_neighbors = np.where(j_distances <= eps)[0]\n                \n                if len(j_neighbors) >= min_samples:\n                    # Add all j's neighbors to the queue if they are unprocessed or noise\n                    for n in j_neighbors:\n                        if labels[n] == -2 or labels[n] == -1:\n                            if labels[n] == -2:\n                                queue.append(n)\n                            labels[n] = cluster_id\n        cluster_id += 1\n    \n    # Ensure any remaining unprocessed points are marked as noise\n    # In the algorithm, all reachable points are processed during expansion, so remaining -2 are impossible\n    # However, during the loop, all points are processed, so no need for this step.\n    return labels\n```\n\n### Explanation\n1. **Initialization**: We start with all points labeled as unprocessed (-2) and initialize the cluster ID to 0.\n2. **Core Point Check**: For each point, we compute the Euclidean distance to all other points to find neighbors within the given radius (eps). If a point has fewer neighbors than the required minimum (min_samples), it is marked as noise (-1).\n3. **Cluster Expansion**: If a point is identified as a core point, we start a new cluster and use a BFS approach to expand the cluster. This involves processing all reachable points (both core and border) from the initial core point, ensuring all reachable points are added to the cluster.\n4. **Queue Processing**: The BFS queue processes each neighbor, checking if it is a core point. If it is, its neighbors are added to the queue for further expansion. This ensures all density-reachable points are included in the cluster.\n\nThis approach efficiently clusters points based on density and correctly labels noise points, adhering to the DBSCAN algorithm's requirements."}
{"task_id": 482, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that converts a vector of integer class labels into a 2-D array where each row is the one-hot encoded representation of the corresponding label. The function should handle both specified and inferred numbers of classes, ensuring that all labels are valid and within the specified range.\n\n### Approach\n1. **Input Conversion**: Convert the input labels into a NumPy array for easier manipulation.\n2. **Validation Checks**: \n   - Ensure all labels are non-negative.\n   - If the number of classes (`num_classes`) is provided, check that all labels are within the valid range [0, num_classes - 1].\n3. **Determine Number of Classes**: If `num_classes` is not provided, calculate it as the maximum label value plus one.\n4. **One-Hot Encoding**: Create a zero-initialized 2-D array and set the appropriate positions to 1 based on the labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    y = np.asarray(y)\n    if np.any(y < 0):\n        raise ValueError(\"Labels must be non-negative.\")\n    \n    if num_classes is not None:\n        if not isinstance(num_classes, int) or num_classes < 0:\n            raise ValueError(\"num_classes must be a non-negative integer\")\n        if np.any(y >= num_classes):\n            raise ValueError(\"Labels must be less than num_classes.\")\n    else:\n        if y.size == 0:\n            num_classes = 0\n        else:\n            num_classes = np.max(y) + 1\n    \n    n_samples = y.shape[0]\n    categorical = np.zeros((n_samples, num_classes), dtype=int)\n    \n    if n_samples > 0 and num_classes > 0:\n        categorical[np.arange(n_samples), y] = 1\n    \n    return categorical\n```\n\n### Explanation\n1. **Input Conversion**: The input labels are converted to a NumPy array using `np.asarray(y)` to ensure uniform handling.\n2. **Validation for Non-Negative Labels**: A check ensures all labels are non-negative, raising a `ValueError` if any negative label is found.\n3. **Handling `num_classes`**:\n   - If `num_classes` is provided, it validates that all labels are within the range [0, num_classes - 1].\n   - If `num_classes` is not provided, it is inferred as the maximum label value plus one. Special handling for empty input ensures the function works correctly with zero samples.\n4. **One-Hot Encoding**: A zero matrix of appropriate shape is created. Using advanced indexing, the positions corresponding to each label are set to 1, efficiently creating the one-hot encoded matrix.\n\nThis approach ensures that the function is both efficient and robust, handling edge cases such as empty input and invalid labels appropriately."}
{"task_id": 485, "completion_id": 0, "completion": "To solve this problem, we need to implement a univariate regression tree using the CART algorithm. The tree will be built by recursively splitting the data based on the feature that minimizes the sum of squared errors (SSE) and will stop splitting when the maximum depth is reached or a node has fewer than two samples. Each leaf node will predict the mean of the target values in that node. The predictions for test points are made by traversing the tree.\n\n### Approach\n1. **Check for Empty Training Data**: If the training data is empty, return an empty list of predictions.\n2. **Sort Training Data**: Sort the training data based on the feature values to facilitate efficient splitting.\n3. **Recursive Tree Construction**: Build the tree recursively by finding the best split at each node, which minimizes the SSE. The best split is determined by iterating through possible split points between sorted feature values.\n4. **Leaf Node Creation**: Create leaf nodes when the maximum depth is reached or a node has fewer than two samples, storing the mean of the target values.\n5. **Prediction**: Traverse the tree for each test point to predict the target value based on the learned splits and leaf node means.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\"\"\"\n\n    if not X_train:\n        return []\n\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n\n    # Sort the training data based on X values\n    sorted_indices = np.argsort(X_train)\n    X_sorted = X_train[sorted_indices]\n    y_sorted = y_train[sorted_indices]\n\n    def build_tree(X, y, current_depth):\n        # Base cases: max depth reached or node has fewer than two samples\n        if current_depth >= max_depth or len(X) < 2:\n            return {'mean': np.mean(y) if len(y) > 0 else 0.0}\n\n        best_sse = float('inf')\n        best_threshold = None\n        best_i = -1\n\n        # Iterate through possible split points\n        for i in range(len(X) - 1):\n            if X[i] >= X[i + 1]:\n                continue  # Skip if not a valid split\n\n            # Calculate SSE for this split\n            left_y = y[:i + 1]\n            right_y = y[i + 1:]\n            mean_left = np.mean(left_y)\n            mean_right = np.mean(right_y)\n\n            sse_left = np.sum((left_y - mean_left) ** 2)\n            sse_right = np.sum((right_y - mean_right) ** 2)\n            total_sse = sse_left + sse_right\n\n            if total_sse < best_sse:\n                best_sse = total_sse\n                best_threshold = (X[i] + X[i + 1]) / 2\n                best_i = i\n\n        # Check if a valid split was found\n        if best_i == -1:\n            return {'mean': np.mean(y)}\n\n        # Split the data into left and right\n        left_X = X[:best_i + 1]\n        left_y = y[:best_i + 1]\n        right_X = X[best_i + 1:]\n        right_y = y[best_i + 1:]\n\n        # Recursively build left and right subtrees\n        left_child = build_tree(left_X, left_y, current_depth + 1)\n        right_child = build_tree(right_X, right_y, current_depth + 1)\n\n        return {'threshold': best_threshold, 'left': left_child, 'right': right_child}\n\n    # Build the regression tree\n    root = build_tree(X_sorted, y_sorted, 0)\n\n    # Function to predict a single value\n    def predict_single(x):\n        node = root\n        while True:\n            if 'mean' in node:\n                return node['mean']\n            else:\n                if x <= node['threshold']:\n                    node = node['left']\n                else:\n                    node = node['right']\n\n    # Generate predictions for all test points\n    return [predict_single(x) for x in X_test]\n```\n\n### Explanation\n1. **Handling Empty Data**: The function immediately returns an empty list if the training data is empty.\n2. **Sorting Data**: The training data is sorted based on the feature values to ensure efficient splitting.\n3. **Tree Construction**: The `build_tree` function recursively constructs the tree. For each node, it checks if it should be a leaf node based on depth or sample size. If not, it finds the best split by minimizing SSE.\n4. **Splitting**: For each potential split point between sorted feature values, the SSE is calculated. The split with the lowest SSE is chosen.\n5. **Leaf Nodes**: When a node can no longer be split, it becomes a leaf node storing the mean of its target values.\n6. **Prediction**: Each test point is predicted by traversing the tree from the root to the appropriate leaf node based on the feature value, using the stored thresholds and means.\n\nThis approach ensures that the tree is built efficiently and predictions are made accurately by leveraging the structure of the binary tree formed through recursive splitting."}
{"task_id": 490, "completion_id": 0, "completion": "To solve this problem, we need to convert a one-dimensional vector into a square diagonal matrix. The input can be a Python list, tuple, or a one-dimensional NumPy array, and the output should be a list of lists where the elements of the input vector are placed on the principal diagonal, and all off-diagonal elements are zeros. If the input vector is empty, the function should return an empty list.\n\n### Approach\n1. **Handle Empty Input**: Check if the input vector is empty. If it is, return an empty list immediately.\n2. **Convert to NumPy Array**: Convert the input (whether it's a list, tuple, or already a NumPy array) into a NumPy array for consistent processing.\n3. **Check Size**: Verify the size of the converted array. If it's empty, return an empty list.\n4. **Create Diagonal Matrix**: Use NumPy's `diag` function to create a diagonal matrix from the input vector. This function places the elements of the input vector on the main diagonal and fills the off-diagonal elements with zeros.\n5. **Convert to List of Lists**: Convert the resulting NumPy matrix into a list of lists using the `tolist()` method for the required output format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | np.ndarray) -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    arr = x if isinstance(x, np.ndarray) else np.array(x)\n    if arr.size == 0:\n        return []\n    return np.diag(arr).tolist()\n```\n\n### Explanation\n- **Handling Empty Input**: The function first checks if the input is empty by converting it into a NumPy array and checking its size. If the size is zero, it returns an empty list.\n- **Conversion to NumPy Array**: The input is converted to a NumPy array if it isn't already one. This ensures uniform processing regardless of the input type.\n- **Diagonal Matrix Creation**: Using `np.diag`, the function creates a diagonal matrix from the input vector. This function efficiently places the elements on the main diagonal and initializes other elements to zero.\n- **Conversion to List of Lists**: The resulting matrix is converted to a list of lists using `tolist()`, which is the required output format.\n\nThis approach ensures that the function efficiently handles all valid input types and correctly converts them into the desired diagonal matrix format."}
{"task_id": 491, "completion_id": 0, "completion": "To solve this problem, we need to implement the FP-Growth algorithm to mine all frequent item-sets from a collection of transactions that appear at least a specified number of times (min_sup). The solution involves constructing an FP-Tree and recursively mining it to discover all frequent item-sets.\n\n### Approach\nThe FP-Growth algorithm works in two main stages:\n1. **FP-Tree Construction**: \n   - Count the frequency of each item in the transactions.\n   - Filter out items that do not meet the minimum support (min_sup).\n   - Sort the remaining items in each transaction by descending frequency and lexicographical order.\n   - Construct the FP-Tree by inserting each transaction into the tree, sharing common prefixes and maintaining a header table for quick access to nodes.\n\n2. **Recursive Mining**:\n   - For each item in the header table (processed in ascending order of frequency), generate the conditional pattern base.\n   - Build a conditional FP-Tree from the conditional pattern base.\n   - If the conditional FP-Tree has a single path, generate all non-empty combinations of items in that path. Otherwise, recursively mine the conditional FP-Tree.\n\n### Solution Code\n```python\nfrom collections import Counter, defaultdict\nimport itertools\n\nclass FPNode:\n    def __init__(self, item, count=0, parent=None):\n        self.item = item\n        self.count = count\n        self.parent = parent\n        self.children = defaultdict(FPNode)\n        self.node_link = None  # Not used in this implementation but kept for structure\n\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\"\"\"\n    # Step 1: Count item frequencies\n    item_counts = Counter()\n    for trans in transactions:\n        for item in trans:\n            item_counts[item] += 1\n\n    # Filter items by minimum support\n    frequent_items = {item: cnt for item, cnt in item_counts.items() if cnt >= min_sup}\n    if not frequent_items:\n        return []\n\n    # Sort frequent items by descending frequency, then lexicographical order\n    sorted_items = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n    rank = {item: idx for idx, item in enumerate(sorted_items)}\n\n    # Build FP-Tree\n    root = FPNode(None)\n    header_table = defaultdict(list)\n\n    for trans in transactions:\n        filtered = [item for item in trans if item in frequent_items]\n        filtered.sort(key=lambda x: rank[x])\n        current = root\n        for item in filtered:\n            if item in current.children:\n                current = current.children[item]\n                current.count += 1\n            else:\n                new_node = FPNode(item, 1, current)\n                current.children[item] = new_node\n                header_table[item].append(new_node)\n                current = new_node\n\n    # Mining the FP-Tree\n    frequent_itemsets = []\n\n    def is_single_path(node):\n        if not node.children:\n            return True\n        if len(node.children) > 1:\n            return False\n        return is_single_path(next(iter(node.children.values())))\n\n    def mine_tree(header, min_sup, suffix, frequent_itemsets):\n        # Process items in ascending order of frequency (sorted by count then lex)\n        items = sorted(header.keys(), key=lambda x: (frequent_items[x], x))\n\n        for item in items:\n            new_suffix = tuple(sorted(list(suffix) + [item]))\n            frequent_itemsets.append(new_suffix)\n\n            conditional_patterns = []\n            for node in header[item]:\n                path = []\n                count = node.count\n                parent = node.parent\n                while parent.item is not None:\n                    path.append(parent.item)\n                    parent = parent.parent\n                path.reverse()\n                conditional_patterns.append((path, count))\n\n            # Count frequencies in conditional patterns\n            cond_counts = Counter()\n            for path, cnt in conditional_patterns:\n                for it in path:\n                    cond_counts[it] += cnt\n\n            # Filter items that meet min_sup\n            cond_freq = {it: cnt for it, cnt in cond_counts.items() if cnt >= min_sup}\n            if not cond_freq:\n                continue\n\n            # Sort conditional frequent items by descending count, then lex\n            cond_sorted = sorted(cond_freq.keys(), key=lambda x: (-cond_counts[x], x))\n\n            # Build conditional FP-Tree\n            cond_root = FPNode(None)\n            cond_header = defaultdict(list)\n            for path, cnt in conditional_patterns:\n                filtered_path = [it for it in path if it in cond_freq]\n                filtered_path.sort(key=lambda x: (-cond_counts[x], x))\n\n                current = cond_root\n                for it in filtered_path:\n                    if it in current.children:\n                        current = current.children[it]\n                        current.count += cnt\n                    else:\n                        new_node = FPNode(it, cnt, current)\n                        current.children[it] = new_node\n                        cond_header[it].append(new_node)\n                        current = new_node\n\n            if is_single_path(cond_root):\n                # Generate all subsets of the path\n                path_items = []\n                current_node = cond_root\n                while current_node.children:\n                    next_node = next(iter(current_node.children.values()))\n                    path_items.append(next_node.item)\n                    current_node = next_node\n\n                for i in range(1, len(path_items) + 1):\n                    for subset in itertools.combinations(path_items, i):\n                        combined = tuple(sorted(list(subset) + [item] + list(suffix)))\n                        frequent_itemsets.append(combined)\n            else:\n                mine_tree(cond_header, min_sup, new_suffix, frequent_itemsets)\n\n    mine_tree(header_table, min_sup, tuple(), frequent_itemsets)\n\n    # Deduplicate and sort\n    unique = list({tuple(sorted(itemset)) for itemset in frequent_itemsets})\n    unique.sort(key=lambda x: (len(x), x))\n\n    return unique\n```\n\n### Explanation\n1. **FP-Tree Construction**:\n   - The algorithm first counts the frequency of each item across all transactions.\n   - Items that do not meet the minimum support threshold are filtered out.\n   - Remaining items are sorted by frequency (descending) and lexicographical order.\n   - Transactions are inserted into the FP-Tree, creating nodes for items and linking them through shared prefixes.\n\n2. **Mining Process**:\n   - The algorithm recursively processes each item from the header table, starting with the least frequent.\n   - For each item, the conditional pattern base is generated, which consists of paths from the FP-Tree that end with the item.\n   - A conditional FP-Tree is built from these paths. If this tree has a single path, all non-empty subsets of the path are combined with the current item to form frequent item-sets. Otherwise, the tree is mined recursively.\n\nThe solution efficiently discovers all frequent item-sets by leveraging the FP-Tree structure, ensuring that the result is sorted by length and lexicographical order as required."}
{"task_id": 492, "completion_id": 0, "completion": "To solve this problem, we need to generate a polynomial feature matrix from a given data matrix such that the resulting matrix includes all monomials of the original features up to a specified degree. The monomials should be ordered first by ascending degree and then lexicographically within each degree.\n\n### Approach\n1. **Check for Negative Degree**: If the specified degree is negative, return -1 immediately.\n2. **Convert Input to NumPy Array**: This allows for easier manipulation and mathematical operations.\n3. **Generate Exponent Tuples**: For each degree from 0 up to the specified degree, generate all possible combinations of feature indices. Each combination represents the exponents of the features such that their sum equals the current degree.\n4. **Compute Monomials**: For each sample (row) in the input data matrix, compute the product of each feature raised to the corresponding exponent from the generated tuples.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    if degree < 0:\n        return -1\n    X = np.array(X, dtype=float)\n    n_samples, m = X.shape\n    exponents = []\n    for current_degree in range(degree + 1):\n        if current_degree == 0:\n            exponents.append((0,) * m)\n        else:\n            for comb in combinations_with_replacement(range(m), current_degree):\n                exp = [0] * m\n                for idx in comb:\n                    exp[idx] += 1\n                exponents.append(tuple(exp))\n    result = []\n    for row in X:\n        new_row = []\n        for exp in exponents:\n            product = 1.0\n            for i in range(m):\n                product *= row[i] ** exp[i]\n            new_row.append(product)\n        result.append(new_row)\n    return result\n```\n\n### Explanation\n1. **Negative Degree Check**: The function first checks if the input degree is negative and returns -1 if true.\n2. **Conversion to NumPy Array**: The input data matrix is converted to a NumPy array to leverage vectorized operations.\n3. **Generating Exponent Tuples**: For each degree from 0 to the specified degree, combinations of feature indices are generated. Each combination is converted into an exponent tuple by counting the occurrences of each feature index.\n4. **Computing Monomials**: For each row in the input matrix, the function computes the product of each feature raised to the corresponding exponent from the generated tuples. These products form the polynomial features for each row.\n\nThis approach ensures that all monomials are generated in the correct order and efficiently computes the polynomial features for each sample, adhering to the specified constraints."}
{"task_id": 493, "completion_id": 0, "completion": "To solve this problem, we need to compute the mean-squared error (MSE) of a given 1-D sample of real numbers. The MSE is the average of the squared differences between each element and the mean of the sample. If the input sample is empty, the function should return -1.\n\n### Approach\n1. **Check for Empty Input**: Convert the input to a NumPy array and check if it is empty. If it is, return -1.\n2. **Calculate the Mean**: Compute the mean of the elements in the sample.\n3. **Compute Squared Differences**: For each element in the sample, compute the squared difference from the mean.\n4. **Average the Squared Differences**: Find the average of these squared differences to get the MSE.\n5. **Round the Result**: Round the computed MSE to four decimal places and return the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    y_arr = np.array(y)\n    if y_arr.size == 0:\n        return -1\n    mean_val = np.mean(y_arr)\n    mse_value = np.mean((y_arr - mean_val) ** 2)\n    return round(mse_value, 4)\n```\n\n### Explanation\n1. **Handling Empty Input**: The input is converted to a NumPy array. If the array has no elements (size is 0), the function immediately returns -1.\n2. **Mean Calculation**: Using NumPy's `mean` function, we efficiently compute the mean of the array elements.\n3. **Squared Differences**: By subtracting the mean from each element and squaring the result, we get the squared differences.\n4. **Averaging Squared Differences**: The mean of these squared differences is computed to obtain the MSE.\n5. **Rounding**: The result is rounded to four decimal places using Python's `round` function to meet the specified precision requirement.\n\nThis approach ensures that the function efficiently handles both Python lists and NumPy arrays, correctly processes empty inputs, and computes the MSE accurately while adhering to the required precision."}
{"task_id": 496, "completion_id": 0, "completion": "To solve this problem, we need to compute the affine activation function along with its first and second derivatives for a given input vector. The affine activation function is defined as \\(f(x) = \\text{slope} \\cdot x + \\text{intercept}\\), and its derivatives are straightforward since the first derivative is a constant (the slope) and the second derivative is zero.\n\n### Approach\n1. **Input Handling**: Convert the input to a 1-D NumPy array. If the input is a scalar, treat it as a single-element vector.\n2. **Compute Activation**: Calculate the affine activation values for each element in the input vector.\n3. **Compute Derivatives**: The first derivative is a constant value (slope) for all elements, and the second derivative is zero for all elements.\n4. **Rounding**: Round each element of the activation values and derivatives to four decimal places.\n5. **Conversion to Lists**: Convert the resulting NumPy arrays to Python lists and return them as a tuple.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to a 1-D numpy array, treating scalars as length-one vectors\n    x_arr = np.atleast_1d(np.asarray(x))\n    \n    # Compute the affine activation\n    y = slope * x_arr + intercept\n    \n    # First derivative is slope for all elements\n    grad = np.full_like(y, slope)\n    \n    # Second derivative is zero for all elements\n    grad2 = np.zeros_like(y)\n    \n    # Round each element to 4 decimal places and convert to lists\n    y_list = np.round(y, 4).tolist()\n    grad_list = np.round(grad, 4).tolist()\n    grad2_list = np.round(grad2, 4).tolist()\n    \n    return (y_list, grad_list, grad2_list)\n```\n\n### Explanation\n1. **Input Conversion**: The input is converted to a 1-D NumPy array using `np.atleast_1d` to handle scalars and ensure the input is treated as a vector.\n2. **Activation Calculation**: The affine function \\(f(x) = \\text{slope} \\cdot x + \\text{intercept}\\) is applied element-wise using vectorized operations for efficiency.\n3. **Derivatives**: The first derivative is simply the slope for all elements, and the second derivative is zero for all elements. These are generated using NumPy functions for consistency and performance.\n4. **Rounding and Conversion**: The results are rounded to four decimal places using `np.round` and converted to Python lists to meet the output requirements.\n\nThis approach ensures that the function is efficient, handles various input types correctly, and returns the results in the specified format."}
{"task_id": 499, "completion_id": 0, "completion": "To solve this problem, we need to implement a Principal Component Analysis (PCA) routine from scratch to reduce the dimensionality of a given dataset. PCA is a statistical technique that transforms the data into a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n\n### Approach\n1. **Standardize the Data**: Each feature (column) of the dataset is standardized to have zero mean and unit population variance. If a feature has zero variance, it remains unchanged after centering.\n2. **Compute Covariance Matrix**: Using the standardized data, compute the sample covariance matrix with Bessel's correction (dividing by \\(n-1\\)).\n3. **Eigen Decomposition**: Perform eigen decomposition on the covariance matrix to obtain eigenvalues and eigenvectors.\n4. **Sort Eigenvalues and Eigenvectors**: Sort the eigenvalues in descending order and arrange the corresponding eigenvectors accordingly.\n5. **Adjust Eigenvector Signs**: Ensure each eigenvector's entry with the largest absolute value is positive to remove sign ambiguity.\n6. **Project Data**: Project the standardized data onto the first \\(k\\) eigenvectors (principal components).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\"\"\"\n    n_samples, n_features = data.shape\n    if k < 1 or k > n_features:\n        return -1\n    \n    # Step 1: Standardize the features (zero mean, unit population variance)\n    data_centered = data - np.mean(data, axis=0)\n    var_pop = np.var(data_centered, axis=0, ddof=0)\n    std = np.sqrt(var_pop)\n    std[std == 0] = 1.0  # Avoid division by zero where variance is zero\n    data_std = data_centered / std\n    \n    # Step 2: Compute the sample covariance matrix with Bessel's correction\n    cov_matrix = np.cov(data_std, rowvar=False, ddof=1)\n    \n    # Step 3: Eigen decomposition of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    # Step 4: Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues_sorted = eigenvalues[sorted_indices]\n    eigenvectors_sorted = eigenvectors[:, sorted_indices]\n    \n    # Step 5: Fix the sign of eigenvectors\n    for i in range(eigenvectors_sorted.shape[1]):\n        eigenvector = eigenvectors_sorted[:, i]\n        max_abs_idx = np.argmax(np.abs(eigenvector))\n        if eigenvector[max_abs_idx] < 0:\n            eigenvectors_sorted[:, i] = -eigenvector\n    \n    # Step 6: Project the standardized data onto the first k eigenvectors\n    projection = data_std @ eigenvectors_sorted[:, :k]\n    \n    # Round and convert to list\n    return np.round(projection, 4).tolist()\n```\n\n### Explanation\n1. **Standardization**: Each feature is centered (mean subtracted) and scaled by its population standard deviation. This ensures that each feature has zero mean and unit variance, making the covariance matrix computation meaningful.\n2. **Covariance Matrix**: The covariance matrix is computed using the standardized data. This matrix captures the covariance between each pair of features.\n3. **Eigen Decomposition**: The eigenvalues and eigenvectors of the covariance matrix are computed. Eigenvectors represent the directions of maximum variance, and eigenvalues represent the magnitude of variance in those directions.\n4. **Sorting**: Eigenvalues are sorted in descending order to prioritize the principal components that explain the most variance.\n5. **Sign Adjustment**: The sign of each eigenvector is adjusted to ensure the largest absolute value element is positive, ensuring deterministic output.\n6. **Projection**: The data is projected onto the first \\(k\\) principal components, reducing the dimensionality while retaining the most significant variance in the data.\n\nThis approach efficiently reduces the dimensionality of the dataset while preserving the essential structure and relationships in the data."}
{"task_id": 500, "completion_id": 0, "completion": "To solve this problem, we need to implement a small-scale Gradient Boosting binary classifier using decision stumps as weak learners. The classifier will use the squared-error loss function and predict binary labels by thresholding the final score at 0.5.\n\n### Approach\n1. **Initialization**: Compute the initial prediction as the mean of the training labels (denoted as `c`).\n2. **Residual Calculation**: At each boosting stage, compute the residuals as the difference between the true labels and the current predictions.\n3. **Decision Stump Training**: For each feature, find the optimal threshold that splits the data into two groups, minimizing the sum of squared errors (SSE) of the residuals. The optimal split for each feature is determined by evaluating all unique values of the feature as potential thresholds.\n4. **Model Update**: Update the model predictions by adding the contribution of the best decision stump found, scaled by the learning rate.\n5. **Prediction**: After training, compute the final score for each test sample by summing the contributions of all decision stumps and apply a threshold of 0.5 to convert the score into a binary label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int = 20, learning_rate: float = 0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\"\"\"\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    n_samples, n_features = X_train.shape\n    c = np.mean(y_train)\n    F = np.full(n_samples, c)\n    stumps = []\n    \n    for m in range(n_estimators):\n        residuals = y_train - F\n        \n        best_sse = float('inf')\n        best_j = None\n        best_t = None\n        best_left_val = None\n        best_right_val = None\n        \n        for j in range(n_features):\n            feature_values = X_train[:, j]\n            unique_values = np.unique(feature_values)\n            \n            for t in unique_values:\n                left_mask = feature_values <= t\n                n_left = left_mask.sum()\n                n_right = n_samples - n_left\n                if n_left == 0 or n_right == 0:\n                    continue\n                \n                left_res = residuals[left_mask]\n                right_res = residuals[~left_mask]\n                \n                left_val = left_res.mean()\n                right_val = right_res.mean()\n                \n                sse = np.sum((left_res - left_val)**2) + np.sum((right_res - right_val)**2)\n                \n                if sse < best_sse:\n                    best_sse = sse\n                    best_j = j\n                    best_t = t\n                    best_left_val = left_val\n                    best_right_val = right_val\n        \n        if best_j is not None:\n            stumps.append((best_j, best_t, best_left_val, best_right_val))\n            h_pred = np.where(X_train[:, best_j] <= best_t, best_left_val, best_right_val)\n            F += learning_rate * h_pred\n    \n    predictions = []\n    for x in X_test:\n        f = c\n        for (j, t, lv, rv) in stumps:\n            if x[j] <= t:\n                f += learning_rate * lv\n            else:\n                f += learning_rate * rv\n        predictions.append(1 if f >= 0.5 else 0)\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The initial prediction `F` is set to the mean of the training labels `c`, which serves as the baseline prediction.\n2. **Residual Calculation**: At each boosting iteration, residuals are computed as the difference between the actual labels and the current predictions.\n3. **Decision Stump Training**: For each feature, potential thresholds are evaluated. The best threshold is chosen based on the SSE of the residuals. The decision stump (weak learner) splits the data into two groups and predicts the mean residual for each group.\n4. **Model Update**: The model's predictions are updated by adding the contribution of the best decision stump found, scaled by the learning rate. This iterative process continues for the specified number of estimators.\n5. **Prediction**: For each test sample, the final score is computed by summing the initial prediction and the contributions of all decision stumps. The score is thresholded at 0.5 to determine the binary class label (0 or 1).\n\nThis approach efficiently builds an additive model using gradient boosting, leveraging decision stumps to iteratively improve predictions and minimize the squared-error loss."}
{"task_id": 505, "completion_id": 0, "completion": "To solve this problem, we need to implement a single weight-update step for a binary Restricted Boltzmann Machine (RBM) using the Contrastive Divergence (CD-k) algorithm. The goal is to update the weight matrix based on the given mini-batch of visible vectors, current weights, biases, learning rate, and the number of Gibbs sampling steps.\n\n### Approach\n1. **Check for Empty Mini-batch**: If the input mini-batch is empty, return an empty list immediately.\n2. **Compute Positive Phase Hidden Probabilities**: Calculate the hidden layer probabilities using the visible input data.\n3. **Gibbs Sampling Steps**: Perform `k` Gibbs sampling steps to approximate the negative phase. Each step involves updating the visible and hidden probabilities without stochastic sampling.\n4. **Compute Gradients**: Calculate the positive and negative gradients using the probabilities from the positive phase and the `k`-th Gibbs step.\n5. **Update Weights**: Adjust the weight matrix using the computed gradients and learning rate.\n6. **Format the Result**: Round the updated weights to 4 decimal places and convert them to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\"\"\"\n    if X.size == 0:\n        return []\n    \n    m = X.shape[0]\n    # Positive phase: compute hidden probabilities given visible input X\n    h0_input = X.dot(W) + hbias\n    h0_prob = 1 / (1 + np.exp(-h0_input))\n    \n    # Negative phase: run k Gibbs steps\n    if k == 0:\n        v_k = X\n        h_k = h0_prob\n    else:\n        current_h = h0_prob\n        for _ in range(k):\n            v_input = current_h.dot(W.T) + vbias\n            v = 1 / (1 + np.exp(-v_input))\n            h_input = v.dot(W) + hbias\n            current_h = 1 / (1 + np.exp(-h_input))\n        v_k = v\n        h_k = current_h\n    \n    # Compute gradients\n    pos_grad = X.T.dot(h0_prob)\n    neg_grad = v_k.T.dot(h_k)\n    \n    # Update weights\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n    \n    # Round and convert to list of lists\n    return np.round(W_new, 4).tolist()\n```\n\n### Explanation\n1. **Check for Empty Mini-batch**: The function first checks if the input mini-batch `X` is empty. If it is, an empty list is returned immediately.\n2. **Positive Phase**: The hidden probabilities (`h0_prob`) are computed using the sigmoid activation function applied to the linear transformation of the visible input `X` and the hidden biases `hbias`.\n3. **Gibbs Sampling Steps**: For each of the `k` steps, the visible probabilities are computed from the hidden probabilities, and then the hidden probabilities are updated again. This process is repeated `k` times to approximate the negative phase.\n4. **Gradients Calculation**: The positive gradient is calculated using the outer product of the visible input `X` and the initial hidden probabilities `h0_prob`. The negative gradient is calculated using the outer product of the visible and hidden probabilities obtained after `k` Gibbs steps.\n5. **Weight Update**: The weight matrix is updated using the difference between the positive and negative gradients, scaled by the learning rate and the number of samples in the mini-batch.\n6. **Result Formatting**: The updated weight matrix is rounded to 4 decimal places and converted to a list of lists for the final output.\n\nThis approach efficiently computes the necessary updates to the weight matrix using the Contrastive Divergence algorithm, ensuring that the model learns from the data distribution effectively."}
{"task_id": 509, "completion_id": 0, "completion": "To solve this problem, we need to compute the Chebyshev distance between two real-valued vectors. The Chebyshev distance is defined as the maximum absolute difference between corresponding elements of the vectors. The solution must handle both Python lists and NumPy 1-D arrays, verify that the vectors are of the same length, and return the result rounded to four decimal places as a Python float.\n\n### Approach\n1. **Check Lengths**: Verify that the two input vectors have the same length. If not, return -1.0.\n2. **Handle Empty Vectors**: If both vectors are empty (length 0), return 0.0 as the Chebyshev distance for empty vectors is considered 0 by convention.\n3. **Convert to NumPy Arrays**: Convert the input vectors to NumPy arrays to leverage efficient element-wise operations.\n4. **Compute Differences**: Calculate the absolute differences between corresponding elements of the two vectors.\n5. **Find Maximum Difference**: Determine the maximum value from the computed differences.\n6. **Round Result**: Round the maximum difference to four decimal places and return it as a Python float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    if len(x) != len(y):\n        return -1.0\n    if len(x) == 0:\n        return 0.0\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    diff = np.abs(x_arr - y_arr)\n    max_diff = np.max(diff)\n    return round(max_diff, 4)\n```\n\n### Explanation\n1. **Check Lengths**: The function first checks if the lengths of the two vectors are equal. If not, it returns -1.0 immediately.\n2. **Handle Empty Vectors**: If both vectors are empty, the function returns 0.0, as there are no elements to compare and the distance is conventionally considered zero.\n3. **Convert to Arrays**: The vectors are converted to NumPy arrays to utilize NumPy's efficient element-wise operations.\n4. **Compute Differences**: Using NumPy's element-wise subtraction and absolute value functions, the differences between corresponding elements are computed.\n5. **Find Maximum**: The maximum value of these differences is found using NumPy's max function.\n6. **Round Result**: The maximum difference is rounded to four decimal places using Python's `round` function to ensure the result is a standard Python float.\n\nThis approach efficiently handles both Python lists and NumPy arrays, ensuring correctness and adherence to the problem constraints."}
{"task_id": 510, "completion_id": 0, "completion": "To solve this problem, we need to implement the Variational Auto-Encoder (VAE) loss function, which consists of two components: the reconstruction loss and the KL divergence. The reconstruction loss is calculated using binary cross-entropy, and the KL divergence measures the difference between the learned latent variable distribution and a standard Gaussian distribution. The final loss is the average of the sum of these two components over a mini-batch, rounded to six decimal places.\n\n### Approach\n1. **Clipping for Numerical Stability**: To avoid numerical instability when computing logarithms, each element of the predicted probabilities (`y_pred`) is clipped to a small interval around [0, 1] using machine epsilon.\n2. **Reconstruction Loss**: Compute the binary cross-entropy loss between the true input (`y`) and the clipped predicted probabilities (`y_pred_clipped`). This is done element-wise and then summed over all features for each sample in the batch.\n3. **KL Divergence**: Calculate the KL divergence using the mean (`t_mean`) and log-variance (`t_log_var`) of the latent variables. The formula provided sums over the latent dimensions for each sample in the batch.\n4. **Total Loss**: Sum the reconstruction loss and KL divergence for each sample, then average these values over the entire batch. The result is rounded to six decimal places to ensure precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    epsilon = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    \n    # Compute reconstruction loss per sample\n    bce = y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped)\n    reconstruction_loss = -np.sum(bce, axis=1)\n    \n    # Compute KL divergence per sample\n    kl = -0.5 * np.sum(1 + t_log_var - np.square(t_mean) - np.exp(t_log_var), axis=1)\n    \n    # Average total loss over the batch\n    total_loss = reconstruction_loss + kl\n    average_loss = np.mean(total_loss)\n    \n    return round(average_loss.item(), 6)\n```\n\n### Explanation\n1. **Clipping**: The predicted probabilities are clipped to avoid values of 0 or 1, which can cause numerical instability when taking logarithms.\n2. **Reconstruction Loss Calculation**: The binary cross-entropy (BCE) is calculated element-wise and summed over all features for each sample. This gives the reconstruction loss for each sample in the batch.\n3. **KL Divergence Calculation**: The KL divergence is computed using the analytical formula provided, which sums over the latent dimensions for each sample. This measures how much the learned latent distribution deviates from a standard Gaussian distribution.\n4. **Averaging**: The total loss for each sample is the sum of the reconstruction loss and KL divergence. The average of these total losses over the batch is computed and rounded to six decimal places to produce the final result.\n\nThis approach ensures numerical stability and correctness while efficiently computing the necessary components of the VAE loss function."}
{"task_id": 513, "completion_id": 0, "completion": "To solve this problem, we need to implement a Factorization Machine (FM) regression model to predict values for a given set of samples. The FM model combines linear regression with pair-wise feature interactions, which allows it to capture complex relationships between features. \n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Convert Inputs to NumPy Arrays**: Convert the input lists to NumPy arrays for efficient numerical computations.\n2. **Compute Linear Term**: Calculate the linear part of the prediction, which includes the bias term and the dot product of the feature matrix with the linear weights.\n3. **Compute Interaction Term**: This term captures the pair-wise interactions between features using latent factors. It involves two main parts:\n   - The sum of squares of the dot product of the feature matrix with the latent factors.\n   - The sum of the squares of the element-wise product of the feature matrix and the latent factors.\n4. **Combine Terms**: Sum the linear term and the interaction term to get the final predictions.\n5. **Round Results**: Round the predictions to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\"\"\"\n    X_np = np.array(X)\n    w_np = np.array(w)\n    V_np = np.array(V)\n    \n    # Compute linear term: w0 + X * w\n    linear_term = w0 + X_np.dot(w_np)\n    \n    # Compute interaction term components\n    XV = X_np.dot(V_np)  # Shape (n_samples, k)\n    sum_XV_sq = (XV ** 2).sum(axis=1)  # Sum over latent factors\n    \n    X_sq = X_np ** 2\n    V_sq = V_np ** 2\n    sum_XV_sq_terms = (X_sq.dot(V_sq)).sum(axis=1)  # Sum over latent factors\n    \n    interaction_term = 0.5 * (sum_XV_sq - sum_XV_sq_terms)\n    \n    # Combine terms and round to 4 decimal places\n    predictions = linear_term + interaction_term\n    rounded_predictions = np.round(predictions, 4)\n    \n    return rounded_predictions.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The input lists `X`, `w`, and `V` are converted to NumPy arrays to leverage efficient vectorized operations.\n2. **Linear Term Calculation**: The linear term is computed by adding the bias term `w0` to the dot product of the feature matrix `X` with the linear weights `w`.\n3. **Interaction Term Calculation**:\n   - **Dot Product with Latent Factors**: The product of the feature matrix `X` and the latent factors `V` is computed. This product is then squared and summed over the latent factors.\n   - **Element-wise Squares**: The feature matrix and latent factors are squared element-wise. The product of these squared matrices is summed over the features and then over the latent factors.\n   - **Interaction Term**: The interaction term is derived by taking the difference between the two computed sums, scaling it by 0.5.\n4. **Combining Terms**: The final predictions are obtained by adding the linear term and the interaction term.\n5. **Rounding**: The predictions are rounded to four decimal places using NumPy's rounding function for precision.\n\nThis approach ensures that the solution is efficient and handles arbitrary dimensions for the input matrices, providing accurate predictions as required."}
{"task_id": 517, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that computes the sigmoid activation function in a numerically stable manner for various input types, including scalars, lists, tuples, and NumPy arrays. The function should return the result rounded to four decimal places and preserve the input structure.\n\n### Approach\n1. **Input Handling**: The function first checks if the input is a scalar (int or float). If it is, the input is converted to a NumPy array to leverage vectorized operations. For other input types (lists, tuples, or NumPy arrays), the input is directly converted to a NumPy array.\n2. **Numerically Stable Sigmoid Calculation**: The sigmoid function is computed using a numerically stable approach to avoid overflow for very large positive or negative values. This is done by splitting the computation based on the sign of the input values:\n   - For non-negative values, compute \\( \\frac{1}{1 + e^{-z}} \\).\n   - For negative values, compute \\( \\frac{e^z}{1 + e^z} \\).\n3. **Rounding**: The computed sigmoid values are rounded to four decimal places using NumPy's rounding function to ensure consistency.\n4. **Output Structure**: The result is converted back to the original input structure. If the input was a scalar, the result is returned as a float. Otherwise, the result is returned as a nested Python list using NumPy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    is_scalar = isinstance(z, (int, float))\n    if is_scalar:\n        z = np.array([z])\n    else:\n        z = np.asarray(z)\n    \n    # Compute the sigmoid in a numerically stable way\n    sigmoid_values = np.where(z >= 0, \n                              1 / (1 + np.exp(-z)), \n                              np.exp(z) / (1 + np.exp(z)))\n    \n    # Round to four decimal places\n    rounded = np.round(sigmoid_values, 4)\n    \n    if is_scalar:\n        return float(rounded[0])\n    else:\n        return rounded.tolist()\n```\n\n### Explanation\n- **Input Handling**: The function checks if the input is a scalar and converts it to a NumPy array for uniform processing. Non-scalar inputs are converted to NumPy arrays to handle element-wise operations efficiently.\n- **Sigmoid Calculation**: Using `np.where`, the function applies different computations based on the sign of each element in the input array. This avoids overflow by preventing the evaluation of large exponentials directly.\n- **Rounding**: The results are rounded to four decimal places using `np.round`, ensuring consistent rounding across all elements.\n- **Output Structure**: The function returns a float if the input was a scalar. For other input types, the result is converted to a nested list using `tolist()`, preserving the structure of the original input.\n\nThis approach ensures that the function is both efficient and numerically stable, handling all specified input types correctly and returning results in the required format."}
{"task_id": 518, "completion_id": 0, "completion": "To solve this problem, we need to create a decorator that automatically converts one-hot encoded label matrices into 1-D integer arrays before passing them to a machine learning metric function. This conversion is necessary because most metric functions expect labels in a 1-D integer format rather than one-hot encoded matrices.\n\n### Approach\nThe decorator, named `unhot`, will wrap around any metric function that takes two arguments: `actual` and `predicted`, both of which are NumPy arrays. The decorator's task is to check if these arrays are one-hot encoded (2-D arrays with the second dimension greater than 1). If they are, it converts them to 1-D integer arrays using `argmax(axis=1)`. If the arrays are already 1-D or have a shape of (n, 1), they are left unchanged. The converted arrays are then passed to the metric function, ensuring compatibility with functions expecting 1-D inputs.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n\n    def wrapper(actual: np.ndarray, predicted: np.ndarray):\n        # Process actual array\n        if actual.ndim == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        \n        # Process predicted array\n        if predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        \n        return function(actual, predicted)\n    \n    return wrapper\n```\n\n### Explanation\n1. **Decorator Definition**: The `unhot` decorator takes a metric function as input and returns a wrapped version of this function.\n2. **Wrapper Function**: The wrapper function processes the `actual` and `predicted` arrays:\n   - **Check for One-Hot Encoding**: For each array, it checks if the array is 2-D and if the second dimension is greater than 1. If true, it converts the array to a 1-D integer array using `argmax(axis=1)`.\n   - **Pass Through**: If the array is already 1-D or has a shape of (n, 1), it is passed through without modification.\n3. **Call Original Function**: The processed arrays are then passed to the original metric function, which computes the result using the converted arrays.\n\nThis approach ensures that the metric function always receives 1-D integer arrays, regardless of whether the inputs were originally one-hot encoded. This allows the metric function to work correctly without needing to handle one-hot encoding internally."}
{"task_id": 520, "completion_id": 0, "completion": "To solve this problem, we need to compute the gradient of the sigmoid function given its output values. The gradient is computed element-wise using the formula \\(g'(z) = z \\cdot (1 - z)\\), where \\(z\\) is the output of the sigmoid function. The solution must handle various input types, including scalars, nested lists, and NumPy arrays, and return the result in the same structure as the input, with each value rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: The function should accept Python scalars, nested lists, or NumPy arrays. Each input type is processed differently to maintain the structure of the output.\n2. **Gradient Calculation**: For each element in the input, compute the gradient using the formula \\(z \\cdot (1 - z)\\).\n3. **Rounding**: Use NumPy's rounding function to ensure each result is rounded to four decimal places.\n4. **Recursive Processing**: For nested lists, recursively process each element to maintain the nested structure in the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        grad = z * (1 - z)\n        return np.round(grad, 4)\n    elif isinstance(z, list):\n        return [sigmoid_grad(elem) for elem in z]\n    else:\n        return np.round(z * (1 - z), 4).item()\n```\n\n### Explanation\n1. **NumPy Arrays**: If the input is a NumPy array, the gradient is computed using vectorized operations for efficiency. The result is rounded and returned as a NumPy array.\n2. **Nested Lists**: If the input is a list (including nested lists), each element is processed recursively. This ensures that the output maintains the same nested structure as the input.\n3. **Scalars**: If the input is a scalar (Python float or int), the gradient is computed and converted to a Python float after rounding, ensuring the output type matches the input type.\n\nThis approach efficiently handles all specified input types and ensures the output structure matches the input structure, with each element correctly transformed and rounded."}
{"task_id": 528, "completion_id": 0, "completion": "To solve this problem, we need to generate a grid of predictions for a 2D linear classifier. The classifier is defined by weights and a bias term, and we need to label each grid point based on the classifier's decision rule. The grid should span the bounding box of the given data and have a specified resolution.\n\n### Approach\n1. **Data Conversion**: Convert the input data into a NumPy array to facilitate numerical operations.\n2. **Determine Bounds**: Compute the minimum and maximum values for each feature (x\u2081 and x\u2082) from the input data to define the grid's range.\n3. **Generate Grid Points**: Create two 1D arrays (`x1_plot` and `x2_plot`) that are linearly spaced between the computed minimum and maximum values for each feature.\n4. **Create Meshgrid**: Use these 1D arrays to generate a meshgrid, which provides all possible combinations of (x\u2081, x\u2082) points in the grid.\n5. **Classify Grid Points**: For each point in the meshgrid, compute the linear combination of the weights and bias, then apply the sign function to determine the label (1 or -1).\n6. **Convert to List**: Convert the resulting NumPy array of labels into a nested Python list of lists as required.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    X_np = np.array(X)\n    x1_min, x1_max = X_np[:, 0].min(), X_np[:, 0].max()\n    x2_min, x2_max = X_np[:, 1].min(), X_np[:, 1].max()\n    \n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    \n    X1, X2 = np.meshgrid(x1_plot, x2_plot, indexing='ij')\n    Z = W[0] * X1 + W[1] * X2 + b\n    labels = np.where(Z >= 0, 1, -1)\n    \n    return labels.tolist()\n```\n\n### Explanation\n1. **Data Conversion**: The input data `X` is converted to a NumPy array to leverage NumPy's efficient numerical operations.\n2. **Determine Bounds**: The minimum and maximum values for each feature (x\u2081 and x\u2082) are computed to ensure the grid spans the entire range of the input data.\n3. **Generate Grid Points**: Using `np.linspace`, we create two 1D arrays (`x1_plot` and `x2_plot`) that cover the range from the minimum to maximum values of each feature with the specified resolution (`grid_n`).\n4. **Create Meshgrid**: The `np.meshgrid` function with `indexing='ij'` generates two 2D arrays (X1 and X2) representing all possible (x\u2081, x\u2082) pairs in the grid.\n5. **Classify Grid Points**: The linear combination of weights, features, and bias is computed for each grid point. The `np.where` function is used to apply the sign function, resulting in a 2D array of labels (1 or -1).\n6. **Convert to List**: The resulting NumPy array of labels is converted to a nested Python list using the `tolist()` method to meet the output requirements.\n\nThis approach efficiently generates the decision boundary grid using vectorized operations, ensuring both correctness and performance."}
{"task_id": 537, "completion_id": 0, "completion": "To solve this problem, we need to implement a Gaussian Naive Bayes classifier from scratch. The classifier will learn the parameters from the training data and use these parameters to predict the class labels for the test data. The solution involves two main phases: training and prediction.\n\n### Approach\n\n1. **Training Phase**:\n   - **Parameter Estimation**: Compute the mean and standard deviation of each feature for both classes (0 and 1) using the training data. To avoid division by zero, a small constant (epsilon) is added to the standard deviations.\n   - **Class Priors**: Calculate the prior probabilities for each class based on their relative frequencies in the training data.\n\n2. **Prediction Phase**:\n   - **Posterior Probability Calculation**: For each test sample, compute the posterior probability for both classes using the Gaussian likelihood and the class priors. The likelihood is calculated under the assumption that features are independent.\n   - **Decision Making**: Predict the class with the higher posterior probability for each test sample. To avoid numerical underflow, log probabilities are used instead of multiplying probabilities directly.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\"\"\"\n    # Separate training data by class\n    mask0 = y_train == 0\n    X0 = X_train[mask0]\n    X1 = X_train[~mask0]\n    \n    # Calculate mean and standard deviation for each feature in each class\n    mean0 = np.mean(X0, axis=0)\n    std0 = np.std(X0, axis=0, ddof=1)  # Using sample standard deviation\n    mean1 = np.mean(X1, axis=0)\n    std1 = np.std(X1, axis=0, ddof=1)\n    \n    # Add epsilon to prevent division by zero\n    epsilon = 1e-9\n    std0 += epsilon\n    std1 += epsilon\n    \n    # Compute class priors\n    prior0 = X0.shape[0] / X_train.shape[0]\n    prior1 = X1.shape[0] / X_train.shape[0]\n    \n    # Predict each test sample\n    predictions = []\n    for x in X_test:\n        # Compute log likelihoods for class 0\n        log_likelihood0 = (-0.5 * np.log(2 * np.pi)) - np.log(std0) - ((x - mean0) ** 2) / (2 * std0 ** 2)\n        log_prob0 = np.log(prior0) + np.sum(log_likelihood0)\n        \n        # Compute log likelihoods for class 1\n        log_likelihood1 = (-0.5 * np.log(2 * np.pi)) - np.log(std1) - ((x - mean1) ** 2) / (2 * std1 ** 2)\n        log_prob1 = np.log(prior1) + np.sum(log_likelihood1)\n        \n        # Determine the predicted class\n        predictions.append(0 if log_prob0 > log_prob1 else 1)\n    \n    return predictions\n```\n\n### Explanation\n\n1. **Training Phase**:\n   - **Separation by Class**: The training data is split into two groups based on their class labels (0 or 1).\n   - **Mean and Standard Deviation Calculation**: For each feature in each class group, compute the mean and standard deviation. A small constant (epsilon) is added to each standard deviation to avoid division by zero during prediction.\n   - **Prior Calculation**: The prior probabilities for each class are computed as the ratio of the number of samples in each class to the total number of training samples.\n\n2. **Prediction Phase**:\n   - **Log Probability Calculation**: For each test sample, the log likelihood for each feature is computed using the Gaussian formula. The log likelihoods are summed up and combined with the log prior to get the log posterior probability for each class.\n   - **Class Decision**: The class with the higher log posterior probability is chosen as the predicted class for the test sample. This approach ensures numerical stability by using log probabilities to prevent underflow.\n\nThis method efficiently computes the necessary parameters and uses them to make predictions, adhering to the principles of the Gaussian Naive Bayes algorithm."}
{"task_id": 539, "completion_id": 0, "completion": "To solve this problem, we need to compute the binary cross-entropy cost, which is a common loss function used in binary classification tasks. This cost function measures the performance of a classification model whose output is a probability between 0 and 1. The goal is to minimize this cost during training.\n\n### Approach\nThe binary cross-entropy cost is defined by the formula:\n\\[ J = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\cdot \\ln(a_i) + (1 - y_i) \\cdot \\ln(1 - a_i)] \\]\nwhere \\( m \\) is the number of training examples, \\( a_i \\) is the predicted probability for the \\( i \\)-th example, and \\( y_i \\) is the actual label (0 or 1) for the \\( i \\)-th example.\n\nTo avoid numerical instability (such as taking the logarithm of zero), we clip the predicted probabilities \\( a_i \\) to the range \\([10^{-15}, 1 - 10^{-15}]\\).\n\nThe steps to implement this are as follows:\n1. **Clip the Probabilities**: Use NumPy's `clip` function to ensure all predicted probabilities are within a safe range.\n2. **Compute Log Values**: Calculate the log of the clipped probabilities and the log of one minus the clipped probabilities.\n3. **Calculate Cross-Entropy Terms**: For each example, compute the terms \\( y_i \\cdot \\ln(a_i) \\) and \\( (1 - y_i) \\cdot \\ln(1 - a_i) \\).\n4. **Sum and Average**: Sum all the cross-entropy terms and average them by the number of examples \\( m \\).\n5. **Return Result**: Convert the result to a Python float to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\"\"\"\n    m = Y.size\n    AL_clipped = np.clip(AL, 1e-15, 1 - 1e-15)\n    cross_entropy = - (Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped))\n    cost = np.sum(cross_entropy) / m\n    return float(cost)\n```\n\n### Explanation\n1. **Clipping Probabilities**: The `np.clip` function ensures that all elements in `AL` are between \\(10^{-15}\\) and \\(1 - 10^{-15}\\), preventing logarithms of zero or one which would cause numerical issues.\n2. **Logarithm Calculation**: Using `np.log`, we compute the logarithms of the clipped probabilities and their complements efficiently.\n3. **Element-wise Operations**: By leveraging NumPy's element-wise operations, we compute the cross-entropy terms for all examples simultaneously, which is both efficient and concise.\n4. **Summation and Averaging**: The sum of all cross-entropy terms is divided by the number of examples \\( m \\) to get the average cost.\n5. **Conversion to Float**: The result is converted to a Python float to ensure the output type matches the requirement.\n\nThis approach efficiently handles the computation while ensuring numerical stability and correctness for various input shapes."}
{"task_id": 552, "completion_id": 0, "completion": "To solve this problem, we need to compute the posterior state distribution \u03b3\u209c for a given Hidden Markov Model (HMM) at a specific time index t using the forward-backward algorithm. The algorithm involves calculating forward probabilities (\u03b1) and backward probabilities (\u03b2), combining them, and normalizing to get the posterior probabilities.\n\n### Approach\n1. **Validation Check**: Ensure the time index t is within the valid range of the observation sequence length.\n2. **Forward Probabilities (\u03b1)**: Compute the probability of being in a state at time t given the observations up to that time. This is done by iterating through each time step and updating the probabilities based on the previous state probabilities, transition matrix, and emission probabilities.\n3. **Backward Probabilities (\u03b2)**: Compute the probability of observing the remaining sequence from time t+1 to the end given the current state. This is done by iterating backward from the last observation to the current time step, updating the probabilities based on the transition matrix, emission probabilities, and future state probabilities.\n4. **Combine \u03b1 and \u03b2**: Multiply the forward and backward probabilities at time t and normalize the result to get the posterior probabilities \u03b3\u209c.\n5. **Rounding**: Round each component of the resulting probabilities to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    T = len(observations)\n    if t < 0 or t >= T:\n        return []\n    \n    pi = np.array(hmm['pi'])\n    A = np.array(hmm['A'])\n    B = np.array(hmm['B'])\n    n_states = len(pi)\n    \n    # Compute forward probabilities (alpha)\n    alpha = np.zeros((T, n_states))\n    alpha[0] = pi * B[:, observations[0]]\n    \n    for time in range(1, T):\n        for j in range(n_states):\n            alpha[time, j] = B[j, observations[time]] * np.dot(alpha[time - 1], A[:, j])\n    \n    # Compute backward probabilities (beta)\n    beta = np.ones((T, n_states))\n    for time in reversed(range(T - 1)):  # time goes from T-2 down to 0\n        for i in range(n_states):\n            beta[time, i] = np.dot(A[i, :], B[:, observations[time + 1]] * beta[time + 1, :])\n    \n    # Compute gamma at time t\n    gamma = alpha[t] * beta[t]\n    gamma_sum = gamma.sum()\n    if gamma_sum == 0:\n        return [0.0] * n_states  # Edge case if all probabilities are zero (unlikely with valid HMM)\n    gamma_normalized = gamma / gamma_sum\n    \n    # Round to four decimal places and convert to list\n    rounded_gamma = [round(prob, 4) for prob in gamma_normalized.tolist()]\n    return rounded_gamma\n```\n\n### Explanation\n1. **Validation Check**: The function first checks if the time index t is within the valid range [0, len(observations) - 1]. If not, it returns an empty list.\n2. **Forward Probabilities (\u03b1)**: The forward probabilities are initialized with the initial state probabilities multiplied by the emission probabilities of the first observation. For each subsequent time step, the probabilities are updated using the previous probabilities, transition matrix, and emission probabilities.\n3. **Backward Probabilities (\u03b2)**: The backward probabilities start from the last observation with all probabilities set to 1. For each previous time step, the probabilities are updated using the transition matrix, emission probabilities of the next observation, and the future backward probabilities.\n4. **Combining \u03b1 and \u03b2**: The posterior probabilities \u03b3\u209c are computed by element-wise multiplication of the forward and backward probabilities at time t, followed by normalization to ensure they sum to 1.\n5. **Rounding**: The resulting probabilities are rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the required probabilities using dynamic programming for both forward and backward passes, ensuring accurate and efficient computation even for longer sequences."}
{"task_id": 555, "completion_id": 0, "completion": "To solve this problem, we need to implement the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm from scratch. This algorithm groups points that are densely packed together and marks points in low-density regions as noise. The key steps involve computing pairwise distances, identifying core points, and expanding clusters using a breadth-first search (BFS).\n\n### Approach\n1. **Compute Pairwise Distances**: Calculate the Euclidean distance between every pair of points to determine their proximity.\n2. **Identify Neighborhoods**: For each point, determine which other points lie within a given radius (\u03b5) to form its neighborhood.\n3. **Determine Core Points**: A point is considered a core point if its neighborhood contains at least a minimum number of points (min_samples).\n4. **Expand Clusters Using BFS**: Starting from an unlabeled core point, expand the cluster by including all reachable points through core points. Points that are not part of any cluster after processing are marked as noise.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\"\"\"\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n    \n    # Compute pairwise Euclidean distances\n    distances = np.sqrt(np.sum((data[:, np.newaxis, :] - data) ** 2, axis=2))\n    \n    # Build the neighborhood list for each sample\n    neighborhood = []\n    for i in range(n_samples):\n        mask = distances[i] <= eps\n        neighbors = np.where(mask)[0].tolist()\n        neighborhood.append(neighbors)\n    \n    # Identify core points\n    core_mask = [len(neighbors) >= min_samples for neighbors in neighborhood]\n    \n    labels = [0] * n_samples  # 0 represents unlabeled\n    cluster_id = 0\n    \n    for i in range(n_samples):\n        if labels[i] != 0:\n            continue  # Already processed\n        if not core_mask[i]:\n            continue  # Not a core point, skip\n        \n        # Start a new cluster\n        cluster_id += 1\n        labels[i] = cluster_id\n        \n        queue = deque()\n        queue.append(i)\n        \n        while queue:\n            current_point = queue.popleft()\n            for neighbor in neighborhood[current_point]:\n                if labels[neighbor] == 0:\n                    labels[neighbor] = cluster_id\n                    if core_mask[neighbor]:\n                        queue.append(neighbor)\n    \n    # Assign -1 to noise points (unlabeled points)\n    for i in range(n_samples):\n        if labels[i] == 0:\n            labels[i] = -1\n    \n    return labels\n```\n\n### Explanation\n1. **Pairwise Distance Calculation**: We compute the Euclidean distance between every pair of points using vectorized operations for efficiency.\n2. **Neighborhood Identification**: For each point, we determine which other points are within the \u03b5 radius, forming its neighborhood.\n3. **Core Point Detection**: Points with neighborhoods containing at least `min_samples` points are marked as core points.\n4. **Cluster Expansion**: Using BFS from each unlabeled core point, we expand the cluster by including all reachable points through core points. Non-core points in the neighborhood are included in the cluster but do not expand it further.\n5. **Noise Assignment**: Points that remain unlabeled after processing all core points are marked as noise (-1).\n\nThis approach efficiently groups points into clusters based on density and correctly identifies noise points, adhering to the DBSCAN algorithm's principles."}
{"task_id": 561, "completion_id": 0, "completion": "To solve this problem, we need to implement the Expectation-Maximization (EM) algorithm for a Gaussian Mixture Model (GMM) to cluster data points into K components. The solution must ensure deterministic output by sorting the components based on their means and re-labeling them accordingly.\n\n### Approach\n1. **Initialization**: For K > 1, randomly initialize the parameters (means, weights, and covariance matrices) of the Gaussian components. The means are initialized using K random data points. Covariance matrices are initialized using the covariance of the entire dataset with a small regularization term to ensure they are positive definite.\n2. **Expectation (E) Step**: Compute the responsibilities (posterior probabilities) of each data point belonging to each component using the current parameters.\n3. **Maximization (M) Step**: Update the parameters (weights, means, covariance matrices) based on the computed responsibilities.\n4. **Convergence Check**: Stop iterating when the change in mixture weights falls below a specified tolerance or the maximum number of iterations is reached.\n5. **Component Reordering**: After convergence, sort the components by the first coordinate of their means to ensure deterministic output and re-label the cluster indices accordingly.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef gmm_em(data: np.ndarray, k: int, max_iter: int = 200, epsilon: float = 1e-4) -> list[int]:\n    \"\"\"Perform Gaussian Mixture Model clustering using the EM algorithm and return deterministic cluster labels.\"\"\"\n    if k == 1:\n        return [0] * len(data)\n    \n    n_samples, d = data.shape\n    # Deterministic seed based on data and k\n    data_sum = np.sum(data)\n    seed = int(abs(data_sum * 1e6)) + k\n    np.random.seed(seed)\n    \n    # Initialize parameters\n    indices = np.random.choice(n_samples, k, replace=False)\n    mu = data[indices]\n    pi = np.ones(k) / k\n    data_cov = np.cov(data, rowvar=False) + 1e-6 * np.eye(d)\n    sigma = np.tile(data_cov, (k, 1, 1))\n    prev_pi = pi.copy()\n    \n    for _ in range(max_iter):\n        # E-step: Compute responsibilities\n        log_probs = np.zeros((n_samples, k))\n        for ki in range(k):\n            mu_k = mu[ki]\n            sigma_k = sigma[ki]\n            # Handle potential singular matrix by adding regularization\n            try:\n                sigma_inv = np.linalg.inv(sigma_k)\n                log_det = np.log(np.linalg.det(sigma_k))\n            except np.linalg.LinAlgError:\n                sigma_k_reg = sigma_k + 1e-6 * np.eye(d)\n                sigma_inv = np.linalg.inv(sigma_k_reg)\n                log_det = np.log(np.linalg.det(sigma_k_reg))\n            diff = data - mu_k\n            quadratic = np.einsum('ij,ij->i', diff @ sigma_inv, diff)\n            log_prob = np.log(pi[ki]) - 0.5 * (d * np.log(2 * np.pi) + log_det + quadratic)\n            log_probs[:, ki] = log_prob\n        \n        # Compute gamma (responsibilities)\n        max_log_probs = np.max(log_probs, axis=1, keepdims=True)\n        log_probs -= max_log_probs\n        gamma = np.exp(log_probs)\n        gamma /= gamma.sum(axis=1, keepdims=True)\n        \n        # M-step: Update parameters\n        n_k = gamma.sum(axis=0)\n        pi = n_k / n_samples\n        \n        # Prevent division by zero by adding a small epsilon\n        n_k = np.maximum(n_k, 1e-6)\n        mu = (gamma.T @ data) / n_k[:, np.newaxis]\n        \n        for ki in range(k):\n            diff = data - mu[ki]\n            sigma[ki] = (gamma[:, ki] * diff.T) @ diff / n_k[ki]\n            sigma[ki] += 1e-6 * np.eye(d)  # Regularization\n        \n        # Check for convergence\n        delta = np.max(np.abs(pi - prev_pi))\n        if delta < epsilon:\n            break\n        prev_pi = pi.copy()\n    \n    # Final E-step to get the correct responsibilities with the last parameters\n    log_probs = np.zeros((n_samples, k))\n    for ki in range(k):\n        mu_k = mu[ki]\n        sigma_k = sigma[ki]\n        sigma_inv = np.linalg.inv(sigma_k)\n        log_det = np.log(np.linalg.det(sigma_k))\n        diff = data - mu_k\n        quadratic = np.einsum('ij,ij->i', diff @ sigma_inv, diff)\n        log_prob = np.log(pi[ki]) - 0.5 * (d * np.log(2 * np.pi) + log_det + quadratic)\n        log_probs[:, ki] = log_prob\n    max_log_probs = np.max(log_probs, axis=1, keepdims=True)\n    log_probs -= max_log_probs\n    gamma = np.exp(log_probs)\n    gamma /= gamma.sum(axis=1, keepdims=True)\n    \n    # Assign clusters\n    original_assignments = np.argmax(gamma, axis=1)\n    permutation = np.argsort(mu[:, 0])\n    inv_perm = np.zeros(k, dtype=int)\n    for new_label, original_label in enumerate(permutation):\n        inv_perm[original_label] = new_label\n    new_assignments = inv_perm[original_assignments]\n    \n    return new_assignments.tolist()\n```\n\n### Explanation\n1. **Initialization**: The means are initialized using K random data points. Covariance matrices are initialized as the dataset's covariance plus a small regularization term to ensure they are positive definite.\n2. **E-Step**: Computes the responsibilities using the current parameters, applying the log-sum-exp trick for numerical stability.\n3. **M-Step**: Updates the parameters using the responsibilities. Regularization is added to covariance matrices to avoid singularities.\n4. **Convergence Check**: Iterates until the change in mixture weights is below the tolerance or the maximum iterations are reached.\n5. **Reordering Components**: After convergence, components are sorted by the first coordinate of their means, and labels are reassigned to ensure deterministic output.\n\nThis approach ensures that the output is deterministic for identical inputs by reordering components based on their means, thus providing consistent cluster labels."}
{"task_id": 562, "completion_id": 0, "completion": "To solve this problem, we need to implement the Spectral Clustering algorithm from scratch without using any third-party machine-learning libraries. The algorithm involves constructing a similarity graph, computing the Laplacian matrix, finding eigenvectors, and performing k-means clustering in the eigenvector space. Let's break down the approach and solution step-by-step.\n\n### Approach\n\n1. **Build the Similarity Matrix (W):** \n   - Compute the pairwise squared Euclidean distances between all points using the Gaussian kernel to form a fully-connected similarity graph. The similarity between points \\(i\\) and \\(j\\) is given by \\(w_{ij} = \\exp(-\\gamma \\|x_i - x_j\\|^2)\\).\n\n2. **Construct the Laplacian Matrix (L):** \n   - For the unnormalized Laplacian, compute \\(L = D - W\\), where \\(D\\) is the diagonal degree matrix.\n   - For the normalized Laplacian, compute \\(L_{sym} = D^{-1/2} (D - W) D^{-1/2}\\).\n\n3. **Compute Eigenvectors:**\n   - Find the eigenvectors corresponding to the smallest `n_cluster` eigenvalues of the Laplacian matrix. If using the normalized Laplacian, row-normalize the eigenvector matrix.\n\n4. **K-means Clustering:**\n   - Perform k-means clustering on the eigenvectors. Initialize centroids as the first `n_cluster` samples and relabel clusters based on the smallest original indices.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\"\"\"\n    \n    if n_cluster == 1:\n        return [0] * len(data)\n    \n    # Step 1: Build the similarity matrix W using Gaussian kernel\n    x_squared = np.sum(data ** 2, axis=1)\n    x_dot = data @ data.T\n    squared_distances = x_squared[:, np.newaxis] + x_squared[np.newaxis, :] - 2 * x_dot\n    W = np.exp(-gamma * squared_distances)\n    \n    # Step 2: Construct the Laplacian matrix\n    degrees = W.sum(axis=1)\n    if method == 'unnormalized':\n        L = np.diag(degrees) - W\n    elif method == 'normalized':\n        D_inv_sqrt = 1.0 / np.sqrt(degrees)\n        L = np.diag(degrees) - W\n        L = D_inv_sqrt[:, np.newaxis] * L * D_inv_sqrt[np.newaxis, :]\n    else:\n        raise ValueError(\"Method must be 'unnormalized' or 'normalized'\")\n    \n    # Step 3: Compute eigenvectors corresponding to the n_cluster smallest eigenvalues\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    k_eigenvectors = eigenvectors[:, :n_cluster]\n    \n    if method == 'normalized':\n        row_norms = np.linalg.norm(k_eigenvectors, axis=1, keepdims=True)\n        row_norms[row_norms == 0] = 1  # Avoid division by zero\n        k_eigenvectors = k_eigenvectors / row_norms\n    \n    # Step 4: Run deterministic k-means and relabel clusters\n    def run_kmeans(X, k):\n        centroids = X[:k].copy()\n        while True:\n            distances = np.linalg.norm(X[:, np.newaxis, :] - centroids, axis=2)\n            labels = np.argmin(distances, axis=1)\n            new_centroids = np.zeros_like(centroids)\n            for i in range(k):\n                mask = (labels == i)\n                if mask.any():\n                    new_centroids[i] = X[mask].mean(axis=0)\n                else:\n                    new_centroids[i] = centroids[i]\n            if np.allclose(new_centroids, centroids):\n                break\n            centroids = new_centroids.copy()\n        \n        # Relabel clusters based on the smallest original index in each cluster\n        cluster_indices = {}\n        for idx, label in enumerate(labels):\n            cluster_indices.setdefault(label, []).append(idx)\n        min_indices = {label: min(indices) for label, indices in cluster_indices.items()}\n        sorted_labels = sorted(min_indices.keys(), key=lambda x: min_indices[x])\n        label_map = {old: new for new, old in enumerate(sorted_labels)}\n        return [label_map[label] for label in labels]\n    \n    labels = run_kmeans(k_eigenvectors, n_cluster)\n    return labels\n```\n\n### Explanation\n\n1. **Similarity Matrix Construction:** The Gaussian kernel is used to compute the similarity matrix \\(W\\) based on pairwise squared distances between data points. This matrix captures the similarity between each pair of points.\n\n2. **Laplacian Matrix Construction:** Depending on the method chosen (normalized or unnormalized), the Laplacian matrix is computed. This matrix helps in capturing the structure of the data graph.\n\n3. **Eigenvector Computation:** The eigenvectors corresponding to the smallest eigenvalues of the Laplacian matrix are computed. These eigenvectors form a low-dimensional embedding of the data.\n\n4. **K-means Clustering:** The k-means algorithm is applied on the eigenvector embeddings. The initial centroids are deterministically chosen as the first `n_cluster` points. After convergence, clusters are relabeled based on the smallest original indices to ensure deterministic output labels.\n\nThis approach efficiently combines spectral techniques with k-means clustering to partition the data into meaningful clusters."}
{"task_id": 563, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\"\"\"\n    N = X_data.shape[0]\n    alpha = np.zeros(N)\n    b = 0.0\n    G = X_data @ X_data.T  # Gram matrix\n    \n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            g = np.dot(alpha * y_data, G[:, i])\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n        if not updated:\n            break\n    \n    w = np.dot(alpha * y_data, X_data)\n    w_rounded = np.round(w, 4)\n    b_rounded = np.round(b, 4)\n    \n    return (w_rounded.tolist(), float(b_rounded))\n```"}
{"task_id": 564, "completion_id": 0, "completion": "To solve this problem, we need to traverse a decision tree to predict the outcome for a given sample based on the tree's structure. The traversal rules differ depending on whether the tree is used for classification or regression. \n\n### Approach\n1. **Classification Tree**:\n   - If the current node is a leaf (contains a `y` value), return `y`.\n   - Otherwise, inspect the feature value at the index specified by the node's `label`.\n   - Traverse to the first child node whose `x` value matches the feature value. If no match is found, default to the second child in the list.\n\n2. **Regression Tree**:\n   - If the current node is a leaf (contains a `y` value), return `y`.\n   - Otherwise, inspect the feature value at the index specified by the node's `label`.\n   - Compare the feature value with the node's split-point `s`. If the value is less than or equal to `s`, traverse to the first child; otherwise, traverse to the second child.\n\n### Solution Code\n```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    current = root\n    while True:\n        if hasattr(current, 'y'):\n            return current.y\n        if task_type == 'classification':\n            feature_val = features[current.label]\n            found_child = None\n            for child in current.child:\n                if child.x == feature_val:\n                    found_child = child\n                    break\n            current = current.child[1] if found_child is None else found_child\n        else:\n            feature_val = features[current.label]\n            if feature_val <= current.s:\n                current = current.child[0]\n            else:\n                current = current.child[1]\n```\n\n### Explanation\n1. **Initialization**: Start at the root node of the tree.\n2. **Leaf Check**: At each node, check if it is a leaf node (contains a `y` value). If so, return the `y` value immediately.\n3. **Classification Handling**:\n   - Extract the feature value using the node's `label`.\n   - Search through the node's children to find the first child whose `x` value matches the feature value. If no match is found, default to the second child.\n4. **Regression Handling**:\n   - Extract the feature value using the node's `label`.\n   - Compare the feature value with the node's split-point `s` to determine which child to traverse to (first child if value \u2264 `s`, otherwise the second child).\n\nThis approach ensures that the tree is traversed efficiently according to the specified rules, leading to the correct leaf node where the prediction value is stored."}
{"task_id": 565, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that selects the feature from a dataset that maximizes the information gain using the ID3 algorithm. The information gain is calculated using the entropy of the labels and the conditional entropy after splitting the data by each feature's values.\n\n### Approach\n1. **Entropy Calculation**: Compute the entropy of the labels (H(y)) using the formula \\( H(y) = -\\sum p \\log_2 p \\), where \\( p \\) is the probability of each class in the labels.\n2. **Conditional Entropy Calculation**: For each feature, split the dataset based on the feature's unique values. For each subset of data corresponding to a feature value, compute the entropy of the labels and then calculate the weighted average entropy (H(y|A)) for that feature.\n3. **Information Gain Calculation**: Subtract the conditional entropy (H(y|A)) from the entropy of the labels (H(y)) to get the information gain (IG) for each feature.\n4. **Feature Selection**: Select the feature with the highest information gain. In case of ties, choose the feature with the smallest index.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\"\"\"\n    n_samples, n_features = X.shape\n    if n_samples == 0:\n        return (0, 0.0)\n    \n    # Compute H(y)\n    counts_y = np.bincount(y)\n    probs_y = counts_y[counts_y > 0] / n_samples\n    H_y = -np.sum(probs_y * np.log2(probs_y))\n    \n    max_ig = -float('inf')\n    best_feature = 0\n    \n    for j in range(n_features):\n        feature_col = X[:, j]\n        values, value_counts = np.unique(feature_col, return_counts=True)\n        cond_entropy = 0.0\n        \n        for v, cnt in zip(values, value_counts):\n            mask = (feature_col == v)\n            y_subset = y[mask]\n            counts_sub = np.bincount(y_subset)\n            probs_sub = counts_sub[counts_sub > 0] / cnt\n            entropy_sub = -np.sum(probs_sub * np.log2(probs_sub))\n            cond_entropy += (cnt / n_samples) * entropy_sub\n        \n        ig = H_y - cond_entropy\n        # Update if current IG is higher, or same IG but smaller index\n        if (ig > max_ig) or (ig == max_ig and j < best_feature):\n            max_ig = ig\n            best_feature = j\n    \n    max_ig_rounded = round(max_ig, 6)\n    return (best_feature, max_ig_rounded)\n```\n\n### Explanation\n1. **Entropy Calculation (H(y))**: The entropy of the labels is calculated by first counting the occurrences of each label using `np.bincount`, converting these counts to probabilities, and then computing the entropy using these probabilities.\n2. **Conditional Entropy (H(y|A))**: For each feature, the dataset is split into subsets based on the feature's unique values. For each subset, the entropy of the labels is computed and then averaged by the proportion of samples in each subset relative to the entire dataset.\n3. **Information Gain (IG)**: The information gain for each feature is computed by subtracting the conditional entropy from the label entropy. The feature with the highest information gain is selected, with ties broken by choosing the smallest feature index.\n\nThis approach ensures that we efficiently compute the information gain for each feature and select the optimal one based on the ID3 algorithm's criteria."}
{"task_id": 566, "completion_id": 0, "completion": "To solve this problem, we need to implement a simplified version of the Isolation Forest algorithm for anomaly detection. The goal is to identify outliers in a given dataset using a set of randomly constructed isolation trees. Each tree is built by recursively splitting the data using random features and thresholds until a certain depth or sample size limit is reached. The anomaly score for each data point is computed based on the average path length across all trees, and the top outliers are identified based on these scores.\n\n### Approach\n1. **Subsampling**: For each tree, draw a subsample of the data. If the sample size is larger than the dataset, use the entire dataset.\n2. **Tree Construction**: Build each tree by recursively splitting the subsample using a random feature and threshold until a depth limit or node size limit is reached.\n3. **Path Length Calculation**: For each data point, compute the path length in each tree, which is the number of edges traversed plus a correction factor based on the number of samples in the leaf node.\n4. **Anomaly Score Calculation**: Compute the average path length for each data point and convert it to an anomaly score using a predefined formula.\n5. **Outlier Detection**: Identify outliers as the top fraction of data points with the highest anomaly scores.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    np.random.seed(42)\n    \n    if epsilon == 0:\n        return []\n    \n    n_samples, n_features = data.shape\n    s = min(sample_size, n_samples)\n    \n    # Handle cases where s is 0 or 1 to avoid invalid phi calculation\n    if s <= 1:\n        phi = 1.0\n    else:\n        phi = 2 * math.log(s - 1) - 2 * (s - 1) / s\n    \n    trees = []\n    \n    for _ in range(n_trees):\n        if sample_size >= n_samples:\n            subsample = data\n        else:\n            indices = np.random.choice(n_samples, size=sample_size, replace=False)\n            subsample = data[indices]\n        \n        # Determine the height limit for this tree\n        if s == 0:\n            height_limit = 0\n        else:\n            height_limit = math.ceil(math.log2(s)) if s > 0 else 0\n        \n        # Recursive function to build the tree\n        def build_tree(node_data, current_depth):\n            if current_depth >= height_limit or len(node_data) <= 1:\n                return {'type': 'leaf', 'n': len(node_data)}\n            else:\n                feature_idx = np.random.randint(n_features)\n                min_val = np.min(node_data[:, feature_idx])\n                max_val = np.max(node_data[:, feature_idx])\n                threshold = np.random.uniform(min_val, max_val)\n                left_mask = node_data[:, feature_idx] <= threshold\n                left_data = node_data[left_mask]\n                right_data = node_data[~left_mask]\n                left_child = build_tree(left_data, current_depth + 1)\n                right_child = build_tree(right_data, current_depth + 1)\n                return {\n                    'type': 'split',\n                    'feature': feature_idx,\n                    'threshold': threshold,\n                    'left': left_child,\n                    'right': right_child\n                }\n        \n        tree = build_tree(subsample, 0)\n        trees.append(tree)\n    \n    path_lengths = np.zeros((n_samples, n_trees))\n    \n    for tree_idx, tree in enumerate(trees):\n        for data_idx in range(n_samples):\n            x = data[data_idx]\n            current_node = tree\n            depth = 0\n            while True:\n                if current_node['type'] == 'leaf':\n                    n = current_node['n']\n                    if n <= 1:\n                        c = 0.0\n                    else:\n                        c = 2 * math.log(n - 1) + 0.5772156649 - 2 * (n - 1) / n\n                    path_lengths[data_idx, tree_idx] = depth + c\n                    break\n                else:\n                    feat = current_node['feature']\n                    thresh = current_node['threshold']\n                    if x[feat] <= thresh:\n                        current_node = current_node['left']\n                    else:\n                        current_node = current_node['right']\n                    depth += 1\n    \n    avg_h = np.mean(path_lengths, axis=1)\n    scores = 2 ** (-avg_h / phi)\n    \n    if epsilon >= 1:\n        selected_indices = np.arange(n_samples)\n    else:\n        m = math.ceil(epsilon * n_samples)\n        sorted_indices = np.argsort(-scores)  # Descending order of scores\n        selected_indices = sorted_indices[:m]\n    \n    selected_indices.sort()\n    return selected_indices.tolist()\n```\n\n### Explanation\n1. **Subsampling**: For each tree, a subsample of the data is drawn. If the sample size is larger than the dataset, the entire dataset is used.\n2. **Tree Construction**: Each tree is built recursively by selecting a random feature and threshold, splitting the data until a depth limit or node size limit is reached.\n3. **Path Length Calculation**: The path length for each data point is calculated by traversing the tree and adding a correction factor based on the number of samples in the leaf node.\n4. **Anomaly Score**: The average path length across all trees is converted to an anomaly score using a predefined formula. Higher scores indicate higher likelihood of being an outlier.\n5. **Outlier Detection**: The top fraction of data points with the highest scores are identified as outliers, and their indices are returned in ascending order."}
{"task_id": 568, "completion_id": 0, "completion": "To solve this problem, we need to implement a Maximum Entropy (MaxEnt) classifier using the Generalized Iterative Scaling (GIS) algorithm. This classifier will learn the weights of features derived from the training data and use these weights to predict labels for test samples.\n\n### Approach\n1. **Feature Collection**: Identify all unique (feature index, feature value, label) triplets from the training data. These triplets form the basis of the features used in the model.\n2. **Empirical Distribution**: Compute the empirical expectation for each feature, which is the average occurrence of each feature in the training data.\n3. **Precompute Indices**: For efficiency, precompute the indices of training samples where each (feature index, feature value) pair occurs.\n4. **Compute C**: Determine the maximum number of active features for any (sample, label) pair in the training data. This helps in stabilizing the GIS algorithm.\n5. **GIS Iterations**: Iteratively update the weights of the features using the GIS algorithm until convergence or the maximum number of iterations is reached. Each iteration involves computing the model's expected feature values, adjusting the weights based on the difference between empirical and model expectations, and checking for convergence.\n6. **Prediction**: For each test sample, compute the scores for each possible label using the learned weights and predict the label with the highest score.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"Implements a Maximum Entropy classifier using Generalized Iterative Scaling.\"\"\"\n    # Collect all (i, v, l) features and their counts\n    feature_counts = defaultdict(int)\n    N, num_features = X_train.shape\n    for x, y in zip(X_train, y_train):\n        for i in range(num_features):\n            v = x[i]\n            feature_counts[(i, v, y)] += 1\n    \n    # Create a list of unique features and map each to its index\n    features = list(feature_counts.keys())\n    num_feature_pairs = len(features)\n    if num_feature_pairs == 0:\n        # Edge case: no features, return arbitrary labels\n        return [0] * X_test.shape[0]\n    feature_to_j = {key: j for j, key in enumerate(features)}\n    \n    # Compute empirical expectations\n    emp_exp = np.array([feature_counts[key] / N for key in features])\n    \n    # Precompute indices for each (i, v) pair\n    iv_to_x_indices = defaultdict(list)\n    for x_idx in range(N):\n        x = X_train[x_idx]\n        for i in range(num_features):\n            v = x[i]\n            iv_to_x_indices[(i, v)].append(x_idx)\n    \n    # Compute the maximum number of active features C\n    unique_labels = np.unique(y_train)\n    C = 0\n    for x in X_train:\n        for y in unique_labels:\n            current_sum = 0\n            for i in range(num_features):\n                v = x[i]\n                if (i, v, y) in feature_to_j:\n                    current_sum += 1\n            if current_sum > C:\n                C = current_sum\n    if C == 0:\n        C = 1  # Avoid division by zero\n    \n    # Initialize weights\n    weights = np.zeros(num_feature_pairs)\n    \n    # GIS iterations\n    K = len(unique_labels)\n    unique_labels_list = unique_labels.tolist()\n    for _ in range(n_iter):\n        # Compute probabilities P(y | x) for all x in training data\n        prob = np.zeros((N, K))\n        for x_idx in range(N):\n            x = X_train[x_idx]\n            scores = np.zeros(K)\n            for yk_idx, yk in enumerate(unique_labels):\n                score = 0.0\n                for i in range(num_features):\n                    v = x[i]\n                    key = (i, v, yk)\n                    if key in feature_to_j:\n                        j = feature_to_j[key]\n                        score += weights[j]\n                scores[yk_idx] = score\n            # Compute softmax with numerical stability\n            max_score = np.max(scores)\n            exp_scores = np.exp(scores - max_score)\n            Z = np.sum(exp_scores)\n            if Z == 0:\n                prob[x_idx] = np.ones(K) / K\n            else:\n                prob[x_idx] = exp_scores / Z\n        \n        # Compute model expectations\n        model_exp = np.zeros(num_feature_pairs)\n        for j in range(num_feature_pairs):\n            i, v, l = features[j]\n            # Find the index of l in unique_labels\n            l_idx = unique_labels_list.index(l)\n            x_indices = iv_to_x_indices.get((i, v), [])\n            if x_indices:\n                model_exp[j] = np.sum(prob[x_indices, l_idx]) / N\n        \n        # Compute delta and update weights\n        # Add small epsilon to avoid division by zero or log(0)\n        delta = (1.0 / C) * np.log(emp_exp / (model_exp + 1e-10))\n        weights += delta\n        \n        # Check convergence\n        max_delta = np.max(np.abs(delta))\n        if max_delta < epsilon:\n            break\n    \n    # Predict labels for test data\n    predictions = []\n    for x in X_test:\n        scores = np.zeros(K)\n        for yk_idx, yk in enumerate(unique_labels):\n            score = 0.0\n            for i in range(num_features):\n                v = x[i]\n                key = (i, v, yk)\n                if key in feature_to_j:\n                    j = feature_to_j[key]\n                    score += weights[j]\n            scores[yk_idx] = score\n        pred_idx = np.argmax(scores)\n        predictions.append(unique_labels[pred_idx])\n    \n    return predictions\n```\n\n### Explanation\n1. **Feature Collection**: The code collects all unique (feature index, feature value, label) triplets from the training data and counts their occurrences.\n2. **Empirical Distribution**: The empirical expectation for each feature is calculated by dividing the count of each feature by the number of training samples.\n3. **Precompute Indices**: For each (feature index, feature value) pair, the code precomputes the indices of training samples where this pair occurs to speed up subsequent calculations.\n4. **Compute C**: The maximum number of active features for any (sample, label) pair is computed to stabilize the GIS algorithm.\n5. **GIS Iterations**: The code iteratively updates the feature weights by comparing the model's expected feature values to the empirical expectations. The weights are adjusted using the GIS update rule until convergence or the maximum number of iterations is reached.\n6. **Prediction**: For each test sample, the code computes the score for each possible label using the learned weights and selects the label with the highest score as the prediction. This is done using the softmax function to ensure numerical stability."}
{"task_id": 569, "completion_id": 0, "completion": "To solve this problem, we need to analyze a collection of text documents to determine the document-frequency ratio of each word and generate a set of the most frequent words based on a specified cut-off. The document-frequency ratio is the fraction of documents in which each word appears, and the results should be ordered by the most common to the least common words.\n\n### Approach\n1. **Count Document Frequencies**: For each document, convert its words into a set to ensure each word is counted once per document. Use a counter to track how many documents each word appears in.\n2. **Calculate Ratios**: For each word, compute its document-frequency ratio by dividing the count of documents it appears in by the total number of documents. Round this ratio to four decimal places.\n3. **Sort Words by Frequency**: Sort the words based on their document-frequency ratios in descending order. If two words have the same ratio, sort them lexicographically to ensure a deterministic order.\n4. **Generate Result Set**: Depending on the cut-off value, generate a set containing the top-k most frequent words. If the cut-off is `None`, include all words; if it is zero, return an empty set.\n\n### Solution Code\n```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\"\"\"\n\n    doc_counts = Counter()\n    total_docs = len(data)\n    for doc in data:\n        _, words = doc\n        unique_words = set(words)\n        for word in unique_words:\n            doc_counts[word] += 1\n\n    ratios = []\n    for word, count in doc_counts.items():\n        ratio = count / total_docs if total_docs != 0 else 0.0\n        ratio_rounded = round(ratio, 4)\n        ratios.append((word, ratio_rounded))\n\n    # Sort by descending ratio, then ascending word to ensure deterministic order\n    sorted_ratios = sorted(ratios, key=lambda x: (-x[1], x[0]))\n\n    if cut_off is None:\n        words_set = {word for word, _ in sorted_ratios}\n    else:\n        if cut_off <= 0:\n            words_set = set()\n        else:\n            k = min(cut_off, len(sorted_ratios))\n            words_set = {word for word, _ in sorted_ratios[:k]}\n\n    return (sorted_ratios, words_set)\n```\n\n### Explanation\n1. **Document Frequency Counting**: We iterate through each document, converting the list of words into a set to avoid counting duplicates within the same document. This helps in accurately determining how many documents each word appears in.\n2. **Ratio Calculation**: For each word, we calculate the ratio by dividing the count of documents it appears in by the total number of documents. This ratio is then rounded to four decimal places.\n3. **Sorting**: Words are sorted primarily by their document-frequency ratio in descending order. If two words have the same ratio, they are sorted lexicographically to maintain a consistent order.\n4. **Result Set Generation**: Based on the cut-off value, we generate a set of the top-k words. If no cut-off is specified, all words are included. If the cut-off is zero, an empty set is returned. If the cut-off exceeds the number of distinct words, all words are included.\n\nThis approach ensures that we efficiently compute the required statistics and handle edge cases such as empty input data and varying cut-off values."}
{"task_id": 571, "completion_id": 0, "completion": "To solve this problem, we need to compute the weight vector and bias term of a linear Support Vector Machine (SVM) using the optimized Lagrange multipliers obtained from the Sequential Minimal Optimization (SMO) algorithm. The solution involves applying mathematical formulas to derive these parameters efficiently using matrix operations.\n\n### Approach\n1. **Compute the Weight Vector (w):**\n   - The weight vector is calculated using the formula \\( \\mathbf{w} = X^\\top (\\boldsymbol{\\alpha} \\odot \\mathbf{y}) \\), where \\( X \\) is the training matrix, \\( \\boldsymbol{\\alpha} \\) is the vector of Lagrange multipliers, and \\( \\mathbf{y} \\) is the label vector. This is efficiently computed using matrix multiplication.\n   \n2. **Compute the Bias Term (b):**\n   - The bias term is derived as the average of the residuals \\( (y_i - \\mathbf{w}^\\top \\mathbf{x}_i) \\) for all samples. This is done by first computing the dot product of the weight vector with each sample, subtracting these values from the corresponding labels, and then taking the mean of the resulting residuals.\n\n3. **Rounding:**\n   - Both the weight vector and the bias term are rounded to four decimal places to meet the specified precision requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    # Compute the weight vector using the formula w = X^T (alpha * y)\n    w = X.T @ (alpha * y)\n    # Round each element of w to four decimal places and convert to a list\n    w_rounded = np.round(w, 4)\n    w_list = w_rounded.tolist()\n    \n    # Compute the bias term using the average of (y_i - w^T x_i)\n    residuals = y - X @ w\n    b = np.round(residuals.mean(), 4)\n    \n    return (w_list, float(b))\n```\n\n### Explanation\n- **Weight Vector Calculation:** The weight vector is obtained by transposing the training matrix \\( X \\) and multiplying it with the element-wise product of the Lagrange multipliers \\( \\boldsymbol{\\alpha} \\) and the labels \\( \\mathbf{y} \\). This operation efficiently sums the contributions of each sample scaled by their respective Lagrange multipliers and labels.\n  \n- **Bias Term Calculation:** The bias term is computed by first determining the residuals (differences between the actual labels and the predicted values using the weight vector). The mean of these residuals gives the bias term, which is then rounded to four decimal places.\n\nThis approach ensures that the computations are both efficient and numerically precise, leveraging NumPy's optimized matrix operations for performance and accuracy. The results are formatted to meet the specified precision requirements and returned in the required format."}
{"task_id": 572, "completion_id": 0, "completion": "To solve this problem, we need to implement the Local Outlier Factor (LOF) algorithm from scratch to detect anomalous samples in a dataset. The LOF algorithm identifies outliers based on the local density of data points compared to their neighbors. \n\n### Approach\nThe LOF algorithm involves several key steps:\n1. **Compute Pairwise Euclidean Distances**: Calculate the Euclidean distance between every pair of points in the dataset.\n2. **Determine k-Distance and Neighbors**: For each point, find the distance to its k-th nearest neighbor (k-distance) and the indices of those k nearest neighbors.\n3. **Reachability Distance**: For each point, compute the reachability distance to its neighbors, which is the maximum of the k-distance of the neighbor and the actual distance.\n4. **Local Reachability Density (LRD)**: Calculate the LRD for each point, which is the inverse of the average reachability distance of its k neighbors.\n5. **Local Outlier Factor (LOF)**: Compute the LOF for each point as the ratio of the average LRD of its neighbors to its own LRD.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    n = data.shape[0]\n    # Compute pairwise Euclidean distance matrix\n    distance_matrix = np.sqrt(np.sum((data[:, np.newaxis] - data) ** 2, axis=2))\n    \n    # Calculate k-distance and neighbors for each point\n    k_distances = []\n    neighbors = []\n    for p in range(n):\n        row = distance_matrix[p]\n        sorted_indices = np.argsort(row)\n        sorted_distances = row[sorted_indices]\n        k_distance = sorted_distances[k]\n        k_neighbors = sorted_indices[1:k+1].tolist()  # Exclude the point itself\n        k_distances.append(k_distance)\n        neighbors.append(k_neighbors)\n    \n    # Compute Local Reachability Density (LRD)\n    LRD = np.zeros(n)\n    for p in range(n):\n        sum_reach = 0.0\n        for q in neighbors[p]:\n            reach_dist = max(k_distances[q], distance_matrix[p, q])\n            sum_reach += reach_dist\n        LRD[p] = k / sum_reach if sum_reach != 0 else 0.0\n    \n    # Compute Local Outlier Factor (LOF)\n    LOF = np.zeros(n)\n    for p in range(n):\n        sum_LRD = 0.0\n        for q in neighbors[p]:\n            sum_LRD += LRD[q]\n        avg_LRD = sum_LRD / k\n        LOF[p] = avg_LRD / LRD[p] if LRD[p] != 0 else 0.0\n    \n    # Round LOF scores to 4 decimal places\n    LOF_rounded = np.round(LOF, 4)\n    \n    # Get indices of outliers where LOF > epsilon, sorted ascending\n    outlier_indices = np.where(LOF_rounded > epsilon)[0].tolist()\n    outlier_indices.sort()\n    \n    return (LOF_rounded.tolist(), outlier_indices)\n```\n\n### Explanation\n1. **Pairwise Euclidean Distance Matrix**: This matrix is computed using broadcasting to efficiently calculate the Euclidean distances between all pairs of points.\n2. **k-Distance and Neighbors**: For each point, the distances are sorted, and the k-th smallest distance (excluding the point itself) is determined. The indices of the k nearest neighbors are also stored.\n3. **Reachability Distance**: For each neighbor of a point, the reachability distance is computed as the maximum of the neighbor's k-distance and the actual distance to the neighbor.\n4. **Local Reachability Density (LRD)**: The LRD is the inverse of the average reachability distance of the k neighbors, providing a measure of how dense the neighborhood around a point is.\n5. **Local Outlier Factor (LOF)**: The LOF is the ratio of the average LRD of the neighbors to the LRD of the point itself, highlighting points with significantly lower density than their neighbors.\n\nThis approach ensures that we identify outliers based on local density deviations, providing a robust method for anomaly detection in various datasets."}
{"task_id": 574, "completion_id": 0, "completion": "To solve this problem, we need to generate a synthetic 2-D dataset for multiple classes, with separate training and validation sets. Each class's data points are uniformly distributed within a specific square region. The solution must ensure reproducibility when a seed is provided and return the data in a structured format with coordinates rounded to four decimal places.\n\n### Approach\n1. **Seed Initialization**: If a seed is provided, initialize the random number generator with this seed to ensure reproducibility.\n2. **Class Loop**: For each class, determine the base coordinate for the square region based on the class index.\n3. **Point Generation**: For each class, generate the required number of training and validation points. Each point's coordinates are sampled uniformly from the class's square region.\n4. **Data Collection**: Collect all generated points and their corresponding labels into lists.\n5. **Conversion and Rounding**: Convert the collected lists into NumPy arrays and round the coordinate values to four decimal places for compactness and determinism.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    X_train = []\n    X_val = []\n    y_train = []\n    y_val = []\n    \n    for c in range(m):\n        base = 10 * (c // 2)\n        low = base + 1\n        high = base + 9\n        \n        # Generate training data for class c\n        for _ in range(n_train):\n            x = random.uniform(low, high)\n            y = random.uniform(low, high)\n            X_train.append([x, y])\n            y_train.append(c)\n        \n        # Generate validation data for class c\n        for _ in range(n_val):\n            x = random.uniform(low, high)\n            y = random.uniform(low, high)\n            X_val.append([x, y])\n            y_val.append(c)\n    \n    # Convert to numpy arrays and round coordinates\n    X_train = np.array(X_train).round(4)\n    X_val = np.array(X_val).round(4)\n    y_train = np.array(y_train)\n    y_val = np.array(y_val)\n    \n    return (X_train, X_val, y_train, y_val)\n```\n\n### Explanation\n1. **Seed Initialization**: The function checks if a seed is provided and initializes the random number generator with it, ensuring that subsequent runs with the same seed produce identical results.\n2. **Class Loop**: For each class index `c`, the base coordinate for the square region is calculated using integer division. This base determines the lower bound of the square.\n3. **Point Generation**: Training and validation points are generated using the `random.uniform` function to sample coordinates within the square region defined for the class. Each point's coordinates are added to the respective lists along with the class label.\n4. **Data Conversion**: The collected points and labels are converted to NumPy arrays. The coordinates are rounded to four decimal places to meet the problem's requirements for compactness and determinism.\n5. **Return Values**: The function returns a tuple of NumPy arrays containing the training and validation data points and their corresponding labels.\n\nThis approach ensures that the generated data is reproducible, structured, and meets the specified constraints for each class's distribution and precision."}
{"task_id": 579, "completion_id": 0, "completion": "To solve this problem, we need to implement the classical K-Means clustering algorithm from scratch. The goal is to cluster a set of data points into a specified number of groups, ensuring deterministic results by using the first k points as initial cluster centers. The algorithm iteratively assigns points to the nearest cluster and updates the cluster centers until convergence or a maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Use the first k data points as the initial cluster centers.\n2. **Assignment Step**: For each data point, compute the Euclidean distance to each cluster center and assign the point to the nearest cluster. In case of a tie, the point is assigned to the cluster with the smallest index.\n3. **Update Step**: Recompute each cluster center as the mean of the points assigned to it. If a cluster has no points, its center remains unchanged.\n4. **Iteration**: Repeat the assignment and update steps until the cluster assignments stabilize or the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\"\"\"\n    m, d = X.shape\n    centers = X[:k].copy()  # Initial cluster centers\n    prev_labels = None\n\n    for _ in range(max_iters):\n        # Compute distances from each point to each center\n        distances = np.sqrt(((X[:, np.newaxis, :] - centers) ** 2).sum(axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        # Check if labels have stabilized\n        if prev_labels is not None and np.array_equal(labels, prev_labels):\n            break\n\n        # Update each cluster center\n        new_centers = np.zeros_like(centers)\n        for i in range(k):\n            mask = (labels == i)\n            if np.any(mask):\n                new_centers[i] = X[mask].mean(axis=0)\n            else:\n                new_centers[i] = centers[i]\n        centers = new_centers.copy()\n        prev_labels = labels.copy()\n\n    # Round the centers to 4 decimal places\n    rounded_centers = np.round(centers, 4)\n    return labels.tolist(), rounded_centers.tolist()\n```\n\n### Explanation\n1. **Initialization**: The initial cluster centers are set to the first k points in the dataset. This ensures deterministic results.\n2. **Distance Calculation**: For each point, the Euclidean distance to each cluster center is computed using vectorized operations for efficiency.\n3. **Assignment**: Each point is assigned to the nearest cluster center using the calculated distances. The `argmin` function handles ties by selecting the smallest cluster index.\n4. **Centroid Update**: Each cluster's center is updated to the mean of its assigned points. If a cluster has no points, its center remains unchanged to avoid division by zero.\n5. **Convergence Check**: The algorithm stops iterating once the cluster assignments no longer change between iterations or the maximum number of iterations is reached.\n6. **Rounding**: The final cluster centers are rounded to four decimal places to ensure consistent formatting for comparison.\n\nThis approach efficiently handles the iterative nature of K-Means clustering, ensuring deterministic results and correct handling of edge cases such as empty clusters."}
{"task_id": 581, "completion_id": 0, "completion": "To solve this problem, we need to implement Elastic-Net regularized linear regression using batch gradient descent. Elastic-Net combines both L1 and L2 regularization to prevent overfitting and handle multicollinearity in the dataset. The goal is to learn the weights and bias that minimize the loss function through iterative updates using gradient descent.\n\n### Approach\n1. **Initialization**: Start with all weights and the bias term set to zero.\n2. **Edge Cases Handling**: If the learning rate or the number of iterations is zero, return the initial parameters immediately.\n3. **Iterative Updates**: For each iteration:\n   - Compute the predicted values using the current weights and bias.\n   - Calculate the residuals (difference between actual and predicted values).\n   - Compute the gradients for the weights and bias, incorporating both L1 and L2 regularization terms.\n   - Update the weights and bias using the computed gradients, learning rate, and the number of samples.\n4. **Regularization**: Use L1 regularization (which can drive some weights to zero) and L2 regularization (which penalizes large weights) to ensure a balanced model complexity.\n5. **Rounding**: After completing all iterations, round the weights and bias to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    m, n = X.shape\n    if learning_rate == 0.0 or iterations == 0:\n        return ([round(0.0, 4) for _ in range(n)], round(0.0, 4))\n    \n    w = np.zeros(n)\n    b = 0.0\n    \n    for _ in range(iterations):\n        y_pred = X.dot(w) + b\n        residual = y - y_pred\n        \n        sign_w = np.where(w == 0, -1, np.sign(w))\n        grad_w = (-2 * X.T.dot(residual)) + (l1_penalty * sign_w) + (2 * l2_penalty * w)\n        grad_b = -2 * np.sum(residual)\n        \n        w -= (learning_rate * grad_w) / m\n        b -= (learning_rate * grad_b) / m\n    \n    weights_rounded = [round(weight, 4) for weight in w]\n    bias_rounded = round(b, 4)\n    return (weights_rounded, bias_rounded)\n```\n\n### Explanation\n1. **Initialization**: We start with weights `w` as a zero vector and bias `b` as 0.0.\n2. **Edge Case Check**: If the learning rate or iterations are zero, return initial zeros.\n3. **Prediction and Residual Calculation**: For each iteration, compute the predicted values and residuals.\n4. **Gradient Calculation**: Compute gradients for weights using the formula that includes both L1 and L2 regularization terms. The sign function for L1 regularization is adjusted to return -1 for zero weights to ensure correct initial updates.\n5. **Parameter Updates**: Adjust weights and bias using the computed gradients, learning rate, and the number of samples to average the gradients.\n6. **Rounding**: Finally, round the weights and bias to four decimal places to meet the output requirements.\n\nThis approach efficiently combines the benefits of both L1 and L2 regularization, ensuring a robust model that generalizes well to new data."}
{"task_id": 591, "completion_id": 0, "completion": "To solve this problem, we need to compute the output of a fully-connected feed-forward neural network with given weights and an input vector. The network uses ReLU activation for hidden layers and a linear activation for the output layer. The solution involves a forward pass through the network, applying the appropriate transformations at each layer.\n\n### Approach\n1. **Input Processing**: Start with the input vector and convert it into a numpy array for efficient numerical operations.\n2. **Layer-wise Processing**:\n   - **Bias Addition**: For each layer, prepend a bias term (1.0) to the input vector.\n   - **Weight Multiplication**: Use the given weights to compute the dot product of the augmented input vector with the weight matrix of the current layer.\n   - **Activation Function**: Apply the ReLU activation function to the output of hidden layers. The output layer uses a linear activation (no transformation).\n3. **Result Formatting**: After processing all layers, round the result to four decimal places and return it as a float if the output layer has one neuron, or as a list of floats otherwise.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    current = np.array(x, dtype=np.float64)\n    for i, layer in enumerate(weights):\n        augmented = np.insert(current, 0, 1.0)\n        layer_weights = np.array(layer, dtype=np.float64)\n        output = np.dot(augmented, layer_weights.T)\n        if i != len(weights) - 1:\n            output = np.maximum(output, 0)\n        current = output\n    rounded = np.round(current, 4)\n    return rounded.item() if len(rounded) == 1 else rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: Convert the input vector `x` to a numpy array for efficient computation.\n2. **Layer Processing**:\n   - **Bias Addition**: For each layer, prepend a 1.0 to the current input vector to include the bias term.\n   - **Weight Matrix Multiplication**: Convert the layer's weights to a numpy array and compute the dot product with the augmented input vector.\n   - **Activation Function**: Apply ReLU activation to hidden layers (all except the last layer) to introduce non-linearity.\n3. **Result Handling**: After processing all layers, round the final output to four decimal places. Return a float if the output layer has a single neuron, otherwise return a list of floats.\n\nThis approach ensures that each layer's computations are performed efficiently using numpy's optimized operations, and the result is formatted correctly according to the problem's requirements."}
{"task_id": 595, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm from scratch to compute the centroids of clusters given a dataset and the number of clusters. The solution must follow specific steps for initialization, iteration, convergence checking, and result formatting.\n\n### Approach\n1. **Initialization**: The centroids are initialized using the first `k` samples from the dataset to ensure deterministic results.\n2. **Iteration**: For up to `max_iters` iterations, each sample is assigned to the nearest centroid using squared Euclidean distance. Centroids are then updated to the mean of the samples assigned to them. If a cluster becomes empty, its centroid remains unchanged.\n3. **Convergence Check**: The algorithm stops early if all centroids move less than `1e-4` in an iteration.\n4. **Sorting and Formatting**: After convergence, centroids are sorted lexicographically and each coordinate is rounded to 4 decimal places before returning as a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Compute the centroids using K-Means clustering algorithm.\"\"\"\n    if k == 0:\n        return []\n    \n    centroids = X[:k].copy()\n    \n    for _ in range(max_iters):\n        # Calculate squared distances between each sample and each centroid\n        distances = np.sum((X[:, np.newaxis] - centroids) ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        new_centroids = np.zeros_like(centroids)\n        for i in range(k):\n            mask = (labels == i)\n            if np.any(mask):\n                new_centroids[i] = X[mask].mean(axis=0)\n            else:\n                new_centroids[i] = centroids[i]\n        \n        # Check for convergence\n        delta = np.linalg.norm(new_centroids - centroids, axis=1)\n        if np.all(delta < 1e-4):\n            break\n        \n        centroids = new_centroids\n    \n    # Sort centroids lexicographically\n    if centroids.size == 0:\n        sorted_centroids = centroids\n    else:\n        keys = [centroids[:, col] for col in reversed(range(centroids.shape[1]))]\n        order = np.lexsort(keys)\n        sorted_centroids = centroids[order]\n    \n    # Round to 4 decimal places and convert to list of lists\n    rounded_centroids = np.round(sorted_centroids, 4)\n    return rounded_centroids.tolist()\n```\n\n### Explanation\n1. **Initialization**: The centroids are initialized using the first `k` samples from the dataset to ensure deterministic results.\n2. **Distance Calculation**: For each sample, the squared Euclidean distance to each centroid is computed using broadcasting to efficiently handle the operations.\n3. **Assignment**: Each sample is assigned to the nearest centroid based on the computed distances.\n4. **Centroid Update**: New centroids are computed as the mean of the samples assigned to each cluster. If a cluster has no samples, the centroid remains unchanged.\n5. **Convergence Check**: The algorithm checks if the movement of all centroids is below the threshold `1e-4`. If so, it stops early.\n6. **Sorting and Rounding**: After convergence, centroids are sorted lexicographically and each coordinate is rounded to 4 decimal places, ensuring the result is formatted as required.\n\nThis approach efficiently handles the K-Means clustering process, ensuring deterministic results and proper formatting of the output."}
