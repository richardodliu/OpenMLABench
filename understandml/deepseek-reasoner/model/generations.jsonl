{"task_id": 0, "completion_id": 0, "completion": "To solve this problem, we need to implement Ridge Regression using its closed-form solution. Ridge Regression is a linear regression technique that includes L2 regularization to prevent overfitting by penalizing large coefficients. The closed-form solution involves matrix operations and a regularization term to ensure the matrix is invertible.\n\n### Approach\n1. **Data Preparation**: Split the input data into features (X) and target (y). The last column of the input array is the target, while the preceding columns are the features.\n2. **Bias Term Addition**: Add a column of ones to the feature matrix to account for the intercept term (bias) in the linear regression model.\n3. **Matrix Construction**: Construct the matrix \\( \\mathbf{X}^\\top \\mathbf{X} + \\alpha \\mathbf{I} \\), where \\( \\mathbf{I} \\) is the identity matrix. This matrix ensures invertibility through the regularization term \\( \\alpha \\).\n4. **Invertibility Check**: Check if the constructed matrix is invertible by computing its determinant. If the determinant is numerically close to zero (using a threshold), return -1 as the matrix is not invertible.\n5. **Weight Calculation**: If the matrix is invertible, compute the weight vector using the closed-form solution and return it rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"Compute the closed-form solution for Ridge Regression.\"\"\"\n    X = data[:, :-1]\n    y = data[:, -1]\n    \n    # Add bias term (intercept column)\n    X_b = np.hstack([np.ones((X.shape[0], 1)), X])\n    \n    n_features = X_b.shape[1]\n    A = X_b.T @ X_b + alpha * np.eye(n_features)\n    \n    det = np.linalg.det(A)\n    if np.abs(det) < 1e-12:\n        return -1\n    \n    w = np.linalg.inv(A) @ (X_b.T @ y)\n    return np.round(w, 4).tolist()\n```\n\n### Explanation\n1. **Data Splitting**: The input data is split into features (X) and target (y). This is done by slicing the input array such that the last column is the target and the rest are features.\n2. **Bias Term**: A column of ones is added to the feature matrix to account for the intercept term, which allows the model to learn a bias parameter.\n3. **Matrix Construction**: The matrix \\( \\mathbf{X}^\\top \\mathbf{X} + \\alpha \\mathbf{I} \\) is constructed. The regularization term \\( \\alpha \\mathbf{I} \\) ensures that the matrix is positive definite and invertible, especially when \\( \\alpha > 0 \\).\n4. **Determinant Check**: The determinant of the matrix is computed to check for invertibility. If the determinant is below a small threshold (1e-12), the matrix is considered singular, and -1 is returned.\n5. **Weight Calculation**: If the matrix is invertible, the weight vector is computed using the closed-form solution \\( \\hat{\\mathbf{w}} = (\\mathbf{X}^\\top \\mathbf{X} + \\alpha \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\). The weights are then rounded to four decimal places and returned as a list.\n\nThis approach efficiently combines linear algebra operations with numerical checks to ensure robustness and correctness in the Ridge Regression implementation."}
{"task_id": 1, "completion_id": 0, "completion": "To solve this problem, we need to implement the dual-form perceptron learning algorithm. This algorithm learns a linear classifier by expressing the weight vector as a linear combination of training samples using dual parameters. The key steps involve initializing these parameters, iterating through the training samples, updating the parameters when misclassifications occur, and stopping when either no updates are needed or a maximum number of updates is reached.\n\n### Approach\n1. **Initialization**: Start with dual parameters (alpha) and bias (b) set to zero.\n2. **Gram Matrix**: Precompute the Gram matrix to efficiently calculate inner products between training samples.\n3. **Iterative Update**: Scan through each sample, compute the activation using the Gram matrix, and check if the sample is misclassified. If misclassified, update the dual parameters and bias, then restart the scan from the beginning.\n4. **Stopping Conditions**: Stop when an entire pass through the data results in no updates or the maximum number of updates is reached.\n5. **Weight Vector Calculation**: Compute the primal weight vector from the dual parameters after training.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\"\"\"\n    n_samples, n_features = X_train.shape\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    updates = 0\n    gram = X_train @ X_train.T  # Precompute Gram matrix\n    i = 0\n    \n    while updates < n_iter:\n        if i >= n_samples:\n            break  # Full pass completed with no updates\n        # Compute activation for sample i\n        activation = np.dot(alpha * y_train, gram[:, i])\n        margin = y_train[i] * (activation + b)\n        if margin <= 0:\n            # Update alpha and b\n            alpha[i] += eta\n            b += eta * y_train[i]\n            updates += 1\n            i = 0  # Restart scan\n        else:\n            i += 1\n    \n    # Compute the primal weight vector w\n    w = np.dot(alpha * y_train, X_train)\n    # Round to four decimal places\n    w_rounded = [round(x, 4) for x in w]\n    b_rounded = round(b, 4)\n    \n    return (w_rounded, b_rounded)\n```\n\n### Explanation\n1. **Initialization**: The dual parameters `alpha` and bias `b` are initialized to zero. The number of updates is tracked to ensure we do not exceed the maximum allowed iterations.\n2. **Gram Matrix**: The Gram matrix is precomputed once to store all inner products of training samples, which speeds up activation calculations during each iteration.\n3. **Iterative Update**: For each sample, the activation is computed using the precomputed Gram matrix. If the sample is misclassified (margin \u2264 0), the dual parameter `alpha` and bias `b` are updated, and the scan restarts from the first sample. This ensures that updates are immediately reflected in subsequent checks.\n4. **Stopping Conditions**: The loop exits if a full pass through all samples is completed without any updates or the maximum number of updates is reached.\n5. **Weight Vector Calculation**: After training, the primal weight vector `w` is derived from the dual parameters and training data, then rounded to four decimal places along with the bias `b`.\n\nThis approach efficiently handles both linearly separable and non-separable data, ensuring convergence within a finite number of iterations for separable cases and stopping after a specified number of updates for non-separable cases."}
{"task_id": 2, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that returns a dictionary containing the link functions, their inverses, and their first derivatives for three common Generalized Linear Models (GLMs): identity, log, and logit. These functions must work with both numpy scalars and arrays, performing element-wise operations.\n\n### Approach\nThe task involves creating three dictionaries (one for each link function) where each dictionary contains three callable functions: the link function, its inverse, and its derivative. The key steps are:\n\n1. **Identity Link**: The link function is the identity function, its inverse is the same, and the derivative is always 1.\n2. **Log Link**: The link function is the natural logarithm, its inverse is the exponential function, and the derivative is the reciprocal of the input.\n3. **Logit Link**: The link function is the log-odds function, its inverse is the logistic function, and the derivative is the reciprocal of the product of the input and its complement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    return {\n        'identity': {\n            'link': lambda mu: mu,\n            'inv_link': lambda eta: eta,\n            'deriv': lambda mu: np.ones_like(mu)\n        },\n        'log': {\n            'link': np.log,\n            'inv_link': np.exp,\n            'deriv': lambda mu: 1.0 / mu\n        },\n        'logit': {\n            'link': lambda mu: np.log(mu / (1 - mu)),\n            'inv_link': lambda eta: 1 / (1 + np.exp(-eta)),\n            'deriv': lambda mu: 1 / (mu * (1 - mu))\n        }\n    }\n```\n\n### Explanation\n1. **Identity Link**:\n   - **Link Function**: Directly returns the input.\n   - **Inverse Link Function**: Directly returns the input.\n   - **Derivative**: Always returns 1, regardless of the input, using `np.ones_like` to handle array shapes.\n\n2. **Log Link**:\n   - **Link Function**: Uses `np.log` to compute the natural logarithm element-wise.\n   - **Inverse Link Function**: Uses `np.exp` to compute the exponential element-wise.\n   - **Derivative**: Computes the reciprocal of the input element-wise.\n\n3. **Logit Link**:\n   - **Link Function**: Computes the log-odds using element-wise division and logarithm.\n   - **Inverse Link Function**: Applies the logistic function using element-wise operations to map the linear predictor back to the mean.\n   - **Derivative**: Computes the reciprocal of the product of the input and its complement element-wise.\n\nThese functions are designed to handle numpy arrays and scalars efficiently, ensuring that all operations are performed element-wise, which is essential for compatibility with numpy's broadcasting and array operations."}
{"task_id": 3, "completion_id": 0, "completion": "To solve this problem, we need to compute the depth of the leaf node each sample reaches in an isolation tree. The isolation tree is constructed by recursively partitioning the data using random splits. Each split is determined by selecting a feature and a split value within the range of the selected feature's values. The depth of a sample is the number of edges from the root to the leaf node where the sample ends up.\n\n### Approach\n1. **Set Random Seed**: To ensure reproducibility, we set the random seed to 0 at the beginning of the function.\n2. **Recursive Tree Construction**: We recursively build the tree by splitting nodes into left and right children based on a randomly selected feature and split value. Each node processes the indices of the samples that reach it.\n3. **Leaf Node Condition**: If a node receives fewer than three samples, it becomes a leaf, and the depth of those samples is recorded.\n4. **Splitting Process**: For nodes with three or more samples, we split the data using a randomly selected feature and a uniformly random split value. If a split results in one of the children being empty, the current node is treated as a leaf to avoid infinite recursion.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\"\"\"\n    np.random.seed(0)\n    n_samples = data.shape[0]\n    depths = [0] * n_samples\n    indices = list(range(n_samples))\n    \n    def process_node(current_indices, current_depth):\n        if len(current_indices) < 3:\n            for i in current_indices:\n                depths[i] = current_depth\n            return\n        # Randomly select feature\n        f = np.random.randint(data.shape[1])\n        feature_values = data[current_indices, f]\n        down = np.min(feature_values)\n        up = np.max(feature_values)\n        v = np.random.uniform(down, up)\n        # Split indices into left and right\n        left = []\n        right = []\n        for i in current_indices:\n            if data[i, f] <= v:\n                left.append(i)\n            else:\n                right.append(i)\n        # Check if split is non-trivial\n        if not left or not right:\n            # Treat as leaf if split doesn't divide the data\n            for i in current_indices:\n                depths[i] = current_depth\n            return\n        # Recursively process children\n        process_node(left, current_depth + 1)\n        process_node(right, current_depth + 1)\n    \n    process_node(indices, 0)\n    return depths\n```\n\n### Explanation\n1. **Setting the Random Seed**: The random seed is set to 0 to ensure that the results are reproducible.\n2. **Initialization**: We initialize the depths list to store the depth of each sample and start with all sample indices at the root node.\n3. **Recursive Processing**: The `process_node` function handles each node. If a node has fewer than three samples, it becomes a leaf, and the current depth is recorded for those samples.\n4. **Feature and Split Selection**: For nodes with three or more samples, a feature is randomly selected, and a split value is chosen uniformly between the minimum and maximum values of the selected feature.\n5. **Splitting Check**: After splitting, if either child is empty, the current node is treated as a leaf to avoid infinite recursion. Otherwise, the function recursively processes the left and right children with incremented depth.\n\nThis approach ensures that the isolation tree is built correctly according to the problem's specifications while handling edge cases to avoid infinite recursion and ensuring termination."}
{"task_id": 4, "completion_id": 0, "completion": "To solve this problem, we need to compute the pair-wise squared Euclidean distance matrix for a given set of samples in a vectorized manner using NumPy. The solution must be efficient and avoid explicit Python loops.\n\n### Approach\nThe key steps to compute the squared Euclidean distance matrix efficiently are as follows:\n\n1. **Squared Norms Calculation**: Compute the squared norm of each sample in the input matrix. This is done by summing the squares of each element in the row. This results in a 1D array where each element is the squared norm of the corresponding sample.\n\n2. **Dot Product Matrix**: Compute the dot product matrix (Gram matrix) of the input matrix. This matrix represents the dot product of each pair of samples.\n\n3. **Distance Matrix Calculation**: Using the identity for squared Euclidean distance, construct the distance matrix by combining the squared norms and the dot product matrix. The formula used is:\n   \\[\n   \\|x - y\\|^2 = \\|x\\|^2 + \\|y\\|^2 - 2 \\cdot x^T y\n   \\]\n   Here, the squared norms are reshaped into column and row vectors to leverage broadcasting, allowing efficient matrix operations.\n\n4. **Rounding and Conversion**: Round the computed distance matrix to four decimal places and convert it into a standard Python nested list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Return the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A Python list of lists where each element [i][j] holds the squared\n        Euclidean distance between the *i*-th and *j*-th samples in *X*,\n        rounded to 4 decimal places.\n    \"\"\"\n    if X.shape[0] == 0:\n        return []\n    x_norms = np.sum(X**2, axis=1)\n    gram = X @ X.T\n    D = x_norms[:, np.newaxis] + x_norms[np.newaxis, :] - 2 * gram\n    D_rounded = np.round(D, 4)\n    return D_rounded.tolist()\n```\n\n### Explanation\n1. **Handling Empty Input**: If the input matrix has no samples (i.e., shape (0, n_features)), we immediately return an empty list.\n\n2. **Squared Norms**: The squared norms of each sample are computed using `np.sum(X**2, axis=1)`, resulting in a 1D array where each element is the sum of squares of the corresponding row in the input matrix.\n\n3. **Gram Matrix**: The dot product matrix (Gram matrix) is computed using matrix multiplication `X @ X.T`, which gives a matrix where each element (i, j) is the dot product of the i-th and j-th samples.\n\n4. **Distance Matrix Construction**: Using broadcasting, the squared norms are reshaped into column and row vectors. Their sum (after reshaping) forms a matrix where each element (i, j) is the sum of the squared norms of the i-th and j-th samples. Subtracting twice the Gram matrix from this sum gives the squared Euclidean distance matrix.\n\n5. **Rounding and Conversion**: The resulting matrix is rounded to four decimal places and converted to a nested list using `np.round` and `tolist()` for the required output format.\n\nThis approach ensures an efficient O(n_samples\u00b2) time complexity and avoids explicit loops, leveraging NumPy's optimized vector operations for performance."}
{"task_id": 6, "completion_id": 0, "completion": "To solve this problem, we need to aggregate the outputs of multiple gradient-boosted decision trees (as used in XGBoost) and convert them into class predictions. The aggregation involves summing the contributions of each tree, scaling by a learning rate, applying a softmax function to convert scores into probabilities, and then selecting the class with the highest probability.\n\n### Approach\n1. **Sum Tree Predictions**: Combine the predictions from all trees by summing them element-wise. Each tree's prediction is a 2D array, so the sum will also be a 2D array where each element represents the cumulative contribution of all trees for a specific sample and class.\n2. **Scale by Learning Rate**: Multiply the summed predictions by the learning rate and then take the negative to form the raw scores.\n3. **Apply Softmax**: Convert the raw scores into probabilities using the softmax function. This ensures that the probabilities for each sample sum to 1 and are suitable for classification.\n4. **Determine Class Labels**: For each sample, select the class with the highest probability using the argmax function. In case of ties, the first occurrence of the maximum value is chosen, as per numpy's default behavior.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef xgboost_predict(predictions: list, learning_rate: float) -> list[int]:\n    \"\"\"Aggregate XGBoost tree outputs and produce final class predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        A list where each element is a 2-D array-like object of shape\n        (n_samples, n_classes) containing the raw outputs produced by one tree.\n    learning_rate : float\n        The learning rate used by the booster.  This value scales every tree\n        output before it is added to the ensemble score.\n\n    Returns\n    -------\n    list[int]\n        A list of length *n_samples* with the predicted class label for every\n        sample.\n    \"\"\"\n    # Convert each prediction to a numpy array and sum across all trees\n    np_preds = [np.array(p) for p in predictions]\n    sum_pred = np.sum(np_preds, axis=0)\n    # Compute the scores as per XGBoost formula\n    scores = -learning_rate * sum_pred\n    # Apply softmax to each row (sample)\n    # For numerical stability, subtract the max of each row\n    max_scores = np.max(scores, axis=1, keepdims=True)\n    exp_scores = np.exp(scores - max_scores)\n    softmax_probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n    # Determine the class with the highest probability for each sample\n    pred_labels = np.argmax(softmax_probs, axis=1)\n    return pred_labels.tolist()\n```\n\n### Explanation\n1. **Summing Predictions**: Each tree's prediction is converted to a numpy array and summed across all trees. This step aggregates the contributions of all trees for each sample and class.\n2. **Scoring**: The summed predictions are scaled by the learning rate and negated to form the raw scores. This step adjusts the contributions based on the learning rate, which controls the step size during gradient descent.\n3. **Softmax Application**: The softmax function is applied row-wise to convert scores into probabilities. This involves exponentiating the scores (after adjusting for numerical stability by subtracting the maximum value) and normalizing them so that they sum to 1.\n4. **Argmax for Class Prediction**: The class with the highest probability for each sample is selected using argmax. This step ensures that we get the most likely class label for each sample, following numpy's tie-breaking rule of selecting the first occurrence of the maximum value.\n\nThis approach efficiently combines the tree outputs, converts them into probabilities, and selects the most probable class for each sample, replicating the final aggregation step of XGBoost's prediction process."}
{"task_id": 7, "completion_id": 0, "completion": "To solve this problem, we need to identify all frequent itemsets in a list of transactions given a minimum support threshold. A frequent itemset is one that appears in a sufficient number of transactions, determined by the threshold. The solution uses the Apriori algorithm, which efficiently generates candidate itemsets and prunes them based on support counts.\n\n### Approach\n1. **Calculate Minimum Support**: Convert the given minimum support threshold from a fraction to an absolute count using the ceiling of the product of the threshold and the number of transactions.\n2. **Preprocess Transactions**: Convert each transaction into a set to remove duplicates and store them for efficient subset checks.\n3. **Generate Frequent 1-Itemsets**: Count the occurrence of each item across all transactions and retain those that meet the minimum support.\n4. **Generate Candidate Itemsets**: For each subsequent level (k \u2265 2), generate candidate itemsets by joining frequent itemsets from the previous level, ensuring they are sorted and valid (all subsets are frequent).\n5. **Prune Candidates**: Check if all subsets of each candidate itemset are frequent. If not, discard the candidate.\n6. **Count Support**: For each valid candidate, count how many transactions contain the itemset and retain those meeting the minimum support.\n7. **Aggregate Results**: Collect all frequent itemsets from all levels and return them as a dictionary.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\"\"\"\n    n_trans = len(transactions)\n    if n_trans == 0 or minsup <= 0:\n        return {}\n    min_support = math.ceil(minsup * n_trans)\n    if min_support <= 0:\n        return {}\n    \n    preprocessed = [set(t) for t in transactions]\n    \n    # Generate frequent 1-itemsets\n    item_counter = Counter()\n    for t in preprocessed:\n        for item in t:\n            item_counter[item] += 1\n    F_prev = { (item,): count for item, count in item_counter.items() if count >= min_support }\n    \n    frequent_itemsets = dict(F_prev)\n    k = 2\n    \n    while F_prev:\n        itemsets = sorted(F_prev.keys())\n        Ck = []\n        # Generate candidates by joining\n        for i in range(len(itemsets)):\n            itemset1 = itemsets[i]\n            for j in range(i + 1, len(itemsets)):\n                itemset2 = itemsets[j]\n                if itemset1[:-1] == itemset2[:-1] and itemset1[-1] < itemset2[-1]:\n                    new_candidate = itemset1 + (itemset2[-1],)\n                    # Check all (k-1)-subsets\n                    valid = True\n                    for subset in combinations(new_candidate, k - 1):\n                        if subset not in F_prev:\n                            valid = False\n                            break\n                    if valid:\n                        Ck.append(new_candidate)\n        # Deduplicate\n        Ck = list(set(Ck))\n        # Compute support for candidates\n        Fk = {}\n        for candidate in Ck:\n            count = 0\n            for t in preprocessed:\n                if all(item in t for item in candidate):\n                    count += 1\n            if count >= min_support:\n                Fk[candidate] = count\n        if not Fk:\n            break\n        frequent_itemsets.update(Fk)\n        F_prev = Fk\n        k += 1\n    \n    return frequent_itemsets\n```\n\n### Explanation\n1. **Minimum Support Calculation**: Converts the relative threshold to an absolute count using the total number of transactions.\n2. **Transaction Preprocessing**: Converts each transaction into a set to handle duplicates efficiently.\n3. **Frequent 1-Itemsets**: Uses a counter to tally item occurrences and filters those meeting the minimum support.\n4. **Candidate Generation**: For each level, generates candidates by joining previous frequent itemsets, ensuring they are sorted and valid.\n5. **Pruning**: Removes candidates if any of their subsets are not frequent, ensuring only valid candidates proceed.\n6. **Support Counting**: Checks each candidate against all transactions to determine if they meet the support threshold.\n7. **Result Aggregation**: Collects all valid frequent itemsets across all levels and returns them in a sorted tuple format.\n\nThis approach efficiently narrows down candidates using the Apriori principle, ensuring that only potentially frequent itemsets are considered, thus optimizing both time and space complexity."}
{"task_id": 8, "completion_id": 0, "completion": "To solve this problem, we need to implement a binary logistic regression model from scratch using batch gradient descent. The model will be trained on a given dataset and then used to predict the class labels of a test dataset. The implementation must not use any third-party machine learning libraries.\n\n### Approach\n1. **Model Initialization**: Initialize the weights (w) and bias (b) to zero. The weights vector will have the same number of features as the training dataset.\n2. **Training Loop**: For the specified number of iterations, compute the linear combination of weights and features, apply the sigmoid function to get probabilities, calculate the error, and update the weights and bias using gradient descent.\n3. **Prediction**: After training, use the learned weights and bias to compute the probabilities for the test dataset and convert these probabilities to binary class labels using standard rounding.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        learning_rate: float = 0.1,\n        n_iterations: int = 4000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model via batch GD and predict labels.\"\"\"\n    n_samples, n_features = X_train.shape\n    w = np.zeros(n_features)\n    b = 0.0\n\n    for _ in range(n_iterations):\n        z = np.dot(X_train, w) + b\n        prob = 1 / (1 + np.exp(-z))\n        error = prob - y_train\n        dw = (1 / n_samples) * np.dot(X_train.T, error)\n        db = (1 / n_samples) * np.sum(error)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n\n    z_test = np.dot(X_test, w) + b\n    prob_test = 1 / (1 + np.exp(-z_test))\n    y_pred = np.round(prob_test).astype(int)\n    return y_pred.tolist()\n```\n\n### Explanation\n1. **Model Initialization**: The weights vector `w` is initialized to zeros with the same length as the number of features in the training data. The bias `b` is initialized to 0.0.\n2. **Training Loop**:\n   - **Linear Combination**: Compute the linear score `z` as the dot product of the training data and weights plus the bias.\n   - **Sigmoid Function**: Convert the linear scores to probabilities using the sigmoid function.\n   - **Error Calculation**: Compute the difference between the predicted probabilities and the actual labels.\n   - **Gradient Descent**: Calculate the gradients for the weights and bias using the average error across all samples. Update the weights and bias by subtracting the product of the learning rate and their respective gradients.\n3. **Prediction**: For the test data, compute the linear scores using the learned weights and bias, convert these scores to probabilities, and round these probabilities to the nearest integer (0 or 1) to get the final class labels.\n\nThis approach ensures that we correctly implement logistic regression from scratch using batch gradient descent, adhering to the problem constraints and efficiently learning the model parameters to make accurate predictions."}
{"task_id": 10, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that applies first-order exponential smoothing to a series of data points. The function should handle both scalar and vector inputs, ensuring proper broadcasting and returning the results in the specified format.\n\n### Approach\n1. **Input Handling**: Convert all inputs (previous smoothed value, current raw observation, and smoothing factor) to numpy arrays to facilitate broadcasting. This allows the function to handle scalars, lists, tuples, and numpy arrays uniformly.\n2. **Validation**: Ensure that the smoothing factor (weight) is within the valid range [0, 1]. Additionally, check that all inputs are either scalars or one-dimensional arrays to meet the problem constraints.\n3. **Computation**: Use the given formula to compute the smoothed value for each element. The formula used is \\(\\tilde{X}_t = \\varepsilon \\tilde{X}_{t-1} + (1 - \\varepsilon) X_t\\), where \\(\\varepsilon\\) is the smoothing factor.\n4. **Rounding and Return**: Round the results to four decimal places. Return a float if all inputs are scalars; otherwise, return a list of floats.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef smooth(prev, cur, weight):\n    \"\"\"Apply first-order exponential smoothing.\n\n    Parameters\n    ----------\n    prev : float | list | tuple | numpy.ndarray\n        Smoothed value from the previous time step (\\(\\tilde{X}_{t-1}\\)).\n    cur : float | list | tuple | numpy.ndarray\n        Current raw observation (\\(X_t\\)).\n    weight : float | list | tuple | numpy.ndarray\n        Smoothing factor \\(\\varepsilon\\), 0 <= weight <= 1.  Scalars or one-\n        dimensional vectors are supported; vectors are used element-wise.\n\n    Returns\n    -------\n    float or list[float]\n        Smoothed value(s) rounded to four decimal places.  A scalar is returned\n        if all inputs are scalars; otherwise a list is returned.\n    \"\"\"\n    # Convert inputs to numpy arrays and check their dimensions\n    prev = np.asarray(prev)\n    if prev.ndim not in (0, 1):\n        raise ValueError(\"prev must be a scalar or 1D array.\")\n    cur = np.asarray(cur)\n    if cur.ndim not in (0, 1):\n        raise ValueError(\"cur must be a scalar or 1D array.\")\n    weight = np.asarray(weight)\n    if weight.ndim not in (0, 1):\n        raise ValueError(\"weight must be a scalar or 1D array.\")\n    \n    # Validate weight values\n    if (weight < 0).any() or (weight > 1).any():\n        raise ValueError(\"All elements in weight must be between 0 and 1 inclusive.\")\n    \n    # Calculate the smoothed values\n    smoothed = weight * prev + (1 - weight) * cur\n    \n    # Round to four decimal places\n    rounded = np.round(smoothed, 4)\n    \n    # Determine if all inputs are scalars\n    all_scalar = (prev.ndim == 0) and (cur.ndim == 0) and (weight.ndim == 0)\n    \n    return rounded.item() if all_scalar else rounded.tolist()\n```\n\n### Explanation\n1. **Input Conversion and Validation**: The inputs are converted to numpy arrays to handle both scalar and array inputs uniformly. Each input is checked to ensure it is either a scalar (0-dimensional array) or a 1-dimensional array. This ensures that the function meets the problem constraints.\n2. **Weight Validation**: The smoothing factor (weight) is checked to ensure all values lie within [0, 1]. This is crucial as values outside this range would not make sense for exponential smoothing.\n3. **Smoothing Calculation**: The formula for exponential smoothing is applied element-wise using numpy's broadcasting capabilities. This allows the function to handle combinations of scalars and vectors efficiently.\n4. **Result Formatting**: The results are rounded to four decimal places. If all inputs are scalars, the result is returned as a float. Otherwise, the result is converted to a list of floats, ensuring the correct return type based on the input types.\n\nThis approach efficiently handles various input types and ensures accurate computation of the smoothed values while adhering to the problem constraints."}
{"task_id": 11, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm from scratch without relying on any external machine-learning libraries. The algorithm must follow specific steps to initialize cluster centers, assign samples to the nearest center, recompute centers, and handle edge cases such as empty clusters.\n\n### Approach\n1. **Initialization**: Start with the first `k` samples from the input data as the initial cluster centers.\n2. **Assignment Step**: For each sample, compute the squared Euclidean distance to each cluster center and assign the sample to the nearest cluster.\n3. **Update Centers**: Recompute each cluster's center as the mean of all samples assigned to it. If a cluster becomes empty, reinitialize its center with a random sample from the dataset.\n4. **Convergence Check**: Stop iterating when the maximum change in any cluster center's position between consecutive iterations is less than `epsilon` or when the maximum number of iterations `max_iter` is reached.\n5. **Rounding and Final Assignment**: After convergence, round each coordinate of the cluster centers to 4 decimal places and reassign each sample to the nearest cluster based on the rounded centers.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\"\"\"\n    if data.size == 0 or k == 0:\n        return [], []\n    \n    # Initialize centers with the first k samples\n    centers = data[:k].copy()\n    \n    for _ in range(max_iter):\n        # Calculate squared distances from each sample to each center\n        distances = np.sum((data[:, np.newaxis] - centers) ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        new_centers = np.zeros_like(centers)\n        for j in range(k):\n            mask = labels == j\n            if np.sum(mask) == 0:\n                # Re-initialize empty cluster with a random sample\n                new_centers[j] = data[np.random.randint(data.shape[0])]\n            else:\n                new_centers[j] = data[mask].mean(axis=0)\n        \n        # Check for convergence\n        delta = np.max(np.linalg.norm(new_centers - centers, axis=1))\n        if delta < epsilon:\n            centers = new_centers.copy()\n            break\n        centers = new_centers.copy()\n    \n    # Round the final centers to 4 decimal places\n    centers = np.round(centers, 4)\n    \n    # Reassign labels based on the rounded centers\n    distances = np.sum((data[:, np.newaxis] - centers) ** 2, axis=2)\n    labels = np.argmin(distances, axis=1).tolist()\n    \n    # Convert centers to a list of lists\n    centers_list = [center.tolist() for center in centers]\n    \n    return (centers_list, labels)\n```\n\n### Explanation\n1. **Initialization**: The initial cluster centers are selected as the first `k` samples from the input data.\n2. **Assignment Step**: For each sample, the squared Euclidean distance to each cluster center is computed using vectorized operations for efficiency. Each sample is assigned to the nearest cluster based on these distances.\n3. **Update Centers**: After assigning all samples, new cluster centers are computed as the mean of the samples in each cluster. If a cluster has no samples, its center is reinitialized with a random sample from the dataset to avoid empty clusters.\n4. **Convergence Check**: The algorithm checks if the maximum change in cluster centers between iterations is below the threshold `epsilon`. If so, it stops early; otherwise, it continues until `max_iter` is reached.\n5. **Rounding and Final Assignment**: After convergence, the cluster centers are rounded to 4 decimal places. Each sample is then reassigned to the nearest cluster based on the rounded centers to ensure the final labels correspond to the rounded centers.\n\nThis approach ensures that the algorithm efficiently clusters the data while handling edge cases such as empty clusters and providing results that meet the specified precision requirements."}
{"task_id": 12, "completion_id": 0, "completion": "To solve this problem, we need to compute the power spectrum of each frame in a given 2-D array of real-valued audio frames. The power spectrum is the squared magnitude of the discrete Fourier transform (DFT) of each frame. Additionally, we need to scale the result by the number of DFT bins if specified.\n\n### Approach\n1. **Compute the Real FFT**: For each frame (row) in the input array, compute the real-valued FFT using `numpy.fft.rfft`. This transforms each frame into its frequency components.\n2. **Compute Squared Magnitude**: Calculate the squared magnitude of each complex number in the FFT result to get the power spectrum.\n3. **Scaling**: If the `scale` parameter is set to `True`, divide each value in the power spectrum by the number of DFT bins, which is `N//2 + 1` where `N` is the length of each frame.\n4. **Rounding and Conversion**: Round the resulting values to four decimal places and convert the NumPy array to a Python list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef power_spectrum(frames: np.ndarray, scale: bool = False) -> list[list[float]]:\n    \"\"\"Compute the power spectrum for every frame of a real-valued signal.\"\"\"\n    N = frames.shape[1]\n    fft_result = np.fft.rfft(frames, axis=1)\n    power = np.abs(fft_result) ** 2\n    if scale:\n        scaling_factor = (N // 2) + 1\n        power = power / scaling_factor\n    power_rounded = np.round(power, 4)\n    return power_rounded.tolist()\n```\n\n### Explanation\n1. **Real FFT Calculation**: The `numpy.fft.rfft` function computes the FFT for real-valued input efficiently, returning only the non-redundant frequency components. This is applied along the rows (axis=1) of the input array.\n2. **Squared Magnitude**: The power spectrum is obtained by taking the squared magnitude of each complex number in the FFT result using `np.abs(fft_result) ** 2`.\n3. **Scaling**: When scaling is enabled, each element in the power spectrum is divided by the number of frequency bins, which is `N//2 + 1`. This normalizes the power spectrum values.\n4. **Rounding and Conversion**: The results are rounded to four decimal places using `np.round` and converted to a Python list of lists with `.tolist()` to meet the output requirements.\n\nThis approach efficiently computes the power spectrum for each frame while handling optional scaling, ensuring the results are formatted correctly as a list of lists with rounded values."}
{"task_id": 13, "completion_id": 0, "completion": "To solve this problem, we need to implement an item-based k-NN collaborative filtering recommender system. The goal is to recommend up to `k` items that a user has not yet rated based on their predicted attractiveness using item-item similarity.\n\n### Approach\n1. **Build Item-Item Similarity Matrix**: Compute the similarity between each pair of items using either cosine similarity or Pearson correlation. This matrix will be symmetric and of shape (n_items, n_items).\n2. **Identify Rated Items**: Determine which items the active user has already rated.\n3. **Predict Attractiveness Scores**: For each unrated item, compute a score based on the similarity matrix and the user's ratings of other items.\n4. **Sort and Recommend**: Sort the unrated items by their predicted scores in descending order, and return the top `k` items, ensuring stability by using item indices for ties.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\"\"\"\n    n_items = data.shape[1]\n    S = np.zeros((n_items, n_items))\n    \n    # Step 1: Build item-item similarity matrix S\n    for i in range(n_items):\n        for j in range(i + 1, n_items):\n            common_rated = (data[:, i] > 0) & (data[:, j] > 0)\n            if not np.any(common_rated):\n                S[i, j] = S[j, i] = 0.0\n                continue\n            \n            v1 = data[common_rated, i]\n            v2 = data[common_rated, j]\n            \n            if criterion == 'cosine':\n                std1 = np.std(v1, ddof=1)\n                if std1 > 1e-3:\n                    v1 = v1 - np.mean(v1)\n                std2 = np.std(v2, ddof=1)\n                if std2 > 1e-3:\n                    v2 = v2 - np.mean(v2)\n                \n                norm1 = np.linalg.norm(v1)\n                norm2 = np.linalg.norm(v2)\n                if norm1 == 0 or norm2 == 0:\n                    sim = 0.0\n                else:\n                    sim = np.dot(v1, v2) / (norm1 * norm2)\n            elif criterion == 'pearson':\n                if len(v1) < 2:\n                    sim = 0.0\n                else:\n                    corr = np.corrcoef(v1, v2)[0, 1]\n                    sim = 0.0 if np.isnan(corr) else corr\n            else:\n                sim = 0.0  # Invalid criterion\n            \n            S[i, j] = S[j, i] = sim\n    \n    # Step 2: Collect rated items for the active user\n    user_ratings = data[user_ind]\n    rated_items = np.where(user_ratings > 0)[0].tolist()\n    \n    if len(rated_items) == n_items:\n        return []\n    \n    # Step 3: Compute scores for unrated items\n    unrated = [t for t in range(n_items) if user_ratings[t] == 0]\n    scores = []\n    \n    for t in unrated:\n        sum_num = 0.0\n        sum_den = 0.0\n        for i in rated_items:\n            sim = S[t, i]\n            sum_num += user_ratings[i] * sim\n            sum_den += abs(sim)\n        \n        score = sum_num / sum_den if sum_den != 0 else 0.0\n        scores.append((score, t))\n    \n    # Step 4: Sort and select top k\n    sorted_scores = sorted(scores, key=lambda x: (-x[0], x[1]))\n    result = [t for (score, t) in sorted_scores[:k]]\n    \n    return result\n```\n\n### Explanation\n1. **Similarity Matrix Construction**: The code first constructs the item-item similarity matrix. For each pair of items, it finds users who rated both items. Depending on the similarity criterion (cosine or Pearson), it computes the similarity and stores it in the matrix.\n2. **Rated Items Identification**: The active user's rated items are identified using the user's rating vector.\n3. **Score Calculation**: For each unrated item, the score is calculated using the sum of the product of the user's ratings and the similarities, normalized by the sum of absolute similarities.\n4. **Sorting and Recommendation**: The unrated items are sorted by their scores in descending order, and the top `k` items are returned. If scores are tied, items with smaller indices are prioritized.\n\nThis approach ensures that the recommendations are based on the most similar items the user has already rated, providing personalized and relevant suggestions."}
{"task_id": 14, "completion_id": 0, "completion": "To solve this problem, we need to compute the binary logistic (cross-entropy) loss along with its first and second derivatives (gradient and Hessian) with respect to the model logits. The solution involves using the sigmoid function to convert logits to probabilities, clipping these probabilities to avoid numerical instability, and then applying the mathematical formulas for loss, gradient, and Hessian.\n\n### Approach\n1. **Sigmoid Function**: Convert the predicted logits to probabilities using the sigmoid function.\n2. **Clipping Probabilities**: Clip the probabilities to a small epsilon value (1e-15) to avoid log(0) errors in the loss calculation.\n3. **Compute Loss**: Calculate the cross-entropy loss using the clipped probabilities.\n4. **Compute Gradient**: Use the original (unclipped) probabilities to compute the gradient, which is the difference between the probabilities and the true labels.\n5. **Compute Hessian**: Use the original probabilities to compute the diagonal of the Hessian matrix, which is the product of each probability and its complement (p*(1-p)).\n6. **Rounding**: Round the computed loss, gradient, and Hessian values to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_metrics(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, list[float], list[float]]:\n    \"\"\"Compute binary logistic loss together with its gradient and Hessian.\"\"\"\n    epsilon = 1e-15\n    p = 1 / (1 + np.exp(-y_pred))\n    # Clip p to avoid log(0) in loss calculation\n    p_clipped = np.clip(p, epsilon, 1 - epsilon)\n    # Compute individual losses\n    loss_i = - (y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped))\n    # Compute mean loss and round\n    mean_loss = np.mean(loss_i).item()\n    mean_loss_rounded = round(mean_loss, 4)\n    # Compute gradient and hessian using original p\n    gradient = (p - y).tolist()\n    hessian = (p * (1 - p)).tolist()\n    # Round gradient and hessian elements to 4 decimals\n    gradient_rounded = [round(g, 4) for g in gradient]\n    hessian_rounded = [round(h, 4) for h in hessian]\n    return (mean_loss_rounded, gradient_rounded, hessian_rounded)\n```\n\n### Explanation\n1. **Sigmoid Function**: The sigmoid function converts logits to probabilities, ensuring they are in the range (0, 1).\n2. **Clipping**: Clipping the probabilities to [epsilon, 1-epsilon] prevents numerical instability when taking the logarithm in the loss calculation.\n3. **Loss Calculation**: The cross-entropy loss is computed using the clipped probabilities to avoid log(0) errors. The loss for each sample is averaged to get the mean loss.\n4. **Gradient**: The gradient is calculated as the difference between the original probabilities (before clipping) and the true labels. This follows the mathematical derivation of the gradient for logistic regression.\n5. **Hessian**: The Hessian diagonal entries are computed using the original probabilities, ensuring the second derivatives are accurate based on the logistic function's properties.\n6. **Rounding**: All results are rounded to four decimal places as specified, ensuring the output meets the required precision constraints.\n\nThis approach efficiently computes the necessary metrics while maintaining numerical stability and adhering to the mathematical definitions provided."}
{"task_id": 16, "completion_id": 0, "completion": "To solve this problem, we need to generate all contiguous N-grams from a given sequence. An N-gram is a sliding window of length N taken from consecutive elements of the sequence. The solution should handle different types of sequences such as lists, tuples, and strings, and return the N-grams in the order they appear in the original sequence.\n\n### Approach\n1. **Check Validity of N**: If the given N is larger than the length of the sequence, return an empty list since no valid N-grams can be formed.\n2. **Generate N-grams**: If N is valid, iterate through the sequence using a sliding window of length N. For each position in the sequence where a valid window can start, extract the window, convert it into a tuple, and collect all such tuples into a list.\n3. **Return Result**: Return the list of tuples, each representing an N-gram.\n\n### Solution Code\n```python\nfrom typing import Any, Iterable, List, Tuple\n\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    L = len(sequence)\n    if N > L:\n        return []\n    return [tuple(sequence[i:i+N]) for i in range(L - N + 1)]\n```\n\n### Explanation\n1. **Check Length**: The function first calculates the length of the input sequence. If the length is less than N, it immediately returns an empty list.\n2. **Generate N-grams**: Using a list comprehension, the function iterates over each possible starting index of the sequence. For each index, it takes a slice of length N starting from that index, converts the slice to a tuple, and collects all such tuples into a list.\n3. **Return List**: The list of tuples, each representing an N-gram, is returned. This approach efficiently handles all valid cases and ensures that the N-grams are in the correct order as they appear in the original sequence.\n\nThis solution is efficient with a time complexity of O(L), where L is the length of the sequence, since it processes each element exactly once in generating the slices. The space complexity is also O(L) as it stores all the generated N-grams in a list."}
{"task_id": 17, "completion_id": 0, "completion": "To solve this problem, we need to construct a tree from three parallel lists (items, counts, and parents) and return the root node as a nested dictionary. The tree must maintain the order of children as they first appear in the input lists.\n\n### Approach\n1. **Identify the Root Node**: The root node is identified by the index where the parent value is -1.\n2. **Build a Children Map**: Create a mapping from each parent index to a list of its children indices. This ensures that children are ordered according to their appearance in the input lists.\n3. **Recursive Tree Construction**: Starting from the root node, recursively build each node and its children using the children map to maintain the correct order.\n\n### Solution Code\n```python\ndef build_tree(items: list[str], counts: list[int], parents: list[int]) -> dict:\n    \"\"\"Construct a rooted tree from parallel *items*, *counts* and *parents* lists.\n\n    Parameters\n    ----------\n    items : list[str]\n        The label (item) stored in each node.\n    counts : list[int]\n        The frequency (count) stored in each node.\n    parents : list[int]\n        The parent index of each node; \u20131 marks the single root.\n\n    Returns\n    -------\n    dict\n        The root node represented as a nested dictionary with keys\n        'item', 'count' and 'children'.  If the inputs are empty, returns\n        an empty dictionary.\n    \"\"\"\n    if not items:\n        return {}\n    \n    # Find the root index\n    root_index = parents.index(-1)\n    \n    # Build a map from parent index to list of child indices\n    children_map = {}\n    for i in range(len(parents)):\n        parent = parents[i]\n        if parent == -1:\n            continue\n        if parent not in children_map:\n            children_map[parent] = []\n        children_map[parent].append(i)\n    \n    # Recursive function to build each node's dictionary\n    def build_node(index: int) -> dict:\n        node = {\n            'item': items[index],\n            'count': counts[index],\n            'children': []\n        }\n        # Add children in the order they were encountered in the input\n        for child_index in children_map.get(index, []):\n            child_node = build_node(child_index)\n            node['children'].append(child_node)\n        return node\n    \n    return build_node(root_index)\n```\n\n### Explanation\n1. **Root Identification**: The root node is found by locating the index with a parent value of -1.\n2. **Children Map Construction**: As we iterate through each node, we map each parent index to a list of its children indices. This ensures that children are added in the order they appear in the input lists.\n3. **Recursive Tree Building**: Starting from the root node, each node's dictionary is constructed recursively. The children of each node are built using the precomputed children map, ensuring the correct order is maintained.\n\nThis approach efficiently constructs the tree in O(n) time complexity, where n is the number of nodes, by leveraging a dictionary to map parent-child relationships and recursion to build the nested structure."}
{"task_id": 18, "completion_id": 0, "completion": "To solve this problem, we need to create a lightweight replacement for NumPy's `zeros` function that generates an array filled with zeros of a specified shape and data type. The solution should handle both single-dimensional and multi-dimensional shapes and allow an optional data type specification.\n\n### Approach\nThe approach involves utilizing NumPy's existing `zeros` function to generate the desired array. The key steps are as follows:\n1. **Shape Handling**: The `shape` parameter can be an integer (for a 1-D array) or an iterable of integers (for multi-dimensional arrays). NumPy's `zeros` function inherently supports both these cases, so we can pass the `shape` directly to it.\n2. **Data Type Handling**: The optional `dtype` keyword argument specifies the data type of the array. If not provided, the default data type is Python's built-in `float`, which corresponds to `np.float64` in NumPy. We extract `dtype` from the keyword arguments and pass it to NumPy's `zeros` function.\n3. **Keyword Arguments**: The function should ignore any additional keyword arguments except `dtype`, as per the problem's requirements. By extracting `dtype` and passing it explicitly, we ensure other keyword arguments are not used.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape.  If an int is provided, a 1-D array of that\n        length is created.  If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs : dict, optional\n        Additional keyword arguments.  Supported argument:\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n    \"\"\"\n    dtype = kwargs.get('dtype', float)\n    return np.zeros(shape, dtype=dtype)\n```\n\n### Explanation\n1. **Shape Handling**: The `shape` parameter is passed directly to NumPy's `zeros` function. This allows the function to handle both integer and iterable inputs naturally, as NumPy's `zeros` function is designed to accept both.\n2. **Data Type Handling**: The `dtype` is extracted from the keyword arguments using `kwargs.get('dtype', float)`, which defaults to Python's `float` if not specified. This ensures the correct data type is used when creating the array.\n3. **Efficiency**: By leveraging NumPy's optimized `zeros` function, the solution remains efficient and concise, ensuring that the generated array meets the specified requirements without unnecessary complexity.\n\nThis approach effectively combines simplicity with the robust functionality provided by NumPy, ensuring the solution is both correct and efficient."}
{"task_id": 19, "completion_id": 0, "completion": "To solve this problem, we need to determine the best split for a gradient-boosting tree node that maximizes the reduction in the regularized loss function. The split must leave at least two training instances on each side, and if no valid split yields a positive gain, we return `None`.\n\n### Approach\n1. **Iterate Over Features**: For each feature in the feature matrix, sort the feature values along with their corresponding gradients (g) and Hessians (h).\n2. **Cumulative Sums**: Compute cumulative sums of the sorted gradients and Hessians to efficiently calculate sums for potential left and right splits.\n3. **Parent Loss Calculation**: Calculate the loss for the current node (parent) using the cumulative sums of the entire dataset.\n4. **Unique Thresholds**: For each unique value of the sorted feature, consider it as a potential threshold for splitting.\n5. **Split Validity Check**: For each threshold, determine the split index and check if both left and right splits have at least two instances.\n6. **Gain Calculation**: Compute the loss for the left and right splits, then calculate the gain. Track the maximum gain and corresponding feature and threshold.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    n_samples, n_features = X.shape\n    best_gain = -np.inf\n    best_feature = None\n    best_threshold = None\n\n    for feature in range(n_features):\n        X_feature = X[:, feature]\n        sorted_indices = np.argsort(X_feature)\n        X_sorted = X_feature[sorted_indices]\n        g_sorted = g[sorted_indices]\n        h_sorted = h[sorted_indices]\n\n        cum_g = np.concatenate([[0], np.cumsum(g_sorted)])\n        cum_h = np.concatenate([[0], np.cumsum(h_sorted)])\n        total_g = cum_g[-1]\n        total_h = cum_h[-1]\n\n        denominator_parent = total_h + lam\n        if denominator_parent == 0:\n            continue  # Avoid division by zero\n        parent_loss = -(total_g ** 2) / (2 * denominator_parent) + gamma\n\n        unique_vals = np.unique(X_sorted)\n\n        for t in unique_vals:\n            split_idx = np.searchsorted(X_sorted, t, side='right')\n            left_count = split_idx\n            right_count = n_samples - split_idx\n\n            if left_count < 2 or right_count < 2:\n                continue\n\n            sum_g_left = cum_g[split_idx]\n            sum_h_left = cum_h[split_idx]\n            sum_g_right = total_g - sum_g_left\n            sum_h_right = total_h - sum_h_left\n\n            denominator_left = sum_h_left + lam\n            denominator_right = sum_h_right + lam\n\n            if denominator_left == 0 or denominator_right == 0:\n                continue  # Avoid division by zero\n\n            left_loss = -(sum_g_left ** 2) / (2 * denominator_left) + gamma\n            right_loss = -(sum_g_right ** 2) / (2 * denominator_right) + gamma\n\n            current_gain = parent_loss - (left_loss + right_loss)\n\n            if current_gain > best_gain:\n                best_gain = current_gain\n                best_feature = feature\n                best_threshold = t\n\n    if best_gain > 0:\n        return (best_feature, best_threshold)\n    else:\n        return None\n```\n\n### Explanation\n1. **Feature Sorting**: Each feature column is sorted, and corresponding gradients and Hessians are reordered to maintain consistency.\n2. **Cumulative Sums**: Cumulative sums of gradients and Hessians allow efficient calculation of sums for potential splits.\n3. **Parent Loss**: The loss for the current node is computed once per feature using the total sums of gradients and Hessians.\n4. **Threshold Evaluation**: For each unique threshold, the split index is determined, and the validity of the split is checked.\n5. **Gain Calculation**: The gain from splitting is calculated by comparing the parent loss with the combined losses of the left and right children. The best gain and corresponding feature and threshold are tracked.\n\nThis approach ensures that we efficiently evaluate all possible splits while adhering to the constraints, resulting in the optimal split that maximizes the gain."}
{"task_id": 20, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that computes both the sigmoid activation function and its gradient for a given input, which can be a scalar, a list, or a NumPy array. The function should return the results rounded to 4 decimal places, formatted appropriately based on the input type.\n\n### Approach\n1. **Input Handling**: The function first checks if the input is a scalar (integer or float). If it is, the sigmoid and gradient are computed directly and returned as rounded floats.\n2. **Array Conversion**: If the input is a list, it is converted to a NumPy array for uniform processing. If the input is already a NumPy array, it is used as-is.\n3. **Scalar Check for Arrays**: If the input is a 0-dimensional NumPy array (scalar), it is treated as a scalar, and the results are returned as rounded floats.\n4. **Array Processing**: For 1-D or 2-D arrays, the sigmoid and gradient are computed element-wise. The results are rounded to 4 decimal places and converted to Python lists before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)):\n        sig = 1 / (1 + np.exp(-x))\n        grad = sig * (1 - sig)\n        return (round(sig, 4), round(grad, 4))\n    else:\n        if isinstance(x, list):\n            arr = np.array(x)\n        else:\n            arr = x  # Assume it's a numpy array\n\n        if arr.ndim == 0:\n            sig = 1 / (1 + np.exp(-arr))\n            grad = sig * (1 - sig)\n            return (round(sig.item(), 4), round(grad.item(), 4))\n        else:\n            sig = 1 / (1 + np.exp(-arr))\n            grad = sig * (1 - sig)\n            sig_rounded = np.round(sig, 4).tolist()\n            grad_rounded = np.round(grad, 4).tolist()\n            return (sig_rounded, grad_rounded)\n```\n\n### Explanation\n- **Scalar Handling**: If the input is a scalar (integer or float), the sigmoid and gradient are computed directly using the formula and returned as rounded floats.\n- **List Conversion**: Lists are converted to NumPy arrays to leverage vectorized operations for efficient computation.\n- **0-D Array Handling**: If a NumPy array is 0-dimensional (scalar), it is processed similarly to a scalar input, and results are returned as rounded floats.\n- **Array Processing**: For 1-D or 2-D arrays, the sigmoid and gradient are computed element-wise. The results are rounded and converted to nested Python lists to maintain the input shape structure.\n\nThis approach ensures that the function works efficiently for all specified input types and returns results in the required format, adhering to the problem constraints."}
{"task_id": 21, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost algorithm using one-dimensional decision stumps as weak learners. The goal is to create an ensemble of stumps that minimizes the training error until it is less than or equal to a specified threshold `epsilon`.\n\n### Approach\n1. **Initialization**: Start with uniform weights for all training samples.\n2. **Generate Thresholds**: Compute potential thresholds for decision stumps by taking midpoints between consecutive sorted training points.\n3. **Weak Learner Selection**: For each threshold and direction (left or right of the threshold), compute the weighted error and select the stump with the minimum error.\n4. **Update Weights**: Adjust the sample weights based on the selected stump's performance, increasing weights for misclassified samples and decreasing weights for correctly classified samples.\n5. **Compute Coefficient**: Calculate the coefficient (alpha) for the selected stump using the formula derived from the weighted error.\n6. **Check Stopping Condition**: After updating the ensemble, compute the training error. If it is less than or equal to `epsilon`, stop training.\n7. **Prediction**: Use the ensemble of stumps to predict labels for the test samples by taking a weighted majority vote.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\"\"\"\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    N = len(x_train)\n    \n    if not np.all(np.isin(y_train, [-1, 1])):\n        raise ValueError(\"y_train must contain only 1 or -1\")\n    \n    sorted_x = np.sort(x_train)\n    if len(sorted_x) == 1:\n        thetas = [sorted_x[0]]\n    else:\n        thetas = [(sorted_x[i] + sorted_x[i+1]) / 2 for i in range(len(sorted_x)-1)]\n    thetas = np.array(thetas)\n    \n    D = np.ones(N) / N\n    stumps = []\n    \n    while True:\n        best_err = np.inf\n        best_d = None\n        best_theta = None\n        best_preds = None\n        \n        for theta in thetas:\n            for d in [0, 1]:\n                if d == 0:\n                    preds = np.where(x_train <= theta, 1, -1)\n                else:\n                    preds = np.where(x_train > theta, 1, -1)\n                err = np.sum(D * (preds != y_train))\n                if err < best_err:\n                    best_err = err\n                    best_d = d\n                    best_theta = theta\n                    best_preds = preds\n        \n        if best_err == 0:\n            alpha = np.inf\n        else:\n            alpha = 0.5 * np.log((1 - best_err) / best_err)\n        \n        stumps.append((alpha, best_d, best_theta))\n        \n        correct = (best_preds == y_train)\n        factors = np.where(correct, np.exp(-alpha), np.exp(alpha))\n        D *= factors\n        D /= D.sum()\n        \n        current_preds = []\n        for x in x_train:\n            sum_alpha = 0.0\n            for a, d_stump, theta_stump in stumps:\n                if d_stump == 0:\n                    h = 1 if x <= theta_stump else -1\n                else:\n                    h = 1 if x > theta_stump else -1\n                sum_alpha += a * h\n            current_pred = 1 if sum_alpha >= 0 else -1\n            current_preds.append(current_pred)\n        \n        training_error = np.mean(np.array(current_preds) != y_train)\n        if training_error <= epsilon:\n            break\n    \n    y_pred = []\n    for x in x_test:\n        sum_alpha = 0.0\n        for a, d, theta in stumps:\n            if d == 0:\n                h = 1 if x <= theta else -1\n            else:\n                h = 1 if x > theta else -1\n            sum_alpha += a * h\n        pred = 1 if sum_alpha >= 0 else -1\n        y_pred.append(pred)\n    \n    return y_pred\n```\n\n### Explanation\n1. **Initialization**: The algorithm starts with uniform weights for all training samples.\n2. **Threshold Generation**: Midpoints between consecutive sorted training points are computed to generate potential thresholds for decision stumps.\n3. **Stump Selection**: For each threshold and direction, the algorithm computes the weighted error and selects the stump with the lowest error.\n4. **Weight Update**: After selecting the best stump, sample weights are updated to focus more on misclassified samples, and the weights are normalized.\n5. **Coefficient Calculation**: The coefficient (alpha) for the selected stump is calculated based on its weighted error.\n6. **Stopping Condition**: The training error of the current ensemble is checked after each iteration. Training stops once this error is less than or equal to `epsilon`.\n7. **Prediction**: The ensemble of stumps is used to predict labels for test samples by taking a weighted majority vote of their predictions.\n\nThis approach ensures that the model iteratively improves by focusing on difficult samples, leading to a robust ensemble classifier."}
{"task_id": 22, "completion_id": 0, "completion": "To solve this problem, we need to generate a random tensor of a given shape where each element is a sum of an integer from a discrete uniform distribution and a float from a continuous uniform distribution. Additionally, if specified, we need to standardize each column (feature) of the tensor to have a mean of 0 and a standard deviation of 1, with a small constant added to the denominator to avoid division by zero.\n\n### Approach\n1. **Generate the Tensor**:\n   - Create an integer array using `np.random.randint` to sample integers uniformly from the range [-300, 299].\n   - Create a float array using `np.random.rand` to sample values uniformly from [0, 1).\n   - Sum these two arrays to form the final tensor.\n\n2. **Standardization**:\n   - If standardization is required, reshape the tensor to a 2D array where each column represents a feature.\n   - Compute the mean and standard deviation for each column.\n   - Add a small epsilon value to the standard deviation to prevent division by zero.\n   - Standardize each column by subtracting the mean and dividing by the adjusted standard deviation.\n   - Reshape the standardized tensor back to its original shape.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_tensor(shape: tuple[int, ...], standardize: bool = False) -> np.ndarray:\n    \"\"\"Generate a random real-valued tensor and optionally standardize it.\n\n    Each element is produced as an integer drawn uniformly from the inclusive\n    range [\u2212300, 299] plus a real number drawn uniformly from the half-open\n    interval [0, 1).\n\n    Args:\n        shape: Desired shape of the output tensor.\n        standardize: If True, each column is shifted to mean 0 and scaled to\n            unit variance.\n\n    Returns:\n        A NumPy ndarray containing the generated data.\n    \"\"\"\n    # Generate the offset values from discrete uniform distribution\n    offset = np.random.randint(low=-300, high=300, size=shape)\n    # Generate the u values from continuous uniform distribution [0, 1)\n    u = np.random.rand(*shape)\n    # Combine to create the tensor\n    tensor = offset.astype(np.float64) + u\n    \n    if not standardize:\n        return tensor\n    \n    # Handle standardization\n    eps = np.finfo(float).eps\n    original_shape = tensor.shape\n    # Reshape to 2D with columns as the last dimension\n    X_2d = tensor.reshape(-1, original_shape[-1])\n    # Compute means and standard deviations along the samples (axis 0)\n    means = X_2d.mean(axis=0)\n    stds = X_2d.std(axis=0, ddof=0)  # Population standard deviation\n    \n    # Add epsilon to avoid division by zero\n    stds += eps\n    \n    # Standardize the data\n    X_2d_standardized = (X_2d - means) / stds\n    \n    # Reshape back to the original shape\n    tensor_standardized = X_2d_standardized.reshape(original_shape)\n    \n    return tensor_standardized\n```\n\n### Explanation\n- **Tensor Generation**: The tensor is created by combining two arrays: one with integers sampled uniformly from [-300, 299] and another with floats sampled from [0, 1). This combination ensures each element is a sum of these two values.\n- **Standardization**: When required, the tensor is reshaped to 2D to treat each column as a feature. The mean and standard deviation of each column are computed, and each column is standardized by subtracting the mean and dividing by the standard deviation (with a small epsilon added to avoid division by zero). This ensures each column has a mean of 0 and a standard deviation of 1, within numerical precision limits. The tensor is then reshaped back to its original dimensions.\n\nThis approach efficiently handles both the generation and standardization of the tensor, ensuring it meets the specified requirements."}
{"task_id": 24, "completion_id": 0, "completion": "To solve this problem, we need to implement a single update step of the Nesterov Accelerated Gradient (NAG) optimization algorithm. This algorithm is an extension of the classical momentum method and involves a \"look-ahead\" step to compute the gradient, which helps in achieving faster convergence in many optimization problems.\n\n### Approach\n1. **Handle Initial Velocity**: If the velocity is an empty list (initial call), initialize it as a vector of zeros with the same shape as the parameter vector \\(w\\).\n2. **Compute Look-ahead Point**: Calculate the look-ahead point using the current parameters and velocity.\n3. **Gradient Evaluation**: Evaluate the gradient at the look-ahead point and clip each element of the gradient to the range \\([-1, 1]\\).\n4. **Update Velocity**: Update the velocity using the momentum coefficient and the clipped gradient.\n5. **Update Parameters**: Adjust the parameters using the updated velocity.\n6. **Round and Convert**: Round the updated parameters and velocity to four decimal places and convert them to Python lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef nesterov_update(w, velocity, grad_func, learning_rate=0.001, momentum=0.9):\n    \"\"\"Perform one Nesterov Accelerated Gradient (NAG) update.\"\"\"\n    w = np.asarray(w)\n    if not velocity:\n        velocity = np.zeros_like(w)\n    else:\n        velocity = np.asarray(velocity)\n    \n    # Compute the look-ahead point\n    look_ahead_w = w - momentum * velocity\n    \n    # Evaluate the gradient at the look-ahead point and clip\n    g = grad_func(look_ahead_w)\n    g = np.asarray(g)\n    g_clipped = np.clip(g, -1.0, 1.0)\n    \n    # Update the velocity\n    new_velocity = momentum * velocity + learning_rate * g_clipped\n    \n    # Update the parameters\n    new_w = w - new_velocity\n    \n    # Round and convert to Python lists\n    new_w_rounded = np.round(new_w, 4).tolist()\n    new_velocity_rounded = np.round(new_velocity, 4).tolist()\n    \n    return (new_w_rounded, new_velocity_rounded)\n```\n\n### Explanation\n1. **Initial Velocity Handling**: The velocity is initialized to zero if it is an empty list, ensuring the same shape as the parameter vector \\(w\\).\n2. **Look-ahead Point Calculation**: The look-ahead point \\(\\tilde{w}\\) is computed by adjusting the current parameters \\(w\\) using the current velocity scaled by the momentum coefficient.\n3. **Gradient Clipping**: The gradient evaluated at the look-ahead point is clipped to prevent excessively large updates, which helps in stabilizing the training process.\n4. **Velocity Update**: The new velocity is computed by combining the current velocity (scaled by momentum) and the clipped gradient (scaled by the learning rate).\n5. **Parameter Update**: The parameters are updated by subtracting the new velocity, which incorporates both the momentum and gradient information.\n6. **Rounding and Conversion**: The results are rounded to four decimal places and converted to Python lists to meet the output requirements.\n\nThis approach efficiently combines the momentum-based update with a look-ahead gradient evaluation, leading to more effective optimization steps compared to standard gradient descent."}
{"task_id": 25, "completion_id": 0, "completion": "To solve this problem, we need to predict the class labels for test samples using a pre-trained Gaussian Kernel Support Vector Machine (SVM). The SVM uses a Radial Basis Function (RBF) kernel, and the prediction is based on the decision function derived from the training data, Lagrange multipliers, bias term, and kernel hyper-parameter.\n\n### Approach\nThe key steps to compute the decision function for each test sample are as follows:\n1. **Compute Squared Euclidean Distances**: For each test sample, compute the squared Euclidean distance to all training samples. This is done efficiently using vectorized operations to avoid explicit loops.\n2. **Compute RBF Kernel Matrix**: Transform the squared distances into the RBF kernel values using the given gamma hyper-parameter.\n3. **Decision Function Calculation**: Use the Lagrange multipliers, training labels, and the kernel matrix to compute the decision values for each test sample.\n4. **Class Prediction**: Convert the decision values into class labels (1 or -1) based on the sign of the decision value. A non-negative decision value results in a class label of 1, otherwise -1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    # Compute squared norms of training and test samples\n    X_train_sq = np.sum(X_train**2, axis=1)\n    X_test_sq = np.sum(X_test**2, axis=1)\n    \n    # Compute the cross term between training and test samples\n    cross_term = X_train @ X_test.T\n    \n    # Compute squared Euclidean distances using broadcasting\n    squared_distances = X_train_sq[:, np.newaxis] + X_test_sq[np.newaxis, :] - 2 * cross_term\n    \n    # Calculate the Gaussian kernel matrix\n    K = np.exp(-gamma * squared_distances)\n    \n    # Compute the decision values for all test samples\n    decision_values = np.dot(alpha * y_train, K) + b\n    \n    # Determine predicted labels (1 if decision value >= 0, else -1)\n    predicted_labels = np.where(decision_values >= 0, 1, -1).astype(int).tolist()\n    \n    return predicted_labels\n```\n\n### Explanation\n1. **Squared Euclidean Distances**: The squared distances between training and test samples are computed using vectorized operations. This avoids explicit loops and leverages NumPy's efficient matrix operations. The formula used is:\n   \\[\n   \\text{squared\\_distance}(x_i, z_j) = \\|x_i\\|^2 + \\|z_j\\|^2 - 2 \\cdot x_i \\cdot z_j^T\n   \\]\n   where \\(x_i\\) is a training sample and \\(z_j\\) is a test sample.\n\n2. **RBF Kernel Matrix**: The RBF kernel values are computed by exponentiating the negative product of gamma and the squared distances matrix. This transforms distances into similarity measures.\n\n3. **Decision Values**: The decision function for each test sample is computed by taking the dot product of the Lagrange multipliers (adjusted by training labels) with the kernel matrix, then adding the bias term. This step efficiently aggregates contributions from all training samples.\n\n4. **Class Prediction**: The predicted class labels are determined by the sign of the decision values. Non-negative values are assigned class 1, and negative values are assigned class -1, ensuring the output adheres to the required format.\n\nThis approach ensures the solution is both efficient and scalable, handling arbitrary numbers of training and test samples using fully vectorized operations."}
{"task_id": 26, "completion_id": 0, "completion": "To solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function and its corresponding gradient for an input that can be a list or a nested list. The ReLU function and its gradient should be returned as nested lists preserving the original structure of the input.\n\n### Approach\n1. **Convert Input to NumPy Array**: Convert the input list (which can be a nested list) into a NumPy array. This allows us to efficiently perform element-wise operations using NumPy functions.\n2. **Compute ReLU Activation**: Using NumPy's `maximum` function, compute the ReLU activation where each element is the maximum of 0 and the element itself.\n3. **Compute Gradient Mask**: Using NumPy's boolean operations and type casting, compute the gradient mask where each element is 1 if the corresponding input element is non-negative, and 0 otherwise.\n4. **Convert Back to Nested Lists**: Convert the resulting NumPy arrays for both the activation and gradient back into nested Python lists to maintain the original structure of the input.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu(x: list[list[int | float]] | list[int | float]) -> tuple[list, list]:\n    \"\"\"Compute the element-wise ReLU activation and its gradient.\n\n    Parameters\n    ----------\n    x : list or nested list\n        Input data. Can be a 1-D list of numbers or a nested list representing\n        higher-dimensional data (e.g., a matrix).\n\n    Returns\n    -------\n    tuple of lists\n        A tuple `(activation, gradient)` where both items are Python lists in\n        the same shape as `x`.\n    \"\"\"\n    x_array = np.array(x)\n    activation = np.maximum(x_array, 0)\n    gradient = (x_array >= 0).astype(int)\n    return (activation.tolist(), gradient.tolist())\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input list is converted to a NumPy array to leverage efficient element-wise operations. This handles any nested structure automatically.\n- **ReLU Activation**: The ReLU function is applied using `np.maximum(x_array, 0)`, which ensures that all negative values in the input are set to 0 while non-negative values remain unchanged.\n- **Gradient Calculation**: The gradient is determined using a boolean mask where elements are `True` (1) if the input element is non-negative and `False` (0) otherwise. This is converted to integers using `.astype(int)`.\n- **Conversion Back to Lists**: The results are converted back to nested Python lists using the `tolist()` method to ensure the output matches the input's structure.\n\nThis approach efficiently handles any nested list structure and ensures that both the ReLU activation and gradient are computed correctly while maintaining the original shape of the input."}
{"task_id": 28, "completion_id": 0, "completion": "To solve this problem, we need to compute the optimal rank-k reconstruction of a given matrix using a truncated Singular Value Decomposition (SVD). This approach is equivalent to using the best linear autoencoder with k latent units. The solution involves checking the validity of k, performing the SVD, truncating it to k components, reconstructing the matrix, and calculating the mean squared error (MSE).\n\n### Approach\n1. **Input Validation**: Check if k is within the valid range (1 \u2264 k \u2264 min(m, n)), where m and n are the dimensions of the input matrix. If not, return -1.\n2. **Matrix Conversion**: Convert the input list of lists into a NumPy array for efficient matrix operations.\n3. **Singular Value Decomposition (SVD)**: Compute the SVD of the matrix. This decomposition breaks the matrix into three components: U, \u03a3 (a vector of singular values), and Vt (the transpose of V).\n4. **Truncation**: Truncate the SVD components to retain only the top k singular values and corresponding vectors.\n5. **Reconstruction**: Use the truncated components to reconstruct the matrix approximation.\n6. **Mean Squared Error (MSE) Calculation**: Compute the MSE using the sum of the squares of the singular values that were excluded from the truncation, divided by the total number of elements in the matrix.\n7. **Rounding and Conversion**: Round the reconstructed matrix and MSE to four decimal places and convert the matrix back to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    X_np = np.array(X)\n    m, n = X_np.shape\n    \n    if k < 1 or k > min(m, n):\n        return -1\n    \n    U, S, Vt = np.linalg.svd(X_np, full_matrices=False)\n    \n    # Truncate to rank k\n    U_k = U[:, :k]\n    S_k = S[:k]\n    Vt_k = Vt[:k, :]\n    \n    # Reconstruct X_hat\n    X_hat = U_k @ np.diag(S_k) @ Vt_k\n    \n    # Compute MSE using the sum of squares of the remaining singular values\n    mse = np.sum(S[k:] ** 2) / (m * n)\n    \n    # Rounding and converting to list of lists\n    X_hat_rounded = np.round(X_hat, 4)\n    X_hat_list = X_hat_rounded.tolist()\n    mse_rounded = round(mse, 4)\n    \n    return (X_hat_list, mse_rounded)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if k is within the valid range. If not, it returns -1.\n2. **Matrix Conversion**: The input matrix is converted to a NumPy array for efficient computation.\n3. **SVD Computation**: Using NumPy's `svd` function with `full_matrices=False`, we get the economy-sized SVD, which is more efficient for non-square matrices.\n4. **Truncation**: The matrices U, \u03a3, and Vt are truncated to retain only the top k components, which are the most significant in terms of variance.\n5. **Reconstruction**: The matrix is reconstructed using the truncated components. This is done by matrix multiplication of the truncated U, diagonal matrix of singular values, and truncated Vt.\n6. **MSE Calculation**: The MSE is computed using the sum of squares of the excluded singular values, providing an efficient and accurate measure of reconstruction error without explicitly constructing the error matrix.\n7. **Rounding and Conversion**: The reconstructed matrix and MSE are rounded to four decimal places and converted back to the required list format for output.\n\nThis approach ensures that the solution is both efficient and accurate, leveraging the mathematical properties of SVD to minimize the reconstruction error."}
{"task_id": 29, "completion_id": 0, "completion": "To solve this problem, we need to implement a simple one-dimensional gradient boosting regressor using decision stumps. The algorithm involves iteratively adding stumps (simple decision trees with one split) to reduce the residual sum of squares (RSS) until it falls below a specified tolerance. Each stump is chosen to minimize the RSS of the current residuals, and the final prediction is the sum of contributions from all stumps.\n\n### Approach\n1. **Sort Training Data**: The training data is sorted based on the feature values to facilitate the calculation of midpoints between consecutive values.\n2. **Generate Split Candidates**: Midpoints between consecutive sorted feature values are computed as potential split points.\n3. **Filter Valid Splits**: Only splits that divide the data into non-empty subsets are considered valid.\n4. **Train the Ensemble**: Iteratively select the best stump (split point) that minimizes the RSS of the residuals, update the residuals, and repeat until the RSS is below the tolerance.\n5. **Predict**: For a query point, sum the contributions from all stumps based on their split points.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                          y_train: list[float] | np.ndarray,\n                          x_query: float,\n                          epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\"\"\"\n    x_train = np.asarray(x_train)\n    y_train = np.asarray(y_train)\n    sorted_indices = np.argsort(x_train)\n    x_sorted = x_train[sorted_indices]\n    y_sorted = y_train[sorted_indices]\n    \n    split_candidates = []\n    n = len(x_sorted)\n    for i in range(n - 1):\n        split = (x_sorted[i] + x_sorted[i + 1]) / 2\n        split_candidates.append(split)\n    \n    valid_splits = []\n    for s in split_candidates:\n        mask_left = x_sorted <= s\n        mask_right = x_sorted > s\n        if np.any(mask_left) and np.any(mask_right):\n            valid_splits.append(s)\n    \n    ensemble = []\n    residuals = y_sorted.copy()\n    current_rss = np.sum(residuals ** 2)\n    \n    if valid_splits:\n        while current_rss > epsilon:\n            best_split = None\n            best_c1 = None\n            best_c2 = None\n            best_new_rss = np.inf\n            \n            for s in valid_splits:\n                mask_left = x_sorted <= s\n                mask_right = ~mask_left\n                c1 = np.mean(residuals[mask_left])\n                c2 = np.mean(residuals[mask_right])\n                predictions = np.where(mask_left, c1, c2)\n                new_residuals = residuals - predictions\n                new_rss = np.sum(new_residuals ** 2)\n                \n                if new_rss < best_new_rss:\n                    best_new_rss = new_rss\n                    best_split = s\n                    best_c1 = c1\n                    best_c2 = c2\n            \n            if best_split is None:\n                break\n            \n            ensemble.append((best_split, best_c1, best_c2))\n            mask_left = x_sorted <= best_split\n            predictions = np.where(mask_left, best_c1, best_c2)\n            residuals -= predictions\n            current_rss = best_new_rss\n            \n            if current_rss <= epsilon:\n                break\n    \n    prediction = 0.0\n    for stump in ensemble:\n        s, c1, c2 = stump\n        if x_query <= s:\n            prediction += c1\n        else:\n            prediction += c2\n    \n    return round(prediction, 4)\n```\n\n### Explanation\n1. **Sorting Data**: The input data is sorted to facilitate the calculation of midpoints between consecutive feature values.\n2. **Split Candidates**: Midpoints between consecutive sorted feature values are generated as potential split points.\n3. **Valid Splits**: Only splits that result in non-empty subsets are considered valid to avoid undefined means.\n4. **Training Loop**: The algorithm iteratively selects the best stump that minimizes the RSS of the current residuals. The residuals are updated by subtracting the predictions from the current model, and the process continues until the RSS is below the specified tolerance.\n5. **Prediction**: The prediction for a query point is the sum of contributions from all stumps, where each stump's contribution is determined based on the query point's position relative to the stump's split point.\n\nThis approach efficiently builds an ensemble of stumps to model the target variable, ensuring that each iteration reduces the residual error until the desired tolerance is met."}
{"task_id": 32, "completion_id": 0, "completion": "To solve this problem, we need to create a helper function `check_data` that sanitizes input values for numeric operations. This function will be used within a decorator `validate_input` to ensure that the inputs to a function are valid numeric types. The goal is to convert numeric strings to their corresponding float values while leaving integers and floats unchanged.\n\n### Approach\n1. **Input Sanitization**: The `check_data` function will check each input argument. If an argument is a string, it will be converted to a float. If the argument is already an integer or float, it will be left unchanged. This ensures that all inputs are numeric before they are processed further.\n2. **Decorator Usage**: The `validate_input` decorator will use the `check_data` function to sanitize inputs before passing them to the decorated function. This decorator is applied to the `sum_numbers` function, which simply returns the sum of its two arguments.\n\n### Solution Code\n```python\ndef check_data(a, b):\n    \"\"\"Validate and convert the two inputs so that both are numeric.\n\n    The function must accept integers, floats, or numeric strings.  Any string\n    should be converted with `float()`; existing numeric values stay as they\n    are.  The pair `(a, b)` must then be returned.\n    \"\"\"\n    def convert(value):\n        if isinstance(value, str):\n            return float(value)\n        return value\n    \n    return (convert(a), convert(b))\n\n# Do NOT change this decorator \u2013 it is required by the grader.\ndef validate_input(function):\n    def wrapper(a, b):\n        a, b = check_data(a, b)\n        return function(a, b)\n\n    return wrapper\n\n@validate_input\ndef sum_numbers(a, b):\n    \"\"\"Return the arithmetic sum of *a* and *b* after validation.\"\"\"\n    return a + b\n```\n\n### Explanation\n1. **check_data Function**:\n   - This function takes two arguments, `a` and `b`.\n   - Each argument is checked using a helper function `convert`.\n   - The `convert` function checks if the argument is a string. If it is, the string is converted to a float. If the argument is already an integer or float, it is returned as is.\n   - The converted values are returned as a tuple.\n\n2. **validate_input Decorator**:\n   - This decorator wraps the target function (in this case, `sum_numbers`).\n   - It uses `check_data` to sanitize the inputs `a` and `b` before passing them to the wrapped function.\n\n3. **sum_numbers Function**:\n   - This function is decorated with `validate_input`, ensuring that its inputs are sanitized.\n   - It returns the sum of `a` and `b`. The result will be an integer if both inputs are integers, otherwise a float.\n\nBy following this approach, we ensure that all inputs are valid numeric types, allowing the `sum_numbers` function to perform arithmetic operations safely and correctly."}
{"task_id": 34, "completion_id": 0, "completion": "To solve this problem, we need to generate a NumPy tensor filled with 0.0s and 1.0s based on a specified shape and sparsity value. The sparsity value represents the probability of generating a 1.0. Additionally, the solution must handle reproducibility using a seed and validate the input sparsity value.\n\n### Approach\n1. **Input Validation**: Check if the sparsity value is within the valid range [0, 1]. If not, return -1.\n2. **Seed Initialization**: If a seed is provided, initialize the NumPy random number generator with this seed to ensure reproducibility.\n3. **Generate Random Values**: Create a tensor of random values uniformly distributed between 0 and 1 using the specified shape.\n4. **Threshold Comparison**: Convert the random values to binary values (0.0 or 1.0) based on the threshold derived from the sparsity value. The threshold is calculated as \\(1 - \\text{sparsity}\\), where values greater than or equal to the threshold become 1.0, and others become 0.0.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None) -> np.ndarray | int:\n    \"\"\"Generates a binary tensor with specified shape and sparsity.\n\n    The tensor is filled with 0.0s and 1.0s, where each element has a probability\n    equal to 'sparsity' of being 1.0. If a seed is provided, the results are reproducible.\n\n    Args:\n        shape: The dimensions of the output tensor.\n        sparsity: Probability of an element being 1.0; must be in [0, 1].\n        seed: Optional seed for the random number generator.\n\n    Returns:\n        A numpy.ndarray with elements 0.0 and 1.0, or -1 if sparsity is invalid.\n    \"\"\"\n    if not (0.0 <= sparsity <= 1.0):\n        return -1\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    threshold = 1.0 - sparsity\n    random_tensor = np.random.rand(*shape)\n    binary_tensor = np.where(random_tensor >= threshold, 1.0, 0.0)\n    \n    return binary_tensor\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the sparsity value is within the valid range [0, 1]. If not, it returns -1 as specified.\n2. **Seed Initialization**: If a seed is provided, the NumPy random number generator is initialized with this seed to ensure that the same sequence of random numbers is generated each time, leading to reproducible results.\n3. **Random Tensor Generation**: Using `np.random.rand`, a tensor of the specified shape is generated with values uniformly distributed between 0 (inclusive) and 1 (exclusive).\n4. **Threshold Comparison**: The threshold is calculated as \\(1 - \\text{sparsity}\\). Each element in the random tensor is compared against this threshold. Elements greater than or equal to the threshold are set to 1.0, and those below are set to 0.0 using `np.where`.\n\nThis approach efficiently generates the desired binary tensor with the specified sparsity and shape while ensuring input validity and reproducibility through seed initialization."}
{"task_id": 35, "completion_id": 0, "completion": "To solve this problem, we need to traverse a decision tree structure to classify a given sample based on the feature values stored in the tree nodes. The goal is to start from the root node and follow the path dictated by the sample's feature values until a leaf node is reached, which will provide the classification result. If any step in the traversal is invalid, we return -1.\n\n### Approach\n1. **Initialization**: Start at the root node of the decision tree.\n2. **Traversal Loop**: Continue moving through the tree nodes until a leaf node is encountered.\n3. **Check Validity**: For each internal node (non-leaf node), check if the feature index is valid within the sample's bounds. If not, return -1.\n4. **Feature Value Lookup**: Retrieve the feature value from the sample using the current node's feature index. If this value is not present in the current node's children, return -1.\n5. **Move to Child Node**: Update the current node to the child node corresponding to the retrieved feature value.\n6. **Leaf Node Check**: Once a leaf node is reached (indicated by a feature index of -1), return the result stored in that leaf node.\n\n### Solution Code\n```python\nfrom typing import Any\n\n\nclass node:  # noqa: N801\n    \"\"\"A minimal tree node for decision-tree-like structures.\n\n    Attributes:\n        fea: int \u2013 index of the feature to split on. A value of -1 marks a leaf.\n        res: Any \u2013 label stored in the leaf. None for internal nodes.\n        child: dict \u2013 mapping from feature value to the corresponding sub-tree.\n    \"\"\"\n\n    def __init__(self, fea: int = -1, res: Any | None = None, child: dict | None = None) -> None:\n        self.fea = fea\n        self.res = res\n        self.child = child or {}\n\n\ndef classify(root: node, sample: list[Any]) -> Any:\n    \"\"\"Return the prediction obtained by traversing a *node* tree.\n\n    Args:\n        root: The root node of the decision tree.\n        sample: A list of feature values representing one data point.\n\n    Returns:\n        The label stored in the reached leaf, or -1 if the sample cannot be\n        classified.\n    \"\"\"\n    current = root\n    while current.fea != -1:\n        fea = current.fea\n        if fea < 0 or fea >= len(sample):\n            return -1\n        value = sample[fea]\n        if value not in current.child:\n            return -1\n        current = current.child[value]\n    return current.res\n```\n\n### Explanation\n- **Initialization**: The traversal starts at the root node provided.\n- **Traversal Loop**: The loop continues as long as the current node is not a leaf (indicated by `fea != -1`).\n- **Validity Check**: For each internal node, we check if the feature index is within the valid range of the sample's indices. If not, the sample cannot be classified, and we return -1.\n- **Feature Value Lookup**: The feature value from the sample is used to determine the next node in the traversal. If this value is not found in the current node's children, the traversal fails, and we return -1.\n- **Leaf Node Check**: When a leaf node is reached (feature index is -1), the result stored in that node is returned as the classification result.\n\nThis approach ensures that the sample is classified correctly if a valid path exists in the tree, and returns -1 if any step in the traversal is invalid, adhering to the problem constraints and requirements."}
{"task_id": 36, "completion_id": 0, "completion": "To solve this problem, we need to predict the outcomes of a batch of samples using a binary decision tree. The tree is structured such that non-leaf nodes split the data based on a feature and threshold, while leaf nodes provide the prediction value. The goal is to traverse the tree for each sample and collect predictions efficiently.\n\n### Approach\n1. **Tree Structure**: The tree nodes are either internal (non-leaf) nodes or leaf nodes. Internal nodes split the data based on a feature index and threshold, directing samples to the left or right child. Leaf nodes contain the prediction value.\n2. **Traversal**: For each sample, start at the root node. If the current node is a leaf, use its threshold as the prediction. Otherwise, move to the left or right child based on the feature comparison until a leaf node is reached.\n3. **Batch Processing**: Iterate over each sample in the input list, apply the traversal logic, and collect predictions in the same order as the input samples.\n\n### Solution Code\n```python\nfrom typing import List, Optional\n\nclass Node:\n    \"\"\"Binary decision-tree node (provided).\"\"\"\n    def __init__(self, left: Optional['Node'], right: Optional['Node'], rule: tuple):\n        self.left = left\n        self.right = right\n        self.feature = rule[0]\n        self.threshold = rule[1]\n\ndef predict_tree(root: Optional[Node], X: List[List[float]]) -> List[float]:\n    if root is None:\n        return []\n    predictions = []\n    for sample in X:\n        current = root\n        while True:\n            if current.left is None and current.right is None:\n                predictions.append(current.threshold)\n                break\n            feature_idx = current.feature\n            threshold = current.threshold\n            if sample[feature_idx] <= threshold:\n                current = current.left\n            else:\n                current = current.right\n    return predictions\n```\n\n### Explanation\n1. **Initial Check**: If the tree is empty (root is None), return an empty list immediately.\n2. **Sample Iteration**: For each sample in the input list, start traversing from the root node.\n3. **Node Check**: At each node, determine if it is a leaf (both children are None). If so, append its threshold (prediction) to the result list.\n4. **Feature Comparison**: For non-leaf nodes, compare the sample's feature value with the node's threshold. Move to the left child if the value is less than or equal to the threshold, otherwise move to the right child.\n5. **Prediction Collection**: Continue traversing until a leaf node is encountered, ensuring each sample's prediction is collected in the order they were provided.\n\nThis approach efficiently processes each sample by leveraging the tree structure, ensuring predictions are made in linear time relative to the number of samples and the depth of the tree."}
{"task_id": 37, "completion_id": 0, "completion": "To solve this problem, we need to construct a kd-tree (k-dimensional tree) from a given NumPy array. The kd-tree is a space-partitioning data structure that allows efficient nearest-neighbor and range searches. The tree is built by recursively partitioning the data along different dimensions, cycling through the dimensions in a specific order.\n\n### Approach\n1. **Check for Empty Input**: If the input array is empty, return `None`.\n2. **Determine the Number of Features**: The number of features (dimensions) is derived from the input array's columns.\n3. **Cycle Through Dimensions**: Use a generator to cycle through the dimensions in the order 0, 1, ..., m-1, 0, 1, ..., where m is the number of features.\n4. **Recursive Tree Construction**: For each node, determine the current splitting dimension using the generator. Sort the data along this dimension, select the median point, and split the data into left and right subsets. Recursively construct the left and right subtrees from these subsets.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import cycle\n\ndef build_kd_tree(data: np.ndarray) -> dict | None:\n    \"\"\"Build a kd-tree from the given NumPy array.\"\"\"\n    if data.size == 0:\n        return None\n    m = data.shape[1] - 1  # Number of features\n    dim_cycle = cycle(range(m))\n    return _build_kd_tree(data, dim_cycle)\n\ndef _build_kd_tree(data: np.ndarray, dim_cycle: cycle) -> dict | None:\n    if data.shape[0] == 0:\n        return None\n    \n    # Get the next splitting dimension from the cycle\n    d = next(dim_cycle)\n    \n    # Sort the data based on the current dimension\n    sorted_indices = data[:, d].argsort()\n    sorted_data = data[sorted_indices]\n    \n    # Find the median index\n    n = sorted_data.shape[0]\n    median_idx = (n - 1) // 2\n    median_row = sorted_data[median_idx]\n    \n    # Split into left and right subsets\n    left_data = sorted_data[:median_idx]\n    right_data = sorted_data[median_idx + 1:]\n    \n    # Recursively build left and right subtrees\n    left_subtree = _build_kd_tree(left_data, dim_cycle)\n    right_subtree = _build_kd_tree(right_data, dim_cycle)\n    \n    # Construct the current node\n    return {\n        \"point\": median_row[:-1].tolist(),\n        \"label\": median_row[-1].item(),\n        \"left\": left_subtree,\n        \"right\": right_subtree\n    }\n```\n\n### Explanation\n1. **Initial Check**: The function first checks if the input array is empty. If so, it returns `None`.\n2. **Cycle Generator**: A generator is created to cycle through the dimensions (0 to m-1).\n3. **Recursive Construction**: The helper function `_build_kd_tree` is called with the data and the cycle generator. This function:\n   - Determines the current splitting dimension using the generator.\n   - Sorts the data based on the current dimension.\n   - Selects the median element to split the data into left and right subsets.\n   - Recursively constructs the left and right subtrees using the subsets.\n4. **Node Construction**: Each node is represented as a dictionary with the point coordinates, label, and left/right subtrees.\n\nThis approach ensures that the kd-tree is built efficiently, with each split using the next dimension in a global cycle, adhering to the problem's requirements."}
{"task_id": 38, "completion_id": 0, "completion": "To solve this problem, we need to implement the arm-selection phase of the LinUCB algorithm for a disjoint contextual linear bandit. The goal is to select the arm that maximizes the LinUCB upper confidence bound given the context matrix, Gram matrices, covariance vectors, and an exploration coefficient.\n\n### Approach\n1. **Problem Understanding**: The LinUCB algorithm balances exploration and exploitation by calculating an upper confidence bound (UCB) for each arm. The UCB is composed of a term representing the expected reward (exploitation) and a term accounting for uncertainty (exploration).\n\n2. **Key Components**:\n   - **Context Matrix**: Each column represents the feature vector of an arm.\n   - **Gram Matrix (A) and Covariance Vector (b)**: These matrices and vectors summarize past interactions for each arm.\n   - **Exploration Coefficient (\u03b1)**: Controls the trade-off between exploration and exploitation.\n\n3. **Steps**:\n   - For each arm, compute the inverse of the Gram matrix \\( A_a \\).\n   - Calculate the parameter vector \\( \\hat{\\theta}_a \\) using the inverse of \\( A_a \\) and the covariance vector \\( b_a \\).\n   - Compute the UCB score for each arm using the context vector, parameter vector, and the exploration term.\n   - Select the arm with the highest UCB score, returning the smallest index in case of ties.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linucb_select_arm(context: np.ndarray, A: list[list[list[float]]], b: list[list[float]], alpha: float) -> int:\n    \"\"\"Select an arm according to the LinUCB rule.\"\"\"\n    n_arms = context.shape[1]\n    scores = []\n    for a in range(n_arms):\n        A_a = np.array(A[a])\n        b_a = np.array(b[a])\n        inv_A_a = np.linalg.inv(A_a)\n        theta_a = inv_A_a @ b_a\n        c_a = context[:, a]\n        term1 = np.dot(theta_a, c_a)\n        cov_term = c_a @ inv_A_a @ c_a\n        term2 = alpha * np.sqrt(cov_term)\n        scores.append(term1 + term2)\n    return np.argmax(scores)\n```\n\n### Explanation\n1. **Matrix Inversion**: For each arm, the Gram matrix \\( A_a \\) is inverted to compute the parameter vector \\( \\hat{\\theta}_a \\), which represents the learned weights from past interactions.\n2. **Parameter Vector Calculation**: The parameter vector \\( \\hat{\\theta}_a \\) is derived using the inverted Gram matrix and the covariance vector \\( b_a \\).\n3. **UCB Score Calculation**: The score for each arm combines the expected reward (dot product of \\( \\hat{\\theta}_a \\) and the context vector) and an exploration term (based on the uncertainty of the estimate).\n4. **Arm Selection**: The arm with the highest score is selected using `np.argmax`, which ensures the smallest index is chosen in case of ties.\n\nThis approach efficiently balances exploration and exploitation by leveraging matrix operations and linear algebra to compute the necessary confidence bounds for each arm, ensuring optimal arm selection in a contextual bandit setting."}
{"task_id": 39, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs a median split on a dataset along a specified column, which is a fundamental step in constructing a KD-Tree. The goal is to efficiently find the median element in the specified column and partition the dataset into elements strictly smaller and larger than this median value.\n\n### Approach\n1. **Identify the Median**: Use the `numpy.argpartition` function to find the median element in O(n) time. This function helps in partitioning the array such that the k-th element is in its sorted position, with all smaller elements before it and all larger elements after it.\n2. **Determine Pivot**: The pivot is the row index of the median element found using `argpartition`.\n3. **Partition Data**: Using the median value, create masks to identify rows with values strictly smaller and larger than the median. Convert these masks into lists of row indices, ensuring they are sorted for deterministic output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\"\"\"\n    n_samples = data.shape[0]\n    k = n_samples // 2\n    col = data[:, d]\n    partitioned_indices = np.argpartition(col, k)\n    pivot_idx = partitioned_indices[k]\n    med_val = col[pivot_idx]\n    \n    # Get left and right indices\n    left_mask = col < med_val\n    right_mask = col > med_val\n    \n    left_indices = np.flatnonzero(left_mask).tolist()\n    right_indices = np.flatnonzero(right_mask).tolist()\n    \n    # Sort the indices to ensure deterministic output\n    left_indices.sort()\n    right_indices.sort()\n    \n    return (pivot_idx, left_indices, right_indices)\n```\n\n### Explanation\n1. **Finding the Median**: The median is determined by partitioning the column using `np.argpartition`, which ensures that the k-th element (where k is the middle index) is in its correct sorted position. This allows us to find the median in O(n) time.\n2. **Pivot Identification**: The pivot index is the row index of the median value in the original dataset.\n3. **Partitioning**: Masks are created to filter rows with values strictly smaller and larger than the median. These masks are converted to lists of indices, which are then sorted to ensure the output is deterministic.\n\nThis approach efficiently partitions the dataset while adhering to the O(n) time complexity requirement for the median split, making it suitable for constructing KD-Trees."}
{"task_id": 40, "completion_id": 0, "completion": "To solve this problem, we need to convert a one-dimensional signal into overlapping frames using NumPy's stride manipulation techniques. This approach ensures that the resulting array is a view of the original data, not a copy, which is efficient in terms of memory usage. The solution must handle edge cases and enforce specific constraints on the input parameters.\n\n### Approach\n1. **Input Validation**: Check that the input array is one-dimensional, the stride is a positive integer, and the length of the input array is at least the frame width.\n2. **Calculate Number of Frames**: Determine the number of frames using the formula `(len(x) - frame_width) // stride + 1`.\n3. **Stride Calculation**: Compute the strides for the new array such that consecutive rows (frames) are separated by the specified hop length (stride).\n4. **Create Strided View**: Use NumPy's `as_strided` function to create a view of the original array with the calculated shape and strides.\n5. **Set Writeability**: Adjust the writeability of the resulting array based on the provided `writeable` flag.\n\n### Solution Code\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Validate input conditions\n    assert x.ndim == 1, \"x must be one-dimensional\"\n    assert stride >= 1, \"stride must be >= 1\"\n    assert len(x) >= frame_width, \"x is shorter than frame_width\"\n    \n    # Calculate the number of frames\n    n_frames = (len(x) - frame_width) // stride + 1\n    \n    # Determine the new shape and strides for the strided view\n    new_shape = (n_frames, frame_width)\n    new_strides = (stride * x.strides[0], x.strides[0])\n    \n    # Create the strided view\n    framed = as_strided(x, shape=new_shape, strides=new_strides)\n    \n    # Set the writeable flag as specified\n    if not writeable:\n        framed.flags.writeable = False\n    \n    return framed\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input array `x` is one-dimensional, the stride is at least 1, and the length of `x` is sufficient to form at least one frame. These checks ensure that the input parameters meet the necessary constraints.\n2. **Frame Calculation**: The number of frames is computed using integer division to determine how many complete frames can be extracted from the input array given the specified frame width and stride.\n3. **Stride Manipulation**: By adjusting the strides, we create a view where each row starts `stride` elements apart from the previous row, allowing efficient overlap without copying data.\n4. **Writeability Control**: The resulting array's writeability is set based on the `writeable` parameter, ensuring that the view can be made read-only if required.\n\nThis approach efficiently handles the conversion of the input signal into overlapping frames using NumPy's advanced stride manipulation, ensuring both memory efficiency and compliance with the specified constraints."}
{"task_id": 41, "completion_id": 0, "completion": "To solve this problem, we need to implement a k-Nearest Neighbors (k-NN) classifier. This classifier predicts the label of a query sample based on the majority vote of the labels of the k closest training samples. The solution involves computing distances, sorting neighbors, and determining the majority label while handling ties lexicographically.\n\n### Approach\n1. **Compute Distances**: For each query sample, compute the Euclidean distance to every training sample.\n2. **Sort Neighbors**: Sort the training samples by their distances to the query sample. If distances are equal, sort by label lexicographically to ensure determinism.\n3. **Select k-Nearest Neighbors**: Select the k closest neighbors. If k exceeds the number of training samples, use all available samples.\n4. **Majority Vote**: Count the occurrences of each label among the selected neighbors. The label with the highest count is chosen. In case of a tie, the lexicographically smallest label is selected.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_data: list[list[float]], y_data: list, X: list[list[float]], k: int = 5) -> list:\n    \"\"\"Predict labels for the query samples X using the k-Nearest Neighbours rule.\"\"\"\n    X_data_np = np.array(X_data)\n    y_data_np = np.array(y_data)\n    X_np = np.array(X)\n    \n    predictions = []\n    for query in X_np:\n        # Compute Euclidean distances between the query and all training samples\n        distances = np.linalg.norm(X_data_np - query, axis=1)\n        # Pair distances with labels and sort by distance then by label\n        dl = list(zip(distances, y_data))\n        dl_sorted = sorted(dl, key=lambda x: (x[0], x[1]))\n        # Determine the actual k to use\n        k_actual = min(k, len(dl_sorted))\n        neighbors = dl_sorted[:k_actual]\n        # Extract labels and count occurrences\n        labels = [label for (d, label) in neighbors]\n        counter = Counter(labels)\n        if not counter:\n            predictions.append(None)\n            continue\n        max_count = max(counter.values())\n        candidates = [label for label, cnt in counter.items() if cnt == max_count]\n        predicted_label = sorted(candidates)[0]\n        predictions.append(predicted_label)\n    return predictions\n```\n\n### Explanation\n1. **Distance Calculation**: Using NumPy's `linalg.norm`, we efficiently compute the Euclidean distance between each query sample and all training samples.\n2. **Sorting**: Training samples are sorted by their computed distances. If distances are equal, labels are sorted lexicographically to ensure deterministic results.\n3. **Neighbor Selection**: The first k elements from the sorted list are selected. If k is larger than the number of training samples, all samples are used.\n4. **Majority Vote**: The most frequent label among the selected neighbors is determined. If there's a tie, the lexicographically smallest label is chosen, ensuring deterministic output.\n\nThis approach ensures that the k-NN classifier works efficiently for any numeric feature dimension and handles labels of any sortable type, providing reliable and deterministic predictions."}
{"task_id": 42, "completion_id": 0, "completion": "To solve this problem, we need to compute the Root Mean Squared Logarithmic Error (RMSLE) between two lists of non-negative numbers. RMSLE is a useful metric for evaluating regression models, especially when dealing with values that can span several orders of magnitude. The solution involves validating the input data and then applying the RMSLE formula.\n\n### Approach\n1. **Input Validation**: Ensure both input lists are of the same non-zero length and all values are non-negative. If any validation fails, return -1.\n2. **Compute Logarithmic Differences**: Convert the input lists into NumPy arrays to efficiently compute the natural logarithms of each element plus 1.\n3. **Calculate Squared Differences**: Compute the squared differences between the logarithms of the predicted and actual values.\n4. **Mean Squared Error**: Calculate the mean of these squared differences.\n5. **Root Mean Squared Error**: Take the square root of the mean squared error and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmsle(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute the Root Mean Squared Logarithmic Error (RMSLE).\"\"\"\n    \n    # Validate input lengths\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1.0\n    \n    # Check for non-negative values in both lists\n    if any(x < 0 for x in actual) or any(y < 0 for y in predicted):\n        return -1.0\n    \n    # Convert lists to numpy arrays for vectorized operations\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    \n    # Compute log(actual + 1) and log(predicted + 1)\n    log_actual = np.log(actual_np + 1)\n    log_predicted = np.log(predicted_np + 1)\n    \n    # Calculate the squared differences\n    squared_diffs = (log_predicted - log_actual) ** 2\n    \n    # Compute mean of squared differences and take the square root\n    mean_squared_error = np.mean(squared_diffs)\n    rmsle_value = np.sqrt(mean_squared_error)\n    \n    # Round to four decimal places and return as a float\n    return round(rmsle_value, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input lists are of the same length and not empty. It then checks if all elements in both lists are non-negative. If any of these checks fail, the function returns -1.0.\n2. **Logarithmic Transformation**: Using NumPy's vectorized operations, the function efficiently computes the natural logarithm of each element plus 1 for both actual and predicted values.\n3. **Squared Differences**: The differences between the log-transformed values are squared to eliminate negative signs and emphasize larger errors.\n4. **Mean and Root Calculation**: The mean of the squared differences is computed, followed by taking the square root to obtain the RMSLE value.\n5. **Rounding**: The final RMSLE value is rounded to four decimal places to ensure the result is a precise and readable float.\n\nThis approach efficiently handles the computation using vectorized operations for performance and clarity, ensuring accurate results while adhering to the problem constraints."}
{"task_id": 44, "completion_id": 0, "completion": "To solve this problem, we need to implement the hyperbolic tangent (tanh) activation function and its derivative using vectorized NumPy operations without relying on the built-in `np.tanh` function. The solution should return a Python list with each element rounded to 4 decimal places.\n\n### Approach\n1. **Understanding the Tanh Function**: The tanh function is given by the formula:\n   $$\\tanh(x) = \\frac{2}{1 + e^{-2x}} - 1$$\n   This formula can be computed using basic exponential functions and arithmetic operations, which allows us to avoid using `np.tanh`.\n\n2. **Derivative of Tanh**: The derivative of the tanh function is given by:\n   $$1 - \\tanh^2(x)$$\n   To compute the derivative, we first compute the tanh values using the formula above, then square these values and subtract them from 1.\n\n3. **Vectorization with NumPy**: Convert the input list or array into a NumPy array to leverage vectorized operations, which are efficient and concise. Compute the necessary exponentials and apply the formula element-wise.\n\n4. **Rounding and Conversion**: After computing the tanh values or their derivatives, round each element to 4 decimal places and convert the result back to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef tanh_activation(x, derivative: bool = False) -> list[float]:\n    \"\"\"Compute tanh or its derivative element-wise.\n\n    Args:\n        x: A 1-D list or NumPy array of numbers.\n        derivative: If True, return the gradient of tanh. Otherwise return tanh.\n\n    Returns:\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    arr_x = np.array(x)\n    exp_neg_2x = np.exp(-2 * arr_x)\n    tanh_values = (2 / (1 + exp_neg_2x)) - 1\n    if derivative:\n        result = 1 - (tanh_values ** 2)\n    else:\n        result = tanh_values\n    return np.round(result, 4).tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input `x` is converted to a NumPy array to facilitate vectorized operations.\n2. **Exponential Calculation**: Compute the exponential term \\( e^{-2x} \\) for each element in the input array.\n3. **Tanh Calculation**: Using the formula provided, compute the tanh values for each element.\n4. **Derivative Calculation**: If the derivative flag is set, compute the derivative using the precomputed tanh values.\n5. **Rounding and Conversion**: The results are rounded to 4 decimal places and converted back to a Python list to meet the problem's requirements.\n\nThis approach ensures efficient computation using vectorized operations, making the solution both concise and performant."}
{"task_id": 46, "completion_id": 0, "completion": "To solve this problem, we need to compute the one-dimensional autocorrelation of a finite real-valued signal. The autocorrelation coefficients for non-negative lags are determined using a direct double-sum formulation, ensuring an O(N\u00b2) time complexity. This approach avoids using optimized libraries like NumPy or FFT to adhere to the problem constraints.\n\n### Approach\n1. **Input Handling**: The function accepts either a Python list or a NumPy array. If the input is a NumPy array, it is converted to a Python list to ensure the result contains native Python numeric types.\n2. **Edge Case Handling**: If the input sequence is empty (length 0), the function returns an empty list immediately.\n3. **Double-Sum Calculation**: For each lag `k` from 0 to N-1 (where N is the length of the input sequence), compute the sum of products of elements separated by the lag `k`. This involves nested loops where the outer loop iterates over each possible lag, and the inner loop computes the sum for that lag.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef autocorrelate_1d(x: list | np.ndarray) -> list:\n    \"\"\"Compute the non-negative-lag autocorrelation of a 1-D real signal.\n\n    Args:\n        x: A one-dimensional sequence of numbers. It can be a Python list or a\n           NumPy array with length *N* (N \u2265 0).\n\n    Returns:\n        A list of length *N* where the *k*-th element is the autocorrelation\n        coefficient a_k.\n    \"\"\"\n    if isinstance(x, np.ndarray):\n        x = x.tolist()\n    N = len(x)\n    if N == 0:\n        return []\n    result = []\n    for k in range(N):\n        sum_ak = 0\n        for n in range(N - k):\n            sum_ak += x[n + k] * x[n]\n        result.append(sum_ak)\n    return result\n```\n\n### Explanation\n1. **Input Conversion**: If the input is a NumPy array, it is converted to a Python list to ensure the result contains native Python types (int or float).\n2. **Edge Case Handling**: If the input sequence is empty, the function returns an empty list immediately.\n3. **Outer Loop (Lags)**: The outer loop iterates over each possible lag `k` from 0 to N-1.\n4. **Inner Loop (Sum Calculation)**: For each lag `k`, the inner loop computes the sum of products of elements `x[n + k]` and `x[n]` for `n` ranging from 0 to N-k-1. This ensures that the sum is computed correctly for each lag.\n5. **Result Construction**: Each computed sum for a lag `k` is appended to the result list, which is returned after processing all lags.\n\nThis approach ensures that the solution adheres to the O(N\u00b2) time complexity requirement and correctly handles both Python lists and NumPy arrays while returning the results as a Python list."}
{"task_id": 48, "completion_id": 0, "completion": "To solve this problem, we need to verify whether a given matrix is row-stochastic. A row-stochastic matrix is defined as a two-dimensional matrix where each element is a valid probability (between 0 and 1 inclusive) and each row sums to 1 within a small numerical tolerance.\n\n### Approach\n1. **Input Handling**: The function accepts either a list of lists or a numpy array. If the input is a list of lists, we first check that all elements are lists and that all rows have the same length to ensure it is a valid 2D structure. If the input is a numpy array, we directly use it.\n2. **Conversion to Numpy Array**: Convert the input to a numpy array if it is a list of lists. This allows us to leverage numpy's efficient array operations for subsequent checks.\n3. **Dimensionality Check**: Ensure the array is two-dimensional.\n4. **Numeric Check**: Verify that the numpy array has a numeric data type.\n5. **Element Range Check**: Ensure all elements are between 0 and 1 inclusive.\n6. **Row Sum Check**: Verify that each row sums to 1 within a tolerance of 1e-8 using numpy's `allclose` function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Handle list input\n    if isinstance(X, list):\n        # Check all elements are lists (ensuring 2D structure)\n        for row in X:\n            if not isinstance(row, list):\n                return False\n        # Check all rows have the same length\n        if X:\n            first_len = len(X[0])\n            for row in X:\n                if len(row) != first_len:\n                    return False\n        # Convert to numpy array\n        try:\n            arr = np.array(X)\n        except:\n            return False\n    elif isinstance(X, np.ndarray):\n        arr = X\n    else:\n        return False  # Invalid input type\n\n    # Check array is 2D\n    if arr.ndim != 2:\n        return False\n\n    # Check array has numeric data type\n    if not np.issubdtype(arr.dtype, np.number):\n        return False\n\n    # Check all elements are between 0 and 1 inclusive\n    if (arr < 0).any() or (arr > 1).any():\n        return False\n\n    # Check each row sums to 1 within tolerance\n    row_sums = arr.sum(axis=1)\n    if not np.allclose(row_sums, 1.0, atol=1e-8):\n        return False\n\n    return True\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input is a list of lists. If so, it ensures each element is a list and all rows have the same length. If the input is a numpy array, it proceeds directly.\n2. **Conversion and Checks**: The input is converted to a numpy array if necessary. The function then checks if the array is two-dimensional and has a numeric data type.\n3. **Element Range Check**: Using numpy operations, it verifies that all elements are within the range [0, 1].\n4. **Row Sum Check**: The function calculates the sum of each row and checks if all rows sum to 1 within a tolerance of 1e-8 using numpy's `allclose` function, which handles floating-point precision issues effectively.\n\nThis approach ensures that the function efficiently and accurately verifies all conditions required for a matrix to be row-stochastic."}
{"task_id": 49, "completion_id": 0, "completion": "To solve this problem, we need to find the k smallest elements in a list of numbers without using the built-in `heapq` module. The solution involves maintaining a max-heap of size k manually. This approach ensures that we only scan the list once and efficiently maintain the k smallest elements using heap operations.\n\n### Approach\n1. **Check Validity of k**: First, check if k is non-positive or greater than the length of the input list. If so, return -1 as specified.\n2. **Initialize Max-Heap**: Use a list to represent the max-heap, where the largest element is at the root (index 0). This helps in efficiently maintaining the k smallest elements.\n3. **Process Each Element**:\n   - If the heap has fewer than k elements, add the current element to the heap and restore the heap property by moving the element up as necessary.\n   - If the heap has k elements, compare the current element with the root (largest element in the heap). If the current element is smaller, replace the root and restore the heap property by moving the new root down.\n4. **Sort the Result**: After processing all elements, the heap contains the k smallest elements. Sort these elements to return them in ascending order.\n\n### Solution Code\n```python\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\ndef k_smallest(nums: List[Number], k: int) -> List[Number]:\n    if k <= 0 or k > len(nums):\n        return -1  # type: ignore\n    \n    def _move_up(heap: List[Number], index: int) -> None:\n        while index > 0:\n            parent = (index - 1) // 2\n            if heap[index] > heap[parent]:\n                heap[index], heap[parent] = heap[parent], heap[index]\n                index = parent\n            else:\n                break\n    \n    def _move_down(heap: List[Number], index: int) -> None:\n        n = len(heap)\n        while True:\n            left = 2 * index + 1\n            right = 2 * index + 2\n            largest = index\n\n            if left < n and heap[left] > heap[largest]:\n                largest = left\n            if right < n and heap[right] > heap[largest]:\n                largest = right\n\n            if largest != index:\n                heap[index], heap[largest] = heap[largest], heap[index]\n                index = largest\n            else:\n                break\n    \n    heap: List[Number] = []\n    for num in nums:\n        if len(heap) < k:\n            heap.append(num)\n            _move_up(heap, len(heap) - 1)\n        else:\n            if num < heap[0]:\n                heap[0] = num\n                _move_down(heap, 0)\n    \n    return sorted(heap)\n```\n\n### Explanation\n1. **Validity Check**: The function first checks if k is within valid bounds. If not, it returns -1.\n2. **Heap Operations**: The helper functions `_move_up` and `_move_down` maintain the max-heap structure. `_move_up` ensures that a newly added element is moved up the heap if it is larger than its parent. `_move_down` ensures that after replacing the root with a smaller element, the new root is moved down to maintain the heap property.\n3. **Processing Elements**: Each element is processed to either build the heap up to size k or replace the root if a smaller element is found. This ensures the heap always contains the smallest k elements encountered so far.\n4. **Sorting**: Finally, the heap is sorted to return the k smallest elements in ascending order.\n\nThis approach efficiently maintains a max-heap of size k, ensuring an optimal time complexity of O(n log k), where n is the number of elements in the input list."}
{"task_id": 51, "completion_id": 0, "completion": "To solve this problem, we need to implement Newton's method to find the optimal weight vector that minimizes the mean-squared error (MSE) for a two-variable linear regression problem. The method involves iteratively updating the weight vector using the gradient and Hessian of the MSE until the gradient's norm falls below a specified threshold or the maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Start with the weight vector initialized to [1, 1].\n2. **Hessian Calculation**: Compute the Hessian matrix once at the beginning since it remains constant for linear regression problems.\n3. **Pseudo-inverse Calculation**: Compute the Moore-Penrose pseudo-inverse of the Hessian to handle cases where the Hessian might be singular (non-invertible).\n4. **Iterative Update**: For each iteration, compute the gradient using the current weights. If the gradient's norm is below the threshold, stop iterating. Otherwise, update the weights using the pseudo-inverse of the Hessian and the gradient.\n5. **Stopping Criteria**: The loop stops when the gradient's norm is sufficiently small or the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef newton(X: np.ndarray, y: np.ndarray, epsilon: float = 1e-6, max_iter: int = 1000) -> list[list[float]]:\n    \"\"\"Implements Newton's method for two-variable linear least-squares regression.\"\"\"\n    n = X.shape[0]\n    y = y.reshape(-1, 1)  # Ensure y is a column vector\n    \n    # Compute Hessian and its pseudo-inverse\n    H = (2.0 / n) * X.T @ X\n    H_pinv = np.linalg.pinv(H)\n    \n    # Initialize weights\n    w = np.array([[1.0], [1.0]])\n    \n    for _ in range(max_iter):\n        residual = X @ w - y\n        gradient = (2.0 / n) * (X.T @ residual)\n        grad_norm = np.linalg.norm(gradient)\n        \n        if grad_norm < epsilon:\n            break\n        \n        # Update weights using the pseudo-inverse of H\n        w -= H_pinv @ gradient\n    \n    # Round to 4 decimal places and return as list of lists\n    return np.round(w, decimals=4).tolist()\n```\n\n### Explanation\n1. **Hessian and Pseudo-inverse**: The Hessian matrix is computed as (2/n) * X\u1d40X. Using the pseudo-inverse ensures that even if the Hessian is singular (non-invertible), the update step can still be performed.\n2. **Gradient Calculation**: The gradient of the MSE is derived from the residuals (difference between predicted and actual values) and is used to determine the direction of the weight update.\n3. **Weight Update**: The weights are updated by subtracting the product of the pseudo-inverse of the Hessian and the gradient from the current weights. This step is crucial as it leverages second-order information (curvature) for faster convergence.\n4. **Stopping Condition**: The iteration stops when the gradient's norm is sufficiently small, indicating that we are close to the minimum, or when the maximum number of iterations is reached to prevent infinite loops.\n\nThis approach efficiently converges to the optimal weight vector using Newton's method, ensuring robustness against singular Hessian matrices through the use of the Moore-Penrose pseudo-inverse."}
{"task_id": 54, "completion_id": 0, "completion": "To solve this problem, we need to tokenize a given text string by whitespace while optionally converting it to lowercase, stripping punctuation, and filtering out common English stop-words. The solution must preserve the original order of tokens and handle various edge cases such as empty tokens after punctuation stripping.\n\n### Approach\n1. **Lowercase Conversion**: If the `lowercase` flag is set to `True`, convert the entire input text to lowercase before any further processing.\n2. **Whitespace Tokenization**: Split the text into tokens using whitespace as the delimiter.\n3. **Punctuation Stripping**: For each token, remove punctuation characters from the beginning and end if the `filter_punctuation` flag is `True`. Discard any tokens that become empty after this step.\n4. **Stop-word Filtering**: Remove any tokens that are present in a predefined list of common English stop-words if the `filter_stopwords` flag is `True`.\n\n### Solution Code\n```python\nimport string\n\n_STOPWORDS = {\n    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",\n    \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he',\n    'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n    \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n    'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with',\n    'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n    'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n    'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n    'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',\n    'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\",\n    'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn',\n    \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n    \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\",\n    'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn',\n    \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n}\n\ndef tokenize_whitespace(line: str, lowercase: bool = True, filter_stopwords: bool = True,\n                        filter_punctuation: bool = True, **kwargs) -> list[str]:\n    \"\"\"Tokenize *line* by whitespace with optional lower-casing, punctuation stripping and\n    stop-word removal.\"\"\"\n    if lowercase:\n        line = line.lower()\n    \n    tokens = line.split()\n    processed_tokens = []\n    \n    for token in tokens:\n        if filter_punctuation:\n            stripped_token = token.strip(string.punctuation)\n        else:\n            stripped_token = token\n        if stripped_token:\n            processed_tokens.append(stripped_token)\n    \n    if filter_stopwords:\n        processed_tokens = [token for token in processed_tokens if token not in _STOPWORDS]\n    \n    return processed_tokens\n```\n\n### Explanation\n1. **Lowercase Conversion**: The input text is converted to lowercase if the `lowercase` parameter is `True`. This ensures that all subsequent processing is done on a case-normalized text.\n2. **Tokenization**: The text is split into tokens using whitespace as the delimiter, which handles multiple spaces and different whitespace characters.\n3. **Punctuation Stripping**: Each token is stripped of punctuation characters from both ends if the `filter_punctuation` flag is `True`. This step ensures that punctuation does not affect the token's validity but retains punctuation within words (e.g., contractions like \"don't\").\n4. **Stop-word Filtering**: Tokens that match entries in the predefined stop-words list are removed if the `filter_stopwords` flag is `True`. This list contains common English words that are typically filtered out in text processing tasks.\n\nThis approach ensures that the tokens are processed efficiently while preserving their order and handling various edge cases such as empty tokens after punctuation stripping."}
{"task_id": 55, "completion_id": 0, "completion": "To solve this problem, we need to implement the RMSprop optimization algorithm for ordinary least-squares (OLS) linear regression. The goal is to find the weight vector that minimizes the mean-squared error using mini-batches of data, while stopping early if the gradient's norm falls below a threshold or the maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Start with an all-zero weight vector and initialize the running average of squared gradients to zero.\n2. **Mini-batch Processing**: If the number of samples is less than the batch size, use the full dataset for each iteration. Otherwise, process the data in successive blocks (mini-batches) and wrap around when the end of the dataset is reached.\n3. **Gradient Calculation**: For each mini-batch, compute the gradient of the loss function.\n4. **Parameter Update**: Update the running average of squared gradients and adjust the weights using the RMSprop rule.\n5. **Stopping Condition**: After each parameter update, compute the full gradient to check if its norm falls below the threshold. If it does, stop early; otherwise, continue until the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\"\"\"\n    n, d = X.shape[0], X.shape[1]\n    w = np.zeros(d)\n    s = np.zeros_like(w)\n    start = 0\n\n    for _ in range(max_iter):\n        # Select the current batch\n        if n < batch_size:\n            X_batch = X\n            y_batch = y\n            m = n\n        else:\n            indices = (start + np.arange(batch_size)) % n\n            X_batch = X[indices]\n            y_batch = y[indices]\n            m = batch_size\n            start = (start + batch_size) % n\n\n        # Compute gradient for the current batch\n        residual_batch = X_batch @ w - y_batch\n        g_batch = X_batch.T @ residual_batch / m\n\n        # Update running average of squared gradients\n        s = rho * s + (1 - rho) * (g_batch ** 2)\n\n        # Update weights using RMSprop rule\n        w -= eta * g_batch / (np.sqrt(s) + eps_station)\n\n        # Compute full gradient for early stopping check\n        residual_full = X @ w - y\n        g_full = X.T @ residual_full / n\n        g_norm = np.linalg.norm(g_full)\n\n        if g_norm < epsilon:\n            break\n\n    # Round and convert to list\n    return np.round(w, 4).tolist()\n```\n\n### Explanation\n1. **Initialization**: The weight vector `w` and the running average `s` are initialized to zeros.\n2. **Mini-batch Selection**: Depending on whether the number of samples is less than the batch size, either the entire dataset is used or successive mini-batches are selected, wrapping around the dataset as needed.\n3. **Gradient Calculation**: For each mini-batch, compute the residual error and the gradient of the loss function using the mini-batch data.\n4. **Parameter Update**: The running average of squared gradients (`s`) is updated using the decay rate `rho`, and the weights (`w`) are adjusted using the RMSprop update rule.\n5. **Stopping Condition**: After each update, the full gradient is computed and its norm is checked against the threshold `epsilon`. If the norm is below the threshold, the optimization stops early; otherwise, it continues until the maximum number of iterations is reached.\n\nThis approach efficiently combines mini-batch processing for updates with full gradient checks for early stopping, ensuring both computational efficiency and convergence accuracy."}
{"task_id": 56, "completion_id": 0, "completion": "To solve this problem, we need to implement the SoftPlus activation function and its gradient (the sigmoid function) in a numerically stable manner. The SoftPlus function is defined as \\( \\text{SoftPlus}(x) = \\log(1 + e^x) \\), and its gradient is the sigmoid function \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). The challenge is to handle large values of \\( x \\) without causing numerical overflow or underflow.\n\n### Approach\n1. **Numerical Stability**: \n   - For the SoftPlus function, when \\( x \\) is large and positive, we compute it as \\( x + \\log(1 + e^{-x}) \\). This avoids overflow by using the identity \\( \\log(1 + e^x) = x + \\log(1 + e^{-x}) \\).\n   - When \\( x \\) is negative, we use \\( \\log(1 + e^x) \\) directly, which is computed accurately using the `log1p` function for small values of \\( e^x \\).\n   - For the sigmoid function, we use a piece-wise approach: \\( 1 / (1 + e^{-x}) \\) for non-negative \\( x \\) and \\( e^x / (1 + e^x) \\) for negative \\( x \\) to avoid overflow.\n\n2. **Input Handling**: \n   - The function accepts scalars, lists, or NumPy arrays. Convert the input to a NumPy array for uniform processing and then convert back to the original type after computation.\n\n3. **Rounding**: \n   - Results are rounded to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"Compute the SoftPlus activation function or its gradient (sigmoid) with numerical stability.\"\"\"\n    is_scalar = not isinstance(x, (list, np.ndarray))\n    x_np = np.array([x]) if is_scalar else np.asarray(x)\n    \n    if deriv:\n        # Compute the sigmoid (derivative) with stable computation\n        sigmoid = np.where(x_np >= 0,\n                           1.0 / (1.0 + np.exp(-x_np)),\n                           np.exp(x_np) / (1.0 + np.exp(x_np)))\n        output = sigmoid\n    else:\n        # Compute SoftPlus with stable computation\n        mask = x_np > 0\n        softplus_vals = np.zeros_like(x_np)\n        x_pos = x_np[mask]\n        softplus_vals[mask] = x_pos + np.log1p(np.exp(-x_pos))\n        x_neg = x_np[~mask]\n        softplus_vals[~mask] = np.log1p(np.exp(x_neg))\n        output = softplus_vals\n    \n    output_rounded = np.round(output, 4)\n    return output_rounded.item() if is_scalar else output_rounded.tolist()\n```\n\n### Explanation\n- **Numerical Stability**: The code uses piece-wise calculations to avoid overflow. For SoftPlus, when \\( x \\) is positive, it leverages the identity \\( \\log(1 + e^x) = x + \\log(1 + e^{-x}) \\). For the sigmoid function, it splits the computation into non-negative and negative regions to avoid overflow.\n- **Input Handling**: The input is converted to a NumPy array for processing, ensuring that both scalar and array inputs are handled uniformly.\n- **Rounding**: The results are rounded to four decimal places using NumPy's `round` function, ensuring the output meets the specified precision requirements.\n\nThis approach ensures that the function is both efficient and numerically stable, handling extreme values of \\( x \\) without overflow or underflow issues."}
{"task_id": 58, "completion_id": 0, "completion": "To solve this problem, we need to compute the log-probability of a given sequence using an unsmoothed Maximum-Likelihood Estimation (MLE) N-gram language model trained on a corpus. The solution involves building count tables for N-grams and their prefixes, then using these counts to compute the probabilities of each N-gram in the sequence.\n\n### Approach\n1. **Check Sequence Length**: If the sequence length is less than the order N, return 0.0 as there are no N-grams to process.\n2. **Generate N-gram Counts**: From the corpus, generate all possible N-grams and count their occurrences.\n3. **Generate Prefix Counts**: If N > 1, generate (N-1)-grams (prefixes) from the corpus and count their occurrences.\n4. **Compute Log-Probability**: For each N-gram in the sequence:\n   - Check if the N-gram exists in the corpus. If not, return negative infinity.\n   - For N > 1, check if the prefix (first N-1 elements of the N-gram) exists in the prefix counts. If not, return negative infinity.\n   - Compute the probability using the counts and sum the log-probabilities. If any count is zero, return negative infinity immediately.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    if len(sequence) < N:\n        return round(0.0, 4)\n    \n    # Generate N-gram counts from the corpus\n    corpus_ngrams = [tuple(corpus[i:i+N]) for i in range(len(corpus) - N + 1)]\n    count_ngrams = Counter(corpus_ngrams)\n    \n    # Generate (N-1)-gram counts if N > 1\n    count_prefix = None\n    if N > 1:\n        corpus_prefix_ngrams = [tuple(corpus[i:i+(N-1)]) for i in range(len(corpus) - (N-1) + 1)]\n        count_prefix = Counter(corpus_prefix_ngrams)\n    \n    total_log = 0.0\n    for i in range(len(sequence) - N + 1):\n        current_ngram = tuple(sequence[i:i+N])\n        numerator = count_ngrams.get(current_ngram, 0)\n        if numerator == 0:\n            return float('-inf')\n        \n        if N == 1:\n            denominator = len(corpus)\n        else:\n            prefix = current_ngram[:-1]\n            denominator = count_prefix.get(prefix, 0)\n        \n        if denominator == 0:\n            return float('-inf')\n        \n        prob = numerator / denominator\n        total_log += np.log(prob)\n    \n    return round(total_log, 4)\n```\n\n### Explanation\n1. **Check Sequence Length**: If the sequence is shorter than N, return 0.0 immediately as there are no N-grams to process.\n2. **Count N-grams**: Generate all possible N-grams from the corpus and count their occurrences using a `Counter`.\n3. **Count Prefixes**: For N > 1, generate all possible (N-1)-grams (prefixes) from the corpus and count their occurrences.\n4. **Iterate Over Sequence**: For each N-gram in the sequence:\n   - Check if the N-gram exists in the corpus. If not, return negative infinity.\n   - For N > 1, check if the prefix exists in the prefix counts. If not, return negative infinity.\n   - Compute the probability as the ratio of the N-gram count to the prefix count (or corpus length for unigrams) and accumulate the log-probability.\n5. **Return Result**: Sum all log-probabilities, round to 4 decimal places, and return the result. If any count check fails, return negative infinity immediately."}
{"task_id": 60, "completion_id": 0, "completion": "To solve this problem, we need to compute the impurity of a node in a decision tree using different metrics: entropy for classification using the ID3/C4.5 algorithms, Gini index for classification using the CART algorithm, and mean squared error (MSE) for regression using the CART algorithm. The impurity metric is determined by the criterion specified.\n\n### Approach\n1. **Handle Empty Array**: If the input array `y` is empty, return 0.0 as the impurity.\n2. **Check Valid Criterion**: Verify that the specified criterion is one of the valid options (\"entropy\", \"gini\", \"mse\"). If not, raise a `ValueError`.\n3. **Entropy Calculation**:\n   - Compute the counts of each unique class in `y`.\n   - Convert these counts to probabilities.\n   - Calculate entropy using the formula: \\(-\\sum (p_i \\log_2 p_i)\\).\n4. **Gini Index Calculation**:\n   - Compute the counts of each unique class in `y`.\n   - Convert these counts to probabilities.\n   - Calculate the Gini index using the formula: \\(1 - \\sum (p_i^2)\\).\n5. **Mean Squared Error (MSE) Calculation**:\n   - Compute the mean of `y`.\n   - Calculate the MSE using the formula: \\(\\frac{1}{N} \\sum (y_i - \\text{mean})^2\\).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_impurity(y, criterion):\n    \"\"\"Compute a node's impurity.\n\n    Args:\n        y (numpy.ndarray): 1-D array with the target values.\n        criterion (str): One of {\"entropy\", \"gini\", \"mse\"}.\n\n    Returns:\n        float: Impurity value rounded to 4 decimal places.\n    \"\"\"\n    if y.size == 0:\n        return 0.0\n    if criterion not in {\"entropy\", \"gini\", \"mse\"}:\n        raise ValueError(\"Invalid criterion\")\n    if criterion == \"entropy\":\n        counts = np.unique(y, return_counts=True)[1]\n        probabilities = counts / y.size\n        entropy = -np.sum(probabilities * np.log2(probabilities))\n        return round(entropy, 4)\n    elif criterion == \"gini\":\n        counts = np.unique(y, return_counts=True)[1]\n        probabilities = counts / y.size\n        gini = 1 - np.sum(probabilities ** 2)\n        return round(gini, 4)\n    else:  # mse\n        mean = np.mean(y)\n        mse = np.mean((y - mean) ** 2)\n        return round(mse, 4)\n```\n\n### Explanation\n- **Empty Array Handling**: Directly return 0.0 if the input array `y` is empty, as there is no impurity in an empty node.\n- **Entropy Calculation**: Uses the probabilities of each class to compute the entropy, which measures the disorder or uncertainty in the node.\n- **Gini Index Calculation**: Measures the probability of a random sample being misclassified by summing the squared probabilities of each class and subtracting from 1.\n- **Mean Squared Error (MSE)**: Computes the average squared deviation from the mean of the target values, used for regression tasks to measure impurity.\n\nThis approach efficiently handles all specified impurity metrics and ensures correct results by leveraging NumPy's optimized operations for array manipulations and mathematical computations."}
{"task_id": 61, "completion_id": 0, "completion": "To solve this problem, we need to compute the gradient of the mean squared error (MSE) cost function for linear regression using vectorized NumPy operations. The gradient is given by the formula \\(\\nabla J(W) = \\frac{X^\\top (XW - y)}{N}\\), where \\(X\\) is the design matrix, \\(y\\) is the target vector, \\(W\\) is the weight vector, and \\(N\\) is the number of training examples. \n\n### Approach\n1. **Input Conversion and Validation**: Convert the inputs \\(X\\), \\(y\\), and \\(W\\) into NumPy arrays and validate their dimensions. Ensure \\(X\\) is a 2-dimensional array, \\(y\\) is reshaped to a column vector if it is 1-dimensional, and \\(W\\) is reshaped to a column vector if it is 1-dimensional.\n2. **Dimension Checks**: Verify that the number of rows in \\(y\\) matches the number of rows in \\(X\\), and the number of rows in \\(W\\) matches the number of columns in \\(X\\).\n3. **Gradient Computation**: Use vectorized operations to compute the residual \\(XW - y\\), then compute the gradient using the formula involving matrix multiplication and division by the number of samples.\n4. **Result Formatting**: Round the resulting gradient values to 4 decimal places and return them as a nested list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef grad(X, y, W):\n    \"\"\"Return the gradient of the MSE loss for linear regression.\"\"\"\n    # Convert inputs to numpy arrays\n    X_arr = np.array(X)\n    y_arr = np.array(y)\n    W_arr = np.array(W)\n    \n    # Check if X is 2-dimensional\n    if X_arr.ndim != 2:\n        return -1\n    n_samples, n_features = X_arr.shape\n    \n    # Process y and validate its shape\n    if y_arr.ndim == 1:\n        y_processed = y_arr.reshape(-1, 1)\n    elif y_arr.ndim == 2 and y_arr.shape[1] == 1:\n        y_processed = y_arr\n    else:\n        return -1\n    if y_processed.shape[0] != n_samples:\n        return -1\n    \n    # Process W and validate its shape\n    if W_arr.ndim == 1:\n        W_processed = W_arr.reshape(-1, 1)\n    elif W_arr.ndim == 2 and W_arr.shape[1] == 1:\n        W_processed = W_arr\n    else:\n        return -1\n    if W_processed.shape[0] != n_features:\n        return -1\n    \n    # Compute the gradient\n    residual = X_arr.dot(W_processed) - y_processed\n    gradient = (X_arr.T.dot(residual)) / n_samples\n    \n    # Round and convert to list of lists\n    gradient_rounded = np.round(gradient, 4)\n    return gradient_rounded.tolist()\n```\n\n### Explanation\n1. **Input Conversion**: Convert \\(X\\), \\(y\\), and \\(W\\) to NumPy arrays to handle both lists and arrays uniformly.\n2. **Dimension Validation**: Check that \\(X\\) is a 2D array, \\(y\\) is either a 1D array reshaped to a column vector or a 2D column vector, and \\(W\\) is similarly validated and reshaped if necessary.\n3. **Residual Calculation**: Compute \\(XW - y\\) to get the residual vector.\n4. **Gradient Calculation**: Use matrix multiplication to compute the gradient efficiently and divide by the number of samples to normalize.\n5. **Result Formatting**: Round the gradient values to 4 decimal places and convert the result to a nested list for the required output format.\n\nThis approach ensures that all input validations are performed efficiently and the gradient is computed using optimized vector operations, making the solution both robust and efficient."}
{"task_id": 62, "completion_id": 0, "completion": "To solve this problem, we need to implement a univariate Lasso regression model using coordinate descent to minimize the Lasso objective function with polynomial feature expansion. The model should handle both regularization and ordinary least squares (OLS) solutions when the regularization strength is zero.\n\n### Approach\n1. **Polynomial Feature Expansion**: Construct a design matrix that includes polynomial features up to the specified degree. This matrix starts with a column of ones (for the bias term) followed by the original features raised to increasing powers up to the given degree.\n2. **Coordinate Descent Optimization**: Use coordinate descent to iteratively update each coefficient. The bias term (intercept) is updated first in each iteration using the mean of the residuals. Subsequent coefficients are updated using a soft thresholding operator to enforce sparsity.\n3. **Residual Update**: Efficiently update the residuals after each coefficient adjustment to ensure the correct residuals are used for subsequent updates.\n4. **Early Stopping**: Terminate the optimization process early if the maximum change in coefficients between iterations falls below a specified tolerance.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef lasso_regression(X: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\", degree: int, reg_factor: float, n_iterations: int = 1000, tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model with polynomial features using coordinate descent.\"\"\"\n    X = np.asarray(X).flatten()\n    y = np.asarray(y).flatten()\n    m = X.shape[0]\n    if m == 0:\n        return [0.0] * (degree + 1)\n    \n    # Create polynomial design matrix\n    X_poly = np.vander(X, degree + 1, increasing=True)\n    \n    # Initialize weights and residual\n    w = np.zeros(degree + 1)\n    residual = y - X_poly @ w  # Initial residual is y\n    \n    for _ in range(n_iterations):\n        max_delta = 0.0\n        \n        # Update the bias term (w0)\n        sum_other_terms = X_poly[:, 1:] @ w[1:]\n        w0_new = np.mean(y - sum_other_terms)\n        delta_w0 = abs(w0_new - w[0])\n        if delta_w0 > max_delta:\n            max_delta = delta_w0\n        residual -= (w0_new - w[0])  # Adjust residual for new w0\n        w[0] = w0_new\n        \n        # Update each weight w_j for j >= 1 using coordinate descent\n        for j in range(1, degree + 1):\n            Xj = X_poly[:, j]\n            Xj_squared_sum = Xj @ Xj\n            \n            if Xj_squared_sum == 0:\n                new_wj = 0.0\n            else:\n                rho_j = Xj @ residual + w[j] * Xj_squared_sum\n                # Soft thresholding\n                if abs(rho_j) <= reg_factor:\n                    new_wj = 0.0\n                else:\n                    new_wj = (np.sign(rho_j) * (abs(rho_j) - reg_factor)) / Xj_squared_sum\n            \n            delta = abs(new_wj - w[j])\n            if delta > max_delta:\n                max_delta = delta\n            \n            # Update residual\n            residual += (w[j] - new_wj) * Xj\n            w[j] = new_wj\n        \n        # Check for early stopping\n        if max_delta < tol:\n            break\n    \n    # Round coefficients to 4 decimal places\n    return [round(coef, 4) for coef in w]\n```\n\n### Explanation\n1. **Polynomial Feature Expansion**: The design matrix is created using `np.vander` with the `increasing=True` parameter to generate polynomial features up to the specified degree.\n2. **Coordinate Descent**: The algorithm iterates over each coefficient, updating the bias term first by computing the mean of the residuals. For each subsequent coefficient, the update involves computing the soft thresholding operator to apply L1 regularization.\n3. **Residual Adjustment**: After each coefficient update, the residuals are adjusted to reflect the new coefficient value, ensuring that subsequent updates use the most recent residuals.\n4. **Early Stopping**: The algorithm stops early if the maximum change in coefficients between iterations is below the specified tolerance, improving efficiency.\n\nThis approach efficiently handles both regularization and OLS solutions, providing a robust implementation of Lasso regression with polynomial features."}
{"task_id": 63, "completion_id": 0, "completion": "To solve this problem, we need to compute the backward probability vector \\(\\beta_t\\) for a given time index \\(t\\) in a Hidden Markov Model (HMM). The backward probability vector \\(\\beta_t(i)\\) represents the probability of observing the remaining sequence from time \\(t+1\\) to the end, given that the system is in state \\(i\\) at time \\(t\\).\n\n### Approach\n1. **Initialization**: Start with the base case where the backward probabilities for the last time step \\(T-1\\) are all 1.0, as there are no future observations beyond this point.\n2. **Iterative Calculation**: For each time step from \\(T-2\\) down to the given time \\(t\\), compute the backward probabilities using the formula:\n   \\[\n   \\beta_t(i) = \\sum_{j=1}^{N} a_{ij} \\cdot b_j(o_{t+1}) \\cdot \\beta_{t+1}(j)\n   \\]\n   Here, \\(a_{ij}\\) is the transition probability from state \\(i\\) to \\(j\\), \\(b_j(o_{t+1})\\) is the emission probability of observing \\(o_{t+1}\\) in state \\(j\\), and \\(\\beta_{t+1}(j)\\) is the backward probability for state \\(j\\) at time \\(t+1\\).\n\n3. **Backward Recursion**: Iterate from the second last time step backward to the given time \\(t\\), updating the backward probabilities at each step using the previously computed values from the next time step.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"Compute the backward probability vector beta_t for the given HMM parameters and observation sequence.\"\"\"\n    N = len(A)\n    T = len(obs)\n    \n    if t == T - 1:\n        return [round(1.0, 4) for _ in range(N)]\n    \n    beta_prev = [1.0 for _ in range(N)]\n    \n    for s in range(T-2, t-1, -1):\n        o = obs[s+1]\n        beta_current = []\n        for i in range(N):\n            total = 0.0\n            for j in range(N):\n                total += A[i][j] * B[j][o] * beta_prev[j]\n            beta_current.append(total)\n        beta_prev = beta_current\n    \n    return [round(x, 4) for x in beta_prev]\n```\n\n### Explanation\n1. **Initialization**: The function first checks if the given time index \\(t\\) is the last time step (i.e., \\(T-1\\)). If so, it returns a list of 1.0s rounded to four decimal places, as there are no future observations to consider.\n2. **Backward Iteration**: Starting from the second last time step \\(T-2\\), the function iterates backward to the given time \\(t\\). For each time step \\(s\\), it calculates the backward probabilities for each state \\(i\\) by summing over all possible transitions to future states \\(j\\), considering the transition probabilities, emission probabilities, and the previously computed backward probabilities.\n3. **Updating Probabilities**: The backward probabilities for each time step are computed iteratively and stored in a list, which is updated at each step until the desired time \\(t\\) is reached. The final result is rounded to four decimal places and returned.\n\nThis approach efficiently computes the backward probabilities using dynamic programming, ensuring that each step builds on the previous computations, leading to an optimal solution."}
{"task_id": 64, "completion_id": 0, "completion": "To solve this problem, we need to implement a naive 1-D Discrete Fourier Transform (DFT) without using any FFT libraries. The solution should explicitly compute the DFT using matrix-vector multiplication, which ensures the intended \\(O(N^2)\\) complexity. Additionally, the solution should return either the full spectrum or only the non-negative frequency terms based on a flag, and round the results to four decimal places.\n\n### Approach\n1. **Matrix Construction**: Construct the DFT matrix using the formula for the DFT coefficients. Each element of the matrix is computed as \\(e^{-2\\pi i k n / N}\\), where \\(k\\) and \\(n\\) are indices ranging from 0 to \\(N-1\\).\n2. **Matrix-Vector Multiplication**: Multiply the constructed DFT matrix with the input signal vector to compute the DFT coefficients.\n3. **Non-Negative Frequencies**: If required, return only the first \\(N//2 + 1\\) coefficients which correspond to the non-negative frequencies.\n4. **Rounding**: Round both the real and imaginary parts of each coefficient to four decimal places and convert the result to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dft(frame: np.ndarray, positive_only: bool = True) -> list:\n    \"\"\"Compute the Discrete Fourier Transform (DFT) of a 1-D signal.\n\n    Args:\n        frame: 1-D NumPy array containing the signal samples.\n        positive_only: Whether to return only the non-negative frequency\n            terms (first *N//2+1* coefficients).  Defaults to ``True``.\n\n    Returns:\n        list[complex]: DFT coefficients rounded to 4 decimals.  Length is\n        *N//2+1* when *positive_only* is ``True`` and *N* otherwise.\n    \"\"\"\n    N = len(frame)\n    k = np.arange(N).reshape(-1, 1)  # Shape (N, 1)\n    n = np.arange(N)                 # Shape (N,)\n    exponent = -2j * np.pi * k * n / N\n    dft_matrix = np.exp(exponent)\n    coefficients = dft_matrix @ frame  # Matrix-vector multiplication\n    rounded_coeffs = np.around(coefficients, 4)\n    if positive_only:\n        return rounded_coeffs[:N//2 + 1].tolist()\n    else:\n        return rounded_coeffs.tolist()\n```\n\n### Explanation\n1. **Matrix Construction**: The DFT matrix is constructed using broadcasting in NumPy. The indices \\(k\\) and \\(n\\) are set up as column and row vectors respectively. The element-wise product \\(k \\times n\\) creates a matrix where each element is used to compute the exponential term for the DFT.\n2. **Matrix-Vector Multiplication**: The DFT coefficients are computed by multiplying the DFT matrix with the input signal vector. This operation efficiently computes all coefficients in \\(O(N^2)\\) time.\n3. **Non-Negative Frequencies**: For real signals, the DFT coefficients are symmetric. The non-negative frequencies are the first \\(N//2 + 1\\) coefficients, which include the DC component and up to the Nyquist frequency.\n4. **Rounding**: The coefficients are rounded using NumPy's `around` function, which handles both real and imaginary parts, ensuring each part is rounded to four decimal places. The result is then converted to a Python list for the final output.\n\nThis approach ensures the solution is both efficient and straightforward, adhering to the problem constraints and requirements."}
{"task_id": 65, "completion_id": 0, "completion": "To solve this problem, we need to compute the probability of a given observation sequence being generated by a Hidden Markov Model (HMM) using the backward algorithm. The backward algorithm efficiently calculates this probability by recursively computing backward variables starting from the last observation and moving backward in time.\n\n### Approach\n1. **Input Validation**: Check if any of the input lists (transition matrix `A`, emission matrix `B`, initial distribution `pi`, or observation sequence `obs`) are empty. If any of them are empty, return 0.0 immediately as no valid probability can be computed.\n2. **Convert Inputs to Numpy Arrays**: Convert the input lists to numpy arrays for efficient matrix operations.\n3. **Initialize Backward Variables**: Start with the backward variables for the last time step, which are all initialized to 1.\n4. **Recursive Calculation**: Compute the backward variables for each preceding time step using the formula that involves the transition probabilities, emission probabilities, and the previously computed backward variables.\n5. **Compute Total Probability**: Combine the initial state probabilities, emission probabilities of the first observation, and the computed backward variables to get the total probability of the observation sequence.\n6. **Round Result**: Return the result rounded to six decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    # Check for empty inputs\n    if not A or not B or not pi or not obs:\n        return 0.0\n    \n    # Convert lists to numpy arrays for efficient computation\n    A = np.array(A, dtype=np.float64)\n    B = np.array(B, dtype=np.float64)\n    pi = np.array(pi, dtype=np.float64)\n    obs = np.array(obs, dtype=int)\n    \n    T = len(obs)\n    N = len(pi)\n    \n    # If there are no observations, return 0.0\n    if T == 0:\n        return 0.0\n    \n    # Initialize the backward variable for the last time step (T-1)\n    beta_prev = np.ones(N, dtype=np.float64)\n    \n    # Iterate from T-2 down to 0\n    for t in reversed(range(T - 1)):\n        # Get the observation index at t+1\n        o = obs[t + 1]\n        # Compute the product of emission probabilities and beta_prev\n        temp = B[:, o] * beta_prev\n        # Update beta_prev by multiplying with transition matrix\n        beta_prev = A.dot(temp)\n    \n    # Calculate the total probability using the initial distribution, first observation, and beta_0\n    total = np.sum(pi * B[:, obs[0]] * beta_prev)\n    \n    # Round to six decimal places\n    return round(total, 6)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if any of the input lists are empty. If so, it returns 0.0 immediately.\n2. **Conversion to Numpy Arrays**: The input lists are converted to numpy arrays to leverage efficient matrix operations.\n3. **Initialization**: The backward variables for the last time step are initialized to 1 since there are no future observations beyond the last time step.\n4. **Recursive Calculation**: For each time step from the second last to the first, the backward variables are updated using the transition probabilities, emission probabilities of the next observation, and the previously computed backward variables.\n5. **Total Probability Calculation**: The total probability is computed by combining the initial state probabilities, emission probabilities of the first observation, and the backward variables from the first time step.\n6. **Rounding**: The result is rounded to six decimal places to meet the specified precision requirement.\n\nThis approach efficiently computes the probability using dynamic programming and matrix operations, ensuring both correctness and performance."}
{"task_id": 67, "completion_id": 0, "completion": "To solve this problem, we need to simulate a Hidden Markov Model (HMM) to generate sequences of hidden states and corresponding observations. The solution involves validating the input probability distributions, setting up the random seed for reproducibility, and then sampling the states and observations according to the specified distributions.\n\n### Approach\n1. **Validation of Probability Distributions**: Ensure that the initial state distribution (S), each row of the state transition matrix (A), and each row of the emission matrix (B) sum to 1 within a tolerance of 1e-8. If any distribution is invalid, return -1 immediately.\n2. **Seed Initialization**: If a seed is provided, initialize the random number generator with it to ensure reproducibility.\n3. **Sampling**:\n   - **Initial State**: Sample the first hidden state from the initial distribution S.\n   - **Subsequent States and Observations**: For each subsequent step, transition to the next state using the current state's transition probabilities and then emit an observation using the new state's emission probabilities. This process is repeated until the desired number of samples (n_sample) is generated.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gen_hmm_data(\n    S: list[float],\n    A: list[list[float]],\n    B: list[list[float]],\n    n_sample: int,\n    seed: int | None = None,\n) -> tuple[list[int], list[int]]:\n    \"\"\"Generate a sequence of hidden states and observations from a Hidden Markov Model.\"\"\"\n    \n    # Validate the initial state distribution S\n    sum_S = np.sum(S)\n    if not np.isclose(sum_S, 1.0, atol=1e-8):\n        return -1\n    \n    # Validate each row of the transition matrix A\n    for row in A:\n        sum_row = np.sum(row)\n        if not np.isclose(sum_row, 1.0, atol=1e-8):\n            return -1\n    \n    # Validate each row of the emission matrix B\n    for row in B:\n        sum_row = np.sum(row)\n        if not np.isclose(sum_row, 1.0, atol=1e-8):\n            return -1\n    \n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate the initial state\n    current_state = np.random.choice(len(S), p=S)\n    states = [current_state]\n    \n    # Generate the first observation\n    observation = np.random.choice(len(B[current_state]), p=B[current_state])\n    observations = [observation]\n    \n    # Generate the remaining states and observations\n    for _ in range(n_sample - 1):\n        # Transition to the next state\n        current_state = np.random.choice(len(A[current_state]), p=A[current_state])\n        states.append(current_state)\n        \n        # Emit observation from the new state\n        observation = np.random.choice(len(B[current_state]), p=B[current_state])\n        observations.append(observation)\n    \n    return (states, observations)\n```\n\n### Explanation\n1. **Validation**: The code first checks if the initial state distribution and each row of the transition and emission matrices sum to 1. This ensures the input distributions are valid.\n2. **Seed Initialization**: If a seed is provided, the random number generator is initialized to ensure consistent results across runs.\n3. **Initial State Sampling**: The first hidden state is sampled using the initial distribution S.\n4. **Observation Sampling**: The first observation is emitted based on the initial state's emission probabilities.\n5. **Subsequent Sampling**: For each subsequent sample, the next state is determined by the current state's transition probabilities, and the corresponding observation is emitted based on the new state's emission probabilities. This loop continues until the desired number of samples is generated.\n\nThis approach efficiently generates the required sequences while ensuring the validity of the input distributions and reproducibility through seed initialization."}
{"task_id": 68, "completion_id": 0, "completion": "To solve this problem, we need to implement the Stochastic Gradient Descent (SGD) optimization algorithm with momentum. This algorithm helps in efficiently navigating the loss landscape during the training of machine learning models by using a momentum term to smooth out the updates.\n\n### Approach\nThe approach involves the following steps:\n1. **Handle Previous Update**: If the previous momentum update (velocity) is not provided, initialize it as a zero tensor with the same shape as the current parameters.\n2. **Compute New Velocity**: Use the momentum factor to blend the previous velocity with the current gradient. The formula for the new velocity is \\(u_t = \\text{momentum} \\times u_{t-1} + (1 - \\text{momentum}) \\times \\nabla_w\\).\n3. **Update Parameters**: Adjust the current parameters by moving against the direction of the gradient scaled by the learning rate and the newly computed velocity.\n4. **Convert and Round Results**: Convert the updated parameters and the new velocity to Python lists and round each element to four decimal places for consistency and readability.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sgd_momentum_update(w: np.ndarray, grad: np.ndarray, learning_rate: float = 0.01, momentum: float = 0.0, prev_update: np.ndarray | None = None) -> tuple[list, list]:\n    \"\"\"Performs one SGD optimisation step with momentum.\"\"\"\n    if prev_update is None:\n        prev_update = np.zeros_like(w)\n    new_update = momentum * prev_update + (1 - momentum) * grad\n    updated_w = w - learning_rate * new_update\n    return (\n        np.round(updated_w, 4).tolist(),\n        np.round(new_update, 4).tolist()\n    )\n```\n\n### Explanation\n1. **Handling Previous Update**: The code checks if `prev_update` is `None` and initializes it to a zero tensor if necessary. This ensures that the first update is computed correctly using only the current gradient.\n2. **Computing New Velocity**: The new velocity is calculated by taking a weighted average of the previous velocity and the current gradient. This helps in maintaining a smooth update direction, especially in high-curvature areas of the loss function.\n3. **Updating Parameters**: The parameters are updated by subtracting the product of the learning rate and the new velocity. This step moves the parameters in the direction that minimizes the loss.\n4. **Conversion and Rounding**: The results are converted to Python lists and rounded to four decimal places to meet the output constraints, ensuring the results are easy to read and use in subsequent steps.\n\nThis approach efficiently combines the benefits of momentum with SGD, leading to faster convergence and better handling of noisy gradients."}
{"task_id": 69, "completion_id": 0, "completion": "To solve this problem, we need to compute the likelihood of an observation sequence given a Hidden Markov Model (HMM) using the forward algorithm. The HMM is defined by an initial state probability vector, a state transition matrix, and an emission matrix. The solution involves validating the input dimensions and observation indices, then applying the forward algorithm to compute the probability of the observation sequence.\n\n### Approach\n1. **Input Validation**: \n   - Check the dimensions of the initial state probabilities, state transition matrix, and emission matrix.\n   - Ensure all observation indices are within the valid range defined by the emission matrix.\n   \n2. **Forward Algorithm**:\n   - **Initialization**: Compute the initial forward probabilities using the initial state probabilities and the emission probabilities for the first observation.\n   - **Iteration**: For each subsequent observation, update the forward probabilities by summing over all possible previous states, multiplying by the transition probabilities, and then by the emission probabilities for the current observation.\n   - **Termination**: Sum the final forward probabilities to get the total likelihood of the observation sequence.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Validate input dimensions\n    n = len(S)\n    if n == 0:\n        return -1.0\n    \n    # Check state-transition matrix A is n x n\n    if len(A) != n:\n        return -1.0\n    for row in A:\n        if len(row) != n:\n            return -1.0\n    \n    # Check emission matrix B is n x m\n    if len(B) != n:\n        return -1.0\n    if n == 0:\n        return -1.0  # redundant check as n is already non-zero\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1.0\n    \n    # Check observations are valid\n    if not observations:\n        return -1.0\n    for obs in observations:\n        if not isinstance(obs, int) or obs < 0 or obs >= m:\n            return -1.0\n    \n    # Initialize forward probabilities\n    T = len(observations)\n    alpha_prev = [S[i] * B[i][observations[0]] for i in range(n)]\n    \n    # Iterate through the rest of the observations\n    for t in range(1, T):\n        current_obs = observations[t]\n        alpha_current = []\n        for j in range(n):\n            total = 0.0\n            for i in range(n):\n                total += alpha_prev[i] * A[i][j]\n            alpha_current.append(total * B[j][current_obs])\n        alpha_prev = alpha_current\n    \n    # Compute the final probability and round to 4 decimal places\n    final_prob = sum(alpha_prev)\n    return round(final_prob, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input matrices (S, A, B) have the correct dimensions. It also ensures that all observation indices are valid and within the range defined by the emission matrix.\n2. **Initialization**: The initial forward probabilities are calculated by multiplying the initial state probabilities by the emission probabilities corresponding to the first observation.\n3. **Iteration**: For each subsequent observation, the forward probabilities are updated by considering all possible transitions from previous states, summing these probabilities, and then multiplying by the emission probability for the current observation.\n4. **Termination**: The final probability is obtained by summing the forward probabilities at the last time step, which gives the total likelihood of the observation sequence. This result is then rounded to four decimal places for the final output."}
{"task_id": 70, "completion_id": 0, "completion": "To solve this problem, we need to implement the FP-Growth algorithm to discover all frequent item-sets in a transactional database. The FP-Growth algorithm efficiently finds frequent item-sets without generating candidate item-sets explicitly, using a prefix tree (FP-tree) structure and recursive mining of conditional FP-trees.\n\n### Approach\n1. **Count Item Frequencies**: First, count the frequency of each item in the transactions and filter out items that do not meet the minimum support threshold.\n2. **Build the FP-Tree**: Construct an FP-tree where each node represents an item and its count in the transactions. The tree is built by processing each transaction, filtering out infrequent items, and sorting the remaining items by their frequency.\n3. **Mine the FP-Tree**: Recursively mine the FP-tree starting from the least frequent items. For each item, generate its conditional pattern base, construct a conditional FP-tree, and recursively mine this tree to discover larger item-sets.\n\n### Solution Code\n```python\nfrom collections import Counter, defaultdict\n\nclass TreeNode:\n    def __init__(self, item, count, parent):\n        self.item = item\n        self.count = count\n        self.parent = parent\n        self.children = {}\n        self.node_link = None\n\ndef build_fp_tree(transactions_with_counts, min_support):\n    item_counts = defaultdict(int)\n    for transaction, count in transactions_with_counts:\n        for item in transaction:\n            item_counts[item] += count\n\n    frequent_items = {item for item, cnt in item_counts.items() if cnt >= min_support}\n    if not frequent_items:\n        return None, None\n\n    sorted_items = sorted(frequent_items, key=lambda x: (-item_counts[x], x))\n\n    root = TreeNode(None, 0, None)\n    header_table = {}\n    for item in sorted_items:\n        header_table[item] = [item_counts[item], None]\n\n    for transaction, count in transactions_with_counts:\n        sorted_transaction = [item for item in sorted_items if item in transaction]\n        if not sorted_transaction:\n            continue\n        current_node = root\n        for item in sorted_transaction:\n            if item in current_node.children:\n                current_node = current_node.children[item]\n                current_node.count += count\n            else:\n                new_node = TreeNode(item, count, current_node)\n                current_node.children[item] = new_node\n                current_node = new_node\n                if header_table[item][1] is None:\n                    header_table[item][1] = new_node\n                else:\n                    last_node = header_table[item][1]\n                    while last_node.node_link is not None:\n                        last_node = last_node.node_link\n                    last_node.node_link = new_node\n\n    return root, header_table\n\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    if not transactions or min_support < 1:\n        return []\n\n    item_counts = defaultdict(int)\n    for transaction in transactions:\n        unique_items = set(transaction)\n        for item in unique_items:\n            item_counts[item] += 1\n\n    frequent_items = {item: count for item, count in item_counts.items() if count >= min_support}\n    if not frequent_items:\n        return []\n\n    sorted_items = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n\n    filtered_transactions = []\n    for trans in transactions:\n        unique_trans = list(set(trans))\n        filtered = [item for item in unique_trans if item in frequent_items]\n        if filtered:\n            filtered.sort(key=lambda x: sorted_items.index(x))\n            filtered_transactions.append(filtered)\n\n    if not filtered_transactions:\n        return []\n\n    transactions_with_counts = [(t, 1) for t in filtered_transactions]\n    root, header_table = build_fp_tree(transactions_with_counts, min_support)\n    if not header_table:\n        return []\n\n    header_list = sorted(header_table.items(), key=lambda x: (-x[1][0], x[0]))\n\n    frequent_itemsets = []\n\n    def mine_tree(header_list, prefix, frequent_itemsets):\n        for item, (support, head_node) in reversed(header_list):\n            new_prefix = prefix.copy()\n            new_prefix.append(item)\n            new_prefix_sorted = sorted(new_prefix)\n            frequent_itemsets.append(tuple(new_prefix_sorted))\n\n            conditional_patterns = []\n            node = head_node\n            while node is not None:\n                path = []\n                current = node.parent\n                while current.parent is not None:\n                    path.append(current.item)\n                    current = current.parent\n                path.reverse()\n                if path:\n                    conditional_patterns.append((path, node.count))\n                node = node.node_link\n\n            if conditional_patterns:\n                cond_root, cond_header_table = build_fp_tree(conditional_patterns, min_support)\n                if cond_header_table is not None:\n                    cond_header_list = sorted(cond_header_table.items(), key=lambda x: (-x[1][0], x[0]))\n                    mine_tree(cond_header_list, new_prefix_sorted, frequent_itemsets)\n\n    mine_tree(header_list, [], frequent_itemsets)\n\n    unique_itemsets = set(frequent_itemsets)\n    sorted_itemsets = [list(items) for items in unique_itemsets]\n    sorted_itemsets.sort(key=lambda x: (len(x), x))\n    return sorted_itemsets\n```\n\n### Explanation\n1. **Counting and Filtering Items**: The algorithm starts by counting the frequency of each item in the transactions and filtering out those that do not meet the minimum support threshold.\n2. **Building the FP-Tree**: Transactions are processed to include only frequent items, sorted by their frequency. The FP-tree is constructed by inserting each transaction into the tree, incrementing counts for existing nodes or creating new nodes as needed.\n3. **Mining the FP-Tree**: The tree is mined recursively starting from the least frequent items. For each item, conditional pattern bases are generated, and conditional FP-trees are built. These trees are recursively mined to discover all frequent item-sets, which are then deduplicated and sorted according to the specified criteria.\n\nThis approach efficiently discovers all frequent item-sets using the FP-Growth algorithm, leveraging the FP-tree structure to avoid the expensive candidate generation step of traditional methods."}
{"task_id": 71, "completion_id": 0, "completion": "To solve this problem, we need to count the occurrences of each class label in a tabular dataset. Each sample in the dataset is represented as a list, with the last element of each list being the class label. Our task is to iterate through the dataset once and return a dictionary where keys are the unique labels and values are their respective counts.\n\n### Approach\n1. **Initialize an empty dictionary**: This dictionary will store the counts of each label.\n2. **Iterate through each sample**: For each sample in the dataset, extract the last element, which is the class label.\n3. **Update the dictionary**: For each label encountered, increment its count in the dictionary. If the label is not already present in the dictionary, initialize its count to 1.\n4. **Return the dictionary**: After processing all samples, return the dictionary containing the label counts.\n\nThis approach ensures that we only iterate through the dataset once, making the solution efficient with a time complexity of O(n), where n is the number of samples in the dataset.\n\n### Solution Code\n```python\ndef label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count the occurrence of each label in a tabular data set.\n\n    Each sample in *data* is a list whose last element represents the\n    class label.  The function must return a dictionary mapping every\n    unique label to the number of times it appears in the data set.\n\n    Args:\n        data: List of samples (list of lists).  The last element of each\n              inner list is treated as the label.\n\n    Returns:\n        A dictionary where keys are unique labels and values are their\n        integer counts.  If *data* is empty an empty dictionary is\n        returned.\n    \"\"\"\n    counts = {}\n    for sample in data:\n        label = sample[-1]\n        counts[label] = counts.get(label, 0) + 1\n    return counts\n```\n\n### Explanation\n- **Initialization**: We start with an empty dictionary `counts` to keep track of label frequencies.\n- **Iteration**: For each sample in the input data, we extract the label using `sample[-1]`, which accesses the last element of the list.\n- **Updating Counts**: Using the `get` method of the dictionary, we retrieve the current count of the label (defaulting to 0 if the label is not present) and increment it by 1. This method efficiently handles both existing and new labels.\n- **Return Result**: After processing all samples, the dictionary `counts` is returned, containing the frequency of each label.\n\nThis solution efficiently processes the dataset in a single pass, ensuring optimal performance while correctly handling all edge cases, including an empty input list."}
{"task_id": 72, "completion_id": 0, "completion": "To solve this problem, we need to calculate the variance reduction (VR) resulting from a split in a regression tree. The VR is determined by comparing the variance of the target values in the parent node with the weighted variances of the target values in the left and right child nodes after the split. The variance is computed for each feature (column) separately and then summed to handle multi-dimensional targets.\n\n### Approach\n1. **Calculate the Parent Variance**: Compute the population variance for each feature in the parent node and sum them up.\n2. **Calculate Child Variances**: Compute the population variance for each feature in both left and right child nodes and sum them up.\n3. **Weight the Child Variances**: Adjust the variances of the left and right child nodes by their respective proportions relative to the parent node's sample size.\n4. **Compute Variance Reduction**: Subtract the weighted child variances from the parent variance to get the variance reduction.\n5. **Round the Result**: Return the result rounded to the fourth decimal place for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef variance_reduction(y: np.ndarray, y_left: np.ndarray, y_right: np.ndarray) -> float:\n    \"\"\"Calculate the variance reduction after splitting a node into two children.\"\"\"\n    n = y.shape[0]\n    n_left = y_left.shape[0]\n    n_right = y_right.shape[0]\n    \n    # Calculate parent variance summed over all features\n    var_parent = np.var(y, axis=0, ddof=0).sum()\n    \n    # Calculate variances for left and right children\n    var_left = np.var(y_left, axis=0, ddof=0).sum()\n    var_right = np.var(y_right, axis=0, ddof=0).sum()\n    \n    # Compute variance reduction\n    vr = var_parent - (n_left / n) * var_left - (n_right / n) * var_right\n    \n    return round(vr, 4)\n```\n\n### Explanation\n- **Parent Variance Calculation**: The variance of the parent node is computed using `np.var` with `axis=0` to compute variance for each feature and `ddof=0` to ensure population variance. The results are summed to get the total variance.\n- **Child Variances**: Similarly, variances for the left and right child nodes are computed and summed.\n- **Weighted Child Variances**: Each child variance is adjusted by the proportion of samples they contribute relative to the parent node. This ensures that larger child nodes have a greater impact on the variance reduction.\n- **Variance Reduction**: The final VR is obtained by subtracting the weighted child variances from the parent variance, providing a measure of how effective the split is in reducing variance.\n\nThis approach efficiently handles both single-dimensional and multi-dimensional target vectors, ensuring accurate and precise results."}
{"task_id": 74, "completion_id": 0, "completion": "To solve this problem, we need to compute the magnitude spectrum of real-valued frames using the non-redundant positive-frequency part of the Discrete Fourier Transform (DFT). The solution should handle both single frames (1D arrays) and batches of frames (2D arrays), returning the results as nested lists with values rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: Check if the input is a 1D array. If so, reshape it into a 2D array with a single row to handle uniformly with 2D inputs.\n2. **Compute FFT**: Use NumPy's real FFT (`numpy.fft.rfft`) to compute the positive-frequency components for each frame. This function efficiently computes the FFT for real-valued inputs and returns only the non-redundant positive frequencies.\n3. **Magnitude Calculation**: Convert the complex FFT results to their magnitudes using the absolute value.\n4. **Rounding**: Round each magnitude value to four decimal places to meet the precision requirement.\n5. **Conversion to List**: Convert the resulting NumPy array into a nested list structure using `tolist()` for compatibility with the specified output format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef magnitude_spectrum(frames: \"np.ndarray\") -> \"list[list[float]]\":\n    \"\"\"Compute the positive-frequency magnitude spectrum for each frame.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray\n        Either a 2-D array of shape (M, N) where each row is a frame, or a\n        1-D array treated as a single frame of length N.\n\n    Returns\n    -------\n    list[list[float]]\n        The magnitude spectrum of every frame, rounded to four decimals.\n    \"\"\"\n    # Ensure input is 2D for uniform processing\n    if frames.ndim == 1:\n        frames = frames.reshape(1, -1)\n    \n    # Compute the real FFT along the second axis (each frame's data)\n    fft_coefficients = np.fft.rfft(frames, axis=1)\n    \n    # Compute the magnitude spectrum\n    magnitudes = np.abs(fft_coefficients)\n    \n    # Round to four decimal places\n    rounded_magnitudes = np.around(magnitudes, decimals=4)\n    \n    # Convert to a list of lists\n    return rounded_magnitudes.tolist()\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input is a 1D array. If so, it reshapes it into a 2D array with one row, ensuring consistent processing for both single and multiple frames.\n2. **FFT Computation**: Using `numpy.fft.rfft`, the function computes the FFT along the columns (axis=1) of the input array. This ensures that each row (frame) is processed independently, returning only the non-redundant positive frequencies.\n3. **Magnitude Calculation**: The absolute value of the complex FFT results is taken to obtain the magnitude spectrum.\n4. **Rounding**: The magnitudes are rounded to four decimal places using `numpy.around` to ensure precision.\n5. **Conversion**: The resulting NumPy array is converted to a nested list using `tolist()`, which is the required output format.\n\nThis approach efficiently handles both single and multiple frames, ensuring accurate and precise results while leveraging NumPy's optimized functions for performance."}
{"task_id": 75, "completion_id": 0, "completion": "To solve this problem, we need to implement a k-Nearest Neighbor (k-NN) classifier that assigns class labels to test samples based on the majority vote of their nearest neighbors in the training set. The solution must handle invalid values of k appropriately and efficiently compute distances using the Euclidean metric.\n\n### Approach\n1. **Check Validity of k**: Ensure that k is a positive integer and does not exceed the number of training samples. If invalid, return a list of -1s for each test sample.\n2. **Convert Data to NumPy Arrays**: Convert the training and test data into NumPy arrays for efficient computation of distances.\n3. **Compute Distances**: For each test sample, compute the squared Euclidean distances to all training samples to avoid unnecessary square root operations.\n4. **Find Nearest Neighbors**: Identify the k nearest neighbors by sorting the distances and selecting the smallest k indices.\n5. **Majority Vote**: Count the class labels of the k nearest neighbors and determine the majority class. In case of a tie, return the smallest class label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\"\"\"\n    n_train = len(X_train)\n    n_test = len(X_test)\n    \n    # Check if k is invalid\n    if not isinstance(k, int) or k <= 0 or k > n_train:\n        return [-1] * n_test\n    \n    # Convert to numpy arrays for efficient computation\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n    \n    predictions = []\n    for x in X_test_np:\n        # Calculate squared Euclidean distances\n        distances = np.sum((X_train_np - x) ** 2, axis=1)\n        \n        # Get indices of k smallest distances\n        nearest_indices = np.argsort(distances)[:k]\n        \n        # Get corresponding labels\n        nearest_labels = y_train_np[nearest_indices]\n        \n        # Count occurrences of each label\n        label_counts = {}\n        for label in nearest_labels:\n            label_counts[label] = label_counts.get(label, 0) + 1\n        \n        # Determine the label with the maximum count, breaking ties by smallest label\n        max_count = max(label_counts.values())\n        candidates = [label for label, count in label_counts.items() if count == max_count]\n        predicted_label = min(candidates)\n        predictions.append(predicted_label)\n    \n    return predictions\n```\n\n### Explanation\n1. **Validity Check**: The function first checks if k is a valid positive integer within the range of the number of training samples. If not, it returns a list of -1s.\n2. **Data Conversion**: Training and test data are converted to NumPy arrays to leverage vectorized operations for efficient distance calculations.\n3. **Distance Calculation**: Squared Euclidean distances are computed for each test sample relative to all training samples, avoiding the computationally expensive square root operation.\n4. **Nearest Neighbors Identification**: Using argsort, the indices of the k smallest distances are identified, and corresponding class labels are retrieved.\n5. **Majority Voting**: The class labels of the nearest neighbors are counted, and the label with the highest count is selected. In case of a tie, the smallest class label is chosen.\n\nThis approach ensures efficient computation and correct handling of edge cases, providing accurate predictions based on the k-NN algorithm."}
{"task_id": 76, "completion_id": 0, "completion": "To solve this problem, we need to compute the categorical cross-entropy loss for a batch of one-hot encoded targets. This loss function is commonly used in classification tasks to measure the difference between predicted probabilities and true labels. The key steps involve converting inputs to numpy arrays for efficient computation, ensuring numerical stability by adding a small epsilon value, and correctly applying the cross-entropy formula.\n\n### Approach\n1. **Input Conversion**: Convert the input labels (y) and predicted probabilities (y_pred) into numpy arrays to leverage efficient element-wise operations.\n2. **Numerical Stability**: Add a small value (machine epsilon) to the predicted probabilities to avoid taking the logarithm of zero, which would be numerically unstable.\n3. **Logarithm Calculation**: Compute the natural logarithm of the adjusted predicted probabilities.\n4. **Element-wise Multiplication**: Multiply the true labels with the computed logarithms. Since the true labels are one-hot encoded, this step effectively isolates the log probability of the true class for each sample.\n5. **Summation and Negation**: Sum all the resulting values and take the negative to obtain the loss.\n6. **Rounding**: Round the computed loss to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | np.ndarray, y_pred: list | np.ndarray) -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y)\n    y_pred = np.asarray(y_pred)\n    eps = np.finfo(y_pred.dtype).eps\n    log_probs = np.log(y_pred + eps)\n    loss = -np.sum(y * log_probs)\n    return round(loss.item(), 4)\n```\n\n### Explanation\n1. **Input Conversion**: The inputs `y` and `y_pred` are converted to numpy arrays using `np.asarray` to handle both list and numpy array inputs uniformly.\n2. **Numerical Stability**: Machine epsilon, the smallest positive number such that `1.0 + eps != 1.0` for the data type of `y_pred`, is added to each element of `y_pred` to prevent taking the logarithm of zero.\n3. **Logarithm Calculation**: The logarithm of the adjusted predicted probabilities is computed using `np.log`.\n4. **Element-wise Multiplication**: The product of the true labels `y` and the log probabilities is computed. Since `y` is one-hot encoded, this operation effectively selects the log probability of the true class for each sample.\n5. **Summation and Negation**: The sum of all elements in the resulting matrix is computed and negated to obtain the loss value.\n6. **Rounding**: The final loss value is rounded to four decimal places using Python's built-in `round` function to meet the specified precision requirement.\n\nThis approach efficiently computes the cross-entropy loss while ensuring numerical stability and correctness, leveraging numpy's optimized operations for performance."}
{"task_id": 77, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward propagation for an L-layer neural network using ReLU activation for the hidden layers and sigmoid activation for the output layer. The goal is to compute the final activation matrix and collect necessary intermediate values (caches) for potential use in backpropagation.\n\n### Approach\n1. **Initialization**: Determine the number of layers (L) from the parameters dictionary. Initialize the activation from the input data (X) and an empty list to store caches.\n2. **Layer Iteration**: Loop through each layer from 1 to L:\n   - **Linear Step**: Compute the linear transformation using the weights (W) and biases (b) from the parameters dictionary.\n   - **Activation Step**: Apply ReLU activation for hidden layers (1 to L-1) and sigmoid activation for the output layer (L).\n   - **Cache Storage**: Store the intermediate values (input activations, weights, biases, and linear output) needed for backpropagation in the caches list.\n3. **Final Output**: After processing all layers, round the final activation matrix (AL) to 4 decimal places and convert it to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> tuple[list[list[float]], list]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\"\"\"\n\n    L = len(parameters) // 2\n    caches = []\n    A_prev = X\n\n    for l in range(1, L + 1):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        Z = np.dot(W, A_prev) + b\n        if l == L:\n            A = 1 / (1 + np.exp(-Z))  # Sigmoid\n        else:\n            A = np.maximum(0, Z)       # ReLU\n\n        linear_cache = (A_prev, W, b)\n        activation_cache = Z\n        caches.append((linear_cache, activation_cache))\n        A_prev = A\n\n    AL_rounded = np.round(A_prev, 4)\n    return AL_rounded.tolist(), caches\n```\n\n### Explanation\n1. **Initialization**: The number of layers (L) is determined by dividing the length of the parameters dictionary by 2 (since each layer has a weight matrix and a bias vector).\n2. **Layer Processing**:\n   - For each layer, the linear transformation is computed using the current weights and biases.\n   - ReLU activation is applied for hidden layers, which introduces non-linearity by outputting the positive part of its input.\n   - Sigmoid activation is applied to the output layer, squashing the values between 0 and 1, suitable for binary classification.\n3. **Caching**: Intermediate values (input activations, weights, biases, and linear outputs) are stored in caches to facilitate efficient gradient computation during backpropagation.\n4. **Output Processing**: The final activation matrix (AL) is rounded to 4 decimal places and converted to a Python list to meet the specified output format.\n\nThis approach ensures that the neural network processes each layer correctly, applies the appropriate activation functions, and prepares necessary intermediate values for potential backpropagation, all while returning the final activations in the required format."}
{"task_id": 78, "completion_id": 0, "completion": "To solve this problem, we need to implement the Adamax optimizer, which is a variant of the Adam optimizer. The Adamax optimizer uses an exponentially weighted infinity-norm instead of the squared gradients, making it more robust in certain scenarios. The goal is to perform one update step of the Adamax optimizer on given parameters, gradients, and other necessary components.\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Convert the input lists (parameters, gradients, first-moment estimates, and exponentially weighted infinity-norms) into numpy arrays for efficient element-wise operations.\n2. **Update First-Moment Estimates (m)**: Compute the new first-moment estimates using an exponential moving average of the gradients.\n3. **Update Exponentially Weighted Infinity-Norms (u)**: Compute the new infinity-norms by taking the element-wise maximum between the decayed previous norms and the absolute values of the current gradients.\n4. **Compute the Step Size**: Adjust the learning rate by the bias correction factor (1 - beta1^t) and compute the step size using the updated first-moment estimates and infinity-norms.\n5. **Update Parameters**: Subtract the computed step from the current parameters.\n6. **Increment Time Step**: Increment the time step counter by 1.\n7. **Round and Convert Back to Lists**: Convert the numpy arrays back to lists of floats, rounding each element to 6 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adamax_step(params: list[float],\n                grads:  list[float],\n                m:      list[float],\n                u:      list[float],\n                t:      int,\n                learning_rate: float = 0.002,\n                beta1:         float = 0.9,\n                beta2:         float = 0.999,\n                epsilon:       float = 1e-8) -> tuple[list[float], list[float], list[float], int]:\n    # Convert input lists to numpy arrays for element-wise operations\n    params_np = np.array(params, dtype=np.float64)\n    grads_np = np.array(grads, dtype=np.float64)\n    m_np = np.array(m, dtype=np.float64)\n    u_np = np.array(u, dtype=np.float64)\n    \n    # Update the first moment estimates (m)\n    m_new = beta1 * m_np + (1 - beta1) * grads_np\n    \n    # Update the exponentially weighted infinity norm (u)\n    u_new = np.maximum(beta2 * u_np, np.abs(grads_np))\n    \n    # Compute the denominator for bias correction\n    denominator = 1 - (beta1 ** t)\n    \n    # Calculate the step size\n    step = (learning_rate / denominator) * m_new / (u_new + epsilon)\n    \n    # Update the parameters\n    params_updated = params_np - step\n    \n    # Increment the time step\n    new_t = t + 1\n    \n    # Round to 6 decimal places and convert back to lists\n    updated_params = np.round(params_updated, 6).tolist()\n    updated_m = np.round(m_new, 6).tolist()\n    updated_u = np.round(u_new, 6).tolist()\n    \n    return (updated_params, updated_m, updated_u, new_t)\n```\n\n### Explanation\n1. **Conversion to Numpy Arrays**: This step allows efficient element-wise operations which are crucial for handling vectors in optimization algorithms.\n2. **First-Moment Estimates (m)**: This step smooths the gradients using an exponential moving average, which helps in reducing noise in the gradients.\n3. **Infinity-Norm Updates (u)**: By taking the element-wise maximum between the decayed previous norms and current gradient magnitudes, we ensure that large gradients have a lasting effect, which is beneficial for sparse data.\n4. **Bias Correction**: The term `1 - beta1^t` corrects the bias in the first-moment estimates, especially important in the initial steps when the moving average is not stabilized.\n5. **Step Calculation**: The learning rate is adjusted by the bias correction factor, and the step is scaled by the infinity-norms to ensure robustness against gradient scales.\n6. **Parameter Update**: The parameters are updated by subtracting the computed step, moving them towards the optimal values.\n7. **Rounding and Conversion**: Ensures the output is in the required format with precision, maintaining consistency with the problem constraints.\n\nThis approach efficiently implements the Adamax update step, ensuring numerical stability and correctness while handling all element-wise operations appropriately."}
{"task_id": 80, "completion_id": 0, "completion": "To solve this problem, we need to implement a single update step of the Adadelta optimization algorithm. Adadelta is an adaptive learning rate method that adjusts the learning rate based on the moving averages of the squared gradients and parameter updates. This problem specifically focuses on the first update step where the running averages are initialized to zero.\n\n### Approach\nThe Adadelta algorithm adjusts the learning rate for each parameter based on the historical gradients and parameter updates. For the first step, the running averages (E_grad and E_delta) are initialized to zero. The key steps involved in the Adadelta update are as follows:\n\n1. **Update the running average of squared gradients (E_grad)**:\n   \\[\n   E_{\\text{grad}} = \\rho \\cdot E_{\\text{grad}} + (1 - \\rho) \\cdot g^2\n   \\]\n   Since this is the first step, the initial E_grad is zero, so the updated E_grad becomes \\((1 - \\rho) \\cdot g^2\\).\n\n2. **Compute the root mean squared (RMS) of the gradients**:\n   \\[\n   \\text{RMS}_{\\text{grad}} = \\sqrt{E_{\\text{grad}} + \\epsilon\n   \\]\n\n3. **Compute the RMS of the parameter updates (\u0394w)**:\n   \\[\n   \\text{RMS}_{\\Delta w} = \\sqrt{\\epsilon}\n   \\]\n   Since E_delta is initialized to zero, the RMS of the parameter updates is simply the square root of the small constant \u03b5.\n\n4. **Calculate the adaptive learning rate**:\n   \\[\n   \\text{adaptive\\_lr} = \\frac{\\text{RMS}_{\\Delta w}}{\\text{RMS}_{\\text{grad}}\n   \\]\n\n5. **Compute the parameter update (\u0394w)**:\n   \\[\n   \\Delta w = \\text{adaptive\\_lr} \\cdot g\n   \\]\n\n6. **Update the weights**:\n   \\[\n   w_{\\text{new}} = w - \\Delta w\n   \\]\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adadelta_update(w: list[float] | np.ndarray,\n                    grad_w: list[float] | np.ndarray,\n                    rho: float = 0.95,\n                    eps: float = 1e-6) -> list[float]:\n    \"\"\"Perform **one** Adadelta optimisation step.\"\"\"\n    # Convert inputs to numpy arrays for element-wise operations\n    w_np = np.asarray(w)\n    grad_w_np = np.asarray(grad_w)\n    \n    # Compute E_grad (since initial E_grad is zero)\n    e_grad = (1 - rho) * (grad_w_np ** 2)\n    \n    # Compute RMS_grad and RMS_delta\n    rms_grad = np.sqrt(e_grad + eps)\n    rms_delta = np.sqrt(eps)  # Since initial E_delta is zero\n    \n    # Calculate adaptive learning rate\n    adaptive_lr = rms_delta / rms_grad\n    \n    # Compute delta_w\n    delta_w = adaptive_lr * grad_w_np\n    \n    # Update weights\n    new_w = w_np - delta_w\n    \n    # Round to 4 decimal places and convert to a regular Python list\n    return np.round(new_w, 4).tolist()\n```\n\n### Explanation\nThe provided code converts the input weights and gradients to numpy arrays to handle element-wise operations efficiently. It then computes the necessary components for the Adadelta update step:\n1. **E_grad** is calculated using the initial zero value, resulting in \\((1 - \\rho) \\cdot g^2\\).\n2. **RMS_grad** and **RMS_delta** are computed using the updated E_grad and the initial zero E_delta, respectively.\n3. The adaptive learning rate is derived from the ratio of RMS_delta to RMS_grad.\n4. The parameter update (\u0394w) is determined by multiplying the adaptive learning rate with the gradient.\n5. The new weights are obtained by subtracting the parameter update from the current weights and rounding the result to four decimal places.\n\nThis approach ensures that the algorithm works element-wise for any shape of input, efficiently handles the first update step, and returns the updated weights as a Python list rounded to four decimal places."}
{"task_id": 81, "completion_id": 0, "completion": "To solve this problem, we need to compute the binary cross-entropy loss (log-loss) between the predicted probabilities from a binary classifier and the ground-truth labels. This loss function is crucial for evaluating the performance of the classifier, as it penalizes incorrect predictions more heavily the more confident they are.\n\n### Approach\n1. **Clipping Predictions**: Since taking the logarithm of zero is undefined, we first clip the predicted probabilities to a small interval around [0, 1]. Specifically, we use \u03b5 = 1e-15 to avoid numerical instability.\n2. **Logarithmic Terms Calculation**: For each sample, compute two terms: one involving the logarithm of the predicted probability (for actual label 1) and another involving the logarithm of one minus the predicted probability (for actual label 0).\n3. **Summation and Averaging**: Sum these terms across all samples, then average and negate the result to get the final cost.\n4. **Rounding**: The computed cost is rounded to six decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    m = A2.size\n    log_probs = Y * np.log(A2_clipped) + (1 - Y) * np.log(1 - A2_clipped)\n    cost = -np.sum(log_probs) / m\n    return round(cost.item(), 6)\n```\n\n### Explanation\n1. **Clipping**: The `np.clip` function ensures that all elements of `A2` are within [\u03b5, 1-\u03b5], preventing division by zero or taking the logarithm of zero.\n2. **Logarithmic Terms**: The terms `Y * np.log(A2_clipped)` and `(1 - Y) * np.log(1 - A2_clipped)` compute the contributions of each sample to the loss, depending on whether the true label is 1 or 0.\n3. **Summation and Averaging**: By summing these terms and dividing by the number of samples `m`, we average the loss over all samples. Multiplying by -1 converts the sum of negative log probabilities into a positive loss value.\n4. **Rounding**: The result is rounded to six decimal places using Python's `round` function to meet the specified precision requirement.\n\nThis approach efficiently computes the binary cross-entropy loss using vectorized operations, ensuring both numerical stability and computational efficiency."}
{"task_id": 82, "completion_id": 0, "completion": "To solve this problem, we need to implement the \u03b5-soft exploration policy in reinforcement learning. This policy ensures that every action has a non-zero probability of being selected while still favoring the greedy (best) action. The goal is to compute the probabilities for each action given a set of Q-values and an exploration parameter \u03b5.\n\n### Approach\n1. **Identify the Greedy Action**: Determine the action with the highest Q-value. In case of ties, select the first occurrence (smallest index).\n2. **Calculate Base Probability**: Compute the base probability for non-greedy actions as \u03b5 divided by the number of actions.\n3. **Adjust Probability for Greedy Action**: The greedy action's probability is adjusted to be higher than the base probability by adding (1 - \u03b5) to the base probability of the greedy action.\n4. **Ensure Numerical Stability**: Convert the resulting probabilities to a list, round each to four decimal places, and ensure the sum of the probabilities is 1 within a numerical tolerance of 1e-12.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef epsilon_soft(Q: list[float] | np.ndarray, epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list must contain the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    Q_array = np.asarray(Q)\n    n = Q_array.size\n    if n == 0:\n        return []\n    a_star = np.argmax(Q_array)\n    base = epsilon / n\n    probs = np.full(n, base, dtype=np.float64)\n    probs[a_star] += 1 - epsilon\n    # Round to 4 decimal places using numpy's around to ensure consistent rounding\n    rounded_probs = np.round(probs, 4)\n    # Convert to list of Python floats (to match the required output type)\n    return rounded_probs.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input Q-values are converted to a NumPy array for easier manipulation.\n2. **Determine Number of Actions**: The number of actions `n` is determined from the size of the Q-values array.\n3. **Identify Greedy Action**: Using `np.argmax`, the index of the first occurrence of the maximum Q-value is found.\n4. **Compute Base Probability**: The base probability for non-greedy actions is calculated as \u03b5/n.\n5. **Adjust Greedy Action Probability**: The probability of the greedy action is increased by (1 - \u03b5) to ensure it has a higher chance of being selected.\n6. **Rounding**: Each probability is rounded to four decimal places to meet the problem's requirements.\n7. **Return as List**: The probabilities are converted back to a list of floats and returned.\n\nThis approach ensures that the probabilities are computed efficiently and meet the numerical stability requirements, while adhering to the \u03b5-soft exploration policy constraints."}
{"task_id": 84, "completion_id": 0, "completion": "To solve this problem, we need to determine the optimal step size (\u03b3) for adding a new base learner in gradient boosting with mean-squared-error (MSE) loss. The optimal step size is calculated using a closed-form solution derived from setting the derivative of the MSE loss to zero. If the denominator in this solution is zero, we return a step size of 1.0 by convention.\n\n### Approach\n1. **Calculate Residuals**: Compute the residuals by subtracting the current predictions from the true target values.\n2. **Compute Numerator and Denominator**: The numerator is the dot product of the residuals and the new base learner's predictions. The denominator is the dot product of the new base learner's predictions with itself.\n3. **Check for Zero Denominator**: If the denominator is zero (indicating all predictions from the new base learner are zero), return 1.0 as the optimal step size.\n4. **Calculate Step Size**: If the denominator is not zero, compute the step size as the ratio of the numerator to the denominator.\n5. **Round the Result**: Return the computed step size rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mse_line_search(y: np.ndarray, y_pred: np.ndarray, h_pred: np.ndarray) -> float:\n    \"\"\"Calculate the optimal step size for adding a new learner when optimising\n    the mean squared error (MSE) loss.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        Array of true target values.\n    y_pred : np.ndarray\n        Array of current predictions produced by the model.\n    h_pred : np.ndarray\n        Array of predictions from the new base learner.\n\n    Returns\n    -------\n    float\n        The optimal step size \u03b3 rounded to four decimal places. If `h_pred`\n        perfectly fits the residuals the function should return 1.0.\n    \"\"\"\n    residuals = y - y_pred\n    numerator = np.dot(residuals, h_pred)\n    denominator = np.dot(h_pred, h_pred)\n    \n    if denominator == 0:\n        gamma = 1.0\n    else:\n        gamma = numerator / denominator\n    \n    return round(gamma, 4)\n```\n\n### Explanation\n1. **Residuals Calculation**: The residuals are the differences between the true target values and the current predictions. This represents the error that the new base learner should aim to correct.\n2. **Numerator and Denominator**: The numerator is the sum of the product of residuals and the new base learner's predictions. The denominator is the sum of the squares of the new base learner's predictions. These are computed efficiently using dot products.\n3. **Zero Denominator Handling**: If all predictions from the new base learner are zero, the denominator becomes zero. In this case, the step size is set to 1.0 by convention, as the model cannot improve further.\n4. **Step Size Calculation**: The step size is the ratio of the numerator to the denominator. This ratio minimizes the MSE loss when adding the new base learner's predictions to the current model's predictions.\n5. **Rounding**: The result is rounded to four decimal places to meet the specified precision requirement.\n\nThis approach ensures that we efficiently compute the optimal step size while handling edge cases where the denominator might be zero, adhering to the problem constraints and conventions."}
{"task_id": 85, "completion_id": 0, "completion": "To solve this problem, we need to perform a single gradient-descent update for a multi-class Softmax regression model using cross-entropy loss. The goal is to update the weight matrix based on the given training data, labels, and learning rate.\n\n### Approach\n1. **Check Edge Cases**: If the learning rate `lr` is zero or the input matrix `X` consists entirely of zeros, return the weight matrix `W` unchanged.\n2. **Compute Logits**: Calculate the logits matrix `Z` by multiplying the input matrix `X` with the current weight matrix `W`.\n3. **Softmax Activation**: Apply the softmax function row-wise to the logits matrix `Z` to obtain the predicted probabilities matrix `\u0176`.\n4. **Compute Gradient**: Calculate the gradient of the average cross-entropy loss with respect to the weights using the formula `(X\u1d40 \u00b7 (\u0176 \u2212 Y)) / N`, where `N` is the number of training samples.\n5. **Update Weights**: Adjust the weights using gradient descent by subtracting the product of the learning rate and the gradient from the current weights.\n6. **Round and Convert**: Round the updated weight matrix to four decimal places and convert it to a Python list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softmax_regression_step(X: np.ndarray, Y: np.ndarray, W: np.ndarray, lr: float) -> list[list[float]]:\n    if lr == 0 or np.all(X == 0):\n        return np.round(W, 4).tolist()\n    \n    N = X.shape[0]\n    \n    # Compute logits\n    Z = X @ W\n    \n    # Compute softmax probabilities\n    exp_Z = np.exp(Z)\n    Y_hat = exp_Z / exp_Z.sum(axis=1, keepdims=True)\n    \n    # Compute gradient\n    grad = (X.T @ (Y_hat - Y)) / N\n    \n    # Update weights\n    W_new = W - lr * grad\n    \n    # Round and convert to list of lists\n    return np.round(W_new, 4).tolist()\n```\n\n### Explanation\n1. **Edge Cases Handling**: The function first checks if the learning rate is zero or if the input matrix `X` is all zeros. If either condition is true, the weight matrix `W` is returned unchanged.\n2. **Logits Calculation**: The logits matrix `Z` is computed using matrix multiplication of `X` and `W`.\n3. **Softmax Activation**: The softmax function is applied to each row of `Z` to convert logits into probabilities, ensuring that each row sums to 1.\n4. **Gradient Calculation**: The gradient is computed using the transpose of `X` and the difference between the predicted probabilities `\u0176` and the actual labels `Y`, scaled by the number of samples.\n5. **Weight Update**: The weights are adjusted by subtracting the product of the learning rate and the gradient from the current weights.\n6. **Rounding and Conversion**: The updated weights are rounded to four decimal places and converted to a list of lists for the final output.\n\nThis approach efficiently computes the necessary updates for the weight matrix while handling edge cases to ensure stability and correctness."}
{"task_id": 86, "completion_id": 0, "completion": "To solve this problem, we need to aggregate the predictions from multiple decision trees in a Random Forest classifier using majority voting. The final prediction for each sample is determined by the most frequent class label among the trees, with ties broken by selecting the smallest label.\n\n### Approach\n1. **Transpose the Predictions**: Convert the input list of lists (where each inner list represents predictions from a single tree) into a structure where each element corresponds to all predictions for a single sample. This is done using the `zip` function.\n2. **Count Votes**: For each sample, count the occurrences of each class label using the `Counter` class from the `collections` module.\n3. **Determine Majority Vote**: Identify the class label(s) with the highest count. If there is a tie (multiple labels with the same highest count), select the smallest label by sorting the candidates and choosing the first element.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    samples = zip(*predictions)\n    aggregated = []\n    for sample_votes in samples:\n        counts = Counter(sample_votes)\n        max_count = max(counts.values())\n        candidates = [k for k, v in counts.items() if v == max_count]\n        candidates_sorted = sorted(candidates)\n        aggregated.append(candidates_sorted[0])\n    return aggregated\n```\n\n### Explanation\n1. **Transposing the Predictions**: The `zip(*predictions)` operation converts rows of tree predictions into columns of sample predictions, allowing us to process each sample's votes individually.\n2. **Counting Votes**: Using `Counter`, we efficiently count the occurrences of each class label for every sample.\n3. **Handling Ties**: After identifying the class labels with the highest votes, we sort these labels and select the smallest one to break any ties. This ensures consistent results for both numeric and string labels, following natural ordering rules.\n\nThis approach efficiently aggregates predictions using majority voting, ensuring correctness and handling ties appropriately by leveraging sorting and counting mechanisms."}
{"task_id": 87, "completion_id": 0, "completion": "To solve this problem, we need to determine the sizes of the input, hidden, and output layers of a feed-forward neural network based on the given input data and labels. The hidden layer size is fixed, while the input and output layer sizes are derived from the dimensions of the input data and labels matrices, respectively.\n\n### Approach\n1. **Input Layer Size (n_x)**: This is determined by the number of rows in the input matrix `X`, as each row represents a different feature of the input data.\n2. **Hidden Layer Size (n_h)**: This is a fixed value set to 10 as per the problem requirements.\n3. **Output Layer Size (n_y)**: This is determined by the number of rows in the labels matrix `Y`, as each row represents a different output unit.\n\nBy extracting the number of rows from the input matrix `X` and the labels matrix `Y`, we can directly obtain the input and output layer sizes. The hidden layer size is simply set to 10.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) containing the training input.\n        Y: 2-D NumPy array of shape (n_y, m) containing the labels.\n\n    Returns:\n        Tuple (n_x, n_h, n_y) where n_h is fixed to 10.\n    \"\"\"\n    n_x = X.shape[0]\n    n_h = 10\n    n_y = Y.shape[0]\n    return (n_x, n_h, n_y)\n```\n\n### Explanation\n- **Input Layer Size (n_x)**: The number of input features is the number of rows in matrix `X`, which is accessed using `X.shape[0]`.\n- **Hidden Layer Size (n_h)**: This is fixed at 10 as specified in the problem statement.\n- **Output Layer Size (n_y)**: The number of output units is the number of rows in matrix `Y`, which is accessed using `Y.shape[0]`.\n\nThis approach efficiently extracts the necessary dimensions from the input matrices and combines them with the fixed hidden layer size to produce the required tuple. The solution is straightforward and leverages the structure of the input matrices to derive the layer sizes directly."}
{"task_id": 88, "completion_id": 0, "completion": "To solve this problem, we need to implement the softplus activation function in a numerically stable manner using NumPy's `logaddexp` function. The solution should handle scalar, list, and NumPy array inputs, returning the results rounded to 4 decimal places and converted to the appropriate Python data type.\n\n### Approach\n1. **Understanding the Softplus Function**: The softplus function is defined as \\( \\text{softplus}(z) = \\ln(1 + e^z) \\). Direct computation using exponentials can lead to overflow or underflow for extreme values of \\( z \\). Using `np.logaddexp(0.0, z)` ensures numerical stability by computing \\( \\ln(e^0 + e^z) \\) accurately.\n2. **Handling Different Input Types**: The function should accept scalars, Python lists, or NumPy arrays. For scalar inputs, the result is returned as a float. For array-like inputs, the result is converted to a Python list using NumPy's `tolist()` method.\n3. **Rounding**: The results are rounded to 4 decimal places to meet the problem's precision requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        s = np.logaddexp(0.0, z)\n        rounded = np.round(s, 4)\n        return float(rounded)\n    else:\n        z_array = np.asarray(z)\n        s = np.logaddexp(0.0, z_array)\n        s_rounded = np.round(s, 4)\n        return s_rounded.tolist()\n```\n\n### Explanation\n1. **Numerical Stability**: By using `np.logaddexp(0.0, z)`, we avoid the potential overflow issues with direct computation of \\( e^z \\) for large values of \\( z \\), and underflow issues for large negative values of \\( z \\).\n2. **Input Handling**: The function checks if the input is a scalar (int or float). If so, it computes the softplus value, rounds it, and returns it as a float. For array-like inputs, the input is converted to a NumPy array, processed element-wise, rounded, and then converted back to a Python list using `tolist()`.\n3. **Rounding**: The results are rounded to 4 decimal places using `np.round` to ensure precision, maintaining the required format for both scalar and array-like outputs.\n\nThis approach efficiently handles different input types and ensures numerical stability while providing accurate and formatted results."}
{"task_id": 89, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs forward propagation for a single layer of a neural network. This involves computing the linear transformation of the input activations followed by applying a non-linear activation function (ReLU or Sigmoid). The function should return the resulting activations and a cache for use in back-propagation.\n\n### Approach\n1. **Linear Transformation**: Compute the linear part of the forward propagation using the formula \\( Z = W \\cdot A_{\\text{prev}} + b \\), where \\( W \\) is the weight matrix, \\( A_{\\text{prev}} \\) is the activation from the previous layer, and \\( b \\) is the bias vector. The result \\( Z \\) is obtained using matrix multiplication and broadcasting the bias vector across all examples.\n2. **Activation Function**: Apply the specified activation function (ReLU or Sigmoid) element-wise to the linear output \\( Z \\).\n3. **Shape Check**: Ensure the resulting activation matrix has the correct shape, which should be \\( (W.shape[0], A_{\\text{prev}}.shape[1]) \\).\n4. **Rounding and Conversion**: Round the activation values to 4 decimal places and convert them to a plain Python list.\n5. **Cache Creation**: Store the inputs and intermediate results needed for back-propagation in a cache tuple.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_activation_forward(A_prev: np.ndarray,\n                              W: np.ndarray,\n                              b: np.ndarray,\n                              activation: str):\n    \"\"\"Forward propagation for a single neural-network layer.\"\"\"\n\n    # Compute linear transformation\n    Z = np.dot(W, A_prev) + b\n    linear_cache = (A_prev, W, b)\n    \n    # Apply activation function\n    if activation == \"relu\":\n        A = np.maximum(0, Z)\n    elif activation == \"sigmoid\":\n        A = 1 / (1 + np.exp(-Z))\n    else:\n        raise ValueError(f\"Unsupported activation function: {activation}\")\n    \n    # Verify output shape\n    assert A.shape == (W.shape[0], A_prev.shape[1]), \"Output shape mismatch\"\n    \n    # Prepare the output with rounding and conversion to list\n    A_rounded = A.round(4).tolist()\n    \n    # Prepare cache\n    activation_cache = Z\n    cache = (linear_cache, activation_cache)\n    \n    return (A_rounded, cache)\n```\n\n### Explanation\n1. **Linear Transformation**: The input activations \\( A_{\\text{prev}} \\) are multiplied by the weight matrix \\( W \\) and the bias vector \\( b \\) is added. This is done using matrix multiplication (`np.dot`) and broadcasting.\n2. **Activation Function**: Depending on the specified activation function, either ReLU or Sigmoid is applied to the linear output \\( Z \\). ReLU sets all negative values to zero, while Sigmoid squashes values between 0 and 1.\n3. **Shape Check**: An assertion verifies that the output activation matrix has the correct dimensions, ensuring the number of rows matches the number of neurons in the current layer and the number of columns matches the number of training examples.\n4. **Rounding and Conversion**: The activations are rounded to 4 decimal places for precision and converted to a Python list to meet the output requirements.\n5. **Cache**: The cache stores both the linear transformation inputs (for gradient computation during back-propagation) and the intermediate result \\( Z \\) (for activation gradient computation).\n\nThis approach efficiently computes the forward propagation step while ensuring correctness through shape checks and proper handling of numerical precision."}
{"task_id": 90, "completion_id": 0, "completion": "To solve this problem, we need to evaluate the performance of a policy in a multi-armed bandit problem by computing the mean-squared error (MSE) between the true expected payouts of each arm and the policy's current estimates of those payouts. The MSE is a measure of how close the policy's estimates are to the true values.\n\n### Approach\n1. **Check for Valid Estimates**: First, we need to ensure that the policy has valid estimates. If the policy does not have any estimates (either missing or an empty dictionary), we return `numpy.nan`.\n2. **Retrieve True Expected Values**: Retrieve the true expected values of the bandit's arms. If these values are missing or not in the correct format, return `numpy.nan`.\n3. **Check Arm Consistency**: Ensure that the policy's estimates cover all the arms present in the bandit. If any arm is missing from the policy's estimates, return `numpy.nan`.\n4. **Compute Squared Errors**: For each arm, compute the squared error between the true expected value and the policy's estimate.\n5. **Calculate MSE**: Average the squared errors and round the result to four decimal places to get the final MSE.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\"\"\"\n    # Check if policy has ev_estimates and it's a non-empty dictionary\n    ev_estimates = None\n    if hasattr(policy, 'ev_estimates'):\n        ev_estimates = policy.ev_estimates\n    elif isinstance(policy, dict) and 'ev_estimates' in policy:\n        ev_estimates = policy['ev_estimates']\n    else:\n        return np.nan\n    \n    if not isinstance(ev_estimates, dict) or len(ev_estimates) == 0:\n        return np.nan\n    \n    # Retrieve arm_evs from bandit and validate\n    arm_evs = None\n    if hasattr(bandit, 'arm_evs'):\n        arm_evs = bandit.arm_evs\n    elif isinstance(bandit, dict) and 'arm_evs' in bandit:\n        arm_evs = bandit['arm_evs']\n    else:\n        return np.nan\n    \n    if not isinstance(arm_evs, (list, tuple)):\n        return np.nan\n    \n    n_arms = len(arm_evs)\n    \n    # Ensure all arm indices are present in ev_estimates\n    for i in range(n_arms):\n        if i not in ev_estimates:\n            return np.nan\n    \n    squared_errors = []\n    for i in range(n_arms):\n        true_val = arm_evs[i]\n        estimate_val = ev_estimates[i]\n        squared_error = (estimate_val - true_val) ** 2\n        squared_errors.append(squared_error)\n    \n    average_mse = sum(squared_errors) / n_arms\n    return round(average_mse, 4)\n```\n\n### Explanation\n1. **Checking Policy Estimates**: The code first checks if the policy's estimates (`ev_estimates`) exist and are in the correct format (a non-empty dictionary). If not, it returns `numpy.nan`.\n2. **Retrieving True Values**: The true expected values (`arm_evs`) are retrieved from the bandit. If these values are missing or not a list/tuple, the function returns `numpy.nan`.\n3. **Consistency Check**: The code ensures that the policy's estimates cover all arms present in the bandit. If any arm is missing, it returns `numpy.nan`.\n4. **Computing Errors**: For each arm, the squared difference between the true value and the policy's estimate is computed and stored.\n5. **Averaging and Rounding**: The average of these squared errors is computed, rounded to four decimal places, and returned as the final MSE.\n\nThis approach ensures that all edge cases are handled, and the MSE is computed accurately based on the policy's current estimates."}
{"task_id": 91, "completion_id": 0, "completion": "To solve this problem, we need to compute the backward pass gradient for the Rectified Linear Unit (ReLU) activation function in a neural network. The ReLU activation function is defined as ReLU(z) = max(0, z), and during back-propagation, we need to compute the gradient of the loss with respect to the pre-activation values z. This gradient is determined using the upstream gradient from the next layer and the cached pre-activation values from the forward pass.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert the given upstream gradient (dA) and the cached pre-activation values (Z) from Python lists to NumPy arrays for efficient vectorized operations.\n2. **Shape Check**: Ensure that both inputs have the same shape. If they do not, return -1 as specified.\n3. **Compute Mask**: Create a boolean mask where each element is True if the corresponding element in Z is greater than 0, and False otherwise.\n4. **Element-wise Multiplication**: Multiply the upstream gradient (dA) by the boolean mask. This operation effectively zeros out the gradients where the pre-activation values were non-positive.\n5. **Convert Result to List**: Convert the resulting NumPy array back to a Python list of lists to match the expected output format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu_backward(dA: list[list[int | float]], activation_cache: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"Backward pass of the ReLU activation function.\n\n    Parameters\n    ----------\n    dA : list[list[int | float]]\n        Upstream gradient from the next layer in the network.\n    activation_cache : list[list[int | float]]\n        Cached pre-activation values (Z) from the forward pass.\n\n    Returns\n    -------\n    list[list[int | float]]\n        Gradient with respect to Z, having the same shape as *dA*.\n        Returns -1 if *dA* and *activation_cache* do not share the same shape.\n    \"\"\"\n    dA_np = np.array(dA)\n    Z_np = np.array(activation_cache)\n    \n    if dA_np.shape != Z_np.shape:\n        return -1\n    \n    mask = Z_np > 0\n    dZ_np = dA_np * mask\n    \n    return dZ_np.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The inputs dA and activation_cache (Z) are converted to NumPy arrays to leverage NumPy's efficient vectorized operations.\n2. **Shape Check**: The shapes of the converted arrays are compared. If they differ, the function returns -1 to indicate an error.\n3. **Boolean Mask Creation**: A boolean mask is created where each element is True if the corresponding element in Z is positive. This mask is used to zero out the gradients of non-positive pre-activation values.\n4. **Element-wise Multiplication**: The upstream gradient (dA) is multiplied element-wise by the boolean mask. This step ensures that only the gradients corresponding to positive pre-activation values are retained.\n5. **Conversion to List**: The resulting gradient (dZ) is converted back to a Python list of lists using NumPy's `tolist()` method to maintain the required output format.\n\nThis approach efficiently computes the backward pass gradient using vectorized operations, ensuring both correctness and performance."}
{"task_id": 92, "completion_id": 0, "completion": "To solve this problem, we need to determine if a given input value is numeric, excluding boolean values. A numeric value is defined as any instance of `numbers.Number` (e.g., int, float, complex, etc.) but not a boolean (True or False).\n\n### Approach\nThe approach involves two main checks:\n1. **Check if the input is an instance of `numbers.Number`**: This ensures that the input is of a numeric type such as int, float, complex, or other numeric types derived from `numbers.Number`.\n2. **Exclude boolean values**: Although boolean values (True and False) are technically subclasses of integers in Python, we need to explicitly exclude them as per the problem's requirements.\n\nBy combining these two checks, we can accurately determine if the input is a numeric type while excluding booleans.\n\n### Solution Code\n```python\nimport numbers\n\ndef is_number(a) -> bool:\n    \"\"\"Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n    \"\"\"\n    return isinstance(a, numbers.Number) and not isinstance(a, bool)\n```\n\n### Explanation\n- **Instance Check**: The function first checks if the input `a` is an instance of `numbers.Number`. This covers all standard numeric types in Python, including integers, floats, complexes, and other numeric types from modules like `fractions.Fraction`.\n- **Boolean Exclusion**: The function then checks if the input `a` is a boolean. Since booleans are a subclass of integers, they would pass the first check, so we explicitly exclude them by ensuring the input is not an instance of `bool`.\n\nThis approach efficiently combines these checks to ensure that only valid numeric types (excluding booleans) are recognized as numeric."}
{"task_id": 93, "completion_id": 0, "completion": "To solve this problem, we need to compute the Rectified Linear Unit (ReLU) activation function for a given input, which can be a scalar, a list, or a higher-dimensional array. The solution must return the ReLU output and the original input, both converted to Python lists to ensure JSON serializability.\n\n### Approach\n1. **Input Conversion**: Convert the input to a NumPy array to handle various input types uniformly.\n2. **Scalar Handling**: Check if the input is a scalar (0-dimensional array) and convert it to a 1-dimensional array to ensure the output is a list.\n3. **ReLU Calculation**: Apply the ReLU function element-wise using NumPy's `maximum` function to ensure the operation is vectorized and efficient.\n4. **Conversion to Lists**: Convert both the ReLU result and the original input (after processing) to Python lists using NumPy's `tolist()` method to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU) of *Z* and return\n    both the activation and the original input.\n\n    Args:\n        Z (array-like): A NumPy array, Python scalar or (nested) list of\n            numbers representing the pre-activation values.\n\n    Returns:\n        tuple[list, list]: A tuple `(A, cache)` where `A` is the ReLU of `Z`\n            and `cache` is `Z` itself, both converted to Python lists.\n    \"\"\"\n    Z_arr = np.asarray(Z)\n    # Convert scalar to a 1-D array to ensure output is a list\n    if Z_arr.ndim == 0:\n        Z_arr = Z_arr.reshape(1)\n    A = np.maximum(Z_arr, 0)\n    return (A.tolist(), Z_arr.tolist())\n```\n\n### Explanation\n1. **Input Conversion**: The input `Z` is converted to a NumPy array using `np.asarray(Z)`, which handles scalars, lists, and nested lists.\n2. **Scalar Handling**: If the input is a scalar (detected by checking if the array's dimension is 0), it is reshaped into a 1-dimensional array to ensure the output is a list.\n3. **ReLU Calculation**: The ReLU function is applied using `np.maximum(Z_arr, 0)`, which efficiently computes the element-wise maximum of the input array and 0.\n4. **List Conversion**: Both the ReLU result `A` and the original input `Z_arr` (after processing) are converted to Python lists using `tolist()`, ensuring the output is JSON-serializable and meets the problem's requirements."}
{"task_id": 94, "completion_id": 0, "completion": "To solve this problem, we need to determine the majority class label from a list of class labels using the k-Nearest Neighbors (k-NN) algorithm. The majority class label is the one that appears most frequently. In case of a tie, we return the smallest label according to Python\u2019s default ordering.\n\n### Approach\n1. **Count Occurrences**: Use a Counter to count the occurrences of each class label in the input list.\n2. **Find Maximum Frequency**: Identify the highest frequency of any class label.\n3. **Collect Candidates**: Gather all class labels that have this maximum frequency.\n4. **Resolve Ties**: If multiple labels have the same highest frequency, sort these labels using Python\u2019s default ordering and return the smallest one.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef knn_majority_vote(neighbors_targets: list[str | int]) -> str | int:\n    \"\"\"Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A list containing the class labels of the k nearest\n            neighbours.\n\n    Returns:\n        The label that appears most frequently. In case of a tie, the smallest\n        label according to Python\u2019s default ordering is returned.\n    \"\"\"\n    counts = Counter(neighbors_targets)\n    max_count = max(counts.values())\n    candidates = [label for label, cnt in counts.items() if cnt == max_count]\n    sorted_candidates = sorted(candidates)\n    return sorted_candidates[0]\n```\n\n### Explanation\n1. **Counting Occurrences**: The `Counter` from the `collections` module efficiently counts the occurrences of each label in the input list.\n2. **Finding Maximum Frequency**: The maximum value among the counts is determined using the `max` function.\n3. **Collecting Candidates**: All labels that have the maximum count are filtered into a list of candidates.\n4. **Sorting and Returning the Smallest**: The candidates are sorted, and the smallest label (according to Python\u2019s default ordering) is returned. This ensures that in case of a tie, the smallest label is selected.\n\nThis approach ensures that we efficiently determine the majority class label while handling ties correctly by leveraging Python's built-in sorting mechanism."}
{"task_id": 96, "completion_id": 0, "completion": "To solve this problem, we need to compute the additive-smoothed log-probability of a given sentence using an N-gram language model. The solution involves building frequency tables from a training corpus, handling unseen words, and applying additive smoothing to estimate probabilities.\n\n### Approach\n1. **Build Vocabulary and N-gram Counts**: \n   - Extract all unique words from the training corpus to form the vocabulary.\n   - Generate n-gram frequency tables for n from 1 to N using the training corpus.\n\n2. **Preprocess Target Sentence**:\n   - Replace any word in the target sentence that is not present in the training corpus vocabulary with the \"<UNK>\" token.\n\n3. **Check Sentence Length**:\n   - If the length of the preprocessed target sentence is less than N, return 0.0 as no valid N-grams can be formed.\n\n4. **Compute Probabilities with Smoothing**:\n   - For each N-gram in the target sentence, compute the probability using additive smoothing. The formula used is:\n     \\[\n     P(w_i | \\text{context}) = \\frac{\\text{count}(\\text{context} \\circ w_i) + K}{\\text{count}(\\text{context}) + K \\cdot |V|}\n     \\]\n   - Here, \\( |V| \\) is the size of the vocabulary including the \"<UNK>\" token.\n\n5. **Sum Log-Probabilities**:\n   - Convert each probability to its natural logarithm and sum these values. Round the result to four decimal places.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter, defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    # Step 1: Collect all words in the corpus to build the training vocabulary\n    training_vocab = set()\n    for sentence in corpus:\n        tokens = sentence.split()\n        training_vocab.update(tokens)\n    \n    # Vocabulary size including <UNK>\n    V_size = len(training_vocab) + 1\n    \n    # Step 2: Build n-gram frequency tables for n from 1 to N\n    ngram_counts = defaultdict(Counter)\n    for sentence in corpus:\n        tokens = sentence.split()\n        for n in range(1, N + 1):\n            for i in range(len(tokens) - n + 1):\n                ngram = tuple(tokens[i:i+n])\n                ngram_counts[n][ngram] += 1\n    \n    # Step 3: Process the target sequence, replacing OOV words with <UNK>\n    sequence_tokens = sequence.split()\n    processed_tokens = [word if word in training_vocab else '<UNK>' for word in sequence_tokens]\n    \n    # Check if the processed sequence is shorter than N\n    if len(processed_tokens) < N:\n        return 0.0\n    \n    total_log_prob = 0.0\n    # Step 4: Iterate over each N-gram in the processed sequence\n    for i in range(len(processed_tokens) - N + 1):\n        current_ngram = tuple(processed_tokens[i:i+N])\n        context = current_ngram[:-1]\n        \n        # Get the count of the current n-gram\n        count_ngram = ngram_counts[N].get(current_ngram, 0)\n        \n        # Get the count of the context (n-1 gram)\n        if N == 1:\n            # For unigrams, context is empty, use sum of all unigrams in corpus\n            count_context = sum(ngram_counts[1].values())\n        else:\n            count_context = ngram_counts[N-1].get(context, 0)\n        \n        # Compute the probability with additive smoothing\n        numerator = count_ngram + K\n        denominator = count_context + K * V_size\n        prob = numerator / denominator\n        \n        # Add the log probability to the total\n        total_log_prob += math.log(prob)\n    \n    # Round to 4 decimal places\n    return round(total_log_prob, 4)\n```\n\n### Explanation\n1. **Vocabulary Construction**: The code first extracts all unique words from the training corpus to build the vocabulary. This helps in identifying out-of-vocabulary (OOV) words in the target sentence, which are then replaced with the \"<UNK>\" token.\n\n2. **N-gram Frequency Tables**: The code constructs frequency tables for all n-grams (from 1 to N) using the training corpus. These tables are used to count occurrences of n-grams and their contexts.\n\n3. **Target Sentence Preprocessing**: The target sentence is processed to replace OOV words with \"<UNK>\", ensuring that all words are part of the known vocabulary.\n\n4. **Probability Calculation**: For each N-gram in the preprocessed target sentence, the code calculates the probability using additive smoothing. This involves adjusting counts with a smoothing constant \\( K \\) to handle unseen n-grams and contexts.\n\n5. **Log-Probability Summation**: The logarithm of each probability is computed and summed to get the final result, which is then rounded to four decimal places for the output. This approach ensures numerical stability and handles underflow issues common in probability multiplications."}
{"task_id": 97, "completion_id": 0, "completion": "To solve this problem, we need to compute the element-wise derivative of the ReLU activation function for various input types, including scalars, lists, nested lists, and NumPy ndarrays. The derivative of ReLU is 1 for positive inputs and 0 otherwise, and the output must preserve the original shape of the input.\n\n### Approach\n1. **Convert Input to NumPy Array**: Convert the input to a NumPy array to handle both scalar and non-scalar inputs uniformly. This conversion allows us to leverage NumPy's efficient array operations.\n2. **Compute Derivative**: Using NumPy operations, compute the derivative where each element is 1.0 if the input element is positive and 0.0 otherwise.\n3. **Determine Output Type**: Check the dimensionality of the converted array. If it is a scalar (0-dimensional), return the result as a float. Otherwise, convert the result back to a Python list using the `tolist()` method to preserve the original structure.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input data that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        Derivative of ReLU with the same shape layout as *Z* (float if *Z* is a scalar).\n    \"\"\"\n    arr = np.asarray(Z)\n    der = (arr > 0).astype(float)\n    if der.ndim == 0:\n        return der.item()\n    else:\n        return der.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input is converted to a NumPy array using `np.asarray()`, which handles scalars, lists, nested lists, and existing NumPy arrays seamlessly.\n2. **Derivative Calculation**: The expression `(arr > 0).astype(float)` creates a boolean array where each element is `True` (1.0) if the corresponding input element is positive and `False` (0.0) otherwise. This is then cast to a float array to meet the output requirements.\n3. **Output Type Handling**: By checking the dimensionality (`ndim`) of the resulting array, we determine if the original input was a scalar. If it was (0-dimensional array), we return a float using `item()`. For non-scalar inputs, the result is converted to a Python list with `tolist()`, preserving the original nested structure if present.\n\nThis approach efficiently handles all specified input types and ensures the output matches the input's structure while correctly computing the ReLU derivative."}
{"task_id": 99, "completion_id": 0, "completion": "To solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function, which applies an element-wise transformation defined as ReLU(x) = max(0, x). The solution must handle various input types (scalar, list, list-of-lists, or NumPy array) and return the result in the same container type as the input.\n\n### Approach\n1. **Input Type Handling**: The function first checks the type of the input to determine how to process it.\n2. **Scalar Handling**: If the input is a scalar (int or float), directly compute the ReLU and return the result as a scalar.\n3. **List Handling**: Convert lists or list-of-lists to a NumPy array, apply the ReLU function, then convert the result back to a list or list-of-lists.\n4. **NumPy Array Handling**: Directly apply the ReLU function using vectorized operations on the NumPy array and return the result as a NumPy array.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu_(Z):\n    \"\"\"Apply the element-wise Rectified Linear Unit (ReLU) activation.\n\n    The function keeps the container type of the input:\n    scalar \u2192 scalar, list \u2192 list, NumPy array \u2192 NumPy array.\n\n    Args:\n        Z (int | float | list | numpy.ndarray): Input data \u2013 scalar, 1-D or 2-D.\n\n    Returns:\n        Same type as *Z* with all negative values clipped to 0.\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        return max(0, Z)\n    elif isinstance(Z, list):\n        # Convert list to numpy array, apply ReLU, convert back to list\n        arr = np.array(Z)\n        return np.maximum(0, arr).tolist()\n    elif isinstance(Z, np.ndarray):\n        return np.maximum(0, Z)\n    else:\n        raise TypeError(\"Unsupported input type\")\n```\n\n### Explanation\n1. **Scalar Input**: If the input is an integer or float, the function directly computes the maximum of 0 and the input value, returning the result as a scalar.\n2. **List Input**: For list inputs (including list-of-lists), the function converts the list to a NumPy array to leverage vectorized operations. The ReLU function is applied using `np.maximum(0, arr)`, which efficiently computes the element-wise maximum. The result is then converted back to a list or list-of-lists using the `tolist()` method.\n3. **NumPy Array Input**: If the input is a NumPy array, the function uses `np.maximum(0, Z)` to apply the ReLU function directly, ensuring efficient computation without converting the data type.\n\nThis approach ensures that the output container type matches the input container type while efficiently applying the ReLU transformation using vectorized operations where possible."}
{"task_id": 100, "completion_id": 0, "completion": "To solve this problem, we need to compute the classification error rate, which is the proportion of incorrectly predicted samples between two sequences: the actual class labels and the predicted class labels. The solution should handle various input types efficiently using NumPy for vectorized operations.\n\n### Approach\n1. **Check Input Lengths**: First, verify if the lengths of the input sequences (actual and predicted) are the same. If not, return -1 immediately.\n2. **Convert to NumPy Arrays**: Convert the input sequences to NumPy arrays to leverage vectorized operations for efficient element-wise comparison.\n3. **Compute Mismatches**: Use NumPy operations to count the number of mismatches between the actual and predicted labels.\n4. **Handle Edge Cases**: If the input sequences are empty, return 0.0 as there are no samples to evaluate.\n5. **Calculate Error Rate**: Compute the error rate as the ratio of mismatches to the total number of samples, then round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef classification_error(actual, predicted):\n    \"\"\"Compute the proportion of incorrect predictions.\n\n    Parameters\n    ----------\n    actual : list | tuple | numpy.ndarray\n        Ground-truth labels.\n    predicted : list | tuple | numpy.ndarray\n        Predicted labels from a classifier.\n\n    Returns\n    -------\n    float | int\n        The classification error rate rounded to four decimals, or \u22121 when\n        the input lengths are not identical.\n    \"\"\"\n    # Check if the lengths of actual and predicted are different\n    if len(actual) != len(predicted):\n        return -1\n    \n    # Convert inputs to NumPy arrays\n    actual_array = np.array(actual)\n    predicted_array = np.array(predicted)\n    \n    # Calculate the number of mismatches\n    mismatches = np.sum(actual_array != predicted_array)\n    total = len(actual_array)\n    \n    # Handle case with zero samples to avoid division by zero\n    if total == 0:\n        return 0.0\n    \n    # Compute the error rate and round to four decimal places\n    error_rate = mismatches / total\n    return round(error_rate, 4)\n```\n\n### Explanation\n1. **Input Length Check**: The function first checks if the lengths of the actual and predicted labels are the same. If not, it returns -1 immediately.\n2. **Conversion to NumPy Arrays**: The inputs are converted to NumPy arrays to utilize efficient vectorized operations.\n3. **Mismatch Calculation**: Using NumPy's element-wise comparison, the number of mismatches is calculated efficiently.\n4. **Edge Case Handling**: If the input sequences are empty, the function returns 0.0 to handle the division by zero scenario gracefully.\n5. **Error Rate Calculation**: The error rate is computed by dividing the number of mismatches by the total number of samples, then rounded to four decimal places for precision.\n\nThis approach ensures that the solution is both efficient and robust, handling various input types and edge cases appropriately."}
{"task_id": 101, "completion_id": 0, "completion": "To solve this problem, we need to implement the softmax activation function using NumPy, which converts logits into probabilities. The implementation must be numerically stable and handle arrays with at least two dimensions, raising an error for 1D inputs. The result should be returned as a nested Python list with probabilities rounded to four decimal places.\n\n### Approach\n1. **Check Input Dimensions**: Verify that the input array has at least two dimensions. If it is 1D, raise a `ValueError`.\n2. **Numerically Stable Softmax Calculation**:\n   - **Compute Maximum Values**: Find the maximum value along the specified axis while keeping the dimensions to allow proper broadcasting.\n   - **Shift Values**: Subtract these maximum values from each element to avoid numerical instability.\n   - **Exponentiate Shifted Values**: Apply the exponential function to the shifted values.\n   - **Normalize**: Sum the exponentiated values along the specified axis (again keeping dimensions) and divide each element by this sum to get probabilities.\n3. **Rounding and Conversion**: Round the resulting probabilities to four decimal places and convert the NumPy array to a nested Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softmax(x: np.ndarray, axis: int = 1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    This function must reproduce the behaviour of Keras' backend version shown\n    in the prompt while working solely with NumPy.  The output should be a\n    Python list and every probability must be rounded to four decimal places.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Integer axis along which to apply the softmax.  Negative indices\n               follow NumPy\u2019s convention (e.g. ``axis=-1`` refers to the last\n               axis).\n\n    Returns:\n        Nested Python lists containing the softmax probabilities (rounded to\n        4 decimals).\n\n    Raises:\n        ValueError: If ``x`` is 1-D (``x.ndim == 1``).\n    \"\"\"\n    if x.ndim == 1:\n        raise ValueError(\"Cannot apply softmax to a tensor that is 1D\")\n    \n    max_x = np.max(x, axis=axis, keepdims=True)\n    shifted = x - max_x\n    exp_shifted = np.exp(shifted)\n    sum_exp = np.sum(exp_shifted, axis=axis, keepdims=True)\n    probabilities = exp_shifted / sum_exp\n    rounded = np.round(probabilities, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the input array is 1D and raises an error if true.\n- **Max Calculation**: Using `np.max` with `keepdims=True` ensures the result maintains the same dimensions, allowing correct broadcasting when subtracted from the original array.\n- **Shifting and Exponentiating**: Subtracting the max value and applying the exponential function ensures numerical stability by avoiding overflow from large values.\n- **Normalization**: The sum of exponentiated values along the specified axis is computed, and each value is divided by this sum to get probabilities.\n- **Rounding and Conversion**: Probabilities are rounded to four decimal places and converted to a nested Python list for the final output.\n\nThis approach efficiently computes the softmax function in a numerically stable manner and handles various input dimensions and axis specifications correctly."}
{"task_id": 102, "completion_id": 0, "completion": "To solve this problem, we need to implement a polynomial regression model that can predict values for given inputs based on training data. The model must handle various error conditions and use numerical methods to fit the polynomial and make predictions.\n\n### Approach\n1. **Error Checking**: Verify that the input data meets the necessary conditions for fitting a polynomial regression model. This includes checking the lengths of the input and target data, ensuring the polynomial degree is non-negative, and confirming there are enough data points to fit the polynomial.\n2. **Design Matrix Construction**: Create a matrix where each column represents a power of the input values up to the specified degree. This matrix is used to fit the polynomial regression model.\n3. **Model Fitting**: Use NumPy's least squares method to determine the optimal coefficients for the polynomial that minimizes the error between the predicted and actual values.\n4. **Prediction**: Use the fitted coefficients to predict values for new inputs by constructing a similar design matrix for the prediction inputs and applying the coefficients.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef polynomial_regression_predict(x: list[float], y: list[float], degree: int, x_pred: list[float]) -> list[float]:\n    \"\"\"Fit a polynomial regression model and return predictions.\n\n    Parameters\n    ----------\n    x : list[float]\n        Training input values.\n    y : list[float]\n        Training target values.\n    degree : int\n        Degree of the polynomial to be fitted (must be >= 0).\n    x_pred : list[float]\n        Values at which the fitted model should be evaluated.\n\n    Returns\n    -------\n    list[float]\n        Predicted values for *x_pred* rounded to 4 decimal places, or -1 if the\n        model cannot be fitted (e.g. not enough data, inconsistent input).\n    \"\"\"\n    # Check for error conditions\n    if len(x) != len(y):\n        return -1\n    if not x:  # Check if there are no training samples\n        return -1\n    if degree < 0:\n        return -1\n    if len(x) < degree + 1:\n        return -1\n    \n    # Convert lists to numpy arrays\n    x_np = np.array(x)\n    y_np = np.array(y)\n    \n    # Create the design matrix for the training data\n    X_train = np.column_stack([x_np**i for i in range(degree + 1)])\n    \n    # Solve for the coefficients using least squares\n    try:\n        coefficients, _, _, _ = np.linalg.lstsq(X_train, y_np, rcond=None)\n    except:\n        return -1\n    \n    # Create the design matrix for the prediction data\n    x_pred_np = np.array(x_pred)\n    X_pred = np.column_stack([x_pred_np**i for i in range(degree + 1)])\n    \n    # Compute the predictions and round to 4 decimal places\n    predictions = X_pred @ coefficients\n    rounded_predictions = np.round(predictions, 4).tolist()\n    \n    return rounded_predictions\n```\n\n### Explanation\n1. **Error Checking**: The function first checks if the lengths of the input and target data match, if there are any data points, if the degree is non-negative, and if there are enough data points to fit the polynomial. If any of these conditions are not met, the function returns -1.\n2. **Design Matrix Construction**: The design matrix for the training data is constructed by raising each input value to powers from 0 up to the specified degree. This matrix is used to fit the polynomial coefficients.\n3. **Model Fitting**: Using NumPy's `lstsq` function, the coefficients for the polynomial are calculated. This function solves the least squares problem to minimize the error between the predicted and actual values.\n4. **Prediction**: The same design matrix approach is used for the prediction inputs, and the fitted coefficients are applied to these values to generate predictions. The predictions are then rounded to four decimal places and returned as a list.\n\nThis approach ensures that the polynomial regression model is fitted correctly and efficiently, even with potential edge cases and numerical stability issues handled appropriately."}
{"task_id": 104, "completion_id": 0, "completion": "To solve this problem, we need to calculate the information gain obtained by splitting a dataset into child nodes based on a given set of splits. The information gain is a measure of how much a split reduces the entropy (uncertainty) in the dataset. \n\n### Approach\nThe approach to compute the information gain involves the following steps:\n\n1. **Compute the Entropy of the Parent Node (H(y))**: Entropy measures the impurity of the labels in the parent node. It is calculated using the formula \\( H(S) = -\\sum_{i=1}^{c} p_i \\log_2 p_i \\), where \\( p_i \\) is the proportion of samples belonging to class \\( i \\).\n\n2. **Compute the Weighted Entropy of Each Child Node**: For each child node produced by the split, compute its entropy and weight it by the proportion of samples in that child relative to the parent node. The weighted entropy of each child node is given by \\( \\frac{|S_k|}{|y|} \\cdot H(S_k) \\), where \\( |S_k| \\) is the number of samples in the k-th child node.\n\n3. **Sum the Weighted Entropies of All Child Nodes**: This gives the total entropy after the split.\n\n4. **Calculate the Information Gain**: Subtract the total weighted entropy of the child nodes from the entropy of the parent node. The result is the information gain, which is then rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef information_gain(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Computes the information gain of a proposed split.\"\"\"\n\n    def entropy(arr: np.ndarray) -> float:\n        if len(arr) == 0:\n            return 0.0\n        counts = np.unique(arr, return_counts=True)[1]\n        probabilities = counts / len(arr)\n        return -np.sum(probabilities * np.log2(probabilities))\n\n    parent_entropy = entropy(y)\n    total_samples = len(y)\n    if total_samples == 0:\n        return 0.0  # Avoid division by zero if parent is empty\n\n    weighted_child_entropy = 0.0\n    for child in splits:\n        child_length = len(child)\n        if child_length == 0:\n            continue\n        child_weight = child_length / total_samples\n        weighted_child_entropy += child_weight * entropy(child)\n\n    information_gain_value = parent_entropy - weighted_child_entropy\n    return round(information_gain_value, 4)\n```\n\n### Explanation\n1. **Entropy Calculation**: The `entropy` function computes the entropy of a given array of labels. It uses `np.unique` to get the counts of each unique label, calculates their probabilities, and then applies the entropy formula.\n\n2. **Parent Entropy**: The entropy of the parent node is computed once using the `entropy` function.\n\n3. **Weighted Child Entropy**: For each child node in the splits, the code checks if the child node is empty. If not, it computes the entropy of the child node, weights it by the proportion of samples in that child relative to the parent, and accumulates the total weighted entropy.\n\n4. **Information Gain Calculation**: The information gain is obtained by subtracting the total weighted entropy of the child nodes from the parent entropy. The result is rounded to 4 decimal places for the final output.\n\nThis approach efficiently handles any number of classes and child nodes, ensuring accurate computation of information gain as per the problem requirements."}
{"task_id": 105, "completion_id": 0, "completion": "To solve this problem, we need to implement the Gaussian (Normal) probability density function (PDF) that evaluates the PDF for a given set of input values. The function must handle various input types and validate the standard deviation parameter to ensure it is strictly positive.\n\n### Approach\n1. **Validate Standard Deviation**: Check if the provided standard deviation (\u03c3) is positive. If not, return -1.\n2. **Handle Input Types**: Convert the input values into a NumPy array to uniformly process lists, tuples, arrays, and scalar values.\n3. **Flatten Input**: Convert the input into a flat array to handle multi-dimensional arrays and scalars uniformly.\n4. **Check Empty Input**: If the input array is empty, return an empty list.\n5. **Compute PDF**: Use the Gaussian PDF formula to compute the values for each element in the input array.\n6. **Round Results**: Round the computed values to five decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_pdf(x, mean, sigma):\n    \"\"\"Compute the Gaussian probability density for each element in *x*.\n\n    The function returns a list of floats rounded to 5 decimal places. If *sigma*\n    is not strictly positive the function must return -1.\n\n    Args:\n        x (list | tuple | np.ndarray | float | int): Values at which to evaluate the PDF.\n        mean (float | int): Distribution mean (\u03bc).\n        sigma (float | int): Distribution standard deviation (\u03c3).\n\n    Returns:\n        list[float] | int: The PDF values or -1 when \u03c3 \u2264 0.\n    \"\"\"\n    if sigma <= 0:\n        return -1\n    x_arr = np.asarray(x)\n    x_flat = x_arr.ravel()\n    if x_flat.size == 0:\n        return []\n    coefficient = 1.0 / (np.sqrt(2 * np.pi) * sigma)\n    exponent = -((x_flat - mean) ** 2) / (2 * (sigma ** 2))\n    pdf_values = coefficient * np.exp(exponent)\n    rounded = np.round(pdf_values, 5)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Validation**: The function first checks if the standard deviation (\u03c3) is positive. If \u03c3 is zero or negative, the function returns -1 immediately.\n2. **Input Handling**: The input values are converted into a NumPy array to handle different input types uniformly. The array is then flattened to ensure all elements are processed, regardless of their original shape.\n3. **Empty Input Check**: If the flattened array is empty (i.e., the input was an empty collection), the function returns an empty list.\n4. **PDF Calculation**: The Gaussian PDF formula is applied element-wise using vectorized operations for efficiency. The formula involves computing a coefficient and an exponent term, then combining them using exponential and multiplication operations.\n5. **Rounding and Conversion**: The results are rounded to five decimal places using NumPy's rounding function and converted to a Python list of floats for the final output.\n\nThis approach ensures that the function handles various input types efficiently and correctly, providing accurate and formatted results as required."}
{"task_id": 107, "completion_id": 0, "completion": "To solve this problem, we need to implement a single optimization step of the Adam algorithm. Adam is an adaptive moment estimation algorithm that uses exponential moving averages of the gradients and squared gradients to adjust the learning rate for each parameter. This approach helps in efficiently navigating the optimization landscape, especially in the presence of sparse gradients.\n\n### Approach\n1. **Update Moments**: Compute the first and second moment estimates using exponential moving averages. The first moment (mean) and the second moment (uncentered variance) are updated using the given gradients and decay rates (beta1 and beta2).\n2. **Bias Correction**: Adjust the moments to account for their initialization bias. This is done by dividing the moments by a factor that scales with the number of iterations.\n3. **Parameter Update**: Use the bias-corrected moments to update the parameters. The learning rate is adjusted by the ratio of the bias-corrected first moment to the square root of the bias-corrected second moment, with a small epsilon added for numerical stability.\n4. **Round and Convert**: Round the updated parameters and moments to 6 decimal places and convert them from numpy arrays to Python lists for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_step(theta: np.ndarray,\n              grad: np.ndarray,\n              m_prev: np.ndarray,\n              v_prev: np.ndarray,\n              t: int,\n              alpha: float = 0.01,\n              beta1: float = 0.9,\n              beta2: float = 0.99,\n              epsilon: float = 1e-9) -> tuple[list, list, list, int]:\n    \"\"\"Perform one iteration of the Adam optimisation algorithm.\"\"\"\n    # Compute the new first and second moments\n    m_t = beta1 * m_prev + (1 - beta1) * grad\n    v_t = beta2 * v_prev + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected moments\n    m_hat = m_t / (1 - beta1 ** t)\n    v_hat = v_t / (1 - beta2 ** t)\n    \n    # Update parameters\n    update = alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n    theta_new = theta - update\n    \n    # Round and convert to Python lists\n    theta_new_list = np.around(theta_new, 6).tolist()\n    m_t_list = np.around(m_t, 6).tolist()\n    v_t_list = np.around(v_t, 6).tolist()\n    \n    return (theta_new_list, m_t_list, v_t_list, t + 1)\n```\n\n### Explanation\n1. **Moment Updates**: The first moment `m_t` and second moment `v_t` are updated using the previous moments, current gradient, and decay rates. This smooths the gradient values over time.\n2. **Bias Correction**: Since the initial moments (especially at early time steps) are biased towards zero, we correct this by scaling them using the cumulative product of the decay rates up to the current time step.\n3. **Parameter Update**: The parameters are adjusted by the learning rate scaled by the bias-corrected first moment divided by the square root of the bias-corrected second moment. This adaptively adjusts the learning rate for each parameter.\n4. **Numerical Stability**: A small epsilon value is added to the denominator to prevent division by zero.\n5. **Rounding and Conversion**: The results are rounded to 6 decimal places and converted to lists to meet the output requirements.\n\nThis approach ensures that the optimization step is adaptive, efficient, and numerically stable, making it suitable for a wide range of optimization problems."}
{"task_id": 108, "completion_id": 0, "completion": "To solve this problem, we need to factorize a real-valued matrix using the regularized Alternating Least Squares (ALS) algorithm. The goal is to find two low-rank matrices such that the regularized Frobenius norm of the reconstruction error is minimized. The algorithm alternates between updating these matrices while keeping one fixed at each step until the loss converges or a maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Initialize the factor matrices \\( W \\) and \\( H \\) with a fixed random seed to ensure reproducibility.\n2. **Alternating Updates**: Update \\( W \\) and \\( H \\) alternately using closed-form solutions derived from regularized least squares. Specifically, update \\( W \\) while keeping \\( H \\) fixed and vice versa.\n3. **Loss Calculation**: After each update, compute the combined loss (reconstruction error plus regularization terms). Track the best (lowest) loss encountered during the iterations.\n4. **Convergence Check**: Stop iterating if the loss drops below the specified tolerance or the maximum number of iterations is reached.\n5. **Return Result**: Return the best reconstruction matrix rounded to four decimal places and converted to a standard Python list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\"\"\"\n    np.random.seed(0)\n    N, M = X.shape\n    W = np.random.randn(N, K)\n    H = np.random.randn(K, M)\n    \n    best_loss = float('inf')\n    best_X_hat = None\n    \n    for _ in range(max_iter):\n        # Update W\n        H_HT = H @ H.T\n        reg_W = H_HT + alpha * np.eye(K)\n        inv_reg_W = np.linalg.inv(reg_W)\n        W = (X @ H.T) @ inv_reg_W\n        \n        # Update H\n        WT_W = W.T @ W\n        reg_H = WT_W + alpha * np.eye(K)\n        inv_reg_H = np.linalg.inv(reg_H)\n        H = inv_reg_H @ (W.T @ X)\n        \n        # Compute current reconstruction and loss\n        X_hat = W @ H\n        frobenius_loss = np.linalg.norm(X - X_hat, 'fro') ** 2\n        reg_term = alpha * (np.linalg.norm(W, 'fro')**2 + np.linalg.norm(H, 'fro')**2)\n        current_loss = frobenius_loss + reg_term\n        \n        # Update best X_hat if current loss is lower\n        if current_loss < best_loss:\n            best_loss = current_loss\n            best_X_hat = X_hat.copy()\n        \n        # Check for convergence\n        if best_loss <= tol:\n            break\n    \n    return best_X_hat.round(4).tolist()\n```\n\n### Explanation\n1. **Initialization**: The matrices \\( W \\) and \\( H \\) are initialized using a fixed random seed to ensure reproducibility. This ensures that the same initial matrices are generated every time the function is run.\n2. **Matrix Updates**: In each iteration, \\( W \\) is updated by solving a regularized least squares problem with \\( H \\) fixed, and then \\( H \\) is updated similarly with \\( W \\) fixed. The updates use matrix inverses to solve the closed-form solutions efficiently.\n3. **Loss Tracking**: After each iteration, the loss is calculated as the sum of the reconstruction error and regularization terms. The best reconstruction matrix is tracked to ensure that the solution returned is the one with the lowest loss encountered, even if subsequent iterations might increase the loss due to numerical instability.\n4. **Convergence Check**: The loop stops early if the loss falls below the specified tolerance, ensuring efficient computation. If the maximum number of iterations is reached without convergence, the best solution found so far is returned.\n5. **Result Formatting**: The final reconstructed matrix is rounded to four decimal places and converted to a Python list of lists for compatibility with standard Python data structures.\n\nThis approach efficiently balances computational complexity and accuracy, leveraging the structure of the ALS algorithm to iteratively improve the solution while ensuring regularization to prevent overfitting."}
{"task_id": 109, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm with K-Means++ initialization. The algorithm must be reproducible by setting both Python's `random` and NumPy's random generator with a provided `random_state` value. The solution involves initializing centroids using the K-Means++ method, performing Lloyd's iterations until convergence or maximum iterations are reached, and returning the sorted centroids rounded to four decimal places.\n\n### Approach\n\n1. **Set Random Seeds**: To ensure reproducibility, both Python's `random` and NumPy's random generator are seeded with the provided `random_state`.\n2. **K-Means++ Initialization**: \n   - The first centroid is chosen uniformly at random from the data points.\n   - Subsequent centroids are chosen based on the squared distances from the nearest existing centroid, ensuring a more optimal initial placement.\n3. **Lloyd's Iterations**:\n   - **Assignment Step**: Assign each data point to the nearest centroid.\n   - **Update Step**: Compute new centroids as the mean of the assigned points. If a cluster becomes empty, its centroid remains unchanged.\n   - **Stopping Condition**: The loop stops when cluster assignments no longer change or the maximum number of iterations is reached.\n4. **Post-Processing**: The final centroids are rounded to four decimal places and sorted by their first coordinate, using the entire centroid tuple as a secondary key for tie-breaking.\n\n### Solution Code\n\n```python\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\"\"\"\n    \n    # Set the seeds for reproducibility\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n    \n    m, n = X.shape\n    \n    # K-Means++ initialisation\n    centroids = []\n    indices = np.arange(m)\n    \n    # Select the first centroid randomly\n    first_idx = np.random.choice(indices)\n    centroids.append(X[first_idx].copy())\n    \n    for _ in range(1, K):\n        current_centroids = np.array(centroids)\n        # Compute squared distances from each point to the nearest centroid\n        dist_sq = np.sum((X[:, np.newaxis, :] - current_centroids) ** 2, axis=2)\n        min_dist_sq = np.min(dist_sq, axis=1)\n        sum_D = min_dist_sq.sum()\n        \n        if sum_D == 0:\n            probabilities = np.ones(m) / m\n        else:\n            probabilities = min_dist_sq / sum_D\n        \n        # Select the next centroid\n        next_idx = np.random.choice(indices, p=probabilities)\n        centroids.append(X[next_idx].copy())\n    \n    centroids = np.array(centroids)\n    \n    # Lloyd's algorithm iterations\n    prev_labels = None\n    for _ in range(max_iter):\n        # Assign each point to the nearest centroid\n        dist_sq = ((X[:, np.newaxis, :] - centroids) ** 2).sum(axis=2)\n        labels = np.argmin(dist_sq, axis=1)\n        \n        # Check for convergence\n        if prev_labels is not None and np.array_equal(labels, prev_labels):\n            break\n        \n        # Update centroids\n        new_centroids = []\n        for i in range(K):\n            mask = (labels == i)\n            if np.sum(mask) == 0:\n                # Keep the centroid unchanged if cluster is empty\n                new_centroid = centroids[i].copy()\n            else:\n                new_centroid = X[mask].mean(axis=0)\n            new_centroids.append(new_centroid)\n        \n        centroids = np.array(new_centroids)\n        prev_labels = labels.copy()\n    \n    # Round and sort the centroids\n    centroids_rounded = np.round(centroids, decimals=4)\n    centroids_list = centroids_rounded.tolist()\n    centroids_list.sort(key=lambda x: (x[0], x))\n    \n    return centroids_list\n```\n\n### Explanation\n\n1. **Random Seed Initialization**: Ensures that both Python's `random` and NumPy's random number generator produce reproducible results.\n2. **K-Means++ Initialization**: This method selects initial centroids in a way that spreads them out, leading to faster convergence and better results compared to random initialization.\n3. **Lloyd's Algorithm**: The core of the K-Means algorithm, iteratively improving centroid positions by alternating between assigning points to the nearest centroid and updating centroids based on assigned points.\n4. **Handling Empty Clusters**: If a cluster becomes empty during iterations, its centroid is kept unchanged to maintain stability.\n5. **Sorting and Rounding**: The final centroids are rounded to four decimal places and sorted to ensure a consistent output format.\n\nThis approach efficiently balances computational performance with the need for accurate and reproducible clustering results."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Encodes a string into a fixed-length list of integer token ids.\"\"\"\n    # Truncate the text to the first 'time_steps' characters if necessary\n    truncated_text = text[:time_steps]\n    # Convert each character to its corresponding token id, using 0 for unknown characters\n    encoded = [vocabulary.get(c, 0) for c in truncated_text]\n    # Pad the encoded list with zeros to reach the desired length\n    pad_length = time_steps - len(encoded)\n    encoded += [0] * pad_length\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer token ids back to a string, ignoring padding tokens (0).\"\"\"\n    # Filter out padding tokens (0) and map the rest to characters\n    return ''.join([inverse_vocab[idx] for idx in indices if idx != 0])\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encodes the input text, runs model prediction, and decodes the output.\"\"\"\n    # Encode the input text into a fixed-length integer sequence\n    encoded_sequence = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # Convert to a batch of size 1 for the model\n    input_batch = np.array([encoded_sequence])\n    # Obtain the model's prediction (assuming output is a probability distribution over classes)\n    prediction = model.predict(input_batch)\n    # Convert predictions to token ids by selecting the most probable class at each time step\n    predicted_ids = np.argmax(prediction, axis=-1).squeeze()\n    # Decode the predicted token ids into a string, omitting padding tokens\n    return int_to_string(predicted_ids, inv_output_vocabulary)\n```"}
{"task_id": 112, "completion_id": 0, "completion": "To solve this problem, we need to convert a raw text string into a list of Token objects, each representing a word's frequency in the text. The solution must handle case insensitivity, extract valid words, and sort the tokens by their frequency and lexicographical order.\n\n### Approach\n1. **Extract Words**: Use a regular expression to find all contiguous sequences of alphabetic characters (A-Z, a-z) in the input text. Convert each extracted word to lowercase to ensure case insensitivity.\n2. **Count Frequencies**: Utilize the `Counter` class from the `collections` module to count the occurrences of each word.\n3. **Create Tokens**: For each distinct word, create a Token object, setting its count to the word's frequency.\n4. **Sort Tokens**: Sort the tokens primarily by descending frequency and secondarily by lexicographical order for words with the same frequency.\n5. **Handle Edge Cases**: Return an empty list if the input text contains no valid words.\n\n### Solution Code\n```python\nimport re\nfrom collections import Counter\n\nclass Token:\n    def __init__(self, word):\n        self.count = 0\n        self.word = word\n\n    def __repr__(self):\n        return \"Token(word='{}', count={})\".format(self.word, self.count)\n\ndef tokenize_and_count(text: str) -> list[Token]:\n    words = [word.lower() for word in re.findall(r'[A-Za-z]+', text)]\n    if not words:\n        return []\n    word_counts = Counter(words)\n    tokens = []\n    for word, count in word_counts.items():\n        token = Token(word)\n        token.count = count\n        tokens.append(token)\n    tokens.sort(key=lambda x: (-x.count, x.word))\n    return tokens\n```\n\n### Explanation\n1. **Extract Words**: The regular expression `[A-Za-z]+` matches sequences of alphabetic characters. Each matched word is converted to lowercase to ensure case insensitivity.\n2. **Count Frequencies**: Using `Counter`, we efficiently count how many times each lowercase word appears in the text.\n3. **Create Tokens**: For each unique word, a Token object is created. The Token's count is set to the word's frequency from the Counter.\n4. **Sort Tokens**: The list of tokens is sorted using a lambda function as the key. The primary key `-x.count` sorts by descending frequency, and the secondary key `x.word` sorts alphabetically for words with the same frequency.\n5. **Edge Cases**: If no valid words are found, the function returns an empty list immediately, ensuring correct handling of edge cases like empty input or non-alphabetic text."}
{"task_id": 113, "completion_id": 0, "completion": "To solve this problem, we need to create a helper function that automates running a sequence-to-sequence model on multiple input examples and collects the predictions. The function should handle cases where the examples are not provided by using a predefined global constant. The predictions should be printed in a readable format and returned in a list.\n\n### Approach\n1. **Handle Optional Examples**: The function should check if the `examples` argument is provided. If not, it should fall back to using a global constant `EXAMPLES`.\n2. **Process Each Example**: For each input example, call another helper function `run_example` to get the model's prediction as a list of characters.\n3. **Format and Print Output**: Convert the list of characters into a string, print both the input and the predicted output in a user-friendly format.\n4. **Collect Predictions**: Collect all predicted strings in a list and return them in the same order as the input examples.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str]\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        output_str = ''.join(output_chars)\n        print(f'input:  {example}')\n        print(f'output: {output_str}')\n        predictions.append(output_str)\n    return predictions\n```\n\n### Explanation\n- **Handling Optional Examples**: The function checks if the `examples` parameter is `None`. If it is, the global `EXAMPLES` constant is used. This allows the function to work even when the user does not explicitly provide examples.\n- **Processing Each Example**: For each input string in the examples, the helper function `run_example` is called. This helper function is assumed to return the model's prediction as a list of characters.\n- **Formatting and Printing**: The list of characters from `run_example` is joined into a single string. The input and predicted output are printed in a structured format for easy readability.\n- **Collecting Predictions**: Each predicted string is collected into a list, which is returned at the end. This ensures that the predictions are in the same order as the input examples, maintaining consistency.\n\nThis approach efficiently automates the process of running a model on multiple inputs, providing clear output, and collecting results, making it a valuable tool for quick model evaluation."}
{"task_id": 114, "completion_id": 0, "completion": "To solve this problem, we need to implement the Scaled Exponential Linear Unit (SELU) activation function and its derivative. The function should work in two modes: activation mode and derivative mode. The output must be a nested list of Python floats rounded to six decimal places, preserving the input shape.\n\n### Approach\n1. **Mathematical Definitions**:\n   - **Activation Mode**: For an input element \\( x \\), the SELU activation is defined as:\n     \\[\n     \\operatorname{selu}(x) = \\lambda \\begin{cases} \n     x & \\text{if } x \\geq 0 \\\\\n     \\alpha (e^x - 1) & \\text{if } x < 0 \n     \\end{cases}\n     \\]\n   - **Derivative Mode**: The derivative of SELU with respect to \\( x \\) is:\n     \\[\n     \\operatorname{selu}'(x) = \\lambda \\begin{cases} \n     1 & \\text{if } x \\geq 0 \\\\\n     \\alpha e^x & \\text{if } x < 0 \n     \\end{cases}\n     \\]\n   where \\( \\alpha \\approx 1.6732632423543772848170429916717 \\) and \\( \\lambda \\approx 1.0507009873554804934193349852946 \\).\n\n2. **Implementation Steps**:\n   - **Compute Activation or Derivative**: Using NumPy's `where` function to apply the activation or derivative element-wise based on the input values.\n   - **Rounding**: Round the results to six decimal places using NumPy's `around` function.\n   - **Conversion to List**: Convert the resulting NumPy array to a nested Python list using the `tolist` method to preserve the shape and structure.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef selu(x: np.ndarray, derivative: bool = False) -> list:\n    \"\"\"Scaled Exponential Linear Unit (SELU).\n\n    Applies SELU activation or its derivative element-wise to *x*.\n\n    Args:\n        x: A NumPy ndarray containing any real values.\n        derivative: If ``False`` (default) the function returns the SELU\n            activation values. If ``True`` the function returns the analytical\n            gradient of SELU with respect to *x*.\n\n    Returns:\n        A Python nested list with the same shape as *x* containing the SELU\n        activation (or derivative) values rounded to **six** decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772848170429916717\n    lambda_val = 1.0507009873554804934193349852946\n\n    if derivative:\n        grad = np.where(x >= 0, 1, alpha * np.exp(x)) * lambda_val\n    else:\n        activated = np.where(x >= 0, x, alpha * (np.exp(x) - 1)) * lambda_val\n        grad = activated\n\n    rounded = np.around(grad, decimals=6)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Constants**: The predefined constants \\( \\alpha \\) and \\( \\lambda \\) are used to scale the activation and derivative values.\n2. **Element-wise Operations**: Using NumPy's `where` function allows efficient element-wise computation of either the activation or derivative based on the condition \\( x \\geq 0 \\).\n3. **Rounding**: The results are rounded to six decimal places to meet the problem's precision requirements.\n4. **Conversion**: The final NumPy array is converted to a nested Python list using `tolist()` to ensure compatibility with non-NumPy environments and preserve the input shape.\n\nThis approach ensures that the function is both efficient and accurate, leveraging NumPy's vectorized operations for performance while adhering to the specified output format."}
{"task_id": 115, "completion_id": 0, "completion": "To solve this problem, we need to compute the average binary cross-entropy loss and its gradient for logistic regression given a feature matrix, a binary target vector, and a weight vector. The solution involves calculating the predicted probabilities using the logistic function, clipping these probabilities to avoid numerical instability, and then using these values to compute the loss and gradient as specified.\n\n### Approach\n1. **Compute Logits and Probabilities**: First, compute the logits (linear combination of features and weights) and apply the sigmoid function to get the predicted probabilities.\n2. **Clip Probabilities**: To avoid numerical issues when taking the logarithm of probabilities that are exactly 0 or 1, clip these probabilities to a small interval around [0, 1].\n3. **Compute Loss**: Use the clipped probabilities to compute the binary cross-entropy loss.\n4. **Compute Gradient**: Use the original (unclipped) probabilities to compute the gradient of the loss with respect to the weights, following the given formula.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\"\"\"\n    m = X.shape[0]\n    y = y.ravel()\n    w = w.ravel()\n    \n    z = X.dot(w)\n    p = 1 / (1 + np.exp(-z))\n    \n    # Clip probabilities to avoid numerical instability in log\n    epsilon = 1e-20\n    p_clipped = np.clip(p, epsilon, 1 - epsilon)\n    \n    # Compute loss using clipped probabilities\n    loss_terms = y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped)\n    loss = -np.sum(loss_terms) / m\n    loss_rounded = np.round(loss, 4).item()\n    \n    # Compute gradient using original probabilities\n    diff = p - y\n    gradient = X.T.dot(diff) / m\n    gradient_rounded = np.round(gradient, 4)\n    \n    # Convert gradient to a nested list of lists\n    gradient_list = gradient_rounded.reshape(-1, 1).tolist()\n    \n    return (loss_rounded, gradient_list)\n```\n\n### Explanation\n1. **Logits and Probabilities**: The logits (z) are computed as the dot product of the feature matrix (X) and the weight vector (w). The probabilities (p) are obtained by applying the sigmoid function to these logits.\n2. **Clipping Probabilities**: To prevent numerical instability, we clip the probabilities to a small interval [\u03b5, 1-\u03b5], where \u03b5 is 1e-20. This ensures that the logarithm operations in the loss calculation do not encounter invalid values.\n3. **Loss Calculation**: The loss is computed using the clipped probabilities. This involves calculating the cross-entropy loss for each sample and averaging it over all samples.\n4. **Gradient Calculation**: The gradient is derived using the original (unclipped) probabilities. The formula for the gradient involves the transpose of the feature matrix multiplied by the difference between the predicted probabilities and the actual target values, scaled by the number of samples.\n\nThis approach ensures numerical stability while maintaining the correct computation of both the loss and gradient as per the problem specifications. The results are then rounded to four decimal places and formatted into the required output format."}
{"task_id": 116, "completion_id": 0, "completion": "To solve this problem, we need to calculate the reduction in mean-squared error (MSE) achieved by splitting a target vector into subsets in a regression decision tree. The goal is to evaluate how effective a split is by comparing the MSE of the parent node with the weighted average MSE of the child nodes.\n\n### Approach\n1. **Calculate Parent MSE**: The parent MSE is computed as the variance of the target vector. This is done by taking the mean of the squared differences between each element and the mean of the parent vector.\n2. **Calculate Child MSE**: For each subset resulting from the split, compute its MSE (variance) and then take the weighted average of these MSE values based on the size of each subset relative to the parent.\n3. **Compute MSE Reduction**: Subtract the weighted child MSE from the parent MSE to get the reduction in MSE. This value is then rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mse_criterion(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Calculate the reduction in mean-squared error achieved by a split.\n\n    Args:\n        y: A 1-D NumPy array containing the original target values.\n        splits: A list where each element is a 1-D NumPy array holding the\n            target values that fall into one child node after the split.\n\n    Returns:\n        A float \u2013 the parent MSE minus the weighted child MSE \u2013 rounded to\n        four decimal places.  A higher value indicates a better split.\n    \"\"\"\n    # Compute parent MSE\n    parent_mean = np.mean(y)\n    parent_mse = np.mean((y - parent_mean) ** 2)\n    \n    n = y.size\n    \n    # Compute weighted child MSE\n    mse_children = 0.0\n    for split in splits:\n        split_size = split.size\n        if split_size == 0:\n            continue\n        split_mean = np.mean(split)\n        split_mse = np.mean((split - split_mean) ** 2)\n        mse_children += (split_size / n) * split_mse\n    \n    # Calculate the MSE reduction and round to four decimal places\n    delta_mse = parent_mse - mse_children\n    return round(delta_mse, 4)\n```\n\n### Explanation\n1. **Parent MSE Calculation**: The parent node's MSE is calculated as the mean of the squared differences between each element and the mean of the parent vector. This gives the variance of the parent data.\n2. **Child MSE Calculation**: For each subset created by the split, compute its MSE similarly. Each subset's MSE is then weighted by its size relative to the parent node to ensure the contribution of each subset is proportional to its size.\n3. **MSE Reduction**: The reduction in MSE is the difference between the parent MSE and the weighted average of the child MSEs. A higher reduction indicates a better split, as it means the split has significantly reduced the variance in the target values.\n\nThis approach efficiently computes the effectiveness of a split by leveraging variance calculations and ensures the result is precise and rounded appropriately for comparison."}
{"task_id": 118, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. AdaBoost is an ensemble method that combines multiple weak classifiers to create a strong classifier. Each weak classifier (decision stump in this case) is trained iteratively, focusing more on the misclassified samples from the previous iteration. The final prediction is a weighted sum of all weak classifiers' predictions.\n\n### Approach\n1. **Initialize Weights**: Start with equal weights for all training samples.\n2. **Iterative Training**: For each iteration, find the best decision stump that minimizes the weighted classification error.\n3. **Compute Stump Weight**: Use the weighted error to compute the weight (alpha) for the current stump.\n4. **Update Weights**: Adjust the weights of the training samples, increasing the weights of misclassified samples and decreasing those of correctly classified samples.\n5. **Aggregate Predictions**: Combine the predictions of all stumps using their respective weights to make the final prediction on the test data.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\"\"\"\n    m, n = X_train.shape\n    n_clf = max(n_clf, 1)\n    D = np.ones(m) / m  # Initial weights\n    stumps = []\n\n    for _ in range(n_clf):\n        best_error = np.inf\n        best_j = -1\n        best_threshold = 0.0\n        best_sign = 1\n\n        # Iterate over each feature to find the best split\n        for j in range(n):\n            sorted_indices = np.argsort(X_train[:, j])\n            X_sorted_j = X_train[sorted_indices, j]\n            y_sorted = y_train[sorted_indices]\n            weights_sorted = D[sorted_indices]\n\n            # Precompute cumulative sums for positive and negative classes\n            pos_mask = (y_sorted == 1)\n            neg_mask = (y_sorted == -1)\n            cum_pos = np.cumsum(pos_mask * weights_sorted)\n            cum_neg = np.cumsum(neg_mask * weights_sorted)\n\n            total_pos = cum_pos[-1] if len(cum_pos) > 0 else 0\n            total_neg = cum_neg[-1] if len(cum_neg) > 0 else 0\n\n            # Check all possible split points\n            for k in range(len(X_sorted_j) - 1):\n                if X_sorted_j[k] == X_sorted_j[k + 1]:\n                    continue  # Skip if values are the same\n\n                sum_pos_left = cum_pos[k]\n                sum_neg_left = cum_neg[k]\n                sum_pos_right = total_pos - sum_pos_left\n                sum_neg_right = total_neg - sum_neg_left\n\n                error1 = sum_neg_left + sum_pos_right\n                error2 = sum_pos_left + sum_neg_right\n\n                current_error = min(error1, error2)\n                if current_error < best_error:\n                    best_error = current_error\n                    best_j = j\n                    best_threshold = (X_sorted_j[k] + X_sorted_j[k + 1]) / 2\n                    best_sign = 1 if error1 < error2 else -1\n\n        # Compute alpha for the best stump\n        eps = 1e-15\n        error_t = max(eps, min(best_error, 1 - eps))\n        alpha = 0.5 * np.log((1 - error_t) / error_t)\n        stumps.append((alpha, best_j, best_threshold, best_sign))\n\n        # Update weights\n        h_train = np.where(X_train[:, best_j] <= best_threshold, best_sign, -best_sign)\n        D *= np.exp(-alpha * y_train * h_train)\n        D /= D.sum()\n\n    # Predict on X_test\n    predictions = []\n    for x in X_test:\n        sum_alpha_h = 0.0\n        for alpha, j, threshold, sign in stumps:\n            if x[j] <= threshold:\n                h = sign\n            else:\n                h = -sign\n            sum_alpha_h += alpha * h\n        predictions.append(1 if sum_alpha_h >= 0 else -1)\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: Each sample is initially given equal weight.\n2. **Finding the Best Stump**: For each feature, we sort the samples and evaluate potential thresholds. The best threshold and direction (sign) are chosen based on the weighted error.\n3. **Weight Update**: After selecting the best stump, the weights of misclassified samples are increased, and those of correctly classified samples are decreased, ensuring the next stump focuses more on the errors.\n4. **Prediction**: The final prediction is a weighted vote of all stumps, where each stump's influence is determined by its alpha value. The sign of the weighted sum determines the class label.\n\nThis approach efficiently combines weak learners to form a strong classifier, leveraging iterative re-weighting to focus on difficult samples, thereby improving overall model performance."}
{"task_id": 119, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs one step of Stochastic Gradient Descent (SGD) to update model parameters. The function should handle edge cases such as empty parameter or gradient lists, and ensure that the input arrays are not modified in-place. The updated parameters must be rounded to four decimal places and converted to Python lists before returning.\n\n### Approach\n1. **Edge Case Handling**: Check if either the parameters (`params`) or gradients (`grads`) list is empty, or if their lengths differ. If any of these conditions are met, return an empty list.\n2. **Parameter Update**: For each parameter and corresponding gradient, compute the updated parameter using the SGD rule: \\(\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L\\). This involves element-wise subtraction of the product of the learning rate (alpha) and the gradient from the current parameter.\n3. **Rounding and Conversion**: After updating each parameter, round the values to four decimal places using NumPy's `round` function and convert the resulting NumPy array to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sgd_update(params: list, grads: list, alpha: float = 0.01) -> list:\n    \"\"\"One step of Stochastic Gradient Descent.\n\n    The function receives a list of parameters and a list of corresponding\n    gradients and must return the updated parameters after applying the SGD\n    rule using the supplied learning-rate `alpha`.\n\n    Parameters\n    ----------\n    params : list[np.ndarray]\n        Current model parameters.\n    grads : list[np.ndarray]\n        Gradients for each parameter.\n    alpha : float, optional\n        Learning-rate, by default 0.01.\n\n    Returns\n    -------\n    list\n        Updated parameters converted to Python lists, each rounded to four\n        decimal places.\n    \"\"\"\n    if not params or not grads or len(params) != len(grads):\n        return []\n    \n    updated_params = []\n    for param, grad in zip(params, grads):\n        new_param = param - alpha * grad\n        rounded_param = np.round(new_param, 4)\n        updated_params.append(rounded_param.tolist())\n    \n    return updated_params\n```\n\n### Explanation\n- **Edge Case Handling**: The function first checks if the input lists are valid. If either `params` or `grads` is empty, or if their lengths do not match, it returns an empty list immediately.\n- **Parameter Update**: Using a loop, each parameter is updated by subtracting the product of the learning rate (alpha) and its corresponding gradient. This operation is performed element-wise on the NumPy arrays.\n- **Rounding and Conversion**: Each updated parameter is rounded to four decimal places using `np.round` and then converted to a Python list using the `tolist()` method. These converted lists are collected into the result list, which is returned as the final output.\n\nThis approach ensures that the original parameter arrays are not modified in-place, adheres to the required precision, and handles all specified edge cases appropriately."}
{"task_id": 120, "completion_id": 0, "completion": "To solve this problem, we need to implement a linear regression model using batch gradient descent to minimize the mean-squared error (MSE). The model will learn the optimal weights for the given training data through iterative updates.\n\n### Approach\n1. **Input Validation**: Check if the number of samples in the input features (X) and target values (y) are the same. If not, return -1.\n2. **Data Preparation**: Convert the input lists to NumPy arrays for efficient matrix operations. Augment the feature matrix X with a column of ones to account for the bias term.\n3. **Weight Initialization**: Initialize the weight vector with values uniformly sampled from the interval \\([-1/\\sqrt{N}, 1/\\sqrt{N}]\\), where N is the number of features including the bias term.\n4. **Gradient Descent**: Perform batch gradient descent for the specified number of iterations. In each iteration, compute the predictions, calculate the error, and update the weights using the gradient of the MSE.\n5. **Return Results**: Return the learned weights rounded to four decimal places as a Python list, with the bias term first.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef linear_regression(X: list[list[int | float]],\n                     y: list[int | float],\n                     n_iterations: int,\n                     learning_rate: float) -> list[float]:\n    \"\"\"Learn linear regression weights with batch gradient descent.\"\"\"\n    \n    # Check if the number of samples in X and y are compatible\n    if len(X) != len(y):\n        return -1\n    \n    # Convert to numpy arrays\n    X_np = np.array(X, dtype=np.float64)\n    y_np = np.array(y, dtype=np.float64)\n    m = X_np.shape[0] if X_np.ndim > 0 else 0  # Number of samples\n    \n    # Augment X with a column of ones for the bias term\n    if m == 0:\n        # Handle case with zero samples (empty X and y)\n        X_aug = np.ones((0, 1))  # Shape (0, 1)\n    else:\n        ones_col = np.ones((m, 1))\n        X_aug = np.hstack((ones_col, X_np))\n    \n    # Number of features including bias (N)\n    N = X_aug.shape[1] if X_aug.size > 0 else 1\n    \n    # Initialize weights\n    low = -1.0 / math.sqrt(N)\n    high = 1.0 / math.sqrt(N)\n    w = np.random.uniform(low=low, high=high, size=N)\n    \n    # Perform batch gradient descent\n    for _ in range(n_iterations):\n        if m == 0:\n            break  # No data, no updates\n        predictions = X_aug.dot(w)\n        errors = predictions - y_np\n        gradient = (X_aug.T @ errors) / m\n        w -= learning_rate * gradient\n    \n    # Round to four decimal places and convert to list\n    return [round(weight, 4) for weight in w]\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the number of rows in X matches the length of y. If not, it returns -1 to indicate incompatible dimensions.\n2. **Data Preparation**: The feature matrix X is converted to a NumPy array and augmented with a column of ones to include the bias term. This allows the model to learn the intercept term.\n3. **Weight Initialization**: Weights are initialized from a uniform distribution scaled by the inverse square root of the number of features, ensuring they start in a range suitable for the problem.\n4. **Gradient Descent**: For each iteration, the model computes predictions using the current weights, calculates the error, and updates the weights using the gradient of the MSE. This process iteratively moves the weights towards the values that minimize the cost function.\n5. **Result Formatting**: The final weights are rounded to four decimal places and returned as a list, ensuring the bias term is the first element of the list.\n\nThis approach efficiently learns the optimal weights for linear regression using gradient descent, handling edge cases such as empty inputs and ensuring numerical stability through appropriate initialization and matrix operations."}
{"task_id": 122, "completion_id": 0, "completion": "To solve this problem, we need to implement a simple ordinary least-squares linear regression solver. The goal is to find the optimal weight vector that minimizes the squared error between the predicted values and the actual target values using the Moore-Penrose pseudo-inverse method.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert the given design matrix `X` and target vector `y` from Python lists to NumPy arrays to facilitate matrix operations.\n2. **Augment the Design Matrix**: Add a column of ones to the beginning of the design matrix `X` to account for the intercept term in the linear regression model.\n3. **Compute the Moore-Penrose Pseudo-Inverse**: Use NumPy's `np.linalg.pinv` function to compute the pseudo-inverse of the augmented design matrix. This method is robust even when the matrix is not invertible.\n4. **Calculate the Optimal Weights**: Multiply the pseudo-inverse of the augmented matrix by the target vector `y` to obtain the weight vector.\n5. **Round the Weights**: Round each element of the weight vector to four decimal places and return the result as a flat Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_weights(X: list[list[int | float]], y: list[int | float]) -> list[float]:\n    \"\"\"Return the ordinary least-squares weight vector for Linear Regression.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        2-D list where each inner list contains the feature values for one sample.\n    y : list[int | float]\n        1-D list with the target value corresponding to each sample.\n\n    Returns\n    -------\n    list[float]\n        Weight vector `[w0, w1, ..., wd]` rounded to 4 decimal places, where\n        `w0` is the intercept term.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    ones = np.ones((X_np.shape[0], 1))\n    X_aug = np.hstack((ones, X_np))\n    weights = np.linalg.pinv(X_aug) @ y_np\n    return [round(w, 4) for w in weights.tolist()]\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The input lists `X` and `y` are converted to NumPy arrays to leverage NumPy's efficient matrix operations.\n2. **Augmenting the Matrix**: A column of ones is prepended to the design matrix to create the augmented matrix, which allows the model to include an intercept term (bias).\n3. **Pseudo-Inverse Calculation**: The Moore-Penrose pseudo-inverse is computed for the augmented matrix. This approach ensures that we can handle cases where the matrix is singular or not invertible.\n4. **Weight Calculation**: The optimal weights are obtained by multiplying the pseudo-inverse of the augmented matrix with the target vector. This step effectively solves the linear regression problem using the least squares method.\n5. **Rounding**: Each weight is rounded to four decimal places to meet the specified precision requirement, and the result is returned as a list.\n\nThis method efficiently computes the optimal weights using fundamental linear algebra operations, ensuring both accuracy and robustness even in cases where traditional matrix inversion methods might fail."}
{"task_id": 123, "completion_id": 0, "completion": "To solve this problem, we need to convert a one-dimensional array of categorical values into its one-hot encoded representation without using any third-party machine-learning libraries. The solution must automatically detect the unique categories, sort them, and create a binary matrix where each row represents an element of the input array with 1s indicating the presence of a category and 0s otherwise.\n\n### Approach\n1. **Extract Unique Categories**: Use numpy's `unique` function to get the sorted unique categories from the input array. This function inherently sorts the categories in ascending order for numerical values and alphabetical order for strings.\n2. **Map Categories to Indices**: Create a dictionary that maps each unique category to its corresponding index in the sorted list of unique categories.\n3. **Generate One-Hot Encoded Rows**: For each element in the input array, generate a row vector of zeros with a length equal to the number of unique categories. Set the position corresponding to the element's category to 1 using the previously created dictionary to find the correct index.\n4. **Return Result**: Collect all the generated row vectors into a list and return it as the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to one-hot encoded format.\"\"\"\n    unique_cats = np.unique(y)\n    category_to_index = {cat: idx for idx, cat in enumerate(unique_cats)}\n    encoded = []\n    for sample in y:\n        row = [0] * len(unique_cats)\n        row[category_to_index[sample]] = 1\n        encoded.append(row)\n    return encoded\n```\n\n### Explanation\n1. **Extract Unique Categories**: The `np.unique(y)` function is used to obtain the sorted unique categories from the input array `y`. This ensures that the categories are in ascending order for numbers and alphabetical order for strings.\n2. **Map Categories to Indices**: A dictionary `category_to_index` is created where each key is a unique category and the value is its index in the sorted list. This allows for efficient lookup of the index for each category.\n3. **Generate Rows**: For each element in the input array, a row vector of zeros is initialized. The position corresponding to the element's category is set to 1 using the index obtained from the dictionary.\n4. **Result Construction**: Each generated row is appended to the result list, which is then returned as the final one-hot encoded matrix.\n\nThis approach efficiently converts the input array into a one-hot encoded format using basic Python operations and numpy for handling the input array, ensuring correctness and adherence to the problem constraints."}
{"task_id": 126, "completion_id": 0, "completion": "To solve this problem, we need to compute the Gram matrix for a polynomial kernel between two data matrices. The polynomial kernel is commonly used in kernel methods like Support Vector Machines (SVMs) and is defined as \\( k(x, y) = (\u03b3 \\cdot x^T y + c\u2080)^d \\). The Gram matrix is constructed such that each element \\( K[i][j] \\) represents the kernel value between the i-th row of matrix X and the j-th row of matrix Y.\n\n### Approach\n1. **Input Handling**: Convert the input matrices X and Y (if provided) into NumPy arrays for efficient computation. If Y is not provided, use X as Y.\n2. **Gamma Calculation**: If gamma (\u03b3) is not provided, compute it as \\( 1/C \\) where C is the number of features in X.\n3. **Dot Product Matrix**: Compute the dot product matrix between X and Y using matrix multiplication.\n4. **Kernel Computation**: Apply the polynomial kernel formula to each element of the dot product matrix, incorporating gamma, c\u2080, and raising the result to the power of d.\n5. **Rounding and Conversion**: Round the resulting Gram matrix to 4 decimal places and convert it back to a nested Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef polynomial_kernel(X: list[list[int | float]],\n                      Y: list[list[int | float]] | None = None,\n                      d: int = 3,\n                      gamma: float | None = None,\n                      c0: float = 1) -> list[list[float]]:\n    \"\"\"Compute the degree-d polynomial kernel between all rows of *X* and *Y*.\"\"\"\n    X_np = np.array(X)\n    Y_np = X_np if Y is None else np.array(Y)\n    \n    C = X_np.shape[1]\n    gamma_val = gamma if gamma is not None else 1.0 / C\n    \n    dot_product = X_np @ Y_np.T\n    gram_matrix = (gamma_val * dot_product + c0) ** d\n    \n    return np.round(gram_matrix, 4).tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input matrices X and Y are converted to NumPy arrays to leverage efficient matrix operations. If Y is not provided, it is set to X.\n2. **Gamma Calculation**: Gamma is determined either from the provided value or computed as \\( 1/C \\), where C is the number of features in X. This ensures proper scaling of the dot product.\n3. **Dot Product Matrix**: Using matrix multiplication, we compute the dot product between every pair of rows from X and Y, resulting in a matrix where each element (i, j) is the dot product of the i-th row of X and the j-th row of Y.\n4. **Kernel Application**: Each element of the dot product matrix is scaled by gamma, adjusted by adding c\u2080, and then raised to the power of d. This transforms the linear dot product into a polynomial kernel space.\n5. **Rounding and Conversion**: The final matrix is rounded to 4 decimal places for precision and converted back to a nested Python list to meet the output requirements.\n\nThis approach efficiently handles the polynomial kernel computation using NumPy for speed and clarity, ensuring the solution is both correct and optimized for performance."}
{"task_id": 127, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that generates a confusion matrix to evaluate the performance of a classification model. The confusion matrix is a square matrix where each entry represents the count of instances where the true class is mapped to the predicted class.\n\n### Approach\n1. **Check Input Lengths**: First, ensure that the input sequences `y_true` and `y_pred` have the same length. If not, return -1 as specified.\n2. **Identify Unique Classes**: Collect all unique class labels from both `y_true` and `y_pred`, then sort them to determine the order of rows and columns in the confusion matrix.\n3. **Create Mapping**: Generate a mapping from each unique class to its index based on the sorted list of classes. This helps in quickly locating the correct row and column for each true-predicted pair.\n4. **Initialize Matrix**: Create a zero-initialized square matrix of size C\u00d7C, where C is the number of unique classes.\n5. **Populate Matrix**: Iterate through each pair of true and predicted labels, updating the matrix counts based on the class indices obtained from the mapping.\n\n### Solution Code\n```python\ndef confusion_matrix(y_true: list, y_pred: list) -> list:\n    \"\"\"Build a confusion matrix for the given true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : list\n        Ground-truth class labels.\n    y_pred : list\n        Predicted class labels. Must have the same length as `y_true`.\n\n    Returns\n    -------\n    list\n        2-D list representing the confusion matrix.  The element at row *i*\n        and column *j* is the number of instances with true class equal to\n        the *i-th* sorted unique label and predicted class equal to the\n        *j-th* sorted unique label.\n\n        If the input lengths differ the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    \n    # Get all unique classes from both lists and sort them\n    classes = sorted(set(y_true) | set(y_pred))\n    \n    # Create a mapping from class to its index in the sorted list\n    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n    \n    # Initialize the confusion matrix with zeros\n    c = len(classes)\n    matrix = [[0 for _ in range(c)] for _ in range(c)]\n    \n    # Populate the matrix\n    for true_cls, pred_cls in zip(y_true, y_pred):\n        i = class_to_idx[true_cls]\n        j = class_to_idx[pred_cls]\n        matrix[i][j] += 1\n    \n    return matrix\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of `y_true` and `y_pred` are equal. If not, it returns -1 immediately.\n2. **Class Collection and Sorting**: By taking the union of the sets of `y_true` and `y_pred`, we gather all unique classes. Sorting these classes ensures a consistent order for rows and columns in the matrix.\n3. **Index Mapping**: Each class is assigned an index based on its position in the sorted list. This index is used to access the correct position in the matrix.\n4. **Matrix Initialization and Population**: A zero matrix of appropriate size is created. For each pair of true and predicted labels, the corresponding matrix entry is incremented, effectively counting occurrences of each true-predicted pair.\n\nThis approach ensures that the confusion matrix is constructed efficiently and correctly, providing a clear visualization of classification performance."}
{"task_id": 128, "completion_id": 0, "completion": "To solve this problem, we need to implement a multi-class Linear Discriminant Analysis (LDA) to project data onto a lower-dimensional space that maximizes the separation between different classes. This involves computing scatter matrices, performing eigen decomposition, and projecting the data onto the most significant eigenvectors.\n\n### Approach\n1. **Compute Within-Class Scatter Matrix (S_W)**: This matrix measures the spread of data within each class. It is computed by summing the covariance matrices of each class.\n2. **Compute Between-Class Scatter Matrix (S_B)**: This matrix measures the spread between different classes. It is computed by summing the outer products of the difference between each class mean and the global mean, weighted by the number of samples in each class.\n3. **Form Matrix A**: Compute the matrix A as the product of the pseudo-inverse of S_W and S_B to handle potential singularity issues in S_W.\n4. **Eigen Decomposition**: Perform eigen decomposition on matrix A to find the eigenvectors and eigenvalues. Sort the eigenvectors by descending eigenvalues.\n5. **Sign Determinism**: Ensure the sign of each eigenvector is deterministic by flipping it if the first non-zero element is negative.\n6. **Project Data**: Project the data onto the top `n_components` eigenvectors and round the results to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\"\"\"\n    if n_components == 0:\n        return []\n    \n    m, d = X.shape\n    classes = np.unique(y)\n    global_mean = np.mean(X, axis=0)\n    \n    # Compute within-class scatter matrix S_W\n    S_W = np.zeros((d, d))\n    for c in classes:\n        X_c = X[y == c]\n        class_mean = np.mean(X_c, axis=0)\n        X_c_centered = X_c - class_mean\n        S_W += X_c_centered.T @ X_c_centered\n    \n    # Compute between-class scatter matrix S_B\n    S_B = np.zeros((d, d))\n    for c in classes:\n        X_c = X[y == c]\n        class_mean = np.mean(X_c, axis=0)\n        n_c = X_c.shape[0]\n        diff = class_mean - global_mean\n        S_B += n_c * np.outer(diff, diff)\n    \n    # Compute matrix A = pinv(S_W) @ S_B\n    A = np.linalg.pinv(S_W) @ S_B\n    \n    # Eigen decomposition and sorting\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Deterministic sign adjustment\n    for i in range(sorted_eigenvectors.shape[1]):\n        eigenvector = sorted_eigenvectors[:, i]\n        for j in range(eigenvector.shape[0]):\n            if not np.isclose(eigenvector[j], 0, atol=1e-8):\n                if eigenvector[j] < 0:\n                    sorted_eigenvectors[:, i] = -eigenvector\n                break\n    \n    # Select the top n_components eigenvectors\n    W = sorted_eigenvectors[:, :n_components]\n    \n    # Project data and round\n    X_proj = X @ W\n    X_proj_rounded = np.round(X_proj, 4)\n    \n    return X_proj_rounded.tolist()\n```\n\n### Explanation\n1. **Within-Class Scatter Matrix (S_W)**: This matrix aggregates the covariance of each class, capturing the spread within each class. It is computed by centering the data around the class mean and summing the outer products.\n2. **Between-Class Scatter Matrix (S_B)**: This matrix captures the separation between different classes by measuring how each class mean deviates from the global mean, weighted by the number of samples in each class.\n3. **Matrix A**: The product of the pseudo-inverse of S_W and S_B ensures numerical stability even if S_W is singular.\n4. **Eigen Decomposition**: Eigenvalues and eigenvectors of matrix A are computed, sorted in descending order to prioritize directions of maximum variance.\n5. **Sign Adjustment**: Eigenvectors are adjusted to ensure a deterministic sign, enhancing consistency across different runs and platforms.\n6. **Projection**: The data is projected onto the selected eigenvectors, and the results are rounded to four decimal places for consistency.\n\nThis approach efficiently reduces the dimensionality while preserving class separability, making it suitable for multi-class classification tasks."}
{"task_id": 129, "completion_id": 0, "completion": "To solve this problem, we need to generate an infinite stream of data by cycling through a given finite sequence and return the first `samples` elements from this stream. If the input sequence is empty or `samples` is non-positive, we should return an empty list.\n\n### Approach\n1. **Check for Edge Cases**: First, we check if the input sequence is empty or if the number of samples requested is non-positive. If either condition is true, we return an empty list immediately.\n2. **Cycle Through the Sequence**: If the input is valid, we use the `itertools.cycle` function to create an infinite iterator over the sequence.\n3. **Slice the Infinite Iterator**: Using `itertools.islice`, we take the first `samples` elements from the infinite iterator generated by `itertools.cycle`.\n4. **Convert to List**: Finally, we convert the sliced elements into a list and return it.\n\n### Solution Code\n```python\nfrom itertools import islice, cycle\n\ndef cycle_sequence(sequence: list, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    if not sequence or samples <= 0:\n        return []\n    return list(islice(cycle(sequence), samples))\n```\n\n### Explanation\n- **Edge Cases Handling**: The function first checks if the input sequence is empty or if `samples` is non-positive. This ensures we handle invalid inputs upfront.\n- **Infinite Cycling**: By using `itertools.cycle`, we create an iterator that endlessly repeats the elements of the input sequence.\n- **Slicing the Stream**: `itertools.islice` allows us to take a specific number of elements from the infinite iterator without causing an infinite loop.\n- **Conversion to List**: The sliced elements are converted to a standard Python list to meet the output requirements.\n\nThis approach efficiently handles all specified cases and leverages Python's built-in itertools library to manage infinite cycling and slicing, ensuring both correctness and performance."}
{"task_id": 130, "completion_id": 0, "completion": "To solve this problem, we need to determine the empirical probability distribution over next states given a specific state and action from a list of experience tuples. This involves counting occurrences of each next state and converting these counts into probabilities.\n\n### Approach\n1. **Filter Relevant Transitions**: Extract all transitions that match the given state and action.\n2. **Count Next States**: Use a counter to tally occurrences of each next state from the filtered transitions.\n3. **Calculate Probabilities**: For each next state, compute its probability by dividing its count by the total number of relevant transitions.\n4. **Sort and Format**: Sort the next states in ascending order and format the results with probabilities rounded to four decimal places.\n\n### Solution Code\n```python\nfrom collections import Counter\nfrom typing import Any, List, Tuple\n\ndef outcome_probs(transitions: List[Tuple[Any, Any, Any, Any]],\n                  state: Any,\n                  action: Any) -> List[List[Any]]:\n    \"\"\"Return empirical outcome probabilities for a given (state, action).\"\"\"\n    next_states = [t[3] for t in transitions if t[0] == state and t[1] == action]\n    if not next_states:\n        return []\n    counts = Counter(next_states)\n    total = len(next_states)\n    sorted_items = sorted(counts.items(), key=lambda x: x[0])\n    result = []\n    for s, cnt in sorted_items:\n        prob = round(cnt / total, 4)\n        result.append([s, prob])\n    return result\n```\n\n### Explanation\n1. **Filtering Transitions**: We iterate through each transition and collect the next states where the state and action match the query.\n2. **Counting Occurrences**: Using `Counter`, we count how many times each next state appears in the filtered list.\n3. **Calculating Probabilities**: For each next state, we compute its probability by dividing its count by the total number of relevant transitions. The result is rounded to four decimal places.\n4. **Sorting Results**: The next states are sorted in ascending order to ensure the output is formatted correctly.\n\nThis approach efficiently processes the transitions and computes the required probabilities, ensuring accuracy and proper formatting as specified."}
{"task_id": 131, "completion_id": 0, "completion": "To solve this problem, we need to retrieve an element from a sequence stored in a shared container while handling invalid accesses gracefully. The shared container is a list of sequences, and each sequence is identified by a unique identifier (uid). The function must return the element at a specified index of the chosen sequence or None if the access is invalid.\n\n### Approach\n1. **Check Valid UID**: First, verify if the provided uid is a valid index for the shared container. A valid uid must be a non-negative integer within the range of the container's length.\n2. **Retrieve Sequence**: If the uid is valid, retrieve the corresponding sequence from the shared container.\n3. **Check Valid Index**: Check if the index (i) is valid for the retrieved sequence. The index can be negative, but it must be within the bounds of the sequence's length when converted to a positive index.\n\n### Solution Code\n```python\ndef get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` contains every available sequence so\n    that several parts of a program can work on different sequences at the\n    same time.\n\n    If *uid* or *i* is invalid the function must return *None* instead of\n    raising an exception.\n\n    Args:\n        uid: Integer identifier of the desired sequence.\n        i:   Position inside the selected sequence (supports negative indices).\n\n    Returns:\n        The requested element, or None if the access is invalid.\n    \"\"\"\n    # Check if uid is a valid index for _SHARED_SEQUENCES\n    if uid < 0 or uid >= len(_SHARED_SEQUENCES):\n        return None\n    sequence = _SHARED_SEQUENCES[uid]\n    # Check if i is a valid index for the sequence\n    n = len(sequence)\n    if n == 0:\n        return None\n    if i < -n or i >= n:\n        return None\n    return sequence[i]\n```\n\n### Explanation\n1. **UID Validation**: The function first checks if the uid is within the valid range of indices for the shared container. If the uid is negative or exceeds the container's length, it returns None.\n2. **Sequence Retrieval**: If the uid is valid, the corresponding sequence is retrieved from the shared container.\n3. **Index Validation**: The function checks if the index (i) is within the valid range for the retrieved sequence. This includes handling negative indices by converting them to their positive counterparts and ensuring they fall within the sequence's bounds.\n4. **Return Element**: If both uid and index are valid, the element at the specified index is returned; otherwise, None is returned for any invalid access.\n\nThis approach ensures that all edge cases, such as invalid indices, empty sequences, and out-of-bound accesses, are handled efficiently without raising exceptions."}
{"task_id": 132, "completion_id": 0, "completion": "To solve this problem, we need to implement a linear activation function that returns the input unchanged but ensures that non-scalar inputs are converted to NumPy ndarrays. The function must handle three types of inputs: scalars (int or float), lists/tuples (even nested ones), and NumPy ndarrays. The goal is to return the same value as the input while ensuring a uniform NumPy output for non-scalar inputs.\n\n### Approach\n1. **Check for Scalar Input**: If the input is a Python scalar (int or float), return it unchanged.\n2. **Handle List/Tuple Input**: If the input is a list or tuple, convert it to a NumPy ndarray. This conversion handles nested structures and ensures the output is a NumPy array.\n3. **Return NumPy Arrays Directly**: If the input is already a NumPy ndarray, return it without modification to avoid unnecessary copying.\n\nThis approach ensures that scalars remain unchanged, lists/tuples are converted to ndarrays, and existing ndarrays are returned as-is, thus meeting the problem requirements efficiently.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear(z):\n    \"\"\"Linear (identity) activation function.\n\n    Args:\n        z: Scalar, list, tuple or NumPy ndarray.\n\n    Returns:\n        The same value(s) as *z*, obeying the rules:\n            \u2022 Scalars are returned unchanged.\n            \u2022 Non-scalar inputs are returned as a NumPy ndarray.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        return z\n    elif isinstance(z, (list, tuple)):\n        return np.array(z)\n    else:  # z is a NumPy ndarray\n        return z\n```\n\n### Explanation\n- **Scalar Handling**: The function first checks if the input is an int or float. If so, it returns the scalar directly.\n- **List/Tuple Conversion**: For list or tuple inputs, the function converts them to a NumPy ndarray using `np.array()`, which handles nested structures and different numeric types.\n- **Direct Return for ndarrays**: If the input is already a NumPy ndarray, it is returned as-is without any modification, ensuring no unnecessary copies are made.\n\nThis approach efficiently handles all specified input types while maintaining the required output constraints, ensuring both correctness and performance."}
{"task_id": 133, "completion_id": 0, "completion": "To solve this problem, we need to convert a batch of integer-encoded sequences into a 3-D one-hot encoded format. This conversion is essential for many sequence-to-sequence or language-modeling tasks in machine learning. The one-hot encoding ensures that each token index is represented as a binary vector where only the corresponding index position is 1, and all other positions are 0. Additionally, we need to validate the input to ensure all token indices are within the valid range.\n\n### Approach\n1. **Validation Check**: First, we check each token in the input sequences to ensure they are within the valid range [0, vocab_length - 1]. If any token is found to be out of this range, the function immediately returns -1.\n2. **One-Hot Encoding**: If all tokens are valid, we proceed to convert each token into its one-hot encoded vector. For each token in each sequence, we create a vector of zeros with length equal to the vocabulary size and set the position corresponding to the token index to 1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef one_hot_targets(X_train: list[list[int]], vocab_length: int) -> list[list[list[int]]]:\n    \"\"\"Convert integer-encoded sequences into a 3-D one-hot representation.\n\n    Parameters\n    ----------\n    X_train : list[list[int]]\n        A batch of sequences where each element is an integer token index.\n    vocab_length : int\n        The size of the vocabulary.\n\n    Returns\n    -------\n    list[list[list[int]]]\n        A nested list with shape (m, time_steps, vocab_length) representing the\n        one-hot encoded targets, or -1 if the input contains invalid indices.\n    \"\"\"\n    # Check for invalid token indices\n    for sample in X_train:\n        for token in sample:\n            if token < 0 or token >= vocab_length:\n                return -1\n    \n    # Create the one-hot encoded structure\n    Y = []\n    for sample in X_train:\n        time_steps = []\n        for token in sample:\n            one_hot = [0] * vocab_length\n            one_hot[token] = 1\n            time_steps.append(one_hot)\n        Y.append(time_steps)\n    \n    return Y\n```\n\n### Explanation\n1. **Validation Check**: The function first iterates through each token in each sequence of the input. If any token is found to be negative or greater than or equal to the vocabulary length, the function returns -1 immediately, indicating invalid input.\n2. **One-Hot Encoding**: For valid input, each token is converted into a one-hot vector. This is done by initializing a vector of zeros with length equal to the vocabulary size and setting the position corresponding to the token's index to 1. These vectors are collected into a 3-D list structure, which is then returned.\n\nThis approach ensures that the input is validated efficiently and the conversion to one-hot encoding is performed correctly, resulting in the required 3-D structure suitable for use in deep learning models."}
{"task_id": 134, "completion_id": 0, "completion": "To solve this problem, we need to identify the optimal arm in a Bernoulli multi-armed bandit scenario. The optimal arm is defined as the one with the highest probability of yielding a reward, and if there are multiple arms with the same highest probability, the smallest index among them should be chosen. Additionally, we need to validate the input to ensure all probabilities are within the valid range [0, 1].\n\n### Approach\n1. **Input Validation**: Check if the input list is empty. If it is, return the error tuple (-1.0, -1). If any probability in the list is outside the valid range [0, 1], return the same error tuple.\n2. **Find Maximum Probability**: Iterate through the list of probabilities to find the maximum value.\n3. **Determine Smallest Index**: Track the smallest index where the maximum probability occurs. This is done by iterating through the list and updating the maximum value and corresponding index whenever a higher probability is found. If the same maximum probability is encountered again, the index is not updated, ensuring the smallest index is retained.\n\n### Solution Code\n```python\ndef best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n    \"\"\"Find the arm with the highest expected reward in a Bernoulli bandit.\n\n    Parameters\n    ----------\n    payoff_probs : list[float]\n        A list where each element is the success probability of the\n        corresponding arm. Probabilities must lie in the interval [0, 1].\n\n    Returns\n    -------\n    tuple\n        (max_expected_reward, best_arm_index).  If the input list is empty or\n        contains an invalid probability (<0 or >1), the function must return\n        (-1.0, -1).\n    \"\"\"\n    # Check if the list is empty\n    if not payoff_probs:\n        return (-1.0, -1)\n    \n    # Check all probabilities are valid\n    for prob in payoff_probs:\n        if not (0.0 <= prob <= 1.0):\n            return (-1.0, -1)\n    \n    # Find the maximum probability and the smallest index\n    max_prob = -1.0\n    best_index = -1\n    for index, prob in enumerate(payoff_probs):\n        if prob > max_prob:\n            max_prob = prob\n            best_index = index\n    \n    return (float(max_prob), best_index)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input list is empty. If it is, it returns (-1.0, -1). Then, it iterates through each probability to ensure they all lie within the valid range [0, 1]. If any probability is invalid, it returns the error tuple.\n2. **Finding Maximum Probability**: The function initializes `max_prob` to -1.0 and `best_index` to -1. As it iterates through the list, it updates these variables whenever a higher probability is found. This ensures that the first occurrence of the highest probability is tracked, thus maintaining the smallest index requirement.\n3. **Return Result**: Finally, the function returns a tuple containing the maximum probability (converted to a float) and the corresponding smallest index. This approach efficiently handles all edge cases and constraints specified in the problem."}
{"task_id": 135, "completion_id": 0, "completion": "To solve this problem, we need to implement Ordinary Least Squares (OLS) linear regression from scratch using the normal equation. The solution involves learning the regression coefficients from a training set and using these coefficients to predict target values for a test set. If the matrix encountered during the computation is singular, we return -1 instead of proceeding with the model fitting.\n\n### Approach\n1. **Data Preparation**: Convert the input training and test data into NumPy arrays for efficient computation. Prepend a column of ones to the feature matrices of both the training and test datasets to account for the intercept term.\n2. **Matrix Inversion Check**: Compute the matrix \\(X^T X\\) (where \\(X\\) is the training data matrix with the intercept term). Check if this matrix is invertible. If it is not invertible (singular), return -1 immediately.\n3. **Coefficient Calculation**: If the matrix is invertible, compute the regression coefficients using the normal equation \\(\\theta = (X^T X)^{-1} X^T y\\).\n4. **Prediction**: Use the learned coefficients to predict the target values for the test data, ensuring each prediction is rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_predict(\n        X_train: list[list[int | float]],\n        y_train: list[int | float],\n        X_test: list[list[int | float]]) -> list[float]:\n    \"\"\"Implements OLS Linear Regression using the normal equation.\"\"\"\n    # Convert input lists to numpy arrays\n    X_train_np = np.array(X_train, dtype=np.float64)\n    y_train_np = np.array(y_train, dtype=np.float64)\n    X_test_np = np.array(X_test, dtype=np.float64)\n    \n    # Add intercept term (column of ones) to X_train\n    n_samples_train = X_train_np.shape[0]\n    X_train_aug = np.hstack([np.ones((n_samples_train, 1)), X_train_np])\n    \n    # Compute X^T X and check if it's invertible\n    try:\n        X_T_X = X_train_aug.T @ X_train_aug\n        X_T_X_inv = np.linalg.inv(X_T_X)\n    except np.linalg.LinAlgError:\n        return -1\n    \n    # Calculate the coefficients using the normal equation\n    theta = X_T_X_inv @ (X_train_aug.T @ y_train_np)\n    \n    # Add intercept term to X_test\n    n_samples_test = X_test_np.shape[0]\n    X_test_aug = np.hstack([np.ones((n_samples_test, 1)), X_test_np])\n    \n    # Make predictions\n    predictions = X_test_aug @ theta\n    \n    # Round each prediction to 4 decimal places and return as a list\n    return [round(float(pred), 4) for pred in predictions]\n```\n\n### Explanation\n1. **Data Preparation**: The training and test datasets are converted into NumPy arrays to leverage efficient matrix operations. A column of ones is added to both datasets to include the intercept term in the model.\n2. **Matrix Inversion Check**: The matrix \\(X^T X\\) is computed and checked for invertibility. If an error occurs during inversion (indicating a singular matrix), the function returns -1.\n3. **Coefficient Calculation**: Using the normal equation, the coefficients (including the intercept) are calculated. This involves matrix multiplication and inversion operations.\n4. **Prediction**: The test data, with the intercept term added, is multiplied by the learned coefficients to generate predictions. Each prediction is rounded to four decimal places to meet the output requirements.\n\nThis approach ensures that all steps are handled efficiently using NumPy's linear algebra utilities, adhering to the problem constraints and requirements."}
{"task_id": 136, "completion_id": 0, "completion": "To solve this problem, we need to implement a numerically stable version of the softmax activation function using NumPy. The softmax function converts raw scores into a probability distribution, and the numerically stable version avoids potential overflow or underflow by subtracting the maximum value from each element before exponentiation.\n\n### Approach\n1. **Subtract the Maximum Value**: For each element along the specified axis, subtract the maximum value to avoid large exponentials which can cause numerical instability.\n2. **Exponentiate the Shifted Values**: Compute the exponential of each element after adjusting by the maximum value.\n3. **Normalize by the Sum of Exponentials**: Sum the exponentials along the specified axis and divide each element by this sum to get probabilities.\n4. **Rounding and Conversion**: Round each probability to four decimal places and convert the resulting NumPy array to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softmax(z: np.ndarray, axis: int = -1) -> list:\n    \"\"\"Compute the numerically-stable softmax of *z* along *axis*.\"\"\"\n    # Compute max along the specified axis, keeping dimensions for broadcasting\n    max_z = np.max(z, axis=axis, keepdims=True)\n    # Subtract max for numerical stability\n    shifted_z = z - max_z\n    # Exponentiate the shifted values\n    exp_z = np.exp(shifted_z)\n    # Sum of exponentials along the same axis\n    sum_exp = np.sum(exp_z, axis=axis, keepdims=True)\n    # Compute probabilities\n    probabilities = exp_z / sum_exp\n    # Round to four decimal places\n    rounded = np.round(probabilities, 4)\n    # Convert to Python list and return\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Numerical Stability**: By subtracting the maximum value from each element before exponentiation, we ensure that the largest value exponentiated is zero, which helps in avoiding overflow or underflow issues.\n2. **Broadcasting**: Using `keepdims=True` ensures that the dimensions of the max and sum arrays are compatible with the input array for proper element-wise operations.\n3. **Efficiency**: The operations are vectorized using NumPy, making the implementation efficient even for large arrays.\n4. **Conversion**: The final result is converted to a Python list with elements rounded to four decimal places to meet the specified output requirements.\n\nThis approach efficiently computes the softmax function in a numerically stable manner, handles any valid axis for multidimensional arrays, and ensures the output is a properly formatted Python list."}
{"task_id": 138, "completion_id": 0, "completion": "To solve this problem, we need to compute the classification accuracy between two sequences of labels. The accuracy is defined as the proportion of elements that are identical in the same positions of both sequences. Additionally, we need to validate the input sequences to ensure they meet specific criteria.\n\n### Approach\n1. **Input Validation**: Check if the input sequences `y_true` and `y_pred` have the same non-zero length. If they don't, return -1.0 as a float to indicate invalid input.\n2. **Count Correct Predictions**: Iterate through the elements of both sequences simultaneously and count how many times the corresponding elements are equal.\n3. **Compute Accuracy**: Divide the count of correct predictions by the total number of elements to get the accuracy.\n4. **Round Result**: Round the computed accuracy to four decimal places and return it as a float.\n\n### Solution Code\n```python\ndef accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    if len(y_true) != len(y_pred) or len(y_true) == 0:\n        return -1.0\n    \n    correct = sum(1 for a, b in zip(y_true, y_pred) if a == b)\n    accuracy = correct / len(y_true)\n    return round(accuracy, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of `y_true` and `y_pred` are equal and non-zero. If either condition is not met, it returns -1.0 to indicate invalid input.\n2. **Counting Correct Predictions**: Using a generator expression with `zip`, the function pairs elements from both sequences and counts how many pairs are equal.\n3. **Computing Accuracy**: The count of correct predictions is divided by the total number of elements to compute the accuracy.\n4. **Rounding**: The computed accuracy is rounded to four decimal places using Python's built-in `round` function to ensure the result is a float with the required precision.\n\nThis approach ensures that the function efficiently handles various data types, including numerical values, strings, and booleans, by relying on Python's native equality checks. The solution is both concise and efficient, with a time complexity of O(n), where n is the length of the input sequences."}
{"task_id": 139, "completion_id": 0, "completion": "To solve this problem, we need to implement the hyperbolic tangent (tanh) activation function using a numerically stable formula. The function should accept a one-dimensional iterable (list or NumPy array) and return a list of tanh values for each element, rounded to four decimal places.\n\n### Approach\n1. **Check for Empty Input**: If the input is an empty list or array, return an empty list immediately.\n2. **Iterate Over Elements**: For each element in the input, compute the tanh value using the numerically stable formula: \n   \\[\n   \\tanh(z) = \\frac{2}{1 + e^{-2z}} - 1\n   \\]\n3. **Compute Exponentials**: Use the NumPy `exp` function to compute the exponential term efficiently and avoid numerical instability.\n4. **Round Results**: Each computed tanh value is rounded to four decimal places using Python's built-in `round` function.\n5. **Return Results**: Collect all rounded values in a list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef tanh_activation(z):\n    \"\"\"Compute the hyperbolic tangent (tanh) for every element of the input.\n\n    Args:\n        z (list[int | float] | np.ndarray): A one-dimensional iterable of\n            numeric values.\n\n    Returns:\n        list[float]: Tanh applied element-wise, rounded to four decimals.\n    \"\"\"\n    if not z:\n        return []\n    result = []\n    for element in z:\n        exponent = -2 * element\n        e_term = np.exp(exponent)\n        val = (2.0 / (1 + e_term)) - 1\n        rounded_val = round(val, 4)\n        result.append(rounded_val)\n    return result\n```\n\n### Explanation\n- **Numerical Stability**: The formula \\(\\frac{2}{1 + e^{-2z}} - 1\\) is used instead of the direct definition to avoid overflow issues for large values of \\(z\\).\n- **Handling Different Input Types**: The function works for both Python lists and NumPy arrays by iterating through each element.\n- **Edge Cases**: The function correctly handles an empty input by returning an empty list and computes values accurately for both very large positive and negative numbers.\n\nThis approach ensures that the tanh values are computed efficiently and accurately, with results rounded to the required precision."}
{"task_id": 140, "completion_id": 0, "completion": "To solve this problem, we need to determine the minimum time for an agent to escape a maze while avoiding spreading fire. The fire spreads first each minute, followed by the agent's movement. The agent can only move into a cell if it is not on fire at the time of arrival.\n\n### Approach\n1. **Precompute Fire Spread Times**: Using a multi-source BFS, compute the earliest time each cell catches fire. This helps us determine which cells are safe for the agent at any given time.\n2. **Initial Checks**: Immediately return -1 if the start or end cell is on fire at time 0. If the grid is 1x1 and safe, return 0.\n3. **Agent BFS**: Use BFS to explore the agent's possible movements, ensuring each move is to a cell that is not a wall and not on fire at the time of arrival. Track the earliest time each cell is visited to avoid redundant checks.\n\n### Solution Code\n```python\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    n = len(grid)\n    if n == 0:\n        return -1\n    \n    # Precompute the earliest time each cell catches fire\n    fire_time = [[math.inf] * n for _ in range(n)]\n    q = deque()\n    \n    # Initialize fire sources\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_time[i][j] = 0\n                q.append((i, j))\n    \n    # Directions for BFS (up, down, left, right)\n    dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    # BFS to compute fire spread times\n    while q:\n        i, j = q.popleft()\n        for di, dj in dirs:\n            ni, nj = i + di, j + dj\n            if 0 <= ni < n and 0 <= nj < n:\n                if grid[ni][nj] != '#' and fire_time[ni][nj] > fire_time[i][j] + 1:\n                    fire_time[ni][nj] = fire_time[i][j] + 1\n                    q.append((ni, nj))\n    \n    # Check if start or end is on fire at t=0\n    if fire_time[0][0] == 0 or fire_time[n-1][n-1] == 0:\n        return -1\n    \n    # Handle 1x1 grid case\n    if n == 1:\n        return 0\n    \n    # BFS for agent's movement\n    visited = [[math.inf] * n for _ in range(n)]\n    q_agent = deque()\n    q_agent.append((0, 0, 0))\n    visited[0][0] = 0\n    \n    while q_agent:\n        i, j, t = q_agent.popleft()\n        for di, dj in dirs:\n            ni, nj = i + di, j + dj\n            if 0 <= ni < n and 0 <= nj < n:\n                if grid[ni][nj] == '#':\n                    continue  # Skip walls\n                new_time = t + 1\n                # Check if the cell is safe to move into at new_time\n                if fire_time[ni][nj] > new_time:\n                    if ni == n-1 and nj == n-1:\n                        return new_time\n                    if visited[ni][nj] > new_time:\n                        visited[ni][nj] = new_time\n                        q_agent.append((ni, nj, new_time))\n    \n    return -1\n```\n\n### Explanation\n1. **Fire Spread Calculation**: Using BFS from all initial fire cells, we determine the earliest time each cell catches fire. This helps in checking if a cell is safe for the agent at any given time.\n2. **Initial Checks**: Ensure the start and end cells are not on fire at time 0. Handle the trivial case of a 1x1 grid.\n3. **Agent Movement**: Using BFS, explore all possible paths for the agent, moving to cells only if they are safe (not on fire and not a wall) at the next time step. Track visited cells to avoid redundant checks and ensure the shortest path is found.\n\nThis approach efficiently combines BFS for both fire spread and agent movement, ensuring optimal pathfinding while considering dynamic obstacles (spreading fire)."}
{"task_id": 141, "completion_id": 0, "completion": "To solve this problem, we need to implement a k-Nearest Neighbors (k-NN) classifier that predicts the labels for an unlabeled test set based on a labeled training set. The classifier uses one of three distance metrics (Euclidean, Manhattan, or Cosine) to determine the nearest neighbors and then applies a majority vote to predict the class, with a tie-breaker that selects the smallest label value in case of a tie.\n\n### Approach\n1. **Compute Distance Matrix**: Calculate the distance between each test sample and every training sample using the specified metric (Euclidean, Manhattan, or Cosine). If an unknown metric is provided, default to Euclidean.\n2. **Find Nearest Neighbors**: For each test sample, identify the k training samples with the smallest distances.\n3. **Majority Vote**: Predict the class label by majority vote among the k nearest neighbors. In case of a tie, return the smallest label value.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"Implement k-Nearest Neighbors classifier.\"\"\"\n    \n    # Handle unknown metric by falling back to Euclidean\n    if metric not in ['euclidean', 'manhattan', 'cosine']:\n        metric = 'euclidean'\n    \n    # Compute pairwise distances between test and training samples\n    if metric == 'euclidean':\n        distances = np.sqrt(((X_test[:, np.newaxis] - X_train) ** 2).sum(axis=2))\n    elif metric == 'manhattan':\n        distances = np.abs(X_test[:, np.newaxis] - X_train).sum(axis=2)\n    elif metric == 'cosine':\n        dot_product = np.dot(X_test, X_train.T)\n        norm_test = np.linalg.norm(X_test, axis=1, keepdims=True)\n        norm_train = np.linalg.norm(X_train, axis=1)\n        cosine_sim = dot_product / (norm_test * norm_train)\n        distances = 1 - cosine_sim\n    \n    # Find indices of k nearest neighbors for each test sample\n    nearest_indices = np.argpartition(distances, k, axis=1)[:, :k]\n    \n    # Collect the labels of the nearest neighbors\n    y_neighbors = y_train[nearest_indices]\n    \n    # Determine the majority vote with tie-breaker (smallest label in case of tie)\n    y_pred = []\n    for neighbors in y_neighbors:\n        labels, counts = np.unique(neighbors, return_counts=True)\n        max_count = np.max(counts)\n        candidates = labels[counts == max_count]\n        y_pred.append(np.min(candidates))\n    \n    return np.array(y_pred)\n```\n\n### Explanation\n1. **Distance Calculation**:\n   - **Euclidean Distance**: Computed as the square root of the sum of squared differences between each feature.\n   - **Manhattan Distance**: Sum of absolute differences between each feature.\n   - **Cosine Distance**: Derived from 1 minus the cosine similarity, which is the dot product of vectors divided by the product of their norms.\n\n2. **Nearest Neighbor Identification**:\n   - Using `np.argpartition`, we efficiently find the indices of the k smallest distances for each test sample without fully sorting the array, which optimizes performance.\n\n3. **Majority Vote with Tie-Breaker**:\n   - For each test sample, we collect the labels of the k nearest neighbors and use `np.unique` to count occurrences. The label with the highest count is selected, and in case of a tie, the smallest label is chosen.\n\nThis approach ensures that the k-NN classifier efficiently computes distances, identifies neighbors, and accurately predicts labels while handling ties appropriately."}
{"task_id": 143, "completion_id": 0, "completion": "To solve this problem, we need to implement the Leaky ReLU activation function and its derivative using NumPy. The Leaky ReLU function is widely used in deep learning models to avoid the dying neuron problem by allowing a small gradient when the input is negative. \n\n### Approach\n1. **Conversion to NumPy Array**: Convert the input `x` to a NumPy array if it is a Python list or tuple. This ensures that the function works with NumPy operations.\n2. **Leaky ReLU Calculation**: For the forward pass (activation), compute the output using the formula:\n   - `f(x) = x` if `x >= 0`\n   - `f(x) = alpha * x` if `x < 0`\n3. **Derivative Calculation**: For the backward pass (derivative), compute the gradient using the formula:\n   - `f'(x) = 1` if `x >= 0`\n   - `f'(x) = alpha` if `x < 0`\n4. **Vectorized Operations**: Use NumPy's `where` function to apply these conditions element-wise on the input array, ensuring efficient and fast computation without using loops.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(x, alpha: float = 0.2, derivative: bool = False):\n    \"\"\"Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative slope coefficient. Default is 0.2.\n    derivative : bool, optional\n        If False (default), compute the Leaky ReLU activation.\n        If True, compute the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with the same shape as *x* containing the computed values.\n    \"\"\"\n    x = np.asarray(x)\n    if derivative:\n        return np.where(x >= 0, 1.0, alpha)\n    else:\n        return np.where(x >= 0, x, alpha * x)\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The function starts by converting the input `x` to a NumPy array using `np.asarray(x)`. This ensures compatibility with NumPy operations and handles both array and list/tuple inputs seamlessly.\n- **Forward Pass (Activation)**: Using `np.where(x >= 0, x, alpha * x)`, the function computes the Leaky ReLU activation. This checks each element of the array: if the element is non-negative, it retains its value; otherwise, it multiplies the element by `alpha`.\n- **Backward Pass (Derivative)**: Using `np.where(x >= 0, 1.0, alpha)`, the function computes the derivative. Here, elements are set to 1 if non-negative and to `alpha` otherwise, reflecting the gradient of the Leaky ReLU function.\n\nThis approach ensures that the function is efficient and handles arrays of any shape, leveraging NumPy's vectorized operations for optimal performance."}
{"task_id": 144, "completion_id": 0, "completion": "To solve this problem, we need to compute the Mean Absolute Error (MAE) between two sequences of numeric values. The MAE is a common regression metric that measures the average magnitude of errors between predicted and actual values. The solution must handle various input types (lists, tuples, NumPy arrays) and ensure they are of the same shape before computing the MAE. If the shapes differ, the function should return -1.\n\n### Approach\n1. **Input Conversion**: Convert both input sequences to NumPy arrays to handle different input types uniformly.\n2. **Shape Check**: Verify if the shapes of the two converted arrays are the same. If not, return -1 immediately.\n3. **Compute Absolute Differences**: Calculate the absolute differences between corresponding elements of the actual and predicted arrays.\n4. **Mean Calculation**: Compute the mean of these absolute differences.\n5. **Rounding**: Round the result to 4 decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mean_absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence containing the true values.\n    predicted : list | tuple | np.ndarray\n        Sequence containing the predicted values. Must have the same length\n        (and shape for multi-dimensional inputs) as ``actual``.\n\n    Returns\n    -------\n    float\n        The MAE rounded to 4 decimal places if inputs have the same shape,\n        otherwise \u20111.\n    \"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    if actual.shape != predicted.shape:\n        return -1\n    mae = np.mean(np.abs(actual - predicted))\n    return round(mae, 4)\n```\n\n### Explanation\n1. **Input Conversion**: The function converts both input sequences to NumPy arrays using `np.asarray()`, ensuring uniform handling of different input types (lists, tuples, or arrays).\n2. **Shape Check**: The shapes of the converted arrays are compared. If they differ, the function returns -1 immediately to indicate invalid input dimensions.\n3. **Absolute Differences**: The element-wise absolute differences between the actual and predicted values are computed using NumPy's `np.abs()` function.\n4. **Mean Calculation**: The mean of these absolute differences is computed using `np.mean()`, which efficiently handles the averaging.\n5. **Rounding**: The result is rounded to 4 decimal places using Python's built-in `round()` function to meet the specified precision requirement.\n\nThis approach ensures that the function efficiently handles various input types and correctly computes the MAE while adhering to the problem constraints."}
{"task_id": 146, "completion_id": 0, "completion": "To solve this problem, we need to implement the k-Nearest Neighbors (k-NN) classifier from scratch. The k-NN algorithm is a simple, instance-based learning algorithm used for classification tasks. The core idea is to predict the label of a test sample based on the majority vote of its k nearest neighbors in the training set, determined by a specified distance metric.\n\n### Approach\n1. **Distance Metrics**: Compute the distance between each test sample and all training samples using one of three metrics: Euclidean, Manhattan, or Cosine distance.\n2. **Neighbor Selection**: For each test sample, identify the k nearest training samples based on the computed distances.\n3. **Majority Vote**: Determine the predicted label by majority vote among the k nearest neighbors. In case of a tie, select the smallest label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\"\"\"\n    predictions = []\n    n_train = X.shape[0]\n    k_adj = min(k, n_train)\n    \n    for x in X_test:\n        if metric == 'euclidean':\n            distances = np.sqrt(np.sum((X - x) ** 2, axis=1))\n        elif metric == 'manhattan':\n            distances = np.sum(np.abs(X - x), axis=1)\n        elif metric == 'cosine':\n            dot_product = np.dot(X, x)\n            norm_x = np.linalg.norm(x)\n            norm_X = np.linalg.norm(X, axis=1)\n            denominator = norm_X * norm_x + 1e-12\n            cosine_sim = dot_product / denominator\n            distances = 1 - cosine_sim\n        else:\n            raise ValueError(f\"Invalid metric '{metric}'. Choose 'euclidean', 'manhattan', or 'cosine'.\")\n        \n        sorted_indices = np.argsort(distances)\n        k_nearest_indices = sorted_indices[:k_adj]\n        k_nearest_labels = y[k_nearest_indices]\n        \n        unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)\n        max_count = np.max(counts)\n        candidates = unique_labels[counts == max_count]\n        predicted_label = np.min(candidates)\n        predictions.append(predicted_label)\n    \n    return np.array(predictions).tolist()\n```\n\n### Explanation\n1. **Distance Calculation**:\n   - **Euclidean Distance**: Computed as the square root of the sum of squared differences between corresponding features.\n   - **Manhattan Distance**: Computed as the sum of absolute differences between corresponding features.\n   - **Cosine Distance**: Computed as 1 minus the cosine similarity, which measures the cosine of the angle between two vectors. An epsilon (1e-12) is added to avoid division by zero.\n\n2. **Neighbor Selection**:\n   - For each test sample, compute distances to all training samples using the chosen metric.\n   - Sort the distances and select the indices of the k smallest distances (or all training samples if k exceeds the number of training samples).\n\n3. **Majority Vote**:\n   - Collect the labels of the k nearest neighbors.\n   - Determine the most frequent label. In case of a tie, select the smallest label using `np.min`.\n\nThis approach ensures efficient computation of distances using vectorized operations and handles edge cases such as ties in the majority vote by selecting the smallest label. The solution is implemented using basic NumPy operations to ensure efficiency and clarity."}
{"task_id": 147, "completion_id": 0, "completion": "To solve this problem, we need to implement a binary logistic regression model from scratch using batch gradient descent. The goal is to learn the weights and intercept parameters from the training data and use them to predict the class labels of unseen test data. The solution must handle the special case where the training data contains only one class, in which case all test predictions should be that class.\n\n### Approach\n\n1. **Check for Single Class in Training Data**: If all training labels are the same (either all 0 or all 1), return that class for all test predictions immediately.\n2. **Initialize Parameters**: Initialize the weights to a zero vector and the intercept to zero.\n3. **Batch Gradient Descent**: For a specified number of epochs, compute the predicted probabilities using the sigmoid function, then update the weights and intercept using the gradients of the negative log-likelihood loss.\n4. **Predict Test Data**: Use the learned parameters to compute the probabilities for the test data and convert these probabilities to binary labels using a threshold of 0.5.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef logistic_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    learning_rate: float = 0.1,\n    epochs: int = 10000,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model and predict labels for `X_test`.\"\"\"\n    unique_classes = np.unique(y_train)\n    if len(unique_classes) == 1:\n        return [unique_classes[0]] * X_test.shape[0]\n    \n    n_samples, n_features = X_train.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    \n    for _ in range(epochs):\n        z = np.dot(X_train, w) + b\n        p = 1 / (1 + np.exp(-z))\n        error = p - y_train\n        dw = np.dot(X_train.T, error) / n_samples\n        db = np.sum(error) / n_samples\n        w -= learning_rate * dw\n        b -= learning_rate * db\n    \n    z_test = np.dot(X_test, w) + b\n    p_test = 1 / (1 + np.exp(-z_test))\n    y_pred = (p_test >= 0.5).astype(int)\n    return y_pred.tolist()\n```\n\n### Explanation\n\n1. **Single Class Check**: The code first checks if all training labels are the same. If so, it directly returns that class for all test predictions, ensuring quick handling of trivial cases.\n2. **Parameter Initialization**: The weights (`w`) are initialized as a zero vector, and the intercept (`b`) is initialized to zero. This is a common starting point for logistic regression.\n3. **Gradient Descent Training**: For each epoch, the linear combination of features and weights plus the intercept is computed. The sigmoid function converts this combination to probabilities. The gradients of the loss with respect to the weights and intercept are computed using the average error over all samples. The parameters are updated iteratively using these gradients scaled by the learning rate.\n4. **Prediction**: After training, the model computes the linear combination for test data, applies the sigmoid function to get probabilities, and thresholds these probabilities at 0.5 to determine the predicted class labels.\n\nThis approach efficiently learns the decision boundary using gradient descent and handles both separable and non-separable data, providing accurate predictions for the test set."}
{"task_id": 148, "completion_id": 0, "completion": "To solve this problem, we need to compute the Exponential Moving Average (EMA) for a given sequence of numerical observations using a specified smoothing factor \\(\\alpha\\). The EMA is a type of moving average that places more weight on recent data points, making it more responsive to changes in the data.\n\n### Approach\n1. **Input Validation**: Check if the input list is empty or if the smoothing factor \\(\\alpha\\) is outside the valid range [0, 1]. If either condition is met, return -1.\n2. **Special Cases Handling**:\n   - **Alpha = 0**: When \\(\\alpha\\) is 0, the output list should be filled with the first element of the input list, as no new data points will influence the average.\n   - **Alpha = 1**: When \\(\\alpha\\) is 1, the output list will be identical to the input list, as each new data point is fully considered without any smoothing.\n3. **General Case Calculation**:\n   - Initialize the smoothed list with the first element of the input data.\n   - Iterate through the remaining elements of the input data, updating each element using the EMA formula: \\(s_k = (1 - \\alpha) \\cdot s_{k-1} + \\alpha \\cdot x_k\\).\n   - Round each element of the resulting smoothed list to four decimal places.\n\n### Solution Code\n```python\nfrom typing import List\n\ndef exp_smoothing(data: List[float], alpha: float) -> List[float] | int:\n    \"\"\"Compute the exponential moving average of a numeric sequence.\n\n    Parameters\n    ----------\n    data : List[float]\n        Sequence of numeric observations.\n    alpha : float\n        Smoothing factor (0 \u2264 alpha \u2264 1).\n\n    Returns\n    -------\n    List[float] | int\n        List of smoothed values rounded to 4 decimal places, or -1 for\n        invalid input.\n    \"\"\"\n    # Check for invalid inputs\n    if not data or alpha < 0 or alpha > 1:\n        return -1\n\n    n = len(data)\n    \n    # Handle alpha = 0 case\n    if alpha == 0:\n        first = round(data[0], 4)\n        return [first] * n\n    \n    # Handle alpha = 1 case\n    if alpha == 1:\n        return [round(x, 4) for x in data]\n    \n    # General case for 0 < alpha < 1\n    smoothed = [data[0]]\n    for x in data[1:]:\n        next_s = (1 - alpha) * smoothed[-1] + alpha * x\n        smoothed.append(next_s)\n    \n    # Round all values to four decimal places\n    return [round(num, 4) for num in smoothed]\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the input list is empty or if \\(\\alpha\\) is outside the valid range. If either condition is true, it returns -1.\n- **Special Cases**:\n  - When \\(\\alpha = 0\\), the output list is created by repeating the first element of the input list, rounded to four decimal places.\n  - When \\(\\alpha = 1\\), the output list is simply the input list with each element rounded to four decimal places.\n- **General Case**: For values of \\(\\alpha\\) between 0 and 1, the function iteratively computes each smoothed value using the EMA formula. Each computed value is then rounded to four decimal places to form the final result list.\n\nThis approach efficiently handles all edge cases and ensures the solution is both correct and optimal."}
{"task_id": 150, "completion_id": 0, "completion": "To solve this problem, we need to find the shortest path in a grid from the top-left corner to the bottom-right corner using the A* search algorithm with the Manhattan distance heuristic. The grid contains free cells (0) and blocked cells (1), and movement is allowed in four cardinal directions.\n\n### Approach\n1. **Initial Checks**: Verify if the start or goal cells are blocked. If either is blocked, return -1 immediately.\n2. **Data Structures**: Use a priority queue (min-heap) to manage nodes to be processed, ordered by the sum of the current path cost and heuristic value (f = g + h). Track the path cost (g) and parent pointers for each cell to reconstruct the path once the goal is reached.\n3. **A* Algorithm**: Process nodes from the priority queue, expanding to valid neighbors (up, down, left, right). For each neighbor, calculate the tentative path cost and update if a cheaper path is found. Use the Manhattan distance heuristic to prioritize nodes closer to the goal.\n4. **Path Reconstruction**: Once the goal is reached, backtrack using parent pointers to construct the path from goal to start, then reverse it to get the correct order.\n\n### Solution Code\n```python\nfrom heapq import heappush, heappop\nfrom typing import List, Tuple, Union\nimport math\n\ndef find_shortest_path(grid: List[List[int]]) -> Union[int, List[Tuple[int, int]]]:\n    \"\"\"Find a shortest path from the top-left to the bottom-right corner of a maze.\"\"\"\n    m = len(grid)\n    n = len(grid[0]) if m > 0 else 0\n    \n    # Check if the grid is empty (though problem states it's non-empty)\n    if m == 0 or n == 0:\n        return -1\n    \n    start = (0, 0)\n    goal = (m - 1, n - 1)\n    \n    # Check if start or goal is blocked\n    if grid[start[0]][start[1]] == 1 or grid[goal[0]][goal[1]] == 1:\n        return -1\n    \n    # Early exit if start is the goal\n    if start == goal:\n        return [start]\n    \n    # Initialize g_scores and parent matrix\n    g_scores = [[math.inf for _ in range(n)] for _ in range(m)]\n    g_scores[start[0]][start[1]] = 0\n    parent = [[None for _ in range(n)] for _ in range(m)]\n    \n    # Directions: up, down, left, right\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    # Priority queue (f, g, row, col)\n    heap = []\n    start_h = (m - 1 - start[0]) + (n - 1 - start[1])\n    heappush(heap, (start_h + 0, 0, start[0], start[1]))\n    \n    while heap:\n        f_current, g_current, row, col = heappop(heap)\n        \n        # Check if current node is the goal\n        if (row, col) == goal:\n            # Reconstruct the path\n            path = []\n            current = (row, col)\n            while current is not None:\n                path.append(current)\n                current = parent[current[0]][current[1]]\n            path.reverse()\n            return path\n        \n        # Skip if a better path has already been found\n        if g_current > g_scores[row][col]:\n            continue\n        \n        # Explore all neighbors\n        for dr, dc in directions:\n            new_row = row + dr\n            new_col = col + dc\n            # Check if the neighbor is valid and not blocked\n            if 0 <= new_row < m and 0 <= new_col < n and grid[new_row][new_col] == 0:\n                tentative_g = g_current + 1\n                # Check if this path to neighbor is better\n                if tentative_g < g_scores[new_row][new_col]:\n                    g_scores[new_row][new_col] = tentative_g\n                    parent[new_row][new_col] = (row, col)\n                    h = (m - 1 - new_row) + (n - 1 - new_col)\n                    f = tentative_g + h\n                    heappush(heap, (f, tentative_g, new_row, new_col))\n    \n    # If no path exists\n    return -1\n```\n\n### Explanation\n1. **Initial Checks**: The code first checks if the start or goal cells are blocked and returns -1 if they are.\n2. **Priority Queue Initialization**: The priority queue is initialized with the start node, using the Manhattan distance heuristic to prioritize node exploration.\n3. **Processing Nodes**: Nodes are processed from the priority queue, expanding to valid neighbors. For each neighbor, the tentative path cost is calculated. If this cost is lower than the previously recorded cost, the neighbor is updated and added to the queue.\n4. **Path Reconstruction**: Once the goal is reached, the path is reconstructed by backtracking from the goal to the start using parent pointers and reversing the result to get the correct order.\n\nThis approach ensures that the shortest path is found efficiently using the A* algorithm with an admissible heuristic, guaranteeing optimality."}
{"task_id": 151, "completion_id": 0, "completion": "To solve this problem, we need to implement four common activation functions (sigmoid, tanh, relu, softmax) and their derivatives using NumPy. These functions will be wrapped in a helper function that computes both the activation output and the gradient during the backward pass, which is essential for neural network training.\n\n### Approach\n1. **Activation Functions**:\n    - **Sigmoid**: Computes the sigmoid function using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n    - **Tanh**: Uses the built-in NumPy `tanh` function.\n    - **ReLU**: Implements the rectified linear unit function using `np.maximum(0, X)`.\n    - **Softmax**: Computes the softmax function along the last axis in a numerically stable manner by subtracting the maximum value before exponentiation to avoid overflow.\n\n2. **Derivatives**:\n    - **Sigmoid Derivative**: Uses the precomputed activation output to compute the derivative as \\( \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) \\).\n    - **Tanh Derivative**: Computed as \\( 1 - \\tanh^2(x) \\) using the activation output.\n    - **ReLU Derivative**: The derivative is 1 where the activation output is positive and 0 otherwise.\n    - **Softmax Derivative**: Returns an array of ones, as the true Jacobian is handled with the loss function in practice.\n\n3. **Helper Function**:\n    - Computes the activation output and the local gradient using the specified activation function.\n    - Multiplies the local gradient with the upstream gradient to get the final gradient.\n    - Rounds both the activation output and the gradient to 4 decimal places for consistency.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef activation_forward_backward(X: np.ndarray, activation: str, upstream_grad: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Apply an activation function and compute its local gradient.\"\"\"\n\n    activation = activation.lower()\n    A = None\n    dA = None\n\n    if activation == \"sigmoid\":\n        A = 1 / (1 + np.exp(-X))\n        dA = A * (1 - A)\n    elif activation == \"tanh\":\n        A = np.tanh(X)\n        dA = 1 - A**2\n    elif activation == \"relu\":\n        A = np.maximum(0, X)\n        dA = (A > 0).astype(np.float64)\n    elif activation == \"softmax\":\n        max_X = np.max(X, axis=-1, keepdims=True)\n        exp_X = np.exp(X - max_X)\n        A = exp_X / np.sum(exp_X, axis=-1, keepdims=True)\n        dA = np.ones_like(A)\n    else:\n        raise ValueError(f\"Unsupported activation function: {activation}\")\n\n    grad = upstream_grad * dA\n    return (np.round(A, 4), np.round(grad, 4))\n```\n\n### Explanation\n- **Activation Functions**:\n  - **Sigmoid**: Transforms input values to a range between 0 and 1, which is useful for binary classification.\n  - **Tanh**: Similar to sigmoid but outputs values between -1 and 1, making it useful for hidden layers.\n  - **ReLU**: Efficiently introduces non-linearity by outputting the input directly if positive, otherwise zero, which helps mitigate the vanishing gradient problem.\n  - **Softmax**: Converts logits to probabilities along the last axis, ensuring numerical stability by subtracting the maximum value before exponentiation.\n\n- **Derivatives**:\n  - Each derivative is computed using the activation output to leverage precomputed values, improving efficiency.\n  - For softmax, the derivative is simplified to an array of ones, as the true gradient is typically handled in conjunction with the loss function during backpropagation.\n\n- **Helper Function**:\n  - Encapsulates both forward and backward passes, making it easy to integrate into neural network layers.\n  - Ensures numerical results are rounded to 4 decimal places for consistency and readability.\n\nThis approach efficiently computes both the activation outputs and gradients, adhering to the constraints and requirements specified in the problem."}
{"task_id": 152, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that calculates the accuracy score of a classifier's predictions using only NumPy. The accuracy score is the proportion of correctly classified samples, and the function must handle both Python lists and NumPy arrays as inputs while adhering to specific constraints.\n\n### Approach\n1. **Input Conversion**: Convert the input lists or arrays into NumPy arrays to handle both types uniformly.\n2. **Validation**: Check if the input arrays are empty or have different lengths. If either condition is met, return -1.\n3. **Correct Predictions Calculation**: Use NumPy to compare the true labels with the predicted labels element-wise and sum the correct predictions.\n4. **Accuracy Calculation**: Compute the accuracy by dividing the number of correct predictions by the total number of samples.\n5. **Rounding**: Round the computed accuracy to four decimal places using Python's built-in `round` function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Calculate the proportion of correctly classified samples.\n\n    Parameters\n    ----------\n    y_true : list | numpy.ndarray\n        The true class labels.\n    y_pred : list | numpy.ndarray\n        The predicted class labels.\n\n    Returns\n    -------\n    float | int\n        The accuracy rounded to four decimal places. If the two inputs do\n        not have the same length or are empty, the function returns -1.\n    \"\"\"\n    # Convert inputs to numpy arrays for uniform handling\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    # Check for invalid inputs: empty arrays or differing lengths\n    if y_true.size == 0 or y_pred.size == 0 or len(y_true) != len(y_pred):\n        return -1\n    \n    # Calculate the number of correct predictions\n    correct = np.sum(y_true == y_pred)\n    \n    # Compute the accuracy and round to four decimal places\n    accuracy = correct / len(y_true)\n    return round(accuracy, 4)\n```\n\n### Explanation\n1. **Input Conversion**: The function converts both `y_true` and `y_pred` into NumPy arrays using `np.asarray()`, ensuring that whether the inputs are lists or arrays, they are handled consistently.\n2. **Validation**: The function checks if either array is empty (using `.size == 0`) or if their lengths differ. If any of these conditions are true, it returns -1 immediately.\n3. **Correct Predictions**: By using NumPy's element-wise comparison (`y_true == y_pred`), the function generates a boolean array where `True` indicates a correct prediction. Summing this array gives the total number of correct predictions.\n4. **Accuracy Calculation**: The accuracy is computed by dividing the number of correct predictions by the total number of samples (length of `y_true`).\n5. **Rounding**: The result is rounded to four decimal places using Python's `round` function to meet the specified precision requirement.\n\nThis approach ensures that the function efficiently handles both list and array inputs, validates edge cases, and computes the accuracy score accurately while adhering to the problem constraints."}
{"task_id": 154, "completion_id": 0, "completion": "To solve this problem, we need to compute the Radial Basis Function (RBF) kernel matrix between two sets of vectors using NumPy. The RBF kernel matrix is defined using pairwise squared Euclidean distances and an exponential decay controlled by a parameter gamma. \n\n### Approach\n1. **Input Handling**: Convert input arrays to 2-dimensional if they are 1-dimensional to handle both cases uniformly.\n2. **Squared Euclidean Distance Calculation**: Efficiently compute the pairwise squared Euclidean distances using matrix operations. This involves expanding the squared distance formula into a sum of squares and dot products.\n3. **Kernel Matrix Computation**: Apply the exponential function scaled by gamma to the computed distances.\n4. **Rounding and Conversion**: Round each element of the resulting matrix to 6 decimal places and convert the matrix to a nested Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float = 0.1) -> list[list[float]]:\n    \"\"\"Return the RBF kernel matrix between two sets of vectors.\n\n    Args:\n        X: First input array of shape (n_samples, n_features) or (n_features,).\n        Y: Second input array of shape (m_samples, n_features) or (n_features,).\n        gamma: Positive scalar controlling the width of the kernel (default 0.1).\n\n    Returns:\n        A nested Python list containing the RBF kernel matrix rounded to six\n        decimal places.\n    \"\"\"\n    # Ensure X and Y are 2-dimensional arrays\n    X = np.atleast_2d(X)\n    Y = np.atleast_2d(Y)\n    \n    # Compute squared Euclidean distances using the expanded formula\n    X_sq = np.sum(X**2, axis=1).reshape(-1, 1)\n    Y_sq = np.sum(Y**2, axis=1).reshape(1, -1)\n    squared_distances = X_sq + Y_sq - 2 * np.dot(X, Y.T)\n    \n    # Compute the RBF kernel matrix\n    K = np.exp(-gamma * squared_distances)\n    \n    # Round each entry to 6 decimal places and convert to a nested list\n    return np.round(K, 6).tolist()\n```\n\n### Explanation\n1. **Input Handling**: Using `np.atleast_2d`, we convert 1-dimensional input arrays to 2-dimensional arrays to ensure consistent handling of samples.\n2. **Squared Euclidean Distance Calculation**: By expanding the squared Euclidean distance formula into a sum of squares and dot products, we efficiently compute the pairwise distances using matrix operations. This avoids explicit loops and leverages NumPy's optimized operations for better performance.\n3. **Kernel Matrix Computation**: The exponential decay term is applied to the computed distances using the parameter gamma, resulting in the RBF kernel matrix.\n4. **Rounding and Conversion**: The resulting matrix elements are rounded to 6 decimal places using `np.round` and converted to a nested Python list using `tolist()` for the required output format.\n\nThis approach efficiently handles both 1-dimensional and 2-dimensional inputs, computes the necessary distances using optimized matrix operations, and ensures the result is formatted as specified."}
{"task_id": 155, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward pass of an actor network commonly used in reinforcement learning. This network processes an environment state through a series of linear transformations and activation functions to produce an action vector within the range [-1, 1]. The network architecture consists of three linear layers with ReLU activation functions after the first two layers and a tanh activation function after the final layer.\n\n### Approach\n1. **Convert Input State**: Convert the input state into a NumPy array to facilitate matrix operations.\n2. **First Linear Layer**: Apply the first linear transformation using weights W1 and biases b1, followed by a ReLU activation.\n3. **Second Linear Layer**: Apply the second linear transformation using weights W2 and biases b2, followed by another ReLU activation.\n4. **Third Linear Layer**: Apply the third linear transformation using weights W3 and biases b3.\n5. **Tanh Activation**: Apply the tanh activation function to the output of the third linear layer to ensure the action values are within the range [-1, 1].\n6. **Rounding and Conversion**: Round the resulting action values to four decimal places and convert the NumPy array to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    x = np.asarray(state)\n    # First layer: Linear + ReLU\n    x = np.dot(x, weights['W1']) + weights['b1']\n    x = np.maximum(x, 0)\n    # Second layer: Linear + ReLU\n    x = np.dot(x, weights['W2']) + weights['b2']\n    x = np.maximum(x, 0)\n    # Third layer: Linear + tanh\n    x = np.dot(x, weights['W3']) + weights['b3']\n    x = np.tanh(x)\n    # Round to 4 decimal places and convert to list\n    return np.round(x, 4).tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input state is converted to a NumPy array to handle both list and array inputs uniformly.\n2. **Linear Transformations**: Each linear layer applies a matrix multiplication (dot product) followed by the addition of a bias vector. This is done using the respective weights (W1, W2, W3) and biases (b1, b2, b3) from the provided dictionary.\n3. **ReLU Activation**: The ReLU function (max(0, x)) is applied element-wise after the first and second linear layers to introduce non-linearity.\n4. **Tanh Activation**: The tanh function is applied to the output of the third linear layer to squash the values into the range [-1, 1], which is suitable for action outputs in many reinforcement learning tasks.\n5. **Rounding and Conversion**: The final action values are rounded to four decimal places for precision and converted to a Python list to meet the output requirements.\n\nThis approach efficiently processes the input state through the defined network architecture using NumPy operations, ensuring both correctness and performance."}
{"task_id": 157, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that calculates the accuracy score of a classification model by comparing the true labels with the predicted labels. The function must handle special cases such as different lengths of input containers and empty inputs appropriately.\n\n### Approach\n1. **Check Lengths**: First, verify if the lengths of the two input containers (true labels and predicted labels) are different. If they are, return -1 as specified.\n2. **Handle Empty Containers**: If both containers are empty (checked after confirming they have the same length), return 0 since there are no samples to evaluate.\n3. **Count Correct Predictions**: Iterate through each pair of corresponding elements in the true and predicted labels. Count how many times they match.\n4. **Calculate Accuracy**: Compute the accuracy by dividing the number of correct predictions by the total number of samples. Round the result to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Return the classification accuracy between *y_true* and *y_pred*.\n\n    The function must:\n      \u2022 return \u20111 if the two containers have different lengths;\n      \u2022 return 0 when both containers are empty;\n      \u2022 otherwise return the proportion of matching elements rounded to\n        4 decimal places.\n\n    Args:\n        y_true (list | tuple): Ground-truth class labels.\n        y_pred (list | tuple): Predicted class labels.\n\n    Returns:\n        float | int: The accuracy or \u20111 according to the rules above.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    if not y_true:  # Both are empty since lengths are equal\n        return 0\n    correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n    accuracy = correct / len(y_true)\n    return round(accuracy, 4)\n```\n\n### Explanation\n1. **Check Lengths**: The function first checks if the lengths of `y_true` and `y_pred` are different. If they are, it returns -1 immediately.\n2. **Handle Empty Containers**: If the lengths are the same and both containers are empty, the function returns 0 as there are no samples to evaluate.\n3. **Count Correct Predictions**: Using a generator expression and the `zip` function, the function counts how many elements in `y_true` and `y_pred` match at each position.\n4. **Calculate and Round Accuracy**: The accuracy is calculated by dividing the count of correct predictions by the total number of samples. The result is then rounded to four decimal places using Python's `round` function to ensure the output meets the specified precision.\n\nThis approach efficiently handles all edge cases and ensures the function returns the correct values as specified in the problem statement."}
{"task_id": 158, "completion_id": 0, "completion": "To solve this problem, we need to implement the closed-form solution for Ridge Regression with the option of adding an intercept term that is not regularized. The solution involves matrix operations and careful handling of the regularization matrix to ensure the intercept term is not penalized.\n\n### Approach\n1. **Convert Input to NumPy Arrays**: Convert the input feature matrix `X` and target vector `y` into NumPy arrays to facilitate matrix operations.\n2. **Add Intercept Term**: If the `fit_intercept` flag is True, prepend a column of ones to the feature matrix `X` to account for the intercept term.\n3. **Construct Regularization Matrix**: Create a diagonal matrix `A` where the diagonal elements are set to the regularization strength `alpha`, except for the first element which is set to 0 if an intercept is included.\n4. **Compute Closed-Form Solution**: Calculate the weight vector using the formula \\((X^T X + A)^{-1} X^T y\\), which involves matrix inversion and multiplication.\n5. **Round and Return Results**: Round the resulting coefficients to four decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_regression(X: list[list[int | float]],\n                     y: list[int | float],\n                     alpha: float = 1.0,\n                     fit_intercept: bool = True) -> list[float]:\n    \"\"\"Compute Ridge Regression coefficients.\"\"\"\n    X_np = np.array(X, dtype=np.float64)\n    y_np = np.array(y, dtype=np.float64)\n    \n    if fit_intercept:\n        ones = np.ones((X_np.shape[0], 1))\n        X_np = np.hstack([ones, X_np])\n    \n    K = X_np.shape[1]\n    \n    if fit_intercept:\n        diag = np.zeros(K)\n        diag[1:] = alpha\n    else:\n        diag = alpha * np.ones(K)\n    \n    A = np.diag(diag)\n    XtX = X_np.T @ X_np\n    XtX_plus_A = XtX + A\n    inv_XtXA = np.linalg.inv(XtX_plus_A)\n    beta = inv_XtXA @ (X_np.T @ y_np)\n    \n    beta_rounded = np.round(beta, 4)\n    return beta_rounded.tolist()\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: The feature matrix `X` and target vector `y` are converted to NumPy arrays to leverage efficient matrix operations.\n- **Intercept Handling**: If `fit_intercept` is True, a column of ones is added to the feature matrix to account for the intercept term. This ensures the intercept is included in the model without regularization.\n- **Regularization Matrix**: The diagonal matrix `A` is constructed such that the intercept term (if present) is not regularized (diagonal element set to 0), while other features are regularized with strength `alpha`.\n- **Matrix Operations**: The closed-form solution is computed using matrix inversion and multiplication, which is efficient and concise using NumPy's linear algebra functions.\n- **Rounding**: The resulting coefficients are rounded to four decimal places to meet the problem's output requirements.\n\nThis approach efficiently computes the Ridge Regression coefficients while correctly handling the intercept term and regularization, ensuring both numerical stability and correctness as per the problem constraints."}
{"task_id": 159, "completion_id": 0, "completion": "To solve this problem, we need to compute the log probability density of a multivariate Gaussian distribution given a sample vector, a mean vector, and a covariance matrix. This involves using efficient numerical methods to ensure stability and accuracy.\n\n### Approach\n1. **Compute the Difference Vector**: Subtract the mean vector from the sample vector to get the difference vector.\n2. **Solve Linear System**: Use the covariance matrix to solve a linear system and avoid explicitly inverting the matrix, which is more efficient and numerically stable.\n3. **Quadratic Term Calculation**: Compute the quadratic form using the solution from the linear system and the difference vector.\n4. **Log Determinant Calculation**: Compute the log determinant of the covariance matrix using a numerically stable method.\n5. **Combine Terms**: Combine all terms according to the formula for the log probability density, including the dimension-adjusted constant term.\n6. **Round Result**: Return the result rounded to four decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef log_gaussian_pdf(x_i: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> float:\n    \"\"\"Compute the log probability density of a multivariate Gaussian.\n\n    The function implements the formula\n        log N(x | \u00b5, \u03a3) = -0.5 * [ d * ln(2\u03c0) + ln det \u03a3 + (x-\u00b5)^T \u03a3\u207b\u00b9 (x-\u00b5) ].\n\n    Args:\n        x_i (np.ndarray): 1-D array representing the sample vector (shape (d,)).\n        mu (np.ndarray): 1-D array representing the mean vector (shape (d,)).\n        sigma (np.ndarray): 2-D array representing the covariance matrix (shape (d,d)).\n\n    Returns:\n        float: The log probability, rounded to 4 decimal places.\n    \"\"\"\n    diff = x_i - mu\n    y = np.linalg.solve(sigma, diff)\n    quadratic_term = np.dot(diff, y)\n    sign, logdet = np.linalg.slogdet(sigma)\n    ln_det = logdet\n    d = x_i.size\n    log_p = -0.5 * (d * np.log(2 * np.pi) + ln_det + quadratic_term\n    return round(log_p, 4)\n```\n\n### Explanation\n1. **Difference Vector**: The difference between the sample vector and the mean vector is computed to center the data around the mean.\n2. **Linear System Solution**: Using `numpy.linalg.solve`, we solve the system defined by the covariance matrix and the difference vector. This avoids matrix inversion, enhancing numerical stability.\n3. **Quadratic Term**: The dot product of the difference vector and the solution vector gives the quadratic term in the log probability formula.\n4. **Log Determinant**: The log determinant of the covariance matrix is computed using `numpy.linalg.slogdet`, which is robust for positive-definite matrices.\n5. **Dimension Constant**: The term involving the dimension `d` and the natural logarithm of `2\u03c0` is computed to adjust for the dimensionality of the data.\n6. **Combine Terms**: All terms are combined according to the formula, scaled by `-0.5`, and rounded to four decimal places to ensure precision.\n\nThis approach efficiently and accurately computes the log probability density using numerical best practices, ensuring stability and correctness for multivariate Gaussian distributions."}
{"task_id": 160, "completion_id": 0, "completion": "To solve this problem, we need to implement Lasso regression using coordinate descent. Lasso regression is a linear regression technique that includes L1 regularization to prevent overfitting and encourage sparsity in the model coefficients. The coordinate descent algorithm iteratively updates each coefficient while keeping others fixed, which is efficient and converges well for Lasso.\n\n### Approach\n1. **Add Intercept Column**: Depending on whether an intercept is needed, we add a column of ones (if `fit_intercept` is True) or zeros (if False) to the design matrix. This intercept term is not regularized.\n2. **Initialize Weights**: All weights, including the intercept, are initialized to zero.\n3. **Coordinate Descent Loop**: For each iteration up to `max_iters`:\n   - **Update Intercept**: If an intercept is being fitted, it is recomputed as the mean of the residuals from the current model.\n   - **Iterate Over Features**: For each feature (excluding the intercept), temporarily set its weight to zero, compute the partial residual, update the weight using soft-thresholding to apply L1 regularization, and then update the weight in the model.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\"\"\"\n    m, n_original = X.shape\n    \n    # Add intercept column (all ones or zeros)\n    if fit_intercept:\n        intercept_col = np.ones((m, 1))\n    else:\n        intercept_col = np.zeros((m, 1))\n    X_new = np.hstack((intercept_col, X))\n    \n    n_new = X_new.shape[1]\n    w = np.zeros(n_new)\n    m = X_new.shape[0]  # Number of samples\n    \n    for _ in range(max_iters):\n        if fit_intercept:\n            # Compute residuals without intercept contribution and update intercept\n            residuals = y - X_new[:, 1:] @ w[1:]\n            w[0] = np.mean(residuals)\n        \n        # Coordinate descent over each feature weight (excluding intercept)\n        for j in range(1, n_new):\n            old_wj = w[j]\n            w[j] = 0.0  # Temporarily set current weight to zero\n            r_j = y - X_new @ w  # Residual with current weight zeroed\n            xj = X_new[:, j]\n            rho_j = xj.dot(r_j)\n            sum_xj_sq = xj.dot(xj)\n            \n            if sum_xj_sq == 0.0:\n                w_j_new = 0.0\n            else:\n                tau = lambda_param * m\n                soft_val = np.sign(rho_j) * np.maximum(np.abs(rho_j) - tau, 0.0)\n                w_j_new = soft_val / sum_xj_sq\n            \n            w[j] = w_j_new  # Update weight\n    \n    # Extract intercept and weights, rounding to four decimals\n    bias = round(float(w[0]), 4)\n    weights = [round(float(weight), 4) for weight in w[1:]]\n    \n    return (weights, bias)\n```\n\n### Explanation\n1. **Intercept Handling**: The intercept is handled by adding a column of ones (or zeros) to the design matrix. This allows the intercept to be treated as part of the weight vector but excluded from regularization.\n2. **Weight Initialization**: All weights are initialized to zero to start the optimization process from a simple baseline.\n3. **Coordinate Descent**: For each iteration, the intercept is updated first (if needed) by computing the mean residual. Each feature weight is then updated by temporarily setting it to zero, computing the residual without that feature's contribution, and applying the soft-thresholding operator to incorporate L1 regularization. This process ensures that each weight update considers the current model's residuals, leading to a sparse solution.\n\nThis approach efficiently optimizes the Lasso objective function using coordinate descent, ensuring both accuracy and sparsity in the resulting model."}
{"task_id": 161, "completion_id": 0, "completion": "To solve this problem, we need to implement a Factorization Machine (FM) model to predict binary class labels based on input features. The FM model incorporates both linear feature effects and pairwise feature interactions efficiently. The solution involves computing the prediction scores using these components and then converting the scores into binary labels.\n\n### Approach\n1. **Linear Term Calculation**: Compute the linear part of the prediction using the global bias \\( w_0 \\) and the linear weights \\( w \\).\n2. **Interaction Term Calculation**: Use the computational trick to efficiently compute the pairwise interactions using latent vectors \\( V \\). This involves matrix operations to avoid the quadratic complexity.\n3. **Score Calculation**: Combine the linear term and the interaction term to get the final prediction score for each sample.\n4. **Label Assignment**: Convert the scores into binary labels (1 or -1) based on whether the score is non-negative.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef factorization_machine_predict(X, w0, w, V):\n    \"\"\"Predict binary labels using Factorization Machine model.\"\"\"\n    # Compute the linear term including the global bias\n    linear_term = w0 + X.dot(w)\n    \n    # Compute the interaction term using the efficient trick\n    xv = X @ V  # Shape (m, k)\n    sum_square = (xv ** 2).sum(axis=1)  # Sum over the k factors\n    \n    # Compute the sum of squared features times sum of squared latent vectors\n    v_squared_sum = (V ** 2).sum(axis=1)  # Sum over k, shape (n,)\n    x_squared = X ** 2\n    sum_other = x_squared @ v_squared_sum  # Shape (m,)\n    \n    interaction_term = 0.5 * (sum_square - sum_other)\n    \n    # Total score is the sum of linear and interaction terms\n    scores = linear_term + interaction_term\n    \n    # Assign labels based on the score\n    labels = np.where(scores >= 0, 1, -1).tolist()\n    \n    return labels\n```\n\n### Explanation\n1. **Linear Term Calculation**: The linear term is computed by adding the global bias \\( w_0 \\) to the dot product of the input matrix \\( X \\) and the linear weights \\( w \\). This captures the individual feature effects.\n2. **Interaction Term Calculation**: \n   - Compute \\( XV \\) where \\( X \\) is the input matrix and \\( V \\) contains the latent vectors. This results in a matrix where each element represents the interaction of a feature with a latent factor.\n   - Compute the squared sum of these interactions for each sample.\n   - Compute the element-wise squared input matrix \\( X^2 \\) and the sum of squared latent vectors \\( V^2 \\). The dot product of these matrices gives the sum of squared interactions efficiently.\n   - The interaction term is derived using the difference between these two sums, scaled by 0.5.\n3. **Score Calculation**: The final score for each sample is the sum of the linear term and the interaction term.\n4. **Label Assignment**: Convert the scores to binary labels using a threshold (non-negative scores become 1, others become -1).\n\nThis approach efficiently computes the predictions in \\( O(nk) \\) time complexity, making it suitable for large datasets."}
{"task_id": 162, "completion_id": 0, "completion": "To solve this problem, we need to compute the cross-entropy loss and its gradient for a batch of predictions and one-hot encoded labels. The cross-entropy loss is a common loss function used in training neural networks for multi-class classification tasks. The gradient of this loss with respect to the softmax output is crucial for backpropagation.\n\n### Approach\n1. **Check Shape Compatibility**: Ensure that the predictions and labels have the same shape.\n2. **Compute Loss**: Add a small epsilon value to the predictions to avoid numerical instability when taking the logarithm. Use element-wise multiplication between the labels and the logarithm of the adjusted predictions, then sum these values and average over the batch size.\n3. **Compute Gradient**: The gradient of the loss with respect to the softmax output is derived from the difference between the predictions and the labels, scaled by the inverse of the batch size.\n4. **Numerical Rounding**: Both the loss and each element of the gradient are rounded to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy(Y_hat: np.ndarray, Y: np.ndarray, epsilon: float = 1e-20) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute average cross-entropy loss of a batch and its gradient.\"\"\"\n    assert Y_hat.shape == Y.shape, \"Shapes must match\"\n    \n    m = Y_hat.shape[0]  # Batch size\n    \n    # Compute the loss\n    Y_hat_eps = Y_hat + epsilon\n    element_wise = Y * np.log(Y_hat_eps)\n    total_loss = np.sum(element_wise)\n    loss = -total_loss / m\n    loss_rounded = round(loss.item(), 4)\n    \n    # Compute the gradient\n    gradient = (Y_hat - Y) / m\n    gradient_rounded = np.round(gradient, 4)\n    gradient_list = gradient_rounded.tolist()\n    \n    return (loss_rounded, gradient_list)\n```\n\n### Explanation\n1. **Shape Check**: The function starts by verifying that the input matrices `Y_hat` and `Y` have the same shape using an assertion. This ensures that the subsequent operations are valid.\n2. **Loss Calculation**: The predictions are adjusted by adding a small epsilon value to each element to prevent taking the logarithm of zero. The element-wise product of the labels and the logarithm of these adjusted predictions is computed, summed up, and then averaged over the batch size to get the loss. The loss is then rounded to four decimal places.\n3. **Gradient Calculation**: The gradient is derived from the difference between the predictions and the labels, divided by the batch size. Each element of the gradient matrix is rounded to four decimal places and converted to a nested list structure for the final output.\n\nThis approach efficiently computes both the loss and gradient using vectorized operations for numerical stability and performance, ensuring the results meet the specified precision requirements."}
{"task_id": 163, "completion_id": 0, "completion": "To solve this problem, we need to implement an Elastic Net regression model from scratch using NumPy. The model should handle polynomial feature expansion and fit the coefficients using either an analytical solution for Ridge regression (when L1 ratio is 0) or gradient descent for other cases. The solution should return predictions rounded to 4 decimal places.\n\n### Approach\n1. **Polynomial Feature Expansion**: Convert the input list `X` into a polynomial feature matrix up to the specified degree. This matrix includes a bias term (all ones) as the first column.\n2. **Analytical Solution for Ridge Regression**: When the L1 ratio (`l1_ratio`) is 0, use the normal equations with a Tikhonov regularization term that excludes the bias coefficient. This involves solving a linear system using the regularized matrix.\n3. **Gradient Descent for Elastic Net**: For non-zero L1 ratios, use batch gradient descent to minimize the Elastic Net cost function. The gradient includes contributions from both L1 and L2 penalties, with the L1 penalty using a sub-gradient approach where the gradient is zero when the coefficient is exactly zero.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: list[float], y: list[float], degree: int, reg_factor: float, l1_ratio: float, learning_rate: float, n_iterations: int) -> list[float]:\n    X_array = np.array(X)\n    y_array = np.array(y)\n    n_samples = len(X_array)\n    \n    # Create polynomial feature matrix\n    X_poly = np.vander(X_array, degree + 1, increasing=True)\n    \n    if l1_ratio == 0:\n        # Analytical solution for Ridge regression\n        n_features = X_poly.shape[1]\n        XtX = X_poly.T @ X_poly\n        # Create regularization matrix (diagonal with 0 for the bias term)\n        reg_matrix = np.eye(n_features)\n        reg_matrix[0, 0] = 0\n        reg_matrix *= reg_factor * (1 - l1_ratio) / 2\n        XtX_reg = XtX + reg_matrix\n        Xty = X_poly.T @ y_array\n        beta = np.linalg.solve(XtX_reg, Xty)\n    else:\n        # Batch gradient descent for Elastic Net\n        beta = np.zeros(X_poly.shape[1])\n        for _ in range(n_iterations):\n            y_hat = X_poly @ beta\n            error = y_hat - y_array\n            gradient_mse = (2.0 / n_samples) * X_poly.T @ error\n            # Compute regularization gradient\n            reg_grad = np.zeros_like(beta)\n            beta_rest = beta[1:]\n            if beta_rest.size > 0:\n                l1_grad = np.sign(beta_rest)\n                # Set to zero where beta_rest is zero\n                l1_grad[beta_rest == 0] = 0\n                reg_grad_rest = reg_factor * (l1_ratio * l1_grad + (1 - l1_ratio) * beta_rest)\n                reg_grad[1:] = reg_grad_rest\n            # Update beta\n            total_gradient = gradient_mse + reg_grad\n            beta -= learning_rate * total_gradient\n    \n    # Calculate predictions and round to 4 decimal places\n    predictions = X_poly @ beta\n    return [round(p, 4) for p in predictions.tolist()]\n```\n\n### Explanation\n1. **Polynomial Feature Matrix**: The input `X` is transformed into a matrix where each row consists of powers of the original feature up to the specified degree. This is done using `np.vander` with increasing powers.\n2. **Analytical Solution (Ridge Regression)**: When `l1_ratio` is 0, the problem reduces to Ridge regression. The coefficients are computed using the normal equations with a regularization matrix that penalizes all coefficients except the bias term.\n3. **Gradient Descent (Elastic Net)**: For non-zero `l1_ratio`, coefficients are updated iteratively using gradient descent. The gradient includes contributions from both the mean squared error, L1 penalty (using sub-gradients), and L2 penalty. The learning rate and number of iterations control the update steps.\n\nThis approach ensures that the model efficiently handles both Ridge and Elastic Net regression by leveraging analytical solutions where possible and iterative optimization otherwise, providing accurate predictions for polynomial features."}
{"task_id": 164, "completion_id": 0, "completion": "To solve this problem, we need to compute the binary classification log-loss (also known as cross-entropy loss) between actual labels and predicted probabilities. The log-loss is a measure of the accuracy of a classifier, where lower values indicate better predictions. The challenge is to handle edge cases where predicted probabilities might be exactly 0 or 1, which would make the logarithm undefined. \n\n### Approach\n1. **Clip Predictions**: First, we clip the predicted probabilities to a small interval around [0, 1] to avoid numerical instability. Specifically, we use a small value \u03b5 (1e-15) to clip the predictions to the interval [\u03b5, 1-\u03b5].\n2. **Vectorized Calculation**: Convert the input lists to NumPy arrays for efficient vectorized operations. Compute the log-loss using the formula for each element, leveraging vectorized operations to ensure efficiency.\n3. **Average Loss**: Compute the average log-loss across all samples and round the result to six decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_log_loss(actual, predicted):\n    \"\"\"Compute the binary classification log-loss (cross-entropy loss) between actual labels and predicted probabilities.\"\"\"\n    epsilon = 1e-15\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    predicted = np.clip(predicted, epsilon, 1 - epsilon)\n    loss = - (actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted)).mean()\n    return round(loss, 6)\n```\n\n### Explanation\n1. **Clipping Predictions**: The predicted probabilities are clipped to the interval [1e-15, 1 - 1e-15] to ensure that the logarithm function does not encounter invalid inputs (0 or 1).\n2. **Vectorized Operations**: By converting the inputs to NumPy arrays, we can efficiently compute the log-loss using vectorized operations, which are both concise and computationally efficient.\n3. **Log-Loss Formula**: For each sample, the log-loss is computed using the formula \\( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\). The negative sign is applied to the average of these values to get the final loss.\n4. **Rounding**: The result is rounded to six decimal places to meet the specified precision requirement.\n\nThis approach ensures that the solution is both efficient and robust to edge cases, providing accurate results even when predicted probabilities are at the boundaries of the valid interval."}
{"task_id": 165, "completion_id": 0, "completion": "To solve this problem, we need to compute the normalized Hamming distance between two 1-D integer vectors. The Hamming distance is the proportion of indices at which the corresponding elements of the vectors differ. If the vectors have different lengths, the function should return -1. Otherwise, the result is rounded to 4 decimal places.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the two vectors are the same. If not, return -1 immediately.\n2. **Handle Empty Vectors**: If both vectors are empty (length 0), return 0.0 since there are no elements to compare.\n3. **Convert to NumPy Arrays**: Convert both input vectors to NumPy arrays to facilitate efficient element-wise comparison.\n4. **Compute Differences**: Use NumPy to compute the element-wise differences between the two arrays and sum these differences.\n5. **Normalize and Round**: Normalize the sum of differences by the length of the vectors and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | np.ndarray, y: list[int] | np.ndarray) -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    \n    N = len(x)\n    if N == 0:\n        return 0.0\n    \n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    \n    sum_diff = np.sum(x_arr != y_arr)\n    normalized = sum_diff / N\n    \n    return round(normalized, 4)\n```\n\n### Explanation\n- **Length Check**: The function first checks if the input vectors have the same length. If not, it returns -1 immediately.\n- **Empty Vectors**: If both vectors are empty (length 0), the function returns 0.0 as there are no elements to compare.\n- **Conversion to NumPy Arrays**: Converting both vectors to NumPy arrays allows for efficient element-wise comparison using NumPy's vectorized operations.\n- **Element-wise Comparison**: Using NumPy's element-wise comparison (`x_arr != y_arr`), we generate a boolean array where `True` indicates differing elements. Summing this array gives the count of differing elements.\n- **Normalization and Rounding**: The count of differing elements is divided by the length of the vectors to get the normalized Hamming distance, which is then rounded to 4 decimal places for the final result.\n\nThis approach efficiently handles both Python lists and NumPy arrays, ensuring correctness and performance for various input sizes."}
{"task_id": 166, "completion_id": 0, "completion": "To solve this problem, we need to implement the classic Perceptron binary classifier from scratch. The Perceptron algorithm is a linear classifier that adjusts its weights and bias to find a separating hyperplane for the given training data. The goal is to predict labels for unseen test data based on the learned model.\n\n### Approach\n1. **Initialization**: Start with a weight vector `w` initialized to zeros and a bias term `b` initialized to zero. These parameters will be adjusted during training to find the optimal hyperplane.\n2. **Training Loop**: For a maximum of `max_iter` iterations, iterate through each training sample. For each sample, compute the signed margin. If the margin is non-positive (indicating misclassification), update the weights and bias.\n3. **Convergence Check**: If during a complete pass through the training data no misclassifications occur, stop training early as the model has converged.\n4. **Prediction**: After training, compute the scores for the test samples using the learned weights and bias. Convert these scores to predicted labels using the sign function, treating a score of zero as +1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_iter: int = 1000) -> list[int]:\n    \"\"\"Train a Perceptron on (X_train, y_train) and predict labels for X_test.\"\"\"\n    # Initialize weights and bias\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features)\n    b = 0.0\n    \n    # Training loop\n    for _ in range(max_iter):\n        errors = 0\n        for i in range(X_train.shape[0]):\n            x_i = X_train[i]\n            y_i = y_train[i]\n            margin = y_i * (np.dot(w, x_i) + b)\n            if margin <= 0:\n                w += y_i * x_i\n                b += y_i\n                errors += 1\n        if errors == 0:\n            break\n    \n    # Compute predictions for X_test\n    scores = X_test.dot(w) + b\n    predictions = np.where(scores >= 0, 1, -1).tolist()\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The weight vector `w` is initialized to zeros with the same number of features as the training data. The bias `b` is initialized to zero.\n2. **Training Loop**: The algorithm iterates up to `max_iter` times. For each iteration, it processes each training sample in sequence. If a sample is misclassified (indicated by a non-positive margin), the weights and bias are updated to correct the misclassification.\n3. **Convergence Check**: If no errors occur during a complete pass through the training data, the loop breaks early, indicating convergence.\n4. **Prediction**: The scores for test samples are computed using the dot product of the test data with the learned weights plus the bias. The sign of each score determines the predicted label, with zero scores treated as +1.\n\nThis approach ensures that the model efficiently learns a separating hyperplane and generalizes well to unseen test data."}
{"task_id": 167, "completion_id": 0, "completion": "To solve this problem, we need to implement the Binary Cross Entropy (BCE) loss and its gradient for binary classification tasks. The BCE loss is a common choice for training models like logistic regression and neural networks with binary outputs. The solution involves clipping predicted probabilities to avoid numerical instability, computing the loss, and then calculating the gradient as specified.\n\n### Approach\n1. **Clipping Predictions**: The predicted probabilities (y_hat) are clipped to a small interval around [\u03b5, 1-\u03b5] to prevent taking the logarithm of zero, which is undefined. This ensures numerical stability.\n2. **Compute BCE Loss**: The loss is calculated using the formula that averages the negative log probabilities over all samples, weighted by their true labels.\n3. **Compute Gradient**: The gradient of the loss with respect to the clipped predictions is derived using the given formula, which simplifies to (y_hat_clipped - y) / m, where m is the number of samples.\n4. **Rounding**: The computed loss and gradient values are rounded to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef binary_cross_entropy(y_hat, y, epsilon: float = 1e-10):\n    \"\"\"Compute the Binary Cross Entropy loss and its gradient.\n\n    Args:\n        y_hat (np.ndarray): Predicted probabilities in [0, 1].\n        y (np.ndarray): Ground-truth binary labels (0 or 1).\n        epsilon (float, optional): Small constant to avoid log(0). Defaults to 1e-10.\n\n    Returns:\n        tuple[float, list[float]]: The rounded BCE loss and the rounded gradient list.\n    \"\"\"\n    # Step 1: Clip the predicted probabilities to avoid log(0)\n    y_hat_clipped = np.clip(y_hat, epsilon, 1 - epsilon)\n    \n    # Step 2: Compute the BCE loss\n    m = y_hat.size\n    loss = -np.mean(y * np.log(y_hat_clipped) + (1 - y) * np.log(1 - y_hat_clipped))\n    \n    # Step 3: Compute the gradient\n    gradient = (y_hat_clipped - y) / m\n    \n    # Rounding to 4 decimal places\n    loss_rounded = np.round(loss, 4).item()\n    gradient_rounded = np.round(gradient, 4).tolist()\n    \n    return (loss_rounded, gradient_rounded)\n```\n\n### Explanation\n1. **Clipping Predictions**: The `np.clip` function ensures that all values in `y_hat` are within [\u03b5, 1-\u03b5], preventing division by zero or taking the logarithm of zero.\n2. **Loss Calculation**: The loss is computed by taking the element-wise logarithm of the clipped predictions and their complements, weighted by the true labels. The mean of these values is then taken and negated to form the BCE loss.\n3. **Gradient Calculation**: The gradient is derived from the difference between the clipped predictions and the true labels, scaled by the number of samples. This simplifies the gradient computation to a straightforward arithmetic operation.\n4. **Rounding**: The results are rounded to four decimal places to meet the specified precision requirements, ensuring consistency and readability in the output.\n\nThis approach efficiently combines numerical stability with mathematical correctness to provide both the BCE loss and its gradient in a form suitable for training binary classifiers."}
{"task_id": 168, "completion_id": 0, "completion": "To solve this problem, we need to implement a Gaussian Naive Bayes classifier from scratch using NumPy. The classifier will learn the parameters of the model from the training data and use these parameters to predict the class labels for unseen test samples.\n\n### Approach\n1. **Model Training**:\n   - **Class Separation**: Separate the training data into subsets based on their class labels.\n   - **Prior Probability Calculation**: For each class, compute the prior probability, which is the proportion of samples belonging to that class.\n   - **Mean and Variance Calculation**: For each feature within each class, compute the mean and variance. A small value (1e-6) is added to the variance to avoid numerical instability when the variance is zero.\n\n2. **Prediction**:\n   - **Log Probability Calculation**: For each test sample, compute the log probability for each class using the Gaussian probability density function. This involves summing the log probabilities of each feature under the Gaussian distribution parameters (mean and variance) of the class, and adding the log prior probability of the class.\n   - **Class Selection**: The class with the highest log probability is selected as the predicted class for the test sample.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]]) -> list[int]:\n    \"\"\"Gaussian Naive Bayes classifier.\"\"\"\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    classes = np.unique(y_train)\n    params = []\n    \n    for c in classes:\n        # Extract training data for class c\n        mask = (y_train == c)\n        X_c = X_train[mask]\n        n_c = X_c.shape[0]\n        \n        # Compute prior probability (log)\n        prior = np.log(n_c / X_train.shape[0])\n        \n        # Compute mean and variance for each feature\n        means = np.mean(X_c, axis=0)\n        variances = np.var(X_c, axis=0, ddof=0) + 1e-6  # Add epsilon to variance\n        \n        params.append((c, prior, means, variances))\n    \n    predictions = []\n    for x in X_test:\n        max_log_p = -np.inf\n        best_class = None\n        for c, prior, means, variances in params:\n            # Compute log likelihood\n            diff = x - means\n            exponent = (diff ** 2) / variances\n            log_terms = np.log(2 * np.pi * variances)\n            log_likelihood = -0.5 * np.sum(exponent + log_terms)\n            \n            # Compute log posterior\n            log_p = prior + log_likelihood\n            \n            # Update best class\n            if log_p > max_log_p:\n                max_log_p = log_p\n                best_class = c\n        predictions.append(best_class)\n    \n    return predictions\n```\n\n### Explanation\n1. **Data Conversion**: The input training and test data are converted into NumPy arrays for efficient numerical operations.\n2. **Class Separation**: The unique classes in the training labels are identified, and the training data is partitioned by these classes.\n3. **Parameter Calculation**:\n   - **Prior Probability**: Calculated as the log of the ratio of the number of samples in the class to the total number of samples.\n   - **Means and Variances**: For each feature in each class, the mean and variance are computed. A small value is added to the variance to ensure numerical stability.\n4. **Log Probability Calculation**: For each test sample, the log probability is computed for each class using the Gaussian distribution parameters. The sum of the log probabilities of each feature under the class distribution is combined with the log prior of the class.\n5. **Prediction**: The class with the highest log probability is selected as the predicted class for each test sample.\n\nThis approach ensures that the Gaussian Naive Bayes model is trained efficiently and makes predictions based on the learned parameters, providing a robust and simple classification mechanism."}
{"task_id": 169, "completion_id": 0, "completion": "To solve this problem, we need to generate a list of window coefficients for various symmetric window functions commonly used in digital signal processing. The goal is to implement a function that can generate these coefficients for specified window types, ensuring correct handling of edge cases and proper rounding of results.\n\n### Approach\n1. **Validation of Inputs**: \n   - Ensure the number of coefficients `N` is a positive integer.\n   - Handle the special case where `N = 1`, which always returns a single coefficient `[1.0]`.\n   - Validate the window type to ensure it is one of the supported types: \"hamming\", \"hann\", \"blackman_harris\", or \"generalized_cosine\".\n\n2. **Generate Coefficients**:\n   - For each window type, compute the coefficients using their respective mathematical formulas.\n   - Use vectorized operations with NumPy for efficient computation.\n   - For the \"generalized_cosine\" window, ensure that coefficients are provided and compute the sum of cosine terms as specified.\n\n3. **Rounding and Conversion**:\n   - Round each coefficient to 4 decimal places using NumPy's rounding function.\n   - Convert the result from a NumPy array to a Python list before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\"\"\"\n\n    # Check if N is a positive integer\n    if not isinstance(N, int) or N < 1:\n        raise ValueError(\"N must be a positive integer\")\n    \n    # Handle the special case where N is 1\n    if N == 1:\n        return [1.0]\n    \n    # Validate the window type\n    valid_windows = [\"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\"]\n    if window not in valid_windows:\n        raise ValueError(\"Unknown window type\")\n    \n    n = np.arange(N)\n    if window == \"hamming\":\n        arg = 2 * np.pi * n / (N - 1)\n        w = 0.54 - 0.46 * np.cos(arg)\n    elif window == \"hann\":\n        arg = 2 * np.pi * n / (N - 1)\n        w = 0.5 - 0.5 * np.cos(arg)\n    elif window == \"blackman_harris\":\n        a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n        arg = 2 * np.pi * n / (N - 1)\n        term1 = a1 * np.cos(arg)\n        term2 = a2 * np.cos(2 * arg)\n        term3 = a3 * np.cos(3 * arg)\n        w = a0 - term1 + term2 - term3\n    elif window == \"generalized_cosine\":\n        if coefficients is None:\n            raise ValueError(\"coefficients must be provided for generalized_cosine window\")\n        arg = 2 * np.pi * n / (N - 1)\n        w = np.zeros(N)\n        for k, a in enumerate(coefficients):\n            w += a * np.cos(k * arg)\n    \n    # Round to 4 decimal places and convert to Python list\n    rounded = np.round(w, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if `N` is a valid positive integer and handles the special case where `N = 1` directly. It then ensures the provided window type is valid.\n\n2. **Coefficient Calculation**: Depending on the window type, the function calculates the coefficients using the respective formula. For the \"generalized_cosine\" window, it checks if coefficients are provided and computes the sum of cosine terms as specified by the coefficients list.\n\n3. **Rounding and Conversion**: After computing the coefficients using efficient vectorized operations, the results are rounded to 4 decimal places and converted to a Python list to meet the output requirements.\n\nThis approach ensures that the function is both efficient and correct, handling all edge cases and providing accurate results as specified."}
{"task_id": 171, "completion_id": 0, "completion": "To solve this problem, we need to implement a binary Logistic Regression classifier using mini-batch Gradient Descent from scratch. The classifier should handle any two distinct numeric labels, add an intercept term to the data, and return predictions for the test set using a decision threshold of 0.5. If all training labels are the same, the classifier should predict that label for all test samples.\n\n### Approach\n1. **Handle Special Case**: Check if all training labels are the same. If so, return that label for all test samples.\n2. **Label Mapping**: Convert the original labels to {0, 1} for training and map predictions back to the original labels.\n3. **Data Preprocessing**: Add an intercept term (bias) to both training and test datasets.\n4. **Model Training**: Initialize weights and use mini-batch Gradient Descent to minimize the cross-entropy loss. Update weights iteratively using gradients computed from each mini-batch.\n5. **Prediction**: Compute probabilities for the test set using the trained weights, apply the decision threshold, and map predictions back to the original labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n    \"\"\"\n    # Handle the case where all training labels are identical\n    unique_labels = list(set(y_train))\n    if len(unique_labels) == 1:\n        return [unique_labels[0]] * len(X_test)\n    \n    # Prepare label mapping to {0, 1}\n    unique_labels = sorted(unique_labels)\n    label0, label1 = unique_labels\n    label_map = {label0: 0, label1: 1}\n    inverse_map = {0: label0, 1: label1}\n    y_internal = np.array([label_map[y] for y in y_train])\n    \n    # Add intercept term to training and test data\n    X_train_np = np.hstack([np.ones((len(X_train), 1)), np.array(X_train)])\n    X_test_np = np.hstack([np.ones((len(X_test), 1)), np.array(X_test)])\n    \n    # Initialize weights\n    weights = np.zeros(X_train_np.shape[1])\n    m = X_train_np.shape[0]\n    \n    # Mini-batch Gradient Descent\n    for epoch in range(epochs):\n        # Shuffle the training data\n        indices = np.random.permutation(m)\n        X_shuffled = X_train_np[indices]\n        y_shuffled = y_internal[indices]\n        \n        # Iterate over mini-batches\n        for i in range(0, m, batch_size):\n            X_batch = X_shuffled[i:i + batch_size]\n            y_batch = y_shuffled[i:i + batch_size]\n            \n            # Compute predictions and gradient\n            z = X_batch.dot(weights)\n            h = 1 / (1 + np.exp(-z))\n            error = h - y_batch\n            gradient = X_batch.T.dot(error) / len(X_batch)\n            \n            # Update weights\n            weights -= learning_rate * gradient\n    \n    # Predict on test data\n    z_test = X_test_np.dot(weights)\n    probabilities = 1 / (1 + np.exp(-z_test))\n    predictions_internal = (probabilities >= 0.5).astype(int)\n    \n    # Map predictions back to original labels\n    return [inverse_map[pred] for pred in predictions_internal.tolist()]\n```\n\n### Explanation\n1. **Special Case Handling**: If all training labels are identical, the function immediately returns that label for all test samples.\n2. **Label Conversion**: The original labels are mapped to {0, 1} for training. After predictions, results are mapped back to the original labels.\n3. **Intercept Addition**: A column of ones is added to the feature matrices to account for the bias term in the logistic regression model.\n4. **Model Training**: Weights are initialized and updated iteratively using mini-batch Gradient Descent. Each epoch involves shuffling the data and processing it in mini-batches to compute gradients and update weights.\n5. **Prediction**: The logistic function (sigmoid) is applied to the dot product of test features and weights. Predictions are thresholded at 0.5 and converted back to the original labels.\n\nThis approach ensures that the model efficiently learns the decision boundary using mini-batch Gradient Descent, handles various label sets, and provides accurate predictions for linearly separable data."}
{"task_id": 172, "completion_id": 0, "completion": "To solve this problem, we need to compute the derivative of the hyperbolic tangent (tanh) activation function element-wise. The derivative of tanh can be expressed as \\(1 - \\tanh^2(z)\\). The solution must handle scalar values, Python lists, and NumPy arrays, returning a float for scalar inputs and a list for non-scalar inputs, with all results rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: Check if the input is a scalar (either a Python scalar or a NumPy scalar). If it is, compute the derivative directly and return it as a float rounded to four decimal places.\n2. **Array Conversion**: For non-scalar inputs, convert the input to a NumPy array to handle both Python lists and NumPy arrays uniformly.\n3. **Derivative Calculation**: Compute the derivative using the formula \\(1 - \\tanh^2(z)\\) for each element in the array.\n4. **Rounding**: Round the results to four decimal places using NumPy's rounding function for consistency.\n5. **Result Conversion**: Convert the result to a Python list. If the original input was a 0-dimensional NumPy array, wrap the result in a list to ensure the output is always a list for non-scalar inputs.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef tanh_grad(z):\n    \"\"\"Derivative of the hyper-bolic tangent (tanh) activation.\n\n    The derivative is computed element-wise as 1 - tanh(z)**2.\n\n    Args:\n        z: A scalar, Python list or NumPy ndarray of floats/ints.\n\n    Returns:\n        float | list[float]: If `z` is scalar a float is returned, otherwise a\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    if np.isscalar(z):\n        derivative = 1 - np.tanh(z) ** 2\n        rounded = np.round(derivative, 4)\n        return float(rounded)\n    else:\n        arr = np.asarray(z)\n        derivative = 1 - np.tanh(arr) ** 2\n        rounded = np.round(derivative, 4)\n        result = rounded.tolist()\n        if arr.ndim == 0:\n            return [result]\n        else:\n            return result\n```\n\n### Explanation\n- **Scalar Handling**: The function first checks if the input is a scalar using `np.isscalar()`. If true, it computes the derivative, rounds it, and returns it as a float.\n- **Array Conversion**: For non-scalar inputs, the input is converted to a NumPy array to ensure uniform processing.\n- **Element-wise Computation**: The derivative is computed for each element using vectorized operations, which is efficient and concise.\n- **Rounding and Conversion**: The results are rounded using `np.round` and converted to a Python list. If the original input was a 0-dimensional array, the result is wrapped in a list to maintain the correct return type.\n\nThis approach ensures that the function handles all input types correctly and efficiently, leveraging NumPy's capabilities for numerical operations and ensuring the output format meets the specified requirements."}
{"task_id": 173, "completion_id": 0, "completion": "To solve this problem, we need to calculate the alphabetical distance between two strings of equal length. The alphabetical distance is defined as the sum of the absolute differences between the ASCII values of corresponding characters in the two strings. If the strings have different lengths, the function should return -1.\n\n### Approach\n1. **Check Lengths**: First, verify if the lengths of the candidate and target strings are equal. If they are not, return -1 immediately.\n2. **Calculate Distance**: If the lengths are equal, iterate over each pair of corresponding characters from the two strings. For each pair, compute the absolute difference of their ASCII values using the `ord()` function and accumulate these differences.\n3. **Return Result**: Sum all the computed differences to get the total alphabetical distance and return this sum.\n\n### Solution Code\n```python\ndef alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        The sum of absolute ASCII differences between corresponding\n        characters. If the strings differ in length, return -1.\n    \"\"\"\n    if len(candidate) != len(target):\n        return -1\n    total = 0\n    for c, t in zip(candidate, target):\n        total += abs(ord(c) - ord(t))\n    return total\n```\n\n### Explanation\n1. **Length Check**: The function first checks if the lengths of the candidate and target strings are the same. If not, it returns -1 as required by the problem constraints.\n2. **Iteration and Calculation**: Using the `zip` function, the code pairs each character from the candidate string with the corresponding character from the target string. For each pair, it calculates the absolute difference of their ASCII values and accumulates these differences in the `total` variable.\n3. **Result**: After processing all character pairs, the accumulated total is returned, which represents the alphabetical distance between the two strings.\n\nThis approach ensures that the function efficiently computes the desired distance while adhering to the problem constraints and handling edge cases such as empty strings appropriately."}
{"task_id": 174, "completion_id": 0, "completion": "To solve this problem, we need to calculate two metrics for a GAN discriminator: the categorical cross-entropy loss and the classification accuracy. These metrics are essential for evaluating how well the discriminator is performing in distinguishing between real and fake data.\n\n### Approach\n1. **Categorical Cross-Entropy Loss**: This loss measures the difference between the predicted probabilities and the true labels. The formula for categorical cross-entropy loss is the negative sum of the product of the true labels and the logarithm of the predicted probabilities, averaged over all samples. To avoid numerical instability (logarithm of zero), we clip the predicted probabilities to a small epsilon value (1e-12) before taking the logarithm.\n\n2. **Classification Accuracy**: This metric measures how often the discriminator's predictions match the true labels. The predicted class for each sample is the one with the highest probability. Accuracy is the proportion of correct predictions over all samples.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gan_discriminator_metrics(predictions: np.ndarray, labels: np.ndarray) -> tuple[float, float]:\n    \"\"\"Calculate categorical cross-entropy loss & accuracy for a GAN discriminator.\n\n    Parameters\n    ----------\n    predictions : np.ndarray\n        Soft-max probabilities with shape (n_samples, n_classes).\n    labels : np.ndarray\n        One-hot encoded true labels with the same shape as *predictions*.\n\n    Returns\n    -------\n    tuple[float, float]\n        (loss, accuracy) \u2013 both rounded to 4 decimal places.\n    \"\"\"\n    # Compute cross-entropy loss with clipping to avoid log(0)\n    epsilon = 1e-12\n    clipped_predictions = np.clip(predictions, epsilon, 1.0)\n    log_p = np.log(clipped_predictions)\n    sum_per_sample = np.sum(labels * log_p, axis=1)\n    loss = -np.mean(sum_per_sample)\n    \n    # Compute accuracy by comparing argmax of predictions and labels\n    predicted_classes = np.argmax(predictions, axis=1)\n    true_classes = np.argmax(labels, axis=1)\n    accuracy = np.mean(predicted_classes == true_classes)\n    \n    # Round both metrics to 4 decimal places\n    return (round(loss, 4), round(accuracy, 4))\n```\n\n### Explanation\n1. **Cross-Entropy Loss Calculation**:\n   - **Clipping**: The predicted probabilities are clipped to a small value (1e-12) to prevent taking the logarithm of zero, which avoids numerical instability.\n   - **Logarithm and Sum**: The logarithm of the clipped probabilities is computed, then multiplied element-wise by the true labels. This product is summed across classes for each sample.\n   - **Average Loss**: The negative mean of these sums gives the average cross-entropy loss.\n\n2. **Accuracy Calculation**:\n   - **Argmax**: The predicted class for each sample is determined by the index of the highest probability. Similarly, the true class is the index of the 1 in the one-hot encoded label.\n   - **Comparison**: The predicted and true classes are compared to compute the accuracy, which is the mean of correct predictions.\n\nThis approach ensures that both metrics are computed efficiently and robustly, handling potential numerical issues while providing clear and precise results."}
{"task_id": 175, "completion_id": 0, "completion": "To solve this problem, we need to compute the L2 regularization penalty for a given set of weights and a regularization strength coefficient. The L2 penalty is calculated by summing the squares of the weights and then multiplying by the regularization strength. The result is then rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: The function accepts weights as either a Python list or a 1-D NumPy array. This flexibility is managed by converting the input to a NumPy array, which allows for consistent processing regardless of the input type.\n2. **Sum of Squares Calculation**: Using NumPy's vectorized operations, we efficiently compute the sum of the squares of the weights.\n3. **Penty Calculation**: Multiply the sum of squares by the regularization strength coefficient \\( C \\).\n4. **Rounding**: The result is rounded to four decimal places using Python's built-in `round` function to ensure the output meets the specified precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n    \"\"\"Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: The penalty value rounded to 4 decimal places.\n    \"\"\"\n    w = np.array(weights)\n    sum_sq = np.sum(w ** 2)\n    penalty = C * sum_sq\n    return round(penalty, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input weights are converted to a NumPy array to handle both list and array inputs uniformly. This conversion also simplifies element-wise operations.\n- **Sum of Squares**: The expression `w ** 2` squares each element of the array, and `np.sum` computes the total sum of these squared values.\n- **Penalty Calculation**: The sum of squares is multiplied by the regularization strength \\( C \\) to compute the penalty.\n- **Rounding**: The result is rounded to four decimal places to meet the specified precision requirement. This ensures consistent formatting of the output regardless of minor floating-point inaccuracies.\n\nThis approach efficiently handles all edge cases, including empty weight vectors or all-zero weights, by naturally resulting in a sum of zero, which when multiplied by \\( C \\) (even if \\( C \\) is zero) yields zero. The use of NumPy ensures efficient and concise computation."}
{"task_id": 176, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. The goal is to train an ensemble of decision stumps on a training dataset and use them to predict class labels for a test dataset.\n\n### Approach\n1. **Convert Labels**: Convert the training labels from {0, 1} to {-1, 1} to align with the AdaBoost formulation.\n2. **Initialize Weights**: Start with uniform weights for all training samples.\n3. **Train Decision Stumps**: For each boosting round, find the best decision stump (feature, threshold, polarity) that minimizes the weighted classification error. The best stump is chosen based on the lowest error, with ties broken by feature index, threshold, and polarity.\n4. **Update Weights**: Adjust the sample weights based on the stump's performance, increasing weights for misclassified samples and decreasing weights for correctly classified samples.\n5. **Early Stopping**: If a perfect stump (zero weighted error) is found, stop training early.\n6. **Predict**: Aggregate the weighted votes of all stumps on the test data and convert the aggregated result to class labels {0, 1}.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    # Convert input to numpy arrays\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    n_samples, n_features = X_train.shape\n    # Convert labels from {0, 1} to {-1, 1}\n    y_converted = np.where(y_train == 1, 1, -1)\n    # Initialize sample weights\n    weights = np.ones(n_samples) / n_samples\n    stumps = []  # To store each stump's parameters and alpha\n    \n    for _ in range(n_estimators):\n        best_error = float('inf')\n        best_j = -1\n        best_threshold = None\n        best_polarity = None\n        best_h = None\n        \n        # Iterate over each feature to find the best stump\n        for j in range(n_features):\n            unique_values = np.unique(X_train[:, j])\n            for threshold in unique_values:\n                for polarity in [1, -1]:\n                    # Compute predictions for current stump\n                    if polarity == 1:\n                        h = np.where(X_train[:, j] < threshold, 1, -1)\n                    else:\n                        h = np.where(X_train[:, j] >= threshold, 1, -1)\n                    # Calculate weighted error\n                    error = np.sum(weights * (h != y_converted))\n                    # Check if current candidate is better\n                    current_candidate = (error, j, threshold, polarity)\n                    if current_candidate < (best_error, best_j, best_threshold, best_polarity):\n                        best_error, best_j, best_threshold, best_polarity = current_candidate\n                        best_h = h.copy()\n        \n        # If perfect stump found, add and break early\n        if best_error < 1e-10:\n            alpha = 0.5 * np.log((1.0 - best_error) / (best_error + 1e-10))\n            stumps.append((best_j, best_threshold, best_polarity, alpha))\n            break\n        \n        # Compute alpha for the best stump\n        alpha = 0.5 * np.log((1.0 - best_error) / (best_error + 1e-10))\n        # Update weights with the best_h\n        exponent = -alpha * y_converted * best_h\n        weights *= np.exp(exponent)\n        weights /= np.sum(weights)  # Normalize\n        # Save the stump parameters\n        stumps.append((best_j, best_threshold, best_polarity, alpha))\n        \n        # Check for early stopping after weight update\n        if best_error < 1e-10:\n            break\n    \n    # Generate predictions for X_test\n    predictions = []\n    for x in X_test:\n        sum_votes = 0.0\n        for j, threshold, polarity, alpha_t in stumps:\n            feature_val = x[j]\n            if polarity == 1:\n                h = 1 if (feature_val < threshold) else -1\n            else:\n                h = 1 if (feature_val >= threshold) else -1\n            sum_votes += alpha_t * h\n        pred = 1 if sum_votes > 0 else 0\n        predictions.append(pred)\n    \n    return predictions\n```\n\n### Explanation\n1. **Label Conversion**: Training labels are converted from {0, 1} to {-1, 1} to align with the AdaBoost formulation which uses -1 and 1 for weight updates.\n2. **Weight Initialization**: Each sample initially has equal weight, ensuring that all samples contribute equally to the first weak learner.\n3. **Stump Training**: For each boosting round, the algorithm iterates through each feature and possible threshold to find the best decision stump. The best stump is selected based on the lowest weighted error, with tie-breaking rules.\n4. **Weight Update**: After selecting the best stump, weights are updated to focus more on misclassified samples, allowing subsequent stumps to correct previous mistakes.\n5. **Early Stopping**: If a stump achieves zero training error, training stops early to avoid unnecessary computations.\n6. **Prediction**: The final prediction is made by aggregating the weighted votes of all stumps. The sign of the aggregated votes determines the class label (0 or 1).\n\nThis approach efficiently combines multiple weak learners to form a strong classifier, leveraging the AdaBoost algorithm's ability to focus on difficult samples through iterative weight updates."}
{"task_id": 177, "completion_id": 0, "completion": "To solve this problem, we need to compute the hyperbolic tangent (tanh) activation function for various input types, including scalars, lists, nested lists, and NumPy arrays. The result must be rounded to 4 decimal places and returned in the appropriate format based on the input type.\n\n### Approach\n1. **Input Handling**: \n   - If the input is a scalar (int or float), compute the tanh directly using the formula and return the result as a float rounded to 4 decimal places.\n   - If the input is a list or a NumPy array, convert it to a NumPy array (if necessary) to leverage vectorized operations for efficient computation.\n2. **Computation**:\n   - Use the formula for tanh, which involves exponentials of the input and its negative. This is computed element-wise for arrays.\n3. **Rounding and Conversion**:\n   - Round the result to 4 decimal places using NumPy's rounding function for consistency.\n   - Convert the result back to a Python list if the input was a list or array, using the `.tolist()` method to maintain the original structure.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import Union\n\ndef tanh(z: Union[int, float, np.ndarray, list]) -> Union[float, list]:\n    \"\"\"Compute the hyperbolic tangent of *z* without using ``numpy.tanh``.\n\n    The function must accept:\n        * A scalar ``int`` or ``float``\n        * A Python list (including nested lists)\n        * A ``numpy.ndarray`` of any shape\n\n    Every output value has to be rounded to 4 decimal places.\n\n    If *z* is a scalar return a single ``float``; otherwise return a Python\n    ``list`` obtained via ``numpy.ndarray.tolist``.\n\n    Args:\n        z: Numerical input (scalar, list, or np.ndarray).\n\n    Returns:\n        Numerical output with the same structure as the input, but rounded\n        to four decimals and converted to ``list`` if the input was not a\n        scalar.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        e_z = np.exp(z)\n        e_neg_z = np.exp(-z)\n        t = (e_z - e_neg_z) / (e_z + e_neg_z)\n        rounded = np.round(t, 4)\n        return float(rounded)\n    else:\n        if isinstance(z, list):\n            arr = np.array(z)\n        else:\n            arr = z\n        e_z = np.exp(arr)\n        e_neg_z = np.exp(-arr)\n        numerator = e_z - e_neg_z\n        denominator = e_z + e_neg_z\n        tanh_arr = numerator / denominator\n        rounded_tanh = np.round(tanh_arr, 4)\n        return rounded_tanh.tolist()\n```\n\n### Explanation\n1. **Scalar Input Handling**: \n   - If the input is a scalar (int or float), the function computes the exponentials of the input and its negative. It then applies the tanh formula, rounds the result to 4 decimal places using NumPy's rounding function, and returns it as a float.\n\n2. **List and Array Input Handling**:\n   - For lists or nested lists, the input is converted to a NumPy array to utilize vectorized operations. The same tanh formula is applied element-wise across the array. The result is rounded to 4 decimal places and converted back to a Python list (preserving nested structures if present) using `.tolist()`.\n\n3. **Consistency in Rounding**:\n   - Using NumPy's rounding function ensures consistent rounding behavior across all elements, whether the input is a scalar, list, or array. This avoids discrepancies between Python's built-in rounding and NumPy's rounding methods.\n\nThis approach efficiently handles all specified input types and ensures the results are correctly formatted and rounded as required."}
{"task_id": 178, "completion_id": 0, "completion": "To solve this problem, we need to implement the Particle Swarm Optimization (PSO) algorithm to minimize the Sphere function. The Sphere function is defined as the sum of the squares of each component of a vector, and we aim to find the vector that minimizes this function within the bounds of -1 to 1 for each component. The PSO algorithm uses a swarm of particles that iteratively update their positions and velocities based on their own best-known positions and the best-known position of the entire swarm.\n\n### Approach\n1. **Input Validation**: Check if the input dimensions, number of particles, and number of iterations are positive integers. If any of them are non-positive, return -1.\n2. **Initialization**: Use a random number generator (RNG) with a given seed to initialize the positions of particles uniformly within the bounds [-1, 1]. Initialize velocities to zero.\n3. **Personal and Global Best Tracking**: Track each particle's best-known position (personal best) and the best-known position across all particles (global best).\n4. **Iterative Updates**: For each iteration, update each particle's velocity and position. The velocity update considers the particle's inertia, cognitive component (movement towards its own best position), and social component (movement towards the swarm's best position). The position is clipped to stay within the bounds [-1, 1].\n5. **Evaluation and Update**: After each position update, evaluate the new position. Update the personal best if the new position is better. Update the global best if any particle's new position is better than the current global best.\n6. **Result**: Return the best value found, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\"\"\"\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1.0\n    \n    w = 0.5\n    c1 = 1.5\n    c2 = 1.5\n    \n    rng = np.random.default_rng(seed)\n    \n    # Initialize positions and velocities\n    positions = rng.uniform(low=-1, high=1, size=(num_particles, n_dims))\n    velocities = np.zeros((num_particles, n_dims))\n    \n    # Personal bests and global best\n    pbest_positions = positions.copy()\n    current_values = np.sum(positions ** 2, axis=1)\n    gbest_idx = np.argmin(current_values)\n    gbest_value = current_values[gbest_idx]\n    gbest_position = pbest_positions[gbest_idx].copy()\n    \n    for _ in range(num_iterations):\n        for i in range(num_particles):\n            # Generate random numbers for each dimension\n            r1 = rng.uniform(0, 1, size=n_dims)\n            r2 = rng.uniform(0, 1, size=n_dims)\n            \n            # Update velocity\n            velocities[i] = (w * velocities[i] \n                             + c1 * r1 * (pbest_positions[i] - positions[i]) \n                             + c2 * r2 * (gbest_position - positions[i]))\n            \n            # Update position with clipping\n            new_pos = np.clip(positions[i] + velocities[i], -1, 1)\n            new_val = np.sum(new_pos ** 2)\n            \n            # Update current position\n            positions[i] = new_pos\n            \n            # Update personal best and global best if improved\n            if new_val < current_values[i]:\n                current_values[i] = new_val\n                pbest_positions[i] = new_pos.copy()\n                \n                if new_val < gbest_value:\n                    gbest_value = new_val\n                    gbest_position = new_pos.copy()\n    \n    return round(gbest_value, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input parameters are valid (positive integers). If not, it returns -1.\n2. **Initialization**: The positions of particles are initialized using a uniform distribution within the bounds [-1, 1], and velocities are initialized to zero.\n3. **Tracking Bests**: Each particle's initial position is its personal best. The global best is determined by evaluating the initial positions.\n4. **Iterative Updates**: For each iteration, each particle's velocity is updated based on its inertia, cognitive component, and social component. The position is updated and clipped to stay within bounds.\n5. **Evaluation**: The new position is evaluated, and the personal and global bests are updated if the new position yields a better result.\n6. **Result**: The best value found during the iterations is returned, rounded to four decimal places.\n\nThis approach ensures that the algorithm efficiently explores the search space and converges towards the optimal solution using the principles of swarm intelligence."}
{"task_id": 179, "completion_id": 0, "completion": "To solve this problem, we need to create a function that tokenizes a text string into individual characters after applying optional preprocessing steps. The preprocessing steps include converting the text to lowercase and removing punctuation. After preprocessing, consecutive spaces are collapsed into a single space, and leading/trailing spaces are removed. If the input is `None` or becomes empty after preprocessing, the function returns an empty list.\n\n### Approach\n1. **Check for None Input**: If the input string is `None`, return an empty list immediately.\n2. **Lowercase Conversion**: Convert the input string to lowercase if the `lowercase` flag is set to `True`.\n3. **Filter Punctuation**: Remove all punctuation marks from the string if the `filter_punctuation` flag is set to `True`.\n4. **Collapse Consecutive Spaces**: Replace runs of two or more consecutive spaces with a single space.\n5. **Trim Leading/Trailing Spaces**: Remove any leading or trailing spaces from the processed string.\n6. **Tokenize into Characters**: Split the processed string into a list of individual characters. If the resulting string is empty, return an empty list.\n\n### Solution Code\n```python\nimport re\nimport string\n\ndef tokenize_chars(line: str, lowercase: bool = True, filter_punctuation: bool = True) -> list[str]:\n    \"\"\"Tokenise a text string at the character level.\n\n    Args:\n        line: Input text. If *None*, an empty list is returned.\n        lowercase: When *True*, convert *line* to lower-case before tokenisation.\n        filter_punctuation: When *True*, remove every character that is present in\n            ``string.punctuation`` (i.e. the 32 standard ASCII punctuation marks).\n\n    Returns:\n        A list of single-character strings after the selected preprocessing\n        steps have been applied. Consecutive spaces are collapsed into a single\n        space, and leading/trailing spaces are removed. If no characters remain\n        after preprocessing, an empty list is returned.\n    \"\"\"\n    if line is None:\n        return []\n    \n    processed_line = line\n    \n    # Convert to lowercase if required\n    if lowercase:\n        processed_line = processed_line.lower()\n    \n    # Remove punctuation if required\n    if filter_punctuation:\n        punct_set = set(string.punctuation)\n        processed_line = ''.join([c for c in processed_line if c not in punct_set])\n    \n    # Collapse runs of two or more spaces into a single space\n    processed_line = re.sub(r' {2,}', ' ', processed_line)\n    \n    # Remove leading and trailing spaces\n    processed_line = processed_line.strip(' ')\n    \n    # Split into characters, return empty list if processed_line is empty\n    return list(processed_line) if processed_line else []\n```\n\n### Explanation\n1. **Check for None Input**: The function immediately returns an empty list if the input is `None`.\n2. **Lowercase Conversion**: The string is converted to lowercase using the `lower()` method if the `lowercase` parameter is `True`.\n3. **Filter Punctuation**: Characters present in `string.punctuation` are removed using a set for efficient lookup and a list comprehension.\n4. **Collapse Consecutive Spaces**: The `re.sub` function replaces runs of two or more spaces with a single space using a regular expression.\n5. **Trim Leading/Trailing Spaces**: The `strip(' ')` method removes any leading or trailing space characters.\n6. **Tokenize into Characters**: The processed string is split into a list of characters using `list()`, ensuring an empty list is returned if the string is empty after preprocessing.\n\nThis approach ensures that the input string is processed efficiently and correctly according to the specified requirements, handling all edge cases such as empty strings and `None` input."}
{"task_id": 180, "completion_id": 0, "completion": "To solve this problem, we need to implement the k-Nearest Neighbours (k-NN) algorithm that can function both as a classifier and a regressor. The algorithm should use Euclidean distance, handle edge cases for the value of k, and return predictions based on majority vote for classification or average for regression.\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Convert the training and test feature matrices into numpy arrays for efficient computation.\n2. **Determine Effective k**: Adjust the value of k based on the number of training samples. If k is 0 or exceeds the number of training samples, use all training samples.\n3. **Compute Distances**: For each test sample, compute the Euclidean distances to all training samples.\n4. **Sort Distances**: Sort the distances to find the k-nearest neighbors.\n5. **Neighbor Indices**: Extract the indices of the k-nearest neighbors.\n6. **Prediction**:\n   - **Classification**: Use majority vote to determine the most frequent label among the neighbors. In case of a tie, return the smallest label.\n   - **Regression**: Compute the average of the neighbors' target values and round to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\"\"\"\n\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n    \n    n_train = X_train_np.shape[0]\n    \n    if k == 0 or k > n_train:\n        effective_k = n_train\n    else:\n        effective_k = k\n    \n    predictions = []\n    \n    for test_point in X_test_np:\n        # Compute Euclidean distances\n        distances = np.sqrt(np.sum((X_train_np - test_point) ** 2, axis=1))\n        # Get sorted indices by distance\n        sorted_indices = np.argsort(distances)\n        # Get neighbor indices\n        neighbors_indices = sorted_indices[:effective_k]\n        # Get corresponding labels\n        neighbor_labels = y_train_np[neighbors_indices]\n        \n        if task == \"classification\":\n            # Find most frequent label, resolving ties by smallest label\n            counts = Counter(neighbor_labels)\n            if not counts:\n                # Edge case when there are no neighbors (unlikely given effective_k)\n                predictions.append(None)  # though per problem constraints, this shouldn't happen\n                continue\n            max_count = max(counts.values())\n            candidates = [label for label, cnt in counts.items() if cnt == max_count]\n            prediction = min(candidates)\n        else:\n            # Regression: compute mean and round\n            mean_value = np.mean(neighbor_labels)\n            prediction = round(mean_value, 4)\n        \n        predictions.append(prediction)\n    \n    return predictions\n```\n\n### Explanation\n1. **Input Conversion**: The training and test data are converted to numpy arrays to leverage efficient vector operations.\n2. **Effective k Calculation**: Adjust k to ensure it is within valid bounds based on the number of training samples.\n3. **Distance Calculation**: For each test point, compute the Euclidean distance to all training points using vectorized operations.\n4. **Sorting and Neighbor Selection**: Distances are sorted, and the indices of the nearest neighbors are selected.\n5. **Prediction Logic**:\n   - **Classification**: Uses a counter to tally the labels of the nearest neighbors, resolving ties by selecting the smallest label.\n   - **Regression**: Computes the mean of the neighbors' targets and rounds the result to four decimal places.\n\nThis approach ensures that the k-NN algorithm efficiently handles both classification and regression tasks while adhering to the specified constraints."}
{"task_id": 181, "completion_id": 0, "completion": "To solve this problem, we need to implement the k-Nearest Neighbors (kNN) algorithm from scratch to predict the targets of query samples using their k closest neighbors from a labeled training set. The algorithm should work in two modes: classification and regression.\n\n### Approach\n1. **Problem Analysis**:\n   - **Classification**: For each query point, return the most common label among its k nearest neighbors. In case of a tie, return the smallest label when sorted.\n   - **Regression**: For each query point, return the mean of the target values of its k nearest neighbors, rounded to 4 decimal places.\n   - **Euclidean Distance**: Always use Euclidean distance to determine the nearest neighbors.\n   - **Handling k**: If k is larger than the number of training samples, use all available samples.\n\n2. **Steps**:\n   - **Compute Distances**: For each test sample, compute the Euclidean distance to all training samples.\n   - **Sort Neighbors**: Sort the training samples by their distance to the test sample.\n   - **Select Neighbors**: Select the top k (or all if fewer than k) neighbors.\n   - **Predict**:\n     - **Classification**: Use majority vote among the neighbors' labels. Break ties by selecting the smallest label.\n     - **Regression**: Compute the mean of the neighbors' target values and round it to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(\n    X_train: list[list[float]],\n    y_train: list,\n    X_test: list[list[float]],\n    k: int = 5,\n    problem: str = \"classify\",\n) -> list:\n    \"\"\"Predict targets of *X_test* using k-Nearest Neighbours.\"\"\"\n\n    predictions = []\n    effective_k = min(k, len(X_train))\n\n    for test_sample in X_test:\n        # Calculate distances to all training samples\n        distances = []\n        for x, y in zip(X_train, y_train):\n            distance = 0.0\n            for xi, ti in zip(x, test_sample):\n                distance += (xi - ti) ** 2\n            distance = np.sqrt(distance)\n            distances.append((distance, y))\n\n        # Sort by distance, then by label to break ties deterministically\n        distances.sort(key=lambda pair: (pair[0], pair[1]))\n        neighbors = distances[:effective_k]\n        labels = [y for (d, y) in neighbors]\n\n        if problem == \"classify\":\n            if not labels:\n                predictions.append(None)\n                continue\n            count = Counter(labels)\n            max_freq = max(count.values())\n            candidates = [label for label, freq in count.items() if freq == max_freq]\n            candidates_sorted = sorted(candidates)\n            predictions.append(candidates_sorted[0])\n        elif problem == \"regress\":\n            if not labels:\n                predictions.append(0.0)\n                continue\n            avg = sum(labels) / len(labels)\n            predictions.append(round(avg, 4))\n        else:\n            raise ValueError(f\"Invalid problem type: {problem}\")\n\n    return predictions\n```\n\n### Explanation\n1. **Distance Calculation**: For each test sample, compute the Euclidean distance to each training sample. This involves summing the squared differences between each feature and taking the square root of the sum.\n2. **Sorting Neighbors**: Neighbors are sorted by their distance to the test sample. In case of ties in distance, neighbors are sorted by their labels to ensure deterministic results.\n3. **Selecting Neighbors**: The top k neighbors (or all if fewer than k) are selected based on the sorted distances.\n4. **Prediction**:\n   - **Classification**: The most frequent label among the neighbors is chosen. If there is a tie, the smallest label (when sorted) is selected.\n   - **Regression**: The mean of the neighbors' target values is computed and rounded to 4 decimal places.\n\nThis approach ensures that the kNN algorithm efficiently computes predictions for both classification and regression tasks while adhering to the problem constraints and requirements."}
{"task_id": 182, "completion_id": 0, "completion": "To solve this problem, we need to compute the polynomial kernel matrix for two given data matrices using a specified polynomial degree. The polynomial kernel matrix is a crucial component in various machine learning algorithms, such as Support Vector Machines (SVM), where it helps in transforming the data into a higher-dimensional space without explicitly computing the coordinates in that space.\n\n### Approach\n1. **Input Validation**: Ensure that the inputs are valid. Specifically, check that:\n   - Both matrices are two-dimensional.\n   - The number of features (columns) in both matrices match.\n   - The specified polynomial degree is a positive integer (\u2265 1).\n\n2. **Matrix Multiplication**: Compute the dot product of the two matrices, which forms the basis of the polynomial kernel.\n\n3. **Polynomial Transformation**: Raise each element of the resulting dot product matrix to the specified polynomial degree.\n\n4. **Rounding and Conversion**: Round each element of the transformed matrix to four decimal places and convert the result into a nested Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef polynomial_kernel(X, Y, degree=2):\n    \"\"\"Compute the polynomial kernel matrix between two datasets X and Y.\n    \n    Args:\n        X (list or numpy.ndarray): A 2-D input data matrix of shape (n, p).\n        Y (list or numpy.ndarray): A 2-D input data matrix of shape (m, p).\n        degree (int, optional): The polynomial degree. Defaults to 2.\n    \n    Returns:\n        list: Polynomial kernel matrix rounded to four decimal places, as a nested list. \n              Returns -1 if any validation fails.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    X = np.array(X)\n    Y = np.array(Y)\n    \n    # Validate inputs\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    if not isinstance(degree, int) or degree < 1:\n        return -1\n    \n    # Compute the dot product matrix\n    dot_product = X @ Y.T\n    \n    # Apply polynomial kernel\n    K = (dot_product) ** degree\n    \n    # Round to four decimal places and convert to list\n    K_rounded = np.round(K, 4)\n    \n    return K_rounded.tolist()\n```\n\n### Explanation\n1. **Input Conversion and Validation**:\n   - Convert input matrices `X` and `Y` to NumPy arrays to leverage NumPy's matrix operations.\n   - Check if both matrices are two-dimensional. If not, return -1.\n   - Ensure the number of columns (features) in both matrices matches. If not, return -1.\n   - Verify that the polynomial degree is a valid positive integer. If not, return -1.\n\n2. **Dot Product Calculation**:\n   - Compute the dot product of `X` and the transpose of `Y` using matrix multiplication. This results in a matrix where each element `(i, j)` is the dot product of the `i`-th row of `X` and the `j`-th row of `Y`.\n\n3. **Polynomial Transformation**:\n   - Raise each element of the dot product matrix to the power of the specified polynomial degree. This transforms the dot product matrix into the polynomial kernel matrix.\n\n4. **Rounding and Conversion**:\n   - Round each element of the kernel matrix to four decimal places to ensure precision.\n   - Convert the resulting NumPy array into a nested Python list using the `tolist()` method for the final output.\n\nThis approach efficiently handles the transformation and validation steps, ensuring that the resulting kernel matrix is both accurate and formatted correctly as required."}
{"task_id": 183, "completion_id": 0, "completion": "To solve this problem, we need to initialize the weights of a neural network layer using the He uniform initialization method. This method is particularly effective for layers followed by ReLU activation functions, as it helps in maintaining the proper flow of gradients during training. The key idea is to draw weights from a uniform distribution scaled by the square root of 6 divided by the fan-in (number of incoming connections) of the layer.\n\n### Approach\n1. **Determine the Fan-in**: The fan-in depends on the type of layer:\n   - For a dense (fully-connected) layer, the weight matrix has shape `(n_in, n_out)`, so the fan-in is `n_in`.\n   - For a 2-D convolutional layer, the filter tensor has shape `(fW, fH, in_channels, out_channels)`, so the fan-in is the product of the spatial dimensions and input channels, i.e., `fW * fH * in_channels`.\n\n2. **Compute the Limit**: The limit for the uniform distribution is calculated as `sqrt(6 / fan_in)`.\n\n3. **Generate Weights**: Sample weights from a uniform distribution within the interval `[-limit, limit]` using the computed limit and the given shape.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    if len(weight_shape) == 2:\n        fan_in = weight_shape[0]\n    else:  # Assuming length 4 for convolutional layers\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    \n    limit = np.sqrt(6 / fan_in)\n    return np.random.uniform(low=-limit, high=limit, size=weight_shape)\n```\n\n### Explanation\n- **Fan-in Calculation**: The function first checks the dimensionality of the input shape to determine the fan-in. For dense layers (2-dimensional), the fan-in is the first dimension of the weight matrix. For convolutional layers (4-dimensional), the fan-in is the product of the filter dimensions and input channels.\n- **Limit Calculation**: The limit for the uniform distribution is derived from the fan-in using the formula `sqrt(6 / fan_in)` to ensure the weights are scaled appropriately.\n- **Weight Generation**: Using NumPy's `uniform` function, weights are sampled from the calculated interval, ensuring they are within the desired range for effective initialization.\n\nThis approach ensures that the weights are initialized in a way that promotes efficient gradient flow during the training of neural networks, adhering to the He uniform initialization principles."}
{"task_id": 184, "completion_id": 0, "completion": "To solve this problem, we need to implement a binary decision-tree classifier using the CART algorithm from scratch. The classifier should use Gini impurity for splitting nodes and handle stopping criteria such as identical labels, maximum depth, and empty child nodes.\n\n### Approach\n1. **Tree Structure**: Represent each node in the decision tree using a `TreeNode` class, which can be either a leaf node (containing a class label) or a decision node (containing a feature index, threshold, and left/right children).\n2. **Gini Impurity**: Calculate the Gini impurity to measure the quality of a split. The Gini impurity for a set of labels is given by \\(1 - \\sum (p_i^2)\\), where \\(p_i\\) is the proportion of samples of class \\(i\\).\n3. **Splitting Criteria**: For each feature, evaluate midpoints between consecutive unique values as potential thresholds. Select the split that maximizes the Gini gain (difference between parent Gini impurity and weighted children Gini impurities).\n4. **Stopping Criteria**: Stop splitting and create a leaf node if all labels are identical, the maximum depth is reached, or a split results in an empty child.\n5. **Recursive Tree Building**: Recursively build the tree by splitting nodes based on the best split found, and handle stopping criteria at each step.\n6. **Prediction**: Traverse the built tree for each test sample to determine its class label based on the decision nodes' thresholds and features.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\"\"\"\n\n    class TreeNode:\n        def __init__(self, value=None, feature=None, threshold=None, left=None, right=None):\n            self.value = value\n            self.feature = feature\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n\n        def is_leaf(self):\n            return self.value is not None\n\n    def gini(y):\n        counts = np.bincount(y)\n        if len(counts) == 0:\n            return 0.0\n        probs = counts / len(y)\n        return 1.0 - np.sum(probs ** 2)\n\n    def get_majority_class(y):\n        counts = np.bincount(y)\n        return np.argmax(counts)\n\n    def build_tree(X, y, current_depth, max_depth):\n        # All samples have the same label\n        unique_classes = np.unique(y)\n        if len(unique_classes) == 1:\n            return TreeNode(value=int(unique_classes[0]))\n\n        # Check if max_depth is reached\n        if max_depth is not None and current_depth >= max_depth:\n            return TreeNode(value=int(get_majority_class(y)))\n\n        parent_gini = gini(y)\n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n        best_left_mask = None\n        best_right_mask = None\n\n        n_samples, n_features = X.shape\n\n        for feature_idx in range(n_features):\n            feature_values = X[:, feature_idx]\n            sorted_indices = np.argsort(feature_values)\n            sorted_features = feature_values[sorted_indices]\n            unique_values = np.unique(sorted_features)\n\n            if len(unique_values) < 2:\n                continue  # No split possible\n\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n\n            for thresh in thresholds:\n                left_mask = feature_values <= thresh\n                right_mask = ~left_mask\n\n                y_left = y[left_mask]\n                y_right = y[right_mask]\n\n                if len(y_left) == 0 or len(y_right) == 0:\n                    continue\n\n                # Calculate weighted Gini impurity\n                g_left = gini(y_left)\n                g_right = gini(y_right)\n                n_left, n_right = len(y_left), len(y_right)\n                weighted_gini = (n_left * g_left + n_right * g_right) / (n_left + n_right)\n                gain = parent_gini - weighted_gini\n\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature_idx\n                    best_threshold = thresh\n                    best_left_mask = left_mask\n                    best_right_mask = right_mask\n\n        # If no valid split found or no gain\n        if best_gain <= 0 or best_feature is None:\n            return TreeNode(value=int(get_majority_class(y)))\n\n        # Split the data\n        X_left, y_left = X[best_left_mask], y[best_left_mask]\n        X_right, y_right = X[best_right_mask], y[best_right_mask]\n\n        # Recursively build children\n        left_child = build_tree(X_left, y_left, current_depth + 1, max_depth)\n        right_child = build_tree(X_right, y_right, current_depth + 1, max_depth)\n\n        return TreeNode(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n\n    # Handle edge case with no training data\n    if X_train.size == 0:\n        return [0] * len(X_test)\n\n    # Build the decision tree\n    root = build_tree(X_train, y_train, current_depth=0, max_depth=max_depth)\n\n    # Predict each test sample\n    predictions = []\n    for sample in X_test:\n        node = root\n        while True:\n            if node.is_leaf():\n                predictions.append(node.value)\n                break\n            feature_val = sample[node.feature]\n            if feature_val <= node.threshold:\n                node = node.left\n            else:\n                node = node.right\n\n    return predictions\n```\n\n### Explanation\n1. **Tree Node Structure**: The `TreeNode` class represents each node in the decision tree, which can be a leaf node (with a class label) or a decision node (with a feature index, threshold, and child nodes).\n2. **Gini Impurity Calculation**: The `gini` function computes the impurity of a set of labels, which helps in evaluating the quality of splits.\n3. **Majority Class Calculation**: The `get_majority_class` function determines the most frequent class label in a node, used when creating leaf nodes due to stopping criteria.\n4. **Tree Construction**: The `build_tree` function recursively constructs the tree by evaluating potential splits, applying stopping criteria, and creating child nodes.\n5. **Prediction**: Each test sample is classified by traversing the tree from the root to a leaf node, applying the decision rules at each node.\n\nThis approach ensures that the decision tree is built efficiently, adhering to the specified stopping criteria and splitting rules, and provides deterministic predictions for test samples."}
{"task_id": 185, "completion_id": 0, "completion": "To solve this problem, we need to determine the shape of a dataset from an array-like input, following specific rules for handling different dimensionalities of the input data. The goal is to extract the number of samples and features in a manner consistent with typical machine learning libraries.\n\n### Approach\n1. **Convert to NumPy Array**: Convert the input to a NumPy array if it isn't already one. This ensures uniform handling of array-like inputs.\n2. **Check for Empty Matrix**: If the converted array is empty, raise a `ValueError`.\n3. **Handle Dimensions**:\n   - **1-Dimensional Arrays**: Treat the array as a single sample with the length of the array as the number of features.\n   - **Multi-Dimensional Arrays (2D or higher)**: Use the first dimension as the number of samples and compute the number of features as the product of the remaining dimensions.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dataset_shape(X):\n    \"\"\"Inspect the input data and return *(n_samples, n_features)*.\n\n    Parameters\n    ----------\n    X : array-like\n        Feature data. If *X* is one-dimensional it is treated as a single\n        sample, otherwise the first axis counts the samples.\n\n    Returns\n    -------\n    tuple of two ints\n        *(n_samples, n_features)* extracted from *X* following the same logic\n        used by the `_setup_input` method in the supplied code snippet.\n    \"\"\"\n    X = np.asarray(X)\n    if X.size == 0:\n        raise ValueError(\"Got an empty matrix.\")\n    if X.ndim == 1:\n        return (1, X.shape[0])\n    else:\n        n_samples = X.shape[0]\n        n_features = np.prod(X.shape[1:])\n        return (n_samples, int(n_features))\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input is converted to a NumPy array using `np.asarray`, which ensures that any array-like input (lists, tuples, etc.) is transformed into a NumPy array without modifying the original input.\n2. **Empty Matrix Check**: If the converted array has a size of zero, an error is raised immediately as an empty matrix is invalid for machine learning tasks.\n3. **Dimension Handling**:\n   - For 1-dimensional arrays, the number of samples is set to 1, and the number of features is the length of the array.\n   - For arrays with two or more dimensions, the first dimension is taken as the number of samples. The product of the remaining dimensions gives the number of features, which accommodates higher-dimensional data (e.g., images stored as 3D arrays).\n\nThis approach ensures that the function correctly handles various input shapes and converts them into a standardized format suitable for machine learning algorithms."}
{"task_id": 186, "completion_id": 0, "completion": "To solve this problem, we need to create a baseline regressor that predicts the mean of the training target values for all test samples. This simple model serves as a reference point for more complex models.\n\n### Approach\n1. **Compute the Mean**: Calculate the arithmetic mean of the training target values `y_train`.\n2. **Check for Empty Test Data**: If the test feature matrix `X_test` is empty (i.e., contains no rows), return an empty list.\n3. **Generate Predictions**: For each row in `X_test`, generate a prediction using the computed mean, rounded to four decimal places.\n4. **Return Results**: Return the predictions as a list of floats, each rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef baseline_mean_predict(X_train, y_train, X_test):\n    \"\"\"Baseline mean regressor.\n\n    This function returns the arithmetic mean of y_train for every sample in X_test.\n\n    Parameters\n    ----------\n    X_train : array-like, shape (n_samples, n_features)\n        Training data (not used in the computation).\n    y_train : array-like, shape (n_samples,)\n        Target values corresponding to *X_train*.\n    X_test : array-like, shape (m_samples, n_features)\n        Test samples for which predictions are required.\n\n    Returns\n    -------\n    list of float\n        A list with *m_samples* elements where every element equals the mean\n        of *y_train* rounded to 4 decimal places.\n    \"\"\"\n    X_test = np.asarray(X_test)\n    if X_test.shape[0] == 0:\n        return []\n    mean_val = np.mean(y_train)\n    rounded_mean = round(mean_val, 4)\n    return [rounded_mean] * X_test.shape[0]\n```\n\n### Explanation\n1. **Compute the Mean**: The mean of `y_train` is computed using `np.mean(y_train)`, which efficiently handles array-like inputs.\n2. **Check for Empty Test Data**: Converting `X_test` to a numpy array allows us to check the number of rows using `X_test.shape[0]`. If there are no rows, an empty list is returned immediately.\n3. **Generate Predictions**: The computed mean is rounded to four decimal places using Python's built-in `round` function. This rounded value is then repeated for each row in `X_test` to form the predictions list.\n4. **Return Results**: The predictions are returned as a list of floats, ensuring each value is rounded to exactly four decimal places, providing a consistent and expected output format."}
{"task_id": 188, "completion_id": 0, "completion": "To solve this problem, we need to implement an exponential \u03b5-decay scheduler that adjusts the exploration rate \u03b5 over time for reinforcement learning algorithms. The goal is to ensure sufficient exploration at the beginning of training while gradually shifting towards exploitation by decaying \u03b5 over time.\n\n### Approach\n1. **Input Validation**: Check if the input parameters meet the specified constraints. If any parameter is invalid, return -1.\n2. **Exponential Decay Calculation**: Compute the exploration rate \u03b5 using the formula \u03b5(t) = \u03b5_min + (\u03b5_max \u2212 \u03b5_min) * exp(\u2212\u03bb * t).\n3. **Clipping**: Ensure the computed \u03b5 does not fall below \u03b5_min due to numerical underflow.\n4. **Rounding**: Round the result to 4 decimal places for consistency and precision.\n\n### Solution Code\n```python\nimport math\n\ndef epsilon_decay(max_epsilon: float, min_epsilon: float, decay_rate: float, step: int) -> float:\n    \"\"\"Compute the exponentially decayed \u03b5 (epsilon) for \u03b5-greedy exploration.\n\n    The function must implement the formula\n        \u03b5(t) = \u03b5_min + (\u03b5_max \u2212 \u03b5_min) * exp(\u2212decay_rate * t)\n    and respect the constraints described in the task description.\n\n    Args:\n        max_epsilon (float): Initial exploration rate \u03b5_max at step 0.\n        min_epsilon (float): Lower bound \u03b5_min that \u03b5 should never fall below.\n        decay_rate (float):  Positive decay rate \u03bb.\n        step (int):          Current time-step.\n\n    Returns:\n        float: The exploration rate \u03b5(t) rounded to 4 decimal places, or \u22121 if\n               the input parameters are invalid.\n    \"\"\"\n    # Validate input parameters\n    if not (0 <= min_epsilon < max_epsilon and decay_rate > 0 and step >= 0):\n        return -1.0\n    \n    # Calculate the decayed epsilon value\n    delta = max_epsilon - min_epsilon\n    exponent = -decay_rate * step\n    epsilon = min_epsilon + delta * math.exp(exponent)\n    \n    # Ensure epsilon does not drop below min_epsilon due to numerical issues\n    epsilon = max(epsilon, min_epsilon)\n    \n    # Round to 4 decimal places\n    return round(epsilon, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the parameters meet the constraints. Specifically, it ensures that 0 \u2264 \u03b5_min < \u03b5_max, the decay rate \u03bb is positive, and the step is non-negative. If any condition is violated, the function returns -1.\n2. **Exponential Decay Calculation**: Using the given formula, the function calculates the current \u03b5 value. The exponential term ensures that \u03b5 decreases over time, starting from \u03b5_max and approaching \u03b5_min.\n3. **Clipping**: To handle any potential numerical underflow, the computed \u03b5 is clipped to ensure it does not fall below \u03b5_min.\n4. **Rounding**: The result is rounded to 4 decimal places to maintain precision and consistency in the output.\n\nThis approach ensures that the exploration rate \u03b5 decays exponentially over time, balancing exploration and exploitation effectively in reinforcement learning algorithms."}
{"task_id": 189, "completion_id": 0, "completion": "To solve this problem, we need to resample a batch of images to a new spatial resolution using either bilinear interpolation or nearest-neighbor interpolation. The goal is to transform the input batch of images, which is a 4-D NumPy array, into a new batch of images with the specified dimensions while maintaining the number of examples and channels.\n\n### Approach\n1. **Generate Output Grid Coordinates**: For each target dimension, generate a grid of coordinates that map the output image's pixels to the input image's coordinate system. This is done using `np.linspace` to create evenly spaced points between 0 and the input dimensions.\n2. **Bilinear Interpolation**: For each output pixel, compute the four nearest input pixels (corners) and their corresponding weights based on the fractional distances from the target coordinates. The final value is a weighted sum of these four pixels.\n3. **Nearest-Neighbor Interpolation**: For each output pixel, find the nearest input pixel by rounding the coordinates to the nearest integer values.\n4. **Vectorized Operations**: Use NumPy's advanced indexing and broadcasting to efficiently compute the interpolated values for the entire batch without explicit loops, ensuring performance scalability.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef batch_resample(X, new_dim, mode=\"bilinear\"):\n    \"\"\"Resample a batch of images to a new spatial resolution.\n\n    Parameters\n    ----------\n    X : numpy.ndarray of shape (n_ex, in_rows, in_cols, in_channels)\n        Input batch of images.\n    new_dim : tuple[int, int]\n        Target dimension ``(out_rows, out_cols)``.\n    mode : {\"bilinear\", \"neighbor\"}, default=\"bilinear\"\n        Interpolation method.\n\n    Returns\n    -------\n    numpy.ndarray\n        Resampled batch with shape (n_ex, out_rows, out_cols, in_channels).\n    \"\"\"\n    n_ex, in_rows, in_cols, in_channels = X.shape\n    out_rows, out_cols = new_dim\n\n    # Generate output grid coordinates\n    y_coords = np.linspace(0, in_rows - 1, out_rows)\n    x_coords = np.linspace(0, in_cols - 1, out_cols)\n    Y, X_grid = np.meshgrid(y_coords, x_coords, indexing='ij')  # Y and X_grid are (out_rows, out_cols)\n\n    if mode == 'bilinear':\n        # Calculate indices for the four corners\n        i0 = np.floor(Y).astype(int)\n        i1 = i0 + 1\n        j0 = np.floor(X_grid).astype(int)\n        j1 = j0 + 1\n\n        # Clip indices to stay within the input dimensions\n        i0 = np.clip(i0, 0, in_rows - 1)\n        i1 = np.clip(i1, 0, in_rows - 1)\n        j0 = np.clip(j0, 0, in_cols - 1)\n        j1 = np.clip(j1, 0, in_cols - 1)\n\n        # Compute weights for bilinear interpolation\n        dy = Y - i0\n        dx = X_grid - j0\n\n        # Sample the four corners from the input batch\n        A = X[:, i0, j0, :]\n        B = X[:, i0, j1, :]\n        C = X[:, i1, j0, :]\n        D = X[:, i1, j1, :]\n\n        # Calculate weighted sum of the four corners\n        w_a = (1 - dy) * (1 - dx)\n        w_b = (1 - dy) * dx\n        w_c = dy * (1 - dx)\n        w_d = dy * dx\n\n        # Expand dimensions for broadcasting\n        w_a = w_a[np.newaxis, ..., np.newaxis]\n        w_b = w_b[np.newaxis, ..., np.newaxis]\n        w_c = w_c[np.newaxis, ..., np.newaxis]\n        w_d = w_d[np.newaxis, ..., np.newaxis]\n\n        interpolated = w_a * A + w_b * B + w_c * C + w_d * D\n        output = interpolated\n\n    elif mode == 'neighbor':\n        # Calculate nearest neighbor indices\n        i = np.round(Y).astype(int)\n        j = np.round(X_grid).astype(int)\n        i = np.clip(i, 0, in_rows - 1)\n        j = np.clip(j, 0, in_cols - 1)\n\n        output = X[:, i, j, :]\n\n    else:\n        raise NotImplementedError(f\"Unrecognized resampling mode: {mode}\")\n\n    # Round to four decimal places as per the problem's requirement\n    output = np.round(output, 4)\n    return output\n```\n\n### Explanation\n1. **Grid Generation**: The output grid coordinates are generated using `np.linspace` to ensure they are evenly spaced within the bounds of the input image dimensions. This grid is then used to map each output pixel to the corresponding input coordinates.\n2. **Bilinear Interpolation**: For each output pixel, the indices of the four surrounding input pixels are computed. Weights are determined based on the fractional distances from the target coordinates, and the final value is a weighted sum of these four pixels. This approach ensures smooth transitions when resampling.\n3. **Nearest-Neighbor Interpolation**: The nearest input pixel is determined by rounding the target coordinates to the nearest integer values. This method is computationally efficient and preserves edges but may result in a blocky appearance when upsampling.\n4. **Vectorization**: By leveraging NumPy's advanced indexing and broadcasting, the solution efficiently processes the entire batch of images without explicit loops, ensuring high performance even for large datasets.\n\nThis approach ensures that the resampled images maintain the correct dimensions and interpolation strategy, providing accurate and efficient results."}
{"task_id": 190, "completion_id": 0, "completion": "To solve this problem, we need to find the best binary split of a dataset based on the Gini impurity, which is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. The goal is to minimize the weighted Gini impurity of the split.\n\n### Approach\n1. **Convert Inputs to Arrays**: Convert the feature matrix `X` and labels `y` to NumPy arrays for efficient computation.\n2. **Compute Gini Impurity of the Whole Dataset**: Calculate the Gini impurity for the entire dataset to use as a fallback if no valid splits are found.\n3. **Check for Constant Features**: Determine if all features are constant (i.e., they have only one unique value). If so, return the fallback result immediately.\n4. **Iterate Over Features and Thresholds**: For each feature, consider each unique value as a potential threshold. For each threshold, split the dataset into left and right subsets.\n5. **Validate Splits**: Skip splits where either subset is empty.\n6. **Compute Gini Impurity for Subsets**: Calculate the Gini impurity for both left and right subsets and compute the weighted Gini impurity for the split.\n7. **Track Best Split**: Keep track of the best split based on the lowest Gini impurity, using tie-breaking rules if necessary.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\"\"\"\n\n    X = np.asarray(X)\n    y = np.asarray(y)\n    n_samples, n_features = X.shape\n\n    # Compute Gini impurity for the entire dataset\n    classes, counts = np.unique(y, return_counts=True)\n    total = len(y)\n    if total == 0:\n        return (-1, None, 0.0)  # Edge case with no samples\n    p = counts / total\n    g_whole = 1.0 - np.sum(p ** 2)\n    g_whole_rounded = round(g_whole, 4)\n\n    # Check if all features are constant\n    all_constant = True\n    for j in range(n_features):\n        if np.unique(X[:, j]).size > 1:\n            all_constant = False\n            break\n    if all_constant:\n        return (-1, None, g_whole_rounded)\n\n    best_feature = -1\n    best_threshold = None\n    best_gini = float('inf')\n\n    for j in range(n_features):\n        feature = X[:, j]\n        unique_values = np.unique(feature)\n        for v in unique_values:\n            left_mask = feature <= v\n            n_left = np.sum(left_mask)\n            n_right = n_samples - n_left\n\n            if n_left == 0 or n_right == 0:\n                continue\n\n            # Compute Gini impurity for left and right\n            y_left = y[left_mask]\n            y_right = y[~left_mask]\n\n            # Left node\n            left_counts = np.unique(y_left, return_counts=True)[1]\n            p_left = left_counts / n_left\n            g_left = 1.0 - np.sum(p_left ** 2)\n\n            # Right node\n            right_counts = np.unique(y_right, return_counts=True)[1]\n            p_right = right_counts / n_right\n            g_right = 1.0 - np.sum(p_right ** 2)\n\n            # Weighted Gini impurity\n            g_split = (n_left / n_samples) * g_left + (n_right / n_samples) * g_right\n\n            # Update best split considering tie-breakers\n            if g_split < best_gini - 1e-12:\n                best_gini = g_split\n                best_feature = j\n                best_threshold = v\n            elif abs(g_split - best_gini) < 1e-12:\n                if (best_feature == -1) or (j < best_feature) or (j == best_feature and v < best_threshold):\n                    best_gini = g_split\n                    best_feature = j\n                    best_threshold = v\n\n    if best_feature == -1:\n        return (-1, None, g_whole_rounded)\n    else:\n        return (best_feature, best_threshold, round(best_gini, 4))\n```\n\n### Explanation\n1. **Input Conversion**: The feature matrix `X` and labels `y` are converted to NumPy arrays for efficient manipulation.\n2. **Gini Impurity Calculation**: The Gini impurity for the entire dataset is computed to handle cases where no valid splits exist.\n3. **Constant Feature Check**: If all features are constant, the function immediately returns the fallback result.\n4. **Feature and Threshold Iteration**: For each feature, each unique value is considered as a potential threshold to split the dataset.\n5. **Split Validation**: Splits resulting in empty subsets are skipped.\n6. **Gini Computation**: For valid splits, the Gini impurity of the left and right subsets is calculated, and the weighted Gini impurity is determined.\n7. **Best Split Tracking**: The best split is tracked using the minimum Gini impurity and tie-breaking rules for equal impurities.\n\nThis approach ensures that the best possible split is found efficiently, adhering to the constraints and requirements specified."}
{"task_id": 191, "completion_id": 0, "completion": "To solve this problem, we need to create a factory function that returns a callable distance metric based on the specified name. The callable should validate the input vectors and compute the distance using the specified metric. If validation fails or the metric name is unsupported, the callable should return -1.\n\n### Approach\n1. **Factory Function**: The `metric` function takes a metric name and returns a callable. If the name is unsupported, the returned callable always returns -1.\n2. **Validation**: The callable checks if the input vectors are valid (lists, tuples, or NumPy arrays, 1-dimensional, non-empty, same length). For the cosine metric, it also checks that neither vector has a zero norm.\n3. **Distance Calculation**: The callable computes the distance using the specified metric (Euclidean, Manhattan, Chebyshev, or Cosine) and rounds the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n    valid_metrics = {'euclidean', 'manhattan', 'chebyshev', 'cosine'}\n    if name not in valid_metrics:\n        def invalid_metric(x, y):\n            return -1\n        return invalid_metric\n    \n    def validated_metric(x, y):\n        try:\n            # Check if inputs are of allowed types\n            if not isinstance(x, (list, tuple, np.ndarray)) or not isinstance(y, (list, tuple, np.ndarray)):\n                return -1\n            \n            # Convert to numpy arrays of float; this handles numeric conversion checks\n            x_arr = np.array(x, dtype=float)\n            y_arr = np.array(y, dtype=float)\n            \n            # Check if arrays are 1-dimensional\n            if x_arr.ndim != 1 or y_arr.ndim != 1:\n                return -1\n            \n            # Check non-empty and same length\n            if x_arr.size == 0 or y_arr.size == 0 or x_arr.shape != y_arr.shape:\n                return -1\n            \n            # Additional checks for cosine distance\n            if name == 'cosine':\n                norm_x = np.linalg.norm(x_arr)\n                norm_y = np.linalg.norm(y_arr)\n                if norm_x == 0.0 or norm_y == 0.0:\n                    return -1\n            \n            # Compute the distance based on the metric name\n            if name == 'euclidean':\n                distance = np.sqrt(np.sum((x_arr - y_arr) ** 2))\n            elif name == 'manhattan':\n                distance = np.sum(np.abs(x_arr - y_arr))\n            elif name == 'chebyshev':\n                distance = np.max(np.abs(x_arr - y_arr))\n            elif name == 'cosine':\n                dot_product = np.dot(x_arr, y_arr)\n                norm_product = np.linalg.norm(x_arr) * np.linalg.norm(y_arr)\n                distance = 1 - (dot_product / norm_product)\n            else:\n                return -1  # This is unreachable due to earlier check\n            \n            # Round to four decimal places as per requirement\n            return round(distance, 4)\n        except:\n            return -1\n    \n    return validated_metric\n```\n\n### Explanation\n1. **Factory Function**: The `metric` function checks if the provided name is valid. If not, it returns a callable that always returns -1.\n2. **Validation**: The returned callable first checks if the inputs are valid types (list, tuple, or NumPy array). It converts them to NumPy arrays and checks if they are 1-dimensional, non-empty, and of the same length.\n3. **Cosine Specific Check**: For the cosine metric, it ensures neither vector has a zero norm to avoid division by zero.\n4. **Distance Calculation**: Depending on the metric name, it computes the distance using the appropriate formula and rounds the result to four decimal places.\n5. **Error Handling**: Any errors during validation or computation are caught, and the callable returns -1 in such cases."}
{"task_id": 193, "completion_id": 0, "completion": "To solve this problem, we need to implement the Softsign activation function, which is used in neural networks as a smooth alternative to the hyperbolic tangent. The function should handle both scalar and one-dimensional array-like inputs, returning the result rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: The function should accept either a scalar (integer or float) or a one-dimensional list/NumPy array. \n2. **Softsign Calculation**: For each element, compute the Softsign value using the formula \\( \\text{Softsign}(z) = \\frac{z}{1 + |z|} \\).\n3. **Rounding**: Round the result to four decimal places. For scalar inputs, return a float. For array-like inputs, return a list of floats.\n4. **Edge Cases**: Handle empty lists/arrays by returning an empty list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softsign(z):\n    \"\"\"Compute the Softsign activation function.\n\n    Softsign(z) = z / (1 + |z|)\n\n    The function accepts either a scalar or a list/NumPy array. For a scalar it\n    returns a float; for an array-like input it returns a list of floats, with\n    every element rounded to 4 decimal places.\n    \"\"\"\n    arr = np.asarray(z)\n    if arr.ndim == 0:  # Scalar input\n        result = arr / (1 + np.abs(arr))\n        return round(result.item(), 4)\n    else:  # Array-like input\n        if arr.size == 0:\n            return []\n        result = arr / (1 + np.abs(arr))\n        return [round(x, 4) for x in result.tolist()]\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input is converted to a NumPy array to handle both scalar and array-like inputs uniformly.\n2. **Scalar Check**: If the input is a scalar (0-dimensional array), compute the Softsign value, convert it to a Python float, round it to four decimal places, and return it.\n3. **Array-like Handling**: For array-like inputs, compute the Softsign for each element using vectorized operations. If the input is an empty array, return an empty list. Otherwise, convert the result to a Python list, round each element to four decimal places, and return the list.\n\nThis approach ensures efficient computation using NumPy's vectorized operations for array-like inputs while correctly handling scalar values and edge cases like empty inputs."}
{"task_id": 194, "completion_id": 0, "completion": "To solve this problem, we need to implement the Adagrad optimization algorithm, which adaptively adjusts the learning rate for each parameter based on the historical sum of squared gradients. This approach helps in converging more effectively for problems with sparse data.\n\n### Approach\nThe Adagrad optimization algorithm involves two main steps:\n1. **Accumulate Squared Gradients**: Maintain a running sum of the squared gradients for each parameter. If this is the first iteration, initialize the sum with the square of the current gradients. Otherwise, add the square of the current gradients to the existing sum.\n2. **Update Parameters**: Adjust each parameter using a learning rate that is scaled by the square root of the accumulated squared gradients (plus a small constant to avoid division by zero).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adagrad_update(weights: list[float],\n                   gradients: list[float],\n                   G: list[float] | None = None,\n                   learning_rate: float = 0.01,\n                   eps: float = 1e-8) -> tuple[list[float], list[float]]:\n    \"\"\"Performs one Adagrad optimisation step.\"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    weights_np = np.array(weights, dtype=np.float64)\n    gradients_np = np.array(gradients, dtype=np.float64)\n    \n    # Compute the new accumulated squared gradients\n    if G is None:\n        new_G = gradients_np ** 2\n    else:\n        G_np = np.array(G, dtype=np.float64)\n        new_G = G_np + gradients_np ** 2\n    \n    # Compute the adaptive learning rate adjustment\n    denominator = np.sqrt(new_G + eps)\n    delta = learning_rate * gradients_np / denominator\n    \n    # Update the weights\n    updated_weights = weights_np - delta\n    \n    # Round to 6 decimal places and convert back to lists\n    new_weights_list = np.round(updated_weights, 6).tolist()\n    new_G_list = np.round(new_G, 6).tolist()\n    \n    return (new_weights_list, new_G_list)\n```\n\n### Explanation\n1. **Conversion to Numpy Arrays**: The input lists (weights, gradients, and G) are converted to numpy arrays to facilitate efficient vectorized operations.\n2. **Accumulating Squared Gradients**: If this is the first iteration (G is None), initialize the sum of squared gradients with the square of the current gradients. Otherwise, update the existing sum by adding the square of the current gradients.\n3. **Adaptive Learning Rate Calculation**: Compute the denominator for each parameter as the square root of the accumulated squared gradients plus a small constant (epsilon). This denominator scales the learning rate individually for each parameter.\n4. **Parameter Update**: Adjust each parameter by subtracting the product of the learning rate and the gradient divided by the computed denominator.\n5. **Rounding and Conversion**: The updated parameters and the new sum of squared gradients are rounded to six decimal places and converted back to lists for the final output.\n\nThis approach ensures that each parameter's learning rate is adaptively adjusted based on its historical gradients, leading to more effective optimization, especially in scenarios with sparse data."}
{"task_id": 197, "completion_id": 0, "completion": "To solve this problem, we need to analyze the action space of a reinforcement learning (RL) environment and return statistics about its structure. The action space can be either discrete or continuous, and it can be single- or multi-dimensional. The goal is to provide the number of actions per dimension, a list of all possible discrete actions (if applicable), and the total number of action dimensions.\n\n### Approach\n1. **Determine the Structure of the Action Space**:\n   - Check if the action space is multi-dimensional by looking for a `spaces` attribute. If present, process each subspace individually.\n   - For each subspace, determine if it is discrete (has an `n` attribute) or continuous (has a `shape` attribute).\n   - For single-dimensional spaces, check if it is discrete or continuous by looking for `n` or `shape` attributes respectively.\n\n2. **Generate Statistics**:\n   - **n_actions_per_dim**: For each dimension, store the number of discrete actions or `inf` for continuous dimensions.\n   - **action_ids**: Generate all possible discrete actions using the Cartesian product if all dimensions are discrete. If any dimension is continuous, set this to `None`.\n   - **action_dim**: Calculate the total number of dimensions by summing up the dimensions from all subspaces.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\"\"\"\n    n_actions_per_dim = []\n    action_dim = 0\n    action_ids = None\n\n    if hasattr(env.action_space, 'spaces'):\n        # Handle multi-dimensional action space with subspaces\n        for subspace in env.action_space.spaces:\n            if hasattr(subspace, 'n'):\n                # Discrete subspace\n                n_actions_per_dim.append(subspace.n)\n                action_dim += 1\n            elif hasattr(subspace, 'shape'):\n                # Continuous subspace\n                dims = int(np.prod(subspace.shape))\n                n_actions_per_dim.extend([np.inf] * dims)\n                action_dim += dims\n            else:\n                raise ValueError(\"Unsupported subspace type in multi-dimensional action space.\")\n    else:\n        # Handle single-dimensional action space\n        if hasattr(env.action_space, 'n'):\n            # Discrete action space\n            n_actions_per_dim.append(env.action_space.n)\n            action_dim = 1\n        elif hasattr(env.action_space, 'shape'):\n            # Continuous action space\n            dims = int(np.prod(env.action_space.shape))\n            n_actions_per_dim.extend([np.inf] * dims)\n            action_dim = dims\n        else:\n            raise ValueError(\"Unsupported action space type.\")\n\n    # Determine if any dimension is continuous\n    has_continuous = any(np.isinf(n) for n in n_actions_per_dim)\n    if has_continuous:\n        action_ids = None\n    else:\n        # Generate all possible discrete action tuples\n        ranges = [range(n) for n in n_actions_per_dim]\n        action_ids = list(product(*ranges))\n\n    return (n_actions_per_dim, action_ids, action_dim)\n```\n\n### Explanation\n1. **Multi-dimensional Action Space**:\n   - If the environment's action space has a `spaces` attribute, iterate through each subspace. For each discrete subspace (with `n`), add the number of actions to `n_actions_per_dim` and increment the dimension count. For continuous subspaces (with `shape`), calculate the number of dimensions from the shape and add `inf` for each dimension.\n\n2. **Single-dimensional Action Space**:\n   - Check if the action space is discrete (using `n`) or continuous (using `shape`). For discrete, store the number of actions. For continuous, calculate the number of dimensions from the shape and add `inf` for each.\n\n3. **Generate Action IDs**:\n   - If any dimension is continuous, set `action_ids` to `None`. Otherwise, generate all possible discrete actions using the Cartesian product of ranges for each dimension.\n\nThis approach ensures that we correctly handle both single- and multi-dimensional action spaces, distinguish between discrete and continuous dimensions, and efficiently generate the required statistics."}
{"task_id": 198, "completion_id": 0, "completion": "To solve this problem, we need to update the word\u2013topic matrix \u03b2 in Latent Dirichlet Allocation (LDA) during the variational M-step. The goal is to compute the maximum-likelihood estimate of \u03b2 using the current variational parameters \u03c6 (phi) and the given corpus, then normalize the columns of \u03b2 so that each sums to 1. \n\n### Approach\n1. **Initialization**: Determine the number of topics (T) from the shape of the phi matrix. Initialize a zero matrix \u03b2 with dimensions V (vocabulary size) x T (number of topics).\n2. **Accumulate Counts**: For each document and each token in the document, update the \u03b2 matrix using the corresponding phi values. This involves adding the phi values for each token to the corresponding row in \u03b2 based on the word index from the corpus.\n3. **Normalization**: Compute the sum of each column in \u03b2 and normalize each column so that the sum equals 1. This ensures that each column represents a valid probability distribution over the vocabulary for the corresponding topic.\n4. **Rounding and Conversion**: Round each entry in the normalized \u03b2 matrix to 4 decimal places and convert the result to a list of lists for the required output format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    # Determine the number of topics T\n    T = phi[0].shape[1] if phi else 0\n    \n    # Initialize beta with zeros\n    beta = np.zeros((V, T), dtype=np.float64)\n    \n    # Iterate over each document and its tokens to accumulate counts\n    for d in range(len(phi)):\n        doc_phi = phi[d]  # Shape (N_d, T)\n        doc_words = corpus[d]  # List of word indices for document d\n        N_d = len(doc_words)\n        \n        for n in range(N_d):\n            v = doc_words[n]\n            beta[v, :] += doc_phi[n, :]\n    \n    # Compute the sum for each column (topic)\n    sums = beta.sum(axis=0)\n    \n    # Avoid division by zero by setting zero sums to 1 (though such columns would remain zero)\n    sums = np.where(sums == 0, 1.0, sums)\n    \n    # Normalize each column to sum to 1\n    beta_normalized = beta / sums.reshape(1, -1)\n    \n    # Round to 4 decimal places\n    beta_rounded = np.round(beta_normalized, 4)\n    \n    # Convert to a list of lists, ensuring all elements are standard Python floats\n    result = [\n        [float(round(val, 4)) for val in row]\n        for row in beta_rounded\n    ]\n    \n    return result\n```\n\n### Explanation\n- **Initialization**: The matrix \u03b2 is initialized with zeros to ensure that any word not present in the corpus will have a row of zeros.\n- **Accumulate Counts**: For each token in each document, we update the corresponding row in \u03b2 using the phi values. This step aggregates the contributions of each token to each topic based on the variational parameters.\n- **Normalization**: Each column of \u03b2 is normalized by dividing by the sum of its entries. This ensures that each column represents a valid probability distribution. If a column sum is zero (indicating no tokens were assigned to that topic), it is handled by setting the sum to 1 to avoid division by zero, resulting in a column of zeros.\n- **Rounding and Conversion**: The normalized \u03b2 matrix is rounded to 4 decimal places and converted to a list of lists, ensuring the output format meets the specified requirements.\n\nThis approach efficiently computes the updated \u03b2 matrix while adhering to the constraints and requirements of the problem."}
{"task_id": 199, "completion_id": 0, "completion": "To solve this problem, we need to split a label vector into two parts based on a threshold applied to a corresponding feature vector. This is a fundamental task in algorithms like decision trees, where data partitioning is crucial.\n\n### Approach\nThe approach involves using boolean indexing to efficiently split the labels based on the threshold. Here's the step-by-step breakdown:\n\n1. **Create a Boolean Mask**: Generate a boolean array where each element is `True` if the corresponding element in the feature vector is less than the threshold value, and `False` otherwise.\n2. **Split Labels Using the Mask**: Use this boolean mask to split the label vector into two parts:\n   - `y_left` contains labels where the corresponding feature values are strictly less than the threshold.\n   - `y_right` contains labels where the corresponding feature values are greater than or equal to the threshold.\n3. **Convert to Python Lists**: Convert the resulting NumPy arrays of labels into Python lists to meet the problem's output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    mask = X < value\n    y_left = y[mask].tolist()\n    y_right = y[~mask].tolist()\n    return (y_left, y_right)\n```\n\n### Explanation\n- **Boolean Mask Creation**: The mask `X < value` creates an array of boolean values where each element is `True` if the corresponding element in `X` is less than `value`, and `False` otherwise.\n- **Label Splitting**: Using this mask, `y[mask]` selects elements from `y` where the mask is `True`, forming `y_left`. The inverse mask `~mask` selects elements where the original mask was `False`, forming `y_right`.\n- **Conversion to Lists**: The `.tolist()` method converts the resulting NumPy arrays into Python lists, ensuring the output format is correct.\n\nThis approach efficiently leverages NumPy's vectorized operations for quick and readable splitting of the labels, maintaining the original order of elements as required."}
{"task_id": 200, "completion_id": 0, "completion": "To solve this problem, we need to compute the L1 regularization penalty for a given set of model weights and a regularization strength parameter. The L1 penalty is calculated as the sum of the absolute values of all weights multiplied by the regularization strength. The result should be rounded to four decimal places.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input weights can be a nested list or a NumPy array. Converting them to a NumPy array allows us to handle multi-dimensional data uniformly.\n2. **Sum Absolute Values**: Calculate the sum of the absolute values of all elements in the converted NumPy array. If the array is empty, this sum will naturally be zero.\n3. **Multiply by Regularization Strength**: Multiply the computed sum by the regularization strength parameter `C`.\n4. **Round the Result**: The final result is rounded to four decimal places to meet the specified precision requirement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_l1_penalty(weights, C):\n    \"\"\"Compute the L1 regularization penalty.\n\n    Args:\n        weights: Sequence or np.ndarray containing numeric weights (any shape).\n        C: Regularization strength (non-negative float).\n\n    Returns:\n        L1 penalty (float) rounded to 4 decimal places.\n    \"\"\"\n    arr = np.array(weights)\n    l1 = C * np.abs(arr).sum()\n    return round(l1, 4)\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input weights are converted to a NumPy array to handle both nested lists and arrays uniformly. This conversion simplifies the process of summing elements regardless of their original structure.\n2. **Summing Absolute Values**: Using `np.abs(arr).sum()`, we compute the sum of absolute values of all elements in the array. This works even if the array is empty, resulting in a sum of zero.\n3. **Applying Regularization Strength**: The sum of absolute values is multiplied by the regularization strength `C`, which scales the penalty according to the given strength.\n4. **Rounding**: The result is rounded to four decimal places using Python's built-in `round` function to ensure the output meets the required precision.\n\nThis approach efficiently handles various input structures and ensures correctness for both empty and non-empty weight inputs, providing an accurate and precise L1 regularization penalty."}
{"task_id": 201, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that clips the Euclidean (L2) norm of slices of a given tensor along a specified axis to a maximum value. This ensures that the norm of each slice does not exceed a given threshold, while maintaining numerical stability and converting the result to a nested Python list with values rounded to four decimal places.\n\n### Approach\n1. **Compute Norms**: Calculate the L2 norm of the input tensor along the specified axis. If no axis is specified, treat the entire tensor as a single vector.\n2. **Numerical Stability**: Add a small epsilon value (1e-12) to the computed norms to avoid division by zero.\n3. **Scaling Factor**: Compute the scaling factor for each norm such that it is the minimum of 1 and the ratio of the maximum allowed norm (m) to the computed norm. This ensures that if the norm is already within the allowed limit, it remains unchanged.\n4. **Apply Scaling**: Multiply the original tensor by the computed scaling factors to clip the norms as required.\n5. **Rounding and Conversion**: Round the resulting tensor values to four decimal places and convert the result to a nested Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef max_norm(p, m=2.0, axis=0):\n    epsilon = 1e-12\n    if axis is None:\n        norm = np.linalg.norm(p) + epsilon\n        scaling = min(1.0, m / norm)\n        clipped_p = p * scaling\n    else:\n        norms = np.linalg.norm(p, axis=axis, keepdims=True) + epsilon\n        scaling = np.minimum(1.0, m / norms)\n        clipped_p = p * scaling\n    rounded = np.round(clipped_p, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Compute Norms**: Using `np.linalg.norm`, we compute the L2 norm along the specified axis. If `axis` is `None`, the entire tensor is treated as a single vector.\n2. **Numerical Stability**: Adding `epsilon` ensures that even if the norm is zero, we avoid division by zero errors.\n3. **Scaling Factor**: The scaling factor is determined by the ratio of the maximum allowed norm `m` to the computed norm (plus `epsilon`), clamped at 1 to ensure values exceeding the norm are scaled down appropriately.\n4. **Apply Scaling**: The original tensor is multiplied by the scaling factors, which are broadcasted to match the tensor's shape, ensuring each slice along the specified axis is scaled correctly.\n5. **Rounding and Conversion**: The result is rounded to four decimal places using `np.round` and converted to a nested Python list using `tolist()` for the required output format.\n\nThis approach efficiently handles both single vector and multi-dimensional tensor cases, ensuring numerical stability and correct scaling while adhering to the specified constraints."}
{"task_id": 202, "completion_id": 0, "completion": "To solve this problem, we need to implement the Partitioning Around Medoids (PAM) clustering algorithm. This algorithm groups data points into clusters by iteratively improving a set of representative points called medoids. The goal is to minimize the total clustering cost, defined as the sum of the Euclidean distances between every sample and the medoid of the cluster it belongs to.\n\n### Approach\n1. **Initialization**: Start with the first `k` samples as the initial medoids.\n2. **Iterative Improvement**: Repeatedly attempt to reduce the total clustering cost by swapping any current medoid with a non-medoid sample. Each swap is evaluated based on the reduction in the total cost, and the swap that provides the largest cost reduction is accepted. This process continues until no further cost reduction is possible.\n3. **Convergence Check**: Stop when no swap can further decrease the cost.\n4. **Cluster Assignment**: After convergence, assign each sample to the cluster of the closest medoid. If two medoids are equidistant, choose the one that appears first in the medoid list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\"\"\"\n    n_samples = X.shape[0]\n    current_medoids = list(range(k))  # Initial medoids are the first k samples\n    \n    # Compute initial total cost\n    current_cost = 0.0\n    for x in X:\n        min_dist = np.min([np.linalg.norm(x - X[med]) for med in current_medoids])\n        current_cost += min_dist\n    \n    while True:\n        best_gain = 0.0\n        best_new_medoids = current_medoids.copy()\n        \n        # Evaluate all possible swaps\n        for m in current_medoids:\n            non_medoids = [s for s in range(n_samples) if s not in current_medoids]\n            for s in non_medoids:\n                # Create new_medoids by replacing m with s\n                new_medoids = current_medoids.copy()\n                idx = new_medoids.index(m)\n                new_medoids[idx] = s\n                \n                # Calculate new cost for this swap\n                new_cost = 0.0\n                for x in X:\n                    min_dist = np.inf\n                    for med in new_medoids:\n                        dist = np.linalg.norm(x - X[med])\n                        if dist < min_dist:\n                            min_dist = dist\n                    new_cost += min_dist\n                \n                gain = current_cost - new_cost\n                if gain > best_gain:\n                    best_gain = gain\n                    best_new_medoids = new_medoids\n        \n        # Check if any swap improved the cost\n        if best_gain > 0:\n            current_medoids = best_new_medoids\n            current_cost -= best_gain\n        else:\n            break  # Converged\n    \n    # Assign each sample to the closest medoid\n    labels = []\n    for x in X:\n        min_dist = np.inf\n        best_label = 0\n        for label_idx, med in enumerate(current_medoids):\n            dist = np.linalg.norm(x - X[med])\n            if dist < min_dist:\n                min_dist = dist\n                best_label = label_idx\n            elif dist == min_dist and label_idx < best_label:\n                best_label = label_idx\n        labels.append(best_label)\n    \n    return labels\n```\n\n### Explanation\n1. **Initialization**: The initial medoids are the first `k` samples of the input matrix `X`.\n2. **Cost Calculation**: The initial cost is calculated by summing the minimum Euclidean distances from each sample to the nearest medoid.\n3. **Iterative Improvement**: For each medoid and each non-medoid sample, compute the new cost if they were swapped. Track the swap that results in the maximum cost reduction. If such a swap exists, update the medoids and repeat the process.\n4. **Convergence Check**: The loop terminates when no further cost-reducing swaps are possible.\n5. **Cluster Assignment**: Each sample is assigned to the cluster of the closest medoid, with ties resolved by the order of medoids in the list.\n\nThis approach ensures that the algorithm follows a deterministic path, making the results reproducible and testable. The use of Euclidean distance and iterative improvement ensures that the total clustering cost is minimized effectively."}
{"task_id": 203, "completion_id": 0, "completion": "To solve this problem, we need to implement the alias method (Vose\u2019s algorithm) to build probability and alias tables for constant-time discrete sampling. This method allows efficient sampling from a discrete probability distribution after an O(N) preprocessing step.\n\n### Approach\n1. **Input Validation**: Check if the input probabilities are valid (non-negative and sum to 1 within a tolerance of 1e-12).\n2. **Scaling Probabilities**: Multiply each probability by the number of elements, N, to get scaled probabilities.\n3. **Initialization**: Create two lists, `small` and `large`, to hold indices of scaled probabilities less than 1 and greater than or equal to 1, respectively.\n4. **Table Construction**: Use a loop to process elements from `small` and `large`, adjusting the probabilities and aliases until one of the lists is empty.\n5. **Final Adjustments**: Set the remaining entries in `small` or `large` to have a probability of 1.0.\n6. **Rounding**: Round the probability table entries to four decimal places for consistency.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef build_alias_table(probs: list[float] | np.ndarray) -> tuple[list[float], list[int]]:\n    \"\"\"Builds probability and alias tables for the alias sampling method.\n\n    Args:\n        probs: 1-D list or NumPy array of positive floats summing to 1.\n\n    Returns:\n        Tuple (prob_table, alias_table) where:\n            * prob_table \u2013 list of floats rounded to 4 decimals.\n            * alias_table \u2013 list of ints.\n        If *probs* is invalid (negative entries or not summing to 1) return -1.\n    \"\"\"\n    # Convert to numpy array if necessary\n    if not isinstance(probs, np.ndarray):\n        probs = np.array(probs, dtype=np.float64)\n    else:\n        probs = probs.copy()\n    \n    # Check for negative values\n    if np.any(probs < 0):\n        return -1\n    \n    # Check sum to 1 within tolerance\n    if not np.isclose(np.sum(probs), 1.0, atol=1e-12):\n        return -1\n    \n    N = len(probs)\n    scaled_probs = probs * N\n    \n    small = [i for i in range(N) if scaled_probs[i] < 1.0]\n    large = [i for i in range(N) if scaled_probs[i] >= 1.0]\n    \n    prob_table = [0.0] * N\n    alias_table = [0] * N\n    \n    while small and large:\n        l = small.pop()\n        g = large.pop()\n        \n        alias_table[l] = g\n        prob_table[l] = scaled_probs[l]\n        \n        scaled_probs[g] += scaled_probs[l] - 1.0\n        \n        if scaled_probs[g] < 1.0:\n            small.append(g)\n        else:\n            large.append(g)\n    \n    # Set remaining entries in small or large to 1.0\n    for i in small:\n        prob_table[i] = 1.0\n    for i in large:\n        prob_table[i] = 1.0\n    \n    # Round to four decimal places\n    prob_table = [round(p, 4) for p in prob_table]\n    \n    return (prob_table, alias_table)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input probabilities are valid. If any probability is negative or the sum is not approximately 1, it returns -1.\n2. **Scaling Probabilities**: Each probability is scaled by multiplying by the number of elements, N. This helps in splitting the probabilities into those that need adjustment (small) and those that can contribute to others (large).\n3. **Table Construction**: The algorithm processes pairs of indices from `small` and `large`, adjusting their probabilities and updating the alias table. This loop continues until one of the lists is empty.\n4. **Final Adjustments**: Any remaining indices in `small` or `large` after the loop are set to have a probability of 1.0, as they no longer need adjustment.\n5. **Rounding**: The probabilities are rounded to four decimal places to ensure consistency and readability.\n\nThis approach efficiently constructs the necessary tables for constant-time sampling, ensuring correctness and adherence to the specified algorithm steps."}
{"task_id": 204, "completion_id": 0, "completion": "To solve this problem, we need to count the number of missing elements in a one-dimensional data container. An element is considered missing if it is either `None` or the special floating-point value `NaN` (Not-a-Number). The solution should work with any iterable that can be converted to a list, such as Python lists, tuples, or NumPy 1-D arrays, without using the `pandas` library.\n\n### Approach\n1. **Convert Input to List**: Convert the input data into a list to handle various iterable types uniformly.\n2. **Iterate Over Elements**: Check each element in the list.\n3. **Check for Missing Values**:\n   - If an element is `None`, it is considered missing.\n   - If an element is a floating-point value, check if it is `NaN` using `math.isnan()`. This function works for both standard Python `NaN` and NumPy `NaN`.\n4. **Handle Non-Float Types**: Use a try-except block to handle non-float types, which will not be considered missing values.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef num_missing(data) -> int:\n    \"\"\"Count missing values in a 1-D iterable.\n\n    An element is missing if it is ``None`` or ``NaN``.\n\n    Args:\n        data: Any 1-D iterable (list, tuple, NumPy array, etc.).\n\n    Returns:\n        int: The number of missing elements.\n    \"\"\"\n    count = 0\n    for elem in list(data):\n        if elem is None:\n            count += 1\n        else:\n            try:\n                if math.isnan(elem):\n                    count += 1\n            except TypeError:\n                pass\n    return count\n```\n\n### Explanation\n- **Conversion to List**: The input data is converted to a list to ensure compatibility with various iterable types, including NumPy arrays.\n- **Element Check**: Each element is checked for being `None` directly. For other elements, `math.isnan()` is used to check for `NaN` values. This function works for both Python and NumPy `NaN` values.\n- **Exception Handling**: A try-except block is used to handle non-float types, which will raise a `TypeError` when passed to `math.isnan()`. These types are safely ignored as they cannot be `NaN`.\n\nThis approach efficiently counts missing values while handling different data types and structures correctly, ensuring the solution is both robust and efficient."}
{"task_id": 205, "completion_id": 0, "completion": "To solve this problem, we need to extract unique environment IDs from a given registry list. The registry may contain various elements, but we are only interested in dictionaries that have an \"id\" key. The goal is to return a list of these IDs while preserving their order of first appearance and ensuring no duplicates.\n\n### Approach\n1. **Iterate through the Registry**: Check each element in the registry to determine if it is a dictionary containing the \"id\" key.\n2. **Track Unique IDs**: Use a set to keep track of IDs that have already been encountered to ensure uniqueness. This helps in efficiently checking for duplicates.\n3. **Maintain Order**: Use a list to store the IDs in the order they first appear. This list will be the result we return.\n\n### Solution Code\n```python\ndef get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    unique_ids = []\n    seen = set()\n    for item in registry:\n        if isinstance(item, dict) and 'id' in item:\n            env_id = item['id']\n            if env_id not in seen:\n                seen.add(env_id)\n                unique_ids.append(env_id)\n    return unique_ids\n```\n\n### Explanation\n1. **Initialization**: We start by initializing an empty list `unique_ids` to maintain the order of IDs as they first appear and a set `seen` to keep track of IDs that have already been added to the list.\n2. **Iteration and Filtering**: For each element in the registry, we check if it is a dictionary and contains the \"id\" key. If both conditions are met, we retrieve the value of the \"id\" key.\n3. **Uniqueness Check**: Before adding an ID to the result list, we check if it is already in the `seen` set. If not, we add it to both the `seen` set and the `unique_ids` list.\n4. **Result**: The function returns the `unique_ids` list, which contains the IDs in their order of first appearance without duplicates.\n\nThis approach ensures that we efficiently filter and track unique IDs while maintaining their order, adhering to the problem constraints."}
{"task_id": 206, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Mean Absolute Error (MAE) between two sequences of numbers. The MAE is the average of the absolute differences between corresponding elements of the actual and predicted sequences. The function must handle various input types, validate their structure, and compute the result efficiently using NumPy operations where possible.\n\n### Approach\n1. **Input Validation**: Ensure both inputs are either Python lists, tuples, or NumPy arrays. Check that they are one-dimensional and of the same length. Additionally, verify that all elements are numerical.\n2. **Conversion to NumPy Arrays**: Convert the inputs to NumPy arrays if they are not already, to leverage efficient NumPy operations.\n3. **MAE Calculation**: Compute the absolute differences between corresponding elements, take the average, and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        A one-dimensional sequence of ground-truth numeric values.\n    predicted : list | tuple | np.ndarray\n        A one-dimensional sequence of predicted numeric values.\n\n    Returns\n    -------\n    float | int\n        The MAE rounded to four decimal places. If the two sequences are not\n        of equal length or are not one-dimensional, return -1.\n    \"\"\"\n    # Validate input types\n    if not isinstance(actual, (list, tuple, np.ndarray)) or not isinstance(predicted, (list, tuple, np.ndarray)):\n        return -1\n    \n    # Check if both sequences have the same length\n    if len(actual) != len(predicted):\n        return -1\n    \n    # Validate each sequence's structure and data types\n    for seq in [actual, predicted]:\n        if isinstance(seq, np.ndarray):\n            if seq.ndim != 1:\n                return -1\n            if not np.issubdtype(seq.dtype, np.number):\n                return -1\n        else:\n            for element in seq:\n                if not isinstance(element, (int, float)):\n                    return -1\n    \n    # Convert to numpy arrays if they are not already\n    np_actual = np.asarray(actual) if not isinstance(actual, np.ndarray) else actual\n    np_predicted = np.asarray(predicted) if not isinstance(predicted, np.ndarray) else predicted\n    \n    # Calculate MAE\n    mae = np.mean(np.abs(np_actual - np_predicted))\n    return round(mae, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the inputs are valid types (list, tuple, or NumPy array). It then checks if they have the same length. Each sequence is validated to ensure it is one-dimensional and contains only numerical values.\n2. **Conversion to NumPy Arrays**: The inputs are converted to NumPy arrays if they are not already, allowing efficient numerical operations.\n3. **MAE Calculation**: Using NumPy's vectorized operations, the absolute differences between corresponding elements are computed, averaged, and rounded to four decimal places for the final result.\n\nThis approach ensures that the function efficiently handles different input types, performs necessary validations, and computes the result accurately using optimized NumPy operations."}
{"task_id": 207, "completion_id": 0, "completion": "To solve this problem, we need to compute the spatial dimensions of the output produced by a 2-D convolution layer in a convolutional neural network (CNN). The output dimensions are determined by several hyper-parameters, including the input image size, kernel size, stride, and padding. The goal is to ensure that the computed output dimensions are valid positive integers, otherwise return -1.\n\n### Approach\n1. **Parameter Validation**: Check that all input parameters (image dimensions, kernel size, stride) are positive integers. If any of these parameters are non-positive, the configuration is invalid.\n2. **Numerator Calculation**: Compute the numerator for both height and width using the formula derived from valid convolution parameters. This involves adjusting the input dimensions with padding and subtracting the kernel size.\n3. **Non-negative Check**: Ensure that the computed numerators for both height and width are non-negative. If either is negative, the configuration is invalid.\n4. **Divisibility Check**: Verify that the computed numerators are divisible by their respective strides. If not, the output dimensions would not be integers, making the configuration invalid.\n5. **Output Dimensions Calculation**: If all checks pass, compute the output dimensions using integer division and return them as a tuple.\n\n### Solution Code\n```python\ndef convolution_shape(img_height: int,\n                      img_width: int,\n                      filter_shape: tuple[int, int],\n                      stride: tuple[int, int],\n                      padding: tuple[int, int]) -> tuple[int, int] | int:\n    \"\"\"Compute the spatial dimensions of the output produced by a 2-D convolution.\n\n    Args:\n        img_height (int): Height of the input image.\n        img_width (int):  Width of the input image.\n        filter_shape (tuple[int, int]): (kernel_height, kernel_width).\n        stride (tuple[int, int]):       (stride_height, stride_width).\n        padding (tuple[int, int]):      (pad_height, pad_width).\n\n    Returns:\n        tuple[int, int] | int: (output_height, output_width) if the parameters\n            yield a valid integer output size; otherwise -1.\n    \"\"\"\n    kh, kw = filter_shape\n    sh, sw = stride\n    ph, pw = padding\n\n    # Check for non-positive parameters which are invalid\n    if (img_height <= 0 or img_width <= 0 or\n        kh <= 0 or kw <= 0 or\n        sh <= 0 or sw <= 0):\n        return -1\n\n    # Calculate numerators for height and width\n    numerator_h = img_height + 2 * ph - kh\n    numerator_w = img_width + 2 * pw - kw\n\n    # Check if numerators are non-negative and divisible by their respective strides\n    if numerator_h < 0 or numerator_w < 0:\n        return -1\n    if (numerator_h % sh != 0) or (numerator_w % sw != 0):\n        return -1\n\n    # Compute output dimensions\n    oh = (numerator_h // sh) + 1\n    ow = (numerator_w // sw) + 1\n\n    return (oh, ow)\n```\n\n### Explanation\n1. **Parameter Validation**: The function first checks if the input image dimensions, kernel size, and strides are all positive. If any of these values are non-positive, the configuration is immediately invalid, and the function returns -1.\n2. **Numerator Calculation**: The numerators for the height and width are calculated by adjusting the input dimensions with padding and subtracting the kernel size. This step determines how much the kernel can move within the input dimensions.\n3. **Non-negative Check**: The function checks if the calculated numerators are non-negative. If either is negative, the kernel cannot fit within the input dimensions even with padding, making the configuration invalid.\n4. **Divisibility Check**: The function ensures that the numerators are exactly divisible by their respective strides. If not, the output dimensions would not be integers, leading to an invalid configuration.\n5. **Output Dimensions Calculation**: If all checks pass, the output dimensions are calculated using integer division and returned as a tuple of positive integers. This ensures the output dimensions are valid and feasible for the given convolution parameters."}
{"task_id": 208, "completion_id": 0, "completion": "To solve this problem, we need to convert a list of categorical labels into their one-hot encoded representation. One-hot encoding is a common technique in machine learning to convert categorical data into a numerical format that algorithms can work with more effectively.\n\n### Approach\n1. **Determine Categories**: If the optional `categories` parameter is not provided, generate the list of categories from the input labels, preserving the order of their first appearance. If `categories` is provided, use it as the definitive list of possible categories.\n2. **Validation**: If `categories` is provided, ensure that all labels are present in the categories list. If any label is not found, return -1.\n3. **Index Mapping**: Create a mapping from each category to its index in the categories list. This helps in efficiently converting each label to its corresponding index.\n4. **Array Construction**: Construct a NumPy array initialized with zeros. Use the index mapping to set the appropriate positions in the array to 1, based on the labels.\n5. **Conversion**: Convert the NumPy array to a list of lists using the `tolist()` method and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef one_hot_encode(labels: list[str], categories: list[str] | None = None) -> list[list[int]]:\n    \"\"\"Convert categorical labels to a one-hot encoded matrix.\"\"\"\n    # Determine the categories if not provided\n    if categories is None:\n        seen = set()\n        categories = []\n        for label in labels:\n            if label not in seen:\n                seen.add(label)\n                categories.append(label)\n    else:\n        # Check if all labels are present in the provided categories\n        category_set = set(categories)\n        for label in labels:\n            if label not in category_set:\n                return -1\n    \n    # Create a mapping from category to index\n    index_map = {cat: idx for idx, cat in enumerate(categories)}\n    # Get the indices for each label\n    indices = [index_map[label] for label in labels]\n    \n    # Create the one-hot encoded array\n    n = len(labels)\n    c = len(categories)\n    array = np.zeros((n, c), dtype=int)\n    rows = np.arange(n)\n    array[rows, indices] = 1\n    \n    return array.tolist()\n```\n\n### Explanation\n1. **Determine Categories**: The function first checks if the `categories` parameter is provided. If not, it generates the categories by iterating through the labels and adding each unique label to the categories list in the order they first appear.\n2. **Validation**: If `categories` is provided, the function checks that every label in the input list exists in the provided categories. If any label is missing, the function returns -1.\n3. **Index Mapping**: A dictionary is created to map each category to its index. This allows for efficient lookup of indices when converting labels.\n4. **Array Construction**: A NumPy array of zeros is created with dimensions corresponding to the number of labels and categories. Using the indices derived from the index mapping, the appropriate positions in the array are set to 1.\n5. **Conversion**: The NumPy array is converted to a list of lists using `tolist()`, which is the required output format.\n\nThis approach ensures that the function efficiently handles both cases where categories are provided or inferred, validates input labels, and constructs the one-hot encoded matrix correctly."}
{"task_id": 209, "completion_id": 0, "completion": "To solve this problem, we need to analyze a multi-armed bandit's expected payoff, validate the input data, compute the expected values for each arm, determine the best arm based on these values, and return the results in a specified format. If the input data is invalid, we return -1 immediately.\n\n### Approach\n1. **Input Validation**:\n   - **Check Arm Count**: Ensure the number of arms (K) is the same for both payoffs and probabilities.\n   - **Check Length Match**: For each arm, ensure the number of payoffs matches the number of probabilities.\n   - **Check Probability Sum**: Ensure the sum of probabilities for each arm is 1 within a tolerance of 1e-8.\n\n2. **Compute Expected Values**:\n   - For each arm, calculate the expected value as the sum of the product of each payoff and its corresponding probability.\n\n3. **Determine Best Arm**:\n   - Round each expected value to 4 decimal places.\n   - Identify the arm with the highest rounded expected value. In case of ties, select the arm with the smallest index.\n\n### Solution Code\n```python\ndef analyze_bandit(payoffs: list[list[int | float]], payoff_probs: list[list[float]]) -> tuple[list[float], int, float] | int:\n    \"\"\"Return expected payoff of each arm, the best arm index, and its expected value.\"\"\"\n\n    # Check if the number of arms matches between payoffs and payoff_probs\n    if len(payoffs) != len(payoff_probs):\n        return -1\n\n    # Validate each arm's data\n    for k in range(len(payoffs)):\n        # Check if the lengths of payoffs and probabilities match for the current arm\n        if len(payoffs[k]) != len(payoff_probs[k]):\n            return -1\n        # Check if the probabilities sum to 1 within the allowed tolerance\n        prob_sum = sum(payoff_probs[k])\n        if abs(prob_sum - 1.0) > 1e-8:\n            return -1\n\n    # Calculate expected values for each arm, rounded to 4 decimal places\n    expected_values = []\n    for k in range(len(payoffs)):\n        ev = sum(payoff * prob for payoff, prob in zip(payoffs[k], payoff_probs[k]))\n        ev_rounded = round(ev, 4)\n        expected_values.append(ev_rounded)\n\n    # Determine the best arm based on the rounded expected values\n    best_ev = max(expected_values)\n    best_arm = expected_values.index(best_ev)\n\n    return (expected_values, best_arm, best_ev)\n```\n\n### Explanation\n1. **Input Validation**:\n   - We first check if the number of arms (K) in the payoffs and probabilities lists are the same. If not, the input is invalid.\n   - For each arm, we check if the number of payoffs matches the number of probabilities. If they don't match, the input is invalid.\n   - We then check if the sum of probabilities for each arm is approximately 1.0 within a tolerance of 1e-8. If any arm fails this check, the input is invalid.\n\n2. **Expected Value Calculation**:\n   - For each valid arm, we compute the expected value by summing the product of each payoff and its corresponding probability. This value is then rounded to 4 decimal places.\n\n3. **Best Arm Determination**:\n   - After computing the rounded expected values for all arms, we find the maximum value. The arm with this maximum value is considered the best. In case of ties (same rounded value), the arm with the smallest index is chosen.\n\nThis approach ensures that we handle all edge cases and validation checks efficiently, providing accurate results for valid inputs and immediate feedback for invalid ones."}
{"task_id": 210, "completion_id": 0, "completion": "To solve this problem, we need to create a decorator called `doublewrap` that allows another decorator to be used both with and without parentheses. This means that the decorated decorator should be able to handle cases where it is applied directly to a function (without any arguments) and cases where it is called with optional parameters before being applied to a function.\n\n### Approach\nThe `doublewrap` decorator works by wrapping another decorator function (referred to as `decorator_func`). The key idea is to check if the first positional argument passed to the wrapped decorator is a callable (function). If it is, the decorator is applied immediately. If not, the arguments are stored, and a new decorator is returned that will later apply the original decorator with the stored arguments when the target function is provided.\n\n1. **Check for Callable Argument**: The wrapped decorator checks if the first positional argument is a callable. If it is, this argument is treated as the function to be decorated, and `decorator_func` is called immediately with this function and any additional arguments.\n2. **Store Arguments and Return a New Decorator**: If the first positional argument is not a callable, the arguments and keyword arguments are stored. A new decorator is returned that, when given the target function, will call `decorator_func` with the stored arguments and the target function.\n\n### Solution Code\n```python\nimport functools\nfrom typing import Callable, Any\n\n\ndef doublewrap(decorator_func: Callable[..., Callable[..., Any]]) -> Callable[..., Any]:\n    \"\"\"Return a decorator that can be used *with* or *without* parentheses.\n\n    Args:\n        decorator_func: A callable that expects the function to decorate as\n            its first positional argument followed only by *optional*\n            positional or keyword arguments.\n\n    Returns:\n        A new decorator that forwards the call in the correct form so that\n        ``decorator_func`` can be applied either directly or after receiving\n        its own optional parameters.\n    \"\"\"\n\n    @functools.wraps(decorator_func)\n    def wrapper_decorator(*args, **kwargs):\n        if args and callable(args[0]):\n            # The first argument is the function to decorate, apply immediately\n            return decorator_func(*args, **kwargs)\n        else:\n            # Store the arguments and return a new decorator\n            def real_decorator(func: Callable) -> Any:\n                return decorator_func(func, *args, **kwargs)\n            return real_decorator\n\n    return wrapper_decorator\n```\n\n### Explanation\n- **Decorator Definition**: The `doublewrap` function takes a `decorator_func` as input and returns a new decorator (`wrapper_decorator`).\n- **Immediate Application Check**: The `wrapper_decorator` checks if the first positional argument is a callable. If true, it immediately applies `decorator_func` with the provided arguments.\n- **Delayed Application**: If the first argument is not a callable, `wrapper_decorator` returns a new decorator (`real_decorator`) that captures the arguments and keyword arguments. This new decorator will apply `decorator_func` to the target function when it is provided.\n- **Metadata Preservation**: Using `functools.wraps`, the `wrapper_decorator` preserves the original metadata (name, docstring, etc.) of the `decorator_func`.\n\nThis approach ensures that the decorated decorator can be used both with and without parentheses, providing flexibility and ease of use while maintaining the correct behavior and preserving metadata."}
{"task_id": 211, "completion_id": 0, "completion": "To solve this problem, we need to compute the classification accuracy of a model's predictions against the actual labels. The accuracy is defined as the proportion of correctly predicted labels, which can be derived by subtracting the classification error from 1. The classification error is the proportion of misclassified samples.\n\n### Approach\n1. **Input Validation**: First, check if the input sequences are of equal length and non-empty. If not, return -1.0.\n2. **Count Mismatches**: Iterate through the paired elements of the actual and predicted labels to count the number of mismatches.\n3. **Compute Error**: Calculate the classification error as the ratio of mismatches to the total number of samples.\n4. **Compute Accuracy**: Subtract the classification error from 1 to get the accuracy.\n5. **Round Result**: Round the accuracy to four decimal places and return it as a float.\n\n### Solution Code\n```python\ndef accuracy(actual, predicted):\n    \"\"\"Return the classification accuracy between two sequences of labels.\n\n    Parameters\n    ----------\n    actual : Sequence\n        The ground-truth labels.\n    predicted : Sequence\n        The predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy rounded to four decimal places, or -1 if the inputs\n        are empty or of unequal length.\n    \"\"\"\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1.0\n    total_samples = len(actual)\n    mismatches = sum(a != p for a, p in zip(actual, predicted))\n    error = mismatches / total_samples\n    acc = 1.0 - error\n    return round(acc, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of `actual` and `predicted` are equal and non-empty. If either condition is violated, it returns -1.0.\n2. **Mismatch Count**: Using a generator expression, the code counts how many pairs of actual and predicted labels do not match.\n3. **Error Calculation**: The classification error is calculated by dividing the number of mismatches by the total number of samples.\n4. **Accuracy Calculation**: The accuracy is derived by subtracting the error from 1.0.\n5. **Rounding**: The result is rounded to four decimal places using Python's `round` function to ensure the required precision.\n\nThis approach ensures that all edge cases are handled efficiently, and the solution is both concise and easy to understand."}
{"task_id": 212, "completion_id": 0, "completion": "To solve this problem, we need to find the shortest path from the \"input\" layer to the \"output_realtime\" layer in a directed acyclic graph (DAG) represented as an adjacency list. If multiple shortest paths exist, we must return the lexicographically smallest path.\n\n### Approach\n1. **Breadth-First Search (BFS)**: We use BFS to explore the graph starting from the \"input\" layer. BFS is suitable here because it efficiently finds the shortest path in an unweighted graph.\n2. **Track Distances and Predecessors**: During the BFS traversal, we track the shortest distance from the \"input\" to each layer and record all possible predecessors that contribute to the shortest path to each layer.\n3. **Backtrack to Construct Path**: Once BFS completes, if the \"output_realtime\" is reachable, we backtrack from \"output_realtime\" to \"input\" using the predecessors. At each step, we choose the lexicographically smallest predecessor to ensure the resulting path is the lexicographically smallest.\n\n### Solution Code\n```python\nfrom collections import deque\n\ndef extract_realtime_model(graph: dict[str, list[str]]) -> list[str]:\n    \"\"\"Find a shortest path from 'input' to 'output_realtime'.\"\"\"\n    all_nodes = set()\n    # Collect all nodes present in the graph, including those mentioned only as neighbors\n    for node in graph:\n        all_nodes.add(node)\n        all_nodes.update(graph[node])\n    # Ensure 'input' and 'output_realtime' are considered even if not in the graph's keys\n    all_nodes.add('input')\n    all_nodes.add('output_realtime')\n    \n    # Initialize distance and predecessors\n    distance = {node: None for node in all_nodes}\n    predecessors = {node: [] for node in all_nodes}\n    queue = deque()\n    \n    # Start BFS from 'input'\n    if 'input' not in distance:\n        return []\n    distance['input'] = 0\n    queue.append('input')\n    \n    while queue:\n        current = queue.popleft()\n        current_distance = distance[current]\n        \n        # Get neighbors from the graph; default to empty list if not present\n        neighbors = graph.get(current, [])\n        for neighbor in neighbors:\n            if distance[neighbor] is None:\n                # First visit to this neighbor\n                distance[neighbor] = current_distance + 1\n                predecessors[neighbor].append(current)\n                queue.append(neighbor)\n            else:\n                # Check if this path is also a shortest path\n                if distance[neighbor] == current_distance + 1:\n                    predecessors[neighbor].append(current)\n    \n    # Check if 'output_realtime' is reachable\n    if distance['output_realtime'] is None:\n        return []\n    \n    # Backtrack to find the lexicographically smallest path\n    path = []\n    current = 'output_realtime'\n    while current != 'input':\n        path.append(current)\n        # Get all predecessors which are part of a shortest path\n        preds = predecessors[current]\n        if not preds:\n            return []  # This should not happen if 'output_realtime' is reachable\n        # Choose the lexicographically smallest predecessor\n        current = min(preds)\n    path.append('input')\n    # Reverse to get the path from input to output_realtime\n    path.reverse()\n    return path\n```\n\n### Explanation\n1. **Collect All Nodes**: We first gather all nodes mentioned in the graph, including those only appearing as neighbors, to ensure we handle all possible layers.\n2. **BFS Initialization**: We start BFS from the \"input\" layer, initializing its distance to 0 and all other nodes' distances to None.\n3. **BFS Traversal**: As we process each node, we explore its neighbors. If a neighbor is visited for the first time, we set its distance and record the current node as its predecessor. If a neighbor has already been visited but the current path provides another shortest path, we add the current node as another predecessor.\n4. **Backtracking**: After BFS completes, if \"output_realtime\" is reachable, we backtrack from it to \"input\" by always choosing the lexicographically smallest predecessor at each step. This ensures the path is the shortest and lexicographically smallest.\n5. **Reverse Path**: The constructed path is reversed to present it from \"input\" to \"output_realtime\".\n\nThis approach efficiently finds the shortest path and ensures lexicographical order when multiple shortest paths exist."}
{"task_id": 213, "completion_id": 0, "completion": "To solve this problem, we need to implement the He Normal weight initialization strategy for neural networks. This strategy initializes weights using a normal distribution with a mean of 0 and a standard deviation derived from the number of incoming connections (fan_in) to the neurons. The goal is to ensure that the weights are initialized in a way that accelerates the training process of deep learning models.\n\n### Approach\n1. **Compute fan_in**: The number of incoming connections (fan_in) is determined based on the shape of the weight tensor:\n   - For 1D tensors (len(shape) == 1), fan_in is the size of the single dimension.\n   - For 2D tensors (len(shape) == 2), fan_in is the number of input units (first dimension).\n   - For 3D or higher-dimensional tensors (len(shape) >= 3), fan_in is calculated as the product of the number of input channels (second dimension) and the receptive field size (product of the remaining dimensions).\n\n2. **Check for zero fan_in**: If fan_in is zero (which can happen if the shape includes a zero), return an empty list.\n\n3. **Generate weights**: Using NumPy's random generator with a fixed seed (42 for reproducibility), draw samples from a normal distribution with mean 0 and standard deviation \u221a(2 / fan_in).\n\n4. **Return as nested list**: Convert the generated NumPy array to a nested Python list using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef he_normal(shape: tuple[int, ...]) -> list:\n    \"\"\"Return weights initialised with He Normal strategy.\n\n    Args:\n        shape: A tuple that represents the desired tensor shape.\n\n    Returns:\n        Nested Python list containing the initialised weights.\n    \"\"\"\n    if not shape:\n        return []\n    \n    n_dims = len(shape)\n    fan_in = 0\n    \n    if n_dims == 1:\n        fan_in = shape[0]\n    elif n_dims == 2:\n        fan_in = shape[0]\n    else:\n        receptive_field = 1\n        for dim in shape[2:]:\n            receptive_field *= dim\n        fan_in = shape[1] * receptive_field\n    \n    if fan_in == 0:\n        return []\n    \n    stddev = np.sqrt(2.0 / fan_in)\n    np.random.seed(42)\n    weights = np.random.normal(loc=0.0, scale=stddev, size=shape)\n    return weights.tolist()\n```\n\n### Explanation\n- **fan_in Calculation**: The function first determines the number of incoming connections (fan_in) based on the shape of the tensor. This is crucial because the variance of the initial weights scales inversely with fan_in, helping to maintain stable gradients during training.\n- **Zero Check**: If fan_in is zero (due to an invalid shape with a zero dimension), the function returns an empty list to avoid division by zero errors.\n- **Reproducibility**: By setting a fixed seed (42) for NumPy's random number generator, the function ensures that the generated weights are reproducible across different runs.\n- **Efficiency**: The function efficiently computes the product of dimensions for higher-dimensional tensors to determine the receptive field size, ensuring correct fan_in calculation for convolutional layers.\n\nThis approach ensures that the weights are initialized optimally for various neural network layers, promoting faster convergence during training."}
{"task_id": 214, "completion_id": 0, "completion": "To solve this problem, we need to compute the Mean Squared Error (MSE) between two numerical sequences, which can be either Python lists or NumPy arrays. The MSE is a measure of the average squared difference between the actual and predicted values. The solution must handle both list and array inputs, check for equal shape or length, and return the result rounded to four decimal places.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert both input sequences (actual and predicted) to NumPy arrays to handle both lists and arrays uniformly.\n2. **Check Shape Compatibility**: Verify if the two arrays have the same shape. If not, return -1 immediately.\n3. **Compute Squared Errors**: Calculate the element-wise squared differences between the actual and predicted values.\n4. **Calculate Mean of Squared Errors**: Compute the mean of these squared differences to get the MSE.\n5. **Round the Result**: Use NumPy's rounding function to round the MSE to four decimal places and return it as a Python float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mean_squared_error(actual, predicted):\n    \"\"\"Calculate the Mean Squared Error (MSE) between two numerical sequences.\n\n    The function must return -1 if the two input sequences do not share the\n    same length. Otherwise, it should return the MSE rounded to 4 decimal\n    places. Both Python lists and NumPy arrays have to be supported.\n\n    Args:\n        actual (list[int | float] | np.ndarray): Ground-truth values.\n        predicted (list[int | float] | np.ndarray): Predicted values.\n\n    Returns:\n        float | int: The rounded MSE, or -1 when the inputs are incompatible.\n    \"\"\"\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    \n    if actual.shape != predicted.shape:\n        return -1\n    \n    squared_errors = (actual - predicted) ** 2\n    mse = np.mean(squared_errors)\n    rounded_mse = np.round(mse, 4)\n    \n    return rounded_mse.item()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The inputs are converted to NumPy arrays to ensure uniform handling, whether the inputs are lists or arrays.\n2. **Shape Check**: The shapes of the converted arrays are compared. If they differ, the function returns -1, indicating incompatible input dimensions.\n3. **Squared Errors Calculation**: Element-wise subtraction followed by squaring gives the squared errors, which are then averaged to compute the MSE.\n4. **Rounding**: The result is rounded to four decimal places using NumPy's rounding function, ensuring the output meets the specified precision requirement. The `.item()` method converts the NumPy scalar to a native Python float for consistent return type handling.\n\nThis approach ensures that the function works efficiently with both Python lists and NumPy arrays while adhering to the problem requirements."}
{"task_id": 216, "completion_id": 0, "completion": "To solve this problem, we need to implement Elastic Net linear regression from scratch using batch gradient descent. The model should generate polynomial features up to a specified degree, standardize these features, and use a combination of L1 and L2 regularization to learn the weights. The solution must also handle input validation and return predictions for unseen data.\n\n### Approach\n1. **Input Validation**: Check if the input matrices `x_train` and `x_test` have the same number of features and if the number of iterations is valid.\n2. **Polynomial Feature Expansion**: Generate polynomial features for each original feature up to the specified degree, including a bias term (column of ones).\n3. **Standardization**: Standardize the polynomial features (excluding the bias term) using the mean and standard deviation from the training data.\n4. **Model Training**: Use batch gradient descent to minimize the mean squared error with Elastic Net regularization. The gradient includes contributions from both L1 and L2 penalties.\n5. **Prediction**: Apply the learned weights to the standardized test data and return the predictions rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_test = np.array(x_test)\n    \n    # Reshape if input is 1D\n    if x_train.ndim == 1:\n        x_train = x_train.reshape(-1, 1)\n    if x_test.ndim == 1:\n        x_test = x_test.reshape(-1, 1)\n    \n    # Check input compatibility\n    if x_train.shape[1] != x_test.shape[1] or n_iterations < 1:\n        return -1\n    \n    # Function to generate polynomial features including bias term\n    def generate_poly(x, degree):\n        n_samples, n_features = x.shape\n        poly = np.ones((n_samples, 1))  # Start with bias term\n        for feature_idx in range(n_features):\n            feature_col = x[:, feature_idx]\n            for power in range(1, degree + 1):\n                new_col = (feature_col ** power).reshape(-1, 1)\n                poly = np.hstack((poly, new_col))\n        return poly\n    \n    # Generate polynomial features for train and test\n    x_train_poly = generate_poly(x_train, degree)\n    x_test_poly = generate_poly(x_test, degree)\n    \n    # Standardize non-bias features using training data's mean and std\n    x_train_non_bias = x_train_poly[:, 1:]\n    if x_train_non_bias.size == 0:\n        x_train_std = x_train_poly\n        x_test_std = x_test_poly\n    else:\n        means = np.mean(x_train_non_bias, axis=0)\n        stds = np.std(x_train_non_bias, axis=0, ddof=0)\n        stds[stds == 0] = 1.0  # Avoid division by zero\n        x_train_std = x_train_poly.copy()\n        x_train_std[:, 1:] = (x_train_non_bias - means) / stds\n        # Apply same transformation to test data\n        x_test_non_bias = x_test_poly[:, 1:]\n        x_test_std = x_test_poly.copy()\n        x_test_std[:, 1:] = (x_test_non_bias - means) / stds\n    \n    # Initialize weights\n    n_samples_train, n_features = x_train_std.shape\n    w = np.zeros(n_features)\n    \n    # Perform batch gradient descent\n    for _ in range(n_iterations):\n        y_pred = x_train_std @ w\n        error = y_pred - y_train\n        gradient_mse = (x_train_std.T @ error) / n_samples_train\n        # Compute regularization gradient for non-bias terms\n        reg_grad = np.zeros_like(w)\n        if n_features > 1:\n            mask = np.arange(n_features) >= 1\n            w_non_bias = w[mask]\n            sign_w = np.sign(w_non_bias)\n            l1_part = l1_ratio * sign_w\n            l2_part = 2 * (1 - l1_ratio) * w_non_bias\n            reg_grad[mask] = reg_factor * (l1_part + l2_part)\n        total_gradient = gradient_mse + reg_grad\n        w -= learning_rate * total_gradient\n    \n    # Predict on test data\n    y_pred_test = x_test_std @ w\n    return np.round(y_pred_test, 4).tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input matrices `x_train` and `x_test` have compatible shapes and if the number of iterations is valid.\n2. **Polynomial Feature Expansion**: Polynomial features are generated for each original feature up to the specified degree. This includes a bias term (column of ones) added as the first column.\n3. **Standardization**: The non-bias features are standardized using the mean and standard deviation from the training data to ensure zero mean and unit variance. The same transformation is applied to the test data.\n4. **Model Training**: The model uses batch gradient descent to update the weights. The gradient includes contributions from both the mean squared error and the Elastic Net regularization term, which combines L1 and L2 penalties.\n5. **Prediction**: The trained model is applied to the standardized test data to generate predictions, which are then rounded to four decimal places and returned as a list."}
{"task_id": 217, "completion_id": 0, "completion": "To solve this problem, we need to compute three core components of the logistic (sigmoid) loss for binary classification tasks. These components are the gradient of the logistic loss, the Hessian (second derivative), and the probability obtained from the sigmoid transformation of predicted scores. The solution must efficiently handle vectorized operations and ensure the results are rounded to six decimal places.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: This allows for efficient vectorized operations, which are crucial for performance when dealing with large datasets.\n2. **Compute Sigmoid Values**: The sigmoid function is applied to the predicted scores to obtain probabilities. This is also used in calculating the Hessian.\n3. **Calculate Gradient**: The gradient for each observation is derived using the sigmoid of the negative product of actual and predicted values.\n4. **Calculate Hessian**: The Hessian is computed using the product of the sigmoid values and their complements.\n5. **Round Results**: Each computed value is rounded to six decimal places and converted back to Python lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    \n    # Compute probabilities and Hessian components\n    s_predicted = 1 / (1 + np.exp(-predicted))\n    hessian = s_predicted * (1 - s_predicted)\n    prob = s_predicted\n    \n    # Compute gradient components\n    s_grad = 1 / (1 + np.exp(actual * predicted))\n    grad = actual * s_grad\n    \n    # Round to 6 decimal places and convert to lists\n    grad_list = np.round(grad, 6).tolist()\n    hessian_list = np.round(hessian, 6).tolist()\n    prob_list = np.round(prob, 6).tolist()\n    \n    return (grad_list, hessian_list, prob_list)\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The inputs are converted to NumPy arrays to leverage vectorized operations, which are more efficient than iterating through each element individually.\n2. **Sigmoid Calculation**: The sigmoid function is applied to the predicted values to get probabilities. This is done using the formula \\( \\text{sigmoid}(z) = 1 / (1 + e^{-z}) \\).\n3. **Gradient Calculation**: The gradient for each observation is computed using the formula \\( \\text{grad}_i = \\text{actual}_i \\times \\text{sigmoid}(-\\text{actual}_i \\times \\text{predicted}_i) \\). This is efficiently computed using vectorized operations.\n4. **Hessian Calculation**: The Hessian is derived from the product of the sigmoid values and their complements, which is the derivative of the sigmoid function.\n5. **Rounding and Conversion**: The results are rounded to six decimal places using NumPy's rounding function and then converted to Python lists to meet the output requirements.\n\nThis approach ensures that the computations are efficient, accurate, and meet the specified formatting requirements."}
{"task_id": 218, "completion_id": 0, "completion": "To solve this problem, we need to determine the outcome of a Blackjack game based on the final hands of the player and the dealer. The solution must adhere to the specific rules of Blackjack regarding hand values, busts, and natural blackjacks to compute the correct reward for the player.\n\n### Approach\n1. **Calculate Hand Scores**: For both the player and the dealer, compute the score of their hands. Each Ace (1) can be counted as 11 if it helps achieve a score of 21 or less. The score is the highest possible value without exceeding 21.\n2. **Check for Busts**: If either the player or the dealer has a score over 21, they bust. The player loses immediately if they bust, and wins if the dealer busts.\n3. **Check for Natural Blackjacks**: A natural blackjack is a two-card hand that sums to 21 (one Ace and one 10-value card). If the player has a natural blackjack and the dealer does not, the player gets a higher reward (1.5 instead of 1). If both have natural blackjacks, it's a draw.\n4. **Compare Scores**: If neither busts and there are no natural blackjacks, compare the scores. The higher score wins, with equal scores resulting in a draw.\n\n### Solution Code\n```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\"\"\"\n\n    # Calculate player's score\n    player_sum = sum(player)\n    player_aces = player.count(1)\n    if player_aces > 0 and (player_sum + 10) <= 21:\n        player_score = player_sum + 10\n    else:\n        player_score = player_sum\n\n    if player_score > 21:\n        return -1.0\n\n    # Calculate dealer's score\n    dealer_sum = sum(dealer)\n    dealer_aces = dealer.count(1)\n    if dealer_aces > 0 and (dealer_sum + 10) <= 21:\n        dealer_score = dealer_sum + 10\n    else:\n        dealer_score = dealer_sum\n\n    if dealer_score > 21:\n        return 1.0\n\n    # Check for naturals\n    player_natural = len(player) == 2 and player_score == 21\n    dealer_natural = len(dealer) == 2 and dealer_score == 21\n\n    if player_natural:\n        if dealer_natural:\n            return 0.0\n        else:\n            return 1.5 if dealer_score != 21 else 0.0\n    elif dealer_natural:\n        return 0.0 if player_score == 21 else -1.0\n    else:\n        if player_score > dealer_score:\n            return 1.0\n        elif player_score < dealer_score:\n            return -1.0\n        else:\n            return 0.0\n```\n\n### Explanation\n1. **Hand Score Calculation**: The score is calculated by summing the card values. If there are Aces and adding 10 to the sum does not exceed 21, one Ace is counted as 11.\n2. **Bust Check**: If the player's score exceeds 21, they lose immediately. If the dealer's score exceeds 21, the player wins.\n3. **Natural Blackjack Check**: A natural blackjack is identified by a two-card hand summing to 21. If both have natural blackjacks, it's a draw. If only the player has one, they win with a reward of 1.5.\n4. **Score Comparison**: If neither has a natural blackjack and neither busts, the scores are compared directly to determine the winner.\n\nThis approach ensures that all Blackjack rules are followed accurately, providing the correct reward based on the game's outcome."}
{"task_id": 220, "completion_id": 0, "completion": "To solve this problem, we need to estimate the word-topic (phi) and document-topic (theta) distributions for an LDA model using symmetric Dirichlet smoothing. The task involves computing these distributions from count matrices after applying Dirichlet priors to smooth the estimates.\n\n### Approach\n1. **Check for Empty Matrices**: If either the word-topic count matrix (C_wt) or the document-topic count matrix (C_dt) is empty, return an empty list for the corresponding distribution.\n2. **Compute Phi (\u03d5) Distribution**:\n   - **Sum Columns**: For each topic (column), sum the counts across all words.\n   - **Denominator Calculation**: For each topic, compute the denominator as the sum of counts for that topic plus the product of the vocabulary size (V) and the Dirichlet prior (\u03b2).\n   - **Smoothing**: Add \u03b2 to each count and divide by the computed denominator for each topic to get the smoothed probabilities.\n3. **Compute Theta (\u03b8) Distribution**:\n   - **Sum Rows**: For each document (row), sum the counts across all topics.\n   - **Denominator Calculation**: For each document, compute the denominator as the sum of counts for that document plus the product of the number of topics (T) and the Dirichlet prior (\u03b1).\n   - **Smoothing**: Add \u03b1 to each count and divide by the computed denominator for each document to get the smoothed probabilities.\n4. **Rounding and Conversion**: Convert the resulting matrices to nested lists with each entry rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef estimate_lda_params(C_wt, C_dt, alpha, beta):\n    \"\"\"Estimate the word-topic (phi) and document-topic (theta) distributions\n    for an LDA model using symmetric Dirichlet smoothing.\n\n    Parameters\n    ----------\n    C_wt : numpy.ndarray\n        Word\u2013topic count matrix of shape (V, T).\n    C_dt : numpy.ndarray\n        Document\u2013topic count matrix of shape (D, T).\n    alpha : float\n        Symmetric Dirichlet prior for theta (document\u2013topic).\n    beta : float\n        Symmetric Dirichlet prior for phi  (word\u2013topic).\n\n    Returns\n    -------\n    tuple[list[list[float]], list[list[float]]]\n        Rounded (to 4 decimals) phi and theta matrices as nested Python lists.\n    \"\"\"\n    # Compute phi matrix\n    phi = []\n    if C_wt.size > 0:\n        V, T = C_wt.shape\n        sum_cols = C_wt.sum(axis=0)\n        denominator = sum_cols + V * beta\n        phi_matrix = (C_wt + beta) / denominator[np.newaxis, :]\n        phi = np.round(phi_matrix, 4).tolist()\n    else:\n        phi = []\n    \n    # Compute theta matrix\n    theta = []\n    if C_dt.size > 0:\n        D, T_theta = C_dt.shape\n        sum_rows = C_dt.sum(axis=1)\n        denominator_theta = sum_rows + T_theta * alpha\n        theta_matrix = (C_dt + alpha) / denominator_theta[:, np.newaxis]\n        theta = np.round(theta_matrix, 4).tolist()\n    else:\n        theta = []\n    \n    return (phi, theta)\n```\n\n### Explanation\n1. **Handling Empty Matrices**: The code first checks if the input matrices (C_wt and C_dt) are empty using the `size` attribute. If either matrix is empty, the corresponding distribution (phi or theta) is returned as an empty list.\n2. **Phi Calculation**: For each topic (column) in the word-topic count matrix, the sum of counts is computed. The denominator for each topic is adjusted by adding the product of the vocabulary size and \u03b2. Each count in the matrix is then smoothed by adding \u03b2 and dividing by the adjusted denominator.\n3. **Theta Calculation**: For each document (row) in the document-topic count matrix, the sum of counts is computed. The denominator for each document is adjusted by adding the product of the number of topics and \u03b1. Each count in the matrix is smoothed by adding \u03b1 and dividing by the adjusted denominator.\n4. **Conversion to Nested Lists**: The resulting matrices are rounded to four decimal places and converted to nested lists using NumPy's `tolist()` method to meet the output requirements.\n\nThis approach ensures that the Dirichlet smoothing is applied correctly and efficiently using vectorized operations, providing accurate and readable results."}
{"task_id": 221, "completion_id": 0, "completion": "To solve this problem, we need to implement the Mean Squared Logarithmic Error (MSLE) metric, which is commonly used to evaluate regression models. The MSLE is defined as the average of the squared differences between the logarithms of the actual and predicted values, adjusted by adding 1 to each value before taking the logarithm. \n\n### Approach\n1. **Input Validation**: Ensure that the actual and predicted values have the same shape and that all values are non-negative. If any check fails, return -1.\n2. **Helper Function**: Compute the element-wise squared logarithmic error between actual and predicted values using the formula \\((\\log(1 + \\text{actual}_i) - \\log(1 + \\text{predicted}_i))^2\\) for each element.\n3. **Main Function**: Use the helper function to compute the squared logarithmic errors, then take the mean of these errors and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"Return element-wise squared log errors.\"\"\"\n    return (np.log1p(actual) - np.log1p(predicted)) ** 2\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    \n    # Check if the shapes are the same\n    if actual.shape != predicted.shape:\n        return -1.0\n    \n    # Check all elements are non-negative\n    if (actual < 0).any() or (predicted < 0).any():\n        return -1.0\n    \n    # Calculate squared log errors and their mean\n    sle = squared_log_error(actual, predicted)\n    msle = np.mean(sle)\n    \n    # Round to 4 decimal places\n    return round(msle, 4)\n```\n\n### Explanation\n1. **Input Validation**: The code first converts the input sequences (lists, tuples, or arrays) into NumPy arrays. It then checks if these arrays have the same shape. If not, it returns -1. Next, it checks if all elements in both arrays are non-negative. If any element is negative, it returns -1.\n2. **Helper Function**: The helper function `squared_log_error` computes the element-wise squared logarithmic errors using NumPy's `log1p` function, which efficiently computes \\(\\log(1 + x)\\).\n3. **Main Function**: The main function `mean_squared_log_error` uses the helper function to get the squared errors, computes their mean, and rounds the result to 4 decimal places before returning it as a float.\n\nThis approach ensures that the implementation is efficient, handles edge cases through input validation, and adheres to the mathematical definition of MSLE."}
{"task_id": 222, "completion_id": 0, "completion": "To solve this problem, we need to implement the sigmoid activation function that works element-wise on scalar values, Python lists, and NumPy arrays. The solution must be numerically stable for very large positive or negative values and return the results in the appropriate format based on the input type.\n\n### Approach\nThe sigmoid function is defined as:\n\\[ \\text{sigmoid}(z) = \\frac{1}{1 + \\exp(-z)} \\]\n\nTo ensure numerical stability, especially for very large positive or negative values, we split the computation into two cases:\n1. For \\( z \\geq 0 \\), we use the standard formula \\( 1 / (1 + \\exp(-z)) \\).\n2. For \\( z < 0 \\), we use the alternative formula \\( \\exp(z) / (1 + \\exp(z)) \\).\n\nThis approach avoids overflow issues by preventing the exponentiation of large positive numbers.\n\nThe solution handles different input types as follows:\n1. **Scalar Input**: Convert to a float, compute the sigmoid value using the stable formula, and return a float rounded to 4 decimal places.\n2. **List or NumPy Array Input**: Convert the input to a NumPy array, apply the sigmoid function element-wise using vectorized operations for efficiency, and convert the result back to a Python list with the same structure, rounding each element to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        # Handle scalar input (int, float) or scalar numpy array\n        if isinstance(z, np.ndarray):\n            z_scalar = z.item()\n        else:\n            z_scalar = z\n        if z_scalar >= 0:\n            s = 1.0 / (1.0 + np.exp(-z_scalar))\n        else:\n            s = np.exp(z_scalar) / (1.0 + np.exp(z_scalar))\n        return round(s, 4)\n    else:\n        # Handle array-like input (list or numpy array)\n        arr = np.array(z, dtype=np.float64)\n        # Compute the sigmoid with numerical stability\n        result = np.where(arr >= 0, 1.0 / (1.0 + np.exp(-arr)), np.exp(arr) / (1.0 + np.exp(arr)))\n        rounded = np.round(result, 4)\n        return rounded.tolist()\n```\n\n### Explanation\n1. **Scalar Handling**: The function first checks if the input is a scalar (either a Python int/float or a scalar NumPy array). If so, it computes the sigmoid using the stable formula, rounds the result to 4 decimal places, and returns it as a float.\n2. **Array Handling**: For non-scalar inputs (lists or NumPy arrays), the input is converted to a NumPy array. The sigmoid function is applied element-wise using `np.where` to handle positive and negative values appropriately. The results are rounded to 4 decimal places and converted back to a Python list to maintain the input structure.\n\nThis approach ensures numerical stability and efficient computation across all input types, adhering to the specified return format."}
{"task_id": 224, "completion_id": 0, "completion": "To solve this problem, we need to implement the Leaky ReLU activation function, which is a common activation function used in neural networks to avoid the \"dying neuron\" problem. The Leaky ReLU allows a small, non-zero gradient when the input is negative. The task requires the function to handle various input types, including scalars, nested lists, and numpy arrays, and return the result as a Python list with the same structure as the input.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Check Input Type**: Determine if the input is a numpy array, a list (possibly nested), or a scalar.\n2. **Process Numpy Arrays**: Use vectorized operations for efficient computation on numpy arrays, then convert the result to a Python list.\n3. **Process Lists Recursively**: Handle nested lists by recursively applying the Leaky ReLU function to each element.\n4. **Handle Scalars Directly**: Apply the Leaky ReLU function directly to scalar values.\n\nThis approach ensures that the function works efficiently for numpy arrays using vectorized operations while correctly handling nested lists through recursion and scalars through direct computation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(element, a) for element in z]\n    else:\n        return z if z >= 0 else a * z\n```\n\n### Explanation\n- **Numpy Array Handling**: When the input is a numpy array, the function uses `np.where` to apply the Leaky ReLU element-wise. This is efficient and leverages numpy's optimized operations. The result is then converted to a Python list using `.tolist()`.\n- **List Handling**: If the input is a list (including nested lists), the function processes each element recursively. Each element is checked and processed according to its type, ensuring the nested structure is preserved.\n- **Scalar Handling**: For scalar values (integers or floats), the function directly applies the Leaky ReLU formula, returning the value itself if non-negative or `a` times the value if negative.\n\nThis solution efficiently handles all specified input types and ensures the output maintains the same structure as the input, converted to a Python list where necessary."}
{"task_id": 225, "completion_id": 0, "completion": "To solve this problem, we need to convert frequencies from Hertz (Hz) to the mel scale using one of two specified formulas. The conversion must handle both one-dimensional and multi-dimensional input arrays, and return the results as a Python list with the same shape as the input, rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: Convert the input to a numpy array to facilitate element-wise operations.\n2. **Formula Validation**: Ensure the conversion formula is either \"htk\" or \"slaney\"; raise an error if not.\n3. **Conversion Calculation**: Apply the appropriate formula based on the input parameter:\n   - **HTK Formula**: `mel = 2595 * log10(1 + hz / 700)`\n   - **Slaney Formula**: `mel = 1127 * ln(1 + hz / 700)`\n4. **Rounding and Conversion**: Round the results to four decimal places and convert the numpy array back to a Python list, preserving the original shape.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hz2mel(hz, formula: str = \"htk\") -> list:\n    \"\"\"Convert a set of frequencies from Hertz to the mel scale.\n\n    Parameters\n    ----------\n    hz : array-like of float\n        One- or multi-dimensional collection of frequencies in Hertz.\n    formula : {\"htk\", \"slaney\"}, optional\n        Conversion formula to use. \"htk\" corresponds to the formula used in\n        the Hidden Markov Model Toolkit, while \"slaney\" corresponds to the\n        formula used in Malcolm Slaney\u2019s Auditory Toolbox. Default is\n        \"htk\".\n\n    Returns\n    -------\n    list of float\n        Mel-scaled frequencies with the same shape as *hz*, rounded to four\n        decimal places.\n    \"\"\"\n    assert formula in (\"htk\", \"slaney\"), \"Invalid formula\"\n    \n    arr = np.asarray(hz, dtype=np.float64)\n    \n    if formula == \"htk\":\n        mel = 2595.0 * np.log10(1.0 + arr / 700.0)\n    else:  # slaney\n        mel = 1127.0 * np.log(1.0 + arr / 700.0)\n    \n    mel_rounded = np.round(mel, 4)\n    return mel_rounded.tolist()\n```\n\n### Explanation\n1. **Input Handling**: The input `hz` is converted to a numpy array to handle both list and array inputs efficiently.\n2. **Formula Validation**: An assertion checks if the formula is valid, ensuring only \"htk\" or \"slaney\" are used.\n3. **Conversion Calculation**: Depending on the formula, the appropriate logarithmic function (base 10 for HTK, natural logarithm for Slaney) is applied element-wise to the input array.\n4. **Rounding and Conversion**: The results are rounded to four decimal places using numpy's rounding function, then converted back to a Python list using `tolist()`, which preserves the original shape of the input array.\n\nThis approach ensures that the function handles any array-like input efficiently, applies the correct conversion formula, and returns the results in the required format."}
{"task_id": 226, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\"\"\"\n    n_samples, n_features = X_train.shape\n    weights = np.ones(n_samples) / n_samples\n    ensemble = []\n\n    for _ in range(n_clf):\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n        best_error = np.inf\n\n        # Search for the best stump\n        for feature in range(n_features):\n            values = X_train[:, feature]\n            unique_values = np.unique(values)\n            if len(unique_values) < 2:\n                continue  # Not enough unique values to split\n\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n            for threshold in thresholds:\n                # Calculate errors for both polarities\n                pred_p1 = np.where(values <= threshold, -1, 1)\n                error_p1 = np.sum(weights * (pred_p1 != y_train))\n                pred_pm1 = np.where(values <= threshold, 1, -1)\n                error_pm1 = np.sum(weights * (pred_pm1 != y_train))\n\n                if error_p1 < error_pm1:\n                    current_error, current_polarity = error_p1, 1\n                else:\n                    current_error, current_polarity = error_pm1, -1\n\n                if current_error < best_error:\n                    best_error = current_error\n                    best_feature = feature\n                    best_threshold = threshold\n                    best_polarity = current_polarity\n\n        # Compute learner weight\n        error = best_error\n        alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n        # Update sample weights\n        pred = np.where(X_train[:, best_feature] <= best_threshold, -best_polarity, best_polarity)\n        misclassified = (pred != y_train)\n        weights *= np.exp(alpha * (2 * misclassified - 1))  # Increase if misclassified\n        weights /= np.sum(weights)  # Normalize\n\n        # Save the stump parameters\n        ensemble.append((alpha, best_feature, best_threshold, best_polarity))\n\n    # Predict on test data\n    y_pred = np.zeros(X_test.shape[0])\n    for alpha, feature, threshold, polarity in ensemble:\n        x_feature = X_test[:, feature]\n        stump_pred = np.where(x_feature <= threshold, -polarity, polarity)\n        y_pred += alpha * stump_pred\n\n    # Convert to final predictions\n    y_pred = np.sign(y_pred)\n    y_pred[y_pred == 0] = 1  # Handle zero predictions as 1\n    return y_pred.astype(int).tolist()\n```"}
{"task_id": 227, "completion_id": 0, "completion": "To solve this problem, we need to implement a forward pass for a two-hidden-layer neural network using NumPy. The network uses tanh activations for the hidden layers and a linear activation for the output layer. The solution must validate the dimensions of the input matrices and return the computed value rounded to four decimal places, or -1 if any dimension check fails.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert the state and weight matrices from lists to NumPy arrays for efficient computation.\n2. **Dimension Validation**: Check the dimensions of the input matrices against the specified rules. If any dimension check fails, return -1.\n3. **Forward Pass Computation**: Compute the forward pass through the network using the validated matrices, applying tanh activations to the hidden layers and a linear activation to the output layer.\n4. **Result Rounding**: Round the final output value to four decimal places and return it as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef value_network_forward(state: list[float],\n                          W1: list[list[float]],\n                          W2: list[list[float]],\n                          W3: list[list[float]] | list[float]) -> float:\n    \"\"\"Forward pass of a 2-hidden-layer value network using tanh activations.\"\"\"\n    \n    # Convert state to a numpy array and check its dimension\n    state_arr = np.array(state)\n    if state_arr.ndim != 1:\n        return -1.0\n    N = state_arr.shape[0]\n    \n    # Validate W1 dimensions\n    try:\n        W1_arr = np.array(W1, dtype=np.float64)\n    except ValueError:\n        return -1.0\n    if W1_arr.ndim != 2 or W1_arr.shape[0] != N:\n        return -1.0\n    H1 = W1_arr.shape[1]\n    \n    # Validate W2 dimensions\n    try:\n        W2_arr = np.array(W2, dtype=np.float64)\n    except ValueError:\n        return -1.0\n    if W2_arr.ndim != 2 or W2_arr.shape[0] != H1:\n        return -1.0\n    H2 = W2_arr.shape[1]\n    \n    # Validate W3 dimensions\n    try:\n        W3_arr = np.array(W3, dtype=np.float64)\n    except ValueError:\n        return -1.0\n    if W3_arr.ndim == 1:\n        if W3_arr.shape[0] != H2:\n            return -1.0\n    elif W3_arr.ndim == 2:\n        if W3_arr.shape[0] != H2 or W3_arr.shape[1] != 1:\n            return -1.0\n    else:\n        return -1.0\n    \n    # Perform forward pass calculations\n    h1 = np.tanh(np.dot(state_arr, W1_arr))\n    h2 = np.tanh(np.dot(h1, W2_arr))\n    W3_flat = W3_arr.squeeze()\n    v = np.dot(h2, W3_flat)\n    \n    # Round and return the value\n    return float(np.round(v, 4))\n```\n\n### Explanation\n1. **Input Conversion**: The state and weight matrices are converted to NumPy arrays to leverage NumPy's efficient array operations.\n2. **Dimension Checks**: Each weight matrix is checked to ensure it has the correct dimensions as specified in the problem. If any matrix fails the dimension check, the function returns -1 immediately.\n3. **Forward Pass**: The input is propagated through the network using matrix multiplications and tanh activations for the hidden layers. The final output is computed as a linear combination of the second hidden layer's activations.\n4. **Result Handling**: The final value is rounded to four decimal places and returned as a float. This ensures the output meets the specified precision requirement.\n\nThis approach ensures that all inputs are validated and processed efficiently using NumPy, providing both correctness and performance."}
{"task_id": 228, "completion_id": 0, "completion": "To solve this problem, we need to compute the pairwise Euclidean distances between each row-vector in two given matrices. If the second matrix is omitted, we compute the distances within the first matrix itself. The solution must be efficient and avoid using explicit Python loops by leveraging vectorized operations.\n\n### Approach\n1. **Handle the Case for Y**: If the second matrix Y is not provided (i.e., it is `None`), set Y to be equal to the first matrix X.\n2. **Compute Squared Norms**: Calculate the squared norms of each row in both matrices X and Y. These will be used to compute the squared distances efficiently.\n3. **Dot Product Matrix**: Compute the dot product matrix between the rows of X and the rows of Y.\n4. **Squared Distance Matrix**: Using the identity for the squared Euclidean distance, construct the matrix of squared distances using the precomputed squared norms and the dot product matrix.\n5. **Ensure Non-Negativity**: To handle any potential numerical precision issues, ensure all values in the squared distance matrix are non-negative.\n6. **Compute Euclidean Distances**: Take the square root of each element in the squared distance matrix to get the Euclidean distances.\n7. **Round and Convert**: Round the resulting distances to four decimal places and convert the result to a nested list structure as required.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pairwise_l2_distances(X: np.ndarray, Y: np.ndarray | None = None) -> list[list[float]]:\n    \"\"\"Compute the pairwise Euclidean (L2) distances between each row of X and each row of Y.\n\n    If *Y* is ``None`` the function should compute the distances inside *X* itself.\n    All distances must be rounded to four decimal places and the function must\n    return a plain Python nested list (use ``tolist()`` if needed).\n\n    Args:\n        X: NumPy array with shape (N, C).\n        Y: NumPy array with shape (M, C) or ``None``.  Default is ``None``.\n\n    Returns:\n        Nested list representing the N \u00d7 M matrix of Euclidean distances.\n    \"\"\"\n    if Y is None:\n        Y = X\n    \n    X_squared = np.sum(X**2, axis=1)\n    Y_squared = np.sum(Y**2, axis=1)\n    \n    dot_product = X @ Y.T\n    \n    # Compute squared distances using the expanded formula\n    D_squared = X_squared[:, np.newaxis] + Y_squared[np.newaxis, :] - 2 * dot_product\n    \n    # Ensure non-negative to avoid numerical errors due to precision\n    D_squared = np.maximum(D_squared, 0)\n    \n    # Compute Euclidean distances\n    D = np.sqrt(D_squared)\n    \n    # Round to four decimal places and convert to list of lists\n    return np.round(D, 4).tolist()\n```\n\n### Explanation\n1. **Handling Y**: The function checks if Y is `None` and sets it to X if necessary. This ensures we compute the pairwise distances within X when Y is not provided.\n2. **Squared Norms**: The squared norms of the rows in X and Y are computed using vectorized operations. These norms are essential for efficiently calculating the squared distances.\n3. **Dot Product Matrix**: The dot product between each row of X and each row of Y is computed using matrix multiplication, which is highly optimized in NumPy.\n4. **Squared Distance Matrix**: Using the identity for the squared Euclidean distance, we construct the matrix using broadcasting to avoid explicit loops. This step is crucial for efficiency.\n5. **Non-Negativity**: To handle any floating-point inaccuracies that might result in negative values, we ensure all elements in the squared distance matrix are non-negative.\n6. **Euclidean Distances**: Taking the square root of the squared distances gives the actual Euclidean distances.\n7. **Rounding and Conversion**: The distances are rounded to four decimal places and converted to a nested list structure to meet the problem's requirements.\n\nThis approach efficiently computes the required distances using vectorized operations, ensuring optimal performance even for large matrices."}
{"task_id": 230, "completion_id": 0, "completion": "To solve this problem, we need to implement the Gaussian Naive Bayes algorithm from scratch for a binary classification task. The algorithm will predict the class labels (0 or 1) for test samples based on the training data provided. The key steps involve calculating the mean and variance for each feature within each class, computing class priors, and using these to determine the posterior probabilities for test samples.\n\n### Approach\n1. **Split Training Data by Class**: Separate the training data into two groups based on their class labels (0 and 1).\n2. **Compute Class Priors**: Calculate the prior probabilities for each class based on their relative frequencies in the training data.\n3. **Calculate Mean and Variance**: For each feature within each class, compute the mean and variance. Add a small constant (\u03b5 = 1e-9) to the variances to avoid division by zero.\n4. **Compute Log Posterior Probabilities**: For each test sample, compute the log posterior probability for each class using the Gaussian probability density function. The log posterior is the sum of the log prior and the log likelihood of the features.\n5. **Predict Class Labels**: Assign the class label with the higher log posterior probability to each test sample.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train, y_train, X_test):\n    \"\"\"Implements Gaussian Naive Bayes for binary classification from scratch.\"\"\"\n    # Split training data into classes\n    mask0 = y_train == 0\n    X_class0 = X_train[mask0]\n    X_class1 = X_train[~mask0]\n    \n    # Compute class priors and their logs\n    prior0 = X_class0.shape[0] / X_train.shape[0]\n    prior1 = X_class1.shape[0] / X_train.shape[0]\n    log_prior0 = np.log(prior0)\n    log_prior1 = np.log(prior1)\n    \n    # Compute means and variances for each class and feature, adding epsilon to variances\n    mean0 = np.mean(X_class0, axis=0)\n    var0 = np.var(X_class0, axis=0, ddof=0) + 1e-9\n    mean1 = np.mean(X_class1, axis=0)\n    var1 = np.var(X_class1, axis=0, ddof=0) + 1e-9\n    \n    # Precompute terms for log likelihood calculations\n    log_2pi_var0_sum = np.sum(np.log(2 * np.pi * var0))\n    log_2pi_var1_sum = np.sum(np.log(2 * np.pi * var1))\n    \n    # Calculate the differences for test samples\n    diff0 = X_test - mean0\n    diff1 = X_test - mean1\n    \n    # Compute sum of squared differences divided by variances for each class\n    sum_sq_div_var0 = np.sum(diff0 ** 2 / var0, axis=1)\n    sum_sq_div_var1 = np.sum(diff1 ** 2 / var1, axis=1)\n    \n    # Compute log likelihoods for each class\n    log_likelihood0 = -0.5 * (log_2pi_var0_sum + sum_sq_div_var0)\n    log_likelihood1 = -0.5 * (log_2pi_var1_sum + sum_sq_div_var1)\n    \n    # Compute log posteriors\n    log_posterior0 = log_prior0 + log_likelihood0\n    log_posterior1 = log_prior1 + log_likelihood1\n    \n    # Determine predictions based on higher log posterior\n    predictions = np.where(log_posterior0 > log_posterior1, 0, 1)\n    \n    return predictions.tolist()\n```\n\n### Explanation\n1. **Splitting Data**: The training data is divided into two subsets based on the class labels (0 and 1).\n2. **Class Priors**: The priors are calculated as the proportion of each class in the training data. These are converted to log probabilities to avoid underflow in subsequent calculations.\n3. **Mean and Variance Calculation**: For each feature in each class, the mean and variance are computed. A small constant is added to the variance to prevent division by zero errors.\n4. **Log Likelihood Calculation**: The log likelihood for each test sample is computed using the Gaussian probability density function. This involves vectorized operations for efficiency.\n5. **Log Posterior Calculation**: The log posterior for each class is the sum of the log prior and the log likelihood. The class with the higher log posterior is chosen as the prediction for each test sample.\n\nThis approach efficiently handles the computation using vectorized operations, ensuring scalability and performance even with larger datasets."}
{"task_id": 231, "completion_id": 0, "completion": "To solve this problem, we need to determine if a given hand in the game of Blackjack is a natural blackjack. A natural blackjack consists of exactly two cards: an Ace (encoded as 1) and a ten-valued card (encoded as 10). The order of the cards does not matter.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Check the Length of the Hand**: A natural blackjack must have exactly two cards. If the hand does not contain exactly two cards, return `False`.\n2. **Check the Card Values**: The two cards must consist of an Ace (1) and a ten-valued card (10). To ensure the order does not matter, we can sort the hand and check if the sorted list equals `[1, 10]`.\n\n### Solution Code\n```python\ndef is_natural(hand: list[int]) -> bool:\n    \"\"\"Determine if the given hand is a natural blackjack (Ace and a ten-valued card).\"\"\"\n    return len(hand) == 2 and sorted(hand) == [1, 10]\n```\n\n### Explanation\n1. **Check Length**: The function first checks if the length of the hand is exactly 2. If not, it immediately returns `False`.\n2. **Check Values**: If the hand has exactly two cards, the function sorts the hand and checks if the sorted list equals `[1, 10]`. This ensures that one card is an Ace (1) and the other is a ten-valued card (10), regardless of their order in the input list.\n\nThis approach efficiently determines if the hand is a natural blackjack with a time complexity of O(1) for the length check and O(1) for the sorting operation (since the list has a fixed size of 2), resulting in an overall time complexity of O(1). This solution is optimal and straightforward for the given problem constraints."}
{"task_id": 232, "completion_id": 0, "completion": "To solve this problem, we need to determine if a given NumPy array is a valid one-hot encoded matrix. A valid one-hot matrix must satisfy three specific conditions: it must be two-dimensional, all elements must be either 0 or 1, and each row must contain exactly one 1.\n\n### Approach\nThe approach involves checking each of the three conditions step-by-step:\n1. **Check if the array is two-dimensional**: This can be verified using the `ndim` attribute of the NumPy array.\n2. **Check if all elements are either 0 or 1**: This can be done using a combination of element-wise comparisons and logical operations.\n3. **Check if each row contains exactly one 1**: This is verified by summing the elements along each row and ensuring every row sums to 1.\n\nBy systematically verifying each condition, we can determine if the given array meets all the criteria for a valid one-hot matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_one_hot(x: np.ndarray) -> bool:\n    \"\"\"Check if *x* is a valid one-hot encoded NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A NumPy array to verify.\n\n    Returns\n    -------\n    bool\n        True if *x* is one-hot encoded, otherwise False.\n    \"\"\"\n    if x.ndim != 2:\n        return False\n    if not np.all((x == 0) | (x == 1)):\n        return False\n    row_sums = x.sum(axis=1)\n    if not np.all(row_sums == 1):\n        return False\n    return True\n```\n\n### Explanation\n1. **Dimensionality Check**: The first condition checks if the input array is two-dimensional using `x.ndim != 2`. If the array is not 2D, the function immediately returns `False`.\n2. **Binary Element Check**: The second condition uses `np.all((x == 0) | (x == 1))` to ensure every element in the array is either 0 or 1. This check is performed element-wise across the entire array.\n3. **Row Sum Check**: The third condition calculates the sum of each row using `x.sum(axis=1)`. It then checks if all these sums are exactly 1 using `np.all(row_sums == 1)`. If any row does not sum to 1, the function returns `False`.\n\nBy sequentially verifying these three conditions, the function efficiently determines if the input array is a valid one-hot matrix, returning `True` if all conditions are met and `False` otherwise."}
{"task_id": 234, "completion_id": 0, "completion": "To solve this problem, we need to implement a single-step Thompson sampling algorithm for a Bernoulli multi-armed bandit. The goal is to select the arm with the highest sampled success probability based on observed successes and failures, using a Beta-Binomial model.\n\n### Approach\n1. **Input Validation**: Check if the input lists `successes` and `failures` are non-empty and of the same length. If not, return -1.\n2. **Set Random Seed**: If a seed is provided, set NumPy's random seed to ensure reproducibility.\n3. **Compute Posterior Parameters**: For each arm, compute the parameters of the Beta posterior distribution using the given prior parameters and observed data.\n4. **Sample from Beta Distributions**: Generate a sample from the Beta distribution for each arm.\n5. **Select Arm**: Return the index of the arm with the highest sampled value, using the smallest index in case of ties.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef thompson_sampling_beta_binomial(successes: list[int],\n                                    failures: list[int],\n                                    prior_alpha: float = 1.0,\n                                    prior_beta: float = 1.0,\n                                    seed: int | None = None) -> int:\n    if not successes or not failures or len(successes) != len(failures):\n        return -1\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    a = prior_alpha + np.array(successes)\n    b = prior_beta + np.array(failures)\n    \n    samples = np.random.beta(a, b)\n    return int(np.argmax(samples))\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if either `successes` or `failures` is empty or if their lengths differ. If any of these conditions are met, it returns -1 to indicate invalid input.\n2. **Setting Random Seed**: To ensure reproducibility, the function sets the NumPy random seed if provided.\n3. **Posterior Parameters Calculation**: For each arm, the parameters of the Beta distribution are calculated by adding the prior parameters (`prior_alpha` and `prior_beta`) to the observed successes and failures, respectively.\n4. **Sampling**: Using NumPy's `beta` function, the function draws a sample from each arm's Beta distribution. This is done efficiently by vectorizing the parameters.\n5. **Arm Selection**: The index of the arm with the highest sampled value is determined using `np.argmax`, which also handles ties by returning the smallest index.\n\nThis approach efficiently combines statistical modeling with vectorized operations for performance, ensuring both correctness and computational efficiency."}
{"task_id": 235, "completion_id": 0, "completion": "To solve this problem, we need to convert a 1-dimensional NumPy array of integer class labels into a one-hot encoded 2-D NumPy array. The resulting array should have each row representing a one-hot vector where the position corresponding to the class label is set to 1.0, and all other positions are 0.0.\n\n### Approach\n1. **Input Validation**: First, ensure the input array is 1-dimensional. If not, raise a `ValueError`.\n2. **Determine Number of Classes**: If the input array is empty and the number of classes (`n_classes`) is not provided, raise an error. Otherwise, use the provided `n_classes` or infer it from the maximum label value in the input array.\n3. **Check Validity of Provided Classes**: If `n_classes` is provided, ensure it is at least one more than the maximum label value in the input array to accommodate all labels.\n4. **Create One-Hot Encoded Array**: Initialize a zero matrix of appropriate dimensions and set the positions corresponding to each label to 1.0 using efficient NumPy indexing.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_one_hot(labels: np.ndarray, n_classes: int | None = None) -> np.ndarray:\n    \"\"\"Convert integer class labels to one-hot encodings.\n\n    Parameters\n    ----------\n    labels : np.ndarray\n        A 1-D NumPy array containing integer class labels.\n    n_classes : int | None, optional (default=None)\n        Total number of classes.  If None, it will be inferred from the\n        data as `labels.max() + 1`.\n\n    Returns\n    -------\n    np.ndarray\n        A two-dimensional NumPy array of shape (N, C) where N is the\n        number of labels and C is the number of classes.  Each row is a\n        one-hot vector corresponding to the associated label.\n    \"\"\"\n    if labels.ndim != 1:\n        raise ValueError(\"Labels must be 1-dimensional.\")\n    \n    if labels.size == 0:\n        if n_classes is None:\n            raise ValueError(\"n_classes must be specified when labels is empty.\")\n        C = n_classes\n    else:\n        max_label = labels.max()\n        if n_classes is not None:\n            if n_classes < max_label + 1:\n                raise ValueError(\"n_classes must be at least max(labels)+1\")\n            C = n_classes\n        else:\n            C = max_label + 1\n    \n    one_hot = np.zeros((labels.size, C), dtype=float)\n    if labels.size > 0:\n        rows = np.arange(labels.size)\n        cols = labels\n        one_hot[rows, cols] = 1.0\n    \n    return one_hot\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input array is 1-dimensional. If not, it raises a `ValueError`.\n2. **Handling Empty Input**: If the input array is empty and `n_classes` is not provided, the function raises an error. If `n_classes` is provided, it uses that value.\n3. **Class Inference**: If `n_classes` is not provided, the function infers the number of classes from the maximum value in the input array. If `n_classes` is provided, it checks if it is sufficient to cover all labels.\n4. **Matrix Initialization**: A zero matrix of shape (N, C) is created, where N is the number of labels and C is the number of classes.\n5. **Setting Values**: Using advanced indexing, the function sets the appropriate positions in the matrix to 1.0 based on the labels.\n\nThis approach ensures efficient handling of both small and large datasets, leveraging NumPy's optimized operations for performance."}
{"task_id": 236, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef minibatch(X: np.ndarray, batchsize: int = 256, shuffle: bool = True):\n    \"\"\"Create a generator that yields index mini-batches.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (N, *). The first axis holds the samples.\n    batchsize : int, default = 256\n        Desired mini-batch size. The last batch may be smaller.\n    shuffle : bool, default = True\n        Whether to shuffle sample indices before splitting.\n\n    Returns\n    -------\n    tuple\n        (mb_generator, n_batches)\n        mb_generator : generator yielding NumPy integer arrays \u2013 the indices\n            of each mini-batch in the order they are processed.\n        n_batches : int \u2013 total number of mini-batches.\n    \"\"\"\n    N = X.shape[0]\n    indices = np.arange(N, dtype=np.int64)\n    if shuffle:\n        np.random.shuffle(indices)\n    n_batches = (N + batchsize - 1) // batchsize\n    mb_gen = (indices[i * batchsize : (i + 1) * batchsize] for i in range(n_batches))\n    return (mb_gen, n_batches)\n```"}
{"task_id": 237, "completion_id": 0, "completion": "To solve this problem, we need to evaluate the total score of a Blackjack hand according to specific rules. The main challenge is handling the Ace cards, which can be worth either 11 or 1, depending on which value benefits the hand the most without exceeding 21.\n\n### Approach\n1. **Sum Non-Ace Cards**: First, we calculate the sum of all cards that are not Aces. For face cards (J, Q, K), we add 10 to the sum. For numeric cards (2-10), we add their face value directly.\n2. **Count Aces**: We count the number of Aces in the hand since each Ace can be either 11 or 1.\n3. **Initial Total Calculation**: We initially treat all Aces as 11 and add their value to the sum of non-Ace cards. This gives us a starting point for the total score.\n4. **Adjust for Aces**: If the initial total exceeds 21, we convert Aces from 11 to 1 (by subtracting 10 for each Ace) until the total is 21 or less, or until we run out of Aces to convert.\n\n### Solution Code\n```python\nfrom typing import List\n\ndef score(hand: List[str]) -> int:\n    \"\"\"Return the Blackjack score of *hand* or 0 if the hand is bust.\n\n    Args:\n        hand: A list of card labels ('2'-'10', 'J', 'Q', 'K', 'A').\n\n    Returns:\n        An integer between 0 and 21 (inclusive).  If every possible value of\n        the hand exceeds 21 the function returns 0.\n    \"\"\"\n    sum_non_ace = 0\n    num_aces = 0\n\n    for card in hand:\n        if card == 'A':\n            num_aces += 1\n        elif card in {'J', 'Q', 'K'}:\n            sum_non_ace += 10\n        else:\n            sum_non_ace += int(card)\n    \n    total = sum_non_ace + 11 * num_aces\n    num_subtract = 0\n\n    while total > 21 and num_subtract < num_aces:\n        total -= 10\n        num_subtract += 1\n    \n    return total if total <= 21 else 0\n```\n\n### Explanation\n1. **Summing Non-Ace Cards**: The code iterates through each card in the hand. For each non-Ace card, it adds its value to `sum_non_ace`. Face cards contribute 10, while numeric cards contribute their respective values.\n2. **Counting Aces**: Each Ace encountered increments the `num_aces` counter.\n3. **Initial Total Calculation**: The initial total is calculated by treating all Aces as 11. This gives the maximum possible value of the hand.\n4. **Adjusting for Aces**: If the initial total exceeds 21, the code converts Aces from 11 to 1 by subtracting 10 for each Ace until the total is 21 or less. If after converting all possible Aces the total still exceeds 21, the hand is considered bust, and the function returns 0.\n\nThis approach ensures that the hand is evaluated optimally, considering all possible values of Aces to maximize the score without exceeding 21."}
{"task_id": 239, "completion_id": 0, "completion": "To solve this problem, we need to compute the padding required for 2-D convolutions in convolutional neural networks (CNNs) to maintain the spatial dimensions of the input (\"same\" convolution) or to perform convolution without padding (\"valid\" convolution). \n\n### Approach\n1. **Valid Convolution**: When the output shape is specified as \"valid\", no padding is required. Hence, the padding values for all borders (top, bottom, left, right) are zero.\n2. **Same Convolution**: For maintaining the input spatial dimensions in the output, padding is calculated based on the filter size. The total padding needed for each dimension (height and width) is derived from the filter size. If the required padding is odd, it is split such that the smaller part is applied to the top/left and the larger part to the bottom/right.\n\n### Solution Code\n```python\ndef determine_padding(filter_shape: tuple[int, int], output_shape: str = \"same\") -> tuple[tuple[int, int], tuple[int, int]]:\n    \"\"\"Calculate the amount of zero-padding needed on all borders for a 2-D convolution.\n\n    Parameters\n    ----------\n    filter_shape : tuple[int, int]\n        (filter_height, filter_width) of the convolution kernel.\n    output_shape : str, optional (default=\"same\")\n        \"same\"  \u2013 pad so that the output spatial dimensions equal the input\n        \"valid\" \u2013 perform the convolution without any zero-padding.\n\n    Returns\n    -------\n    tuple[tuple[int, int], tuple[int, int]]\n        ((pad_top, pad_bottom), (pad_left, pad_right)) where every value is an\n        integer \u2265 0.\n    \"\"\"\n    if output_shape == \"valid\":\n        return ((0, 0), (0, 0))\n    # Handle \"same\" padding\n    fh, fw = filter_shape\n    # Calculate padding for height\n    pad_total_h = fh - 1\n    pad_top = pad_total_h // 2\n    pad_bottom = pad_total_h - pad_top\n    # Calculate padding for width\n    pad_total_w = fw - 1\n    pad_left = pad_total_w // 2\n    pad_right = pad_total_w - pad_left\n    return ((pad_top, pad_bottom), (pad_left, pad_right))\n```\n\n### Explanation\n1. **Valid Convolution**: Directly returns `((0, 0), (0, 0))` as no padding is needed.\n2. **Same Convolution**: \n   - **Filter Height and Width**: The total padding required for each dimension is calculated as `filter_size - 1`.\n   - **Padding Distribution**: The total padding is split into two parts. The smaller part (using floor division) is applied to the top (for height) or left (for width), and the remaining part is applied to the bottom or right. This ensures that the output dimensions match the input dimensions when using a stride of 1.\n\nThis approach efficiently calculates the necessary padding to maintain spatial dimensions or perform valid convolution, adhering to the constraints and requirements specified."}
{"task_id": 240, "completion_id": 0, "completion": "To solve this problem, we need to build four dictionaries that map observation and action identifiers to unique integers and vice versa. These dictionaries are essential for efficiently indexing tables, tensors, or neural-network outputs in reinforcement-learning applications. The solution must handle both cases where explicit encoders are provided and where they are not, ensuring internal consistency between forward and inverse mappings.\n\n### Approach\n1. **Input Handling**: The function takes lists of observation and action identifiers, along with optional pre-defined encoders.\n2. **Observation Encoding**:\n   - If an encoder is provided, use it directly.\n   - If not, generate the encoder by enumerating the observation identifiers in the order they appear in the list. If the list is empty, the encoder will be an empty dictionary.\n3. **Action Encoding**:\n   - Similar to observation encoding, using the provided encoder if available, otherwise generating it from the action list.\n4. **Inverse Dictionaries**: Create inverse mappings by swapping the keys and values of the forward encoders.\n5. **Special Cases**: Ensure that if a list is empty and no encoder is provided, the corresponding dictionaries are also empty.\n\n### Solution Code\n```python\ndef create_encoding_dicts(obs_ids: list, action_ids: list,\n                          obs_encoder: dict | None = None,\n                          act_encoder: dict | None = None) -> dict:\n    \"\"\"Generate forward and backward mappings for observations and actions.\n\n    Args:\n        obs_ids (list): All distinct observation identifiers.\n        action_ids (list): All distinct action identifiers.\n        obs_encoder (dict | None, optional): Existing obs\u2192number mapping.\n        act_encoder (dict | None, optional): Existing act\u2192number mapping.\n\n    Returns:\n        dict: A dictionary that contains the keys 'obs2num', 'num2obs',\n        'act2num', and 'num2act'.\n    \"\"\"\n\n    # Process observations\n    if obs_encoder is not None:\n        obs2num = dict(obs_encoder)\n    else:\n        obs2num = {obs: idx for idx, obs in enumerate(obs_ids)} if obs_ids else {}\n    num2obs = {v: k for k, v in obs2num.items()}\n\n    # Process actions\n    if act_encoder is not None:\n        act2num = dict(act_encoder)\n    else:\n        act2num = {act: idx for idx, act in enumerate(action_ids)} if action_ids else {}\n    num2act = {v: k for k, v in act2num.items()}\n\n    return {\n        'obs2num': obs2num,\n        'num2obs': num2obs,\n        'act2num': act2num,\n        'num2act': num2act\n    }\n```\n\n### Explanation\n- **Observation Handling**: The function first checks if an observation encoder is provided. If so, it uses this encoder. Otherwise, it generates the encoder by enumerating the observation identifiers. If the list is empty, the encoder is an empty dictionary.\n- **Action Handling**: Similarly, the function checks for an action encoder, using it if provided. If not, it generates the encoder from the action list, handling empty lists appropriately.\n- **Inverse Dictionaries**: These are created by reversing the key-value pairs of the forward encoders, ensuring that each number maps back to its original identifier.\n- **Special Cases**: The solution ensures that empty lists without encoders result in empty dictionaries, maintaining consistency and correctness.\n\nThis approach efficiently handles both provided encoders and dynamically generated ones, ensuring that the resulting dictionaries are always internally consistent."}
{"task_id": 241, "completion_id": 0, "completion": "To solve this problem, we need to initialize a tensor with an orthogonal matrix using the algorithm described by Saxe et al. (2014). The orthogonal matrix helps in maintaining the stability of neural network activations during the initial stages of training.\n\n### Approach\nThe algorithm involves the following steps:\n1. **Check Input Validity**: Ensure the input shape has at least two dimensions. If not, return -1.\n2. **Reshape Input**: Convert the input shape into a 2D matrix where the first dimension is `d0` (the first element of the shape) and the second dimension is the product of the remaining elements.\n3. **Generate Random Matrix**: Create a matrix filled with samples from a standard normal distribution.\n4. **Compute SVD**: Perform Singular Value Decomposition (SVD) on the generated matrix to get orthogonal matrices U and V\u1d40.\n5. **Select Orthogonal Factor**: Choose either U or V\u1d40 based on which one matches the shape of the generated matrix.\n6. **Reshape and Scale**: Reshape the selected orthogonal factor back to the original input shape and scale it by the given factor.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n    \n    d0 = shape[0]\n    remaining_dims = int(np.prod(shape[1:]))\n    # Create the 2D matrix A with shape (d0, remaining_dims)\n    A = np.random.normal(size=(d0, remaining_dims))\n    \n    # Compute the SVD of A\n    U, S, Vh = np.linalg.svd(A, full_matrices=False)\n    \n    # Determine which factor to use (U or Vh)\n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = Vh\n    \n    # Reshape Q to the desired shape and apply scaling\n    tensor = Q.reshape(shape) * scale\n    return tensor.tolist()\n```\n\n### Explanation\n1. **Input Validity Check**: The function first checks if the input shape has at least two dimensions. If not, it returns -1 as specified.\n2. **Reshape Input**: The input shape is split into the first dimension `d0` and the product of the remaining dimensions to form a 2D matrix `A`.\n3. **Generate Random Matrix**: Using NumPy's `np.random.normal`, a matrix `A` of the specified shape is filled with random values from a standard normal distribution.\n4. **Compute SVD**: The SVD of matrix `A` is computed to obtain matrices U, S, and Vh. The `full_matrices=False` parameter ensures the decomposition is economical.\n5. **Select Orthogonal Factor**: Depending on whether U matches the shape of `A`, either U or Vh (transpose of V) is selected. This ensures the selected matrix maintains the orthogonality required.\n6. **Reshape and Scale**: The selected orthogonal matrix is reshaped back to the original input shape and scaled by the given factor to produce the final tensor.\n\nThis approach ensures that the resulting tensor meets the orthogonality condition required, either through the rows or columns being orthonormal up to the specified scaling factor."}
{"task_id": 242, "completion_id": 0, "completion": "To solve this problem, we need to implement the arm selection rule of the Upper-Confidence-Bound (UCB1) algorithm for the stochastic multi-armed bandit problem. The goal is to select the arm that balances exploration and exploitation based on the current empirical mean rewards and the number of times each arm has been pulled.\n\n### Approach\n1. **Check for Unpulled Arms**: First, we check if any arm has never been pulled (i.e., its pull count is zero). If such an arm exists, we immediately return the index of the first arm with zero pulls. This ensures that every arm is tried at least once.\n2. **Compute UCB Scores**: If all arms have been pulled at least once, we compute the UCB score for each arm. The UCB score is calculated using the formula:\n   \\[\n   \\text{score}(a) = \\text{ev\\_estimates}[a] + C \\times \\sqrt{\\frac{2 \\ln(\\text{current\\_step} + 1)}{\\text{pull\\_counts}[a]}}\n   \\]\n   where \\(C\\) is the exploration parameter, and \\(\\text{current\\_step} + 1\\) is the time-step \\(T\\).\n3. **Select the Best Arm**: After computing the UCB scores for all arms, we select the arm with the highest score. In case of ties (multiple arms with the same highest score), we return the arm with the smallest index.\n\n### Solution Code\n```python\nimport math\n\ndef ucb1_select(ev_estimates: list[float], pull_counts: list[int], current_step: int, C: float = 1.0) -> int:\n    \"\"\"Return the arm index that UCB1 would pick next.\"\"\"\n    # Check if any arm has not been pulled yet\n    for i in range(len(pull_counts)):\n        if pull_counts[i] == 0:\n            return i\n    # Calculate the UCB scores for each arm\n    T = current_step + 1\n    scores = []\n    for i in range(len(ev_estimates)):\n        exploration_term = C * math.sqrt((2 * math.log(T)) / pull_counts[i])\n        scores.append(ev_estimates[i] + exploration_term)\n    # Find the index of the maximum score (first occurrence if there are ties)\n    return scores.index(max(scores))\n```\n\n### Explanation\n1. **Check for Unpulled Arms**: The algorithm first ensures that each arm is pulled at least once by iterating through the pull counts. If an arm with a zero pull count is found, its index is returned immediately.\n2. **Compute UCB Scores**: For each arm that has been pulled at least once, the UCB score is computed using the given formula. This score balances the empirical mean reward (exploitation) and an exploration term that increases with the logarithm of the time-step but decreases with the number of times the arm has been pulled.\n3. **Select the Best Arm**: The arm with the highest UCB score is selected. If multiple arms have the same highest score, the arm with the smallest index is chosen, which aligns with the behavior of Python's `max` and `index` functions when handling lists.\n\nThis approach ensures that the algorithm efficiently balances exploration and exploitation, leading to optimal decision-making in the multi-armed bandit problem."}
{"task_id": 243, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward pass of a small fully-connected Actor-Critic network with shared feature extractor layers and separate output heads for the actor and critic. The network uses fixed weights and biases, all set to 1.0 and 0.0 respectively, leading to deterministic outputs.\n\n### Approach\n1. **Input Processing**: The input state is a list of three floating-point numbers. We first compute the sum of these three elements.\n2. **Feature Extraction**:\n   - **Dense-1 Layer**: The sum of the state elements is passed through a ReLU activation function. If the sum is negative, it becomes zero; otherwise, it remains the same.\n   - **Dense-2 and Dense-3 Layers**: These layers process the output of the previous layer. Given the fixed weights and biases, each subsequent layer's output is a scaled version of the previous layer's output.\n3. **Actor Head**: The actor head processes the output from the last feature extraction layer. Since all weights are 1.0, the logits (pre-softmax values) are identical, leading to equal probabilities for each action when passed through a softmax activation.\n4. **Critic Head**: The critic head produces a single state value by linearly combining the outputs of the last feature extraction layer.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\"\"\"\n    sum_s = sum(state)\n    z1 = max(0.0, sum_s)\n    state_value = 64.0 * z1\n    action_probs = [round(1/3, 4)] * 3\n    return (action_probs, round(state_value, 4))\n```\n\n### Explanation\n1. **Sum Calculation**: The sum of the input state elements is computed to determine the initial feature extraction value.\n2. **ReLU Activation**: The sum is passed through a ReLU function to ensure non-negative output.\n3. **State Value Calculation**: The state value is derived by scaling the ReLU output by a factor of 64, which is a result of the fixed weights through the network layers.\n4. **Action Probabilities**: Since all logits are identical due to uniform weights, the softmax activation results in equal probabilities for each action, each rounded to four decimal places.\n\nThis approach efficiently computes the forward pass using the deterministic nature of the fixed weights and biases, ensuring accurate and efficient results."}
{"task_id": 244, "completion_id": 0, "completion": "To solve this problem, we need to generate a random binary tensor of a specified shape where each element is either `1.0` or `0.0`. The probability of an element being `1.0` is given by the `sparsity` parameter. The solution must validate the input arguments and use NumPy for generating random numbers.\n\n### Approach\n1. **Input Validation**:\n   - **Shape Validation**: The shape must be a non-empty tuple of positive integers.\n   - **Sparsity Validation**: The sparsity must be a real number between 0 and 1 (inclusive).\n\n2. **Tensor Generation**:\n   - Use NumPy's `random.rand` function to generate random numbers in the interval [0, 1).\n   - Convert these random numbers to a binary tensor where each element is `1.0` if it is greater than or equal to the threshold (1 - sparsity), and `0.0` otherwise.\n\n3. **Conversion to Nested List**:\n   - Convert the generated NumPy array to a nested Python list using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple, sparsity: float = 0.5) -> list:\n    \"\"\"Generate a random binary tensor.\n\n    The function should create a tensor of the requested ``shape`` such that\n    each entry independently takes the value ``1.0`` with probability equal\n    to ``sparsity`` and ``0.0`` otherwise.\n\n    The result **must** be returned as a regular (nested) Python list obtained\n    via NumPy\u2019s ``tolist`` method.  If either ``shape`` or ``sparsity`` is\n    illegal (see task description) the function must return **-1**.\n    \"\"\"\n    # Validate shape\n    if not isinstance(shape, tuple) or len(shape) == 0:\n        return -1\n    for dim in shape:\n        if not isinstance(dim, int) or dim <= 0:\n            return -1\n    \n    # Validate sparsity\n    if isinstance(sparsity, bool):\n        return -1\n    if not isinstance(sparsity, (int, float)):\n        return -1\n    if sparsity < 0 or sparsity > 1:\n        return -1\n    \n    # Generate random tensor and convert to binary values\n    rnd = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_tensor = (rnd >= threshold).astype(float)\n    \n    return binary_tensor.tolist()\n```\n\n### Explanation\n- **Input Validation**:\n  - The shape is checked to ensure it is a non-empty tuple of positive integers. If not, the function returns -1.\n  - The sparsity is checked to ensure it is a valid number (not a boolean) within the range [0, 1]. If invalid, the function returns -1.\n\n- **Tensor Generation**:\n  - Using `np.random.rand`, we generate a tensor of random numbers in the interval [0, 1).\n  - We compute the threshold as `1 - sparsity`. Elements of the random tensor that are greater than or equal to this threshold are set to `1.0`, others to `0.0`.\n\n- **Conversion**:\n  - The resulting NumPy array is converted to a nested list using `tolist()`, ensuring the output format meets the requirement.\n\nThis approach ensures that the generated tensor is both valid and efficiently generated using NumPy, while the input checks ensure robustness against invalid arguments."}
{"task_id": 247, "completion_id": 0, "completion": "To solve this problem, we need to determine the arm with the highest expected reward in a Gaussian bandit scenario. Each arm in the bandit has a stochastic reward structure defined by a Gaussian distribution and a payout probability. The goal is to validate the input parameters and compute the expected reward for each arm to identify the optimal one.\n\n### Approach\n1. **Input Validation**: \n   - Check if the lengths of the input lists are non-zero and equal.\n   - Ensure all variances are positive and all probabilities are within the range [0, 1].\n\n2. **Expected Reward Calculation**:\n   - Convert the input lists into NumPy arrays for efficient computation.\n   - Calculate the expected reward for each arm using element-wise multiplication of the mean values and probabilities.\n\n3. **Determine Optimal Arm**:\n   - Identify the maximum expected reward and the corresponding arm index, using NumPy functions to handle ties by selecting the smallest index.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_bandit_oracle(payoff_dists: list[tuple[float, float]],\n                           payoff_probs: list[float]) -> tuple[float, int] | int:\n    # Validate input lengths\n    K = len(payoff_dists)\n    if K == 0 or len(payoff_probs) != K:\n        return -1\n    \n    # Validate variances and probabilities\n    for mu, sigma_sq in payoff_dists:\n        if sigma_sq <= 0:\n            return -1\n    \n    for p in payoff_probs:\n        if not (0.0 <= p <= 1.0):\n            return -1\n    \n    # Calculate expected values\n    mus = np.array([mu for mu, sigma_sq in payoff_dists])\n    p_array = np.array(payoff_probs)\n    evs = p_array * mus\n    \n    # Find best arm\n    best_ev = evs.max()\n    best_arm = evs.argmax()\n    \n    # Round to 4 decimals\n    best_ev_rounded = round(best_ev, 4)\n    \n    return (best_ev_rounded, best_arm)\n```\n\n### Explanation\n1. **Input Validation**:\n   - The function first checks if the input lists are non-empty and of the same length. If not, it returns -1.\n   - Each variance in the Gaussian distributions is checked to ensure it is positive. If any variance is non-positive, the function returns -1.\n   - Each probability is checked to ensure it lies within the interval [0, 1]. If any probability is out of bounds, the function returns -1.\n\n2. **Expected Reward Calculation**:\n   - The means and probabilities are converted into NumPy arrays to leverage vectorized operations for efficient computation.\n   - The expected rewards are computed by element-wise multiplication of the means and probabilities.\n\n3. **Optimal Arm Selection**:\n   - The maximum expected reward is found using NumPy's `max` function.\n   - The index of the first occurrence of this maximum value is determined using `argmax`, which handles ties by returning the smallest index.\n   - The maximum expected reward is rounded to four decimal places as specified.\n\nThis approach ensures efficient and correct validation and computation, leveraging NumPy for performance and accuracy."}
{"task_id": 248, "completion_id": 0, "completion": "To solve this problem, we need to implement a baseline classifier that predicts the same probability for every example based on the empirical proportion of the positive class observed in the training labels. The solution involves computing this proportion, creating an array of probabilities for the test set, and ensuring the results are rounded and formatted correctly.\n\n### Approach\n1. **Compute the Positive-Class Probability**: Calculate the proportion of positive labels (1s) in the training data. This is done by dividing the sum of positive labels by the total number of training samples.\n2. **Handle Edge Cases**: If the test set has no rows, return an empty list immediately.\n3. **Create Probability Array**: Generate a NumPy array filled with the computed positive-class probability, with the same length as the number of rows in the test set.\n4. **Round Probabilities**: Round each element in the probability array to four decimal places.\n5. **Convert to List**: Convert the rounded NumPy array to a Python list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef baseline_prob_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[float]:\n    \"\"\"Return a constant probability equal to the fraction of positive labels.\"\"\"\n    if X_test.shape[0] == 0:\n        return []\n    p = y_train.sum() / len(y_train)\n    prob_array = np.full(X_test.shape[0], p)\n    rounded_array = np.round(prob_array, 4)\n    return rounded_array.tolist()\n```\n\n### Explanation\n1. **Compute Positive-Class Probability**: The positive-class probability `p` is calculated by taking the sum of all positive labels (1s) in `y_train` and dividing it by the total number of samples in `y_train`.\n2. **Edge Case Handling**: If `X_test` has no rows (checked using `X_test.shape[0] == 0`), the function returns an empty list immediately.\n3. **Array Creation**: Using `np.full`, we create a NumPy array of length equal to the number of rows in `X_test`, filled with the computed probability `p`.\n4. **Rounding**: Each element of the array is rounded to four decimal places using `np.round`, ensuring the precision required by the problem statement.\n5. **Conversion to List**: The resulting NumPy array is converted to a Python list using the `tolist()` method to meet the output requirements.\n\nThis approach efficiently computes the required probabilities, handles edge cases, and ensures the output is correctly formatted and rounded."}
{"task_id": 249, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward pass of an Actor-Critic neural network using NumPy. The network consists of three shared fully-connected layers with ReLU activations, followed by separate actor and critic heads. The actor head outputs action probabilities using a softmax function, while the critic head outputs a scalar state value. The results are rounded to four decimal places, with special handling for exact 0 or 1 values.\n\n### Approach\n1. **Convert Input State**: Convert the input state from a list to a NumPy array for matrix operations.\n2. **Shared Layers Processing**: Apply three affine transformations followed by ReLU activations using the provided weights and biases.\n3. **Actor Head**: Compute action scores using the final hidden layer output, then apply softmax to get probabilities.\n4. **Critic Head**: Compute the state value using the same hidden layer output.\n5. **Rounding**: Round the probabilities and state value to four decimal places, converting exact 0.0 or 1.0 values to one decimal place.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\"\"\"\n    x = np.array(state, dtype=np.float32)\n    \n    # Shared layers processing\n    for layer in ['1', '2', '3']:\n        W = np.array(params[f'W{layer}'], dtype=np.float32)\n        b = np.array(params[f'b{layer}'], dtype=np.float32)\n        x = x @ W + b\n        x = np.maximum(x, 0)  # ReLU activation\n    \n    # Actor head\n    Wa = np.array(params['Wa'], dtype=np.float32)\n    ba = np.array(params['ba'], dtype=np.float32)\n    actor_scores = x @ Wa + ba\n    exp_scores = np.exp(actor_scores - np.max(actor_scores))  # Numerical stability\n    probabilities = exp_scores / exp_scores.sum()\n    prob_list = probabilities.tolist()\n    \n    # Critic head\n    Wc = np.array(params['Wc'], dtype=np.float32)\n    bc = np.array(params['bc'], dtype=np.float32)\n    critic_value = (x @ Wc + bc).item()\n    \n    # Rounding probabilities\n    rounded_probs = []\n    for p in prob_list:\n        p_rounded = round(p, 4)\n        if p_rounded in (0.0, 1.0):\n            rounded_probs.append(round(p_rounded, 1))\n        else:\n            rounded_probs.append(round(p_rounded, 4))\n    \n    # Rounding critic value\n    value_rounded = round(critic_value, 4)\n    if value_rounded in (0.0, 1.0):\n        value_final = round(value_rounded, 1)\n    else:\n        value_final = round(value_rounded, 4)\n    \n    return (rounded_probs, value_final)\n```\n\n### Explanation\n1. **Shared Layers**: The input state is processed through three dense layers with ReLU activations. Each layer's output is computed as a linear transformation followed by an element-wise ReLU activation.\n2. **Actor Head**: The final hidden layer output is transformed into action scores, which are then converted to probabilities using the softmax function to ensure they sum to 1.\n3. **Critic Head**: The same hidden layer output is used to compute the state value through a linear transformation.\n4. **Rounding**: Probabilities and the state value are rounded to four decimal places. If the result is exactly 0.0 or 1.0 after rounding, it is further rounded to one decimal place for consistency with the problem's requirements. This ensures the output meets the specified precision and formatting constraints."}
{"task_id": 251, "completion_id": 0, "completion": "To solve this problem, we need to implement five widely-used activation functions in neural networks (sigmoid, tanh, ReLU, Leaky ReLU, and softmax) along with a helper function `get_activation` that returns the appropriate activation function based on the given name. Each activation function must handle scalar, list/tuple, and 1-D NumPy array inputs, and return the result with the same structure, rounded to four decimal places.\n\n### Approach\n1. **Activation Functions**:\n    - **Sigmoid**: Maps input values to the range (0, 1) using the logistic function.\n    - **Tanh**: Maps input values to the range (-1, 1) using the hyperbolic tangent function.\n    - **ReLU**: Returns the input if positive, otherwise returns 0.\n    - **Leaky ReLU**: Returns the input if positive, otherwise returns a small fraction (0.01) of the input.\n    - **Softmax**: Converts input values into probabilities that sum to 1, using an exponential function adjusted for numerical stability.\n\n2. **Handling Input Types**:\n    - Each function checks if the input is a scalar (int or float). If so, it computes the activation, rounds the result using NumPy for consistency, and returns a scalar.\n    - For non-scalar inputs (list, tuple, 1-D array), the input is converted to a NumPy array, processed element-wise, rounded, and returned as a 1-D array.\n\n3. **Helper Function**:\n    - `get_activation` maps function names to their respective implementations. For Leaky ReLU, a lambda is used to fix the alpha parameter at 0.01.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    if isinstance(x, (int, float)):\n        result = 1 / (1 + np.exp(-x))\n        rounded = np.round(result, 4)\n        return rounded.item()\n    else:\n        x = np.asarray(x)\n        result = 1 / (1 + np.exp(-x))\n        rounded = np.round(result, 4)\n        return rounded\n\ndef tanh(x):\n    if isinstance(x, (int, float)):\n        result = np.tanh(x)\n        rounded = np.round(result, 4)\n        return rounded.item()\n    else:\n        x = np.asarray(x)\n        result = np.tanh(x)\n        rounded = np.round(result, 4)\n        return rounded\n\ndef relu(x):\n    if isinstance(x, (int, float)):\n        result = max(0.0, x)\n        rounded = np.round(result, 4)\n        return rounded.item()\n    else:\n        x = np.asarray(x)\n        result = np.maximum(0, x)\n        rounded = np.round(result, 4)\n        return rounded\n\ndef leaky_relu(x, alpha: float = 0.01):\n    if isinstance(x, (int, float)):\n        val = x if x >= 0 else alpha * x\n        rounded = np.round(val, 4)\n        return rounded.item()\n    else:\n        x = np.asarray(x)\n        result = np.where(x >= 0, x, x * alpha)\n        rounded = np.round(result, 4)\n        return rounded\n\ndef softmax(x):\n    if isinstance(x, (int, float)):\n        return np.round(1.0, 4).item()\n    else:\n        x = np.asarray(x)\n        max_x = np.max(x)\n        e_x = np.exp(x - max_x)\n        sum_e_x = np.sum(e_x)\n        result = e_x / sum_e_x\n        rounded = np.round(result, 4)\n        return rounded\n\ndef get_activation(name):\n    activations = {\n        \"sigmoid\": sigmoid,\n        \"tanh\": tanh,\n        \"relu\": relu,\n        \"leaky_relu\": lambda x: leaky_relu(x, alpha=0.01),\n        \"softmax\": softmax\n    }\n    if name not in activations:\n        raise ValueError('Invalid activation function.')\n    return activations[name]\n```\n\n### Explanation\n- **Sigmoid**: Converts each input value to a value between 0 and 1 using the logistic function.\n- **Tanh**: Converts each input value to a value between -1 and 1 using the hyperbolic tangent function.\n- **ReLU**: Returns the input value if positive, otherwise returns 0.\n- **Leaky ReLU**: Returns the input value if positive, otherwise returns a small fraction (0.01) of the input to avoid dead neurons.\n- **Softmax**: Converts input values into probabilities by exponentiating each value, adjusting for numerical stability by subtracting the maximum value, and normalizing by the sum of exponentials.\n- **get_activation**: Returns the corresponding activation function based on the input name, ensuring the correct parameters are used (e.g., alpha=0.01 for Leaky ReLU).\n\nEach function ensures the output structure matches the input structure (scalar or 1-D array) and rounds results to four decimal places using NumPy for consistency across all input types."}
{"task_id": 252, "completion_id": 0, "completion": "To solve this problem, we need to compute the gradient and the diagonal Hessian of the least-squares loss function for given actual and predicted values. The least-squares loss function is defined as \\( L(actual, predicted) = 0.5 \\times || actual - predicted ||^2 \\). The gradient and Hessian are derived based on this loss function.\n\n### Approach\n1. **Gradient Calculation**: The gradient of the least-squares loss with respect to the predictions is given by the difference between the actual values and the predicted values. This is computed directly using NumPy array operations and then converted to a Python list.\n2. **Hessian Calculation**: The Hessian matrix for the least-squares loss is a diagonal matrix where each diagonal entry is 1. Since we only need the diagonal entries, we generate a list of ones with the same length as the actual values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef least_squares_loss(actual: np.ndarray, predicted: np.ndarray) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the gradient and Hessian of the least-squares loss.\n\n    The least-squares loss is defined as 0.5 * ||actual \u2212 predicted||\u00b2.\n\n    Args:\n        actual: 1-D NumPy array containing the true labels/targets.\n        predicted: 1-D NumPy array containing the model predictions.\n\n    Returns:\n        A tuple (grad, hess):\n            grad  \u2013 Python list representing the gradient w.r.t. each prediction.\n            hess  \u2013 Python list representing the diagonal Hessian entries (all ones).\n    \"\"\"\n    grad = (actual - predicted).tolist()\n    hess = [1.0] * len(actual)\n    return (grad, hess)\n```\n\n### Explanation\n- **Gradient Calculation**: The gradient is computed as the element-wise difference between the actual values and the predicted values. This is done using NumPy's array subtraction, which is efficient and concise. The result is then converted to a Python list using the `tolist()` method.\n- **Hessian Calculation**: The Hessian matrix for the least-squares loss is diagonal with each entry being 1. This is because the second derivative of the loss function with respect to each prediction coordinate is 1. The Hessian is represented as a list of ones with the same length as the actual values, ensuring it matches the required dimensions.\n\nThis approach efficiently leverages NumPy for vector operations and converts the results to Python lists as specified, ensuring both correctness and performance."}
{"task_id": 253, "completion_id": 0, "completion": "To solve this problem, we need to compute either the Elastic-Net penalty value or its analytical gradient for a given weight vector. The Elastic-Net penalty is a combination of L1 and L2 regularization terms, which helps in reducing model complexity and preventing overfitting. The solution involves calculating the penalty or its gradient based on the provided parameters and returning the result rounded to four decimal places.\n\n### Approach\n1. **Problem Analysis**:\n   - The Elastic-Net penalty combines L1 and L2 regularization terms. The penalty is calculated using a convex combination of these terms, controlled by the L1 ratio (\u03bb).\n   - The gradient of the penalty is derived from the sum of the gradients of the L1 and L2 terms.\n\n2. **Key Insights**:\n   - The L1 term is the sum of the absolute values of the weight vector elements.\n   - The L2 term is half the sum of the squares of the weight vector elements.\n   - The gradient of the L1 term is the sign of each weight element, and the gradient of the L2 term is the weight element itself.\n\n3. **Algorithm Design**:\n   - **Penalty Calculation**:\n     - Convert the weight vector to a NumPy array for efficient computation.\n     - Compute the L1 and L2 terms using the given parameters.\n     - Combine the terms and multiply by the regularization strength (\u03b1) to get the penalty value.\n   - **Gradient Calculation**:\n     - Compute the gradient for each weight element by combining the gradients of the L1 and L2 terms.\n     - Round each element of the gradient vector to four decimal places and return as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w_np = np.asarray(w)\n    if gradient:\n        grad = alpha * (l1_ratio * np.sign(w_np) + (1 - l1_ratio) * w_np)\n        grad_rounded = np.round(grad, 4)\n        return grad_rounded.tolist()\n    else:\n        l1_term = l1_ratio * np.sum(np.abs(w_np))\n        l2_sq_term = 0.5 * (1 - l1_ratio) * np.sum(w_np ** 2)\n        penalty = alpha * (l1_term + l2_sq_term)\n        return round(penalty, 4)\n```\n\n### Explanation\n- **Penalty Calculation**:\n  - The penalty is computed by combining the L1 norm (sum of absolute values) and the squared L2 norm (sum of squares) of the weight vector. Each term is scaled by its respective coefficient (\u03bb and 1-\u03bb) and then combined with the overall regularization strength (\u03b1).\n\n- **Gradient Calculation**:\n  - The gradient is derived by taking the sign of each element for the L1 term and the element itself for the L2 term. These gradients are combined using the respective coefficients and scaled by \u03b1. Each element of the resulting gradient vector is rounded to four decimal places.\n\nThis approach ensures that the function efficiently computes either the penalty or its gradient based on the input parameters, providing accurate results rounded to the required precision."}
{"task_id": 255, "completion_id": 0, "completion": "To solve this problem, we need to determine if a Blackjack hand contains a usable ace. A usable ace is defined as an ace that can be counted as 11 without causing the hand's total value to exceed 21.\n\n### Approach\n1. **Check for Presence of Ace**: First, we check if there is at least one ace (represented by the value 1) in the hand. If there are no aces, we immediately return False.\n2. **Calculate Hand Total with All Aces as 1**: Compute the total value of the hand considering all aces as 1. This is straightforward as it involves summing all the card values directly.\n3. **Check Usable Ace Condition**: Convert one of the aces from 1 to 11 and check if the new total is within the limit of 21. This conversion effectively adds 10 to the total value (since 11 - 1 = 10). If this adjusted total is less than or equal to 21, the hand has a usable ace.\n\n### Solution Code\n```python\ndef usable_ace(hand: list[int]) -> bool:\n    \"\"\"Determine whether a Blackjack hand has a usable ace.\n\n    A *usable ace* is an ace that can be counted as 11 without causing the\n    hand's total value to exceed 21.\n\n    Args:\n        hand: List of integers where each integer represents the value of a card\n              in a Blackjack hand (ace is 1, face cards are 10, etc.).\n\n    Returns:\n        True if the hand contains a usable ace, otherwise False.\n    \"\"\"\n    if 1 not in hand:\n        return False\n    total = sum(hand)\n    return (total + 10) <= 21\n```\n\n### Explanation\n1. **Presence of Ace**: The function first checks if there is at least one ace in the hand using `1 in hand`. If no ace is found, the function returns False.\n2. **Hand Total Calculation**: The total value of the hand is calculated by summing all the card values. This total assumes all aces are counted as 1.\n3. **Usable Ace Check**: By adding 10 to the calculated total (which effectively converts one ace from 1 to 11), we check if the new total is within the limit of 21. If it is, the hand has a usable ace, and the function returns True; otherwise, it returns False.\n\nThis approach efficiently checks for the presence of a usable ace by leveraging simple arithmetic and conditional checks, ensuring optimal performance and correctness."}
{"task_id": 256, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\"\"\"\n\n    if param_name in {\"X\", \"y\"}:\n        return None\n\n    # Handle Ba and Bx special cases\n    if param_name == \"Ba\":\n        key = \"ba\"\n    elif param_name == \"Bx\":\n        key = \"bx\"\n    else:\n        key = param_name\n\n    # Check if the key exists in the parameters (assuming model.parameters is accessible)\n    if key not in model.parameters:\n        return None  # Or raise an error, but problem statement implies existence\n\n    param = model.parameters[key]\n    original_param = param.copy()  # Save original parameter to restore later\n\n    # Convert the parameter to float64 for accurate computations\n    param_float64 = param.astype(np.float64)\n    model.parameters[key] = param_float64  # Temporarily replace with float64 version\n\n    # Initialize gradients array\n    grads = np.zeros_like(param_float64, dtype=np.float64)\n\n    # Iterate over each element of the parameter\n    for idx in np.ndindex(param_float64.shape):\n        original_val = param_float64[idx].item()  # Ensure scalar value\n\n        # Compute L(\u03b8_i + \u03b5)\n        param_float64[idx] = original_val + epsilon\n        model.flush_gradients()  # Reset any internal state/gradients\n        predictions_plus = []\n        for t in range(n_t):\n            X_t = X[:, :, t].astype(np.float64)  # Ensure input is float64\n            pred = model.forward(X_t)\n            predictions_plus.append(pred)\n        L_plus = loss_func(predictions_plus)\n\n        # Compute L(\u03b8_i - \u03b5)\n        param_float64[idx] = original_val - epsilon\n        model.flush_gradients()\n        predictions_minus = []\n        for t in range(n_t):\n            X_t = X[:, :, t].astype(np.float64)\n            pred = model.forward(X_t)\n            predictions_minus.append(pred)\n        L_minus = loss_func(predictions_minus)\n\n        # Restore original value\n        param_float64[idx] = original_val\n\n        # Centered difference\n        grad = (L_plus - L_minus) / (2 * epsilon)\n        grads[idx] = grad\n\n    # Restore the original parameter (dtype and value)\n    model.parameters[key] = original_param\n\n    return grads.T  # Return the transposed numerical gradient\n```"}
{"task_id": 257, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. AdaBoost works by iteratively training weak classifiers, adjusting the weights of misclassified samples, and combining the weak classifiers into a strong classifier. Each decision stump (one-level decision tree) is trained to minimize the weighted classification error, and the contribution of each stump is weighted based on its accuracy.\n\n### Approach\n1. **Initialize Weights**: Start with uniform weights for all training samples.\n2. **Iterate for Each Weak Classifier**: For each iteration, find the best decision stump (feature, threshold, and direction) that minimizes the weighted classification error.\n3. **Compute Classifier Weight**: Use the weighted error to compute the classifier's weight (alpha) using the formula: \n   \\[\n   \\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\varepsilon_t}{\\varepsilon_t}\\right)\n   \\]\n   where \\(\\varepsilon_t\\) is the weighted error of the stump.\n4. **Update Sample Weights**: Increase the weights of misclassified samples and decrease the weights of correctly classified samples, then normalize the weights.\n5. **Predict on Test Data**: Combine the predictions of all weak classifiers weighted by their alpha values to form the final prediction for each test sample.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\"\"\"\n    \n    m, n = X_train.shape\n    n_clf = max(n_clf, 1)\n    weights = np.ones(m) / m\n    stumps = []\n    \n    for _ in range(n_clf):\n        best_error = np.inf\n        best_feature = None\n        best_threshold = None\n        best_direction = None\n        \n        for j in range(n):\n            feature_values = X_train[:, j]\n            sorted_indices = np.argsort(feature_values)\n            sorted_feature = feature_values[sorted_indices]\n            sorted_y = y_train[sorted_indices]\n            sorted_weights = weights[sorted_indices]\n            \n            # Check thresholds between consecutive values\n            for i in range(len(sorted_feature) - 1):\n                th = (sorted_feature[i] + sorted_feature[i+1]) / 2\n                # Direction +1: left predicts -1, right predicts +1\n                error_plus = np.sum(sorted_weights[:i+1][sorted_y[:i+1] != -1]) + np.sum(sorted_weights[i+1:][sorted_y[i+1:] != 1])\n                # Direction -1: left predicts +1, right predicts -1\n                error_minus = np.sum(sorted_weights[:i+1][sorted_y[:i+1] != 1]) + np.sum(sorted_weights[i+1:][sorted_y[i+1:] != -1])\n                \n                current_error = min(error_plus, error_minus)\n                current_direction = 1 if error_plus < error_minus else -1\n                \n                if current_error < best_error:\n                    best_error = current_error\n                    best_feature = j\n                    best_threshold = th\n                    best_direction = current_direction\n            \n            # Check threshold before the first element\n            th = sorted_feature[0] - 1e-5\n            error_plus = np.sum(sorted_weights[sorted_y != 1])\n            error_minus = np.sum(sorted_weights[sorted_y != -1])\n            current_error = min(error_plus, error_minus)\n            current_direction = 1 if error_plus < error_minus else -1\n            if current_error < best_error:\n                best_error = current_error\n                best_feature = j\n                best_threshold = th\n                best_direction = current_direction\n            \n            # Check threshold after the last element\n            th = sorted_feature[-1] + 1e-5\n            error_plus = np.sum(sorted_weights[sorted_y != -1])\n            error_minus = np.sum(sorted_weights[sorted_y != 1])\n            current_error = min(error_plus, error_minus)\n            current_direction = 1 if error_plus < error_minus else -1\n            if current_error < best_error:\n                best_error = current_error\n                best_feature = j\n                best_threshold = th\n                best_direction = current_direction\n        \n        epsilon = max(best_error, 1e-10)\n        epsilon = min(epsilon, 1 - 1e-10)\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        \n        # Update weights\n        pred = np.where(X_train[:, best_feature] >= best_threshold, best_direction, -best_direction)\n        incorrect = (pred != y_train)\n        weights *= np.exp(alpha * incorrect)\n        weights /= np.sum(weights)\n        \n        stumps.append((best_feature, best_threshold, best_direction, alpha))\n    \n    # Predict on X_test\n    predictions = []\n    for x in X_test:\n        sum_alpha = 0.0\n        for j, th, d, a in stumps:\n            if x[j] >= th:\n                h = d\n            else:\n                h = -d\n            sum_alpha += a * h\n        predictions.append(1 if sum_alpha >= 0 else -1)\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: We start by initializing the weights of all training samples to be equal.\n2. **Weak Classifier Training**: For each weak classifier, we iterate over each feature and potential threshold to find the best split that minimizes the weighted error. This involves checking all possible thresholds and both directions (predicting +1 or -1 for values above the threshold).\n3. **Error Calculation**: For each potential split, we calculate the weighted error for both possible directions and select the one with the lower error.\n4. **Alpha Calculation**: The weight (alpha) of each classifier is computed based on its error, ensuring numerical stability by clipping error values.\n5. **Weight Update**: The weights of misclassified samples are increased, and the weights of correctly classified samples are decreased, followed by normalization.\n6. **Prediction**: For each test sample, the predictions of all weak classifiers are combined using their respective alpha values, and the final prediction is determined by the sign of the weighted sum.\n\nThis approach efficiently combines multiple weak classifiers to form a strong classifier, leveraging the iterative re-weighting mechanism of AdaBoost to focus on difficult samples in subsequent iterations."}
{"task_id": 258, "completion_id": 0, "completion": "To solve this problem, we need to implement the hinge-loss function, which is commonly used in training linear Support Vector Machines (SVMs). The hinge loss function measures the error between the predicted scores and the actual class labels, which are encoded as -1 or 1. The goal is to compute the average hinge loss efficiently using vectorized operations in NumPy.\n\n### Approach\n1. **Input Conversion**: Convert the input lists or arrays to NumPy arrays to leverage vectorized operations.\n2. **Length Check**: Ensure that the lengths of the actual and predicted arrays are the same. If not, return -1.\n3. **Compute Per-Sample Loss**: Calculate the hinge loss for each sample using the formula \\( L_i = \\max(0, 1 - y_i \\cdot \\hat{y}_i) \\), where \\( y_i \\) is the actual label and \\( \\hat{y}_i \\) is the predicted score.\n4. **Compute Mean Loss**: Calculate the arithmetic mean of all per-sample losses.\n5. **Round the Result**: Round the mean loss to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hinge_loss(actual, predicted):\n    \"\"\"Compute the average hinge loss for binary classification.\n\n    Parameters\n    ----------\n    actual : list[int | float] | numpy.ndarray\n        True class labels encoded as -1 or 1.\n    predicted : list[int | float] | numpy.ndarray\n        Real-valued prediction scores.\n\n    Returns\n    -------\n    float | int\n        The mean hinge loss rounded to four decimal places, or -1 if the\n        two inputs have different lengths.\n    \"\"\"\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    \n    if len(actual) != len(predicted):\n        return -1\n    \n    product = actual * predicted\n    losses = np.maximum(0, 1 - product)\n    mean_loss = np.mean(losses)\n    return round(mean_loss, 4)\n```\n\n### Explanation\n1. **Input Conversion**: The function starts by converting the input lists into NumPy arrays to utilize NumPy's efficient vectorized operations.\n2. **Length Check**: It checks if the lengths of the actual and predicted arrays are the same. If they differ, the function returns -1 immediately.\n3. **Per-Sample Loss Calculation**: The element-wise product of actual and predicted arrays is computed. This product is then subtracted from 1, and the result is clipped at 0 using `np.maximum` to ensure non-negative values, which gives the per-sample hinge losses.\n4. **Mean Loss Calculation**: The mean of these per-sample losses is computed using `np.mean`.\n5. **Rounding**: The mean loss is rounded to four decimal places using Python's built-in `round` function to meet the specified precision requirement.\n\nThis approach ensures that the computation is efficient and leverages NumPy's optimized operations for handling array computations, making it suitable for large datasets."}
{"task_id": 259, "completion_id": 0, "completion": "To solve this problem, we need to create a deterministic sampler for an experience replay buffer used in reinforcement learning algorithms. The sampler should select a specified number of transitions from the buffer using a fixed seed for reproducibility, ensuring deterministic results. If the requested sample size exceeds the buffer's current length, the function should return -1.\n\n### Approach\n1. **Check Buffer Length**: First, we check if the buffer length is less than the requested batch size. If it is, return -1 as specified.\n2. **Seed Random Generator**: To ensure deterministic sampling, we seed the random number generator with a fixed value (42) each time the function is called.\n3. **Sample Transitions**: Use the `random.sample` function to select the specified number of transitions from the buffer. This function ensures that the selected elements are unique and returns them in the order they were sampled.\n\n### Solution Code\n```python\nimport random\n\ndef sample_replay_memory(memory: list, batch_size: int) -> list:\n    \"\"\"Randomly samples *batch_size* transitions from the replay buffer.\n\n    The selection must be deterministic (use random.seed(42)).  If\n    *batch_size* exceeds the buffer length, return -1.\n    \"\"\"\n    if len(memory) < batch_size:\n        return -1\n    random.seed(42)\n    return random.sample(memory, batch_size)\n```\n\n### Explanation\n- **Deterministic Sampling**: By seeding the random number generator with 42 before each call to `random.sample`, we ensure that the same sequence of random numbers is generated each time the function is called. This makes the sampling process deterministic.\n- **Handling Insufficient Buffer Size**: The function immediately checks if the buffer has enough elements to fulfill the requested batch size. If not, it returns -1 to indicate an error.\n- **Order of Sampled Elements**: The `random.sample` function returns elements in the order they are selected, which is preserved in the output list, ensuring consistency across different runs.\n\nThis approach guarantees that the sampling process is both deterministic and efficient, adhering to the problem constraints and requirements."}
{"task_id": 260, "completion_id": 0, "completion": "To solve this problem, we need to implement a function called `tileswrap` that generates tile indices for given continuous and integer variables using a wrapping tile coding technique. This technique is commonly used in reinforcement learning to discretize continuous state variables by overlaying multiple grids (tilings) and computing indices for each tiling.\n\n### Approach\n1. **Quantize Floats**: Convert each continuous variable (float) into a quantized integer value using the formula `q_i = floor(f_i * numtilings)`.\n2. **Process Each Tiling**: For each tiling from 0 to `numtilings-1`:\n   - Initialize the coordinates list with the current tiling index.\n   - For each quantized float value, compute its coordinate considering the current tiling's offset and apply wrapping if specified.\n   - Update the offset for subsequent floats within the same tiling.\n   - Append additional integer variables to the coordinates list.\n3. **Hash Coordinates**: Use a deterministic polynomial hash function to convert the coordinates list into an index within the specified range.\n\n### Solution Code\n```python\nfrom math import floor\n\ndef tileswrap(ihtORsize: int,\n              numtilings: int,\n              floats: list[float],\n              wrapwidths: list[int | None],\n              ints: list[int] | None = None,\n              readonly: bool = False) -> list[int]:\n    \"\"\"Return tile indices for the given floats/ints using wrapping tile coding.\"\"\"\n\n    q_list = [floor(f * numtilings) for f in floats]\n    ints = [] if ints is None else ints\n    indices = []\n\n    for t in range(numtilings):\n        coords = [t]\n        b = t\n\n        for q_i, wrapwidth in zip(q_list, wrapwidths):\n            mod_b = b % numtilings\n            c = (q_i + mod_b) // numtilings\n            if wrapwidth is not None and wrapwidth > 0:\n                c %= wrapwidth\n            coords.append(c)\n            b += t * 2  # Increment b after processing each float\n\n        coords.extend(ints)\n\n        index = 0\n        for coord in coords:\n            index = (index * 31 + coord) % ihtORsize\n        indices.append(index)\n\n    return indices\n```\n\n### Explanation\n1. **Quantization**: Each float is scaled by the number of tilings and floored to get a quantized integer. This helps in mapping continuous values to discrete tiles.\n2. **Tiling Processing**: For each tiling, starting with the tiling index, we compute coordinates for each float. The offset `b` is adjusted for each float to ensure different tilings produce different coordinates. If a wrap width is specified, the coordinate is wrapped modulo the wrap width.\n3. **Integer Variables**: Additional integer variables are appended directly to the coordinates list without modification.\n4. **Hashing**: The coordinates list is hashed using a polynomial function to produce a unique index within the specified range. This ensures deterministic and efficient mapping of coordinates to indices.\n\nThis approach efficiently handles both continuous and integer variables, applying wrapping where necessary to support periodic dimensions, and generates the required tile indices for each tiling."}
{"task_id": 261, "completion_id": 0, "completion": "To solve this problem, we need to implement the Glorot (Xavier) normal weight-initialization function for neural networks. This function initializes the weights of a neural network layer such that the variance of the activations is consistent across layers, which helps in faster convergence during training.\n\n### Approach\nThe Glorot normal initialization involves three main steps:\n1. **Calculate fan_in and fan_out**: These values depend on the dimensions of the input and output layers. For a 2-D tensor, `fan_in` and `fan_out` are the dimensions of the input and output layers respectively. For higher-dimensional tensors (e.g., convolutional layers), `fan_in` and `fan_out` are computed considering the product of the kernel dimensions (receptive field size) along with the input and output channels.\n2. **Compute the standard deviation**: Using the formula \\( s = \\sqrt{\\frac{2}{fan\\_in + fan\\_out}} \\), we determine the standard deviation for the normal distribution.\n3. **Generate the weight matrix**: Using NumPy's random number generator, we create a tensor with the specified shape, where each element is drawn from a normal distribution with mean 0 and the computed standard deviation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    if len(shape) == 2:\n        fan_in, fan_out = shape[0], shape[1]\n    else:\n        receptive_field_size = np.prod(shape[2:])\n        fan_in = shape[1] * receptive_field_size\n        fan_out = shape[0] * receptive_field_size\n    \n    std = np.sqrt(2.0 / (fan_in + fan_out))\n    return np.random.normal(0.0, std, shape)\n```\n\n### Explanation\n1. **Fan_in and Fan_out Calculation**:\n   - For a 2-D tensor (e.g., a fully-connected layer), `fan_in` is the number of input units, and `fan_out` is the number of output units.\n   - For higher-dimensional tensors (e.g., convolutional layers), the first two dimensions are the number of output and input channels, respectively. The product of the remaining dimensions (kernel size) gives the receptive field size. `fan_in` is the product of input channels and receptive field size, while `fan_out` is the product of output channels and receptive field size.\n\n2. **Standard Deviation Calculation**:\n   - The standard deviation is computed using the formula derived from the Glorot initialization theory, which balances the variance of the activations across layers.\n\n3. **Weight Matrix Generation**:\n   - Using NumPy's `random.normal` function, we generate a tensor with the specified shape, ensuring the elements are sampled from a normal distribution with the calculated standard deviation and mean 0.\n\nThis approach ensures that the initial weights are set in a way that promotes efficient training by maintaining consistent variance across network layers."}
{"task_id": 263, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward pass of a fully-connected neural network given the input, weights, and activation functions for each layer. The goal is to compute the output of the network after applying each layer's transformations and activation functions, and then return the result rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: Convert the input `x` into a numpy array and reshape it into a column vector to facilitate matrix operations.\n2. **Layer Processing**: For each layer, prepend a bias term to the activation vector from the previous layer, compute the pre-activation by multiplying with the weight matrix, and apply the specified activation function.\n3. **Activation Functions**: Implement the sigmoid, ReLU, and linear activation functions to transform the pre-activation values.\n4. **Output Processing**: After processing all layers, round the final output to four decimal places and return it as a scalar if the output layer has a single neuron, otherwise return it as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef feedforward_predict(x, weights, activations):\n    \"\"\"Runs a forward pass through a fully-connected neural network.\"\"\"\n    # Convert input to a column vector\n    a_prev = np.array(x, dtype=np.float64).reshape(-1, 1)\n    \n    for W, activation in zip(weights, activations):\n        # Convert weight matrix to numpy array\n        W = np.array(W, dtype=np.float64)\n        # Prepend the bias term (1) to the previous activation\n        tilde_a_prev = np.vstack(([[1]], a_prev))\n        # Compute pre-activation\n        z = W @ tilde_a_prev\n        # Apply activation function\n        if activation == 'sigmoid':\n            a_prev = 1 / (1 + np.exp(-z))\n        elif activation == 'relu':\n            a_prev = np.maximum(0, z)\n        elif activation == 'linear':\n            a_prev = z\n        else:\n            raise ValueError(f\"Unsupported activation function: {activation}\")\n    \n    # Process the output to meet the specified format\n    output = np.round(a_prev, 4)\n    if output.size == 1:\n        return output.item()\n    else:\n        return output.flatten().tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input `x` is converted to a numpy array and reshaped into a column vector to ensure proper matrix operations.\n2. **Bias Term Prepending**: For each layer, a bias term (1) is prepended to the activation vector from the previous layer to form the extended activation vector.\n3. **Pre-activation Calculation**: The pre-activation vector is computed by multiplying the weight matrix (which includes the bias weights in the first column) with the extended activation vector.\n4. **Activation Application**: The appropriate activation function (sigmoid, ReLU, or linear) is applied element-wise to the pre-activation vector to get the activation vector for the next layer.\n5. **Output Handling**: The final activation vector is rounded to four decimal places. If the output layer has a single neuron, the result is returned as a scalar; otherwise, it is returned as a list.\n\nThis approach efficiently processes each layer of the neural network using matrix operations, ensuring correctness and adherence to the specified activation functions, and handles the output formatting as required."}
{"task_id": 265, "completion_id": 0, "completion": "To solve this problem, we need to implement a piece-wise function called King\u2019s loss that evaluates a scalar input based on specific conditions and returns the result rounded to four decimal places. The function has four distinct segments, each applicable to a different range of the input value.\n\n### Approach\n1. **Understand the Segments**: The King\u2019s loss function is divided into four segments based on the input value `x`:\n   - For `x \u2264 250`, the loss is calculated using a linear formula.\n   - For `250 < x \u2264 600`, the loss is a constant value.\n   - For `600 < x \u2264 700`, the loss is another linear formula.\n   - For `x > 700`, the loss is another constant value.\n\n2. **Conditional Checks**: The function checks the value of `x` against these ranges in a specific order to ensure the correct segment is applied. The order of checks is crucial to handle boundary values correctly.\n\n3. **Compute and Round**: For each segment, compute the loss using the respective formula or constant. The result is then rounded to four decimal places using Python's `round` function.\n\n### Solution Code\n```python\ndef king_loss_fn(x: int | float) -> float:\n    \"\"\"Compute King\u2019s piece-wise loss for a scalar value.\n\n    The loss function L(x) is defined by four segments:\n      \u2022 x \u2264 250:   L(x) = \u20130.25 * x + 82.50372665317208\n      \u2022 250 < x \u2264 600: L(x) = 20.00372665317208\n      \u2022 600 < x \u2264 700: L(x) = \u20130.2 * x + 140.00372665317207\n      \u2022 x > 700:  L(x) = 0.003726653172066108\n\n    The result must be rounded to 4 decimal places.\n\n    Args:\n        x: An integer or float input.\n\n    Returns:\n        A float rounded to four decimal places.\n    \"\"\"\n    if x <= 250:\n        loss = -0.25 * x + 82.50372665317208\n    elif x <= 600:\n        loss = 20.00372665317208\n    elif x <= 700:\n        loss = -0.2 * x + 140.00372665317207\n    else:\n        loss = 0.003726653172066108\n    return round(loss, 4)\n```\n\n### Explanation\n1. **Conditional Checks**: The function first checks if `x` is less than or equal to 250. If true, it uses the linear formula for that range. If not, it proceeds to check if `x` is within the next range (250 < x \u2264 600) and uses the constant value. This continues for the subsequent ranges, ensuring the correct segment is applied based on the input value.\n\n2. **Computation**: Depending on the range determined by the conditional checks, the appropriate formula or constant is used to compute the loss. Each formula is applied as specified in the problem statement.\n\n3. **Rounding**: The computed loss value is rounded to four decimal places using Python's built-in `round` function, ensuring the result adheres to the required precision.\n\nThis approach efficiently handles all possible input values and ensures the correct segment is applied, even for boundary cases, providing an accurate and precise result."}
{"task_id": 266, "completion_id": 0, "completion": "To solve this problem, we need to convert an undirected graph given by its vertex list `V` and edge list `E` into an adjacency list representation. The adjacency list must maintain the order of vertices as they appear in `V`, ensure no duplicate neighbors, and sort each neighbor list according to their appearance in `V`.\n\n### Approach\n1. **Vertex Set Creation**: Create a set of vertices from `V` for quick membership checks.\n2. **Adjacency Dictionary Initialization**: Initialize a dictionary where each vertex maps to a set of its adjacent vertices. This helps in efficiently avoiding duplicate edges.\n3. **Edge Processing**: For each edge in `E`, add each vertex to the other's adjacency set if both vertices are present in `V`.\n4. **Vertex Index Mapping**: Create a dictionary to map each vertex to its index in `V` for sorting purposes.\n5. **Result Construction**: For each vertex in `V`, convert its adjacency set to a list sorted by the vertices' indices in `V`.\n\n### Solution Code\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\"\"\"\n    vertices_set = set(V)\n    adj_dict = {v: set() for v in V}\n    \n    for u, v in E:\n        if u in vertices_set and v in vertices_set:\n            adj_dict[u].add(v)\n            adj_dict[v].add(u)\n    \n    vertex_to_index = {v: idx for idx, v in enumerate(V)}\n    result = []\n    \n    for v in V:\n        adj_vertices = list(adj_dict[v])\n        adj_vertices.sort(key=lambda x: vertex_to_index[x])\n        result.append(adj_vertices)\n    \n    return result\n```\n\n### Explanation\n1. **Vertex Set Creation**: Using a set of vertices from `V` allows O(1) time complexity for checking if a vertex is present in `V`.\n2. **Adjacency Dictionary**: Each vertex starts with an empty set to store its neighbors. This ensures that each neighbor is added exactly once.\n3. **Edge Processing**: For each edge `(u, v)`, both vertices are added to each other's adjacency sets if they exist in `V`. This handles the undirected nature of the graph.\n4. **Index Mapping**: A dictionary maps each vertex to its index in `V`, allowing efficient sorting of neighbors based on their order in `V`.\n5. **Result Construction**: Each adjacency set is converted to a list, sorted by the vertices' indices in `V`, ensuring the required order in the output.\n\nThis approach efficiently constructs the adjacency list while maintaining the required constraints and order, ensuring correctness and optimal performance."}
{"task_id": 267, "completion_id": 0, "completion": "To solve this problem, we need to find the optimal weighted decision stump for a binary classification task. A decision stump is a simple model that makes predictions based on a single feature and a threshold, with a polarity parameter that determines the direction of the comparison. The goal is to minimize the weighted classification error given the sample weights.\n\n### Approach\n1. **Iterate Over Each Feature**: For each feature in the dataset, we consider all unique values of that feature as potential thresholds.\n2. **Sort and Preprocess**: For each feature, sort the feature values along with their corresponding labels and weights. This allows efficient computation of errors for different thresholds.\n3. **Compute Cumulative Sums**: Precompute cumulative sums of errors for both possible polarities (1 and -1) to efficiently calculate the error for each threshold.\n4. **Evaluate Thresholds**: For each unique threshold value in the sorted feature values, compute the error for both polarities. Track the threshold, feature, and polarity that yield the minimum error.\n5. **Return Best Parameters**: After evaluating all features and thresholds, return the parameters of the decision stump with the lowest weighted error.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\"\"\"\n    best_error = np.inf\n    best_feature = 0\n    best_threshold = 0.0\n    best_polarity = 1\n\n    n_samples, n_features = X.shape\n\n    for j in range(n_features):\n        x_j = X[:, j]\n        sorted_indices = np.argsort(x_j)\n        sorted_x = x_j[sorted_indices]\n        sorted_y = y[sorted_indices]\n        sorted_weights = sample_weights[sorted_indices]\n\n        unique_thetas, split_indices = np.unique(sorted_x, return_index=True)\n\n        n = len(sorted_x)\n        if n == 0:\n            continue\n\n        mask_neq_1 = (sorted_y != 1)\n        mask_neq_minus1 = (sorted_y != -1)\n\n        contrib_neq_1 = mask_neq_1 * sorted_weights\n        contrib_neq_minus1 = mask_neq_minus1 * sorted_weights\n\n        cum_neq_1 = np.zeros(n + 1)\n        cum_neq_1[1:] = np.cumsum(contrib_neq_1)\n        cum_neq_minus1 = np.zeros(n + 1)\n        cum_neq_minus1[1:] = np.cumsum(contrib_neq_minus1)\n\n        total_neq_1 = cum_neq_1[-1]\n        total_neq_minus1 = cum_neq_minus1[-1]\n\n        for theta, split_idx in zip(unique_thetas, split_indices):\n            error_p1 = cum_neq_1[split_idx] + (total_neq_minus1 - cum_neq_minus1[split_idx])\n            error_pm1 = cum_neq_minus1[split_idx] + (total_neq_1 - cum_neq_1[split_idx])\n\n            if error_p1 < best_error:\n                best_error = error_p1\n                best_feature = j\n                best_threshold = theta\n                best_polarity = 1\n            if error_pm1 < best_error:\n                best_error = error_pm1\n                best_feature = j\n                best_threshold = theta\n                best_polarity = -1\n\n    best_threshold = round(best_threshold, 4)\n    best_error = round(best_error, 4)\n\n    return {\n        \"feature_index\": best_feature,\n        \"threshold\": best_threshold,\n        \"polarity\": best_polarity,\n        \"weighted_error\": best_error\n    }\n```\n\n### Explanation\n1. **Feature Iteration**: The algorithm iterates over each feature in the dataset to evaluate all possible decision stumps based on that feature.\n2. **Sorting and Unique Thresholds**: For each feature, the data is sorted, and unique values are extracted as potential thresholds. This ensures that all possible split points are considered.\n3. **Cumulative Sums**: Cumulative sums of errors for both polarities are precomputed. This allows efficient calculation of the error for each threshold by leveraging these precomputed sums.\n4. **Error Calculation**: For each threshold, the error is calculated for both polarities (1 and -1) by considering the cumulative sums of misclassified samples. The algorithm keeps track of the threshold, feature, and polarity that result in the minimum error.\n5. **Result**: The best parameters (feature, threshold, polarity) and the minimal error are returned, with values rounded to four decimal places as required.\n\nThis approach ensures that we efficiently evaluate all possible decision stumps while minimizing the weighted classification error, making it suitable for integration into boosting algorithms like AdaBoost."}
{"task_id": 268, "completion_id": 0, "completion": "To solve this problem, we need to generate an Erd\u0151s\u2013R\u00e9nyi random graph using the \\(G(n, p)\\) model, which can be either directed or undirected. The graph is represented as an adjacency matrix where each entry indicates the presence of an edge between two vertices.\n\n### Approach\n1. **Problem Analysis**:\n   - **Vertices and Edges**: The graph has `n_vertices` vertices. Each possible edge (ordered pair for directed, unordered pair for undirected) is considered once.\n   - **Probability**: Each edge is included with probability `edge_prob`, excluding self-loops.\n   - **Matrix Representation**: The adjacency matrix must be symmetric for undirected graphs and can be asymmetric for directed graphs.\n\n2. **Key Insights**:\n   - **Directed Graphs**: For each ordered pair (i, j) where \\(i \\neq j\\), include the edge if a random number is \u2264 `edge_prob`.\n   - **Undirected Graphs**: For each unordered pair (i, j) where \\(i < j\\), include the edge in both directions (i, j) and (j, i) if the random number is \u2264 `edge_prob`.\n\n3. **Algorithm Selection**:\n   - **Directed Graphs**: Generate a random matrix and zero out the diagonal to exclude self-loops.\n   - **Undirected Graphs**: Generate random values for the upper triangle (excluding the diagonal) and mirror them to the lower triangle to ensure symmetry.\n\n4. **Complexity Considerations**:\n   - **Time Complexity**: Generating the matrix using vectorized operations ensures efficiency even for large graphs.\n   - **Space Complexity**: The adjacency matrix requires \\(O(n^2)\\) space, which is optimal for dense graphs.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_unweighted_graph(n_vertices: int, \n                           edge_prob: float = 0.5, \n                           directed: bool = False) -> list[list[int]]:\n    \"\"\"Generate an un-weighted Erd\u0151s\u2013R\u00e9nyi random graph.\n\n    The function must return the adjacency matrix (a nested list) of the graph.\n\n    Args:\n        n_vertices (int): Number of vertices in the graph (labelled 0..n-1).\n        edge_prob  (float): Probability *p* that an admissible edge exists.\n        directed   (bool):  Whether the graph is directed (default ``False``).\n\n    Returns:\n        list[list[int]]: The adjacency matrix *A* where *A[i][j] == 1* iff an\n        edge from vertex *i* to vertex *j* exists.  The matrix must consist of\n        integers (0/1) only and have shape *(n_vertices, n_vertices)*.\n    \"\"\"\n    if n_vertices == 1:\n        return [[0]]\n    \n    if directed:\n        # Generate random matrix and set diagonal to 0 for directed graphs\n        adj = (np.random.rand(n_vertices, n_vertices) <= edge_prob).astype(int)\n        np.fill_diagonal(adj, 0)\n    else:\n        # For undirected graphs, generate upper triangle and mirror it\n        rows, cols = np.triu_indices(n_vertices, k=1)\n        r = np.random.rand(len(rows))\n        mask = r <= edge_prob\n        adj = np.zeros((n_vertices, n_vertices), dtype=int)\n        adj[rows[mask], cols[mask]] = 1\n        adj[cols[mask], rows[mask]] = 1\n    \n    return adj.tolist()\n```\n\n### Explanation\n- **Directed Graphs**: A random matrix is generated where each element (i, j) is checked against `edge_prob`. The diagonal is set to 0 to exclude self-loops.\n- **Undirected Graphs**: Random values are generated for the upper triangle (excluding the diagonal). If a value is \u2264 `edge_prob`, both (i, j) and (j, i) in the adjacency matrix are set to 1, ensuring symmetry.\n- **Edge Cases**: Handles the special case where `n_vertices` is 1 by returning a 1x1 matrix with 0, as self-loops are not allowed.\n\nThis approach efficiently generates the adjacency matrix using vectorized operations, ensuring both correctness and performance."}
{"task_id": 269, "completion_id": 0, "completion": "To solve this problem, we need to calculate the total number of trainable parameters in a two-layer Deep Q-Network (DQN) architecture. The network consists of two fully connected (dense) layers with a ReLU activation function in between, which does not contribute any trainable parameters. The goal is to compute the sum of trainable parameters (weights and biases) from both layers.\n\n### Approach\n1. **First Layer Calculation**: \n   - The first layer maps the input state vector of size `state_dimensions` to 164 hidden neurons. \n   - The number of weights in this layer is given by `state_dimensions * 164`.\n   - The number of biases is equal to the number of hidden neurons, which is 164.\n   - Therefore, the total parameters for the first layer are `(state_dimensions * 164) + 164`, which can be factored as `164 * (state_dimensions + 1)`.\n\n2. **Second Layer Calculation**:\n   - The second layer maps the 164 hidden neurons to `action_dimensions` output neurons.\n   - The number of weights here is `164 * action_dimensions`.\n   - The number of biases is equal to the number of output neurons, which is `action_dimensions`.\n   - Thus, the total parameters for the second layer are `(164 * action_dimensions) + action_dimensions`, which can be factored as `165 * action_dimensions`.\n\n3. **Total Parameters**:\n   - Sum the parameters from both layers to get the total number of trainable parameters in the network.\n\n### Solution Code\n```python\ndef count_trainable_params(state_dimensions: int, action_dimensions: int) -> int:\n    \"\"\"Return the total number of trainable parameters of the DQN network.\n\n    Parameters\n    ----------\n    state_dimensions : int\n        Length of the state (observation) vector.\n    action_dimensions : int\n        Number of possible discrete actions.\n\n    Returns\n    -------\n    int\n        Total count of trainable parameters (weights + biases).\n    \"\"\"\n    return 164 * (state_dimensions + 1) + 165 * action_dimensions\n```\n\n### Explanation\nThe function `count_trainable_params` computes the total number of trainable parameters by applying the formula for each layer's parameters and summing them up. The first layer's parameters are calculated as `164 * (state_dimensions + 1)`, and the second layer's parameters are calculated as `165 * action_dimensions`. The sum of these two values gives the total number of trainable parameters in the network, which is returned as the result. This approach efficiently breaks down the problem into manageable parts and uses simple arithmetic operations to derive the solution."}
{"task_id": 270, "completion_id": 0, "completion": "To solve this problem, we need to determine the optimal expected reward and the corresponding arm index for a contextual linear multi-armed bandit problem. The solution involves computing the expected reward for each arm based on given context and parameter matrices, rounding these rewards to four decimal places, and selecting the arm with the highest rounded reward. If multiple arms have the same highest reward, the smallest index is chosen.\n\n### Approach\n1. **Compute Expected Rewards**: For each arm, calculate the expected reward using the inner product of the corresponding context and parameter vectors. This is done efficiently using element-wise multiplication and summation.\n2. **Round Rewards**: Round each computed expected reward to four decimal places to determine the optimal reward.\n3. **Identify Optimal Arm**: Find the index of the arm with the highest rounded reward. If there are multiple arms with the same highest reward, the smallest index is selected.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef oracle_payoff(context: np.ndarray, thetas: np.ndarray) -> tuple[float, int]:\n    \"\"\"Determine the best arm for a contextual linear bandit.\n\n    Each column *k* of *context* (shape D\u00d7K) is a context vector x_k \u2208 \u211d^D.\n    Each column *k* of *thetas*  (shape D\u00d7K) is the corresponding parameter\n    vector \u03b8_k \u2208 \u211d^D.\n\n    The expected reward of arm *k* is x_k^T \u03b8_k.\n\n    The function must return the maximal expected reward rounded to four\n    decimals and the index (0-based) of the arm that achieves it.  If more\n    than one arm attains the maximal reward (after rounding), return the\n    smallest index.\n\n    Args:\n        context: numpy.ndarray of shape (D, K)\n        thetas : numpy.ndarray of shape (D, K)\n\n    Returns:\n        Tuple containing (best_reward, best_arm).\n    \"\"\"\n    expected_rewards = (context * thetas).sum(axis=0)\n    rounded_rewards = np.round(expected_rewards, 4)\n    best_arm = np.argmax(rounded_rewards)\n    max_reward = rounded_rewards[best_arm].item()\n    return (max_reward, best_arm)\n```\n\n### Explanation\n1. **Expected Rewards Calculation**: The element-wise multiplication of the context and parameter matrices followed by summation along the rows (axis 0) gives the expected reward for each arm. This is efficient and leverages vectorized operations in NumPy for performance.\n2. **Rounding**: Each expected reward is rounded to four decimal places to handle precision requirements.\n3. **Optimal Arm Selection**: Using `np.argmax` on the rounded rewards array finds the index of the first occurrence of the maximum value, ensuring the smallest index is chosen in case of ties.\n\nThis approach efficiently computes the required values using vectorized operations, ensuring both correctness and performance. The solution handles edge cases such as ties in rounded rewards by leveraging the behavior of `np.argmax` to select the smallest index."}
{"task_id": 271, "completion_id": 0, "completion": "To solve this problem, we need to compute the gain of a split in XGBoost when using the squared-error loss function. The gain is calculated based on the first- and second-order derivatives (gradient and hessian) of the loss function with respect to the predictions. The gain formula involves the gradients and hessians of the parent node and its two children nodes (left and right).\n\n### Approach\n1. **Compute Gradient and Hessian for Children Nodes**:\n   - For the left child, calculate the sum of gradients (G_L) and the number of samples (H_L).\n   - Similarly, compute the sum of gradients (G_R) and the number of samples (H_R) for the right child.\n\n2. **Compute Gradient and Hessian for Parent Node**:\n   - The parent's gradient (G_P) is the sum of the gradients of the left and right children.\n   - The parent's hessian (H_P) is the sum of the hessians of the left and right children.\n\n3. **Calculate Scores for Each Node**:\n   - The score for each node (left, right, parent) is given by the formula \\( \\text{score} = \\frac{G^2}{H} \\).\n\n4. **Compute Gain**:\n   - The gain is calculated using the formula: \\( \\text{gain} = 0.5 \\times (\\text{score}_L + \\text{score}_R - \\text{score}_P) \\).\n\n5. **Round the Result**:\n   - The result is rounded to 4 decimal places. If the rounded result is exactly zero, ensure it is returned as 0.0 instead of -0.0.\n\n### Solution Code\n```python\ndef xgboost_split_gain(\n    left_true: list[float],\n    left_pred: list[float],\n    right_true: list[float],\n    right_pred: list[float],\n) -> float:\n    \"\"\"Compute XGBoost split gain for squared-error loss.\"\"\"\n\n    # Calculate gradients and hessians for left and right children\n    G_L = sum(lp - lt for lp, lt in zip(left_pred, left_true))\n    H_L = len(left_true)\n    G_R = sum(rp - rt for rp, rt in zip(right_pred, right_true))\n    H_R = len(right_true)\n\n    # Calculate parent gradients and hessians\n    G_P = G_L + G_R\n    H_P = H_L + H_R\n\n    # Compute scores for left, right, and parent\n    score_L = (G_L ** 2) / H_L\n    score_R = (G_R ** 2) / H_R\n    score_P = (G_P ** 2) / H_P\n\n    # Calculate the gain\n    gain = 0.5 * (score_L + score_R - score_P)\n\n    # Round and ensure -0.0 is returned as 0.0\n    rounded_gain = round(gain, 4)\n    return 0.0 if rounded_gain == 0.0 else rounded_gain\n```\n\n### Explanation\n- **Gradient and Hessian Calculation**: For each child node (left and right), the gradient is computed as the sum of the differences between the predicted and true values. The hessian is simply the count of samples in each node since each hessian value is 1 for squared-error loss.\n- **Parent Node Calculation**: The parent node's gradient and hessian are the sums of the respective values from the left and right children.\n- **Score Calculation**: The score for each node is the squared gradient divided by the hessian.\n- **Gain Calculation**: The gain is derived from the difference between the combined scores of the children nodes and the score of the parent node, scaled by 0.5.\n- **Rounding and Handling Zero**: The result is rounded to 4 decimal places, and any occurrence of -0.0 is explicitly converted to 0.0 to meet the problem's requirements."}
{"task_id": 273, "completion_id": 0, "completion": "To solve this problem, we need to determine the optimal numerical threshold for splitting a decision tree based on information gain. The goal is to find the threshold that maximizes the information gain when splitting the dataset into left and right subsets.\n\n### Approach\n1. **Sort the Features and Targets**: Combine the feature and target values into pairs and sort them by the feature values. This helps in efficiently generating candidate thresholds and splitting the dataset.\n2. **Generate Candidate Thresholds**: Identify midpoints between consecutive distinct values in the sorted feature list. These midpoints are the candidate thresholds for splitting the data.\n3. **Compute Parent Entropy**: Calculate the entropy of the entire target set, which serves as the baseline entropy before any split.\n4. **Evaluate Each Threshold**: For each candidate threshold, split the dataset into left and right subsets, compute their entropies, and calculate the information gain.\n5. **Select the Best Threshold**: Choose the threshold that maximizes the information gain. If multiple thresholds yield the same gain, select the smallest threshold. If no threshold provides a positive gain, return `(None, 0.0)`.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\nimport bisect\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\"\"\"\n    # Sort the features and corresponding targets\n    sorted_pairs = sorted(zip(feature, target), key=lambda x: x[0])\n    sorted_features = [x[0] for x in sorted_pairs]\n    sorted_targets = [x[1] for x in sorted_pairs]\n    \n    # Generate candidate thresholds between consecutive distinct sorted features\n    unique_sorted = []\n    prev = None\n    for val in sorted_features:\n        if val != prev:\n            unique_sorted.append(val)\n            prev = val\n    candidate_thresholds = []\n    for i in range(len(unique_sorted) - 1):\n        mid = (unique_sorted[i] + unique_sorted[i+1]) / 2\n        candidate_thresholds.append(mid)\n    \n    # Check if there are no legal thresholds\n    if not candidate_thresholds:\n        return (None, 0.0)\n    \n    # Compute parent entropy\n    parent_counts = Counter(sorted_targets)\n    total_parent = len(sorted_targets)\n    parent_entropy = 0.0\n    for count in parent_counts.values():\n        p = count / total_parent\n        if p > 0:\n            parent_entropy -= p * math.log2(p)\n    \n    # If parent entropy is zero, no split will improve\n    if parent_entropy == 0.0:\n        return (None, 0.0)\n    \n    max_gain = -float('inf')\n    best_threshold = None\n    \n    # Evaluate each candidate threshold\n    for t in candidate_thresholds:\n        split_idx = bisect.bisect_left(sorted_features, t)\n        left_targets = sorted_targets[:split_idx]\n        right_targets = sorted_targets[split_idx:]\n        \n        # Calculate left entropy\n        left_total = len(left_targets)\n        left_entropy = 0.0\n        if left_total > 0:\n            left_counts = Counter(left_targets)\n            for count in left_counts.values():\n                p = count / left_total\n                if p > 0:\n                    left_entropy -= p * math.log2(p)\n        \n        # Calculate right entropy\n        right_total = len(right_targets)\n        right_entropy = 0.0\n        if right_total > 0:\n            right_counts = Counter(right_targets)\n            for count in right_counts.values():\n                p = count / right_total\n                if p > 0:\n                    right_entropy -= p * math.log2(p)\n        \n        # Compute information gain\n        n = total_parent\n        weighted_avg = (left_total / n) * left_entropy + (right_total / n) * right_entropy\n        gain = parent_entropy - weighted_avg\n        \n        # Update best threshold based on maximum gain and smallest threshold in case of ties\n        if gain > max_gain or (gain == max_gain and t < best_threshold):\n            max_gain = gain\n            best_threshold = t\n    \n    # Check if maximum gain is positive\n    if max_gain <= 0.0:\n        return (None, 0.0)\n    else:\n        return (round(best_threshold, 4), round(max_gain, 4))\n```\n\n### Explanation\n1. **Sorting and Generating Thresholds**: The feature and target values are sorted together to maintain correspondence. Candidate thresholds are generated as midpoints between consecutive distinct values in the sorted feature list.\n2. **Entropy Calculation**: Entropy is computed using the Shannon entropy formula, which measures the impurity of the target labels. The entropy of the parent node (entire dataset) is calculated once, and then the entropies of the left and right subsets are computed for each threshold.\n3. **Information Gain**: For each threshold, the information gain is calculated by subtracting the weighted average entropy of the left and right subsets from the parent entropy. The threshold with the highest gain is selected, with ties resolved by choosing the smallest threshold.\n4. **Edge Cases**: The solution handles cases where all feature values are identical or no split provides a positive information gain by returning `(None, 0.0)`.\n\nThis approach efficiently evaluates all possible splits and selects the optimal threshold, ensuring the decision tree split maximizes the information gain."}
{"task_id": 274, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that clips each element of a given numeric vector (either a Python list or a one-dimensional NumPy array) to the interval \\([-5, 5]\\). The result should be returned as a Python list of floats, preserving the original order and dimensionality of the input.\n\n### Approach\nThe approach to solve this problem efficiently involves the following steps:\n1. **Convert Input to a NumPy Array**: This allows us to handle both Python lists and NumPy arrays uniformly. By converting the input to a NumPy array of floats, we ensure that all elements are treated as floating-point numbers, which is necessary for the output requirements.\n2. **Clip Values Using NumPy**: Utilize NumPy's `clip` function to efficiently limit each element to the interval \\([-5, 5]\\). This function is optimized for performance and handles large datasets efficiently.\n3. **Convert Back to Python List**: After clipping, convert the result back to a Python list of floats to meet the output requirements.\n\nThis approach leverages NumPy's optimized operations for handling numerical data, ensuring both correctness and efficiency.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef small_norm_clip(values):\n    \"\"\"Clip each element of *values* to the closed interval [-5, 5].\"\"\"\n    arr = np.array(values, dtype=float)\n    clipped = np.clip(arr, -5, 5)\n    return clipped.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input is converted to a NumPy array with dtype `float` to ensure all elements are treated as floating-point numbers. This handles both lists and arrays and ensures consistency in the output type.\n2. **Clipping**: The `np.clip` function is used to clamp all elements of the array within the specified range \\([-5, 5]\\). This function is efficient and works element-wise on the array.\n3. **Conversion to List**: The resulting clipped NumPy array is converted back to a Python list using the `tolist()` method, which ensures that the output is a list of floats as required.\n\nThis method is efficient and concise, leveraging NumPy's optimized operations for numerical computations while ensuring the result meets the specified format."}
{"task_id": 276, "completion_id": 0, "completion": "To solve this problem, we need to compute the log probability of a target sentence using an additive-smoothed N-gram language model trained on a given corpus. The solution involves several steps, including processing the training corpus, handling out-of-vocabulary tokens, and applying additive smoothing to compute probabilities.\n\n### Approach\n1. **Process the Training Corpus**:\n   - Convert each sentence to lowercase and split into tokens.\n   - Strip leading and trailing punctuation from each token.\n   - Pad each sentence with `<bol>` at the beginning and `<eol>` at the end.\n   - Collect all unique tokens from the processed corpus to form the vocabulary.\n\n2. **Handle Out-of-Vocabulary Tokens**:\n   - If the `unk` parameter is True, add a special `<unk>` token to the vocabulary.\n   - Replace any token in the target sentence that is not found in the training corpus with `<unk>`.\n\n3. **Build N-gram Counts**:\n   - Count occurrences of each N-gram (sequence of N words) in the training corpus.\n   - Track the counts of each history (prefix of N-1 words) to compute probabilities.\n\n4. **Process the Target Sentence**:\n   - Convert the target sentence to lowercase, split into tokens, and strip punctuation.\n   - Replace out-of-vocabulary tokens with `<unk>` if needed.\n   - Pad the target sentence with `<bol>` and `<eol>`.\n\n5. **Compute Probabilities**:\n   - For each N-gram in the processed target sentence, compute the smoothed probability using additive smoothing.\n   - Sum the logarithms (base 10) of these probabilities and round the result to four decimal places.\n\n### Solution Code\n```python\nimport math\nimport string\nfrom collections import Counter\n\ndef additive_ngram_log_prob(corpus: list[str], sentence: str, N: int, k: float = 1.0, unk: bool = True) -> float:\n    \"\"\"Return the base-10 log probability of *sentence* under an additive-smoothed N-gram model.\"\"\"\n    # Step 1: Process the corpus to build counts and collect training tokens\n    training_tokens = set()\n    counts = Counter()\n    h_counts = Counter()\n\n    for s in corpus:\n        # Split and process the sentence\n        tokens = s.lower().split()\n        stripped = [t.strip(string.punctuation) for t in tokens]\n        processed = ['<bol>'] + stripped + ['<eol>']\n        \n        # Add tokens to training_tokens\n        for token in processed:\n            training_tokens.add(token)\n        \n        # Generate N-grams and update counts\n        for i in range(len(processed) - (N - 1)):\n            ngram = processed[i:i+N]\n            h = tuple(ngram[:-1])\n            w = ngram[-1]\n            key = (h, w)\n            counts[key] += 1\n            h_counts[h] += 1\n\n    # Step 2: Compute vocabulary size\n    if unk:\n        vocab = training_tokens.union({'<unk>'})\n    else:\n        vocab = training_tokens\n    vocab_size = len(vocab)\n\n    # Step 3: Process the target sentence\n    # Split and process the target\n    target_tokens = sentence.lower().split()\n    stripped_target = [t.strip(string.punctuation) for t in target_tokens]\n    \n    # Replace OOV tokens with <unk> if needed\n    if unk:\n        processed_target = ['<unk>' if t not in training_tokens else t for t in stripped_target]\n    else:\n        processed_target = stripped_target\n    \n    # Pad the target with <bol> and <eol>\n    padded_target = ['<bol>'] + processed_target + ['<eol>']\n    \n    # Step 4: Generate target N-grams\n    target_ngrams = []\n    for i in range(len(padded_target) - (N - 1)):\n        ngram = padded_target[i:i+N]\n        h = tuple(ngram[:-1])\n        w = ngram[-1]\n        target_ngrams.append((h, w))\n\n    # Step 5: Compute log probabilities\n    total_log = 0.0\n    for h, w in target_ngrams:\n        count_hw = counts.get((h, w), 0)\n        count_h = h_counts.get(h, 0)\n        numerator = count_hw + k\n        denominator = count_h + k * vocab_size\n        if denominator <= 0:\n            prob = 0.0\n        else:\n            prob = numerator / denominator\n        total_log += math.log10(prob) if prob > 0 else float('-inf')\n    \n    # Handle cases where total_log is -inf (probability zero)\n    if total_log == float('-inf'):\n        return round(float('-inf'), 4)\n    return round(total_log, 4)\n```\n\n### Explanation\n- **Training Corpus Processing**: Each sentence is converted to lowercase, split into tokens, and stripped of leading/trailing punctuation. The processed sentences are padded with `<bol>` and `<eol>`.\n- **Vocabulary Construction**: Tokens from the training corpus form the initial vocabulary. If `unk` is True, `<unk>` is added to handle unseen tokens.\n- **N-gram Counts**: We count each N-gram (sequence of N words) and their prefixes (history) in the training corpus.\n- **Target Sentence Processing**: The target sentence is processed similarly to the training corpus, with out-of-vocabulary tokens replaced by `<unk>` if needed.\n- **Probability Calculation**: For each N-gram in the target sentence, we compute the smoothed probability using additive smoothing, sum the log probabilities, and round the result to four decimal places.\n\nThis approach ensures that we handle unseen tokens and apply smoothing correctly to compute the log probability efficiently."}
{"task_id": 277, "completion_id": 0, "completion": "To solve this problem, we need to determine if the action and observation spaces in a given reinforcement learning environment are composite. A composite space is defined as a built-in Python `tuple` or `dict`. The solution involves checking the type of these spaces and returning the results as a tuple of booleans.\n\n### Approach\n1. **Extract Spaces**: Retrieve the `action_space` and `observation_space` from the given environment dictionary.\n2. **Check Composite Type**: Use the `isinstance` function to check if each space is an instance of either `tuple` or `dict`. This function returns `True` if the object is an instance of the specified class or any of its subclasses.\n3. **Return Results**: Return a tuple containing two booleans indicating whether each space (action and observation) is composite.\n\n### Solution Code\n```python\ndef is_tuple(env: dict) -> tuple:\n    \"\"\"Determine whether the *action* and *observation* spaces contained in\n    ``env`` are composite (tuple or dictionary).\n\n    A *composite* space is defined as a built-in ``tuple`` or ``dict``.\n\n    Args:\n        env (dict): A dictionary that **must** contain the keys\n            ``\"action_space\"`` and ``\"observation_space\"``.\n\n    Returns:\n        tuple: Two booleans ``(tuple_action, tuple_obs)`` indicating whether\n            each space is composite.\n    \"\"\"\n    tuple_action = isinstance(env[\"action_space\"], (tuple, dict))\n    tuple_obs = isinstance(env[\"observation_space\"], (tuple, dict))\n    return (tuple_action, tuple_obs)\n```\n\n### Explanation\n- **Extract Spaces**: The environment dictionary is guaranteed to have the keys `\"action_space\"` and `\"observation_space\"`, which we directly access.\n- **Check Composite Type**: Using `isinstance(space, (tuple, dict))`, we check if each space is an instance of either `tuple` or `dict`. This check is inclusive of subclasses, meaning any subclass of `tuple` or `dict` will also be considered composite.\n- **Return Results**: The results for the action and observation spaces are returned as a tuple of booleans. The first boolean indicates if the action space is composite, and the second indicates if the observation space is composite.\n\nThis approach efficiently checks the composite nature of the spaces using Python's built-in type checking, ensuring correctness and adherence to the problem requirements."}
{"task_id": 278, "completion_id": 0, "completion": "To solve this problem, we need to traverse a binary decision tree represented by nested dictionaries to predict the class label or regression value for a given input sample. The decision tree consists of internal nodes that direct the traversal based on feature thresholds and leaf nodes that provide the final prediction.\n\n### Approach\nThe approach involves starting at the root of the decision tree and iteratively navigating through the tree based on the feature values and thresholds of the internal nodes. Each internal node specifies a feature index and a threshold value. The input sample's feature value at the specified index is compared against the threshold to determine the next node to visit. If the feature value is less than or equal to the threshold, the traversal follows the true branch; otherwise, it follows the false branch. This process continues until a leaf node is reached, which contains the final prediction value.\n\n### Solution Code\n```python\ndef evaluate_decision_tree(tree: dict, sample: list[float]):\n    \"\"\"Traverse a binary decision-tree represented by nested dictionaries.\n\n    Args:\n        tree (dict): Root node of the decision tree. Internal nodes contain\n            'feature_i', 'threshold', 'true_branch', 'false_branch'. Leaf\n            nodes contain only 'value'.\n        sample (list[float]): Feature vector that will be classified/regressed.\n\n    Returns:\n        The value stored in the reached leaf (int, float, or str).\n    \"\"\"\n    current_node = tree\n    while True:\n        if 'value' in current_node:\n            return current_node['value']\n        feature_i = current_node['feature_i']\n        threshold = current_node['threshold']\n        if sample[feature_i] <= threshold:\n            current_node = current_node['true_branch']\n        else:\n            current_node = current_node['false_branch']\n```\n\n### Explanation\n1. **Initialization**: Start at the root node of the decision tree.\n2. **Leaf Node Check**: At each step, check if the current node is a leaf node by verifying the presence of the 'value' key. If it is a leaf node, return the value.\n3. **Internal Node Processing**: If the current node is an internal node, retrieve the feature index and threshold.\n4. **Feature Comparison**: Compare the input sample's feature value at the specified index with the threshold. Based on the comparison, move to the true branch or false branch.\n5. **Iteration**: Repeat the process until a leaf node is reached, ensuring that the traversal follows the correct path through the tree based on the sample's feature values.\n\nThis approach efficiently navigates the decision tree structure, ensuring that each decision at an internal node correctly guides the traversal towards the appropriate leaf node, ultimately providing the prediction for the input sample."}
{"task_id": 279, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that calculates the Mean Squared Error (MSE) loss, its gradient with respect to predictions, and the classification accuracy using a 0.5 threshold. The solution involves converting inputs to numpy arrays for efficient computation, performing necessary mathematical operations, and rounding results to four decimal places.\n\n### Approach\n1. **Convert Inputs**: Convert the input lists or numpy arrays into numpy arrays to leverage vectorized operations.\n2. **Compute MSE**: Calculate the MSE by averaging the squared differences between the true labels and predictions.\n3. **Compute Gradient**: Derive the gradient using the formula \\( \\frac{2}{N} \\times (y\\_pred - y\\_true) \\), where \\( N \\) is the number of samples.\n4. **Compute Accuracy**: Convert predictions to binary classes (0 or 1) using a 0.5 threshold and compare with true labels to determine accuracy.\n5. **Round Results**: Ensure all results are rounded to four decimal places for consistency.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mse_loss(y_true, y_pred):\n    \"\"\"Mean Squared Error (MSE) loss, its gradient and accuracy.\"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    N = len(y_true)\n    \n    # Compute MSE\n    mse = np.mean((y_pred - y_true) ** 2)\n    mse_rounded = round(mse, 4)\n    \n    # Compute gradient\n    gradient = (2.0 / N) * (y_pred - y_true)\n    gradient_list = [round(g, 4) for g in gradient.tolist()]\n    \n    # Compute accuracy\n    pred_classes = (y_pred >= 0.5).astype(int)\n    accuracy = np.mean(pred_classes == y_true)\n    accuracy_rounded = round(accuracy, 4)\n    \n    return (mse_rounded, gradient_list, accuracy_rounded)\n```\n\n### Explanation\n1. **Input Conversion**: The inputs `y_true` and `y_pred` are converted to numpy arrays to facilitate element-wise operations.\n2. **MSE Calculation**: The MSE is computed by taking the mean of the squared differences between the predicted and true values. This gives a measure of how well the predictions match the actual values.\n3. **Gradient Calculation**: The gradient of the MSE with respect to the predictions is derived using the formula \\( \\frac{2}{N} \\times (y\\_pred - y\\_true) \\). Each element in the resulting array is then rounded and converted to a list.\n4. **Accuracy Calculation**: Predictions are converted to binary classes (0 or 1) using a 0.5 threshold. The accuracy is the proportion of correct predictions compared to the true labels.\n5. **Rounding**: All results are rounded to four decimal places to meet the specified precision requirements.\n\nThis approach efficiently computes the required metrics using vectorized operations for performance and clarity, ensuring accurate and precise results."}
{"task_id": 280, "completion_id": 0, "completion": "To solve this problem, we need to compute the L2 regularization term and its gradient for a given weight vector or matrix. The L2 regularization term is used to penalize large model parameters in machine learning algorithms to prevent overfitting. The task involves calculating either the regularization value or its gradient based on a flag parameter.\n\n### Approach\n1. **Regularization Value Calculation**:\n   - The L2 regularization value is given by the formula \\(0.5 \\cdot \\text{alpha} \\cdot \\sum w_i^2\\), where the summation runs over all elements of the weight matrix or vector. This is computed by squaring each element of the input weight array, summing these squared values, multiplying by 0.5 and the regularization coefficient (alpha), and then rounding the result to four decimal places.\n\n2. **Gradient Calculation**:\n   - The gradient of the L2 regularization term with respect to the weights is simply the product of the regularization coefficient (alpha) and the weight matrix or vector. Each element of the resulting gradient matrix is then rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l2_regularization(w: np.ndarray, alpha: float, return_grad: bool = False):\n    \"\"\"Compute the L2 regularization value or its gradient.\"\"\"\n    if return_grad:\n        grad = alpha * w\n        rounded_grad = np.round(grad, 4)\n        return rounded_grad\n    else:\n        sum_sq = np.sum(w ** 2)\n        value = 0.5 * alpha * sum_sq\n        rounded_value = round(float(value), 4)\n        return rounded_value\n```\n\n### Explanation\n- **Regularization Value**:\n  - When `return_grad` is `False`, the function computes the sum of the squares of all elements in the weight matrix, multiplies this sum by 0.5 and alpha, converts the result to a Python float, and rounds it to four decimal places. This gives the scalar regularization value.\n\n- **Gradient Calculation**:\n  - When `return_grad` is `True`, the function computes the gradient by multiplying each element of the weight matrix by alpha. The resulting gradient matrix is then rounded to four decimal places and returned as a NumPy ndarray, preserving the original shape of the input weight matrix.\n\nThis approach ensures that both the regularization value and its gradient are efficiently computed and correctly formatted according to the problem requirements, handling any shape of input weight matrix and correctly processing the case when alpha is zero."}
{"task_id": 281, "completion_id": 0, "completion": "To solve this problem, we need to implement L1 Regularization (Lasso Penalty) which is used to combat over-fitting in linear models. The solution involves creating a class that can compute the L1 penalty value and its gradient for a given weight vector. The regularization strength is controlled by a parameter alpha, which must be non-negative.\n\n### Approach\n1. **Class Definition**: We define a class `L1Regularizer` within a factory function `L1Regularization`. This class will handle the computation of the L1 penalty and its gradient.\n2. **Initialization**: The class constructor takes a non-negative float `alpha` which controls the regularization strength. If `alpha` is negative, a `ValueError` is raised.\n3. **Callable Method**: The `__call__` method computes the L1 penalty value by summing the absolute values of the weight vector elements and scaling by `alpha`.\n4. **Gradient Method**: The `grad` method computes the gradient of the L1 penalty using the sign of the weight vector elements, scaled by `alpha`. The sign function in NumPy naturally handles zero by returning zero for such elements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef L1Regularization(alpha: float):\n    \"\"\"Create a callable object that implements L1 regularisation.\n\n    The returned object must support two operations:\n    1. call with a NumPy array to obtain the L1 penalty value\n    2. call its .grad(w) method to obtain the gradient of the penalty\n\n    Args:\n        alpha: Non-negative float controlling the strength of the regularisation.\n\n    Returns:\n        An instance that fulfils the described interface.\n    \"\"\"\n    if alpha < 0:\n        raise ValueError(\"alpha must be non-negative\")\n    \n    class L1Regularizer:\n        def __init__(self, alpha: float):\n            self.alpha = alpha\n        \n        def __call__(self, w: np.ndarray) -> float:\n            return float(self.alpha * np.sum(np.abs(w)))\n        \n        def grad(self, w: np.ndarray) -> np.ndarray:\n            return self.alpha * np.sign(w)\n    \n    return L1Regularizer(alpha)\n```\n\n### Explanation\n1. **Initialization Check**: The factory function `L1Regularization` first checks if the provided `alpha` is non-negative. If not, it raises a `ValueError`.\n2. **L1Regularizer Class**: This class is defined within the factory function to encapsulate the functionality. It initializes with the given `alpha`.\n3. **__call__ Method**: This method computes the L1 penalty by taking the sum of absolute values of all elements in the input array `w` and scaling it by `alpha`. The result is returned as a Python float.\n4. **grad Method**: This method computes the gradient of the L1 penalty by applying the sign function to each element of `w` and scaling the result by `alpha`. The sign function ensures that zeros in `w` result in zeros in the gradient, which is handled correctly by NumPy.\n\nThis approach ensures that the L1 regularization is efficiently computed for any shape of NumPy array, and the gradient is correctly calculated to maintain the same shape as the input array. The solution handles edge cases such as `alpha = 0` gracefully, resulting in zero penalty and gradient values."}
{"task_id": 282, "completion_id": 0, "completion": "To solve this problem, we need to compute the running mean, variance, and standard deviation of a stream of data using an online algorithm. The solution must process each data point exactly once, making it efficient and suitable for real-time applications. The Welford algorithm is used here to ensure numerical stability and accuracy without requiring the entire dataset to be stored in memory.\n\n### Approach\n1. **Initialization**: Start with counters for the number of data points (n), the current mean (m), and the sum of squared differences (s) initialized to zero.\n2. **Iterate through the data**: For each new data point, update the counters and statistics using the Welford algorithm:\n   - Increment the count of data points (n).\n   - Calculate the difference (delta) between the new data point and the current mean.\n   - Update the mean (m) using the delta and the new count.\n   - Update the sum of squared differences (s) using the delta and the new mean.\n3. **Final Computations**: After processing all data points, compute the sample variance and standard deviation. The variance is adjusted for the sample by dividing by (n-1) when there are more than one data points. The standard deviation is the square root of the variance.\n4. **Edge Cases**: Handle empty input by returning zeros for all statistics. Ensure numerical stability by avoiding division by zero and correctly handling cases with insufficient data points.\n\n### Solution Code\n```python\nimport math\n\ndef running_stats(data: list[int | float]) -> tuple[float, float, float]:\n    \"\"\"Return the running mean, variance and standard deviation of a stream.\n\n    The computation must be *online*: iterate over the data once while\n    keeping only a constant amount of memory (do not call numpy.mean,\n    numpy.var, statistics.mean, etc.).  Implement Welford's method as\n    explained in the task description.  All three returned numbers must\n    be rounded to 4 decimal places.  If the input list is empty return\n    (0.0, 0.0, 0.0).\n    \"\"\"\n    if not data:\n        return (0.0, 0.0, 0.0)\n    \n    n = 0\n    m = 0.0\n    s = 0.0\n    \n    for x in data:\n        n += 1\n        delta = x - m\n        m += delta / n\n        s += delta * (x - m)\n    \n    if n <= 1:\n        variance = 0.0\n    else:\n        variance = s / (n - 1)\n    \n    std_dev = math.sqrt(variance)\n    \n    mean_rounded = round(m, 4)\n    var_rounded = round(variance, 4)\n    std_rounded = round(std_dev, 4)\n    \n    return (mean_rounded, var_rounded, std_rounded)\n```\n\n### Explanation\n- **Initialization**: The variables `n`, `m`, and `s` are initialized to zero to start accumulating statistics.\n- **Processing Each Data Point**: For each new value, the algorithm updates the mean incrementally. The difference (delta) between the new value and the current mean is used to adjust the mean and update the sum of squared differences (s), ensuring numerical stability.\n- **Final Computations**: After processing all data points, the sample variance is computed by dividing the sum of squared differences by (n-1) for n > 1. The standard deviation is derived as the square root of the variance.\n- **Rounding**: Each of the computed statistics (mean, variance, standard deviation) is rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently processes data in a single pass, making it suitable for large datasets or real-time streams, while maintaining numerical accuracy and stability."}
{"task_id": 283, "completion_id": 0, "completion": "To solve this problem, we need to compute the approximate Newton leaf value and split gain for binary cross-entropy (logistic) loss in gradient boosting algorithms. These metrics are essential for constructing decision trees in gradient boosting, where the leaf values and split gains are derived using the gradients and Hessians of the loss function.\n\n### Approach\n1. **Compute Probabilities**: Convert the raw prediction scores into probabilities using the sigmoid function. The sigmoid function maps any real-valued number into the range [0, 1], which can be interpreted as a probability.\n2. **Calculate Gradients (g_i)**: The gradient for each sample is the difference between the computed probability and the actual label.\n3. **Calculate Hessians (h_i)**: The Hessian for each sample is the product of the probability and its complement (1 - probability).\n4. **Sum Gradients and Hessians**: Sum all the gradients and Hessians across the dataset.\n5. **Compute Leaf Value and Gain**: Using the sums of gradients and Hessians along with the regularization term, compute the leaf value and gain. The leaf value is the sum of gradients divided by the sum of Hessians plus regularization. The gain is half the square of the sum of gradients divided by the same denominator.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef binary_log_loss_metrics(actual, predicted, regularization=1.0):\n    \"\"\"Compute the approximate Newton leaf value and split gain for binary\n    cross-entropy (logistic) loss.\n\n    Parameters\n    ----------\n    actual : np.ndarray\n        1-D array with true labels (0 or 1).\n    predicted : np.ndarray\n        1-D array with raw prediction scores.\n    regularization : float, default 1.0\n        L2 regularisation term \u03bb used in gradient boosting formulas.\n\n    Returns\n    -------\n    list[float]\n        A two-element list `[leaf_value, gain]`, each rounded to 4 decimals.\n    \"\"\"\n    p = 1.0 / (1.0 + np.exp(-predicted))\n    g = p - actual\n    sum_g = np.sum(g)\n    sum_h = np.sum(p * (1 - p))\n    denominator = sum_h + regularization\n    leaf_value = sum_g / denominator\n    gain = 0.5 * (sum_g ** 2) / denominator\n    return [round(leaf_value, 4), round(gain, 4)]\n```\n\n### Explanation\n1. **Sigmoid Function**: The sigmoid function converts raw scores into probabilities, which are necessary for both gradient and Hessian calculations.\n2. **Gradients Calculation**: The gradient for each sample is the difference between the predicted probability and the actual label. This measures how far off our prediction is from the true value.\n3. **Hessians Calculation**: The Hessian for each sample is the product of the probability and its complement, providing a measure of the curvature of the loss function.\n4. **Summing Gradients and Hessians**: Summing these values across all samples aggregates the information needed for the leaf value and gain calculations.\n5. **Leaf Value and Gain**: These are computed using the aggregated gradients and Hessians, adjusted by the regularization term to prevent overfitting. The leaf value is a weighted average of the gradients, and the gain quantifies the improvement in the loss function from splitting the data at this node.\n\nThis approach efficiently computes the necessary metrics using vectorized operations for performance and clarity, ensuring the solution is both correct and optimized."}
{"task_id": 284, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that maps arbitrary hashable objects to compact, consecutive integer indices using a fixed-capacity hash table. The function should handle both the cases where the table is not full and where it is full, allowing collisions once the table is full. Additionally, the function should respect a readonly flag to prevent modifications to the table or collision counter.\n\n### Approach\n1. **Check Existing Entry**: If the object is already present in the table, return its stored index immediately.\n2. **Readonly Handling**: If the readonly flag is set and the object is not in the table, return `None` to prevent any modifications.\n3. **Table Full Handling**: If the table is full, compute the index using the hash of the object modulo the table size, increment the collision counter, and return the computed index.\n4. **Insert New Entry**: If the table is not full, insert the object with the next consecutive index (equal to the current size of the table) and return this index.\n\n### Solution Code\n```python\ndef iht_get_index(\n    obj: object,\n    size: int,\n    table: dict,\n    overfull_count: list[int],\n    readonly: bool = False,\n) -> int | None:\n    \"\"\"Map *obj* to a stable integer index using a bounded index hash table.\n\n    Args:\n        obj:               Hashable object that should receive an index.\n        size:              Maximum number of *stored* entries.\n        table:             Mutable dictionary holding the current mapping.\n        overfull_count:    One-element list whose integer counts collisions.\n        readonly:          If True, never modify *table* or *overfull_count*.\n\n    Returns:\n        The integer index for *obj*, or None when *readonly* is True and the\n        object is not yet in *table*.\n    \"\"\"\n    if obj in table:\n        return table[obj]\n    if readonly:\n        return None\n    if len(table) >= size:\n        index = hash(obj) % size\n        overfull_count[0] += 1\n        return index\n    else:\n        index = len(table)\n        table[obj] = index\n        return index\n```\n\n### Explanation\n- **Existing Entry Check**: The function first checks if the object is already present in the table. If it is, the stored index is returned immediately, ensuring quick access for previously stored objects.\n- **Readonly Handling**: If the readonly flag is set to `True`, the function returns `None` for any object not already in the table, preventing any modifications to the table or collision counter.\n- **Table Full Handling**: When the table is full, the function computes the index using the hash of the object modulo the table size. Each such computation increments the collision counter to track the number of collisions.\n- **Insert New Entry**: If the table is not full, the object is inserted with the next consecutive index (equal to the current size of the table), ensuring indices are assigned in a consecutive manner starting from 0.\n\nThis approach efficiently handles both insertion and collision scenarios while respecting the readonly constraint, ensuring optimal performance and correctness as per the problem requirements."}
{"task_id": 285, "completion_id": 0, "completion": "To solve this problem, we need to compute the linear kernel (Gram matrix) between two datasets represented as NumPy arrays. The linear kernel is defined as the matrix product of the first dataset and the transpose of the second dataset. The function must handle both one-dimensional and two-dimensional input arrays and return the result as a nested Python list, or return -1 if the feature dimensions of the two datasets do not match.\n\n### Approach\n1. **Reshape Inputs**: Convert any one-dimensional input arrays into two-dimensional row vectors. This ensures that both inputs are in a consistent format for matrix operations.\n2. **Check Feature Dimensions**: Verify that the number of features (columns) in both datasets is the same. If not, return -1.\n3. **Compute the Kernel Matrix**: Perform the matrix multiplication of the reshaped first dataset with the transpose of the reshaped second dataset.\n4. **Convert to List**: Convert the resulting kernel matrix from a NumPy array to a nested Python list using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_kernel(x: np.ndarray, y: np.ndarray) -> list | int:\n    \"\"\"Compute the linear kernel (Gram matrix) between two data sets.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        First input data. Can be 1-D (single sample) or 2-D (multiple samples).\n    y : np.ndarray\n        Second input data. Can be 1-D (single sample) or 2-D (multiple samples).\n\n    Returns\n    -------\n    list | int\n        The linear kernel matrix as a nested Python list. If *x* and *y* have\n        different feature dimensions, returns -1.\n    \"\"\"\n    # Reshape to 2D if inputs are 1D\n    x_2d = x.reshape(1, -1) if x.ndim == 1 else x\n    y_2d = y.reshape(1, -1) if y.ndim == 1 else y\n    \n    # Check feature dimensions compatibility\n    if x_2d.shape[1] != y_2d.shape[1]:\n        return -1\n    \n    # Compute the linear kernel matrix\n    kernel_matrix = x_2d @ y_2d.T\n    \n    return kernel_matrix.tolist()\n```\n\n### Explanation\n1. **Reshape Inputs**: The function first ensures that both input arrays are two-dimensional. If an input array is one-dimensional, it is reshaped into a row vector (1 row, n columns).\n2. **Feature Dimension Check**: The function checks if the number of columns (features) in both reshaped arrays are the same. If they differ, the function returns -1 to indicate incompatibility.\n3. **Matrix Multiplication**: The kernel matrix is computed using matrix multiplication of the reshaped first array with the transpose of the reshaped second array. This operation effectively computes the dot product of each pair of samples from the two datasets.\n4. **Conversion to List**: The resulting NumPy array is converted to a nested Python list using the `tolist()` method to meet the output requirements.\n\nThis approach efficiently handles both one-dimensional and two-dimensional inputs, ensures compatibility of feature dimensions, and returns the result in the required format."}
{"task_id": 286, "completion_id": 0, "completion": "To solve this problem, we need to create a utility function that retrieves a weight initializer function based on a given name. The utility should check if the name corresponds to a callable function in the global namespace and return it, or raise an error if it does not exist or is not callable.\n\n### Approach\n1. **Retrieve Global Symbols**: Use Python's `globals()` function to get all global symbols in the current module.\n2. **Check Existence and Callability**: Verify if the given name exists in the global symbols and if the corresponding object is callable.\n3. **Return or Raise Error**: Return the callable if valid; otherwise, raise a `ValueError` with a specific message.\n\n### Solution Code\n```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[1.0 for _ in range(cols)] for _ in range(rows)]\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    random.seed(seed)\n    rows, cols = shape\n    return [[random.uniform(low, high) for _ in range(cols)] for _ in range(rows)]\n\ndef get_initializer(name: str):\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    global_symbols = globals()\n    initializer = global_symbols.get(name)\n    if initializer is not None and callable(initializer):\n        return initializer\n    raise ValueError(\"Invalid initialization function.\")\n```\n\n### Explanation\n- **zeros_init**: This function takes a shape tuple and returns a matrix filled with zeros.\n- **ones_init**: Similar to `zeros_init`, but fills the matrix with ones.\n- **random_uniform_init**: Generates a matrix with uniformly distributed random numbers within a specified range, using a seed for reproducibility.\n- **get_initializer**: This utility function checks the global namespace for a callable object with the given name. If found, it returns the callable; otherwise, it raises a `ValueError`.\n\nThis approach ensures that we dynamically retrieve the correct initializer function based on the provided name, adhering to the specified requirements and handling invalid cases appropriately."}
{"task_id": 287, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    s_h, s_w = stride\n    \n    # Compute output dimensions\n    H_out = (H - p_h) // s_h + 1\n    W_out = (W - p_w) // s_w + 1\n    \n    # Forward pass: create windowed view and compute mean\n    X_strides = X.strides\n    new_strides = (X_strides[0], X_strides[1], X_strides[2] * s_h, X_strides[3] * s_w, X_strides[2], X_strides[3])\n    X_windows = as_strided(X, shape=(N, C, H_out, W_out, p_h, p_w), strides=new_strides)\n    pooled_output = np.mean(X_windows, axis=(-2, -1))\n    \n    # Backward pass: distribute gradients to input windows\n    grad_input = np.zeros_like(X)\n    grad_windows = as_strided(grad_input, shape=(N, C, H_out, W_out, p_h, p_w), strides=new_strides)\n    scaled_grad = accum_grad[..., np.newaxis, np.newaxis] / (p_h * p_w)\n    np.add(grad_windows, scaled_grad, out=grad_windows)\n    \n    # Round and convert to lists\n    pooled_output_rounded = np.round(pooled_output, 4)\n    grad_input_rounded = np.round(grad_input, 4)\n    \n    return (pooled_output_rounded.tolist(), grad_input_rounded.tolist())\n```"}
{"task_id": 288, "completion_id": 0, "completion": "To solve this problem, we need to compute the Mean Squared Error (MSE) between two numeric sequences or the squared error between two individual numbers. The function should handle both scalar values (integers or floats) and one-dimensional iterables (lists, tuples, or NumPy 1-D arrays). If the inputs are invalid (e.g., different lengths, empty iterables, non-numeric elements), the function should return -1.\n\n### Approach\n1. **Check Scalar Inputs**: First, determine if both inputs are scalars (either Python integers/floats or NumPy scalars). If so, compute the squared difference between them.\n2. **Handle Mixed Scalar and Iterable Inputs**: If one input is a scalar and the other is an iterable, return -1 as they are incompatible.\n3. **Validate Iterable Inputs**: For iterables, check if they are valid one-dimensional sequences (lists, tuples, or NumPy 1-D arrays). If not, return -1.\n4. **Check Lengths**: Ensure both iterables have the same non-zero length. If not, return -1.\n5. **Compute MSE**: Convert the valid iterables to NumPy arrays, compute the mean of the squared differences between corresponding elements, and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef squared_error(actual, predicted):\n    \"\"\"Calculate the squared error or mean squared error.\n\n    If *actual* and *predicted* are scalars, return the squared difference.\n    If they are one-dimensional iterables (same length), return the mean squared\n    error.  Return -1 for length mismatches or empty iterables.\n\n    All results must be rounded to 4 decimal places.\n    \"\"\"\n    # Check if both inputs are scalars (including numpy scalars)\n    if np.isscalar(actual) and np.isscalar(predicted):\n        se = (actual - predicted) ** 2\n        return round(float(se), 4)\n    \n    # Check if one is scalar and the other is not\n    if np.isscalar(actual) or np.isscalar(predicted):\n        return -1\n    \n    # Check if they are valid numpy arrays (1-D if they are arrays)\n    if isinstance(actual, np.ndarray) and actual.ndim != 1:\n        return -1\n    if isinstance(predicted, np.ndarray) and predicted.ndim != 1:\n        return -1\n    \n    # Check if they are iterables and get their lengths\n    try:\n        len_actual = len(actual)\n        len_predicted = len(predicted)\n    except TypeError:\n        # Not iterable\n        return -1\n    \n    # Check for empty or length mismatch\n    if len_actual == 0 or len_predicted == 0 or len_actual != len_predicted:\n        return -1\n    \n    # Convert to numpy arrays and compute MSE\n    try:\n        a = np.asarray(actual, dtype=np.float64)\n        p = np.asarray(predicted, dtype=np.float64)\n    except (ValueError, TypeError):\n        # Non-numeric elements encountered\n        return -1\n    \n    mse = np.mean((a - p) ** 2)\n    return round(float(mse), 4)\n```\n\n### Explanation\n1. **Scalar Check**: The function first checks if both inputs are scalars using `np.isscalar`, which handles both Python and NumPy scalars. If true, it computes the squared difference.\n2. **Mixed Input Handling**: If one input is a scalar and the other is not, the function returns -1 immediately.\n3. **Iterable Validation**: For iterables, the function checks if they are valid one-dimensional NumPy arrays. If not, it returns -1.\n4. **Length Check**: The function ensures both iterables have the same non-zero length. If not, it returns -1.\n5. **MSE Calculation**: The iterables are converted to NumPy arrays, and the mean of the squared differences is computed. The result is rounded to four decimal places and returned.\n\nThis approach ensures that all edge cases are handled, including invalid inputs, and efficiently computes the required error metric using NumPy for vectorized operations."}
{"task_id": 289, "completion_id": 0, "completion": "To solve this problem, we need to format a list of field entries into a specific string format. Each entry consists of a field name and a value, which should be formatted as `field: \"value\"` and joined with newline characters. If the list is empty, we return an empty string.\n\n### Approach\n1. **Check for Empty List**: If the input list is empty, return an empty string immediately.\n2. **Format Each Entry**: For each tuple in the list, convert the field name and value into the required format `field: \"value\"`. The value is converted to its string representation to handle any non-string types.\n3. **Join Entries**: Combine all formatted entries into a single string with each entry separated by a newline character, ensuring there is no trailing newline at the end.\n\n### Solution Code\n```python\ndef format_entries(entries: list[tuple[str, str]]) -> str:\n    \"\"\"Formats a list of (field, value) tuples.\n\n    Each tuple is converted into a line of the form:\n        field: \"value\"\n    and all lines are joined together by a single newline character. No extra\n    newline is added at the end.\n\n    Args:\n        entries: List of tuples where the first element is the field name and\n                 the second element is the corresponding value. Both should be\n                 strings; however, any value will be converted to its string\n                 representation.\n\n    Returns:\n        A single string containing all formatted lines separated by a newline.\n        Returns an empty string if *entries* is empty.\n    \"\"\"\n    lines = [f'{field}: \"{str(value)}\"' for field, value in entries]\n    return '\\n'.join(lines)\n```\n\n### Explanation\n- **Handling Empty List**: The solution handles an empty list by creating an empty list of lines, which when joined, results in an empty string.\n- **String Conversion**: Each value is explicitly converted to a string using `str(value)` to ensure non-string values are properly formatted.\n- **List Comprehension**: The list comprehension efficiently processes each tuple to generate the formatted strings.\n- **Joining Lines**: The formatted strings are joined using `'\\n'.join(lines)`, which ensures each entry is on a new line without an extra trailing newline.\n\nThis approach ensures that the solution is efficient, concise, and meets all specified requirements."}
{"task_id": 290, "completion_id": 0, "completion": "To solve this problem, we need to determine if two binary decision trees are equivalent based on their structure and values. The equivalence criteria include having the same shape, matching feature indices at internal nodes, and thresholds and leaf values being numerically close within a specified tolerance.\n\n### Approach\nThe solution involves a recursive comparison of the two trees. The key steps are as follows:\n1. **Check Node Types**: If one node is a `Leaf` and the other is a `Node`, they are not equivalent.\n2. **Leaf Comparison**: If both nodes are `Leaf` nodes, compare their values using `numpy.allclose` with an absolute tolerance of 1e-8.\n3. **Node Comparison**: If both nodes are `Node` types, check if their feature indices and thresholds match. The thresholds must be within the specified tolerance using `numpy.isclose`.\n4. **Recursive Check**: Recursively compare the left and right children of the nodes to ensure the entire subtree structure and values match.\n\n### Solution Code\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    value: object\n\n@dataclass\nclass Node:\n    feature: int\n    threshold: float\n    left: object\n    right: object\n\ndef compare_trees(tree_a, tree_b):\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-8, rtol=0.0)\n    elif isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8, rtol=0.0):\n            return False\n        return compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    else:\n        return False\n```\n\n### Explanation\n- **Node Type Check**: The function first checks if both nodes are either `Leaf` or `Node` instances. If there is a mismatch in types, the trees are not equivalent.\n- **Leaf Value Comparison**: For `Leaf` nodes, their values are compared using `numpy.allclose`, which handles various data types (int, float, list, numpy array) and ensures numerical closeness within the specified tolerance.\n- **Node Feature and Threshold Check**: For `Node` instances, the feature indices must be identical, and thresholds must be within 1e-8 absolute tolerance using `numpy.isclose`.\n- **Recursive Subtree Check**: The function recursively checks the left and right subtrees to ensure structural and value equivalence throughout the entire tree.\n\nThis approach ensures that both the structure and values of the trees are thoroughly checked, adhering to the problem constraints and requirements."}
{"task_id": 291, "completion_id": 0, "completion": "To solve this problem, we need to implement Principal Component Analysis (PCA) to project a data matrix onto its first `n` principal components. The steps involve computing the covariance matrix, finding eigenvalues and eigenvectors, sorting them, selecting the top `n` components, adjusting their signs, and projecting the data.\n\n### Approach\n1. **Input Validation**: Check if the number of components `n_components` is within the valid range [1, d], where `d` is the number of features in the input data matrix `X`.\n2. **Covariance Matrix Computation**: Compute the sample covariance matrix using `np.cov` with `rowvar=False` to handle features as columns.\n3. **Eigenvalue and Eigenvector Calculation**: Use `np.linalg.eig` to find eigenvalues and eigenvectors of the covariance matrix, ensuring they are real numbers.\n4. **Sorting Eigenpairs**: Sort eigenvalues in descending order and reorder the corresponding eigenvectors.\n5. **Selecting Components**: Choose the first `n` eigenvectors (principal components) and adjust their signs such that the first element is non-negative.\n6. **Projection**: Project the original data matrix onto the selected components, round the result to 4 decimal places, and convert it to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca_transform(X: np.ndarray, n_components: int) -> list[list[float]]:\n    m, d = X.shape\n    if n_components < 1 or n_components > d:\n        return -1\n    \n    # Compute covariance matrix\n    sigma = np.cov(X, rowvar=False)\n    \n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eig(sigma)\n    eigenvalues = np.real(eigenvalues)\n    eigenvectors = np.real(eigenvectors)\n    \n    # Sort eigenvectors by descending eigenvalues\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Select first n_components eigenvectors\n    selected = sorted_eigenvectors[:, :n_components]\n    \n    # Adjust sign based on the first element\n    for i in range(selected.shape[1]):\n        if selected[0, i] < 0:\n            selected[:, i] *= -1\n    \n    # Project data and round\n    transformed = X @ selected\n    transformed_rounded = np.round(transformed, 4)\n    \n    return transformed_rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: Ensures that the number of components requested is within the valid range, returning -1 if not.\n2. **Covariance Matrix**: Computed using `np.cov` to get the covariance matrix of the input data, which inherently centers the data.\n3. **Eigen Decomposition**: Calculates eigenvalues and eigenvectors of the covariance matrix, converting them to real numbers to handle any numerical issues.\n4. **Sorting**: Eigenvalues and corresponding eigenvectors are sorted in descending order to prioritize the most significant components.\n5. **Sign Adjustment**: Ensures the first element of each eigenvector is non-negative for consistency.\n6. **Projection**: The original data matrix is projected onto the selected eigenvectors, and the result is rounded and converted to a list for the final output.\n\nThis approach follows the classical PCA steps, ensuring the data is transformed into a lower-dimensional space using the most significant principal components while maintaining numerical stability and consistency."}
{"task_id": 292, "completion_id": 0, "completion": "To solve this problem, we need to perform a single-point column crossover on two parent weight matrices to generate two offspring matrices. This crossover involves swapping columns from a specified cutoff point onwards between the two parents. The solution must validate the input matrices to ensure they have the same shape and then efficiently perform the column swap using NumPy for optimal performance.\n\n### Approach\n1. **Validation**: Check if both parent matrices have the same shape. This involves ensuring they have the same number of rows and columns. If they do not, return -1.\n2. **Edge Case Handling**: If the matrices are empty, return empty matrices as children.\n3. **Conversion to NumPy Arrays**: Convert the parent matrices to NumPy arrays for efficient slicing and manipulation.\n4. **Column Crossover**: Use NumPy slicing to create the children matrices by combining columns from both parents based on the specified cutoff point.\n5. **Conversion to Python Lists**: Convert the resulting NumPy arrays back to nested Python lists before returning them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                           parent2: list[list[int | float]],\n                           cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\"\"\"\n    # Check if both parents have the same number of rows\n    parent1_rows = len(parent1)\n    parent2_rows = len(parent2)\n    if parent1_rows != parent2_rows:\n        return -1\n    \n    # Handle empty matrices\n    if parent1_rows == 0:\n        return ([], [])\n    \n    # Check all rows in parent1 have the same number of columns\n    parent1_cols = len(parent1[0])\n    for row in parent1:\n        if len(row) != parent1_cols:\n            return -1\n    \n    # Check all rows in parent2 have the same number of columns\n    parent2_cols = len(parent2[0])\n    for row in parent2:\n        if len(row) != parent2_cols:\n            return -1\n    \n    # Check if the number of columns matches between parents\n    if parent1_cols != parent2_cols:\n        return -1\n    \n    # Convert parents to numpy arrays\n    np_parent1 = np.array(parent1)\n    np_parent2 = np.array(parent2)\n    \n    # Perform the crossover\n    child1 = np.hstack((np_parent1[:, :cutoff], np_parent2[:, cutoff:]))\n    child2 = np.hstack((np_parent2[:, :cutoff], np_parent1[:, cutoff:]))\n    \n    # Convert back to Python lists and return\n    return (child1.tolist(), child2.tolist())\n```\n\n### Explanation\n1. **Validation**: The function first checks if the number of rows in both parent matrices is the same. If not, it returns -1. It then checks if the matrices are non-empty and if each row within each matrix has the same number of columns. If any of these checks fail, it returns -1.\n2. **Edge Case Handling**: If both matrices are empty, the function immediately returns empty matrices as children.\n3. **Conversion to NumPy Arrays**: The parent matrices are converted to NumPy arrays to leverage efficient array operations.\n4. **Column Crossover**: Using NumPy's `hstack` function, the columns from both parents are combined based on the cutoff point. Columns before the cutoff are taken from the original parent, and columns from the cutoff onwards are taken from the other parent.\n5. **Conversion to Python Lists**: The resulting NumPy arrays are converted back to nested Python lists using the `tolist()` method to meet the output requirements.\n\nThis approach ensures that the solution is both efficient and correct, handling all edge cases and validating inputs appropriately."}
{"task_id": 293, "completion_id": 0, "completion": "```python\ndef map_agent_environment(agent_name: str):\n    \"\"\"Return the environment name and access type that should be used for a given RL agent.\n\n    The mapping is case-insensitive and ignores leading/trailing spaces. If the\n    agent name is not recognised, the function returns -1.\n\n    Args:\n        agent_name: Name of the RL agent (e.g. \"dqn\", \"A3C\", etc.).\n\n    Returns:\n        Tuple[str, str] if the agent is known, or -1 otherwise.\n    \"\"\"\n    processed_name = agent_name.strip().lower()\n    agent_mapping = {\n        'dqn': ('CartPole-v0', 'unwrapped'),\n        'ddpg': ('Pendulum-v0', 'env'),\n        'a3c': ('Pendulum-v0', 'unwrapped'),\n        'a2c': ('CartPole-v0', 'env'),\n        'a2c_multi': ('CartPole-v0', 'raw'),\n        'trpo': ('Pendulum-v0', 'unwrapped')\n    }\n    return agent_mapping.get(processed_name, -1)\n```"}
{"task_id": 294, "completion_id": 0, "completion": "To solve this problem, we need to convert a custom graph representation into an adjacency dictionary. The adjacency dictionary must meet specific requirements, especially ensuring that undirected edges are represented in both directions and that neighbor lists are sorted lexicographically by neighbor labels.\n\n### Approach\n1. **Initialize Adjacency Dictionary**: Create an empty dictionary where each key is a vertex label from the input graph, initialized with an empty list to store edges.\n2. **Process Edges**: Iterate through each edge in the input graph. For each edge, add it to the adjacency list of the source vertex. If the graph is undirected, also add the reverse edge to ensure both directions are present.\n3. **Sort Neighbor Lists**: After processing all edges, sort each vertex's list of neighbors lexicographically by the neighbor's label to ensure deterministic output.\n\n### Solution Code\n```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\"\"\"\n    adj = {}\n    v2i = G['_V2I']\n    # Initialize adjacency dict with all vertices\n    for vertex in v2i:\n        adj[vertex] = []\n    # Process each edge in the graph\n    for edges in G['_G']:\n        for edge in edges:\n            s, t, w = edge\n            adj[s].append((t, w))\n            # Add reverse edge for undirected graphs\n            if not G['is_directed']:\n                adj[t].append((s, w))\n    # Sort each adjacency list lexicographically by neighbor label\n    for u in adj:\n        adj[u].sort(key=lambda x: x[0])\n    return adj\n```\n\n### Explanation\n1. **Initialization**: The adjacency dictionary `adj` is initialized with all vertex labels from `G['_V2I']`, each mapped to an empty list. This ensures that even vertices with no edges are present in the result.\n2. **Edge Processing**: Each edge from the input graph's `_G` list is processed. For directed graphs, edges are added as-is. For undirected graphs, each edge is added in both directions (once from source to target and once from target to source).\n3. **Sorting**: After all edges are processed, each vertex's list of neighbors is sorted lexicographically by neighbor labels to ensure the output is deterministic and meets the problem's requirements.\n\nThis approach efficiently converts the custom graph structure into the required adjacency dictionary, handling both directed and undirected graphs appropriately while ensuring the output format is correct."}
{"task_id": 296, "completion_id": 0, "completion": "To solve this problem, we need to verify if all elements in a given NumPy array are binary, meaning each element is either 0 or 1. The solution should handle arrays of any shape and data type, including integers, floats, and booleans.\n\n### Approach\nThe approach involves the following steps:\n1. **Convert Input to NumPy Array**: Ensure the input is converted to a NumPy array to handle array-like inputs uniformly.\n2. **Create a Boolean Mask**: Generate a boolean mask where each element is `True` if it is either 0 or 1, and `False` otherwise. This mask is created by checking each element against 0 and 1 using element-wise comparison.\n3. **Check All Elements**: Use NumPy's `np.all` function to verify if all elements in the boolean mask are `True`. This function efficiently checks the entire array for the condition.\n4. **Return Result as Python Boolean**: Convert the result from a NumPy boolean type to a Python built-in boolean type to meet the output constraints.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    x = np.asarray(x)\n    return bool(np.all((x == 0) | (x == 1)))\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `x` is converted to a NumPy array using `np.asarray(x)`. This ensures that even if the input is a list or another array-like structure, it is handled as a NumPy array.\n2. **Boolean Mask Creation**: The mask `(x == 0) | (x == 1)` checks each element in the array to see if it is either 0 or 1. This works for all numeric types and booleans, as boolean values `True` and `False` are treated as 1 and 0, respectively.\n3. **Check All Elements**: The `np.all` function checks if every element in the boolean mask is `True`, indicating all elements in the original array are 0 or 1.\n4. **Result Conversion**: The result from `np.all` is converted to a Python boolean using `bool()` to ensure the return type is a built-in boolean, as required.\n\nThis approach efficiently handles all specified data types and array shapes, ensuring correctness and adherence to the problem constraints."}
{"task_id": 297, "completion_id": 0, "completion": "To solve this problem, we need to perform Min-Max scaling on a 2-D numerical dataset. This process involves rescaling each feature column of the dataset to a user-defined interval, handling constant columns appropriately, and ensuring the result is free of invalid values like NaN or infinity.\n\n### Approach\n1. **Check Validity of Feature Range**: Immediately return -1 if the provided feature range is invalid (i.e., the lower bound is greater than or equal to the upper bound).\n2. **Convert Data to NumPy Array**: Convert the input list of lists into a NumPy array for efficient numerical operations.\n3. **Compute Min and Max for Each Column**: Calculate the minimum and maximum values for each feature column to determine the scaling parameters.\n4. **Handle Constant Columns**: If a column has constant values (max == min), set the denominator to 1 to avoid division by zero.\n5. **Apply Scaling Formula**: Use the Min-Max scaling formula to transform each feature column to the desired range.\n6. **Replace Invalid Values**: Replace any NaN, infinity, or negative infinity values with 0.0 using NumPy's `nan_to_num` function.\n7. **Round and Convert Back to List**: Round the scaled values to four decimal places and convert the result back to a nested Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef min_max_scale(data: list[list[int | float]], feature_range: tuple = (0, 1)) -> list[list[float]]:\n    a, b = feature_range\n    if a >= b:\n        return -1\n    data_np = np.array(data, dtype=np.float64)\n    mins = data_np.min(axis=0)\n    maxs = data_np.max(axis=0)\n    denominator = maxs - mins\n    denominator[denominator == 0] = 1\n    scaled = (data_np - mins) / denominator * (b - a) + a\n    scaled = np.nan_to_num(scaled, nan=0.0, posinf=0.0, neginf=0.0)\n    rounded = np.round(scaled, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Validity Check**: The function first checks if the provided feature range is valid. If the lower bound is not less than the upper bound, it returns -1 immediately.\n2. **Data Conversion**: The input data is converted into a NumPy array to leverage efficient array operations.\n3. **Min and Max Calculation**: The minimum and maximum values for each column are computed to determine the scaling parameters for each feature.\n4. **Handling Constant Columns**: If a column's values are constant, the denominator (max - min) is set to 1 to avoid division by zero, ensuring all values in such columns are scaled to the lower bound of the feature range.\n5. **Scaling Formula Application**: Each value in the dataset is scaled using the formula \\( x_{\\text{scaled}} = \\left(\\frac{x - \\min}{\\max - \\min}\\right) \\times (b - a) + a \\), adjusted for constant columns.\n6. **Invalid Value Replacement**: Any NaN or infinity values resulting from computations are replaced with 0.0 to ensure clean output.\n7. **Rounding and Conversion**: The scaled values are rounded to four decimal places and converted back to a nested Python list to meet the output requirements.\n\nThis approach ensures that the dataset is scaled correctly, handles edge cases like constant columns and invalid values, and returns the result in the required format."}
{"task_id": 298, "completion_id": 0, "completion": "To solve this problem, we need to calculate the total log-probability of a query sentence using a Maximum-Likelihood-Estimation (MLE) N-gram language model trained on a given corpus. The solution involves processing both the corpus and the query to build the model, then using it to compute the probabilities of the query's N-grams.\n\n### Approach\n\n1. **Corpus Processing**:\n   - **Tokenization**: Split each sentence into tokens based on whitespace.\n   - **Padding**: For N-grams where N > 1, prepend each token sequence with N-1 special tokens `<bol>` and append a `<eol>` token. No padding is needed for unigrams (N=1).\n   - **N-gram and Prefix Counting**: For each sentence in the corpus, generate all N-grams and their (N-1)-gram prefixes. Count the occurrences of these N-grams and prefixes to build the model.\n\n2. **Query Processing**:\n   - **Tokenization and Padding**: Similar to the corpus processing, split the query into tokens and pad it appropriately based on the value of N.\n   - **N-gram Generation**: Generate all possible N-grams from the padded query tokens.\n   - **Probability Calculation**: For each N-gram in the query, compute its probability using the counts from the corpus. If any N-gram or its prefix is unseen (count is zero), return negative infinity. Otherwise, sum the natural logarithms of these probabilities and round the result to four decimal places.\n\n### Solution Code\n\n```python\nimport math\nfrom collections import Counter\nfrom typing import List\n\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\"\"\"\n    if N < 1:\n        raise ValueError(\"N must be \u2265 1\")\n\n    # Process the corpus to build the model\n    if N == 1:\n        unigram_counts = Counter()\n        for sentence in corpus:\n            tokens = sentence.split()\n            unigram_counts.update(tokens)\n        total_tokens = sum(unigram_counts.values())\n    else:\n        ngram_counts = Counter()\n        prefix_counts = Counter()\n        for sentence in corpus:\n            original_tokens = sentence.split()\n            padded_tokens = ['<bol>'] * (N - 1) + original_tokens + ['<eol>']\n            # Generate all N-grams for the current sentence\n            for i in range(len(padded_tokens) - N + 1):\n                ngram = tuple(padded_tokens[i:i + N])\n                ngram_counts[ngram] += 1\n            # Generate all (N-1)-grams for the current sentence\n            for i in range(len(padded_tokens) - (N - 1) + 1):\n                prefix = tuple(padded_tokens[i:i + (N - 1)])\n                prefix_counts[prefix] += 1\n\n    # Process the query\n    query_tokens = query.split()\n    if N > 1:\n        padded_query = ['<bol>'] * (N - 1) + query_tokens + ['<eol>']\n    else:\n        padded_query = query_tokens\n\n    # Check if the padded query is long enough\n    if len(padded_query) < N:\n        return float('-inf')\n\n    # Generate all N-grams from the padded query\n    query_ngrams = [tuple(padded_query[i:i + N]) for i in range(len(padded_query) - N + 1)]\n\n    log_sum = 0.0\n    for ngram in query_ngrams:\n        if N == 1:\n            unigram = ngram[0]\n            count = unigram_counts.get(unigram, 0)\n            if count == 0 or total_tokens == 0:\n                return float('-inf')\n            prob = count / total_tokens\n        else:\n            # Check if the ngram exists\n            count_ngram = ngram_counts.get(ngram, 0)\n            if count_ngram == 0:\n                return float('-inf')\n            # Check the prefix\n            prefix = ngram[:-1]\n            count_prefix = prefix_counts.get(prefix, 0)\n            if count_prefix == 0:\n                return float('-inf')\n            prob = count_ngram / count_prefix\n        log_sum += math.log(prob)\n\n    return round(log_sum, 4)\n```\n\n### Explanation\n\n- **Corpus Processing**: The corpus is processed to count N-grams and their prefixes. For N=1, we simply count the occurrences of each token. For N>1, we pad each sentence, then count each N-gram and its (N-1)-gram prefix.\n- **Query Processing**: The query is tokenized and padded similarly. We check if the padded query is long enough to form at least one N-gram. If not, we return negative infinity.\n- **Probability Calculation**: For each N-gram in the query, we check if it and its prefix exist in the corpus counts. If either is missing, we return negative infinity. Otherwise, we compute the probability and accumulate the log-probability sum, which is then rounded to four decimal places.\n\nThis approach ensures that we accurately model the N-gram probabilities and handle edge cases such as unseen N-grams or insufficient query length appropriately."}
{"task_id": 299, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm from scratch. The algorithm groups samples into clusters by iteratively assigning each sample to the nearest centroid and updating the centroids based on the current assignments. The process continues until the centroids stabilize or a maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: The initial centroids are set to the first `k` samples of the input data matrix `X`.\n2. **Assignment Step**: For each sample, compute the squared Euclidean distance to each centroid and assign the sample to the nearest centroid.\n3. **Update Step**: Recompute each centroid as the mean of all samples assigned to that cluster. If a cluster becomes empty, its centroid remains unchanged.\n4. **Termination Check**: The algorithm stops if the centroids do not change after an update or if the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef k_means(X: np.ndarray, k: int, max_iterations: int = 500) -> list[int]:\n    \"\"\"K-Means clustering.\n\n    Args:\n        X: 2-D NumPy array of shape (m, n) containing the input samples.\n        k: Number of clusters to form.\n        max_iterations: Maximum number of iterations (default: 500).\n\n    Returns:\n        A list of length *m* where the i-th element is the cluster index of the\n        i-th sample.\n    \"\"\"\n    centroids = X[:k].copy()\n    for _ in range(max_iterations):\n        # Calculate squared distances between each sample and centroids\n        squared_distances = np.sum((X[:, np.newaxis, :] - centroids) ** 2, axis=2)\n        assignments = np.argmin(squared_distances, axis=1)\n        \n        # Compute new centroids\n        new_centroids = centroids.copy()\n        for j in range(k):\n            mask = (assignments == j)\n            if mask.any():\n                new_centroids[j] = X[mask].mean(axis=0)\n        \n        # Check for convergence\n        if np.array_equal(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    \n    return assignments.tolist()\n```\n\n### Explanation\n1. **Initialization**: The centroids are initialized to the first `k` samples of `X` to ensure determinism.\n2. **Squared Distance Calculation**: Squared Euclidean distances are computed between each sample and each centroid to avoid the computational cost of taking square roots.\n3. **Assignment**: Each sample is assigned to the cluster corresponding to the nearest centroid using the computed distances.\n4. **Centroid Update**: New centroids are calculated as the mean of the samples assigned to each cluster. If a cluster has no samples, its centroid remains unchanged.\n5. **Convergence Check**: The algorithm checks if the centroids have changed after each iteration. If they remain the same, the process stops early.\n\nThis approach efficiently iterates between assigning samples to clusters and updating centroids, ensuring that the algorithm stops as soon as the clusters stabilize or the maximum iterations are reached, providing an optimal and deterministic clustering solution."}
{"task_id": 300, "completion_id": 0, "completion": "To solve this problem, we need to efficiently find the top-k elements from a stream of numerical values using a heap data structure. The solution should maintain a memory footprint of O(k) and ensure each heap operation (push/pop) runs in O(log k) time. The result should be sorted according to the specified order, either non-increasing for the largest elements or non-decreasing for the smallest elements.\n\n### Approach\n1. **Edge Cases Handling**: Check if the input sequence is empty or if k is non-positive, returning an empty list in such cases. If k is greater than or equal to the length of the sequence, return the entire sequence sorted as required.\n2. **Heap Initialization**: Use a min-heap to track the k largest elements and a max-heap (simulated using negative values) to track the k smallest elements.\n3. **Processing Elements**: For each element in the sequence:\n   - For the largest elements, maintain a min-heap of size k. Replace the smallest element in the heap if the current element is larger.\n   - For the smallest elements, maintain a max-heap (using negative values) of size k. Replace the largest element in the heap if the current element is smaller.\n4. **Result Extraction**: After processing all elements, extract and reverse the heap elements to achieve the required order.\n\n### Solution Code\n```python\nimport heapq\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\ndef top_k_elements(sequence: List[Number], k: int, order: str = \"largest\") -> List[Number]:\n    \"\"\"Keeps only the k largest or k smallest numbers from *sequence*.\"\"\"\n    if not sequence or k <= 0:\n        return []\n    n = len(sequence)\n    if k >= n:\n        if order == \"largest\":  # Typo here: should be \"largest\"\n            return sorted(sequence, reverse=True)\n        else:\n            return sorted(sequence)\n    \n    if order == \"largest\":\n        heap = []\n        for num in sequence:\n            if len(heap) < k:\n                heapq.heappush(heap, num)\n            else:\n                if num > heap[0]:\n                    heapq.heappushpop(heap, num)\n        res = []\n        while heap:\n            res.append(heapq.heappop(heap))\n        res.reverse()\n        return res\n    else:  # order == \"smallest\"\n        heap = []\n        for num in sequence:\n            if len(heap) < k:\n                heapq.heappush(heap, -num)\n            else:\n                if num < -heap[0]:\n                    heapq.heappushpop(heap, -num)\n        res = []\n        while heap:\n            res.append(-heapq.heappop(heap))\n        res.reverse()\n        return res\n```\n\n### Explanation\n1. **Edge Cases Handling**: The function first checks if the input sequence is empty or if k is non-positive, returning an empty list immediately. If k is larger than the sequence length, it returns the sorted sequence as required.\n2. **Heap Processing**:\n   - For the largest elements, a min-heap is used. Each element is pushed into the heap if it is larger than the smallest element in the heap once the heap reaches size k. This ensures the heap always contains the largest k elements.\n   - For the smallest elements, a max-heap is simulated using negative values. Each element is pushed into the heap if it is smaller than the largest element in the heap (using negative values) once the heap reaches size k.\n3. **Result Extraction**: After processing all elements, the heap elements are extracted and reversed to meet the required order. For the largest elements, the min-heap elements are reversed to get non-increasing order. For the smallest elements, the simulated max-heap elements are converted back to positive values and reversed to get non-decreasing order.\n\nThis approach efficiently maintains the top-k elements using heaps with the desired time and space complexity, ensuring optimal performance for large input streams."}
{"task_id": 301, "completion_id": 0, "completion": "To solve this problem, we need to build a binary search tree (BST) from a list of (key, value) pairs and return the keys obtained from an in-order traversal of the tree. The BST is ordered based on the values of the nodes, and the comparison operations are implemented using the Node class's comparison operators.\n\n### Approach\n1. **Node Class Definition**: Define a Node class with key and value attributes, and implement comparison operators based on the value. This allows us to use standard comparison operators (<, >, etc.) on Node instances directly.\n2. **Tree Construction**: \n   - Create nodes from each (key, value) pair.\n   - Insert the first node as the root of the BST.\n   - For subsequent nodes, insert them into the BST according to the rules: go left if the new node is strictly smaller than the current node, otherwise go right.\n3. **In-order Traversal**: Perform an in-order traversal of the BST to collect the keys in non-decreasing order of their values.\n\n### Solution Code\n```python\ndef inorder_keys(pairs):\n    \"\"\"Build a BST from (key, value) *pairs* and return keys from an in-order traversal.\n\n    Args:\n        pairs (list[tuple[Any, int | float]]): List of (key, numeric value) pairs.\n\n    Returns:\n        list: Keys in in-order (ascending by numeric value). Return an empty\n        list if *pairs* is empty.\n    \"\"\"\n    if not pairs:\n        return []\n    \n    from functools import total_ordering\n\n    @total_ordering\n    class Node:\n        def __init__(self, key, val):\n            self.key = key\n            self.val = val\n            self.left = None\n            self.right = None\n\n        def __lt__(self, other):\n            if not isinstance(other, Node):\n                return NotImplemented\n            return self.val < other.val\n\n        def __eq__(self, other):\n            if not isinstance(other, Node):\n                return NotImplemented\n            return self.val == other.val\n\n    nodes = [Node(k, v) for k, v in pairs]\n    root = nodes[0]\n    \n    for node in nodes[1:]:\n        current = root\n        while True:\n            if node < current:\n                if current.left is None:\n                    current.left = node\n                    break\n                else:\n                    current = current.left\n            else:\n                if current.right is None:\n                    current.right = node\n                    break\n                else:\n                    current = current.right\n    \n    result = []\n    def in_order(node):\n        if node is not None:\n            in_order(node.left)\n            result.append(node.key)\n            in_order(node.right)\n    \n    in_order(root)\n    return result\n```\n\n### Explanation\n1. **Node Class**: The Node class is defined with `key` and `val` attributes. The `total_ordering` decorator is used to automatically generate the remaining comparison operators based on `__lt__` and `__eq__`, which compare the `val` attribute of nodes.\n2. **Tree Construction**: The first node from the list of pairs is set as the root. Each subsequent node is inserted into the BST by comparing it with the current nodes using their values. Nodes are inserted to the left if they are smaller and to the right otherwise.\n3. **In-order Traversal**: The in-order traversal visits the left subtree, then the current node, and finally the right subtree. This ensures that the keys are collected in non-decreasing order of their values, which is the desired output.\n\nThis approach efficiently constructs the BST and performs the in-order traversal, ensuring the correct order of keys based on their associated values."}
{"task_id": 302, "completion_id": 0, "completion": "To solve this problem, we need to implement the Spectral Clustering algorithm from scratch. Spectral clustering is a technique that uses the eigenvalues of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The steps involved are constructing a similarity matrix, computing the graph Laplacian, finding eigenvalues and eigenvectors, and then applying k-means clustering on the eigenvectors.\n\n### Approach\n1. **Construct the Weighted Adjacency Matrix (W):** Compute the pairwise distances between data points and convert these distances into weights using the formula \\( w_{ij} = 1 / (1 + \\text{Euclidean distance between } x_i \\text{ and } x_j) \\). The diagonal elements of this matrix are set to zero to avoid self-connections.\n\n2. **Compute the Graph Laplacian (L):** The Laplacian matrix is computed as \\( L = D - W \\), where \\( D \\) is the diagonal degree matrix whose elements are the sum of the weights for each node.\n\n3. **Eigenvalue Decomposition:** Compute the eigenvalues and eigenvectors of the Laplacian matrix \\( L \\). The first \\( k \\) eigenvectors corresponding to the smallest eigenvalues are selected to form a matrix \\( E \\).\n\n4. **K-means Clustering:** Apply the k-means clustering algorithm on the rows of the matrix \\( E \\). The initial centroids for k-means are the first \\( k \\) rows of \\( E \\). If a cluster becomes empty during iterations, its centroid is reinitialized with a random row from \\( E \\).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n = X.shape[0]\n    \n    # Step 1: Build the weighted adjacency matrix W\n    diff = X[:, np.newaxis, :] - X[np.newaxis, :, :]\n    distance_matrix = np.sqrt((diff ** 2).sum(axis=2))\n    W = 1 / (1 + distance_matrix)\n    np.fill_diagonal(W, 0)\n    \n    # Step 2: Compute the unnormalized graph Laplacian L = D - W\n    D = np.diag(W.sum(axis=1))\n    L = D - W\n    \n    # Step 3: Compute the first k eigenvectors of L\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    E = eigenvectors[:, :k]\n    \n    # Step 4: Run k-means on the rows of E\n    centroids = E[:k, :].copy()\n    prev_labels = None\n    \n    for _ in range(100):\n        # Compute distances from each point to each centroid\n        distances = np.sqrt(((E[:, np.newaxis, :] - centroids) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=1)\n        \n        # Check for convergence\n        if prev_labels is not None and np.array_equal(labels, prev_labels):\n            break\n        prev_labels = labels.copy()\n        \n        # Update centroids\n        new_centroids = np.zeros_like(centroids)\n        counts = np.zeros(k, dtype=int)\n        \n        # Sum all points in each cluster\n        for i in range(n):\n            cluster = labels[i]\n            new_centroids[cluster] += E[i]\n            counts[cluster] += 1\n        \n        # Handle empty clusters and compute new centroids\n        for c in range(k):\n            if counts[c] == 0:\n                # Re-initialize with a random row from E\n                random_idx = np.random.randint(n)\n                new_centroids[c] = E[random_idx]\n            else:\n                new_centroids[c] = new_centroids[c] / counts[c]\n        \n        centroids = new_centroids\n    \n    return labels.tolist()\n```\n\n### Explanation\n1. **Weighted Adjacency Matrix (W):** This matrix captures the similarity between each pair of data points using the inverse of the Euclidean distance. The diagonal elements are zero to prevent self-connections.\n\n2. **Graph Laplacian (L):** The Laplacian matrix is derived from the degree matrix \\( D \\) (sum of weights for each node) and the adjacency matrix \\( W \\). It helps in studying the properties of the graph.\n\n3. **Eigenvalue Decomposition:** The eigenvectors corresponding to the smallest eigenvalues of the Laplacian matrix provide a low-dimensional embedding of the data, which is easier to cluster.\n\n4. **K-means Clustering:** The low-dimensional embedding from the eigenvectors is clustered using k-means. The initial centroids are deterministic (first \\( k \\) rows of the eigenvector matrix), and empty clusters are reinitialized with random points to avoid convergence issues.\n\nThis approach efficiently combines spectral techniques with traditional clustering to handle non-convex data distributions, which are challenging for standard k-means."}
{"task_id": 303, "completion_id": 0, "completion": "To solve this problem, we need to generate a formatted debugging string that compares the current and previous outputs of a program with their corresponding gold-standard outputs. The output must follow a specific structure with precise formatting, including dashes, line breaks, and labels.\n\n### Approach\n1. **Determine Previous and Current Indices**: Calculate the previous index as `max(ix-1, 0)` to handle the case where `ix` is 0. This ensures we always have valid indices for both previous and current elements.\n2. **Extract Values**: Retrieve the previous and current outputs (both program-generated and gold-standard) using their respective indices and labels.\n3. **Construct the Debug String**: Build the debug string using the specified format, including the required number of dashes, labels, and outputs. Append any provided warning message directly to the current gold-standard output.\n\n### Solution Code\n```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    prev_index = max(ix - 1, 0)\n    prev_mine, prev_label = params[prev_index]\n    curr_mine, curr_label = params[ix]\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    \n    debug_lines = [\n        '-' * 25 + ' DEBUG ' + '-' * 25,\n        f\"Mine (prev) [{prev_label}]:\",\n        prev_mine,\n        f\"Theirs (prev) [{prev_label}]:\",\n        prev_gold,\n        f\"Mine [{curr_label}]:\",\n        curr_mine,\n        f\"Theirs [{curr_label}]:\",\n        curr_gold + warn_str,\n        '-' * 23 + ' END DEBUG ' + '-' * 23\n    ]\n    return '\\n'.join(debug_lines)\n```\n\n### Explanation\n- **Previous Index Calculation**: The previous index is determined to ensure it is within valid bounds. If `ix` is 0, the previous index is also 0.\n- **Extracting Values**: The previous and current outputs (both program-generated and gold-standard) are extracted using their respective indices and labels from the input parameters.\n- **String Construction**: The debug string is constructed line by line, adhering to the specified format. Each section (previous and current outputs) is formatted with appropriate labels and values. The warning message is appended directly to the current gold-standard output to maintain the exact structure required.\n\nThis approach ensures that the output is correctly formatted with the required number of dashes, line breaks, and labels, providing a clear and concise comparison for debugging purposes."}
{"task_id": 304, "completion_id": 0, "completion": "To solve this problem, we need to compute the posterior mean (Maximum-A-Posteriori estimate) of the regression coefficients in Bayesian linear regression with an unknown noise variance. The model uses a normal-inverse-gamma conjugate prior on the coefficients and noise variance. The key steps involve handling the prior parameters and computing the posterior mean using closed-form formulas.\n\n### Approach\n1. **Add Intercept**: If `fit_intercept` is True, prepend a column of ones to the design matrix X to account for the bias term.\n2. **Process Prior Mean (mu)**: Convert scalar input to a vector of appropriate length. Ensure the vector length matches the number of features (including the intercept if added).\n3. **Process Prior Scale (V)**: Convert different input forms (scalar, list, matrix) into a diagonal or full matrix as required. Compute the inverse of this matrix.\n4. **Compute Posterior Parameters**: Use the closed-form formulas to compute the posterior covariance (up to noise variance) and mean of the coefficients. The posterior mean is derived using matrix operations involving the prior parameters and the data.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu=0.0,\n    V=None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility.\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    # Add intercept column if required\n    if fit_intercept:\n        X_augmented = np.hstack([np.ones((X.shape[0], 1), dtype=X.dtype), X])\n    else:\n        X_augmented = X.copy()\n    \n    n_samples, n_features = X_augmented.shape\n    \n    # Process mu to ensure it's a vector of correct length\n    if np.isscalar(mu):\n        mu_prior = np.full(n_features, mu, dtype=np.float64)\n    else:\n        mu_prior = np.asarray(mu, dtype=np.float64)\n        if mu_prior.shape != (n_features,):\n            raise ValueError(\n                \"mu must be a scalar or have length equal to the number of features.\"\n            )\n    \n    # Process V to create the prior covariance matrix and compute its inverse\n    if V is None:\n        V_prior = np.eye(n_features)\n    elif np.isscalar(V):\n        V_prior = V * np.eye(n_features)\n    elif isinstance(V, (list, tuple)):\n        if len(V) != n_features:\n            raise ValueError(\n                \"V as list/tuple must have length equal to the number of features.\"\n            )\n        V_prior = np.diag(V)\n    elif isinstance(V, np.ndarray):\n        if V.ndim == 1:\n            if V.size != n_features:\n                raise ValueError(\n                    \"V as 1D array must have length equal to the number of features.\"\n                )\n            V_prior = np.diag(V)\n        elif V.ndim == 2:\n            if V.shape != (n_features, n_features):\n                raise ValueError(\n                    \"V as 2D array must be a square matrix matching feature count.\"\n                )\n            V_prior = V\n        else:\n            raise ValueError(\"V must be 1D or 2D numpy array.\")\n    else:\n        raise TypeError(\"V must be None, scalar, list/tuple, or numpy array.\")\n    \n    V_inv = np.linalg.inv(V_prior)\n    \n    # Compute components for the posterior mean\n    XTX = X_augmented.T @ X_augmented\n    XTy = X_augmented.T @ y\n    Sigma_b = np.linalg.inv(V_inv + XTX)\n    mu_b = Sigma_b @ (V_inv @ mu_prior + XTy)\n    \n    # Round to four decimal places and return as list\n    return [round(coef, 4) for coef in mu_b.tolist()]\n```\n\n### Explanation\n1. **Intercept Handling**: The code checks if an intercept should be added and modifies the design matrix accordingly.\n2. **Prior Mean (mu)**: Converts scalar inputs to vectors and ensures existing vectors match the feature count.\n3. **Prior Scale (V)**: Converts different input forms into a matrix, handling scalars as identity matrices scaled by the scalar, lists as diagonal matrices, and checks the dimensions for 2D arrays.\n4. **Matrix Operations**: Computes the posterior covariance and mean using matrix inversions and multiplications as per the derived formulas, ensuring numerical stability and efficiency.\n5. **Result Formatting**: Rounds the results to four decimal places and returns them as a list for consistency with the problem requirements."}
{"task_id": 305, "completion_id": 0, "completion": "To solve this problem, we need to implement a numerically stable version of the logistic sigmoid activation function. The function must handle various input types, including scalars, lists, tuples, and NumPy arrays, and return the element-wise sigmoid values as a Python list rounded to 4 decimal places. The solution must avoid numerical overflow by using different computations for positive and negative values.\n\n### Approach\n1. **Input Handling**: Convert the input into a NumPy array to handle scalars, lists, tuples, and existing arrays uniformly. This conversion ensures we can apply vectorized operations efficiently.\n2. **Numerically Stable Computation**: Use a piecewise approach to compute the sigmoid values. For non-negative values, compute \\( \\frac{1}{1 + e^{-x}} \\). For negative values, compute \\( \\frac{e^x}{1 + e^x} \\) to avoid overflow.\n3. **Flattening**: Convert the input array into a 1D array to ensure the output is a flat list, regardless of the input's original structure.\n4. **Rounding and Conversion**: Round the computed values to 4 decimal places and convert the result to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    \"\"\"Compute the element-wise logistic sigmoid in a numerically stable way.\n\n    The function must\n        \u2022 Accept a scalar, list, tuple or NumPy array of real numbers.\n        \u2022 Return a Python list with each sigmoid value rounded to 4 decimals.\n\n    Args:\n        x (int | float | list | tuple | np.ndarray): Input value(s).\n\n    Returns:\n        list[float]: List containing the sigmoid of every input element.\n    \"\"\"\n    x_arr = np.asarray(x, dtype=np.float64).flatten()\n    s = np.where(x_arr >= 0,\n                 1 / (1 + np.exp(-x_arr)),\n                 np.exp(x_arr) / (1 + np.exp(x_arr)))\n    s_rounded = np.round(s, decimals=4)\n    return s_rounded.tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input is converted to a NumPy array using `np.asarray`, which handles scalars, lists, tuples, and existing arrays. The `.flatten()` method ensures the array is 1D, making the output a flat list.\n2. **Numerically Stable Calculation**: Using `np.where`, we apply the appropriate formula based on the sign of each element. For non-negative values, the standard sigmoid formula is used. For negative values, the alternative formula prevents overflow by avoiding large exponents.\n3. **Rounding and Conversion**: The computed sigmoid values are rounded to 4 decimal places using `np.round` and then converted to a Python list with `.tolist()`, ensuring the output format matches the requirements.\n\nThis approach efficiently handles all input types, ensures numerical stability, and returns the results in the required format."}
{"task_id": 307, "completion_id": 0, "completion": "To solve this problem, we need to generate a bootstrap sample from a given feature matrix and target vector. Bootstrap sampling involves drawing samples with replacement from the original dataset to create a new dataset of the same size. This technique is commonly used in ensemble learning methods like Bagging and Random Forests.\n\n### Approach\n1. **Determine the Number of Samples**: Extract the number of samples from the first dimension of the feature matrix `X`.\n2. **Generate Indices**: Create an array of indices by randomly selecting `n_samples` indices with replacement from the range `[0, n_samples - 1]`.\n3. **Create Bootstrap Samples**: Use the generated indices to create bootstrap samples from both the feature matrix `X` and the target vector `y`.\n\nThis approach ensures that the bootstrap samples are generated using NumPy's global random number generator, making the results reproducible if a seed is set beforehand.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bootstrap_sample(X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return a bootstrap sample of the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, ...).\n    y : np.ndarray\n        Target vector (or array) of shape (n_samples, ...).\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        ``(X_sample, y_sample)`` where each has the same shape as its\n        counterpart in the input.\n    \"\"\"\n    n_samples = X.shape[0]\n    idxs = np.random.choice(n_samples, size=n_samples, replace=True)\n    return X[idxs], y[idxs]\n```\n\n### Explanation\n1. **Number of Samples**: The number of samples (`n_samples`) is determined from the first dimension of the feature matrix `X` using `X.shape[0]`.\n2. **Index Generation**: Indices are generated using `np.random.choice`, which randomly selects `n_samples` indices from the range `[0, n_samples - 1]` with replacement. This ensures that some indices may be repeated and others may be omitted in the bootstrap sample.\n3. **Bootstrap Samples**: The generated indices are used to index both `X` and `y`, creating new arrays (`X_sample` and `y_sample`) that maintain the original shapes but contain the resampled data.\n\nThis method efficiently creates bootstrap samples using NumPy's array indexing, ensuring that the resulting arrays have the same structure as the inputs. The use of NumPy's global RNG guarantees reproducibility when a seed is set, which is crucial for consistent results in experiments and model training."}
{"task_id": 308, "completion_id": 0, "completion": "To solve this problem, we need to construct a Mel filterbank matrix that converts a power spectrum vector into Mel band energies using triangular filters defined on the Mel scale. The Mel scale is a perceptual scale that emphasizes lower frequencies more than higher frequencies, which is crucial in speech and audio processing.\n\n### Approach\n1. **Convert Frequency Limits to Mel Scale**: Convert the given minimum and maximum frequencies (in Hertz) to the Mel scale using the formula `mel = 2595 * log10(1 + f/700)`.\n2. **Generate Equally Spaced Mel Points**: Generate `n_filters + 2` points equally spaced in the Mel scale between the converted minimum and maximum Mel values.\n3. **Convert Mel Points Back to Hertz**: Convert these Mel points back to Hertz using the inverse formula `f = 700 * (10^(mel/2595) - 1)`.\n4. **Compute Triangular Filters**: For each filter, compute the weights for each DFT bin based on the triangular response defined by three consecutive points from the converted Hertz values.\n5. **Normalization**: If required, normalize each filter so that its area in Mel space equals 1.\n6. **Return Filterbank Matrix**: Return the resulting filterbank as a list of lists, with each element rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\"\"\"\n    \n    if max_freq is None:\n        max_freq = fs / 2\n    \n    # Convert min and max frequencies to Mel scale\n    min_mel = 2595 * np.log10(1 + min_freq / 700)\n    max_mel = 2595 * np.log10(1 + max_freq / 700)\n    \n    # Generate n_filters + 2 points equally spaced in Mel\n    mels = np.linspace(min_mel, max_mel, num=n_filters + 2)\n    \n    # Convert Mel points back to Hertz\n    f_Hz = 700 * (10 ** (mels / 2595) - 1)\n    \n    # Compute bin frequencies\n    bin_freqs = np.arange(N // 2 + 1) * fs / N\n    \n    filterbank = []\n    \n    for i in range(n_filters):\n        f_left = f_Hz[i]\n        f_center = f_Hz[i + 1]\n        f_right = f_Hz[i + 2]\n        \n        # Calculate the mask for frequencies within [f_left, f_right]\n        mask = (bin_freqs >= f_left) & (bin_freqs <= f_right)\n        \n        # Compute slopes\n        left_slope = (bin_freqs - f_left) / (f_center - f_left)\n        right_slope = (f_right - bin_freqs) / (f_right - f_center)\n        \n        # Compute weights\n        weights = np.minimum(left_slope, right_slope)\n        weights = np.clip(weights, 0.0, None)\n        weights = np.where(mask, weights, 0.0)\n        \n        # Apply normalization\n        if normalize:\n            scale = 2.0 / (f_right - f_left)\n            weights *= scale\n        \n        # Round to 4 decimal places and append as list\n        rounded_weights = [round(w, 4) for w in weights.tolist()]\n        filterbank.append(rounded_weights)\n    \n    return filterbank\n```\n\n### Explanation\n1. **Frequency Conversion**: The given minimum and maximum frequencies are converted to the Mel scale to create a non-linear spacing that better reflects human hearing sensitivity.\n2. **Equally Spaced Mel Points**: These points are then used to generate the edges of the triangular filters. By converting back to Hertz, we ensure the filters are spaced appropriately on the frequency axis.\n3. **Triangular Filter Calculation**: Each filter is defined by three consecutive points in the converted Hertz values. The weight for each DFT bin is determined by its position relative to the triangular filter's edges.\n4. **Normalization**: Each filter is normalized to ensure that the area under the filter in Mel space is 1, which helps in maintaining consistent energy levels across different filters.\n5. **Efficiency**: The use of vectorized operations with NumPy ensures efficient computation of weights for all bins simultaneously, making the solution both concise and efficient.\n\nThis approach ensures that the resulting Mel filterbank matrix accurately transforms the power spectrum into Mel band energies, suitable for further processing in speech and audio applications."}
{"task_id": 309, "completion_id": 0, "completion": "To solve this problem, we need to implement the prediction formula for Factorization Machines (FM), which combines a linear model with pair-wise feature interactions. The goal is to compute the predictions for each sample in a given feature matrix using the provided bias, linear weights, and factor matrix.\n\n### Approach\nThe FM prediction formula consists of three components: a bias term, a linear term, and an interaction term. Here's a breakdown of each component and how they are computed:\n\n1. **Bias Term (w0):** This is a scalar added to each prediction.\n2. **Linear Term:** This is the dot product of the feature vector with the linear weight vector.\n3. **Interaction Term:** This term captures the pair-wise interactions between features using a factor matrix. It is computed using the sum of squared dot products of the feature vector with each column of the factor matrix, adjusted by the sum of squared features and factors.\n\nThe steps to compute these components efficiently using NumPy are as follows:\n1. Compute the linear term using matrix multiplication.\n2. Compute the interaction term by first calculating the matrix product of the feature matrix and the factor matrix. This product is then squared and summed over the columns. Additionally, compute the sum of squared features and squared factors, then adjust the interaction term accordingly.\n3. Combine all components and round the results to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef fm_predict(X: np.ndarray, w0: float, w: np.ndarray, v: np.ndarray) -> list[float]:\n    \"\"\"Return Factorization Machine predictions for all samples in *X*.\n\n    The function must implement the FM prediction formula using the bias *w0*,\n    the linear weights *w* and the factor matrix *v* and return a Python list\n    of floats rounded to four decimal places.\n    \"\"\"\n    # Compute the linear term\n    linear_term = X.dot(w) + w0\n    \n    # Compute the interaction term components\n    XV = X.dot(v)\n    sum_XV_sq = (XV ** 2).sum(axis=1)\n    \n    X_sq = X ** 2\n    V_sq_sum = (v ** 2).sum(axis=1)\n    sum_X_sq_V_sq = X_sq.dot(V_sq_sum)\n    \n    interaction = 0.5 * (sum_XV_sq - sum_X_sq_V_sq)\n    \n    # Combine all terms and round\n    predictions = linear_term + interaction\n    return [round(pred, 4) for pred in predictions.tolist()]\n```\n\n### Explanation\n1. **Linear Term Calculation:** The linear term is computed using a dot product between the feature matrix `X` and the linear weight vector `w`, plus the bias `w0`.\n2. **Interaction Term Calculation:** \n   - First, compute the product of the feature matrix `X` and the factor matrix `V`, resulting in a matrix where each element represents the interaction between a feature and a factor.\n   - Square each element of this product matrix and sum across columns to get the sum of squared interactions.\n   - Compute the element-wise squares of the feature matrix and the factor matrix. Sum the squared factors across columns and then compute the dot product with the squared features to adjust the interaction term.\n3. **Combining Terms:** The final predictions are obtained by summing the linear term and the interaction term. Each prediction is rounded to four decimal places to meet the specified precision requirement.\n\nThis approach efficiently leverages vectorized operations in NumPy to ensure the computations are both concise and performant."}
{"task_id": 310, "completion_id": 0, "completion": "To solve this problem, we need to compute the Root Mean Squared Error (RMSE) between two numeric sequences. The RMSE is a commonly used metric to evaluate the accuracy of predictions made by a model compared to the actual observed values. The function must validate the input data, compute the RMSE using the provided formula, and return the result rounded to four decimal places.\n\n### Approach\n1. **Input Validation**: Check if the input sequences (actual and predicted) are of the same non-zero length. If not, return -1.\n2. **Conversion to NumPy Arrays**: Convert the input lists or arrays to NumPy arrays for efficient numerical operations.\n3. **Compute Squared Differences**: Calculate the squared differences between the corresponding elements of the actual and predicted values.\n4. **Mean Squared Error (MSE)**: Compute the mean of these squared differences.\n5. **Root Mean Squared Error (RMSE)**: Take the square root of the MSE to get the RMSE.\n6. **Rounding**: Round the RMSE value to four decimal places and return it as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef root_mean_squared_error(actual: list[float] | np.ndarray, predicted: list[float] | np.ndarray) -> float:\n    \"\"\"Compute the Root Mean Squared Error (RMSE) between two numeric sequences.\n\n    The function returns ``-1`` if the two inputs do not have the same\n    non-zero length.\n\n    Args:\n        actual: A list or NumPy array containing the true values.\n        predicted: A list or NumPy array containing the predicted values.\n\n    Returns:\n        A float \u2013 the RMSE rounded to 4 decimal places, or ``-1`` on invalid\n        input.\n    \"\"\"\n    # Check if the inputs are of the same non-zero length\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1.0\n    \n    # Convert inputs to NumPy arrays to handle both lists and arrays uniformly\n    actual_arr = np.asarray(actual)\n    predicted_arr = np.asarray(predicted)\n    \n    # Calculate the squared differences\n    squared_errors = (actual_arr - predicted_arr) ** 2\n    \n    # Compute the mean of the squared errors\n    mse = np.mean(squared_errors)\n    \n    # Take the square root to get RMSE\n    rmse = np.sqrt(mse)\n    \n    # Round to 4 decimal places and return as a float\n    return round(rmse, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of the actual and predicted values are equal and non-zero. If not, it returns -1.0 immediately.\n2. **Conversion to Arrays**: The inputs are converted to NumPy arrays to handle both lists and arrays uniformly, leveraging NumPy's efficient vectorized operations.\n3. **Squared Differences**: The element-wise squared differences between actual and predicted values are computed.\n4. **Mean Squared Error**: The mean of these squared differences is calculated to get the MSE.\n5. **RMSE Calculation**: The square root of the MSE is taken to obtain the RMSE.\n6. **Rounding**: The result is rounded to four decimal places to meet the specified precision requirement, ensuring the output is a float.\n\nThis approach efficiently computes the RMSE using NumPy for numerical operations, ensuring both accuracy and performance while handling input validation gracefully."}
{"task_id": 311, "completion_id": 0, "completion": "To solve this problem, we need to create a function that takes a numeric container (either a Python list or a NumPy array) and replaces all negative values with 0.0. The result should be returned as a Python list with the same nested structure, containing only floating-point numbers. The original input should not be modified if it is a NumPy array.\n\n### Approach\n1. **Input Handling**: Convert the input to a NumPy array if it is a list. If the input is already a NumPy array, create a copy and convert it to a float type to avoid modifying the original array.\n2. **Vectorized Clipping**: Use NumPy's `clip` method to replace all negative values with 0.0. This method is efficient and operates on the entire array without explicit loops.\n3. **Conversion to List**: Convert the processed NumPy array back to a Python list using the `tolist()` method, ensuring the output maintains the original nested structure with all elements as floats.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef clip_non_negative(arr):\n    \"\"\"Replace every negative number in *arr* with 0.0.\n\n    Parameters\n    ----------\n    arr : list | numpy.ndarray\n        A list (possibly nested) or NumPy array containing numeric values.\n\n    Returns\n    -------\n    list\n        A Python list of the same shape where all negative values are replaced\n        by 0.0. All elements in the returned list are floats.\n    \"\"\"\n    if isinstance(arr, list):\n        np_arr = np.array(arr, dtype=float)\n    else:\n        np_arr = arr.astype(float)\n    clipped = np.clip(np_arr, a_min=0.0, a_max=None)\n    return clipped.tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input is first converted to a NumPy array. If the input is a list, `np.array(arr, dtype=float)` ensures it is converted to a float array. If the input is a NumPy array, `arr.astype(float)` creates a float copy, preserving the original array.\n2. **Clipping**: Using `np.clip` with `a_min=0.0` ensures all negative values are set to 0.0 in a vectorized manner, which is efficient for large or multi-dimensional data.\n3. **Output Conversion**: The `tolist()` method converts the processed NumPy array back to a Python list, maintaining the nested structure and ensuring all elements are floats.\n\nThis approach efficiently handles both lists and NumPy arrays, ensuring the original input is not modified and the output meets the specified requirements."}
{"task_id": 312, "completion_id": 0, "completion": "To solve this problem, we need to generate a Blackman-Harris window of arbitrary length, which can be either symmetric or periodic. The symmetric form is typically used for FIR filter design, while the periodic form is suitable for FFT-based spectral analysis. The periodic form ensures that the window tiles seamlessly when wrapped for an FFT.\n\n### Approach\n1. **Special Cases Handling**: If the window length is less than or equal to 0, return an empty list. If the window length is 1, return a list containing a single 1.0.\n2. **Coefficients Definition**: Use the predefined coefficients for the Blackman-Harris window.\n3. **Determine Parameters**:\n   - For the symmetric window, use `N = window_len - 1` and generate `window_len` samples.\n   - For the periodic window, generate `window_len + 1` samples using `N = window_len` and then discard the last sample to get the desired length.\n4. **Compute Window Values**: Apply the Blackman-Harris formula using vectorized operations for efficiency.\n5. **Rounding**: Round the computed values to 4 decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n    \n    if symmetric:\n        N = window_len - 1\n        n = np.arange(window_len)\n    else:\n        # Generate window_len + 1 samples, then discard the last\n        N = window_len\n        n = np.arange(window_len + 1)\n    \n    window = (\n        a0 \n        - a1 * np.cos(2 * np.pi * n / N) \n        + a2 * np.cos(4 * np.pi * n / N) \n        - a3 * np.cos(6 * np.pi * n / N)\n    )\n    \n    if not symmetric:\n        window = window[:-1]  # Discard the last sample for periodic\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(window, 4).tolist()\n```\n\n### Explanation\n- **Special Cases Handling**: Directly handle edge cases where the window length is non-positive or exactly 1.\n- **Coefficients Definition**: The coefficients for the Blackman-Harris window are fixed and provided in the problem statement.\n- **Parameter Determination**: For the symmetric window, the length parameter `N` is set to `window_len - 1`, while for the periodic window, `N` is set to `window_len` and an extra sample is generated and then discarded.\n- **Window Computation**: Using NumPy's vectorized operations ensures efficient computation of the window values based on the derived formula.\n- **Rounding**: The results are rounded to four decimal places to meet the output requirements and converted to a Python list for the final output.\n\nThis approach ensures that both symmetric and periodic forms of the Blackman-Harris window are generated efficiently and correctly, adhering to the problem constraints and requirements."}
{"task_id": 313, "completion_id": 0, "completion": "To solve this problem, we need to compute the expected value of the logarithm of a component of a Dirichlet-distributed random variable. The key challenge is implementing the digamma function without relying on external libraries, using a combination of recursion and asymptotic expansion for accurate approximation.\n\n### Approach\n1. **Digamma Function Implementation**: The digamma function, \u03c8(x), is computed using a recursive approach for small values of x (x < 6) and an asymptotic expansion for larger values. The recursion \u03c8(x) = \u03c8(x+1) - 1/x is used to shift small x values up to a moderate value (6). For x \u2265 6, the asymptotic expansion \u03c8(x) \u2248 ln(x) - 1/(2x) - 1/(12x\u00b2) + 1/(120x\u2074) - 1/(252x\u2076) is applied.\n\n2. **Extract Parameters**: Given a 2-D array (or list) of Dirichlet parameters, extract the relevant row and column values based on the provided indices.\n\n3. **Compute the Expected Value**: Using the digamma function, compute the expected value of the logarithm of the specified Dirichlet component and the sum of the components in the specified row. Subtract these two values and round the result to four decimal places.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    def digamma(x):\n        shift = 0.0\n        x = float(x)\n        while x < 6:\n            shift -= 1.0 / x\n            x += 1.0\n        ln_x = math.log(x)\n        inv_x = 1.0 / x\n        inv_x2 = inv_x * inv_x\n        inv_x4 = inv_x2 * inv_x2\n        inv_x6 = inv_x4 * inv_x2\n        result = ln_x - 0.5 * inv_x - (1.0 / 12.0) * inv_x2 + (1.0 / 120.0) * inv_x4 - (1.0 / 252.0) * inv_x6\n        return result + shift\n    \n    gamma_row = gamma[d]\n    sum_gamma = sum(gamma_row)\n    gamma_dt = gamma_row[t]\n    \n    term1 = digamma(gamma_dt)\n    term2 = digamma(sum_gamma)\n    \n    result = term1 - term2\n    return round(result, 4)\n```\n\n### Explanation\n1. **Digamma Function**: The function `digamma(x)` handles values of x by recursively shifting values less than 6 to higher values using the relation \u03c8(x) = \u03c8(x+1) - 1/x. For x \u2265 6, it uses an asymptotic expansion to approximate \u03c8(x) accurately.\n\n2. **Parameter Extraction**: The relevant row (gamma[d]) and column (gamma[d][t]) are extracted from the input matrix. The sum of the row elements is computed to get the denominator for the digamma function.\n\n3. **Result Calculation**: The difference between the digamma values of the specified component and the sum of the row elements is computed, then rounded to four decimal places for the final result.\n\nThis approach ensures that we accurately compute the expected value using mathematical approximations and recursion, adhering to the constraints of not using external libraries."}
{"task_id": 315, "completion_id": 0, "completion": "To solve this problem, we need to create a helper function that returns a ready-to-use loss function based on a given name. The function should recognize three widely-used loss functions: Mean Absolute Error (MAE), Mean Squared Error (MSE), and Binary Cross Entropy (BCE). The function should handle case-insensitive names and ignore underscores, and it should raise an error if the name is invalid.\n\n### Approach\n1. **Normalization of Input Name**: Convert the input name to lowercase and remove underscores to handle case insensitivity and underscore variations.\n2. **Loss Function Definitions**: Define the three loss functions (MAE, MSE, BCE) inside the helper function. Each loss function converts inputs to NumPy arrays, computes the loss, and returns the result rounded to four decimal places.\n3. **Mapping of Aliases**: Create a dictionary that maps normalized names to their corresponding loss functions. This dictionary includes all valid aliases for each loss function.\n4. **Lookup and Validation**: Check the normalized input name against the dictionary. If it exists, return the corresponding loss function; otherwise, raise an error.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_loss(name: str):\n    \"\"\"Return a loss function specified by *name*.\n\n    The function must recognise the following identifiers (case-insensitive,\n    underscores are ignored):\n        \u2022 MAE  \u2192  \"mae\" | \"mean_absolute_error\"\n        \u2022 MSE  \u2192  \"mse\" | \"mean_squared_error\"\n        \u2022 Binary Cross Entropy  \u2192  \"cross_entropy\" | \"log_loss\" | \"ce\"\n\n    The returned object has the signature `fn(y_true, y_pred) -> float` where\n    *y_true* and *y_pred* are 1-D sequences (list or NumPy array) of equal\n    length.  The resulting float must be rounded to **four** decimal places.\n\n    Args:\n        name: Name or alias of the desired loss function.\n\n    Returns:\n        A Python callable implementing the requested loss.\n\n    Raises:\n        ValueError: If *name* does not correspond to a supported loss.\n    \"\"\"\n    processed = name.lower().replace('_', '')\n    \n    def mae_loss(y_true, y_pred):\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n        loss = np.mean(np.abs(y_true - y_pred))\n        return round(float(loss), 4)\n    \n    def mse_loss(y_true, y_pred):\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n        loss = np.mean((y_true - y_pred) ** 2)\n        return round(float(loss), 4)\n    \n    def bce_loss(y_true, y_pred):\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        return round(float(loss), 4)\n    \n    loss_mapping = {\n        'mae': mae_loss,\n        'meanabsoluteerror': mae_loss,\n        'mse': mse_loss,\n        'meansquarederror': mse_loss,\n        'crossentropy': bce_loss,\n        'logloss': bce_loss,\n        'ce': bce_loss,\n    }\n    \n    if processed not in loss_mapping:\n        raise ValueError(\"Invalid loss function.\")\n    \n    return loss_mapping[processed]\n```\n\n### Explanation\n1. **Normalization**: The input name is converted to lowercase and underscores are removed to handle variations like \"MAE\", \"mean_absolute_error\", etc.\n2. **Loss Functions**:\n   - **MAE**: Computes the mean of absolute differences between true and predicted values.\n   - **MSE**: Computes the mean of squared differences between true and predicted values.\n   - **BCE**: Computes the cross-entropy loss after clipping predictions to avoid log(0). This is used for binary classification tasks.\n3. **Mapping**: A dictionary maps normalized names to their respective loss functions, ensuring all valid aliases point to the correct function.\n4. **Validation**: If the processed name is not found in the dictionary, a ValueError is raised. Otherwise, the corresponding loss function is returned. Each loss function converts inputs to NumPy arrays, computes the loss, and rounds the result to four decimal places."}
{"task_id": 316, "completion_id": 0, "completion": "To solve this problem, we need to compute the Euclidean (L2) distance between two vectors of equal length. The vectors can be Python lists, tuples, or 1-D NumPy arrays. If the vectors have different lengths, the function should return -1. Otherwise, it converts the inputs to NumPy arrays, computes the Euclidean distance, and returns the result rounded to four decimal places.\n\n### Approach\n1. **Check Vector Lengths**: First, verify if the input vectors have the same length. If not, return -1 immediately.\n2. **Convert to NumPy Arrays**: Convert both input vectors to NumPy arrays to handle all input types uniformly and leverage NumPy's efficient operations.\n3. **Compute Difference**: Calculate the element-wise difference between the two vectors.\n4. **Calculate Euclidean Distance**: Use NumPy's `linalg.norm` function to compute the L2 norm of the difference vector, which directly gives the Euclidean distance.\n5. **Round Result**: Round the computed distance to four decimal places and return it as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef euclidean(x, y):\n    \"\"\"Compute the Euclidean (L2) distance between two equal-length vectors.\n\n    Args:\n        x (list|tuple|np.ndarray): First real vector.\n        y (list|tuple|np.ndarray): Second real vector.\n\n    Returns:\n        float | int: The L2 distance rounded to four decimals, or -1 if the\n        vectors have unequal lengths.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    x_np = np.array(x)\n    y_np = np.array(y)\n    distance = np.linalg.norm(x_np - y_np)\n    return round(distance, 4)\n```\n\n### Explanation\n- **Check Vector Lengths**: The function starts by checking if the lengths of the input vectors are equal. This is done using the `len` function, which works for lists, tuples, and 1-D NumPy arrays.\n- **Convert to NumPy Arrays**: Converting the inputs to NumPy arrays allows us to use NumPy's optimized operations, ensuring efficient computation regardless of the input type.\n- **Compute Euclidean Distance**: The difference between the two vectors is calculated using element-wise subtraction. The `np.linalg.norm` function then computes the L2 norm of this difference vector, which is the Euclidean distance.\n- **Round Result**: The result is rounded to four decimal places using Python's `round` function to meet the specified precision requirement.\n\nThis approach ensures that the function handles various input types efficiently and correctly computes the Euclidean distance while adhering to the problem constraints."}
{"task_id": 317, "completion_id": 0, "completion": "To solve this problem, we need to generate a formatted debug error report that compares the predictions made by a program against the expected gold values. The report should include both the previous and current predictions and their corresponding gold values, formatted in a specific structure.\n\n### Approach\n1. **Determine Previous and Current Indices**: The previous index is calculated as the maximum of the given index minus one and zero to ensure it's within valid bounds. The current index is the given index.\n2. **Retrieve Predictions and Labels**: Extract the prediction and label for both the previous and current indices from the input parameters.\n3. **Fetch Gold Values**: Use the labels to fetch the corresponding gold values from the provided dictionary.\n4. **Construct the Report**: Format the header, previous predictions, current predictions, and footer into the specified structure, including handling optional warnings and ensuring the correct number of dashes and newlines.\n\n### Solution Code\n```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\"\"\"\n\n    prev_ix = max(ix - 1, 0)\n    prev_pred, prev_label = params[prev_ix]\n    prev_gold = golds[prev_label]\n\n    curr_pred, curr_label = params[ix]\n    curr_gold = golds[curr_label]\n\n    header = '-' * 25 + ' DEBUG ' + '-' * 25 + '\\n'\n    prev_mine = f\"Mine (prev) [{prev_label}]:\\n{prev_pred}\\n\"\n    prev_theirs = f\"Theirs (prev) [{prev_label}]:\\n{prev_gold}\\n\"\n    two_blanks = '\\n\\n'\n    current_mine = f\"Mine [{curr_label}]:\\n{curr_pred}\\n\"\n    current_theirs = f\"Theirs [{curr_label}]:\\n{curr_gold}{warn_str}\\n\"\n    footer = '-' * 23 + ' END DEBUG ' + '-' * 23 + '\\n'\n\n    return (header + prev_mine + prev_theirs + two_blanks +\n            current_mine + current_theirs + footer)\n```\n\n### Explanation\n1. **Header and Footer**: The header and footer lines are constructed with a specific number of dashes and the words \"DEBUG\" and \"END DEBUG\" respectively to meet the formatting requirements.\n2. **Previous Block**: The previous prediction and gold value are formatted with their corresponding labels. This block includes both the prediction made by the program and the expected gold value.\n3. **Current Block**: The current prediction and gold value are formatted similarly, with an optional warning string appended to the gold value line.\n4. **Blank Lines**: Two blank lines are inserted between the previous block and the current block to enhance readability.\n5. **String Construction**: All parts of the report are concatenated in the correct order to form the final output string, ensuring all newlines and dashes are placed exactly as specified.\n\nThis approach ensures that the generated report is both accurate and meets the exact formatting requirements specified in the problem statement."}
{"task_id": 318, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost.M1 algorithm from scratch using decision stumps as weak learners. The AdaBoost algorithm iteratively trains weak learners, adjusting the weights of training instances based on classification errors, and combines these learners into a strong classifier. \n\n### Approach\n1. **Initialization**: Start with uniform weights for all training samples.\n2. **Weak Learner Training**: For each boosting round, find the best decision stump (feature, threshold, and polarity) that minimizes the weighted classification error.\n3. **Error Calculation**: Compute the weighted error for the selected stump.\n4. **Weight Update**: Adjust the weights of misclassified samples to focus on harder cases in subsequent iterations.\n5. **Combine Weak Learners**: After training, combine the weak learners using weighted majority voting to form the final classifier.\n6. **Prediction**: Use the final ensemble of weak learners to predict the labels of the test data.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    n_samples, n_features = X_train.shape\n    D = np.ones(n_samples) / n_samples  # Initialize weights\n    stumps = []\n\n    for _ in range(n_estimators):\n        best_error = np.inf\n        best_j = -1\n        best_t = None\n        best_p = None\n\n        # Search for the best stump\n        for j in range(n_features):\n            sorted_indices = np.argsort(X_train[:, j])\n            X_j_sorted = X_train[sorted_indices, j]\n            y_sorted = y_train[sorted_indices]\n            D_sorted = D[sorted_indices]\n\n            mask_pos = (y_sorted == 1)\n            mask_neg = ~mask_pos\n\n            # Cumulative sums for left side\n            left_pos = np.cumsum(D_sorted * mask_pos)\n            left_neg = np.cumsum(D_sorted * mask_neg)\n\n            total_pos = left_pos[-1] if len(left_pos) > 0 else 0\n            total_neg = left_neg[-1] if len(left_neg) > 0 else 0\n\n            for k in range(len(X_j_sorted) - 1):\n                if X_j_sorted[k] == X_j_sorted[k + 1]:\n                    continue\n                t_candidate = (X_j_sorted[k] + X_j_sorted[k + 1]) / 2\n\n                current_left_pos = left_pos[k]\n                current_left_neg = left_neg[k]\n                current_right_pos = total_pos - current_left_pos\n                current_right_neg = total_neg - current_left_neg\n\n                error_p1 = current_left_pos + current_right_neg\n                error_m1 = current_left_neg + current_right_pos\n\n                if error_p1 < error_m1:\n                    current_error = error_p1\n                    current_p = 1\n                else:\n                    current_error = error_m1\n                    current_p = -1\n\n                if current_error < best_error:\n                    best_error = current_error\n                    best_j = j\n                    best_t = t_candidate\n                    best_p = current_p\n\n        # Handle case where no split was found (all features are constant)\n        if best_j == -1:\n            weighted_error_p1 = np.sum(D[y_train == -1])\n            weighted_error_m1 = np.sum(D[y_train == 1])\n            if weighted_error_p1 < weighted_error_m1:\n                best_p = 1\n                best_error = weighted_error_p1\n            else:\n                best_p = -1\n                best_error = weighted_error_m1\n            best_j = 0\n            best_t = -np.inf  # Ensure x_j >= t is always True\n\n        # Compute epsilon and alpha\n        epsilon_m = best_error\n        epsilon_m = max(epsilon_m, 1e-15)\n        epsilon_m = min(epsilon_m, 1 - 1e-15)\n        alpha_m = 0.5 * math.log((1 - epsilon_m) / epsilon_m)\n\n        # Update weights\n        h_m = best_p * np.where(X_train[:, best_j] >= best_t, 1, -1)\n        D *= np.exp(-alpha_m * y_train * h_m)\n        D_sum = D.sum()\n        if D_sum > 0:\n            D /= D_sum\n        else:\n            D = np.ones_like(D) / n_samples  # Reset weights if sum is zero (unlikely)\n\n        stumps.append((best_j, best_t, best_p, alpha_m))\n\n    # Predict on X_test\n    sum_alpha_h = np.zeros(X_test.shape[0])\n    for j, t, p, alpha in stumps:\n        h = p * np.where(X_test[:, j] >= t, 1, -1)\n        sum_alpha_h += alpha * h\n\n    y_pred = [1 if s >= 0 else -1 for s in sum_alpha_h]\n    return y_pred\n```\n\n### Explanation\n1. **Initialization**: The initial weights for all samples are set uniformly.\n2. **Weak Learner Training**: For each feature, sort the values and evaluate possible thresholds. Compute the weighted error for each potential split and select the best one.\n3. **Error Handling**: If no valid split is found (e.g., all features are constant), default to predicting the majority class based on current weights.\n4. **Weight Update**: Adjust weights to focus on misclassified samples using exponential updates, normalized to maintain a valid distribution.\n5. **Prediction**: Combine predictions from all weak learners using their respective weights to form the final prediction for test data.\n\nThis approach ensures that each weak learner corrects the errors of the previous ones, leading to a robust ensemble model."}
{"task_id": 319, "completion_id": 0, "completion": "To solve this problem, we need to implement a linear regression model using Stochastic Gradient Descent (SGD). The goal is to learn the weight vector \u03b8 that best fits the given feature matrix X and target vector y. The solution involves iteratively updating the weights using each training example to minimize the prediction error.\n\n### Approach\n1. **Data Preparation**: Convert the input feature matrix X and target vector y into NumPy arrays if they are provided as lists. This ensures compatibility with NumPy operations.\n2. **Bias Addition**: Prepend a column of ones to the feature matrix X to account for the bias term \u03b8\u2080 in the linear model.\n3. **Weight Initialization**: Initialize the weight vector \u03b8 with zeros, including the bias term.\n4. **Stochastic Gradient Descent**: For each epoch (pass over the entire dataset), iterate through each training example in the original order. For each example, compute the prediction, calculate the error, and update the weights using the gradient of the error.\n5. **Result Formatting**: After training, round the learned weights to four decimal places and return them as a standard Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef stochastic_gradient_descent(X: np.ndarray | list, y: np.ndarray | list, learning_rate: float = 0.01, epochs: int = 1000) -> list[float]:\n    \"\"\"Learn a linear model with Stochastic Gradient Descent.\"\"\"\n    \n    # Convert inputs to numpy arrays if they are lists\n    X = np.asarray(X)\n    y = np.asarray(y).flatten()  # Ensure y is 1D\n    \n    # Add bias column of ones to X\n    m = X.shape[0]\n    X_with_bias = np.c_[np.ones((m, 1)), X]\n    \n    # Initialize theta to zeros\n    n_features = X_with_bias.shape[1]\n    theta = np.zeros(n_features)\n    \n    # Perform SGD for the specified number of epochs\n    for _ in range(epochs):\n        for i in range(m):\n            x_i = X_with_bias[i]\n            y_i = y[i]\n            prediction = np.dot(theta, x_i)\n            error = prediction - y_i\n            theta -= learning_rate * error * x_i\n    \n    # Round the theta values to four decimal places and return as a list\n    theta_rounded = np.round(theta, 4).tolist()\n    return theta_rounded\n```\n\n### Explanation\n1. **Data Conversion**: The input data (X and y) are converted to NumPy arrays to leverage efficient array operations. The target vector y is flattened to ensure it is a 1D array.\n2. **Bias Term**: A column of ones is added to the feature matrix X to incorporate the bias term \u03b8\u2080, allowing it to be learned alongside other weights.\n3. **Weight Initialization**: The weight vector \u03b8 is initialized to zeros, which includes the bias term and all other weights.\n4. **SGD Training**: For each epoch, the algorithm iterates through each training example, computes the prediction and error for that example, and updates the weights using the gradient of the error. This process is repeated for the specified number of epochs.\n5. **Result Formatting**: The learned weights are rounded to four decimal places and converted to a Python list to meet the output requirements.\n\nThis approach efficiently updates the weights using each training example individually, making it suitable for large datasets and allowing the model to converge to an optimal solution with appropriate hyperparameters (learning rate and number of epochs)."}
{"task_id": 321, "completion_id": 0, "completion": "To solve this problem, we need to implement a Genetic Algorithm (GA) that evolves a population of random strings towards a given target string. The GA follows a cycle of population initialization, fitness evaluation, selection, crossover, mutation, and replacement. The goal is to find the string that most closely matches the target string based on a fitness score.\n\n### Approach\n1. **Population Initialization**: Create a population of random strings, each of the same length as the target string. Characters are chosen from a valid set including space, lowercase letters, and uppercase letters.\n\n2. **Fitness Evaluation**: Compute the fitness of each individual in the population. The fitness is inversely proportional to the sum of the absolute differences between the indices of the characters in the individual and the target string. The indices are defined such that space is 0, 'a'-'z' are 1-26, and 'A'-'Z' are 27-52.\n\n3. **Selection**: Select parents using roulette-wheel sampling without replacement. This means each individual can be selected only once, with a probability proportional to their fitness.\n\n4. **Crossover**: For each pair of parents, produce two children by exchanging suffixes at a randomly chosen crossover point.\n\n5. **Mutation**: Each character in the new children has a probability of being replaced by a random valid character, based on the mutation rate.\n\n6. **Replacement**: The new children form the next generation, replacing the old population.\n\n7. **Termination**: The algorithm stops after a specified number of iterations or if the target string is found.\n\n### Solution Code\n```python\nimport numpy as np\nimport string\n\ndef genetic_algorithm(target: str, population_size: int, mutation_rate: float, iterations: int) -> str:\n    np.random.seed(42)\n    characters = [' '] + list(string.ascii_lowercase) + list(string.ascii_uppercase)\n    char_to_index = {c: idx for idx, c in enumerate(characters)}\n    target_len = len(target)\n    target_indices = [char_to_index[c] for c in target]\n    \n    # Initialize population\n    population = [''.join(np.random.choice(characters, size=target_len)) for _ in range(population_size)]\n    best_individual = None\n    best_fitness = -np.inf\n    \n    for _ in range(iterations):\n        # Evaluate fitness\n        fitness = []\n        for individual in population:\n            sum_diff = 0\n            for i, c in enumerate(individual):\n                sum_diff += abs(char_to_index[c] - target_indices[i])\n            ind_fitness = 1.0 / (sum_diff + 1e-6)\n            fitness.append(ind_fitness)\n            if ind_fitness > best_fitness:\n                best_fitness = ind_fitness\n                best_individual = individual\n        # Early stopping if target is found\n        if best_individual == target:\n            break\n        \n        # Selection (roulette-wheel without replacement)\n        parents = []\n        remaining_pop = population.copy()\n        remaining_fit = fitness.copy()\n        total_fit = sum(remaining_fit)\n        for _ in range(population_size):\n            if total_fit <= 0:\n                chosen_idx = np.random.choice(len(remaining_pop))\n            else:\n                probs = [f / total_fit for f in remaining_fit]\n                chosen_idx = np.random.choice(len(remaining_pop), p=probs)\n            parents.append(remaining_pop[chosen_idx])\n            total_fit -= remaining_fit[chosen_idx]\n            del remaining_pop[chosen_idx]\n            del remaining_fit[chosen_idx]\n        \n        # Crossover\n        next_generation = []\n        for i in range(population_size // 2):\n            parent1 = parents[2 * i]\n            parent2 = parents[2 * i + 1]\n            crossover_point = np.random.randint(0, target_len + 1)\n            child1 = parent1[:crossover_point] + parent2[crossover_point:]\n            child2 = parent2[:crossover_point] + parent1[crossover_point:]\n            next_generation.extend([child1, child2])\n        \n        # Mutation\n        mutated_next_gen = []\n        for child in next_generation:\n            mutated = []\n            for c in child:\n                if np.random.random() < mutation_rate:\n                    mutated.append(np.random.choice(characters))\n                else:\n                    mutated.append(c)\n            mutated_next_gen.append(''.join(mutated))\n        \n        # Update population for next iteration\n        population = mutated_next_gen\n    \n    return best_individual\n```\n\n### Explanation\n1. **Population Initialization**: The initial population is created by generating random strings of the same length as the target. Each character is chosen from a valid set of characters.\n\n2. **Fitness Evaluation**: For each individual, the fitness is calculated using the sum of absolute differences between the indices of the characters in the individual and the target. The fitness is the reciprocal of this sum, ensuring higher values for closer matches.\n\n3. **Selection**: Parents are selected using roulette-wheel sampling without replacement. This ensures that each individual is selected exactly once, with probabilities proportional to their fitness.\n\n4. **Crossover**: Each pair of parents produces two children by exchanging suffixes at a random crossover point. This helps in exploring different combinations of parent genes.\n\n5. **Mutation**: Each character in the children has a chance to be replaced by a random valid character, introducing genetic diversity and preventing premature convergence.\n\n6. **Replacement**: The new generation replaces the old population, and the process repeats until the target is found or the maximum number of iterations is reached.\n\nThis approach efficiently evolves the population towards the target string by iteratively improving the fitness of individuals through selection, crossover, and mutation."}
{"task_id": 322, "completion_id": 0, "completion": "To solve this problem, we need to implement the binary cross-entropy (BCE) loss function, which is a common loss function used in machine learning for binary classification tasks. The goal is to compute the mean BCE loss given ground-truth binary labels and predicted probabilities, while ensuring numerical stability by clipping the predicted probabilities to avoid log(0) operations.\n\n### Approach\n1. **Clip Predicted Probabilities**: To prevent numerical instability when dealing with logarithms of 0 or 1, we clip the predicted probabilities to a small interval around these values. Specifically, we use a small constant \u03b5 (epsilon) to clip the probabilities to the interval [\u03b5, 1-\u03b5].\n2. **Compute Logarithmic Terms**: For each element in the actual and clipped predicted arrays, compute the two terms of the BCE formula: one involving the logarithm of the predicted probability and another involving the logarithm of one minus the predicted probability.\n3. **Sum and Average**: Sum the computed terms for all elements, then take the negative average of this sum to obtain the mean BCE loss.\n4. **Round the Result**: Finally, round the computed loss to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\nEPS = 1e-15  # small constant for numerical stability\n\ndef binary_crossentropy(actual: np.ndarray, predicted: np.ndarray) -> float:\n    \"\"\"Compute the mean binary cross-entropy loss.\n\n    Parameters\n    ----------\n    actual : np.ndarray\n        Ground-truth labels containing only 0s and 1s.\n    predicted : np.ndarray\n        Predicted probabilities for the positive class. Must have the same\n        shape as `actual`.\n\n    Returns\n    -------\n    float\n        Mean BCE value rounded to four decimal places.\n    \"\"\"\n    # Clip predicted values to avoid numerical instability\n    predicted_clipped = np.clip(predicted, EPS, 1 - EPS)\n    \n    # Calculate the terms of the BCE formula\n    term1 = actual * np.log(predicted_clipped)\n    term2 = (1 - actual) * np.log(1 - predicted_clipped)\n    \n    # Sum all terms and compute the mean loss\n    total_loss = -np.sum(term1 + term2) / actual.size\n    \n    # Round to four decimal places and return as a float\n    return round(total_loss, 4)\n```\n\n### Explanation\n1. **Clipping Predicted Values**: The `np.clip` function ensures that all predicted probabilities are within the interval [\u03b5, 1-\u03b5], avoiding the logarithm of 0 or 1 which can cause numerical instability.\n2. **Logarithmic Calculations**: The terms `term1` and `term2` compute the contributions of each element to the BCE loss using vectorized operations for efficiency. These terms correspond to the two parts of the BCE formula, one for the positive class (actual=1) and one for the negative class (actual=0).\n3. **Summing and Averaging**: The sum of all individual contributions is computed using `np.sum`, and the mean loss is obtained by dividing by the total number of elements in the actual array (using `actual.size` to handle any shape of input arrays).\n4. **Rounding**: The final loss value is rounded to four decimal places to meet the specified precision requirement, ensuring the result is returned as a Python float.\n\nThis approach efficiently computes the BCE loss using NumPy's vectorized operations, ensuring both numerical stability and computational efficiency."}
{"task_id": 324, "completion_id": 0, "completion": "To solve this problem, we need to implement bilinear interpolation for arbitrary (x, y) positions in a 2-D image. Bilinear interpolation is a method to estimate the pixel values at non-integer coordinates by considering the weighted average of the four nearest pixels. The solution must handle edge cases such as coordinates outside the image boundaries and different image channel configurations (e.g., grayscale, RGB).\n\n### Approach\n1. **Check Input Validity**: Ensure the input lists `x` and `y` have the same length. If not, return an empty list.\n2. **Clip Coordinates**: Clip the given coordinates to ensure they lie within the valid range of the image dimensions.\n3. **Determine Neighboring Pixels**: For each clipped coordinate, find the four nearest integer grid points (pixels) around the given coordinate.\n4. **Compute Weights**: Calculate the weights for interpolation based on the distances from the given coordinate to the neighboring pixels.\n5. **Bilinear Interpolation**: Perform linear interpolation in both the horizontal (x) and vertical (y) directions using the computed weights.\n6. **Round Results**: Round the interpolated values to four decimal places and convert them to a list of lists as required.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bilinear_interpolate(X: np.ndarray, x: list[float], y: list[float]) -> list[list[float]]:\n    if len(x) != len(y):\n        return []\n    \n    rows = X.shape[0]\n    cols = X.shape[1]\n    channels = X.shape[2]\n    result = []\n    \n    for xi, yi in zip(x, y):\n        xi_clipped = max(0.0, min(float(xi), cols - 1.0)\n        yi_clipped = max(0.0, min(float(yi), rows - 1.0)\n        \n        # Compute x components\n        x_floor = int(np.floor(xi_clipped))\n        x_ceil = x_floor + 1\n        if x_ceil >= cols:\n            x_ceil = cols - 1\n        dx = xi_clipped - x_floor\n        \n        # Compute y components\n        y_floor = int(np.floor(yi_clipped))\n        y_ceil = y_floor + 1\n        if y_ceil >= rows:\n            y_ceil = rows - 1\n        dy = yi_clipped - y_floor\n        \n        # Get neighboring pixels\n        top_left = X[y_floor, x_floor, :]\n        top_right = X[y_floor, x_ceil, :]\n        bottom_left = X[y_ceil, x_floor, :]\n        bottom_right = X[y_ceil, x_ceil, :]\n        \n        # Interpolate in x direction\n        top = top_left * (1 - dx) + top_right * dx\n        bottom = bottom_left * (1 - dx) + bottom_right * dx\n        \n        # Interpolate in y direction\n        interpolated = top * (1 - dy) + bottom * dy\n        \n        # Round and append\n        rounded = np.round(interpolated, 4).tolist()\n        result.append(rounded)\n    \n    return result\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of `x` and `y` are equal. If not, it returns an empty list immediately.\n2. **Coordinate Clipping**: Each coordinate is clipped to ensure it lies within the valid range of the image dimensions, preventing out-of-bounds errors.\n3. **Neighboring Pixels**: For each clipped coordinate, the integer parts (floors) and the next higher integers (ceils) are computed. These values determine the four neighboring pixels used in interpolation.\n4. **Weight Calculation**: The weights (dx and dy) represent the fractional parts of the coordinates, which determine the contribution of each neighboring pixel.\n5. **Bilinear Interpolation**: The interpolation is performed in two steps: first horizontally between the left and right pixels, then vertically between the top and bottom results.\n6. **Rounding and Conversion**: The interpolated values are rounded to four decimal places and converted to a list of lists to meet the output requirements.\n\nThis approach ensures accurate and efficient bilinear interpolation, handling various edge cases and different image channel configurations seamlessly."}
{"task_id": 326, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svm_predict(\n    X: list[list[float]],\n    support_vectors: list[list[float]],\n    lagr_multipliers: list[float],\n    support_vector_labels: list[int],\n    intercept: float,\n    kernel: str = \"rbf\",\n    power: int = 3,\n    gamma: float | None = None,\n    coef: float = 1.0,\n) -> list[int]:\n    \"\"\"Predict labels for a batch of samples using a pre-trained SVM.\"\"\"\n    \n    # Convert input lists to numpy arrays\n    support_vectors = np.array(support_vectors)\n    X = np.array(X)\n    \n    # Number of features in the support vectors\n    n_features = support_vectors.shape[1]\n    \n    # Set default gamma if not provided\n    if gamma is None:\n        gamma = 1.0 / n_features\n    \n    # Compute alpha_i * y_i for all support vectors\n    alpha_y = np.array(lagr_multipliers) * np.array(support_vector_labels)\n    \n    # Compute the kernel matrix\n    if kernel == \"linear\":\n        kernel_matrix = support_vectors @ X.T\n    elif kernel == \"polynomial\":\n        linear_part = support_vectors @ X.T\n        kernel_matrix = (gamma * linear_part + coef) ** power\n    elif kernel == \"rbf\":\n        # Squared Euclidean distances between support vectors and samples\n        S_sq = np.sum(support_vectors**2, axis=1)[:, np.newaxis]\n        X_sq = np.sum(X**2, axis=1)\n        distances = S_sq + X_sq - 2 * support_vectors @ X.T\n        kernel_matrix = np.exp(-gamma * distances)\n    else:\n        raise ValueError(f\"Invalid kernel: {kernel}\")\n    \n    # Compute decision values and determine labels\n    decision_values = alpha_y @ kernel_matrix + intercept\n    labels = np.where(decision_values >= 0, 1, -1).tolist()\n    \n    return labels\n```"}
{"task_id": 328, "completion_id": 0, "completion": "To solve this problem, we need to find the first word in a given list that starts with an uppercase letter and return it in a list. If no such word exists, we return an empty list.\n\n### Approach\n1. **Iterate through the List**: We loop through each word in the given list (corpus) one by one.\n2. **Check Conditions**: For each word, we first check if the word is non-empty (to avoid errors when accessing the first character). Then, we check if the first character of the word is uppercase using the `isupper()` method.\n3. **Return Result**: As soon as we find the first word that meets both conditions, we return it in a list. If the loop completes without finding any such word, we return an empty list.\n\n### Solution Code\n```python\nfrom typing import List\n\ndef first_capitalized_word(corpus: List[str]) -> List[str]:\n    \"\"\"Find the first capitalized word in *corpus* and return it in a list.\n\n    A *capitalized* word is one whose very first character is an uppercase\n    letter. If no word in the corpus meets this condition, return an empty\n    list instead.\n\n    Args:\n        corpus: List of candidate words.\n\n    Returns:\n        List containing the first capitalized word, or an empty list if none\n        exists.\n    \"\"\"\n    for word in corpus:\n        if word and word[0].isupper():\n            return [word]\n    return []\n```\n\n### Explanation\n- **Iteration**: The function iterates through each word in the input list `corpus`.\n- **Non-empty Check**: The condition `if word` ensures that we skip any empty strings to avoid runtime errors when accessing the first character.\n- **Uppercase Check**: The condition `word[0].isupper()` checks if the first character of the word is uppercase.\n- **Early Return**: The function returns immediately upon finding the first valid word, ensuring efficiency. If no valid word is found after checking all elements, an empty list is returned.\n\nThis approach efficiently handles all edge cases, including empty lists, lists with non-empty but lowercase words, and lists with valid capitalized words. The solution is optimal with a time complexity of O(n), where n is the number of words in the corpus."}
{"task_id": 329, "completion_id": 0, "completion": "To solve this problem, we need to compute the center frequencies of DFT bins for a given number of DFT coefficients and sampling frequency. The solution must handle both cases where only non-negative frequencies are required and where all frequencies (including negative ones) are needed, arranged in a specific order similar to NumPy's `fftfreq`.\n\n### Approach\n1. **Input Validation**: Check if the number of DFT coefficients (N) or the sampling frequency (fs) is non-positive. If either is non-positive, return an empty list.\n2. **Non-negative Frequencies (positive_only=True)**:\n   - Compute the frequency spacing (delta) as the ratio of the sampling frequency (fs) to the number of coefficients (N).\n   - Generate frequencies for indices from 0 to floor(N/2) using the formula \\( f_k = k \\cdot \\Delta f \\), where \\( \\Delta f = \\frac{fs}{N} \\).\n3. **All Frequencies (positive_only=False)**:\n   - Use NumPy's `fftfreq` to generate the frequency bins, then scale these frequencies by the sampling frequency (fs) to convert them to Hz.\n4. **Rounding**: Round all computed frequencies to 4 decimal places and return them as a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    if N <= 0 or fs <= 0:\n        return []\n    \n    if positive_only:\n        max_k = N // 2\n        k_values = np.arange(max_k + 1)\n        freqs = k_values * (fs / N)\n    else:\n        freqs = np.fft.fftfreq(N) * fs\n    \n    rounded_freqs = np.round(freqs, 4)\n    return rounded_freqs.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if N or fs is non-positive, returning an empty list if so.\n2. **Non-negative Frequencies**: When `positive_only` is True, the function calculates the frequency for each index from 0 to floor(N/2). This is done by generating an array of indices up to the midpoint of N and multiplying each by the frequency spacing (fs/N).\n3. **All Frequencies**: When `positive_only` is False, the function uses NumPy's `fftfreq` to generate normalized frequency bins, which are then scaled by the sampling frequency (fs) to convert them to Hz. This ensures the correct order of positive followed by negative frequencies.\n4. **Rounding**: The computed frequencies are rounded to 4 decimal places using NumPy's `round` function and converted to a Python list using `tolist()` for the required output format.\n\nThis approach efficiently handles both modes of operation and ensures accurate frequency bin computation with proper rounding and formatting."}
{"task_id": 330, "completion_id": 0, "completion": "To solve this problem, we need to update the parameters and hyperparameters of a kernel object based on a summary dictionary. The summary dictionary may contain nested dictionaries under the keys \"parameters\" and \"hyperparameters\", which need to be merged into the top level. The goal is to apply these updates without modifying the original dictionaries and ignore any keys that do not exist in the original parameters or hyperparameters.\n\n### Approach\n1. **Create Copies**: First, create copies of the original parameters and hyperparameters to avoid mutating the original dictionaries.\n2. **Flatten the Summary Dictionary**: Merge the nested dictionaries under \"parameters\" and \"hyperparameters\" into the top level of the summary dictionary. This involves combining all top-level keys (except \"parameters\" and \"hyperparameters\") with the contents of the nested dictionaries.\n3. **Apply Updates**: Iterate over the flattened summary dictionary and update the copies of parameters and hyperparameters based on the keys present in the original dictionaries. Ignore any keys that do not exist in the original dictionaries.\n\n### Solution Code\n```python\ndef update_kernel_settings(parameters: dict, hyperparameters: dict, summary_dict: dict) -> tuple[dict, dict]:\n    \"\"\"Update kernel parameters and hyperparameters.\n\n    The function must not modify the input dictionaries.  It should return two\n    *new* dictionaries in which the supplied `summary_dict` updates have been\n    applied following the rules explained in the task description.\n\n    Args:\n        parameters (dict):      Existing kernel parameters.\n        hyperparameters (dict): Existing kernel hyperparameters.\n        summary_dict (dict):    Dictionary that may contain the keys\n            \"parameters\" and/or \"hyperparameters\" whose values are dictionaries\n            of updates.  Any top-level keys are also treated as updates.\n\n    Returns:\n        tuple[dict, dict]: `(new_parameters, new_hyperparameters)` \u2013 the\n        updated dictionaries.\n    \"\"\"\n    new_parameters = parameters.copy()\n    new_hyperparameters = hyperparameters.copy()\n    \n    # Flatten the summary_dict\n    flattened = {}\n    \n    # Add top-level keys except 'parameters' and 'hyperparameters'\n    for key in summary_dict:\n        if key not in ('parameters', 'hyperparameters'):\n            flattened[key] = summary_dict[key]\n    \n    # Merge parameters and hyperparameters from summary_dict into flattened\n    if 'parameters' in summary_dict:\n        for k, v in summary_dict['parameters'].items():\n            flattened[k] = v\n    if 'hyperparameters' in summary_dict:\n        for k, v in summary_dict['hyperparameters'].items():\n            flattened[k] = v\n    \n    # Apply updates to the new dictionaries\n    for key, value in flattened.items():\n        if key in new_parameters:\n            new_parameters[key] = value\n        elif key in new_hyperparameters:\n            new_hyperparameters[key] = value\n    \n    return new_parameters, new_hyperparameters\n```\n\n### Explanation\n1. **Create Copies**: The original parameters and hyperparameters are copied to new dictionaries to ensure the originals are not modified.\n2. **Flatten the Summary Dictionary**: The summary dictionary is processed to merge any nested dictionaries under \"parameters\" and \"hyperparameters\" into the top level. This is done by iterating over the summary dictionary and combining all top-level keys (excluding \"parameters\" and \"hyperparameters\") with the contents of the nested dictionaries.\n3. **Apply Updates**: The flattened dictionary is iterated over, and each key-value pair is checked against the copies of parameters and hyperparameters. If a key exists in the parameters copy, it is updated; if it exists in the hyperparameters copy, it is updated. Keys not present in either are ignored.\n\nThis approach ensures that all updates from the summary dictionary are applied correctly while preserving the original dictionaries and ignoring irrelevant keys."}
{"task_id": 331, "completion_id": 0, "completion": "To solve this problem, we need to compute the Out-of-Bag Mean Squared Error (OOB-MSE) for a Random Forest regressor. This involves using predictions from trees that did not include specific samples during their training phase to evaluate the model's performance.\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Ensure that all input parameters (y_true, predictions, oob_mask) are converted to numpy arrays for efficient numerical operations.\n2. **Compute OOB Counts and Valid Samples**: For each sample, determine how many trees considered it out-of-bag (OOB). Identify samples that have at least one OOB prediction.\n3. **Handle Edge Case**: If no samples have OOB predictions, return -1 as specified.\n4. **Compute OOB Predictions Sum**: Calculate the sum of OOB predictions for each sample using element-wise multiplication with the OOB mask.\n5. **Calculate Averages**: For valid samples (those with OOB predictions), compute the average of their OOB predictions.\n6. **Compute Squared Errors**: Determine the squared error between the true values and the averaged OOB predictions for each valid sample.\n7. **Calculate MSE**: Compute the mean of these squared errors and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\"\"\"\n    y_true = np.asarray(y_true)\n    predictions = np.asarray(predictions)\n    oob_mask = np.asarray(oob_mask).astype(bool)\n    \n    oob_counts = oob_mask.sum(axis=1)\n    valid_samples = oob_counts > 0\n    \n    if not np.any(valid_samples):\n        return -1.0\n    \n    oob_sums = (predictions * oob_mask).sum(axis=1)\n    averages = oob_sums[valid_samples] / oob_counts[valid_samples]\n    \n    squared_errors = (y_true[valid_samples] - averages) ** 2\n    mse = squared_errors.mean()\n    \n    return round(mse, 4)\n```\n\n### Explanation\n1. **Input Conversion**: The inputs are converted to numpy arrays to leverage numpy's efficient array operations.\n2. **OOB Counts**: By summing the OOB mask along the tree axis (axis=1), we determine how many OOB predictions each sample has.\n3. **Valid Samples Identification**: Samples with at least one OOB prediction are identified using a boolean mask.\n4. **Edge Case Handling**: If no valid samples are found, the function returns -1 immediately.\n5. **Sum and Average Calculation**: The sum of OOB predictions for each sample is computed using element-wise multiplication with the OOB mask. The average prediction for valid samples is then derived by dividing the sum by the count of OOB predictions.\n6. **Squared Error Calculation**: The squared error for each valid sample is computed using the true values and the averaged predictions.\n7. **MSE Calculation**: The mean of these squared errors is computed and rounded to four decimal places to produce the final result.\n\nThis approach efficiently handles the computation using vectorized operations, ensuring both accuracy and performance."}
{"task_id": 332, "completion_id": 0, "completion": "To solve this problem, we need to construct a Frequent-Pattern Tree (FP-Tree) from a list of transactions without using any classes. The FP-Tree will be represented as a nested dictionary where each node contains a support count and a dictionary of its child nodes. The construction of the tree follows specific steps to ensure efficiency and correctness.\n\n### Approach\n1. **Count Item Supports**: First, we count the occurrence of each item in the transactions to determine their support. This is done by iterating through each transaction and counting each unique item once per transaction.\n2. **Filter and Sort Items**: Items that do not meet the minimum support threshold are discarded. The remaining items are sorted first by decreasing support and then alphabetically to establish a global order.\n3. **Process Transactions**: Each transaction is processed to remove duplicates, filter out infrequent items, and sort the remaining items according to the global order.\n4. **Build the FP-Tree**: Using the processed transactions, we construct the FP-Tree. Starting from the root node, we increment the support count of existing nodes or create new nodes as we traverse through each transaction.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\"\"\"\n    # Step 1: Compute the support for each item\n    item_counter = Counter()\n    for transaction in transactions:\n        unique_items = set(transaction)\n        item_counter.update(unique_items)\n    \n    # Step 2: Filter items by min_support and create global ordering\n    frequent_items = {item: count for item, count in item_counter.items() if count >= min_support}\n    if not frequent_items:\n        return {\"support\": 0, \"children\": {}}\n    \n    # Create the global order sorted by decreasing support, then alphabetically\n    sorted_items = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n    order_dict = {item: idx for idx, item in enumerate(sorted_items)}\n    \n    # Initialize the root of the FP-Tree\n    root = {\"support\": 0, \"children\": {}}\n    \n    # Step 3: Process each transaction to build the tree\n    for transaction in transactions:\n        # Remove duplicates and filter infrequent items\n        unique_trans = set(transaction)\n        filtered = [item for item in unique_trans if item in frequent_items]\n        if not filtered:\n            continue\n        \n        # Sort the transaction items according to the global order\n        filtered_sorted = sorted(filtered, key=lambda x: order_dict[x])\n        \n        # Insert into the FP-Tree\n        current_node = root\n        for item in filtered_sorted:\n            children = current_node[\"children\"]\n            if item in children:\n                children[item][\"support\"] += 1\n                current_node = children[item]\n            else:\n                new_node = {\"support\": 1, \"children\": {}}\n                children[item] = new_node\n                current_node = new_node\n    \n    return root\n```\n\n### Explanation\n1. **Counting Item Supports**: We use a `Counter` to track how many times each item appears across all transactions. Each item is counted once per transaction, regardless of duplicates within the same transaction.\n2. **Filtering and Sorting**: Items that do not meet the minimum support threshold are excluded. The remaining items are sorted to create a global order, which ensures that items with higher support come first, and alphabetical order breaks ties.\n3. **Processing Transactions**: Each transaction is converted to a set to remove duplicates, filtered to retain only frequent items, and sorted according to the global order. This processed transaction is then used to update the FP-Tree.\n4. **Building the FP-Tree**: Starting from the root node, each item in the processed transaction is either added as a new node or increments the support count of an existing node. This ensures that the tree structure efficiently represents frequent item sets while maintaining their support counts.\n\nThis approach efficiently constructs the FP-Tree by leveraging sorting and dictionary operations, ensuring that the tree is both compact and representative of the frequent patterns in the transactions."}
{"task_id": 333, "completion_id": 0, "completion": "To solve this problem, we need to implement a logistic regression classifier from scratch using batch gradient descent. The goal is to train the classifier on a given dataset and return the predicted class labels for the same dataset using a 0.5 threshold.\n\n### Approach\n1. **Standardize Features**: Normalize each feature column of the input matrix using z-score normalization to ensure that each feature has a mean of 0 and a standard deviation of 1. This helps in speeding up the convergence of gradient descent.\n2. **Augment with Bias Column**: Add a column of ones to the normalized feature matrix to account for the bias term in the logistic regression model.\n3. **Initialize Parameters**: Initialize the model parameters (theta) to zeros.\n4. **Batch Gradient Descent**: Iteratively update the parameters using the batch gradient descent algorithm. Compute the hypothesis using the sigmoid function, calculate the gradient of the cost function, and update the parameters accordingly.\n5. **Predict Class Labels**: After training the model, compute the probabilities for each training example and convert these probabilities to class labels using a 0.5 threshold.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression(X: np.ndarray, y: np.ndarray, alpha: float = 0.1, num_iter: int = 10000) -> list[int]:\n    \"\"\"Train a logistic regression classifier on the provided dataset and\n    return the predicted labels for *the same* dataset.\n\n    Args:\n        X: 2-D NumPy array of shape (m, n) containing the training examples.\n        y: 1-D NumPy array of length m with binary labels (0 or 1).\n        alpha: Learning rate for gradient descent. Defaults to 0.1.\n        num_iter: Number of iterations for gradient descent. Defaults to 10000.\n\n    Returns:\n        A Python list of length *m* holding the predicted class labels (0/1)\n        produced by the trained classifier.\n    \"\"\"\n    # Standardize the feature columns\n    mean_X = np.mean(X, axis=0)\n    std_X = np.std(X, axis=0)\n    X_normalized = (X - mean_X) / (std_X + 1e-8)\n    \n    # Augment with a bias column of ones\n    m = X.shape[0]\n    X_augmented = np.hstack([np.ones((m, 1)), X_normalized])\n    \n    # Initialize model parameters to zeros\n    theta = np.zeros(X_augmented.shape[1])\n    \n    # Perform batch gradient descent\n    for _ in range(num_iter):\n        z = X_augmented.dot(theta)\n        h = 1 / (1 + np.exp(-z))\n        gradient = (X_augmented.T.dot(h - y)) / m\n        theta -= alpha * gradient\n    \n    # Compute final predictions\n    z_final = X_augmented.dot(theta)\n    probabilities = 1 / (1 + np.exp(-z_final))\n    predicted_labels = (probabilities >= 0.5).astype(int)\n    \n    return predicted_labels.tolist()\n```\n\n### Explanation\n1. **Standardization**: Each feature column is standardized by subtracting its mean and dividing by its standard deviation (with a small constant to avoid division by zero). This ensures that all features contribute equally to the model training.\n2. **Bias Augmentation**: A column of ones is added to the feature matrix to incorporate the bias term (intercept) in the model.\n3. **Parameter Initialization**: The model parameters are initialized to zeros, which is a common starting point for optimization algorithms.\n4. **Gradient Descent**: The parameters are updated iteratively using the entire training dataset (batch gradient descent). The gradient of the cost function is computed using matrix operations for efficiency.\n5. **Prediction**: After training, the model computes the probabilities for each training example and converts these probabilities to class labels using a threshold of 0.5. The results are returned as a Python list of integers.\n\nThis approach ensures that the logistic regression model is trained efficiently and can make accurate predictions on the training data, particularly for linearly separable datasets."}
{"task_id": 334, "completion_id": 0, "completion": "To solve this problem, we need to implement the Noam learning-rate scheduler, which is commonly used in training Transformer models. The learning rate follows a specific formula that increases linearly during a warm-up period and then decays proportionally to the inverse square root of the step number.\n\n### Approach\nThe Noam learning rate schedule is defined by the formula:\n\\[ \\text{lr} = \\text{scale\\_factor} \\times \\text{model\\_dim}^{-0.5} \\times \\min(\\text{step}^{-0.5}, \\text{step} \\times \\text{warmup\\_steps}^{-1.5}) \\]\n\nKey steps to compute the learning rate:\n1. **Check for Step Validity**: If the step is less than 1, return 0 immediately.\n2. **Model Dimension Factor**: Compute the inverse square root of the model dimension.\n3. **Term Calculations**: Calculate two terms:\n   - Term 1: Inverse square root of the step.\n   - Term 2: Step multiplied by the inverse of warmup_steps raised to the power of 1.5.\n4. **Minimum of Terms**: Determine the minimum value between the two terms.\n5. **Combine Factors**: Multiply the minimum term with the model dimension factor and the scale factor.\n6. **Rounding**: Round the result to 9 decimal places for consistency.\n\n### Solution Code\n```python\nimport math\n\ndef noam_learning_rate(step: int,\n                       model_dim: int = 512,\n                       scale_factor: float = 1.0,\n                       warmup_steps: int = 4000) -> float:\n    \"\"\"Return the learning-rate given by the Noam schedule.\n\n    The function must follow the formula described in the task description and\n    round the result to 9 decimal places.\n    \"\"\"\n    if step < 1:\n        return round(0.0, 9)\n    \n    model_dim_factor = math.pow(model_dim, -0.5)\n    term1 = math.pow(step, -0.5)\n    term2 = step * math.pow(warmup_steps, -1.5)\n    min_term = min(term1, term2)\n    \n    lr = scale_factor * model_dim_factor * min_term\n    return round(lr, 9)\n```\n\n### Explanation\n1. **Step Validity Check**: The function first checks if the step is less than 1. If so, it returns 0.0, as specified.\n2. **Model Dimension Factor**: This factor is calculated as the inverse square root of the model dimension, which scales the learning rate based on the model's hidden size.\n3. **Term Calculations**:\n   - **Term 1** decreases as the step increases, representing the decay phase.\n   - **Term 2** increases linearly with the step during the warm-up period.\n4. **Minimum Value**: The minimum of the two terms ensures that during the warm-up period, the learning rate increases linearly, and after the warm-up steps, it decays.\n5. **Combining Factors**: The final learning rate is computed by combining the scale factor, model dimension factor, and the minimum term.\n6. **Rounding**: The result is rounded to 9 decimal places to meet the specified precision requirement.\n\nThis approach efficiently computes the learning rate according to the Noam schedule, ensuring both correctness and precision."}
{"task_id": 335, "completion_id": 0, "completion": "To solve this problem, we need to create a function `get_initializer` that returns a callable weight initializer based on the specified strategy. The initializer must generate a numpy array of a given shape according to the chosen strategy. The strategies include zero-initialization, one-initialization, normal distribution, and Glorot uniform initialization.\n\n### Approach\n1. **Input Handling**: Convert the input strategy name to lowercase to ensure case-insensitivity.\n2. **Strategy Mapping**: Based on the input name, return the corresponding initializer function.\n3. **Zero Initialization**: Return an array filled with zeros, ignoring the scale parameter.\n4. **One Initialization**: Return an array filled with ones scaled by the given factor.\n5. **Normal Distribution**: Return an array with values drawn from a normal distribution with mean 0 and standard deviation equal to the scale parameter.\n6. **Glorot Uniform Initialization**: Compute the range for uniform distribution based on the fan-in and fan-out (first two dimensions of the shape) and return an array with values uniformly distributed within this range. Raise an error for one-dimensional shapes.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_initializer(name: str):\n    \"\"\"Return a weight-initialisation callable associated with *name*.\n\n    The callable must have the signature\n        initializer(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray\n    and create a numpy array filled according to the requested strategy.\n\n    Supported strategies (case-insensitive):\n        - \"zeros\"\n        - \"ones\"\n        - \"normal\"\n        - \"glorot_uniform\"\n\n    If *name* is not recognised the function must raise `ValueError`.\n    \"\"\"\n    lower_name = name.lower()\n    if lower_name == \"zeros\":\n        return lambda shape, scale=1.0: np.zeros(shape)\n    elif lower_name == \"ones\":\n        return lambda shape, scale=1.0: np.ones(shape) * scale\n    elif lower_name == \"normal\":\n        return lambda shape, scale=1.0: np.random.normal(0, scale, shape)\n    elif lower_name == \"glorot_uniform\":\n        def initializer(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray:\n            if len(shape) < 2:\n                raise ValueError(\"Glorot uniform initializer requires at least two dimensions.\")\n            fan_in = shape[0]\n            fan_out = shape[1]\n            limit = scale * np.sqrt(6.0 / (fan_in + fan_out))\n            return np.random.uniform(-limit, limit, shape)\n        return initializer\n    else:\n        raise ValueError(f\"Unsupported initializer: {name}\")\n```\n\n### Explanation\n1. **Zero Initialization**: The lambda function returns an array of zeros using `np.zeros`, ignoring the scale parameter.\n2. **One Initialization**: The lambda function returns an array of ones scaled by the given factor using `np.ones` and multiplying by the scale.\n3. **Normal Distribution**: The lambda function generates values from a normal distribution with mean 0 and standard deviation equal to the scale using `np.random.normal`.\n4. **Glorot Uniform Initialization**: This initializer checks if the shape has at least two dimensions. If not, it raises an error. It computes the range (limit) based on the fan-in and fan-out (first two dimensions of the shape) and generates uniformly distributed values within this range using `np.random.uniform`.\n\nThis approach ensures that each initializer is correctly implemented and handles edge cases such as invalid strategy names or incompatible shapes for specific strategies."}
{"task_id": 336, "completion_id": 0, "completion": "To solve this problem, we need to compute the linear kernel matrix between two collections of input vectors, X and Y. The kernel function is defined as the dot product of two vectors plus an optional constant. The solution must handle various edge cases, including empty inputs and dimension mismatches.\n\n### Approach\n1. **Handle Empty Input for X**: If X is an empty list, return an empty list immediately.\n2. **Convert to NumPy Arrays**: Convert X and Y to NumPy arrays to handle both lists and arrays uniformly.\n3. **Reshape 1D Arrays**: Ensure that 1D arrays are reshaped to 2D arrays to treat each input as a collection of vectors.\n4. **Check for Empty X**: If X has zero rows after conversion, return an empty list.\n5. **Handle Y**: If Y is not provided, set Y to X. Convert Y to a NumPy array and reshape if necessary.\n6. **Column Compatibility Check**: Ensure X and Y have the same number of columns; if not, return -1.\n7. **Compute Kernel Matrix**: Use matrix multiplication to compute the dot product efficiently, add the constant term, and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_kernel(X: list[list[int | float]] | \"np.ndarray\", Y: list[list[int | float]] | \"np.ndarray\" | None = None, c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\"\"\"\n    \n    # Handle empty X list case\n    if isinstance(X, list) and len(X) == 0:\n        return []\n    \n    # Convert X to numpy array and ensure it's 2D\n    x = np.array(X)\n    if x.ndim == 1:\n        x = x.reshape(1, -1)\n    # Check if X has zero rows after conversion\n    if x.shape[0] == 0:\n        return []\n    \n    # Handle Y\n    if Y is None:\n        y = x.copy()\n    else:\n        y = np.array(Y)\n        if y.ndim == 1:\n            y = y.reshape(1, -1)\n    \n    # Check if columns match\n    if x.shape[1] != y.shape[1]:\n        return -1\n    \n    # Compute kernel matrix\n    dot_product = np.dot(x, y.T)\n    k_matrix = dot_product + c0\n    # Round to 4 decimal places\n    k_rounded = np.round(k_matrix, 4)\n    # Convert to list of lists\n    return k_rounded.tolist()\n```\n\n### Explanation\n1. **Handling Empty Input**: The function first checks if X is an empty list and returns an empty list immediately if true. This ensures compliance with the problem's requirement for empty inputs.\n2. **Conversion and Reshaping**: X and Y are converted to NumPy arrays. If they are 1D, they are reshaped to 2D arrays to ensure each input is treated as a collection of vectors.\n3. **Dimension Checks**: After conversion, the function checks if X has zero rows (indicating an empty collection) and returns an empty list. It also checks if X and Y have the same number of columns, returning -1 if they do not.\n4. **Matrix Computation**: The dot product of X and Y (transposed) is computed using NumPy's optimized matrix multiplication. The constant term is added, and the result is rounded to four decimal places before converting to a list of lists for the final output.\n\nThis approach efficiently handles various edge cases and ensures compatibility between different input types while adhering to the problem's constraints and requirements."}
{"task_id": 337, "completion_id": 0, "completion": "To solve this problem, we need to perform Nearest-Neighbor interpolation on a 2D image grid using given floating-point coordinates. The task involves rounding the coordinates to the nearest integer, clipping them to ensure they fall within the image boundaries, and then fetching the corresponding pixel values from the image.\n\n### Approach\n1. **Check for Empty Input**: If both coordinate lists are empty, return an empty list immediately.\n2. **Check Length Consistency**: Ensure the coordinate lists have the same length; if not, return an empty list.\n3. **Convert to Numpy Arrays**: Convert the coordinate lists to numpy arrays for efficient numerical operations.\n4. **Round Coordinates**: Use NumPy's `around` function to round the coordinates to the nearest integers.\n5. **Clip Coordinates**: Clip the rounded coordinates to ensure they are within the valid range of the image dimensions.\n6. **Fetch Pixels**: Use the clipped coordinates to index into the image array and retrieve the corresponding pixel values for all channels.\n7. **Convert to List**: Convert the resulting array of pixel values to a Python list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef nn_interpolate_2D(X: 'np.ndarray', x: list[float | int], y: list[float | int]) -> list:\n    \"\"\"Nearest-Neighbour interpolation on a 2-D regular grid.\"\"\"\n    \n    # Check if both coordinate lists are empty\n    if len(x) == 0 and len(y) == 0:\n        return []\n    \n    # Ensure the coordinate lists are of the same length\n    if len(x) != len(y):\n        return []\n    \n    # Convert input lists to numpy arrays\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    \n    # Get the dimensions of the image\n    rows, cols, _ = X.shape\n    \n    # Round the coordinates to the nearest integer using banker's rounding\n    x_rounded = np.around(x_arr).astype(int)\n    y_rounded = np.around(y_arr).astype(int)\n    \n    # Clip the coordinates to stay within the image bounds\n    x_clipped = np.clip(x_rounded, 0, cols - 1)\n    y_clipped = np.clip(y_rounded, 0, rows - 1)\n    \n    # Fetch the pixel values for all channels\n    pixels = X[y_clipped, x_clipped, :]\n    \n    # Convert the result to a Python list and return\n    return pixels.tolist()\n```\n\n### Explanation\n- **Empty Input Handling**: The function first checks if both coordinate lists are empty and returns an empty list if true. This ensures we handle edge cases correctly.\n- **Length Consistency Check**: The function checks if the coordinate lists have the same length to avoid processing mismatched pairs of coordinates.\n- **Rounding and Clipping**: The coordinates are rounded using NumPy's `around` function, which employs banker's rounding. Clipping ensures that the indices remain within valid bounds of the image dimensions.\n- **Pixel Fetching**: The image array is indexed using the clipped and rounded coordinates to retrieve the corresponding pixel values efficiently.\n- **Conversion to List**: The resulting pixel values are converted to a Python list using the `tolist()` method to meet the output requirements.\n\nThis approach efficiently handles the interpolation using vectorized operations in NumPy, ensuring both correctness and performance."}
{"task_id": 338, "completion_id": 0, "completion": "To solve this problem, we need to implement the closed-form solution for Ridge Regression, which includes an L2 regularization term. The goal is to compute the weight vector analytically using the given formula and return the coefficients rounded to four decimal places.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert the input feature matrix `X` and target vector `y` into NumPy arrays to facilitate matrix operations.\n2. **Add Bias Term**: Append a column of ones to the feature matrix `X` to account for the bias term (intercept) in the regression model.\n3. **Compute Regularized Matrix**: Calculate the matrix product of the transposed modified feature matrix with itself, then add the regularization term (product of the regularization factor and the identity matrix).\n4. **Matrix Inversion and Weights Calculation**: Invert the regularized matrix and multiply it by the product of the transposed modified feature matrix and the target vector to obtain the weight vector.\n5. **Rounding and Conversion**: Round each coefficient to four decimal places and convert the result to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_regression(X, y, reg_factor):\n    # Convert inputs to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Number of samples and features in the original X\n    n_samples = X.shape[0] if X.ndim > 0 else 0  # Handle 0D case if X is empty\n    n_features = X.shape[1] if X.ndim > 1 else 0\n    \n    # Add a column of ones for the bias term (leading column)\n    X_bar = np.hstack([np.ones((n_samples, 1)), X]) if n_samples != 0 else np.ones((0, n_features + 1))\n    \n    # Compute the identity matrix of size (d+1) x (d+1)\n    I = np.eye(X_bar.shape[1])\n    \n    # Compute (X\u0304^T X\u0304 + \u03b1I)\n    XT_X = X_bar.T @ X_bar\n    A = XT_X + reg_factor * I\n    \n    # Compute the inverse of matrix A\n    A_inv = np.linalg.inv(A)\n    \n    # Compute the weights: A_inv * (X\u0304^T y)\n    w = A_inv @ (X_bar.T @ y)\n    \n    # Round each coefficient to 4 decimal places and convert to a list\n    w_rounded = np.round(w, 4).tolist()\n    \n    return w_rounded\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: Ensures that the input matrices and vectors are compatible with NumPy operations, which are necessary for efficient matrix computations.\n2. **Bias Term Addition**: By appending a column of ones to the feature matrix, we explicitly model the intercept term in the regression equation.\n3. **Regularized Matrix Computation**: The matrix product `X\u0304.T @ X\u0304` represents the unregularized covariance matrix of the features. Adding `reg_factor * I` (where `I` is the identity matrix) ensures the matrix is invertible and applies L2 regularization.\n4. **Matrix Inversion and Weights Calculation**: The inverse of the regularized matrix is multiplied by the product of the transposed feature matrix and the target vector to obtain the optimal weight vector.\n5. **Rounding and Conversion**: The coefficients are rounded to four decimal places for readability and returned as a list, ensuring the bias term is the first element in the list.\n\nThis approach efficiently computes the Ridge Regression coefficients using matrix operations, ensuring numerical stability and correctness through regularization."}
{"task_id": 339, "completion_id": 0, "completion": "To solve this problem, we need to determine the most frequently occurring element (label) in a given list. If there are multiple elements with the same highest frequency, the element that appears first in the list should be returned. If the list is empty, the function should return `None`.\n\n### Approach\n1. **Check for Empty List**: If the input list is empty, return `None`.\n2. **Count Frequencies**: Use the `Counter` class from the `collections` module to count the occurrences of each element in the list.\n3. **Determine Maximum Frequency**: Find the highest frequency value from the counts.\n4. **Identify Candidates**: Collect all elements that have this maximum frequency.\n5. **Resolve Ties**: Iterate through the original list and return the first element that is among the candidates. This ensures that in case of a tie, the element appearing first in the list is chosen.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef most_common_label(y: list) -> object:\n    \"\"\"Return the most frequent label in *y*.\n\n    If *y* is empty, the function returns ``None``.\n    In case of a tie, the label whose first appearance comes earliest\n    in *y* is returned.\n\n    Args:\n        y: A list containing hashable elements (e.g., strings, numbers).\n\n    Returns:\n        The label that occurs most frequently in *y*, or ``None`` if *y* is\n        empty.\n    \"\"\"\n    if not y:\n        return None\n    counts = Counter(y)\n    max_count = max(counts.values())\n    candidates = {k for k, v in counts.items() if v == max_count}\n    for element in y:\n        if element in candidates:\n            return element\n```\n\n### Explanation\n1. **Check for Empty List**: The function first checks if the input list `y` is empty. If it is, it immediately returns `None`.\n2. **Count Frequencies**: Using `Counter`, the function counts how many times each element appears in the list.\n3. **Determine Maximum Frequency**: The maximum value among the counts is found using the `max` function.\n4. **Identify Candidates**: A set of elements that have the maximum frequency is created. This set helps in efficiently checking membership as we iterate through the original list.\n5. **Resolve Ties**: By iterating through the original list and checking for membership in the candidate set, the function ensures that the first occurrence of any candidate element is returned, thus resolving ties based on the earliest appearance.\n\nThis approach efficiently combines counting and iteration to handle both frequency calculation and tie resolution in a straightforward manner."}
{"task_id": 340, "completion_id": 0, "completion": "To solve this problem, we need to implement a single optimization step of the Adam (Adaptive Moment Estimation) algorithm. The Adam algorithm is an extension to stochastic gradient descent that has been widely adopted for training deep learning models due to its computational efficiency and effectiveness in practice. The algorithm computes adaptive learning rates for each parameter by estimating the first and second moments of the gradients.\n\n### Approach\nThe Adam optimization algorithm involves the following steps for each parameter update:\n1. **Update the first moment estimate (mean)**: This is done by combining the previous first moment estimate with the current gradient.\n2. **Update the second moment estimate (uncentered variance)**: This is done by combining the previous second moment estimate with the square of the current gradient.\n3. **Bias correction**: Since the initial estimates of the moments are biased towards zero, bias-corrected versions of the moments are computed.\n4. **Parameter update**: The parameters are updated using the bias-corrected moments and the learning rate. If the gradient is exactly zero, the parameter remains unchanged, but the moments are still updated.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\"\"\"\n\n    # Compute new moments\n    new_m = beta1 * m + (1 - beta1) * grad\n    new_v = beta2 * v + (1 - beta2) * (grad ** 2)\n\n    # Compute bias-corrected estimates\n    m_hat = new_m / (1 - beta1 ** t)\n    v_hat = new_v / (1 - beta2 ** t)\n\n    # Compute the denominator for the parameter update\n    denominator = np.sqrt(v_hat) + eps\n\n    # Calculate the delta for the parameter update\n    delta = lr * m_hat / denominator\n\n    # Check if all elements of grad are zero to decide parameter update\n    if np.all(grad == 0):\n        updated_param = param.copy()\n    else:\n        updated_param = param - delta\n\n    # Round and convert to lists with 8 decimal places\n    updated_param_list = np.round(updated_param, 8).tolist()\n    new_m_list = np.round(new_m, 8).tolist()\n    new_v_list = np.round(new_v, 8).tolist()\n\n    return (updated_param_list, new_m_list, new_v_list)\n```\n\n### Explanation\n1. **Moment Updates**: The first and second moments (`m` and `v`) are updated using exponential moving averages of the gradient and squared gradient, respectively. This helps in smoothing out the noisy gradients.\n2. **Bias Correction**: The moments are adjusted to account for their initialization bias towards zero. This correction is crucial in the initial steps when the estimates are more biased.\n3. **Parameter Update**: The parameters are updated using the bias-corrected moments. The learning rate is scaled by the square root of the second moment estimate, adjusted by a small constant `eps` to avoid division by zero.\n4. **Handling Zero Gradient**: If the gradient is exactly zero, the parameters remain unchanged, but the moments are still updated to ensure they reflect the current state accurately.\n\nThis implementation efficiently handles multi-dimensional parameters using NumPy's array operations, ensuring that the solution is both vectorized and efficient. The results are returned as Python lists with elements rounded to 8 decimal places for consistency and precision."}
{"task_id": 342, "completion_id": 0, "completion": "To solve this problem, we need to compute the Manhattan (L1) distance between two real-valued vectors of equal length. The Manhattan distance is the sum of the absolute differences of their corresponding elements. If the vectors are of unequal length, we return -1.0 as a float.\n\n### Approach\n1. **Check Vector Lengths**: First, verify if the lengths of the two input vectors are equal. If they are not, return -1.0 immediately.\n2. **Compute Absolute Differences**: For each corresponding pair of elements in the two vectors, compute the absolute difference.\n3. **Sum the Differences**: Sum all the absolute differences to get the Manhattan distance.\n4. **Round the Result**: Round the computed Manhattan distance to four decimal places and return it as a float.\n\nThis approach ensures that we handle both integer and float elements in the input vectors efficiently. The use of Python's built-in functions and list comprehensions allows for a clear and concise implementation without the need for external libraries beyond the given numpy import, which is not necessary here but included as per the initial code setup.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef manhattan(x: list[float | int], y: list[float | int]) -> float:\n    \"\"\"Compute the Manhattan (L1) distance between two real vectors.\n\n    Args:\n        x: List of numbers representing the first vector.\n        y: List of numbers representing the second vector.\n\n    Returns:\n        The L1 distance rounded to 4 decimal places as a float. If the\n        vectors have different lengths, returns -1.\n    \"\"\"\n    if len(x) != len(y):\n        return -1.0\n    distance = sum(abs(a - b) for a, b in zip(x, y))\n    return round(distance, 4)\n```\n\n### Explanation\n- **Vector Length Check**: The function first checks if the lengths of the input vectors `x` and `y` are equal. If not, it returns -1.0.\n- **Absolute Differences Calculation**: Using a generator expression with `zip(x, y)`, we iterate through corresponding elements of the vectors, compute their absolute differences, and sum these differences to get the Manhattan distance.\n- **Rounding**: The result is rounded to four decimal places using Python's `round` function, ensuring the output is a float as required.\n\nThis method efficiently handles the computation with a time complexity of O(n), where n is the length of the vectors, making it suitable for large vectors as well."}
{"task_id": 343, "completion_id": 0, "completion": "To solve this problem, we need to implement the core numerical step of the Cross-Entropy Method (CEM) to update the parameters of a sampling distribution based on the performance of sampled parameter vectors. The goal is to retain the top-performing samples and compute their mean and variance to update the distribution parameters.\n\n### Approach\n1. **Determine the Number of Samples to Retain**: Calculate the number of samples to retain using the given fraction. If the product of the fraction and the total number of samples is not an integer, we take the floor of this product.\n2. **Sort Samples by Performance**: Sort the parameter vectors (rows of the input array) based on their corresponding rewards in descending order to identify the top-performing samples.\n3. **Retain Top Samples**: Select the top samples based on the calculated number of samples to retain.\n4. **Compute Mean and Variance**: Calculate the mean and variance for each dimension (column) of the retained samples.\n5. **Round and Convert to Lists**: Round the computed mean and variance values to four decimal places and convert them to Python lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\"\"\"\n    N = theta_samples.shape[0]\n    k = int(retain_prcnt * N)\n    \n    rewards_arr = np.array(rewards)\n    sorted_indices = np.argsort(-rewards_arr)\n    elite_indices = sorted_indices[:k]\n    elite_samples = theta_samples[elite_indices]\n    \n    mean = np.mean(elite_samples, axis=0)\n    variance = np.var(elite_samples, axis=0)\n    \n    rounded_mean = np.round(mean, 4)\n    rounded_variance = np.round(variance, 4)\n    \n    return (rounded_mean.tolist(), rounded_variance.tolist())\n```\n\n### Explanation\n1. **Determine Number of Samples to Retain**: The number of samples to retain (`k`) is computed by multiplying the total number of samples (`N`) by the given fraction (`retain_prcnt`) and converting the result to an integer using the floor function.\n2. **Sort Samples**: The indices of the rewards are sorted in descending order to prioritize higher rewards. This allows us to select the top `k` samples.\n3. **Retain Top Samples**: Using the sorted indices, we extract the top `k` rows from the parameter vectors array (`theta_samples`).\n4. **Compute Mean and Variance**: The mean and variance are computed along each dimension (columns) of the retained samples. This gives us the new distribution parameters.\n5. **Round and Convert**: The computed mean and variance values are rounded to four decimal places and converted to Python lists to meet the output requirements.\n\nThis approach ensures that we efficiently update the distribution parameters based on the best-performing samples, which is crucial for optimizing policies in reinforcement learning using the Cross-Entropy Method."}
{"task_id": 344, "completion_id": 0, "completion": "To solve this problem, we need to implement the Huffman coding algorithm for a list of tokens. The goal is to generate a prefix-free binary code for each token based on its frequency, ensuring the codes are optimal (minimum expected length). The algorithm involves building a Huffman tree using a min-heap and traversing the tree to generate the codes.\n\n### Approach\n1. **Handle Edge Cases**: If the input list is empty, return an empty dictionary. If there is only one unique token, assign it the code \"0\".\n2. **Count Frequencies**: Use a frequency counter to determine the occurrence of each token.\n3. **Build Min-Heap**: Create a min-heap where each node is ordered by frequency and lexicographical order of the token. Internal nodes (merged nodes) use an empty string as their token.\n4. **Merge Nodes**: Repeatedly merge the two nodes with the smallest frequencies until one node remains, forming the Huffman tree.\n5. **Generate Codes**: Traverse the Huffman tree from the root to each leaf, appending '0' for left edges and '1' for right edges to build the codes.\n\n### Solution Code\n```python\nimport heapq\nfrom collections import Counter\n\nclass Node:\n    def __init__(self, freq, token, left=None, right=None):\n        self.freq = freq\n        self.token = token\n        self.left = left\n        self.right = right\n\n    def __lt__(self, other):\n        if self.freq == other.freq:\n            return self.token < other.token\n        return self.freq < other.freq\n\ndef huffman_encode(tokens: list[str]) -> dict[str, str]:\n    \"\"\"Generate a Huffman code for the supplied tokens.\"\"\"\n    if not tokens:\n        return {}\n    freq = Counter(tokens)\n    unique = list(freq.items())\n    if len(unique) == 1:\n        only_token = unique[0][0]\n        return {only_token: '0'}\n    \n    heap = []\n    for token, count in unique:\n        heapq.heappush(heap, Node(count, token))\n    \n    while len(heap) > 1:\n        left = heapq.heappop(heap)\n        right = heapq.heappop(heap)\n        merged_freq = left.freq + right.freq\n        merged_node = Node(merged_freq, \"\", left, right)\n        heapq.heappush(heap, merged_node)\n    \n    root = heapq.heappop(heap)\n    codes = {}\n    stack = [(root, \"\")]\n    while stack:\n        node, code = stack.pop()\n        if node.left is None and node.right is None:\n            codes[node.token] = code\n        else:\n            if node.right:\n                stack.append((node.right, code + '1'))\n            if node.left:\n                stack.append((node.left, code + '0'))\n    \n    return codes\n```\n\n### Explanation\n1. **Edge Cases Handling**: The function immediately returns an empty dictionary for an empty input list. For a single unique token, it returns a dictionary with the token mapped to \"0\".\n2. **Frequency Counting**: The `Counter` from the `collections` module efficiently counts token frequencies.\n3. **Heap Construction**: Nodes are created for each token and pushed into the min-heap, ordered by frequency and lexicographical order.\n4. **Merging Nodes**: The two smallest nodes (by frequency) are repeatedly merged into a new node with their combined frequency, until one node remains as the root of the Huffman tree.\n5. **Code Generation**: A depth-first traversal of the Huffman tree using a stack generates the binary codes by appending '0' for left moves and '1' for right moves, ensuring prefix-free codes.\n\nThis approach ensures that the Huffman codes are optimal and prefix-free, adhering to the problem constraints and requirements."}
{"task_id": 345, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_linear_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        mu0: np.ndarray,\n        omega0: np.ndarray) -> list[float]:\n    \"\"\"Predict targets for X_test using Bayesian linear regression.\"\"\"\n\n    # Add intercept column to training and test data\n    X_train_aug = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n    X_test_aug = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n    \n    # Compute X^T X and add prior precision matrix\n    XT_X = X_train_aug.T @ X_train_aug\n    inv_term = XT_X + omega0\n    \n    # Invert the matrix term\n    inv_matrix = np.linalg.inv(inv_term)\n    \n    # Compute X^T y and add prior term\n    XT_y = X_train_aug.T @ y_train\n    rhs = XT_y + omega0 @ mu0\n    \n    # Calculate posterior mean weights\n    mu_n = inv_matrix @ rhs\n    \n    # Make predictions on test data\n    predictions = X_test_aug @ mu_n\n    \n    # Round to four decimal places and convert to list\n    return np.round(predictions, 4).tolist()\n```"}
{"task_id": 346, "completion_id": 0, "completion": "To solve this problem, we need to compute the Shannon entropy of a given list of class labels. Shannon entropy is a measure of the impurity or unpredictability of a distribution, which is widely used in machine learning algorithms such as decision trees.\n\n### Approach\nThe approach to compute Shannon entropy involves the following steps:\n1. **Count Occurrences**: Identify the unique class labels and count their occurrences in the input data.\n2. **Calculate Probabilities**: Convert these counts into probabilities by dividing each count by the total number of samples.\n3. **Compute Entropy**: For each probability, compute the product of the probability and the logarithm (base 2) of the probability. Sum these products and multiply by -1 to get the entropy.\n4. **Round Result**: Round the computed entropy value to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef entropy(y):\n    \"\"\"Compute the base-2 Shannon entropy of a vector of integer labels.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        One-dimensional iterable containing non-negative integer class labels.\n\n    Returns\n    -------\n    float\n        The entropy value rounded to 4 decimal places.\n    \"\"\"\n    labels, counts = np.unique(y, return_counts=True)\n    probs = counts / counts.sum()\n    entropy_value = -np.sum(probs * np.log2(probs))\n    return round(entropy_value, 4)\n```\n\n### Explanation\n1. **Count Occurrences**: Using `np.unique` with `return_counts=True`, we obtain the unique labels and their respective counts in the input data. This efficiently handles both lists and NumPy arrays.\n2. **Calculate Probabilities**: The counts are converted to probabilities by dividing each count by the total number of samples (sum of all counts). This ensures each probability is a measure of how frequently a class label appears relative to the entire dataset.\n3. **Compute Entropy**: For each probability, we compute the product of the probability and the base-2 logarithm of the probability. Summing these products and multiplying by -1 gives the Shannon entropy, which quantifies the unpredictability in the data distribution.\n4. **Round Result**: The result is rounded to four decimal places using Python's built-in `round` function to meet the specified precision requirement.\n\nThis approach efficiently handles any non-negative integer class labels and ensures accurate computation of entropy even for non-contiguous or non-zero-starting labels. The use of NumPy functions ensures both efficiency and readability."}
{"task_id": 347, "completion_id": 0, "completion": "To solve this problem, we need to implement the Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) loss function for both the critic and the generator. The WGAN-GP loss function introduces a gradient penalty term to stabilize training, which is critical for the critic's performance.\n\n### Approach\nThe WGAN-GP loss function has two components: one for the critic (discriminator) and one for the generator. The critic's loss includes a gradient penalty term to enforce the Lipschitz constraint, while the generator's loss is straightforward.\n\n1. **Critic Loss (L_C)**:\n   - The critic loss is computed as the difference between the mean critic scores on fake samples and real samples.\n   - A gradient penalty term is added, which penalizes the deviation of the gradient norms of interpolated samples from 1. This ensures the critic function is 1-Lipschitz.\n\n2. **Generator Loss (L_G)**:\n   - The generator loss is simply the negative mean of the critic scores on fake samples, encouraging the generator to produce samples that the critic assigns higher scores.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef wgan_gp_loss(\n        Y_fake: np.ndarray,\n        module: str,\n        Y_real: np.ndarray | None = None,\n        gradInterp: np.ndarray | None = None,\n        lambda_: float = 10.0) -> float:\n    \"\"\"Compute the WGAN-GP loss for the critic or the generator.\"\"\"\n    \n    if module == \"C\":\n        if Y_real is None or gradInterp is None:\n            raise ValueError(\"Y_real and gradInterp must be provided when module is 'C'\")\n        \n        # Compute means for real and fake samples\n        mean_fake = np.mean(Y_fake)\n        mean_real = np.mean(Y_real)\n        \n        # Compute gradient penalty term\n        grad_flat = gradInterp.reshape(gradInterp.shape[0], -1)\n        norms = np.linalg.norm(grad_flat, axis=1)\n        gp_term = lambda_ * np.mean((norms - 1) ** 2)\n        \n        # Total critic loss\n        loss = (mean_fake - mean_real) + gp_term\n    elif module == \"G\":\n        # Generator loss\n        loss = -np.mean(Y_fake)\n    else:\n        raise ValueError(\"module must be either 'C' or 'G'\")\n    \n    # Round to four decimal places and return as Python float\n    return float(np.round(loss, 4))\n```\n\n### Explanation\n- **Critic Loss Calculation**:\n  - **Mean Scores**: Compute the mean of the critic scores for both real and fake samples.\n  - **Gradient Penalty**: Reshape the gradients of the interpolated samples to flatten each sample's gradient into a vector. Compute the L2 norm of these vectors, then calculate the gradient penalty term as the mean of the squared deviations from 1, scaled by the hyperparameter \u03bb (lambda_).\n  - **Total Loss**: Combine the difference of mean scores and the gradient penalty term to form the total critic loss.\n\n- **Generator Loss Calculation**:\n  - The generator loss is the negative mean of the critic scores on the fake samples, encouraging the generator to produce samples that the critic evaluates highly.\n\nThis approach ensures the critic enforces the Lipschitz constraint through the gradient penalty, leading to more stable and reliable training of the GAN model."}
{"task_id": 348, "completion_id": 0, "completion": "To solve this problem, we need to determine if a given real square matrix is both symmetric and positive-definite. This involves checking the matrix's shape, symmetry, and the positivity of its eigenvalues.\n\n### Approach\n1. **Check if the matrix is square**: A matrix is square if the number of rows equals the number of columns. If not, return `False`.\n2. **Check symmetry**: A matrix is symmetric if it is equal to its transpose. Due to numerical precision issues, we use `np.allclose` with a tolerance of `1e-8` to compare the matrix with its transpose.\n3. **Compute eigenvalues**: For a symmetric matrix, compute the eigenvalues using `np.linalg.eigvalsh`, which is efficient for symmetric matrices.\n4. **Check positive-definiteness**: Ensure all eigenvalues are strictly greater than `1e-8`. If any eigenvalue is less than or equal to this threshold, the matrix is not positive-definite.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_symmetric_positive_definite(X: np.ndarray) -> bool:\n    \"\"\"Check whether a real matrix is symmetric and positive-definite.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        A two-dimensional NumPy array representing the matrix to test.\n\n    Returns\n    -------\n    bool\n        True if *X* is symmetric and positive-definite, otherwise False.\n    \"\"\"\n    # Check if the matrix is 2D and square\n    if X.ndim != 2 or X.shape[0] != X.shape[1]:\n        return False\n    \n    # Check symmetry within tolerance\n    if not np.allclose(X, X.T, atol=1e-8):\n        return False\n    \n    # Compute eigenvalues of the symmetric matrix\n    eigenvalues = np.linalg.eigvalsh(X)\n    \n    # Check all eigenvalues are greater than 1e-8\n    return np.all(eigenvalues > 1e-8)\n```\n\n### Explanation\n1. **Square Matrix Check**: The function first verifies if the input matrix is two-dimensional and square. This is done by checking the `ndim` attribute and comparing the number of rows and columns.\n2. **Symmetry Check**: Using `np.allclose`, the matrix is compared to its transpose with a tolerance of `1e-8` to account for floating-point precision errors.\n3. **Eigenvalue Calculation**: For symmetric matrices, eigenvalues are computed efficiently using `np.linalg.eigvalsh`, which returns real eigenvalues.\n4. **Positive-Definiteness Check**: The function checks if all eigenvalues are strictly positive (greater than `1e-8`). If any eigenvalue fails this condition, the matrix is not positive-definite.\n\nThis approach ensures that the function efficiently and accurately determines if the matrix meets both symmetry and positive-definiteness criteria."}
{"task_id": 350, "completion_id": 0, "completion": "To solve this problem, we need to implement a polynomial ridge regression model from scratch using the closed-form solution. The goal is to find the regression coefficients that best fit the given data with L2 regularization. \n\n### Approach\n1. **Design Matrix Construction**: Convert the input feature vector into a polynomial design matrix where each row corresponds to a polynomial expansion of the original feature up to the specified degree. For example, a feature vector `[x1, x2]` with degree 2 becomes `[[1, x1, x1^2], [1, x2, x2^2]]`.\n\n2. **Matrix Operations**: Compute the closed-form solution for ridge regression using the formula:\n   \\[\n   \\mathbf{w} = (\\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^T \\mathbf{y}\n   \\]\n   where \\(\\mathbf{X}\\) is the design matrix, \\(\\lambda\\) is the regularization factor, and \\(\\mathbf{I}\\) is the identity matrix.\n\n3. **Regularization**: Add the regularization term \\(\\lambda \\mathbf{I}\\) to the matrix \\(\\mathbf{X}^T \\mathbf{X}\\) to ensure the matrix is invertible and to penalize large coefficients.\n\n4. **Inversion and Multiplication**: Compute the inverse of the regularized matrix and multiply it by the transpose of the design matrix and the target vector to get the coefficients.\n\n5. **Rounding**: Round the resulting coefficients to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef polynomial_ridge_regression(X: list[float | int],\n                                y: list[float | int],\n                                degree: int,\n                                reg_factor: float) -> list[float]:\n    \"\"\"Fit a polynomial ridge regression model and return its coefficients.\"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Create the design matrix with polynomial features\n    design_matrix = np.column_stack([X_np**i for i in range(degree + 1)])\n    \n    # Compute X^T X + regularization term\n    XTX = design_matrix.T @ design_matrix\n    reg_matrix = reg_factor * np.eye(degree + 1)\n    XTX_plus_reg = XTX + reg_matrix\n    \n    # Compute the inverse of the regularized X^T X matrix\n    inv_XTX_plus_reg = np.linalg.inv(XTX_plus_reg)\n    \n    # Compute X^T y\n    XTy = design_matrix.T @ y_np\n    \n    # Calculate the coefficients\n    coefficients = inv_XTX_plus_reg @ XTy\n    \n    # Round to 4 decimal places and return as a list\n    return np.round(coefficients, 4).tolist()\n```\n\n### Explanation\n1. **Design Matrix**: The design matrix is constructed by raising each element of the input feature vector to powers from 0 up to the specified degree. This is done using vectorized operations for efficiency.\n\n2. **Matrix Multiplication and Regularization**: The matrix \\(\\mathbf{X}^T \\mathbf{X}\\) is computed, and the regularization term \\(\\lambda \\mathbf{I}\\) is added to ensure the matrix is invertible even if \\(\\mathbf{X}^T \\mathbf{X}\\) is singular.\n\n3. **Inversion and Coefficient Calculation**: The inverse of the regularized matrix is computed, and this inverse is multiplied by \\(\\mathbf{X}^T \\mathbf{y}\\) to obtain the regression coefficients.\n\n4. **Rounding**: The coefficients are rounded to four decimal places to meet the problem's output requirements.\n\nThis approach efficiently computes the polynomial ridge regression coefficients using matrix operations and handles the regularization to prevent overfitting, ensuring numerical stability even with high-degree polynomials."}
{"task_id": 351, "completion_id": 0, "completion": "To solve this problem, we need to implement a 2-D max-pooling operation commonly used in Convolutional Neural Networks (CNNs). The goal is to reduce the spatial dimensions of an input tensor by taking the maximum value within sliding windows of a specified size and stride.\n\n### Approach\nThe approach involves using NumPy's `as_strided` function from the `numpy.lib.stride_tricks` module to efficiently create sliding windows over the input tensor. Here's a detailed breakdown of the steps:\n\n1. **Extract Input Dimensions**: Determine the dimensions of the input tensor (N, C, H, W) and the pooling window (p_h, p_w).\n2. **Calculate Output Dimensions**: Compute the output spatial dimensions (out_h, out_w) using the given formulas.\n3. **Create Sliding Windows**: Use `as_strided` to create a view of the input tensor that represents non-overlapping or overlapping windows based on the specified stride. This transforms the input tensor into a 6-dimensional array (N, C, out_h, out_w, p_h, p_w).\n4. **Compute Max Values**: Take the maximum value over the last two dimensions (p_h, p_w) of the windowed array to produce the pooled output tensor.\n5. **Convert to List**: Convert the resulting NumPy array to a nested Python list using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef max_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: int) -> list:\n    \"\"\"Perform 2-D max pooling over a 4-D minibatch of images.\n\n    Args:\n        X: A 4-D NumPy array with shape (N, C, H, W).\n        pool_shape: Tuple (p_h, p_w) \u2013 height and width of the pooling window.\n        stride: The number of pixels the window moves vertically and horizontally.\n\n    Returns:\n        Nested Python list that represents the pooled tensor obtained via\n        the NumPy ``tolist()`` method. The returned list has shape\n        (N, C, out_h, out_w).\n    \"\"\"\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    out_h = (H - p_h) // stride + 1\n    out_w = (W - p_w) // stride + 1\n\n    # Compute the new shape and strides for the windowed array\n    new_shape = (N, C, out_h, out_w, p_h, p_w)\n    new_strides = (X.strides[0], X.strides[1],\n                   stride * X.strides[2],\n                   stride * X.strides[3],\n                   X.strides[2], X.strides[3])\n\n    # Generate the windowed view using as_strided\n    windows = as_strided(X, shape=new_shape, strides=new_strides)\n\n    # Compute the max over the last two dimensions (p_h and p_w)\n    pooled = windows.max(axis=(-2, -1))\n\n    # Convert the result to a nested Python list\n    return pooled.tolist()\n```\n\n### Explanation\n1. **Input Dimensions**: The input tensor X has dimensions (N, C, H, W), where N is the batch size, C is the number of channels, H is the height, and W is the width of each image.\n2. **Pooling Window and Stride**: The pooling window size (p_h, p_w) and stride determine how the window slides over the input tensor.\n3. **Output Dimensions**: The output spatial dimensions (out_h, out_w) are calculated to ensure the pooling window does not step outside the input boundaries.\n4. **Sliding Window Creation**: Using `as_strided`, we create a view of the input tensor that efficiently represents each window as a subarray. This avoids copying data and leverages NumPy's stride manipulation for performance.\n5. **Max Pooling**: The maximum value within each window is computed, reducing the spatial dimensions of the tensor.\n6. **Conversion to List**: The final result is converted to a nested Python list to meet the problem's requirements.\n\nThis approach efficiently handles both non-overlapping and overlapping windows, leveraging NumPy's capabilities for high performance and correctness."}
{"task_id": 353, "completion_id": 0, "completion": "To solve this problem, we need to compute the Shannon entropy of a set of class labels. Shannon entropy is a measure of the impurity or disorder of the labels and is calculated using the probabilities of each unique label's occurrence.\n\n### Approach\n1. **Handle Empty Input**: If the input sequence is empty, return 0.0 immediately as there is no entropy to compute.\n2. **Count Occurrences**: Use a counter to tally the occurrences of each unique label in the input sequence.\n3. **Calculate Probabilities**: For each unique label, compute its probability by dividing its count by the total number of labels.\n4. **Compute Entropy**: Using the probabilities, compute the entropy using the formula \\(H = -\\sum_i p_i \\log_2 p_i\\). Sum the contributions of each probability and then take the negative of the sum to get the entropy.\n5. **Round Result**: Round the computed entropy to 5 decimal places before returning.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    if not labels:\n        return 0.0\n    counts = Counter(labels)\n    total = len(labels)\n    entropy_val = 0.0\n    for cnt in counts.values():\n        p = cnt / total\n        entropy_val += p * math.log2(p)\n    entropy_val = -entropy_val\n    return round(entropy_val, 5)\n```\n\n### Explanation\n- **Empty Input Check**: The function first checks if the input is empty. If it is, the entropy is 0.0.\n- **Counting Labels**: Using `collections.Counter`, we count how many times each label appears in the input sequence.\n- **Probability Calculation**: For each unique label, we determine its probability by dividing its count by the total number of labels.\n- **Entropy Calculation**: For each probability, we compute the term \\( p \\log_2 p \\) and sum these terms. The negative of this sum gives the Shannon entropy.\n- **Rounding**: The result is rounded to 5 decimal places to meet the specified precision requirement.\n\nThis approach efficiently handles all edge cases, including empty input and uniform labels, ensuring accurate and precise entropy calculation."}
{"task_id": 354, "completion_id": 0, "completion": "To solve this problem, we need to implement a high-performance 2D convolution using the im2col + GEMM (General Matrix Multiply) strategy. This method efficiently converts the convolution operation into matrix multiplication, which can be optimized using NumPy's linear algebra routines.\n\n### Approach\n1. **Padding Handling**: Depending on the padding specification, compute the necessary padding for each side (top, bottom, left, right) of the input images. The padding can be an integer, a tuple, or 'same' to maintain the spatial dimensions of the output.\n2. **Dilation Handling**: Adjust the kernel's effective size by considering the dilation factor, which dictates the spacing between kernel elements.\n3. **im2col Transformation**: Convert the input images into columns (patches) that the kernel will slide over. This transformation considers both the stride and dilation parameters to generate the appropriate patches.\n4. **Matrix Multiplication**: Reshape the kernel into a matrix and perform matrix multiplication with the im2col-transformed input to compute the convolution result efficiently.\n5. **Reshaping and Conversion**: Reshape the result into the desired output format and convert it to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    n_ex, in_rows, in_cols, in_ch = X.shape\n    kr, kc, in_ch_W, out_ch = W.shape\n    if in_ch != in_ch_W:\n        raise ValueError(\"Mismatch in input channels between X and W\")\n    \n    kr_eff = (kr - 1) * dilation + 1\n    kc_eff = (kc - 1) * dilation + 1\n    \n    if pad == 'same':\n        pad_row_total = max(0, (in_rows - 1) * stride + kr_eff - in_rows)\n        pad_col_total = max(0, (in_cols - 1) * stride + kc_eff - in_cols)\n        pr1 = pad_row_total // 2\n        pr2 = pad_row_total - pr1\n        pc1 = pad_col_total // 2\n        pc2 = pad_col_total - pc1\n    elif isinstance(pad, int):\n        pr1 = pr2 = pad\n        pc1 = pc2 = pad\n    elif isinstance(pad, tuple):\n        if len(pad) == 2:\n            pr, pc = pad\n            pr1 = pr2 = pr\n            pc1 = pc2 = pc\n        elif len(pad) == 4:\n            pr1, pr2, pc1, pc2 = pad\n        else:\n            pr1 = pr2 = pc1 = pc2 = 0\n    else:\n        pr1 = pr2 = pc1 = pc2 = 0\n    \n    X_pad = np.pad(X, ((0, 0), (pr1, pr2), (pc1, pc2), (0, 0)), mode='constant')\n    H_pad, W_pad = X_pad.shape[1], X_pad.shape[2]\n    \n    H_out = (H_pad - kr_eff) // stride + 1\n    W_out = (W_pad - kc_eff) // stride + 1\n    \n    if H_out <= 0 or W_out <= 0:\n        return np.zeros((n_ex, H_out, W_out, out_ch)).tolist()\n    \n    s_batch, s_H, s_W, s_ch = X_pad.strides\n    new_shape = (n_ex, H_out, W_out, kr, kc, in_ch)\n    new_strides = (\n        s_batch,\n        s_H * stride,\n        s_W * stride,\n        s_H * (dilation + 1),\n        s_W * (dilation + 1),\n        s_ch\n    )\n    \n    patches = np.lib.stride_tricks.as_strided(\n        X_pad,\n        shape=new_shape,\n        strides=new_strides,\n        writeable=False\n    )\n    \n    patches_reshaped = patches.reshape(-1, kr * kc * in_ch)\n    W_reshaped = W.reshape(-1, out_ch)\n    result = np.matmul(patches_reshaped, W_reshaped)\n    output = result.reshape(n_ex, H_out, W_out, out_ch)\n    \n    return output.tolist()\n```\n\n### Explanation\n1. **Padding Calculation**: The padding is computed based on the input parameters. For 'same' padding, it ensures the output dimensions match the input dimensions by calculating the necessary symmetric padding.\n2. **Dilation Handling**: The effective kernel size is adjusted using the dilation factor to account for the spacing between kernel elements.\n3. **im2col Transformation**: Using NumPy's `as_strided` function, the input image is transformed into columns (patches) that the kernel will convolve over. This step efficiently handles both stride and dilation.\n4. **Matrix Multiplication**: The reshaped kernel and im2col-transformed input are multiplied using matrix multiplication, which is highly optimized in NumPy.\n5. **Result Reshaping**: The resulting matrix is reshaped to match the desired output dimensions and converted to a Python list for the final output.\n\nThis approach leverages NumPy's efficiency in matrix operations and strided views to perform the convolution operation in a highly optimized manner."}
{"task_id": 355, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\"\"\"\n\n    # Step 1: Mean centering\n    centered_data = data - np.mean(data, axis=0)\n\n    # Step 2: Obtain directions based on the solver\n    if solver == \"svd\":\n        U, s, Vt = np.linalg.svd(centered_data, full_matrices=False)\n        directions = Vt  # Rows are principal directions\n    elif solver == \"eigen\":\n        cov_matrix = np.cov(centered_data, rowvar=False, ddof=1)\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        sorted_idx = np.argsort(eigenvalues)[::-1]\n        sorted_eigenvalues = eigenvalues[sorted_idx]\n        sorted_eigenvectors = eigenvectors[:, sorted_idx]\n        directions = sorted_eigenvectors.T  # Transpose to get rows as directions\n    else:\n        raise ValueError(\"solver must be 'svd' or 'eigen'.\")\n\n    # Step 3: Keep top n_components directions\n    n_features = data.shape[1]\n    n_components = min(n_components, n_features)\n    directions = directions[:n_components]\n\n    # Step 4: Fix the sign determinism\n    for i in range(directions.shape[0]):\n        direction = directions[i]\n        non_zero_indices = np.flatnonzero(direction)\n        if non_zero_indices.size > 0:\n            first_non_zero = non_zero_indices[0]\n            if direction[first_non_zero] < 0:\n                directions[i] *= -1\n\n    # Step 5: Project data to obtain scores\n    scores = centered_data @ directions.T\n\n    # Step 6: Compute explained variance ratios\n    if solver == \"svd\":\n        total_variance = (s ** 2).sum()\n        explained_var_ratios = (s[:n_components] ** 2) / total_variance\n    else:  # eigen solver\n        total_variance = sorted_eigenvalues.sum()\n        explained_var_ratios = sorted_eigenvalues[:n_components] / total_variance\n\n    # Round and convert to list of lists\n    scores_rounded = np.round(scores, 4).tolist()\n    explained_var_ratios_rounded = np.round(explained_var_ratios, 4).tolist()\n\n    return (scores_rounded, explained_var_ratios_rounded)\n```"}
{"task_id": 356, "completion_id": 0, "completion": "To solve this problem, we need to create a function that extracts the correct prediction from a tree leaf node based on whether the tree is a classification tree or a regression tree. \n\n### Approach\n1. **Classification Tree**: For a classification tree, the leaf node stores a list or tuple of class probabilities. The prediction is the index of the maximum probability in this list. If there are multiple maximum probabilities, the index of the first occurrence is chosen, consistent with the behavior of `numpy.argmax`.\n2. **Regression Tree**: For a regression tree, the leaf node stores a scalar value representing the mean of the target values. The prediction is simply this scalar value.\n\n### Solution Code\n```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        probs = leaf.value\n        max_prob = max(probs)\n        return probs.index(max_prob)\n    else:\n        return leaf.value\n```\n\n### Explanation\n1. **Classification Handling**: When `classifier` is `True`, the function retrieves the list of probabilities from the leaf node. It then finds the maximum probability and returns the index of the first occurrence of this maximum value using `list.index()`.\n2. **Regression Handling**: When `classifier` is `False`, the function directly returns the scalar value stored in the leaf node, which is the mean target value for that region in a regression tree.\n\nThis approach ensures that the function efficiently determines the correct prediction based on the type of tree, adhering to the specified behavior for both classification and regression scenarios."}
{"task_id": 357, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm from scratch. The goal is to partition samples into K clusters based on the nearest Euclidean centroid. The algorithm is deterministic, initializing centroids as the first K samples, and iteratively refining them until convergence or a maximum number of iterations.\n\n### Approach\n1. **Initialization**: Start with the first K samples as initial centroids.\n2. **Iteration**: Repeat the following steps until convergence or maximum iterations:\n   - **Assignment**: Assign each sample to the nearest centroid using Euclidean distance.\n   - **Update**: Recompute centroids as the mean of assigned samples. If a centroid has no samples, it remains unchanged.\n   - **Convergence Check**: Stop if centroids change by less than a tolerance (1e-6).\n3. **Sorting and Rounding**: Sort centroids lexicographically and round each coordinate to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    centroids = data[:K].copy()\n    for _ in range(max_iters):\n        # Calculate distances from each data point to each centroid\n        distances = np.sqrt(((data[:, np.newaxis] - centroids) ** 2).sum(axis=2))\n        closest = np.argmin(distances, axis=1)\n        \n        # Update centroids\n        new_centroids = centroids.copy()\n        for j in range(K):\n            mask = (closest == j)\n            if mask.any():\n                new_centroids[j] = data[mask].mean(axis=0)\n        \n        # Check for convergence\n        if np.allclose(centroids, new_centroids, atol=1e-6):\n            break\n        centroids = new_centroids.copy()\n    \n    # Sort centroids lexicographically and round to four decimal places\n    sorted_centroids = sorted(centroids.tolist())\n    rounded_centroids = [[round(val, 4) for val in centroid] for centroid in sorted_centroids]\n    return rounded_centroids\n```\n\n### Explanation\n1. **Initialization**: The centroids are initialized using the first K samples from the data matrix.\n2. **Assignment Step**: For each sample, the Euclidean distance to each centroid is computed, and the sample is assigned to the nearest centroid.\n3. **Update Step**: Each centroid is updated to the mean of the samples assigned to it. If no samples are assigned to a centroid, it remains unchanged.\n4. **Convergence Check**: The algorithm stops early if the centroids do not change significantly (within a tolerance of 1e-6).\n5. **Sorting and Rounding**: The final centroids are sorted lexicographically and each coordinate is rounded to four decimal places for the result.\n\nThis approach ensures that the algorithm efficiently converges to stable centroids while adhering to the deterministic initialization and termination conditions specified."}
{"task_id": 358, "completion_id": 0, "completion": "To solve this problem, we need to compute the classification accuracy between two sequences of class labels. The accuracy is defined as the proportion of correct predictions out of the total predictions. If the input sequences have different lengths, we should return -1.\n\n### Approach\n1. **Check Lengths**: First, verify if the lengths of the two input sequences are the same. If not, return -1.\n2. **Convert to Arrays**: Convert both input sequences to NumPy arrays to facilitate element-wise comparison.\n3. **Handle Empty Inputs**: If both sequences are empty, return 0.0 to avoid division by zero.\n4. **Compute Correct Predictions**: Use NumPy's element-wise comparison to count the number of correct predictions.\n5. **Calculate Accuracy**: Divide the number of correct predictions by the total number of predictions and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef accuracy(y_true, y_pred):\n    \"\"\"Compute the classification accuracy between two equal-length sequences.\n\n    The function should return the proportion of positions where `y_true` and\n    `y_pred` are exactly the same, rounded to four decimal places. If the two\n    sequences do not have the same length, return -1.\n\n    Args:\n        y_true (list | np.ndarray): Ground-truth class labels.\n        y_pred (list | np.ndarray): Predicted class labels.\n\n    Returns:\n        float | int: Accuracy rounded to 4 decimals, or -1 on length mismatch.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.size == 0:\n        return 0.0\n    correct = (y_true == y_pred).sum()\n    total = len(y_true)\n    acc = correct / total\n    return round(acc, 4)\n```\n\n### Explanation\n1. **Check Lengths**: The function first checks if the lengths of `y_true` and `y_pred` are different. If they are, it returns -1 immediately.\n2. **Convert to Arrays**: The input sequences are converted to NumPy arrays to handle both lists and arrays uniformly and leverage NumPy's efficient element-wise operations.\n3. **Handle Empty Inputs**: If both sequences are empty (i.e., their size is zero), the function returns 0.0 to handle the edge case of zero predictions.\n4. **Compute Correct Predictions**: Using NumPy's element-wise comparison, the function counts how many elements in `y_true` and `y_pred` are exactly the same.\n5. **Calculate Accuracy**: The accuracy is computed as the ratio of correct predictions to the total number of predictions. This value is then rounded to four decimal places using Python's `round` function to meet the specified precision requirement.\n\nThis approach ensures that the function efficiently handles various input types and edge cases while providing the correct accuracy metric as defined."}
{"task_id": 359, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that computes the cross-entropy loss and its gradient for given one-hot encoded targets and predicted class probabilities. The function should return the loss value when the derivative flag is False and the gradient matrix when the flag is True. The solution must handle numerical stability by adding a small constant to the logarithm input and round the results to four decimal places.\n\n### Approach\n1. **Cross-Entropy Loss Calculation**:\n   - **Formula**: The cross-entropy loss for one-hot encoded targets is computed as \\( L = -\\sum_{i=1}^{n} \\sum_{j=1}^{m} y_{ij} \\cdot \\log(\\hat{y}_{ij} + \\epsilon) \\), where \\( \\epsilon \\) is a small constant to avoid numerical instability.\n   - **Implementation**: Multiply the targets `y` with the logarithm of the predictions `y_pred` (with added epsilon), sum these values, and take the negative of the result. Round the final loss to four decimal places.\n\n2. **Gradient Calculation**:\n   - **Formula**: The gradient of the loss with respect to the softmax output is given by \\( \\hat{y} - y \\).\n   - **Implementation**: Subtract the targets `y` from the predictions `y_pred`, round each element of the resulting matrix to four decimal places, and convert the result to a nested list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy(y, y_pred, derivative=False):\n    \"\"\"Cross-entropy utility (starter code).\n\n    Parameters\n    ----------\n    y : numpy.ndarray\n        One-hot encoded true labels, shape (n_samples, n_classes).\n    y_pred : numpy.ndarray\n        Predicted class probabilities, same shape as *y*.\n    derivative : bool, default False\n        If *True* return the gradient, else return the loss.\n\n    Returns\n    -------\n    float | list[list[float]]\n        Rounded loss (scalar) or gradient (nested list).\n    \"\"\"\n    eps = np.finfo(float).eps\n    if not derivative:\n        # Compute the cross-entropy loss\n        loss = -np.sum(y * np.log(y_pred + eps))\n        # Round to 4 decimal places and convert to float\n        return round(loss.item(), 4)\n    else:\n        # Compute the gradient (y_pred - y)\n        gradient = y_pred - y\n        # Round each element to 4 decimal places and convert to list of lists\n        return np.round(gradient, 4).tolist()\n```\n\n### Explanation\n- **Cross-Entropy Loss**:\n  - The loss is calculated by element-wise multiplication of the targets `y` with the logarithm of the predictions `y_pred` (with a small epsilon added to avoid log(0)). The sum of these values is taken and negated to get the final loss, which is then rounded to four decimal places.\n\n- **Gradient Calculation**:\n  - The gradient is computed as the difference between the predictions `y_pred` and the targets `y`. Each element of the resulting matrix is rounded to four decimal places, and the matrix is converted to a nested list format to meet the output requirements.\n\nThis approach ensures numerical stability, correct handling of one-hot encoded targets, and precise rounding of results as specified."}
{"task_id": 360, "completion_id": 0, "completion": "To solve this problem, we need to compute the one-dimensional Discrete Cosine Transform (DCT-II) for a given signal frame. The DCT-II is a widely used transformation in signal processing and data compression. The task involves implementing the DCT-II formula directly and applying optional orthonormal scaling to ensure the resulting transform matrix is orthonormal.\n\n### Approach\n1. **Understand the DCT-II Formula**: The DCT-II coefficient for a signal frame is calculated using the formula:\n   \\[\n   c_k = 2 \\sum_{n=0}^{N-1} x_n \\cos\\left(\\frac{\\pi k(2n+1)}{2N}\\right)\n   \\]\n   where \\(N\\) is the length of the input signal frame, \\(x_n\\) is the nth element of the input signal, and \\(k\\) ranges from 0 to \\(N-1\\).\n\n2. **Orthonormal Scaling**: If the `orthonormal` parameter is set to True, the coefficients must be scaled to ensure orthonormality. The scaling factors are:\n   - For \\(k=0\\): Multiply by \\(1/\\sqrt{N}\\)\n   - For \\(k \\geq 1\\): Multiply by \\(\\sqrt{2/N}\\)\n\n3. **Direct Implementation**: Use nested loops to compute each coefficient directly from the formula without relying on external libraries for DCT computation.\n\n4. **Rounding**: The computed coefficients are rounded to four decimal places before returning them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dct(frame: np.ndarray | list[float], orthonormal: bool = True) -> list[float]:\n    x = np.array(frame)\n    N = x.size\n    coefficients = np.zeros(N)\n    for k in range(N):\n        sum_total = 0.0\n        for n in range(N):\n            angle = np.pi * k * (2 * n + 1) / (2 * N)\n            sum_total += x[n] * np.cos(angle)\n        c_k = 2 * sum_total\n        if orthonormal:\n            if k == 0:\n                scale = np.sqrt(1 / N)\n            else:\n                scale = np.sqrt(2 / N)\n            c_k *= scale\n        coefficients[k] = c_k\n    rounded = np.round(coefficients, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input signal frame is converted to a NumPy array for efficient computation.\n2. **Coefficient Calculation**: For each coefficient \\(c_k\\), the sum over all elements of the input signal is computed using the DCT-II formula. This sum is then doubled as per the formula.\n3. **Orthonormal Scaling**: If orthonormal scaling is required, each coefficient is adjusted using the specified scaling factors based on the value of \\(k\\).\n4. **Rounding and Conversion**: The computed coefficients are rounded to four decimal places and converted to a list for the final output.\n\nThis approach ensures that the DCT-II is computed accurately and efficiently, adhering to the given specifications and ensuring orthonormality when required."}
{"task_id": 362, "completion_id": 0, "completion": "To solve this problem, we need to convert a given 2-D list or NumPy array of non-negative numbers into a row-stochastic matrix. A row-stochastic matrix is one where each row sums to 1. If any row in the input matrix sums to zero, normalization is impossible, and we should return -1 in such cases.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input is converted to a NumPy array of floats to handle both lists and NumPy arrays uniformly and to facilitate numerical operations.\n2. **Compute Row Sums**: Calculate the sum of each row. If any row sums to zero, return -1 immediately as normalization is impossible.\n3. **Normalize Rows**: For each row, divide every element by the row's sum. This ensures that each row sums to 1.\n4. **Round Values**: Round each element of the resulting matrix to 4 decimal places for precision.\n5. **Convert to Python List**: Convert the normalized and rounded NumPy array back to a Python list of lists before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    arr = np.array(data, dtype=float)\n    row_sums = arr.sum(axis=1)\n    if np.any(row_sums == 0):\n        return -1\n    normalized = arr / row_sums[:, np.newaxis]\n    rounded = np.round(normalized, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input data is converted to a float NumPy array to ensure all elements are treated as floating-point numbers, which is necessary for division operations.\n2. **Row Sum Calculation**: Using `arr.sum(axis=1)`, we compute the sum of each row. This results in a 1-D array where each element corresponds to the sum of a row in the input matrix.\n3. **Check for Zero Sums**: If any element in the row sums array is zero, normalization is impossible, so we return -1 immediately.\n4. **Row Normalization**: Each element in a row is divided by the row's sum using broadcasting. `row_sums[:, np.newaxis]` reshapes the row sums into a column vector, allowing division across all columns of each row.\n5. **Rounding and Conversion**: The normalized matrix is rounded to 4 decimal places using `np.round`, and the result is converted back to a Python list of lists using `tolist()` to meet the output requirements.\n\nThis approach efficiently handles the conversion, normalization, and rounding, ensuring the solution is both correct and optimized for performance."}
{"task_id": 363, "completion_id": 0, "completion": "To solve this problem, we need to determine whether the action and observation spaces of a reinforcement learning (RL) environment are continuous. The continuity of these spaces is determined based on their structure and the types of their subspaces. \n\n### Approach\nThe approach involves checking the structure of the action and observation spaces using flags provided as input. The flags indicate whether the respective space is a composite type (Tuple or Dict). For composite spaces, we need to check all subspaces to ensure they are continuous (i.e., all are Box instances). For non-composite spaces, we simply check if the space itself is a Box.\n\n1. **Check Composite Spaces**: If the flag for a space (action or observation) is set to `True`, indicating it is a Tuple or Dict, we recursively check all subspaces to ensure they are all Box instances.\n2. **Check Non-Composite Spaces**: If the flag is `False`, we directly check if the space is an instance of Box.\n\n### Solution Code\n```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    def all_subspaces_are_box(space):\n        if isinstance(space, Tuple):\n            return all(isinstance(s, Box) for s in space.spaces)\n        elif isinstance(space, Dict):\n            return all(isinstance(s, Box) for s in space.spaces.values())\n        else:\n            return False\n    \n    cont_action = all_subspaces_are_box(env.action_space) if tuple_action else isinstance(env.action_space, Box)\n    cont_obs = all_subspaces_are_box(env.observation_space) if tuple_obs else isinstance(env.observation_space, Box)\n    \n    return (cont_action, cont_obs)\n```\n\n### Explanation\n- **Helper Function `all_subspaces_are_box`**: This function checks if all subspaces of a given composite space (either Tuple or Dict) are Box instances. For a Tuple, it iterates through all subspaces, and for a Dict, it checks all values in the dictionary of subspaces.\n- **Continuity Check**: The main function uses the helper function to determine the continuity of the action and observation spaces based on the provided flags. If the flag indicates a composite space, the helper function is used; otherwise, it checks if the space itself is a Box.\n\nThis approach ensures that we correctly identify continuous spaces both for simple and composite structures, adhering to the problem constraints and requirements."}
{"task_id": 364, "completion_id": 0, "completion": "To solve this problem, we need to implement the mathematical sign function, which returns 1 for positive numbers, -1 for negative numbers, and 0 for zero. The function should handle both integers and floating-point numbers.\n\n### Approach\nThe approach to solving this problem involves checking the value of the input number and determining its sign based on its value:\n1. If the number is greater than 0, return 1.\n2. If the number is less than 0, return -1.\n3. If the number is exactly 0 (whether as an integer or floating-point zero), return 0.\n\nThis approach leverages straightforward comparison operations which work seamlessly with both integers and floats in Python. The comparisons handle all real numbers as specified, including edge cases like floating-point zero (0.0) and negative zero (-0.0), which are treated as zero in Python.\n\n### Solution Code\n```python\ndef sign(x: int | float) -> int:\n    \"\"\"Return the sign of *x*.\n\n    The function should return 1 for positive numbers, -1 for negative numbers,\n    and 0 when the input is exactly zero.\n\n    Args:\n        x: A real number (int or float).\n\n    Returns:\n        -1, 0, or 1 depending on the sign of *x*.\n    \"\"\"\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else:\n        return 0\n```\n\n### Explanation\n- **Positive Check**: If the input `x` is greater than 0, the function returns 1. This handles all positive integers and floats.\n- **Negative Check**: If the input `x` is less than 0, the function returns -1. This handles all negative integers and floats.\n- **Zero Check**: If the input `x` is exactly 0 (either integer 0, float 0.0, or -0.0), the function returns 0. This is because Python treats these values as equal in comparisons.\n\nThis solution efficiently determines the sign of a real number using simple conditional checks, ensuring correctness for all valid inputs as specified."}
{"task_id": 366, "completion_id": 0, "completion": "To solve this problem, we need to compute the pair-wise squared Euclidean distance matrix for a set of samples efficiently using vectorized operations in NumPy. The goal is to avoid loops and ensure numerical stability by clamping negative values to zero and ensuring the diagonal entries are exactly zero.\n\n### Approach\n1. **Sum of Squares Calculation**: For each sample, compute the sum of the squares of its features. This helps in decomposing the Euclidean distance formula.\n2. **Gram Matrix Calculation**: Compute the dot product of each pair of samples, which forms the Gram matrix.\n3. **Distance Matrix Calculation**: Using the sum of squares and the Gram matrix, compute the squared Euclidean distances using the formula derived from expanding the squared difference.\n4. **Clamp Negative Values**: Due to numerical precision issues, some values might be slightly negative. These are clamped to zero.\n5. **Set Diagonal Entries**: Explicitly set the diagonal entries of the matrix to zero to ensure they are exactly zero.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Compute the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X (np.ndarray): A 2-D array with shape (n_samples, n_features).\n\n    Returns:\n        list[list[float]]: The n\u00d7n squared distance matrix where the element at\n            position (i, j) equals the squared \u2113\u2082 distance between samples i and\n            j. The diagonal must be zeros.\n    \"\"\"\n    if X.size == 0:\n        return []\n    sum_X = np.sum(X**2, axis=1)\n    gram = X @ X.T\n    distances = sum_X[:, np.newaxis] + sum_X[np.newaxis, :] - 2 * gram\n    np.maximum(distances, 0, out=distances)\n    np.fill_diagonal(distances, 0.0)\n    return distances.tolist()\n```\n\n### Explanation\n1. **Sum of Squares (sum_X)**: This vector holds the sum of the squares of each sample's features. It is computed using `np.sum(X**2, axis=1)`.\n2. **Gram Matrix (gram)**: This matrix is the result of the dot product of each pair of samples, computed efficiently using matrix multiplication `X @ X.T`.\n3. **Distance Calculation**: Using broadcasting, we expand the sum of squares into a matrix form and compute the squared Euclidean distances using the formula derived from algebraic expansion. The formula used is `sum_X[i] + sum_X[j] - 2 * gram[i][j]` for each pair (i, j).\n4. **Clamping Negative Values**: The `np.maximum` function ensures all negative values due to numerical errors are clamped to zero.\n5. **Diagonal Entries**: Explicitly setting the diagonal entries to zero ensures they are exactly zero, as required.\n\nThis approach efficiently computes the distance matrix using vectorized operations, ensuring both performance and numerical stability."}
{"task_id": 367, "completion_id": 0, "completion": "To solve this problem, we need to create a factory function `initialize_kernel` that can convert various input representations into instances of supported kernel classes: `LinearKernel`, `PolynomialKernel`, and `RBFKernel`. The function should handle inputs such as `None`, existing kernel objects, string descriptions, and dictionaries, and return the appropriate kernel instance.\n\n### Approach\nThe approach to solve this problem involves handling each type of input separately:\n\n1. **None Input**: Return a default `LinearKernel` instance.\n2. **Kernel Object**: Return the object as-is if it is an instance of `KernelBase`.\n3. **String Description**: Parse the string to determine the kernel type and parameters. The string can be in the form of a kernel name with optional parameters (e.g., \"RBF(gamma=0.5)\"). The parameters are extracted, converted to appropriate types, and used to instantiate the kernel.\n4. **Dictionary**: Check for the presence of the `hyperparameters` key and validate its contents. The dictionary should contain the kernel type and parameters, which are used to instantiate the kernel.\n\n### Solution Code\n```python\nimport re\nfrom typing import Any, Dict, Optional\n\nclass KernelBase:\n    \"\"\"Minimal base class for all kernels.  Do NOT modify this class.\"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        self.params: Dict[str, Any] = kwargs\n\n    def set_params(self, state: Dict[str, Any]):\n        \"\"\"(Re)sets parameters from a previously stored summary dict.\"\"\"\n        hp = state.get(\"hyperparameters\", {})\n        self.params.update({k: v for k, v in hp.items() if k != \"id\"})\n        return self\n\n    def summary(self) -> Dict[str, Any]:\n        \"\"\"Dictionary description that can be fed back to *initialize_kernel*.\"\"\"\n        return {\"hyperparameters\": {\"id\": self.__class__.__name__, **self.params}}\n\n\nclass LinearKernel(KernelBase):\n    def __init__(self):\n        super().__init__()\n\n\nclass PolynomialKernel(KernelBase):\n    def __init__(self, degree: int = 3, coef0: float = 1.0):\n        super().__init__(degree=degree, coef0=coef0)\n\n\nclass RBFKernel(KernelBase):\n    def __init__(self, gamma: float = 1.0):\n        super().__init__(gamma=gamma)\n\n\ndef initialize_kernel(param: Optional[Any] = None) -> KernelBase:\n    \"\"\"Create and return a kernel object from various representations.\"\"\"\n\n    if param is None:\n        return LinearKernel()\n\n    if isinstance(param, KernelBase):\n        return param\n\n    if isinstance(param, str):\n        match = re.match(r\"^\\s*([a-zA-Z]+)\\s*(?:\\((.*)\\))?\\s*$\", param, re.IGNORECASE)\n        if not match:\n            raise NotImplementedError(f\"Unknown kernel specification: {param}\")\n        kernel_name = match.group(1).lower()\n        params_str = match.group(2)\n\n        params = {}\n        if params_str is not None:\n            for pair in params_str.split(','):\n                pair = pair.strip()\n                if not pair:\n                    continue\n                if '=' not in pair:\n                    raise ValueError(f\"Invalid parameter format in kernel string: {pair}\")\n                key, value_str = pair.split('=', 1)\n                key = key.strip()\n                value_str = value_str.strip()\n\n                def parse_value(s):\n                    s = s.strip()\n                    lower_s = s.lower()\n                    if lower_s == 'true':\n                        return True\n                    elif lower_s == 'false':\n                        return False\n                    elif lower_s == 'none':\n                        return None\n                    try:\n                        return int(s)\n                    except ValueError:\n                        pass\n                    try:\n                        return float(s)\n                    except ValueError:\n                        pass\n                    if len(s) >= 2 and s[0] == s[-1] and s[0] in ('\"', \"'\"):\n                        return s[1:-1]\n                    return s\n                value = parse_value(value_str)\n                params[key] = value\n\n        kernel_map = {\n            'linear': LinearKernel,\n            'polynomial': PolynomialKernel,\n            'rbf': RBFKernel\n        }\n        kernel_cls = kernel_map.get(kernel_name)\n        if kernel_cls is None:\n            raise NotImplementedError(f\"Unknown kernel type: {kernel_name}\")\n        try:\n            return kernel_cls(**params)\n        except TypeError as e:\n            raise TypeError(f\"Invalid parameters for {kernel_cls.__name__}: {e}\")\n\n    elif isinstance(param, dict):\n        if 'hyperparameters' not in param:\n            raise ValueError(\"Dictionary parameter missing 'hyperparameters' key\")\n        hp = param['hyperparameters']\n        if 'id' not in hp:\n            raise ValueError(\"hyperparameters missing 'id' key\")\n        kernel_id = hp['id']\n        kernel_classes = {\n            'LinearKernel': LinearKernel,\n            'PolynomialKernel': PolynomialKernel,\n            'RBFKernel': RBFKernel\n        }\n        if kernel_id not in kernel_classes:\n            raise NotImplementedError(f\"Unknown kernel ID: {kernel_id}\")\n        kernel_cls = kernel_classes[kernel_id]\n        params = {k: v for k, v in hp.items() if k != 'id'}\n        try:\n            return kernel_cls(**params)\n        except TypeError as e:\n            raise TypeError(f\"Invalid parameters for {kernel_cls.__name__}: {e}\")\n\n    else:\n        raise NotImplementedError(f\"Unsupported kernel specification type: {type(param)}\")\n```\n\n### Explanation\n1. **None Input**: Directly returns a `LinearKernel` instance.\n2. **Existing Kernel Object**: Checks if the input is an instance of `KernelBase` and returns it.\n3. **String Handling**: Uses regular expressions to parse the kernel name and parameters. Parameters are converted to appropriate types (e.g., integers, floats, booleans) using a helper function.\n4. **Dictionary Handling**: Validates the presence of required keys (`hyperparameters` and `id`). Extracts parameters and instantiates the appropriate kernel class.\n5. **Error Handling**: Raises appropriate errors for invalid inputs, such as unknown kernel types or missing keys in dictionaries.\n\nThis approach ensures that the function correctly handles all specified input types and converts them into the appropriate kernel instances, adhering to the problem constraints and requirements."}
{"task_id": 368, "completion_id": 0, "completion": "To solve this problem, we need to compute the Minkowski distance between two one-dimensional vectors. The Minkowski distance is a generalization of various distance metrics, such as the Manhattan distance (p=1) and the Euclidean distance (p=2). The task involves validating input parameters, computing the distance, and rounding the result to four decimal places.\n\n### Approach\n1. **Input Validation**: \n   - Convert input vectors to NumPy arrays.\n   - Check if both vectors are one-dimensional and of the same length.\n   - Ensure the order parameter `p` is a positive real number.\n   \n2. **Compute Differences**:\n   - Calculate the absolute differences between corresponding elements of the two vectors.\n   \n3. **Sum of Powered Differences**:\n   - Raise each absolute difference to the power of `p` and sum these values.\n   \n4. **Distance Calculation**:\n   - Take the p-th root of the summed powered differences to get the Minkowski distance.\n   \n5. **Rounding**:\n   - Round the computed distance to four decimal places before returning it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef minkowski(x: np.ndarray | list, y: np.ndarray | list, p: float) -> float:\n    \"\"\"Compute the Minkowski\u2013p distance between two vectors.\n\n    Args:\n        x (np.ndarray | list): First one-dimensional vector.\n        y (np.ndarray | list): Second one-dimensional vector of the same length.\n        p (float): Order of the norm (must be > 0).\n\n    Returns:\n        float: The Minkowski distance rounded to 4 decimal places, or \u20111 when\n               input validation fails.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    \n    # Check if both are 1-dimensional\n    if x_arr.ndim != 1 or y_arr.ndim != 1:\n        return -1\n    \n    # Check if shapes are identical\n    if x_arr.shape != y_arr.shape:\n        return -1\n    \n    # Check if p is positive\n    if p <= 0:\n        return -1\n    \n    # Calculate absolute differences\n    differences = np.abs(x_arr - y_arr)\n    \n    # Compute the sum of each element raised to the power of p\n    sum_powered = np.sum(differences ** p)\n    \n    # Compute the p-th root of the sum\n    distance = sum_powered ** (1.0 / p)\n    \n    # Round to four decimal places\n    return round(distance, 4)\n```\n\n### Explanation\n1. **Input Conversion and Validation**: The input vectors are converted to NumPy arrays to leverage NumPy operations. We then check if both vectors are one-dimensional and of the same length. Additionally, we ensure the order parameter `p` is positive. If any validation fails, the function returns -1.\n\n2. **Difference Calculation**: The absolute differences between corresponding elements of the vectors are computed using NumPy's `abs` function for efficient element-wise operations.\n\n3. **Summing Powered Differences**: Each element of the difference array is raised to the power of `p` and summed. This step is efficiently handled using NumPy's vectorized operations.\n\n4. **Distance Calculation**: The p-th root of the summed powered differences is computed to obtain the Minkowski distance. This is done using the exponentiation operator.\n\n5. **Rounding**: The result is rounded to four decimal places using Python's built-in `round` function to meet the specified precision requirement.\n\nThis approach ensures that the function is efficient, handles edge cases appropriately, and adheres to the mathematical definition of the Minkowski distance."}
{"task_id": 369, "completion_id": 0, "completion": "To solve this problem, we need to aggregate the outputs of multiple weak learners (decision trees) in a Gradient Boosting model for both regression and classification tasks. The aggregation involves starting with a zero-initialized running prediction and iteratively updating it by subtracting the scaled outputs of each tree. The final prediction is derived differently for regression and classification tasks.\n\n### Approach\n1. **Sum Updates**: Sum the outputs of all trees. Each tree's output is scaled by a learning rate and subtracted from the running prediction.\n2. **Running Prediction**: Compute the running prediction by scaling the summed updates with the learning rate and negating the result.\n3. **Regression Handling**: For regression tasks, return the running prediction rounded to four decimal places.\n4. **Classification Handling**: For classification tasks, convert the running prediction (logits) to probabilities using the softmax function and then determine the class labels using the argmax function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\"\"\"\n    sum_updates = sum(updates)\n    running_pred = -learning_rate * sum_updates\n\n    if regression:\n        return np.round(running_pred, 4)\n    else:\n        # Compute softmax probabilities\n        logits = running_pred\n        max_logits = np.max(logits, axis=1, keepdims=True)\n        shifted_logits = logits - max_logits\n        exp_logits = np.exp(shifted_logits)\n        softmax = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        return np.argmax(softmax, axis=1).astype(int)\n```\n\n### Explanation\n1. **Summing Updates**: The sum of all tree outputs is computed using `sum(updates)`, which efficiently combines the outputs element-wise.\n2. **Running Prediction Calculation**: The running prediction is obtained by scaling the summed updates with the learning rate and negating the result. This is done using vectorized operations for efficiency.\n3. **Regression Handling**: For regression, the running prediction is directly returned after rounding to four decimal places using `np.round`.\n4. **Classification Handling**: For classification, the running prediction (logits) is converted to probabilities using the softmax function. To ensure numerical stability, the maximum logit value is subtracted from each logit before exponentiation. The softmax probabilities are then used to determine the class labels using `argmax`.\n\nThis approach ensures that the solution is fully vectorized, efficient, and adheres to the mathematical principles of Gradient Boosting for both regression and classification tasks."}
{"task_id": 370, "completion_id": 0, "completion": "To solve this problem, we need to implement the multivariate Gaussian probability density function. This function calculates the density value for each sample in a given dataset under a multivariate normal distribution with specified mean and covariance matrix.\n\n### Approach\nThe multivariate Gaussian density function is given by:\n\n\\[ f(x) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right) \\]\n\nWhere:\n- \\( X \\) is a 2-D array where each row is a sample.\n- \\( \\mu \\) is the mean vector of the distribution.\n- \\( \\Sigma \\) is the covariance matrix (positive-definite).\n\nThe steps to compute the density values are as follows:\n1. **Compute the determinant and inverse of the covariance matrix**: These are needed for the normalization factor and the quadratic form in the exponent.\n2. **Calculate the difference between each sample and the mean vector**: This is done using broadcasting.\n3. **Compute the quadratic form**: This involves matrix operations to compute the Mahalanobis distance for each sample.\n4. **Normalization factor**: Derived from the determinant of the covariance matrix and the dimensionality of the data.\n5. **Exponential term**: Calculated using the quadratic form results.\n6. **Combine normalization and exponential terms**: Multiply these to get the density values and round them to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef multivariate_gaussian(X: np.ndarray, mu: np.ndarray, cov: np.ndarray) -> list[float]:\n    m, n = X.shape\n    det = np.linalg.det(cov)\n    inv_cov = np.linalg.inv(cov)\n    diff = X - mu  # Broadcasting mu to each row of X\n    quadratic = np.sum((diff @ inv_cov) * diff, axis=1)\n    normalization = 1.0 / ((2 * np.pi) ** (n / 2) * np.sqrt(det))\n    densities = normalization * np.exp(-0.5 * quadratic)\n    return [round(density, 4) for density in densities.tolist()]\n```\n\n### Explanation\n1. **Determinant and Inverse Calculation**:\n   - The determinant of the covariance matrix is computed using `np.linalg.det`.\n   - The inverse of the covariance matrix is computed using `np.linalg.inv`, which is efficient for positive-definite matrices.\n\n2. **Difference Calculation**:\n   - Subtracting the mean vector from each sample in the dataset is done efficiently using NumPy broadcasting.\n\n3. **Quadratic Form**:\n   - The quadratic form \\((x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\) for each sample is computed using matrix multiplication and element-wise product summation, which is vectorized for efficiency.\n\n4. **Normalization Factor**:\n   - The normalization factor combines the determinant and the dimensionality of the data to scale the exponential term correctly.\n\n5. **Exponential Term**:\n   - The exponential term is calculated using the quadratic form results, scaled by -0.5.\n\n6. **Result Compilation**:\n   - The final densities are computed by combining the normalization and exponential terms, then rounded to four decimal places for each sample.\n\nThis approach ensures efficient computation using vectorized operations, making it suitable for large datasets while maintaining numerical stability and precision."}
{"task_id": 371, "completion_id": 0, "completion": "To solve this problem, we need to implement a 1-D cross-correlation function that supports batches of examples, multiple input and output channels, strides, padding (as an integer, tuple, or \"same\"), and dilation. The goal is to produce the resulting 3-D volume as a Python list using numpy for efficient array operations.\n\n### Approach\n1. **Padding Calculation**: Determine the padding values (left and right) based on the input parameters. The padding can be specified as an integer, a tuple, or \"same\" to mimic TensorFlow's \"SAME\" padding.\n2. **Input Padding**: Apply the calculated padding to the input signal.\n3. **Effective Kernel Size**: Compute the effective kernel size considering dilation, which affects the spacing between kernel elements.\n4. **Output Length Calculation**: Determine the length of the output signal based on the padded input length, effective kernel size, and stride.\n5. **Indices Generation**: Generate indices for the sliding window positions considering the stride and dilation.\n6. **Window Extraction and Convolution**: For each example in the batch, extract windows from the padded input and perform the cross-correlation with the kernel using vectorized operations.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\"\"\"\n\n    n_examples, l_in, in_channels = X.shape\n    kernel_width = W.shape[0]\n    out_channels = W.shape[2]\n\n    # Calculate padding\n    if pad == 'same':\n        out_len = np.ceil(l_in / stride).astype(int)\n        effective_kernel = (kernel_width - 1) * (dilation + 1) + 1\n        total_pad = max(0, (out_len - 1) * stride + effective_kernel - l_in)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n    elif isinstance(pad, int):\n        p_left = pad\n        p_right = pad\n    elif isinstance(pad, tuple) and len(pad) == 2:\n        p_left, p_right = pad\n    else:\n        raise ValueError(\"Invalid pad argument. Must be 'same', int, or 2-tuple.\")\n\n    # Pad the input\n    X_padded = np.pad(X, [(0, 0), (p_left, p_right), (0, 0)], mode='constant')\n    padded_length = X_padded.shape[1]\n\n    # Compute effective kernel size\n    effective_kernel_size = (kernel_width - 1) * (dilation + 1) + 1\n\n    # Calculate output length\n    output_length = (padded_length - effective_kernel_size) // stride + 1\n    output_length = max(0, output_length)\n\n    if output_length == 0:\n        return np.zeros((n_examples, 0, out_channels)).tolist()\n\n    # Generate indices for sliding window\n    output_positions = np.arange(output_length) * stride\n    kernel_offsets = np.arange(kernel_width) * (dilation + 1)\n    indices = output_positions[:, None] + kernel_offsets[None, :]  # (output_length, kernel_width)\n\n    # Check if indices are within bounds\n    valid_indices = (indices >= 0) & (indices < padded_length)\n    indices = np.where(valid_indices, indices, 0)  # Use 0 as dummy index, values will be multiplied by 0 later\n\n    # Prepare output array\n    output = np.zeros((n_examples, output_length, out_channels))\n\n    for ex_idx in range(n_examples):\n        example = X_padded[ex_idx]  # (padded_length, in_channels)\n        window = example[indices]  # (output_length, kernel_width, in_channels)\n        # Apply mask for out-of-bound indices\n        window = window * valid_indices[..., None]\n        # Compute cross-correlation\n        example_output = np.einsum('okc,kco->oc', window, W)\n        output[ex_idx] = example_output\n\n    return output.tolist()\n```\n\n### Explanation\n1. **Padding Calculation**: Depending on the input `pad`, the function calculates the left and right padding. For \"same\" padding, it ensures the output length matches the input length divided by stride, rounded up.\n2. **Input Padding**: The input signal is padded with zeros on both ends based on the calculated padding values.\n3. **Effective Kernel Size**: The kernel size is adjusted for dilation, which determines the spacing between kernel elements.\n4. **Output Length Calculation**: The output length is determined by the padded input length, effective kernel size, and stride.\n5. **Indices Generation**: Indices for the sliding window positions are generated to efficiently extract the required windows from the padded input.\n6. **Window Extraction and Convolution**: For each example, windows are extracted from the padded input using the generated indices. The cross-correlation is performed using matrix multiplication to sum over the kernel and input channels efficiently.\n\nThis approach ensures the solution handles various padding strategies, dilation, and stride efficiently using numpy's vectorized operations, making it both correct and performant."}
{"task_id": 373, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Gini impurity of a sequence of integer class labels. The Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. \n\n### Approach\n1. **Check for Empty Input**: If the input sequence is empty, return 0.0 immediately as per the problem's convention.\n2. **Count Unique Labels**: Use NumPy's `unique` function to count the occurrences of each unique class label in the input sequence. This function efficiently returns both the unique elements and their respective counts.\n3. **Calculate Probabilities**: For each unique class label, compute the probability by dividing the count of the class by the total number of samples.\n4. **Sum of Squares of Probabilities**: Compute the sum of the squares of these probabilities. This sum is used to determine the Gini impurity.\n5. **Compute Gini Impurity**: Subtract the sum of the squared probabilities from 1 to get the Gini impurity.\n6. **Round the Result**: Round the computed Gini impurity to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    if len(y) == 0:\n        return 0.0\n    _, counts = np.unique(y, return_counts=True)\n    probs = counts / len(y)\n    sum_squares = np.sum(probs ** 2)\n    return round(1.0 - sum_squares, 4)\n```\n\n### Explanation\n- **Empty Input Handling**: The function first checks if the input is empty and returns 0.0 if true. This handles edge cases where there are no labels to process.\n- **Unique Labels and Counts**: Using `np.unique` with `return_counts=True` efficiently retrieves the unique class labels and their counts. This is more efficient than manually iterating and counting, especially for large datasets.\n- **Probability Calculation**: Each count is divided by the total number of samples to get the probability of each class. This step ensures we are working with relative frequencies.\n- **Sum of Squares**: Squaring each probability and summing them gives a measure of the purity of the dataset. A higher sum of squares indicates lower impurity.\n- **Gini Impurity Calculation**: Subtracting the sum of squared probabilities from 1 gives the Gini impurity. This value is then rounded to four decimal places to meet the specified precision requirement.\n\nThis approach efficiently computes the Gini impurity using vectorized operations provided by NumPy, ensuring both clarity and performance."}
{"task_id": 374, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that trains a multiple linear regression model using batch gradient descent. The goal is to minimize the mean-squared error (MSE) by iteratively updating the model parameters (weights and bias) using the gradients of the loss function.\n\n### Approach\n1. **Check Data Consistency**: Verify that the number of samples in the feature matrix `X` matches the number of targets in vector `y`. If not, return -1.\n2. **Initialize Parameters**: Start with weights (\u03b8) as a vector of zeros and bias (b) as zero.\n3. **Iterative Gradient Descent**: For a specified number of iterations, compute the predicted values, calculate the error, and update the weights and bias using the gradients derived from the error.\n4. **Return Results**: After completing the iterations, return the weights and bias rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\"\"\"\n    \n    # Check if the number of samples in X and y are the same\n    if X.shape[0] != y.shape[0]:\n        return -1\n    \n    m, n = X.shape  # m is number of samples, n is number of features\n    y = y.ravel()   # Ensure y is a 1D array\n    \n    # Initialize parameters\n    theta = np.zeros(n)\n    b = 0.0\n    \n    # Perform gradient descent for n_iterations\n    for _ in range(n_iterations):\n        y_hat = X.dot(theta) + b  # Compute predictions\n        error = y_hat - y         # Compute error\n        \n        # Compute gradients\n        dw = (2 / m) * X.T.dot(error)\n        db = (2 / m) * np.sum(error)\n        \n        # Update parameters\n        theta -= learning_rate * dw\n        b -= learning_rate * db\n    \n    # Round the results to 4 decimal places\n    theta_rounded = [round(val, 4) for val in theta]\n    b_rounded = round(b, 4)\n    \n    return (theta_rounded, b_rounded)\n```\n\n### Explanation\n1. **Data Consistency Check**: The function first checks if the number of rows in `X` (samples) matches the length of `y`. If not, it returns -1 to indicate inconsistent data.\n2. **Parameter Initialization**: The weights (\u03b8) are initialized as a zero vector with the same number of features as `X`, and the bias (b) is initialized to zero.\n3. **Gradient Descent Iterations**:\n   - **Prediction Calculation**: For each iteration, compute the predicted values `y_hat` using the current weights and bias.\n   - **Error Calculation**: Determine the error between the predicted values and the actual targets.\n   - **Gradient Calculation**: Compute the gradients of the loss with respect to the weights (dw) and bias (db).\n   - **Parameter Update**: Adjust the weights and bias using the computed gradients and the learning rate.\n4. **Result Rounding**: After completing all iterations, the weights and bias are rounded to four decimal places to meet the specified precision requirement.\n\nThis approach efficiently minimizes the MSE using batch gradient descent, ensuring that the model parameters are iteratively adjusted to find the optimal fit for the given data."}
{"task_id": 375, "completion_id": 0, "completion": "To solve this problem, we need to compute the amount of zero-padding required on each side of a 4-D input tensor to ensure that a 2-D convolution operation produces an output of the desired spatial dimensions. This involves considering the kernel size, stride, and dilation factors to determine the effective filter size and then calculating the necessary padding to achieve the specified output dimensions.\n\n### Approach\n1. **Type Checking**: Validate the input parameters to ensure they are of the correct type and structure. This includes checking that the input tensor shape, output dimensions, kernel shape, stride, and dilation are all valid integers or tuples of integers.\n2. **Effective Filter Size Calculation**: Compute the effective size of the convolution kernel considering the dilation factor. This adjusts the kernel size to account for the spaces introduced by dilation.\n3. **Total Padding Calculation**: Determine the total padding required for both the height (rows) and width (columns) dimensions using the formula derived from the convolution output size equation. This formula takes into account the desired output dimensions, input dimensions, effective kernel size, and stride.\n4. **Padding Distribution**: Distribute the total padding between the top/bottom (for rows) and left/right (for columns) sides. If the total padding is odd, the extra padding is allocated to the bottom and right sides to mimic TensorFlow's behavior.\n5. **Error Handling**: Raise appropriate errors if the input parameters are invalid or if the computed padding is negative, indicating an impossible output size.\n\n### Solution Code\n```python\ndef calc_pad_dims_2D(X_shape: tuple,\n                     out_dim: tuple,\n                     kernel_shape: tuple,\n                     stride: int,\n                     dilation: int = 0) -> tuple:\n    \"\"\"Return the amount of zero-padding needed on each side of a 4-D tensor.\n\n    Parameters\n    ----------\n    X_shape : tuple\n        Shape of the input tensor ``(n_ex, in_rows, in_cols, in_ch)``.\n    out_dim : tuple\n        Desired output height and width ``(out_rows, out_cols)``.\n    kernel_shape : tuple\n        Height and width of the convolution kernel ``(fr, fc)``.\n    stride : int\n        Stride used by the convolution.\n    dilation : int, optional (default=0)\n        Dilation factor (number of inserted zeros between kernel elements).\n\n    Returns\n    -------\n    tuple\n        A 4-tuple ``(top, bottom, left, right)`` indicating how many zero rows\n        and columns must be added to the **top**, **bottom**, **left**, and\n        **right** sides of the input volume.\n    \"\"\"\n    # Validate input types\n    if not (isinstance(X_shape, tuple) and len(X_shape) == 4 and all(isinstance(d, int) for d in X_shape)):\n        raise ValueError(\"X_shape must be a tuple of four integers\")\n    if not (isinstance(out_dim, tuple) and len(out_dim) == 2 and all(isinstance(d, int) for d in out_dim)):\n        raise ValueError(\"out_dim must be a tuple of two integers\")\n    if not (isinstance(kernel_shape, tuple) and len(kernel_shape) == 2 and all(isinstance(k, int) for k in kernel_shape)):\n        raise ValueError(\"kernel_shape must be a tuple of two integers\")\n    if not isinstance(stride, int):\n        raise ValueError(\"stride must be an integer\")\n    if not isinstance(dilation, int):\n        raise ValueError(\"dilation must be an integer\")\n\n    # Extract input dimensions\n    in_rows = X_shape[1]\n    in_cols = X_shape[2]\n    fr, fc = kernel_shape\n    out_rows, out_cols = out_dim\n\n    # Compute effective filter sizes considering dilation\n    effective_fr = fr * (dilation + 1) - dilation\n    effective_fc = fc * (dilation + 1) - dilation\n\n    # Calculate total padding required for rows and columns\n    total_pad_row = (out_rows - 1) * stride + effective_fr - in_rows\n    total_pad_col = (out_cols - 1) * stride + effective_fc - in_cols\n\n    # Check if padding is non-negative\n    if total_pad_row < 0 or total_pad_col < 0:\n        raise ValueError(\"Impossible output size: negative padding required\")\n\n    # Distribute padding with TensorFlow-like handling for odd numbers\n    up_pad = total_pad_row // 2\n    down_pad = total_pad_row - up_pad\n\n    left_pad = total_pad_col // 2\n    right_pad = total_pad_col - left_pad\n\n    return (up_pad, down_pad, left_pad, right_pad)\n```\n\n### Explanation\n1. **Type Checking**: Ensures all inputs are of the correct type and structure, raising a `ValueError` if any check fails.\n2. **Effective Filter Size**: Adjusts the kernel dimensions to account for dilation, effectively increasing the kernel size based on the number of zeros inserted between kernel elements.\n3. **Total Padding Calculation**: Uses the convolution output formula to compute the minimum padding required to achieve the desired output dimensions. This involves solving for padding in the equation derived from the output size formula.\n4. **Padding Distribution**: Splits the total padding into top/bottom and left/right, ensuring that any odd padding is allocated to the bottom and right sides, as per TensorFlow's convention.\n5. **Error Handling**: Checks for negative padding values, which indicate impossible output dimensions given the input parameters, and raises an appropriate error."}
{"task_id": 376, "completion_id": 0, "completion": "To solve this problem, we need to determine if a given 2-D numeric array is symmetric with respect to its main diagonal. A matrix is symmetric if it is square (same number of rows and columns) and the element at row i and column j is equal to the element at row j and column i for all valid indices (i, j). Additionally, due to potential floating-point rounding errors, we must treat two numbers as equal if their absolute difference does not exceed 1 \u00d7 10\u207b\u2078.\n\n### Approach\n1. **Check if the Matrix is Square**: First, verify that the input matrix is square. This means the number of rows must be equal to the number of columns. If any row has a different length than the number of rows, the matrix is not square, and we return False.\n2. **Convert to NumPy Array**: Convert the input list of lists into a NumPy array for efficient matrix operations.\n3. **Check Symmetry with Tolerance**: Use NumPy's `allclose` function to compare the matrix with its transpose. This function checks if all corresponding elements are close within a specified absolute tolerance (1e-8), effectively handling floating-point precision issues.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    n = len(X)\n    # Check if all rows have the same length as the number of rows (n)\n    for row in X:\n        if len(row) != n:\n            return False\n    # Convert to numpy array and check symmetry with tolerance\n    x_np = np.array(X)\n    return np.allclose(x_np, x_np.T, rtol=0, atol=1e-8)\n```\n\n### Explanation\n1. **Square Matrix Check**: The function first checks if the matrix is square by ensuring each row's length matches the number of rows. If any row fails this check, the function returns False immediately.\n2. **Conversion to NumPy Array**: Converting the input matrix to a NumPy array allows efficient matrix operations, including transposition and element-wise comparison.\n3. **Symmetry Check with Tolerance**: Using `np.allclose` with an absolute tolerance of 1e-8 ensures that minor floating-point discrepancies are ignored, providing a robust check for symmetry even in the presence of rounding errors.\n\nThis approach efficiently combines structural checks with numerical tolerance handling to accurately determine if the matrix is symmetric."}
{"task_id": 377, "completion_id": 0, "completion": "To solve this problem, we need to compute the negative gradient for the logistic loss function used in Gradient Boosting for binary classification. The goal is to transform the given binary labels and current prediction scores into the required negative gradients, ensuring all inputs are valid.\n\n### Approach\n1. **Input Validation**: Check if the provided labels (y) contain only 0s and 1s. If any other value is found, return -1.\n2. **Label Transformation**: Convert the binary labels from {0, 1} to {-1, 1} using the formula \\( y' = 2y - 1 \\).\n3. **Gradient Calculation**: Compute the negative gradient for each sample using the formula \\( g_i = \\frac{y'_i}{1 + \\exp(y'_i f_i)} \\), where \\( y'_i \\) is the transformed label and \\( f_i \\) is the current prediction score.\n4. **Rounding**: Round each computed gradient to four decimal places and return the result as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"Compute the negative gradient for logistic loss in binary classification.\"\"\"\n    y_arr = np.array(y)\n    if np.any((y_arr != 0) & (y_arr != 1)):\n        return -1\n    f_arr = np.array(f)\n    y_prime = 2 * y_arr - 1\n    denominator = 1 + np.exp(y_prime * f_arr)\n    g = y_prime / denominator\n    g_rounded = np.round(g, 4)\n    return g_rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The input labels are converted to a numpy array, and we check if all elements are either 0 or 1. If not, the function returns -1 immediately.\n2. **Label Transformation**: The labels are transformed using \\( y' = 2y - 1 \\), converting 0 to -1 and 1 to 1.\n3. **Gradient Calculation**: The denominator is computed using the exponential of the product of transformed labels and prediction scores, plus 1. Each gradient value is then calculated by dividing the transformed label by this denominator.\n4. **Rounding**: The results are rounded to four decimal places using numpy's rounding function and converted back to a list for the final output.\n\nThis approach ensures that all edge cases are handled efficiently, and the computations are vectorized for performance using numpy operations."}
{"task_id": 378, "completion_id": 0, "completion": "To solve this problem, we need to create a Python function that acts as an indicator (or Kronecker delta) function. This function will return 1 if the input integer is exactly 1 and 0 for any other integer value.\n\n### Approach\nThe approach to solve this problem is straightforward. We need to check if the input integer is 1. If it is, the function returns 1. For any other integer value, the function returns 0. This can be efficiently achieved using a conditional expression in Python, which allows us to return the appropriate value based on the condition in a single line of code.\n\n### Solution Code\n```python\ndef indicator(flag: int) -> int:\n    \"\"\"Return 1 if *flag* is 1, otherwise return 0.\n\n    Args:\n        flag (int): An integer value.\n\n    Returns:\n        int: 1 if flag == 1, else 0.\n    \"\"\"\n    return 1 if flag == 1 else 0\n```\n\n### Explanation\nThe function `indicator` takes an integer `flag` as input. The core logic checks if `flag` is equal to 1 using a conditional expression. If the condition evaluates to True (i.e., `flag` is 1), the function returns 1. If the condition evaluates to False (i.e., `flag` is any integer other than 1), the function returns 0. This solution efficiently handles all possible integer inputs as specified, ensuring the correct output with minimal computation."}
{"task_id": 379, "completion_id": 0, "completion": "To solve this problem, we need to sort a list of priority queue nodes based on their priority and entry ID, and then return the keys of these nodes in the sorted order. The sorting should be done such that nodes with smaller priority values come first, and if two nodes have the same priority, the one with the smaller entry ID comes first.\n\n### Approach\n1. **Understand the Sorting Criteria**: The nodes must be sorted primarily by their priority in ascending order. If two nodes have the same priority, they should be sorted by their entry ID in ascending order.\n2. **Use Python's Built-in Sorting**: Python's `sorted` function allows us to specify a custom sorting key. We can use a tuple of (priority, entry_id) as the key to sort the nodes according to the given criteria.\n3. **Extract Keys**: After sorting the nodes, we extract the 'key' values from each node in the sorted order and return them as a list.\n\n### Solution Code\n```python\ndef sort_priority_nodes(nodes: list[dict]) -> list[str]:\n    \"\"\"Sort the given priority-queue nodes and return their keys.\n\n    Two nodes are ordered as follows:\n      1. The one with the smaller ``priority`` value comes first.\n      2. If priorities are equal, the node with the smaller ``entry_id``\n         comes first.\n\n    Args:\n        nodes: A list of dictionaries.  Each dictionary must contain the\n                fields ``'key'``, ``'val'``, ``'priority'`` and\n                ``'entry_id'``.\n\n    Returns:\n        A list of ``key`` values ordered from highest to lowest priority\n        according to the rule above.\n    \"\"\"\n    # Sort the nodes based on priority and entry_id\n    sorted_nodes = sorted(nodes, key=lambda x: (x['priority'], x['entry_id']))\n    # Extract the 'key' from each node in the sorted order\n    return [node['key'] for node in sorted_nodes]\n```\n\n### Explanation\n1. **Sorting the Nodes**: The `sorted` function is used with a lambda function as the key. This lambda function returns a tuple (priority, entry_id) for each node, ensuring that the nodes are sorted first by priority and then by entry ID in ascending order.\n2. **Extracting Keys**: After sorting, a list comprehension is used to create a list of 'key' values from the sorted nodes. This list is returned as the result.\n\nThis approach efficiently sorts the nodes in O(n log n) time complexity due to the use of Python's highly optimized sorting algorithm, and it ensures that the nodes are ordered correctly according to the specified criteria."}
{"task_id": 380, "completion_id": 0, "completion": "To solve this problem, we need to implement the Scaled Exponential Linear Unit (SELU) activation function along with its first and second derivatives. The SELU function is a scaled version of the Exponential Linear Unit (ELU) function, which helps in self-normalizing neural networks. The implementation must handle scalar values, 1-D arrays, and multi-dimensional arrays, preserving the input shape and returning results rounded to four decimal places.\n\n### Approach\n1. **Understanding SELU Function**: The SELU function is defined as `scale * ELU(x, alpha)`, where ELU is `x` if `x > 0` and `alpha * (exp(x) - 1)` otherwise. The constants `alpha` and `scale` are provided.\n2. **Derivatives**:\n   - **First Derivative**: For `x > 0`, the derivative is `scale`. For `x <= 0`, it is `scale * alpha * exp(x)`.\n   - **Second Derivative**: For `x > 0`, the second derivative is `0`. For `x <= 0`, it remains `scale * alpha * exp(x)`.\n3. **Handling Input Types**: Convert the input to a NumPy array to handle scalars, lists, and multi-dimensional arrays uniformly. Use NumPy's element-wise operations to compute results efficiently.\n4. **Rounding and Conversion**: After computing the results, round them to four decimal places and convert to Python lists or scalars using NumPy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n\n    x_np = np.asarray(x)\n\n    if order == 0:\n        elu = np.where(x_np > 0, x_np, alpha * (np.exp(x_np) - 1))\n        result = scale * elu\n    elif order == 1:\n        derivative = np.where(x_np > 0, 1, alpha * np.exp(x_np))\n        result = scale * derivative\n    elif order == 2:\n        result = np.where(x_np > 0, 0, scale * alpha * np.exp(x_np))\n    else:\n        raise ValueError(\"Order must be 0, 1, or 2.\")\n\n    rounded_result = np.round(result, 4)\n    return rounded_result.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `x` is converted to a NumPy array to handle both scalar and array inputs uniformly.\n2. **Element-wise Operations**: Using `np.where`, we apply the SELU function and its derivatives element-wise across the input array. This ensures efficient computation for any input shape.\n3. **Rounding and Conversion**: The results are rounded to four decimal places using `np.round` and converted to Python lists or scalars with `tolist()`, ensuring the output matches the required format.\n\nThis approach efficiently handles all input types and dimensions, computes the necessary values using vectorized operations for performance, and ensures the results are correctly formatted and rounded."}
{"task_id": 382, "completion_id": 0, "completion": "To solve this problem, we need to implement the Gaussian Naive Bayes algorithm to predict the labels of test data based on the training data. The algorithm involves learning the parameters (mean and variance) of each feature for each class from the training data and then using these parameters to compute the posterior probabilities for each test sample to determine the most likely class.\n\n### Approach\n1. **Training Phase**:\n   - **Extract Classes**: Identify the unique classes from the training labels.\n   - **Compute Priors**: Calculate the prior probability for each class as the ratio of the number of samples in the class to the total number of samples.\n   - **Compute Mean and Variance**: For each class and each feature, compute the mean and variance. The variance is adjusted by adding a small constant (epsilon) to avoid division by zero.\n\n2. **Prediction Phase**:\n   - **Log Posterior Calculation**: For each test sample, compute the log posterior probability for each class. This is done using the logarithms to avoid numerical underflow from multiplying small probabilities.\n   - **Class Prediction**: The class with the highest log posterior probability is selected as the predicted class. If multiple classes have the same highest probability, the first one in the sorted order of classes is chosen.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef gaussian_naive_bayes_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray) -> list:\n    \"\"\"Train a Gaussian Naive Bayes model on *X_train*, *y_train* and predict\n    the labels of *X_test*.\n    \"\"\"\n    # Extract unique classes from the training labels\n    classes = np.unique(y_train)\n    \n    # Training phase: compute priors, means, and variances for each class\n    priors = []\n    means = []\n    variances = []\n    for c in classes:\n        # Select samples belonging to the current class\n        X_c = X_train[y_train == c]\n        # Compute prior probability for the class\n        prior = X_c.shape[0] / X_train.shape[0]\n        priors.append(prior)\n        # Compute mean and variance for each feature in the class\n        mu = np.mean(X_c, axis=0)\n        var = np.var(X_c, axis=0, ddof=0)\n        means.append(mu)\n        variances.append(var)\n    \n    # Prediction phase: compute log posteriors for each test sample\n    predictions = []\n    for x in X_test:\n        log_posteriors = []\n        for i in range(len(classes)):\n            prior = priors[i]\n            mu = means[i]\n            var = variances[i]\n            # Compute variance with epsilon to avoid division by zero\n            var_plus_eps = var + 1e-4\n            # Calculate the numerator and exponent for Gaussian likelihood\n            numerator = (x - mu) ** 2\n            exponent = - numerator / (2 * var_plus_eps)\n            # Calculate the log denominator\n            log_denominator = 0.5 * np.log(2 * np.pi * var_plus_eps)\n            # Sum the log likelihoods across all features\n            log_likelihood = np.sum(exponent - log_denominator)\n            # Compute log posterior (log prior + log likelihood sum)\n            log_posterior = math.log(prior) + log_likelihood\n            log_posteriors.append(log_posterior)\n        # Determine the class with the highest log posterior\n        predicted_class_idx = np.argmax(log_posteriors)\n        predicted_class = classes[predicted_class_idx]\n        predictions.append(predicted_class)\n    \n    return predictions\n```\n\n### Explanation\n- **Training Phase**: The algorithm first identifies all unique classes in the training data. For each class, it calculates the mean and variance of each feature, which are essential for the Gaussian likelihood calculation. The prior probability of each class is also computed based on the relative frequency of the class in the training data.\n- **Prediction Phase**: For each test sample, the algorithm computes the log posterior probability for each class. This involves using the Gaussian likelihood formula adjusted with a small epsilon to prevent division by zero. The log posterior is calculated by summing the log prior and the log likelihoods of all features. The class with the highest log posterior is chosen as the predicted class, ensuring numerical stability by using logarithms to avoid underflow issues. If multiple classes have the same highest probability, the first class in the sorted order is selected."}
{"task_id": 383, "completion_id": 0, "completion": "To solve this problem, we need to compute the feasible interval [L, H] for the new value of the Lagrange multiplier \u03b1\u2c7c in the Sequential Minimal Optimization (SMO) algorithm for training Support Vector Machines (SVMs). The interval is determined based on whether the labels of two training instances are the same or different.\n\n### Approach\n1. **Problem Analysis**:\n   - The feasible interval [L, H] for \u03b1\u2c7c depends on the labels of the two instances (y\u1d62 and y\u2c7c).\n   - If y\u1d62 and y\u2c7c are different, the interval is computed using different formulas compared to when they are the same.\n   - The formulas for L and H are derived based on constraints from the SVM optimization problem.\n\n2. **Key Insights**:\n   - **Different Labels (y\u1d62 \u2260 y\u2c7c)**:\n     - L is the maximum of 0 and (\u03b1\u2c7c - \u03b1\u1d62).\n     - H is the minimum of the regularization constant C and (C - \u03b1\u1d62 + \u03b1\u2c7c).\n   - **Same Labels (y\u1d62 = y\u2c7c)**:\n     - L is the maximum of 0 and (\u03b1\u1d62 + \u03b1\u2c7c - C).\n     - H is the minimum of C and (\u03b1\u1d62 + \u03b1\u2c7c).\n\n3. **Algorithm**:\n   - Check if the labels y\u1d62 and y\u2c7c are the same.\n   - Compute L and H based on the appropriate formulas.\n   - Round the results to four decimal places for precision.\n\n### Solution Code\n```python\ndef find_bounds(y_i: int, y_j: int, alpha_i: float, alpha_j: float, C: float) -> list[float]:\n    \"\"\"Return the feasible interval [L, H] for alpha_j in SMO.\n\n    Parameters\n    ----------\n    y_i : int\n        Label of the i-th example (\u22121 or 1).\n    y_j : int\n        Label of the j-th example (\u22121 or 1).\n    alpha_i : float\n        Current Lagrange multiplier for the i-th example.\n    alpha_j : float\n        Current Lagrange multiplier for the j-th example.\n    C : float\n        Regularisation constant of the SVM.\n\n    Returns\n    -------\n    list[float]\n        A two-element list [L, H] with both values rounded to 4 decimals.\n    \"\"\"\n    if y_i != y_j:\n        L = max(0.0, alpha_j - alpha_i)\n        H = min(C, C - alpha_i + alpha_j)\n    else:\n        L = max(0.0, alpha_i + alpha_j - C)\n        H = min(C, alpha_i + alpha_j)\n    return [round(L, 4), round(H, 4)]\n```\n\n### Explanation\n- **Different Labels (y\u1d62 \u2260 y\u2c7c)**:\n  - **L Calculation**: The lower bound is determined by the difference between \u03b1\u2c7c and \u03b1\u1d62 but cannot be negative.\n  - **H Calculation**: The upper bound considers the regularization constant C and adjusts for the current values of \u03b1\u1d62 and \u03b1\u2c7c.\n\n- **Same Labels (y\u1d62 = y\u2c7c)**:\n  - **L Calculation**: The lower bound ensures that the sum of \u03b1\u1d62 and \u03b1\u2c7c does not exceed the regularization constant C.\n  - **H Calculation**: The upper bound is the minimum of the sum of \u03b1\u1d62 and \u03b1\u2c7c or the regularization constant C.\n\nBy following these steps, the function efficiently computes the feasible interval for \u03b1\u2c7c, ensuring the values are within the constraints required by the SMO algorithm. The results are rounded to four decimal places for precision, as specified."}
{"task_id": 384, "completion_id": 0, "completion": "To solve this problem, we need to find the k smallest eigenvectors of a given Laplacian matrix. These eigenvectors correspond to the smallest eigenvalues and are essential in various spectral algorithms. The solution must handle normalization, sign adjustment, and formatting requirements.\n\n### Approach\n1. **Input Validation**: Check if the number of requested eigenvectors (k) is greater than the matrix dimension. If so, return -1.\n2. **Eigenvalue Decomposition**: Use the numpy function `eigh` to compute eigenvalues and eigenvectors of the Laplacian matrix, which is symmetric. This function returns eigenvalues in ascending order and corresponding eigenvectors.\n3. **Eigenvector Selection**: Extract the first k eigenvectors from the result, as they correspond to the smallest eigenvalues.\n4. **Normalization**: Ensure each eigenvector is L2-normalized to unit length, even though `eigh` typically returns normalized vectors.\n5. **Sign Adjustment**: Adjust the sign of each eigenvector so that the first non-zero element is positive to ensure deterministic signs.\n6. **Rounding and Formatting**: Round each element of the resulting matrix to 4 decimal places and convert it to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_eigvecs(L: list[list[int | float]], k: int):\n    \"\"\"Calculate the k eigenvectors that correspond to the smallest k eigenvalues.\"\"\"\n    # Convert the input matrix to a numpy array\n    L_np = np.array(L, dtype=np.float64)\n    \n    # Check if k is valid\n    n = L_np.shape[0]\n    if k > n:\n        return -1\n    \n    # Compute eigenvalues and eigenvectors using eigh for symmetric matrices\n    eigenvalues, eigenvectors = np.linalg.eigh(L_np)\n    \n    # Select the first k eigenvectors (columns) corresponding to smallest eigenvalues\n    selected_vecs = eigenvectors[:, :k].copy()\n    \n    # Normalize each eigenvector to unit length (even though eigh returns normalized vectors)\n    for i in range(k):\n        vec = selected_vecs[:, i]\n        norm = np.linalg.norm(vec)\n        if norm != 0:\n            selected_vecs[:, i] = vec / norm\n    \n    # Adjust the sign based on the first non-zero element\n    epsilon = 1e-9\n    for i in range(k):\n        vec = selected_vecs[:, i]\n        for element in vec:\n            if abs(element) > epsilon:\n                if element < 0:\n                    selected_vecs[:, i] *= -1\n                break\n    \n    # Round to 4 decimal places and convert to a list\n    rounded = np.round(selected_vecs, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The matrix is converted to a numpy array, and we check if k exceeds the matrix dimension. This ensures valid input handling.\n2. **Eigenvalue Decomposition**: Using `np.linalg.eigh` leverages the matrix's symmetry, ensuring efficient and accurate computation of eigenvalues and eigenvectors.\n3. **Eigenvector Selection**: The first k eigenvectors are selected as they correspond to the smallest eigenvalues due to the ascending order result from `eigh`.\n4. **Normalization**: Each eigenvector is explicitly normalized to unit length to meet the problem's requirements, even though `eigh` returns normalized vectors.\n5. **Sign Adjustment**: Each eigenvector's sign is adjusted based on the first non-zero element to ensure consistency. This avoids sign ambiguity in eigenvectors.\n6. **Rounding and Formatting**: The result is rounded to 4 decimal places and converted to a list, ensuring the output format matches the problem's requirements.\n\nThis approach efficiently handles the problem's constraints and requirements, ensuring accurate and deterministic results."}
{"task_id": 385, "completion_id": 0, "completion": "To solve this problem, we need to implement Linear Discriminant Analysis (LDA) for dimensionality reduction. The goal is to project a labelled dataset onto its first k Linear Discriminant components, which maximize the ratio of between-class scatter to within-class scatter. Here's the structured approach to achieve this:\n\n### Approach\n1. **Convert Input Data**: Convert the input data and labels into NumPy arrays for efficient computation.\n2. **Determine Class Information**: Identify the unique classes and adjust k to be at most (c-1), where c is the number of classes.\n3. **Compute Scatter Matrices**:\n   - **Within-Class Scatter (S_w)**: Sum of the covariance matrices of each class.\n   - **Between-Class Scatter (S_b)**: Sum of the outer products of the difference between class means and the overall mean, weighted by class size.\n4. **Solve Eigenvalue Problem**: Use the Moore-Penrose pseudo-inverse of S_w to solve the generalized eigenvalue problem for S_w^{-1} * S_b.\n5. **Sort and Select Eigenvectors**: Sort eigenvectors by descending eigenvalues, select the top k, normalize them, and adjust their orientation.\n6. **Project Data**: Transform the data using the selected eigenvectors and round the results to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef lda_transform(X: list[list[int | float]], y: list[int], k: int) -> list[list[float]]:\n    X = np.array(X, dtype=np.float64)\n    y = np.array(y)\n    n_samples, n_features = X.shape\n    \n    classes = np.unique(y)\n    c = len(classes)\n    k = min(k, c - 1)  # Silently adjust k to c-1 if necessary\n    \n    if k <= 0:\n        return [[] for _ in range(n_samples)]\n    \n    # Compute overall mean\n    m = np.mean(X, axis=0)\n    \n    # Compute class means and counts\n    class_means = []\n    class_counts = []\n    for cls in classes:\n        X_cls = X[y == cls]\n        class_means.append(np.mean(X_cls, axis=0))\n        class_counts.append(X_cls.shape[0])\n    class_means = np.array(class_means)\n    class_counts = np.array(class_counts)\n    \n    # Compute within-class scatter matrix S_w\n    S_w = np.zeros((n_features, n_features))\n    for i, cls in enumerate(classes):\n        X_cls = X[y == cls]\n        X_cls_centered = X_cls - class_means[i]\n        S_w += X_cls_centered.T @ X_cls_centered\n    \n    # Compute between-class scatter matrix S_b\n    S_b = np.zeros((n_features, n_features))\n    for i in range(c):\n        diff = class_means[i] - m\n        S_b += class_counts[i] * np.outer(diff, diff)\n    \n    # Solve generalized eigenvalue problem using pseudo-inverse of S_w\n    Sw_inv = np.linalg.pinv(S_w)\n    M = Sw_inv @ S_b\n    \n    eigenvalues, eigenvectors = np.linalg.eig(M)\n    eigenvalues = np.real(eigenvalues)\n    eigenvectors = np.real(eigenvectors)\n    \n    # Sort eigenvectors by descending eigenvalues\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Select top k eigenvectors\n    W = eigenvectors[:, :k]\n    \n    # Normalize each eigenvector to unit length and adjust sign of the first element\n    for i in range(W.shape[1]):\n        vec = W[:, i].copy()\n        norm = np.linalg.norm(vec)\n        if norm == 0:\n            continue  # Avoid division by zero (unlikely case)\n        vec /= norm\n        if vec[0] < 0:\n            vec *= -1\n        W[:, i] = vec\n    \n    # Project data and round to 4 decimal places\n    projected = X @ W\n    projected_rounded = np.round(projected, 4)\n    \n    return projected_rounded.tolist()\n```\n\n### Explanation\n1. **Data Conversion**: The input data is converted to NumPy arrays for efficient numerical operations.\n2. **Class Adjustment**: The number of unique classes (c) is determined, and k is adjusted to be at most (c-1) since LDA can only provide that many meaningful directions.\n3. **Mean Calculation**: The overall mean and class-specific means are computed to form the basis for scatter matrices.\n4. **Scatter Matrices**: Within-class scatter (S_w) and between-class scatter (S_b) matrices are computed to capture intra-class and inter-class variances.\n5. **Eigenvalue Problem**: The generalized eigenvalue problem is solved using the pseudo-inverse of S_w to handle potential singularity, ensuring numerical stability.\n6. **Eigenvector Handling**: Eigenvectors are sorted, normalized, and oriented to ensure deterministic results. The data is projected onto these eigenvectors, and the results are rounded and converted to a list for output.\n\nThis approach ensures that the data is transformed into a lower-dimensional space that maximizes class separability, adhering to the principles of LDA."}
{"task_id": 386, "completion_id": 0, "completion": "To solve this problem, we need to implement the Binary Cross-Entropy (BCE) loss function, which is commonly used in training binary classifiers. The BCE loss measures the difference between the predicted probabilities and the actual labels, and it is crucial to handle numerical stability by clipping predictions to avoid taking the logarithm of zero.\n\n### Approach\n1. **Clipping Predictions**: To avoid numerical instability when dealing with logarithms, we clip the predicted probabilities to a small interval around [0, 1]. Specifically, we use a small value (epsilon = 1e-15) to clip the predictions to the interval [epsilon, 1 - epsilon].\n\n2. **Compute BCE Loss**: Using the clipped predictions, we compute the BCE loss using the formula:\n   \\[\n   \\text{BCE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n   \\]\n   where \\( y_i \\) are the true labels and \\( \\hat{y}_i \\) are the clipped predicted probabilities.\n\n3. **Average and Round**: The computed BCE loss is averaged over all samples and then rounded to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef binary_crossentropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the Binary Cross-Entropy loss.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        1-D array of true labels (0 or 1).\n    y_pred : np.ndarray\n        1-D array of predicted probabilities (between 0 and 1).\n\n    Returns\n    -------\n    float\n        The average BCE loss rounded to 4 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    y_pred_clipped = np.clip(y_pred, epsilon, 1.0 - epsilon)\n    terms = y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped)\n    bce = -np.mean(terms)\n    return round(bce, 4)\n```\n\n### Explanation\n- **Clipping**: The `np.clip` function ensures that all predicted probabilities are within the interval [epsilon, 1 - epsilon], preventing log(0) which would lead to numerical errors.\n- **Vectorized Operations**: Using NumPy's vectorized operations, we efficiently compute the terms for each sample, which is both concise and computationally efficient.\n- **Mean and Rounding**: The average of the computed terms is taken and multiplied by -1 to get the BCE loss. The result is then rounded to four decimal places to meet the specified precision.\n\nThis approach ensures numerical stability and efficiently computes the BCE loss using vectorized operations, making it suitable for large datasets typically encountered in machine learning tasks."}
{"task_id": 387, "completion_id": 0, "completion": "To solve this problem, we need to implement a gradient boosting regressor that works on one-dimensional data using decision stumps. The regressor will iteratively improve its predictions by focusing on the residuals (errors) of the previous predictions, building a sequence of simple models (stumps) to correct these errors.\n\n### Approach\n1. **Initial Prediction**: Start with the mean of the target values as the initial prediction for all samples.\n2. **Iterative Boosting**: For each boosting round:\n   - Compute the residuals (differences between actual and predicted values).\n   - Sort the data points based on the feature values.\n   - Generate candidate split points between consecutive different feature values.\n   - For each candidate split, calculate the mean residuals on both sides of the split and compute the sum of squared errors.\n   - Select the split that minimizes the sum of squared errors.\n   - Update the predictions using the selected split, scaled by a learning rate.\n3. **Edge Cases**: Handle cases where all feature values are identical by using the global mean of residuals for updates.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    x = np.array(x)\n    y = np.array(y)\n    y_hat = np.full_like(y, np.mean(y), dtype=np.float64)\n    \n    for _ in range(n_estimators):\n        residuals = y - y_hat\n        sorted_indices = np.argsort(x)\n        x_sorted = x[sorted_indices]\n        r_sorted = residuals[sorted_indices]\n        \n        split_candidates = []\n        for i in range(len(x_sorted) - 1):\n            if x_sorted[i] != x_sorted[i+1]:\n                split = (x_sorted[i] + x_sorted[i+1]) / 2\n                split_candidates.append(split)\n        \n        if not split_candidates:\n            global_mean = np.mean(residuals)\n            y_hat += learning_rate * global_mean\n            continue\n        \n        best_t = None\n        best_loss = float('inf')\n        best_left_mean = 0.0\n        best_right_mean = 0.0\n        \n        for t in split_candidates:\n            left_mask = x_sorted <= t\n            right_mask = ~left_mask\n            \n            left_mean = np.mean(r_sorted[left_mask])\n            right_mean = np.mean(r_sorted[right_mask])\n            \n            pred = np.where(left_mask, left_mean, right_mean)\n            loss = np.sum((r_sorted - pred) ** 2)\n            \n            if loss < best_loss:\n                best_loss = loss\n                best_t = t\n                best_left_mean = left_mean\n                best_right_mean = right_mean\n        \n        mask = x <= best_t\n        r_hat = np.where(mask, best_left_mean, best_right_mean)\n        y_hat += learning_rate * r_hat\n    \n    return np.round(y_hat, 4).tolist()\n```\n\n### Explanation\n- **Initial Prediction**: The initial prediction for all samples is the mean of the target values.\n- **Residual Calculation**: For each iteration, residuals are computed as the difference between the actual values and the current predictions.\n- **Sorting and Splitting**: The data points are sorted by the feature values. Potential split points are identified between consecutive different feature values.\n- **Split Evaluation**: For each candidate split, the mean residuals on both sides of the split are computed. The split that minimizes the sum of squared errors is selected.\n- **Prediction Update**: The predictions are updated by adding the contribution of the current stump's predictions, scaled by the learning rate.\n- **Edge Handling**: If all feature values are identical, the global mean of residuals is used to update predictions, ensuring the model can still adjust even with no valid splits.\n\nThis approach efficiently builds a sequence of simple models to correct errors iteratively, leading to a robust regressor for one-dimensional data."}
{"task_id": 389, "completion_id": 0, "completion": "To solve this problem, we need to compute the (unnormalized) Laplacian matrix of an undirected graph given its weighted adjacency matrix. The Laplacian matrix is a fundamental tool in spectral graph theory and has various applications in clustering, dimensionality reduction, and more.\n\n### Approach\n1. **Input Validation**: Check if the input matrix is square. If not, return -1.\n2. **Conversion to NumPy Array**: Convert the input to a NumPy array for easier manipulation.\n3. **Degree Matrix Calculation**: Compute the degree matrix, which is a diagonal matrix where each diagonal entry is the sum of the corresponding row in the adjacency matrix.\n4. **Laplacian Matrix Calculation**: Subtract the adjacency matrix from the degree matrix to get the Laplacian matrix.\n5. **Rounding and Conversion**: Round the values of the Laplacian matrix to 4 decimal places and convert it to a nested Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_laplacian(W):\n    \"\"\"Compute the (unnormalised) Laplacian matrix of an undirected graph.\n\n    Parameters\n    ----------\n    W : list[list[int | float]] | np.ndarray\n        Square weighted adjacency matrix of the graph.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The Laplacian matrix rounded to 4 decimal places and converted to a\n        Python list of lists. Returns -1 if the input is not a square matrix.\n    \"\"\"\n    # Convert input to a numpy array if it isn't already\n    W_np = np.array(W)\n    \n    # Check if the matrix is 2-dimensional and square\n    if W_np.ndim != 2 or W_np.shape[0] != W_np.shape[1]:\n        return -1\n    \n    # Compute the degree matrix D\n    row_sums = W_np.sum(axis=1)\n    D = np.diag(row_sums)\n    \n    # Compute the Laplacian matrix\n    L = D - W_np\n    \n    # Round the result to 4 decimal places and convert to list of lists\n    L_rounded = np.around(L, decimals=4)\n    \n    return L_rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The input matrix is converted to a NumPy array. We then check if it is a 2-dimensional square matrix. If not, we return -1.\n2. **Degree Matrix Calculation**: The degree matrix is constructed by summing the elements of each row of the adjacency matrix and placing these sums on the diagonal of a new matrix.\n3. **Laplacian Matrix Calculation**: The Laplacian matrix is obtained by subtracting the adjacency matrix from the degree matrix.\n4. **Rounding and Conversion**: The resulting Laplacian matrix is rounded to 4 decimal places for precision and converted to a nested list using NumPy's `tolist()` method to ensure compatibility with Python's native data structures.\n\nThis approach efficiently handles both list and NumPy array inputs, ensures the input is valid, and performs the necessary computations to derive the Laplacian matrix accurately."}
{"task_id": 391, "completion_id": 0, "completion": "To solve this problem, we need to perform nearest-neighbor interpolation on a one-dimensional, possibly multi-channel signal. This involves rounding query positions to the nearest valid indices, clipping these indices to ensure they lie within the valid range, and then retrieving the corresponding samples from the input array.\n\n### Approach\n1. **Convert Query Positions to Indices**: Convert the list of query positions `t` into a NumPy array and round each value to the nearest integer using NumPy's `around` function. This function handles ties by rounding to the nearest even integer.\n2. **Clip Indices to Valid Range**: Ensure the rounded indices are within the valid range of the input array `X`, which is from 0 to `in_length - 1` (where `in_length` is the number of rows in `X`).\n3. **Retrieve Samples**: Use the valid indices to extract the corresponding rows from the input array `X` and convert the result into a list of lists as required.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef nn_interpolate_1D(X: np.ndarray, t: list[float]) -> list[list[float]]:\n    t_np = np.array(t)\n    indices = np.around(t_np).astype(int)\n    in_length = X.shape[0]\n    clipped_indices = np.clip(indices, 0, in_length - 1)\n    samples = X[clipped_indices]\n    return samples.tolist()\n```\n\n### Explanation\n1. **Convert to NumPy Array**: The query positions `t` are converted into a NumPy array to leverage vectorized operations for efficient computation.\n2. **Round Indices**: Using `np.around`, we round each query position to the nearest integer. This handles ties by rounding to the nearest even integer, which is the standard behavior of NumPy's rounding function.\n3. **Clip Indices**: The clipped indices ensure that all values are within the valid range of the input array `X`. This prevents out-of-bounds errors by restricting indices between 0 and `in_length - 1`.\n4. **Retrieve and Convert Samples**: The valid indices are used to extract the corresponding rows from `X`, and the resulting NumPy array is converted to a list of lists using `tolist()`, which matches the required output format.\n\nThis approach efficiently processes the input query positions, ensures valid index access, and returns the results in the specified format, providing an optimal solution to the problem."}
{"task_id": 392, "completion_id": 0, "completion": "To solve this problem, we need to apply a pre-emphasis filter to a 1-D signal. This filter enhances high-frequency components and attenuates low-frequency ones, which is useful in various signal processing applications such as speech recognition. The filter is defined by the equation \\(\\hat x_t = x_t - \\alpha x_{t-1}\\), where the first sample remains unchanged.\n\n### Approach\n1. **Flatten the Input**: Convert the input signal to a NumPy array and flatten it to ensure it is 1-D, regardless of the original dimensions.\n2. **Shift the Array**: Create a shifted version of the input array where each element is moved one position to the right, with the first element set to zero. This helps in applying the filter formula efficiently.\n3. **Apply the Filter**: Compute the filtered signal by subtracting the product of the pre-emphasis coefficient (\\(\\alpha\\)) and the shifted array from the original array.\n4. **Round and Convert**: Round each element of the filtered signal to four decimal places and convert the result to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef preemphasis(x: \"list[float] | np.ndarray\", alpha: float) -> list[float]:\n    \"\"\"Apply a first-order pre-emphasis filter to a 1-D signal.\n\n    Args:\n        x: Input signal.  A one-dimensional array-like object containing the\n           samples to be filtered.\n        alpha: Pre-emphasis coefficient in the half-open interval ``[0, 1)``.\n\n    Returns:\n        The filtered signal as a Python ``list`` with every element rounded to\n        four decimal places.\n    \"\"\"\n    arr = np.array(x).flatten()\n    shifted = np.zeros_like(arr)\n    shifted[1:] = arr[:-1]\n    filtered = arr - alpha * shifted\n    filtered_rounded = np.round(filtered, 4)\n    return filtered_rounded.tolist()\n```\n\n### Explanation\n1. **Flatten the Input**: The input is converted to a NumPy array and flattened to handle any multi-dimensional input, ensuring the signal is processed as a 1-D sequence.\n2. **Shifted Array Creation**: A new array `shifted` is created with the same shape as the input array. This array is filled with zeros except for elements from index 1 onwards, which are set to the elements of the input array from the start up to the second last element. This effectively shifts the input array to the right by one position.\n3. **Filter Application**: The filtered signal is computed by subtracting the product of \\(\\alpha\\) and the shifted array from the original array. This operation leverages vectorized operations in NumPy for efficiency.\n4. **Rounding and Conversion**: The filtered signal is rounded to four decimal places using NumPy's `round` function and then converted to a Python list to meet the output requirements.\n\nThis approach ensures that the pre-emphasis filter is applied efficiently and correctly, handling any input dimensionality and producing the desired output format."}
{"task_id": 394, "completion_id": 0, "completion": "To solve this problem, we need to implement the RMSprop optimization algorithm, which is an adaptive learning rate method used for training neural networks. The algorithm adjusts the learning rate of each parameter based on the magnitude of its gradients, leading to more efficient and stable convergence.\n\n### Approach\nThe RMSprop algorithm maintains an exponentially decaying average of the squared gradients to scale the learning rate for each parameter. The key steps involved in the implementation are:\n\n1. **Initialization of Running Average**: If the running average of squared gradients (Eg) is not provided (None), it is initialized to a zero array of the same shape as the gradient array.\n\n2. **Update Running Average**: The running average is updated using a decay rate (rho) to combine the previous average with the current squared gradient. This helps in smoothing out the variability in gradient values over time.\n\n3. **Compute Parameter Update**: The parameters are updated by scaling the learning rate with the square root of the running average (plus a small constant for numerical stability). This scaling ensures that parameters with larger gradients have smaller effective learning rates and vice versa.\n\n4. **Rounding and Conversion**: Both the updated parameters and the new running average are rounded to four decimal places and converted to Python lists before returning them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n    # Update the running average of squared gradients\n    Eg_next = rho * Eg + (1 - rho) * (grad ** 2)\n    # Compute the denominator with epsilon for numerical stability\n    denominator = np.sqrt(Eg_next + 1e-8)\n    # Update parameters\n    w_next = w - (learning_rate * grad) / denominator\n    # Round to four decimal places and convert to Python lists\n    return (np.round(w_next, 4).tolist(), np.round(Eg_next, 4).tolist())\n```\n\n### Explanation\n1. **Initialization**: If the running average `Eg` is not provided, it is initialized to a zero array matching the shape of the gradient array. This is done using `np.zeros_like(grad)`.\n\n2. **Running Average Update**: The new running average `Eg_next` is computed using the decay rate `rho` to blend the previous average with the current squared gradient. This is achieved with the formula `Eg_next = rho * Eg + (1 - rho) * (grad ** 2)`.\n\n3. **Parameter Update**: The parameters `w` are updated by subtracting the product of the learning rate and the gradient, divided by the square root of the running average (with a small epsilon added for numerical stability). This step ensures that each parameter's update is scaled appropriately based on its gradient history.\n\n4. **Rounding and Conversion**: Both the updated parameters and the new running average are rounded to four decimal places using `np.round` and converted to Python lists with `tolist()` to meet the output requirements.\n\nThis approach efficiently implements the RMSprop optimization algorithm, ensuring numerical stability and proper scaling of learning rates for each parameter based on historical gradient information."}
{"task_id": 395, "completion_id": 0, "completion": "To solve this problem, we need to format the result of the Backward Algorithm from a Hidden Markov Model (HMM) into a specific string layout. This involves presenting a \u03b2-matrix and the probability of an observation sequence in a structured manner.\n\n### Approach\nThe task requires creating a formatted string that includes:\n1. A header line with exactly 50 asterisks.\n2. The \u03b2-matrix as a string representation.\n3. The probability of the observation sequence as a string.\n\nThe steps to achieve this are:\n1. Generate the header line with 50 asterisks.\n2. Convert the \u03b2-matrix (a NumPy array) to its string representation using Python's built-in `str()` function.\n3. Convert the probability (a float) to its string representation similarly using `str()`.\n4. Combine these components into a single string with the specified format, ensuring each part is on a new line.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef print_backward_result(beta: np.ndarray, b_prob: float) -> str:\n    \"\"\"Return a formatted string with a \u03b2-matrix and its probability.\n\n    The string must look like:\n        **************************************************\n        Beta:\n        <beta as str(beta)>\n        Probability of sequence: <b_prob>\n\n    Args:\n        beta: A 2-D NumPy array of \u03b2-values.\n        b_prob: Probability of the observation sequence.\n\n    Returns:\n        The formatted multi-line string (do not print inside this function).\n    \"\"\"\n    header = '*' * 50\n    beta_str = str(beta)\n    prob_str = str(b_prob)\n    return f\"{header}\\nBeta:\\n{beta_str}\\nProbability of sequence: {prob_str}\"\n```\n\n### Explanation\n1. **Header Line**: The header line is created by repeating the asterisk character 50 times.\n2. **Beta Matrix Conversion**: The \u03b2-matrix is converted to a string using `str(beta)`, which handles any formatting internally, including multi-line representations for 2-D arrays.\n3. **Probability Conversion**: The probability value is converted to a string using `str(b_prob)` to ensure it uses Python's default string representation.\n4. **String Construction**: The components are combined into a single string using formatted string literals (f-strings) in Python, ensuring each part is placed on a new line as specified.\n\nThis approach efficiently constructs the required string by leveraging Python's built-in string conversion functions and formatted string literals, ensuring the output adheres strictly to the specified format."}
{"task_id": 396, "completion_id": 0, "completion": "To solve this problem, we need to compute the hyperbolic tangent (tanh) activation function along with its first and second derivatives for a given 1-D list or NumPy array of real numbers. The results must be rounded to six decimal places and returned as a list of three lists.\n\n### Approach\n1. **Convert Input to NumPy Array**: Convert the input to a NumPy array if it's a list to ensure element-wise operations can be efficiently performed. This conversion does not modify the original input.\n2. **Compute tanh Values**: Using NumPy's `tanh` function, compute the hyperbolic tangent for each element in the input array.\n3. **Compute First Derivative**: The first derivative of tanh is given by \\(1 - \\text{tanh}^2(x)\\). This is computed using the previously obtained tanh values.\n4. **Compute Second Derivative**: The second derivative of tanh is given by \\(-2 \\cdot \\text{tanh}(x) \\cdot (1 - \\text{tanh}^2(x))\\), which also uses the tanh values from the first step.\n5. **Round and Convert to Lists**: Each of the computed results (tanh, first derivative, and second derivative) is rounded to six decimal places using NumPy's `round` function and then converted to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef tanh_with_derivatives(x):\n    \"\"\"Compute tanh and its first two derivatives element-wise.\"\"\"\n    x = np.asarray(x)\n    tanh_values = np.tanh(x)\n    first_derivative = 1 - tanh_values ** 2\n    second_derivative = -2 * tanh_values * first_derivative\n    \n    # Round to 6 decimal places and convert to lists\n    tanh_list = np.round(tanh_values, 6).tolist()\n    first_deriv_list = np.round(first_derivative, 6).tolist()\n    second_deriv_list = np.round(second_derivative, 6).tolist()\n    \n    return [tanh_list, first_deriv_list, second_deriv_list]\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input is converted to a NumPy array to leverage efficient vectorized operations. This ensures that both list and NumPy array inputs are handled uniformly without modifying the original input.\n2. **Element-wise Computation**: The tanh function and its derivatives are computed element-wise using vectorized operations, which are efficient and concise.\n3. **Rounding and Conversion**: Each result array is rounded to six decimal places using `np.round` and then converted to a Python list to meet the output requirements. This ensures that the results are returned in the required format of a list of lists.\n\nThis approach efficiently computes the required values using vectorized operations, ensuring both accuracy and performance. The solution adheres to the problem constraints and correctly handles both list and NumPy array inputs."}
{"task_id": 397, "completion_id": 0, "completion": "To solve this problem, we need to generate a random one-hot encoded matrix using NumPy. A one-hot matrix is defined such that each row contains exactly one element equal to 1, with all other elements being 0. The position of the 1 in each row should be chosen uniformly at random from the available class indices.\n\n### Approach\n1. **Generate Random Indices**: For each row, generate a random integer representing the column index where the 1 will be placed. This can be efficiently done using NumPy's `randint` function.\n2. **Initialize Zero Matrix**: Create a matrix of zeros with the desired shape `(n_examples, n_classes)`.\n3. **Set Indices to 1**: Using NumPy's advanced indexing, set the elements at the generated random indices to 1. This avoids explicit loops and leverages vectorized operations for efficiency.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_one_hot_matrix(n_examples: int, n_classes: int):\n    \"\"\"Create a random one-hot matrix.\n\n    Parameters\n    ----------\n    n_examples : int\n        Number of rows (examples) to generate.  May be 0.\n    n_classes : int\n        Size of the vocabulary / number of distinct classes (>0).\n\n    Returns\n    -------\n    numpy.ndarray\n        A matrix of shape (n_examples, n_classes) where every row is a one-hot\n        encoded vector chosen uniformly at random from all `n_classes` possible\n        class indices.\n    \"\"\"\n    if n_examples == 0:\n        return np.zeros((0, n_classes))\n    # Generate random indices for each example\n    indices = np.random.randint(n_classes, size=n_examples)\n    # Create a matrix of zeros with the appropriate shape\n    matrix = np.zeros((n_examples, n_classes))\n    # Use advanced indexing to set the corresponding elements to 1.0\n    matrix[np.arange(n_examples), indices] = 1.0\n    return matrix\n```\n\n### Explanation\n1. **Handling Zero Examples**: If `n_examples` is 0, the function immediately returns a zero matrix with shape `(0, n_classes)` to handle the edge case efficiently.\n2. **Random Indices Generation**: Using `np.random.randint`, we generate an array of random integers between 0 (inclusive) and `n_classes` (exclusive). This array has a length of `n_examples`, ensuring each row has a randomly chosen index.\n3. **Matrix Initialization**: A matrix of zeros is created with the shape `(n_examples, n_classes)`.\n4. **Advanced Indexing**: By using `np.arange(n_examples)` to generate row indices and the previously generated `indices` for column indices, we efficiently set the corresponding positions in the matrix to 1. This approach is both concise and leverages NumPy's optimized operations for performance.\n\nThis method ensures that the matrix is generated efficiently and correctly, adhering to the constraints and requirements of a one-hot encoded matrix."}
{"task_id": 398, "completion_id": 0, "completion": "To solve this problem, we need to implement the Exponential Linear Unit (ELU) activation function and its gradient. The ELU function is widely used in deep learning models due to its ability to help networks converge faster and mitigate the vanishing gradient problem. The task requires handling both the activation function and its derivative efficiently.\n\n### Approach\nThe ELU activation function is defined as:\n- ELU(x, \u03b1) = x for x \u2265 0\n- ELU(x, \u03b1) = \u03b1( e\u02e3 \u2212 1 ) for x < 0\n\nThe derivative of ELU is:\n- ELU\u2032(x, \u03b1) = 1 for x \u2265 0\n- ELU\u2032(x, \u03b1) = \u03b1e\u02e3 for x < 0\n\nThe approach involves:\n1. Converting the input list or array to a NumPy array for efficient element-wise operations.\n2. Using NumPy's `where` function to apply the ELU activation or its derivative based on the value of each element.\n3. Rounding the results to four decimal places and returning them as a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    x_arr = np.asarray(x)\n    if derivative:\n        result = np.where(x_arr >= 0, 1.0, alpha * np.exp(x_arr))\n    else:\n        result = np.where(x_arr >= 0, x_arr, alpha * (np.exp(x_arr) - 1))\n    return [round(val, 4) for val in result.tolist()]\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `x` is converted to a NumPy array to leverage efficient vectorized operations.\n2. **Activation Function Calculation**: Using `np.where`, the activation function is computed by applying `x` directly for non-negative values and `\u03b1( e\u02e3 \u2212 1 )` for negative values.\n3. **Derivative Calculation**: Similarly, the derivative is computed using `np.where` where the result is 1 for non-negative values and `\u03b1e\u02e3` for negative values.\n4. **Rounding and Conversion**: The results are rounded to four decimal places using Python's `round` function and converted back to a list to meet the problem's output requirements.\n\nThis approach ensures efficiency and clarity by utilizing NumPy's optimized operations for handling element-wise computations, making the solution both concise and effective."}
{"task_id": 399, "completion_id": 0, "completion": "To solve this problem, we need to compute the pairwise Euclidean distance matrix between two sets of vectors using NumPy for efficient calculations. The solution must check for valid input dimensions and handle edge cases appropriately.\n\n### Approach\n1. **Input Validation**: Check if both input matrices are two-dimensional and have the same number of columns. If not, return -1.\n2. **Sum of Squares Calculation**: Compute the sum of squares for each row in both matrices. This helps in expanding the Euclidean distance formula efficiently.\n3. **Dot Product Calculation**: Compute the dot product between the two matrices to leverage vectorized operations for efficiency.\n4. **Euclidean Distance Calculation**: Using the expanded form of the Euclidean distance formula, compute the squared distances matrix. Apply element-wise square root to get the actual distances.\n5. **Rounding and Conversion**: Round the computed distances to four decimal places and convert the result from a NumPy array to a Python list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pairwise_distances(X, Y):\n    \"\"\"Compute the Euclidean distance between every pair of points in X and Y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        A 2-D array of shape (m, d) where each row is a point.\n    Y : numpy.ndarray\n        A 2-D array of shape (n, d) where each row is a point.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The m\u00d7n matrix of distances rounded to four decimals as a list of\n        lists, or \u22121 if the inputs are invalid.\n    \"\"\"\n    if X.ndim != 2 or Y.ndim != 2 or X.shape[1] != Y.shape[1]:\n        return -1\n    \n    X_sq = np.sum(X**2, axis=1)\n    Y_sq = np.sum(Y**2, axis=1)\n    dot_product = np.dot(X, Y.T)\n    \n    D_squared = X_sq[:, np.newaxis] + Y_sq - 2 * dot_product\n    D_squared = np.maximum(D_squared, 0)  # Avoid negative values due to numerical errors\n    \n    D = np.sqrt(D_squared)\n    D_rounded = np.round(D, 4)\n    \n    return D_rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input matrices are two-dimensional and have the same number of columns. This ensures that the inputs are valid for the subsequent calculations.\n2. **Sum of Squares**: The sum of squares for each row in matrices X and Y is computed. This is used to expand the Euclidean distance formula efficiently.\n3. **Dot Product**: The dot product between matrices X and Y is computed, which helps in vectorizing the calculation of pairwise interactions between the rows of the two matrices.\n4. **Distance Calculation**: Using the expanded formula for squared Euclidean distances, the function computes the distances efficiently. A small adjustment is made to ensure no negative values due to floating-point errors.\n5. **Rounding and Conversion**: The computed distances are rounded to four decimal places and converted to a list of lists for the required output format.\n\nThis approach ensures that the solution is both efficient and robust, leveraging NumPy's vectorized operations for performance while handling edge cases and numerical stability appropriately."}
{"task_id": 400, "completion_id": 0, "completion": "To solve this problem, we need to compute the cumulative distribution function (CDF) of a univariate Gaussian (normal) random variable. The CDF gives the probability that a Gaussian random variable \\( X \\) with a given mean \\( \\mu \\) and variance \\( \\sigma^2 \\) is less than or equal to a given value \\( x \\). \n\n### Approach\nThe solution involves two main cases based on the variance:\n1. **Zero Variance**: When the variance is zero, the distribution is a point mass at the mean \\( \\mu \\). In this case, the CDF is determined by comparing \\( x \\) with \\( \\mu \\):\n   - If \\( x \\) is less than \\( \\mu \\), the CDF is 0.\n   - If \\( x \\) equals \\( \\mu \\), the CDF is 0.5.\n   - If \\( x \\) is greater than \\( \\mu \\), the CDF is 1.\n\n2. **Positive Variance**: When the variance is positive, the CDF is computed using the error function (erf). The formula used is:\n   \\[\n   \\Phi(x; \\mu, \\sigma^2) = 0.5 \\left[ 1 + \\text{erf}\\left( \\frac{x - \\mu}{\\sigma \\sqrt{2}} \\right) \\right]\n   \\]\n   where \\( \\sigma \\) is the standard deviation (square root of variance).\n\n### Solution Code\n```python\nimport math\n\ndef gaussian_cdf(x: float, mean: float, variance: float) -> float:\n    \"\"\"Compute \u03a6(x; \u03bc, \u03c3\u00b2) \u2013 the Gaussian CDF evaluated at *x*.\n\n    Parameters\n    ----------\n    x : float\n        The point at which to evaluate the CDF.\n    mean : float\n        The mean (\u03bc) of the Gaussian distribution.\n    variance : float\n        The variance (\u03c3\u00b2) of the Gaussian distribution. Must be non-negative.\n\n    Returns\n    -------\n    float\n        The probability that a Gaussian random variable with given parameters\n        does not exceed *x*, rounded to six decimal places.\n    \"\"\"\n    if variance == 0:\n        if x < mean:\n            prob = 0.0\n        elif x == mean:\n            prob = 0.5\n        else:\n            prob = 1.0\n    else:\n        sigma = math.sqrt(variance)\n        z = (x - mean) / (sigma * math.sqrt(2))\n        prob = 0.5 * (1 + math.erf(z))\n    return round(prob, 6)\n```\n\n### Explanation\n- **Zero Variance Case**: The code checks if the variance is zero. If so, it directly compares \\( x \\) with \\( \\mu \\) to determine the CDF, which can be 0, 0.5, or 1 based on the comparison.\n- **Positive Variance Case**: For non-zero variance, the standard deviation \\( \\sigma \\) is computed as the square root of the variance. The argument to the error function is calculated, and the CDF is computed using the given formula. The result is then rounded to six decimal places to meet the output requirements.\n\nThis approach efficiently handles both edge cases (zero variance) and general cases (positive variance) using mathematical properties of the Gaussian distribution and the error function."}
{"task_id": 401, "completion_id": 0, "completion": "To solve this problem, we need to convert a vector of integer class labels into a one-hot encoded NumPy matrix. The solution must handle various input formats and ensure the output meets specific constraints, such as being a float array containing only 0.0 and 1.0.\n\n### Approach\n1. **Check for Existing One-Hot Encoding**: First, we check if the input array is already a valid one-hot encoded matrix. This involves verifying that the array is 2D, each row sums to 1, and all elements are either 0 or 1. If these conditions are met, we return a copy of the input array cast to float to ensure it meets the output constraints.\n2. **Flatten the Input**: If the input is not a valid one-hot matrix, we reshape it into a 1D array to handle both 1D and 2D (N, 1) inputs uniformly.\n3. **Determine Number of Classes**: If the number of classes (`num_classes`) is not provided, it is inferred as the maximum value in the flattened array plus 1.\n4. **Create One-Hot Matrix**: We initialize a matrix of zeros with the appropriate shape and set the corresponding positions to 1.0 using advanced indexing.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_one_hot(y: np.ndarray, num_classes: int | None = None) -> np.ndarray:\n    \"\"\"Convert a vector of integer class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        1-D or 2-D NumPy array containing integer labels. If the array is\n        already one-hot encoded (shape (N, C) with rows summing to 1), the\n        function should return a copy of the input.\n    num_classes : int, optional\n        Total number of classes. If ``None`` the number is inferred from ``y``.\n\n    Returns\n    -------\n    np.ndarray\n        One-hot encoded matrix of shape (N, C) with dtype ``float``.\n    \"\"\"\n    # Check if the input is already a valid one-hot encoded matrix\n    if y.ndim == 2:\n        sum_ok = np.allclose(y.sum(axis=1), 1.0)\n        elements_ok = np.all((y == 0) | (y == 1))\n        if sum_ok and elements_ok:\n            return y.copy().astype(float)\n    \n    # Flatten the input to handle both 1D and 2D (N, 1) cases\n    y_flat = y.reshape(-1)\n    \n    # Determine the number of classes if not provided\n    if num_classes is None:\n        num_classes = y_flat.max() + 1\n    \n    # Create the one-hot encoded matrix\n    n_samples = y_flat.size\n    one_hot = np.zeros((n_samples, num_classes), dtype=float)\n    one_hot[np.arange(n_samples), y_flat] = 1.0\n    \n    return one_hot\n```\n\n### Explanation\n1. **Check for Valid One-Hot Matrix**: The function first checks if the input is a valid one-hot encoded matrix by verifying that it is 2D, each row sums to 1, and all elements are 0 or 1. If true, it returns a float copy of the input.\n2. **Reshape Input**: The input is reshaped into a 1D array to handle both 1D and column vector (2D) inputs uniformly.\n3. **Infer Number of Classes**: If the number of classes is not specified, it is determined based on the maximum value in the flattened array.\n4. **Generate One-Hot Matrix**: A matrix of zeros is created, and positions corresponding to the class labels are set to 1.0 using advanced indexing, ensuring the result is a float array with the correct shape and values.\n\nThis approach efficiently handles various input formats and ensures the output meets the specified constraints, providing a robust solution for converting class labels to one-hot encoded matrices."}
{"task_id": 402, "completion_id": 0, "completion": "To solve this problem, we need to implement a Gaussian Naive Bayes (GNB) classifier from scratch. The GNB model assumes that each feature is independent given the class label and follows a normal distribution. The task involves training the model on the provided training data and then using it to predict labels for the test data.\n\n### Approach\n1. **Class Priors**: Compute the prior probability for each class by dividing the number of samples in each class by the total number of training samples. Convert these probabilities to logarithms to avoid underflow during calculations.\n2. **Means and Variances**: For each class and each feature, calculate the mean and variance. The variance includes a small constant (epsilon) to prevent division by zero.\n3. **Log-Posterior Calculation**: For each test instance, compute the log-posterior probability for each class using the log-prior, means, and variances. The log-posterior is derived from the sum of the log-likelihoods of each feature under the normal distribution.\n4. **Prediction**: For each test instance, predict the class with the highest log-posterior probability.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_nb_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n    \"\"\"Predict labels for X_test using a Gaussian Naive Bayes classifier.\"\"\"\n    # Get unique classes and their counts\n    classes, counts = np.unique(y_train, return_counts=True)\n    n_classes = len(classes)\n    n_features = X_train.shape[1]\n    \n    # Compute log priors\n    log_priors = np.log(counts / len(y_train))\n    \n    # Compute means and variances for each class and feature\n    means = np.zeros((n_classes, n_features))\n    variances = np.zeros((n_classes, n_features))\n    \n    for i, c in enumerate(classes):\n        mask = (y_train == c)\n        X_c = X_train[mask]\n        means[i] = np.mean(X_c, axis=0)\n        # Calculate variance with ddof=0 (divided by N_c) and add epsilon\n        variances[i] = np.var(X_c, axis=0, ddof=0) + eps\n    \n    # Calculate log posteriors for each test example and class\n    n_test = X_test.shape[0]\n    log_posteriors = np.zeros((n_test, n_classes))\n    \n    for i in range(n_classes):\n        mu = means[i]\n        var = variances[i]\n        log_prior = log_priors[i]\n        \n        # Compute difference squared and terms for each feature\n        diff = X_test - mu\n        squared = diff ** 2\n        term = squared / var\n        \n        # Precompute log(2*pi*var) for all features\n        log_2pi_var = np.log(2 * np.pi * var)\n        \n        # Sum over features for each test example\n        sum_terms = np.sum(log_2pi_var + term, axis=1)\n        log_likelihood = -0.5 * sum_terms\n        \n        log_posteriors[:, i] = log_prior + log_likelihood\n    \n    # Find the class with the maximum log-posterior for each test example\n    predicted_indices = np.argmax(log_posteriors, axis=1)\n    \n    return classes[predicted_indices]\n```\n\n### Explanation\n1. **Class Priors**: The prior probabilities are calculated by dividing the count of each class by the total number of samples. This is converted to a log scale to prevent numerical underflow.\n2. **Means and Variances**: For each class, the mean of each feature is calculated. The variance is computed with a small constant added to avoid zero variance, which ensures numerical stability.\n3. **Log-Posterior Calculation**: For each test instance, the log-posterior is computed by summing the log-likelihoods of each feature under the normal distribution assumption, adjusted by the log-prior of the class.\n4. **Prediction**: The class with the highest log-posterior for each test instance is selected as the predicted label.\n\nThis approach efficiently computes the necessary parameters and leverages vector operations to handle multiple test instances simultaneously, ensuring both accuracy and performance."}
{"task_id": 405, "completion_id": 0, "completion": "To solve this problem, we need to implement an activation function factory that applies a specified activation function to a given input vector. The function should handle various activation functions, including Identity, ReLU, Tanh, Sigmoid, Leaky ReLU, and Affine transformations. Each activation function may require different parameters, and the solution must handle these parameters appropriately while ensuring the input is processed correctly and the output is formatted as required.\n\n### Approach\n1. **Input Conversion**: Convert the input vector into a NumPy array to handle both lists and arrays uniformly.\n2. **Parameter Parsing**: Parse the activation function name and its parameters from the input string. The activation function name is case-insensitive, and parameters are extracted from the string if provided.\n3. **Validation**: Ensure that the activation function is valid and that any parameters provided are appropriate for the chosen function. Raise errors for unknown functions or invalid parameters.\n4. **Apply Activation Function**: Based on the parsed activation function and parameters, apply the corresponding mathematical transformation to the input vector.\n5. **Formatting Output**: Round the results to six decimal places and convert the result to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef apply_activation(x, param=None):\n    \"\"\"Apply a chosen activation to the input vector.\n\n    Args:\n        x (list[float] | np.ndarray): Input data. Anything that can be\n            converted to a NumPy array of floats is accepted.\n        param (str | None, optional): Description of the activation. See the\n            task specification for the accepted values. If *None* the\n            identity function is applied.\n\n    Returns:\n        list[float]: Result of applying the activation element-wise, rounded\n            to six decimals and converted to a standard Python *list*.\n    \"\"\"\n    x = np.asarray(x, dtype=np.float_)\n    \n    if param is None:\n        activation_type = 'identity'\n        params = {}\n    else:\n        param_str = param.strip()\n        if '(' in param_str:\n            act_name_part, rest = param_str.split('(', 1)\n            act_name = act_name_part.strip().lower()\n            params_part = rest.split(')', 1)[0].strip()\n        else:\n            act_name = param_str.strip().lower()\n            params_part = ''\n        \n        params = {}\n        if params_part:\n            parts = params_part.split(',')\n            for part in parts:\n                part = part.strip()\n                if '=' not in part:\n                    raise ValueError(f\"Invalid parameter format in '{param}': '{part}'\")\n                key, val = part.split('=', 1)\n                key = key.strip().lower()\n                val = val.strip()\n                try:\n                    params[key] = float(val)\n                except ValueError:\n                    raise ValueError(f\"Invalid parameter value '{val}' in activation '{param}'\")\n        \n        # Determine activation type and validate parameters\n        if act_name == 'identity':\n            activation_type = 'identity'\n            allowed_params = set()\n        elif act_name == 'relu':\n            activation_type = 'relu'\n            allowed_params = set()\n        elif act_name == 'tanh':\n            activation_type = 'tanh'\n            allowed_params = set()\n        elif act_name == 'sigmoid':\n            activation_type = 'sigmoid'\n            allowed_params = set()\n        elif act_name == 'leaky relu':\n            activation_type = 'leaky_relu'\n            allowed_params = {'alpha'}\n        elif act_name == 'affine':\n            activation_type = 'affine'\n            allowed_params = {'slope', 'intercept'}\n        else:\n            raise ValueError(f\"Unknown activation: {param}\")\n        \n        # Check for unknown parameters\n        for key in params:\n            if key not in allowed_params:\n                raise ValueError(f\"Invalid parameter '{key}' for activation '{act_name}' in '{param}'\")\n        \n        # Check required parameters and set defaults\n        if activation_type == 'leaky_relu':\n            alpha = params.get('alpha', 0.01)\n        elif activation_type == 'affine':\n            required = {'slope', 'intercept'}\n            missing = required - params.keys()\n            if missing:\n                raise ValueError(f\"Missing required parameters {missing} for affine activation in '{param}'\")\n            slope = params['slope']\n            intercept = params['intercept']\n        elif activation_type in ['identity', 'relu', 'tanh', 'sigmoid']:\n            if params:\n                raise ValueError(f\"Activation '{act_name}' does not accept parameters in '{param}'\")\n    \n    # Apply the activation function\n    if activation_type == 'identity':\n        result = x\n    elif activation_type == 'relu':\n        result = np.maximum(0, x)\n    elif activation_type == 'tanh':\n        result = np.tanh(x)\n    elif activation_type == 'sigmoid':\n        result = 1 / (1 + np.exp(-x))\n    elif activation_type == 'leaky_relu':\n        result = np.where(x > 0, x, alpha * x)\n    elif activation_type == 'affine':\n        result = slope * x + intercept\n    else:\n        raise ValueError(f\"Unhandled activation type: {activation_type}\")\n    \n    # Round and convert to list\n    result_rounded = np.round(result, 6)\n    return result_rounded.tolist()\n```\n\n### Explanation\n1. **Input Handling**: The input vector is converted to a NumPy array to ensure uniform processing.\n2. **Parameter Parsing**: The activation function name and parameters are extracted from the input string. The function name is case-insensitive, and parameters are parsed into a dictionary.\n3. **Validation**: The code checks for valid activation functions and parameters, raising appropriate errors for invalid inputs.\n4. **Activation Application**: Based on the parsed function and parameters, the corresponding mathematical transformation is applied using efficient NumPy operations.\n5. **Output Formatting**: The result is rounded to six decimal places and converted to a Python list to meet the specified output format.\n\nThis approach ensures that all specified activation functions are handled correctly, parameters are validated, and the output is formatted as required."}
{"task_id": 406, "completion_id": 0, "completion": "To solve this problem, we need to implement the k-Nearest Neighbors (k-NN) algorithm for classification. The goal is to predict the class labels for a set of test samples based on the majority vote of their k nearest neighbors from the training data. The solution involves computing Euclidean distances, selecting nearest neighbors, and determining the most frequent class label among them.\n\n### Approach\n1. **Compute Distances**: Calculate the squared Euclidean distances between each test sample and all training samples. This avoids the computational cost of taking square roots while maintaining the order of distances.\n2. **Select Nearest Neighbors**: For each test sample, identify the indices of the k training samples with the smallest distances. If k exceeds the number of training samples, use all available samples.\n3. **Retrieve Labels**: Obtain the class labels of these nearest neighbors from the training data.\n4. **Majority Vote**: Count the occurrences of each class label among the nearest neighbors and predict the label with the highest count. In case of a tie, the smallest label value is chosen.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int = 5) -> list[int]:\n    \"\"\"k-Nearest Neighbors classifier.\"\"\"\n    \n    # Compute squared Euclidean distances between each test sample and all training samples\n    squared_diff = (X_test[:, np.newaxis, :] - X_train) ** 2\n    squared_distances = np.sum(squared_diff, axis=2)\n    \n    # Get the indices of the k smallest distances for each test sample\n    sorted_indices = np.argsort(squared_distances, axis=1)\n    k_nearest_indices = sorted_indices[:, :k]\n    \n    # Retrieve the labels of the k nearest neighbors\n    k_nearest_labels = y_train[k_nearest_indices]\n    \n    # Determine the most frequent label for each test sample\n    predictions = []\n    for labels in k_nearest_labels:\n        counts = np.bincount(labels)\n        if len(counts) == 0:\n            # This case is only possible if k=0, which is invalid per the problem constraints\n            predictions.append(0)\n        else:\n            predictions.append(np.argmax(counts))\n    \n    return predictions\n```\n\n### Explanation\n1. **Distance Calculation**: Using broadcasting, we compute the squared Euclidean distances between each test sample and all training samples efficiently. This avoids the need for explicit loops and leverages vectorized operations for performance.\n2. **Neighbor Selection**: By sorting the distances and selecting the indices of the k smallest values, we efficiently determine the nearest neighbors. The use of `argsort` ensures we get the correct indices, and slicing handles cases where k exceeds the number of training samples.\n3. **Label Retrieval and Voting**: For each test sample, the corresponding labels of the nearest neighbors are gathered. Using `np.bincount`, we count the occurrences of each label, and `argmax` selects the label with the highest count, naturally handling ties by choosing the smallest label.\n\nThis approach ensures that the k-NN algorithm is implemented efficiently and correctly, adhering to the problem constraints and requirements."}
{"task_id": 410, "completion_id": 0, "completion": "To solve this problem, we need to parse a textual description of a neural-network activation function into a structured Python dictionary. The activation function can be one of several predefined types, some of which include parameters. The goal is to validate the input string and convert it into a dictionary with the appropriate keys and values, or return -1 if the input is invalid.\n\n### Approach\n1. **Identify Simple Cases**: Check if the input string matches one of the simple activation functions (\"ReLU\", \"Sigmoid\", \"Tanh\"). If it does, return a dictionary with the \"type\" key set to the input string.\n2. **Handle Parameterized Functions**: For activation functions with parameters (\"Affine\" and \"Leaky ReLU\"), use regular expressions to validate the format and extract parameters. The regular expressions are designed to match the exact structure specified in the problem, ensuring no extra spaces and correct parameter order.\n3. **Convert Parameters to Floats**: For valid parameterized functions, convert the extracted parameters from strings to floats and include them in the result dictionary.\n4. **Return -1 for Invalid Inputs**: If the input does not match any valid pattern, return -1.\n\n### Solution Code\n```python\nimport re\n\ndef parse_activation(act_str: str):\n    \"\"\"Parse an activation-function description string.\n\n    The string can be one of the following forms:\n        \u2022 \"ReLU\"\n        \u2022 \"Sigmoid\"\n        \u2022 \"Tanh\"\n        \u2022 \"Affine(slope=<number>, intercept=<number>)\"\n        \u2022 \"Leaky ReLU(alpha=<number>)\"\n\n    Args:\n        act_str: Description of the activation function.\n\n    Returns:\n        A dictionary with key \"type\" and, when applicable, its parameters.\n        If the input format is invalid, return -1.\n    \"\"\"\n    # Check for simple activation types\n    if act_str in [\"ReLU\", \"Sigmoid\", \"Tanh\"]:\n        return {\"type\": act_str}\n    \n    # Regular expression for matching numeric values (integers, floats, scientific notation)\n    number_pattern = r'([+-]?(?:\\d+\\.?\\d*|\\.\\d+)(?:[eE][+-]?\\d+)?)'\n    \n    # Check for Affine activation\n    affine_re = re.compile(\n        r'^Affine\\('\n        r'slope=' + number_pattern + r', '\n        r'intercept=' + number_pattern + r'\\)$'\n    )\n    affine_match = affine_re.fullmatch(act_str)\n    if affine_match:\n        try:\n            slope = float(affine_match.group(1))\n            intercept = float(affine_match.group(2))\n            return {\n                \"type\": \"Affine\",\n                \"slope\": slope,\n                \"intercept\": intercept\n            }\n        except:\n            return -1\n    \n    # Check for Leaky ReLU activation\n    leaky_relu_re = re.compile(\n        r'^Leaky ReLU\\('\n        r'alpha=' + number_pattern + r'\\)$'\n    )\n    leaky_match = leaky_relu_re.fullmatch(act_str)\n    if leaky_match:\n        try:\n            alpha = float(leaky_match.group(1))\n            return {\n                \"type\": \"Leaky ReLU\",\n                \"alpha\": alpha\n            }\n        except:\n            return -1\n    \n    # If none of the patterns matched\n    return -1\n```\n\n### Explanation\n1. **Simple Cases Handling**: The function first checks if the input string is one of the simple activation functions (\"ReLU\", \"Sigmoid\", \"Tanh\") and returns the corresponding dictionary immediately if a match is found.\n2. **Regular Expressions for Parameterized Functions**: For \"Affine\" and \"Leaky ReLU\", regular expressions are used to ensure the input string matches the expected format. The regex patterns are designed to capture numeric values, including integers, floats, and scientific notation, and enforce the correct structure and order of parameters.\n3. **Parameter Extraction and Conversion**: When a valid match is found for a parameterized function, the parameters are extracted from the regex groups, converted to floats, and included in the result dictionary. If conversion fails (due to invalid numeric format), the function returns -1.\n4. **Invalid Input Handling**: If the input string does not match any valid pattern, the function returns -1, indicating an invalid input format.\n\nThis approach ensures that all valid activation function descriptions are parsed correctly, while invalid inputs are efficiently detected and handled."}
{"task_id": 411, "completion_id": 0, "completion": "To solve this problem, we need to compute various statistics about actions and observations collected in a reinforcement learning environment. These statistics help in understanding the structure and nature of the data, such as whether the data is multi-dimensional, discrete, or continuous, and the number of unique values in each dimension.\n\n### Approach\n1. **Check for Iterables**: Determine if the elements in the actions or observations lists are iterables (lists, tuples, or numpy arrays).\n2. **Dimensionality**: Compute the dimensionality of the actions and observations. If the elements are iterables, the dimensionality is the length of the first element. Otherwise, the dimensionality is 1.\n3. **Multi-dimensional Check**: Check if the dimensionality is greater than 1.\n4. **Continuous Check**: Flatten the lists and check if any element is a non-integer float, indicating continuous data.\n5. **Unique Values per Dimension**: Collect values for each dimension, compute the number of unique values, and sort these unique values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\"\"\"\n\n    def to_python_scalar(value):\n        if isinstance(value, np.generic):\n            return value.item()\n        return value\n\n    def get_tuple_flag(lst):\n        if not lst:\n            return False\n        return isinstance(lst[0], (list, tuple, np.ndarray))\n\n    def is_continuous(value):\n        if isinstance(value, np.generic):\n            value = value.item()\n        if isinstance(value, float):\n            return not value.is_integer()\n        if isinstance(value, int):\n            return False\n        return False\n\n    def process_dimensions(lst, is_tuple):\n        if not lst:\n            return [], []\n        dimensions = []\n        if is_tuple:\n            num_dim = len(lst[0])\n            dimensions = [[] for _ in range(num_dim)]\n            for elem in lst:\n                for i in range(num_dim):\n                    val = elem[i]\n                    py_val = to_python_scalar(val)\n                    dimensions[i].append(py_val)\n        else:\n            dimensions = [[to_python_scalar(elem) for elem in lst]]\n        n_per_dim = []\n        ids_per_dim = []\n        for dim in dimensions:\n            unique = sorted(set(dim))\n            n_per_dim.append(len(unique))\n            ids_per_dim.append(list(unique))\n        return n_per_dim, ids_per_dim\n\n    # Process actions\n    tuple_actions = get_tuple_flag(actions)\n    if tuple_actions:\n        action_dim = len(actions[0]) if actions else 0\n    else:\n        action_dim = 1 if actions else 0\n    multidim_actions = action_dim > 1\n\n    flattened_actions = []\n    if tuple_actions and actions:\n        for elem in actions:\n            flattened_actions.extend(elem)\n    else:\n        flattened_actions = list(actions)\n    continuous_actions = any(is_continuous(val) for val in flattened_actions)\n\n    n_actions_per_dim, action_ids = process_dimensions(actions, tuple_actions)\n\n    # Process observations\n    tuple_observations = get_tuple_flag(observations)\n    if tuple_observations:\n        obs_dim = len(observations[0]) if observations else 0\n    else:\n        obs_dim = 1 if observations else 0\n    multidim_observations = obs_dim > 1\n\n    flattened_observations = []\n    if tuple_observations and observations:\n        for elem in observations:\n            flattened_observations.extend(elem)\n    else:\n        flattened_observations = list(observations)\n    continuous_observations = any(is_continuous(val) for val in flattened_observations)\n\n    n_obs_per_dim, obs_ids = process_dimensions(observations, tuple_observations)\n\n    return {\n        'tuple_actions': tuple_actions,\n        'tuple_observations': tuple_observations,\n        'multidim_actions': multidim_actions,\n        'multidim_observations': multidim_observations,\n        'continuous_actions': continuous_actions,\n        'continuous_observations': continuous_observations,\n        'n_actions_per_dim': n_actions_per_dim,\n        'action_dim': action_dim,\n        'n_obs_per_dim': n_obs_per_dim,\n        'obs_dim': obs_dim,\n        'action_ids': action_ids,\n        'obs_ids': obs_ids\n    }\n```\n\n### Explanation\n1. **Check for Iterables**: The function `get_tuple_flag` checks if the elements of the list are iterables by inspecting the first element (if the list is non-empty).\n2. **Dimensionality**: For iterables, the dimensionality is the length of the first element. For scalars, the dimensionality is 1.\n3. **Multi-dimensional Check**: If the dimensionality is greater than 1, the data is considered multi-dimensional.\n4. **Continuous Check**: The list is flattened, and each element is checked to see if it is a non-integer float using the `is_continuous` function.\n5. **Unique Values per Dimension**: The `process_dimensions` function collects values for each dimension, computes the number of unique values, and sorts these values to provide the required statistics.\n\nThis approach efficiently computes all required statistics by leveraging Python's built-in data structures and functions, ensuring clarity and correctness in handling both scalar and multi-dimensional data."}
{"task_id": 412, "completion_id": 0, "completion": "To solve this problem, we need to implement four common evaluation metrics for binary classification and a helper function that retrieves these metrics by name. The metrics are accuracy, precision, recall, and F1 score. Each metric must be computed and rounded to exactly four decimal places.\n\n### Approach\n1. **Accuracy**: This metric is the ratio of correct predictions to the total number of predictions. If the total number of predictions is zero, return 0.0.\n2. **Precision**: This metric is the ratio of true positives (TP) to the sum of true positives and false positives (FP). If the denominator is zero, return 0.0.\n3. **Recall**: This metric is the ratio of true positives (TP) to the sum of true positives and false negatives (FN). If the denominator is zero, return 0.0.\n4. **F1 Score**: This metric is the harmonic mean of precision and recall. If both precision and recall are zero, return 0.0.\n\nThe helper function `get_metric` will return the corresponding metric function based on the input name. If the name is invalid, it returns -1.\n\n### Solution Code\n```python\ndef get_metric(name: str):\n    \"\"\"Return an evaluation metric function by *name*.\n\n    Supported names:\n    - 'accuracy'\n    - 'precision'\n    - 'recall'\n    - 'f1'\n\n    If the name is invalid, the function should return -1.\n    The returned metric functions must accept two equal-length iterables\n    (y_true, y_pred) containing only 0s and 1s and return the metric value\n    rounded to 4 decimal places.\n    \"\"\"\n    if name == 'accuracy':\n        def metric(y_true, y_pred):\n            correct = sum(t == p for t, p in zip(y_true, y_pred))\n            total = len(y_true)\n            return round(correct / total, 4) if total != 0 else 0.0\n        return metric\n    elif name == 'precision':\n        def metric(y_true, y_pred):\n            tp = sum((t == 1 and p == 1) for t, p in zip(y_true, y_pred))\n            fp = sum((t == 0 and p == 1) for t, p in zip(y_true, y_pred))\n            denominator = tp + fp\n            return round(tp / denominator, 4) if denominator != 0 else 0.0\n        return metric\n    elif name == 'recall':\n        def metric(y_true, y_pred):\n            tp = sum((t == 1 and p == 1) for t, p in zip(y_true, y_pred))\n            fn = sum((t == 1 and p == 0) for t, p in zip(y_true, y_pred))\n            denominator = tp + fn\n            return round(tp / denominator, 4) if denominator != 0 else 0.0\n        return metric\n    elif name == 'f1':\n        def metric(y_true, y_pred):\n            tp = sum((t == 1 and p == 1) for t, p in zip(y_true, y_pred))\n            fp = sum((t == 0 and p == 1) for t, p in zip(y_true, y_pred))\n            fn = sum((t == 1 and p == 0) for t, p in zip(y_true, y_pred))\n            \n            # Compute precision\n            denom_p = tp + fp\n            precision = tp / denom_p if denom_p != 0 else 0.0\n            \n            # Compute recall\n            denom_r = tp + fn\n            recall = tp / denom_r if denom_r != 0 else 0.0\n            \n            # Compute F1\n            if (precision + recall) == 0:\n                return 0.0\n            f1 = 2 * (precision * recall) / (precision + recall)\n            return round(f1, 4)\n        return metric\n    else:\n        return -1\n```\n\n### Explanation\n- **Accuracy**: Calculates the ratio of correct predictions to the total number of predictions. If the total is zero, returns 0.0.\n- **Precision**: Calculates the ratio of true positives to the sum of true positives and false positives. If the sum is zero, returns 0.0.\n- **Recall**: Calculates the ratio of true positives to the sum of true positives and false negatives. If the sum is zero, returns 0.0.\n- **F1 Score**: Computes the harmonic mean of precision and recall. If both precision and recall are zero, returns 0.0.\n\nEach metric function is defined within the `get_metric` function and returned based on the input name. The helper function ensures that only valid metric names return the corresponding function, while invalid names return -1. All results are rounded to four decimal places as required."}
{"task_id": 413, "completion_id": 0, "completion": "To solve this problem, we need to implement a simplified Gradient Boosting Decision Tree (GBDT) regressor from scratch. The goal is to predict the target values for test data using an ensemble of regression trees, each built to correct the residuals of the previous predictions.\n\n### Approach\n1. **Initial Prediction**: Start with an initial prediction as the mean of the training targets.\n2. **Residual Calculation**: For each boosting iteration, compute the residuals (differences between the actual and predicted values).\n3. **Tree Construction**: Fit a regression tree to these residuals using the CART algorithm, which splits nodes to minimize the mean squared error (MSE) and stops growing when the maximum depth is reached or no further error reduction is possible.\n4. **Prediction Update**: Update the predictions by adding the tree's predictions multiplied by a learning rate.\n5. **Iterative Process**: Repeat the residual calculation, tree construction, and prediction update for a specified number of iterations (n_estimators).\n6. **Final Prediction**: Combine the initial prediction with all subsequent tree predictions to form the final prediction for test data.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 10, learning_rate: float = 0.1, max_depth: int = 3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\"\"\"\n\n    def build_tree(X, y, depth, max_depth):\n        if depth >= max_depth or len(y) <= 1:\n            return {'value': np.mean(y)}\n        if np.var(y) == 0:\n            return {'value': np.mean(y)}\n        \n        best_feature = None\n        best_threshold = None\n        best_mse = np.inf\n        n_samples, n_features = X.shape\n        \n        for feature in range(n_features):\n            sorted_indices = np.argsort(X[:, feature])\n            sorted_X_feature = X[sorted_indices, feature]\n            sorted_y = y[sorted_indices]\n            \n            sum_total = sorted_y.sum()\n            sum_sq_total = (sorted_y ** 2).sum()\n            sum_left = 0.0\n            sum_sq_left = 0.0\n            \n            for i in range(1, len(sorted_y)):\n                sum_left += sorted_y[i-1]\n                sum_sq_left += sorted_y[i-1] ** 2\n                sum_right = sum_total - sum_left\n                sum_sq_right = sum_sq_total - sum_sq_left\n                n_left = i\n                n_right = len(sorted_y) - i\n                \n                if n_left == 0 or n_right == 0:\n                    continue\n                \n                mse_left = sum_sq_left - (sum_left ** 2) / n_left\n                mse_right = sum_sq_right - (sum_right ** 2) / n_right\n                total_mse = mse_left + mse_right\n                \n                if total_mse < best_mse:\n                    best_mse = total_mse\n                    best_feature = feature\n                    best_threshold = (sorted_X_feature[i-1] + sorted_X_feature[i]) / 2.0\n        \n        if best_feature is None:\n            return {'value': np.mean(y)}\n        \n        left_mask = X[:, best_feature] <= best_threshold\n        right_mask = ~left_mask\n        \n        if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n            return {'value': np.mean(y)}\n        \n        left_subtree = build_tree(X[left_mask], y[left_mask], depth + 1, max_depth)\n        right_subtree = build_tree(X[right_mask], y[right_mask], depth + 1, max_depth)\n        \n        return {\n            'feature': best_feature,\n            'threshold': best_threshold,\n            'left': left_subtree,\n            'right': right_subtree\n        }\n    \n    def predict_tree(node, X):\n        predictions = np.zeros(X.shape[0])\n        for i in range(X.shape[0]):\n            current_node = node\n            while True:\n                if 'value' in current_node:\n                    predictions[i] = current_node['value']\n                    break\n                else:\n                    feat = current_node['feature']\n                    thr = current_node['threshold']\n                    if X[i, feat] <= thr:\n                        current_node = current_node['left']\n                    else:\n                        current_node = current_node['right']\n        return predictions\n    \n    initial_pred = np.mean(y_train)\n    current_train_pred = np.full_like(y_train, initial_pred, dtype=np.float64)\n    test_pred = np.full(X_test.shape[0], initial_pred, dtype=np.float64)\n    \n    for _ in range(n_estimators):\n        residuals = y_train - current_train_pred\n        tree = build_tree(X_train, residuals, 0, max_depth)\n        \n        # Update training predictions\n        pred_train = predict_tree(tree, X_train)\n        current_train_pred += learning_rate * pred_train\n        \n        # Update test predictions\n        pred_test = predict_tree(tree, X_test)\n        test_pred += learning_rate * pred_test\n    \n    test_pred_rounded = np.round(test_pred, 4)\n    return test_pred_rounded.tolist()\n```\n\n### Explanation\n1. **Initial Prediction**: The initial prediction for all samples is the mean of the training targets.\n2. **Residual Calculation**: For each iteration, residuals are computed as the difference between the actual target values and the current predictions.\n3. **Tree Construction**: A regression tree is built using the residuals. Each split in the tree is determined by minimizing the MSE. The tree stops growing when the maximum depth is reached or no valid split reduces the error.\n4. **Prediction Update**: The predictions are updated by adding the tree's predictions scaled by the learning rate. This process corrects the residuals iteratively.\n5. **Final Prediction**: After all iterations, the final prediction for test data is the sum of the initial prediction and all subsequent tree predictions, each scaled by the learning rate. The results are rounded to four decimal places and returned as a list."}
{"task_id": 414, "completion_id": 0, "completion": "To solve this problem, we need to implement an exponential learning-rate scheduler that adjusts the learning rate based on the current optimization step. The learning rate decreases exponentially either smoothly or in a stair-step fashion depending on the provided parameters. \n\n### Approach\n1. **Input Validation**: Check if the provided inputs are valid. Specifically, ensure that the step is non-negative, the stage length is a positive integer, and the decay factor is positive. If any of these conditions are not met, return -1.\n2. **Current Stage Calculation**: Compute the current stage based on the optimization step and stage length. If the staircase parameter is True, use floor division to get an integer stage. Otherwise, use real-valued division for a smooth transition.\n3. **Learning Rate Calculation**: Use the exponential decay formula to compute the learning rate based on the initial learning rate, decay factor, and current stage.\n4. **Rounding**: Round the computed learning rate to six decimal places before returning it.\n\n### Solution Code\n```python\ndef exponential_scheduler(step: int,\n                          initial_lr: float = 0.01,\n                          stage_length: int = 500,\n                          staircase: bool = False,\n                          decay: float = 0.1) -> float:\n    \"\"\"Return the exponentially decayed learning-rate for a given optimisation step.\n\n    The learning-rate is computed as::\n\n        lr = initial_lr * decay ** curr_stage\n\n    where ``curr_stage`` is either the real-valued ratio *step / stage_length*\n    (smooth schedule) or its floor value (stair-case schedule).\n\n    Args:\n        step:          Current optimisation step (must be non-negative).\n        initial_lr:    Learning-rate at step 0.\n        stage_length:  Number of steps that constitute one decay stage.\n        staircase:     If ``True`` a step-wise decay is used.\n        decay:         Multiplicative decay factor (> 0).\n\n    Returns:\n        The learning rate for the given ``step`` rounded to six decimal\n        places, or ``-1`` if an invalid argument is supplied.\n    \"\"\"\n    # Check for invalid arguments\n    if step < 0:\n        return -1\n    if not isinstance(stage_length, int) or stage_length <= 0:\n        return -1\n    if decay <= 0:\n        return -1\n\n    # Compute current stage\n    if staircase:\n        curr_stage = step // stage_length\n    else:\n        curr_stage = step / stage_length\n\n    # Calculate learning rate\n    lr = initial_lr * (decay ** curr_stage)\n\n    # Round to six decimal places\n    return round(lr, 6)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the step is non-negative, the stage length is a positive integer, and the decay factor is positive. If any of these checks fail, the function returns -1 to indicate an invalid input.\n2. **Current Stage Calculation**: Depending on the `staircase` parameter, the current stage is calculated either using floor division (for staircase decay) or real division (for smooth decay). This determines how the learning rate decreases over time.\n3. **Learning Rate Calculation**: The learning rate is computed using the exponential decay formula, which multiplies the initial learning rate by the decay factor raised to the power of the current stage.\n4. **Rounding**: The computed learning rate is rounded to six decimal places to ensure precision and consistency in the output.\n\nThis approach efficiently handles both smooth and stair-step decay scenarios while ensuring input validity and precise output formatting."}
{"task_id": 415, "completion_id": 0, "completion": "To solve this problem, we need to implement the one-dimensional Type-II Discrete Cosine Transform (DCT-II) for a given input signal. The DCT-II is a widely used transformation in signal processing and data compression. The goal is to transform a given list of real numbers into their corresponding DCT-II coefficients using NumPy for efficient numerical computations.\n\n### Approach\n1. **Check for Empty Input**: If the input list is empty, return an empty list immediately.\n2. **Convert Input to NumPy Array**: Convert the input list into a NumPy array to leverage efficient array operations.\n3. **Compute Necessary Indices**: Generate indices for the input elements and reshape them to facilitate matrix operations.\n4. **Construct the Cosine Matrix**: Create a matrix where each element represents the cosine term specified in the DCT-II formula. This involves broadcasting to handle the multiplication of indices efficiently.\n5. **Matrix Multiplication**: Multiply the cosine matrix with the input vector to compute the DCT coefficients.\n6. **Rounding**: Round the resulting coefficients to four decimal places and convert them back to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dct2(frame: list[int | float]) -> list[float]:\n    \"\"\"Implements the un-normalized 1-D DCT-II as per the given definition.\"\"\"\n    if not frame:\n        return []\n    x = np.array(frame)\n    N = len(x)\n    n = np.arange(N)\n    n_ph = n + 0.5\n    k = np.arange(N).reshape(-1, 1)\n    arg = (np.pi / N) * k * n_ph\n    cos_matrix = np.cos(arg)\n    coeffs = cos_matrix @ x\n    rounded = np.round(coeffs, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Empty Input Handling**: The function first checks if the input list is empty and returns an empty list if true.\n2. **Array Conversion**: The input list is converted into a NumPy array to utilize NumPy's efficient array operations.\n3. **Indices Generation**: Indices for the input elements (n) are generated, and each index is adjusted by 0.5 to form `n_ph`. The indices for the DCT coefficients (k) are reshaped into a column vector to facilitate broadcasting.\n4. **Cosine Matrix Construction**: The matrix of cosine terms is constructed using broadcasting. Each element (k, n) in the matrix is computed using the formula \\(\\cos\\left(\\frac{\\pi}{N} \\cdot k \\cdot (n + 0.5)\\right)\\).\n5. **Matrix Multiplication**: The cosine matrix is multiplied by the input vector (converted to a NumPy array) to compute the DCT coefficients. This operation efficiently computes the sum of products using matrix multiplication.\n6. **Rounding and Conversion**: The resulting coefficients are rounded to four decimal places and converted back to a list to match the required output format.\n\nThis approach ensures that the computation is efficient and leverages NumPy's capabilities for handling large datasets and performing complex mathematical operations succinctly."}
{"task_id": 416, "completion_id": 0, "completion": "To solve this problem, we need to implement the probability density function (PDF) of a multivariate Gaussian distribution without using any third-party libraries except NumPy. The solution involves calculating the PDF for each data point in the input array using the given mean vector and covariance matrix.\n\n### Approach\n1. **Input Handling**: Ensure the input data array `X` is reshaped to a 2D array if it is 1D. This ensures consistent handling of data points.\n2. **Inverse and Determinant**: Compute the inverse and determinant of the covariance matrix using NumPy's linear algebra functions.\n3. **Normalization Term**: Calculate the normalization constant using the determinant of the covariance matrix and the dimensionality of the data.\n4. **Quadratic Form**: For each data point, compute the quadratic form using the inverse of the covariance matrix. This involves vectorized operations to efficiently compute the necessary matrix multiplications.\n5. **Exponential Term**: Compute the exponential part of the PDF using the quadratic form results.\n6. **PDF Calculation**: Combine the normalization term with the exponential term to get the PDF values for each data point.\n7. **Rounding**: Round each PDF value to four decimal places and return the results as a list.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"Compute the PDF of a multivariate Gaussian distribution for given data points.\n    \n    Args:\n        X (np.ndarray): Data points, shape (n_samples, n_features) or (n_samples,) for 1D.\n        mean (np.ndarray): Mean vector of the distribution, shape (n_features,).\n        cov (np.ndarray): Covariance matrix, shape (n_features, n_features).\n        \n    Returns:\n        list: PDF values for each data point, rounded to four decimal places.\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    \n    d = len(mean)\n    cov_inv = np.linalg.inv(cov)\n    cov_det = np.linalg.det(cov)\n    \n    # Compute the normalization constant\n    denominator = (2 * np.pi) ** (d / 2) * np.sqrt(cov_det)\n    norm = 1.0 / denominator\n    \n    # Compute the quadratic term (x - mean)^T @ cov_inv @ (x - mean) for each x in X\n    diff = X - mean\n    temp = diff @ cov_inv\n    quadratic = np.sum(temp * diff, axis=1)\n    exponents = -0.5 * quadratic\n    \n    # Compute PDF values and round to 4 decimal places\n    pdf_values = norm * np.exp(exponents)\n    return [round(val, 4) for val in pdf_values]\n```\n\n### Explanation\n1. **Input Handling**: The input `X` is converted to a NumPy array and reshaped to ensure it is 2D. This allows consistent processing whether the input is 1D or 2D.\n2. **Covariance Matrix Operations**: Using NumPy's `linalg.inv` and `linalg.det`, we compute the inverse and determinant of the covariance matrix, which are essential for the PDF formula.\n3. **Normalization Constant**: The normalization term is derived from the determinant and the dimensionality of the data, ensuring the PDF integrates to 1 over the entire space.\n4. **Quadratic Form Calculation**: By vectorizing operations, we efficiently compute the quadratic form for all data points, which measures the squared Mahalanobis distance from the mean.\n5. **Exponential and PDF Calculation**: The exponential term adjusts the normalization by how far each data point is from the mean, scaled by the covariance structure. This is combined to get the final PDF values, which are then rounded to four decimal places for the output list.\n\nThis approach ensures efficient and accurate computation of the multivariate Gaussian PDF using vectorized operations and proper handling of matrix inverses and determinants."}
{"task_id": 417, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int = 1) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Perform Principal Component Analysis and return the projected data and its reconstruction.\"\"\"\n    # Step 1: Center the data\n    mean = np.mean(data, axis=0)\n    centered = data - mean\n\n    # Step 2: Compute the unbiased covariance matrix\n    n_samples = data.shape[0]\n    if n_samples > 1:\n        cov = (centered.T @ centered) / (n_samples - 1)\n    else:\n        cov = np.zeros((centered.shape[1], centered.shape[1]))\n\n    # Step 3: Eigen decomposition\n    eigenvalues, eigenvectors = np.linalg.eig(cov)\n    eigenvalues = np.real(eigenvalues)\n    eigenvectors = np.real(eigenvectors)\n\n    # Step 4: Sort eigenvectors by descending eigenvalues\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Step 5: Select the first k eigenvectors\n    k_eigenvectors = sorted_eigenvectors[:, :k]\n\n    # Step 6: Enforce deterministic eigenvector orientation\n    for i in range(k_eigenvectors.shape[1]):\n        vec = k_eigenvectors[:, i]\n        non_zero_mask = np.abs(vec) > 1e-8\n        if np.any(non_zero_mask):\n            first_non_zero = np.argmax(non_zero_mask)\n            if vec[first_non_zero] < 0:\n                k_eigenvectors[:, i] *= -1\n\n    # Step 7: Project data onto selected eigenvectors\n    low_dim = centered @ k_eigenvectors\n\n    # Step 8: Reconstruct data in original space\n    reconstructed_centered = low_dim @ k_eigenvectors.T\n    reconstructed = reconstructed_centered + mean\n\n    # Step 9: Round and convert to lists\n    low_dim_rounded = np.round(low_dim, 4)\n    reconstructed_rounded = np.round(reconstructed, 4)\n\n    return low_dim_rounded.tolist(), reconstructed_rounded.tolist()\n```"}
{"task_id": 418, "completion_id": 0, "completion": "To solve this problem, we need to compute the sample covariance matrix of a given data matrix. The sample covariance matrix is a crucial component in various statistical analyses, including Principal Component Analysis (PCA). The solution must handle different input formats, edge cases, and ensure numerical precision.\n\n### Approach\n1. **Input Handling**: Convert the input data matrix to a NumPy array if it is provided as a Python list. This ensures uniformity in processing.\n2. **Dimensionality Check**: Ensure the input is treated as a 2-dimensional array. If the input is a 1-dimensional array, reshape it to a 2-dimensional array with a single column.\n3. **Edge Case Handling**: If the number of samples (rows) is less than 2, return a zero matrix of appropriate dimensions since covariance cannot be computed with insufficient data.\n4. **Covariance Computation**: Use NumPy's built-in function `np.cov` to compute the covariance matrix. This function expects features as rows and samples as columns, so we transpose the input matrix before computation.\n5. **Precision and Formatting**: Round the resulting covariance matrix to 4 decimal places and convert it to a nested Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_covariance_matrix(X: np.ndarray | list[list[int | float]]) -> list[list[float]]:\n    \"\"\"Compute and return the sample covariance matrix of *X*.\n\n    Args:\n        X: A 2-D NumPy array or a nested Python list in which rows correspond to\n           samples and columns correspond to features.\n\n    Returns:\n        The covariance matrix as a nested Python list, rounded to 4 decimal\n        places.\n    \"\"\"\n    # Convert to numpy array if necessary\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    # Ensure X is 2D, reshaping 1D arrays to (n, 1)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    n, d = X.shape\n    if n < 2:\n        cov_matrix = np.zeros((d, d))\n    else:\n        # Transpose X to have features as rows for np.cov\n        cov_matrix = np.cov(X.T, ddof=1)\n    # Round and convert to list\n    return np.round(cov_matrix, 4).tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input is converted to a NumPy array if it is not already one. This allows consistent handling of the data using NumPy functions.\n2. **Reshaping 1D Arrays**: If the input is a 1-dimensional array, it is reshaped to a 2-dimensional array with a single column. This ensures that each element is treated as a sample with one feature.\n3. **Edge Case Handling**: When there are fewer than two samples, the covariance matrix cannot be computed, so a zero matrix of appropriate size is returned.\n4. **Covariance Calculation**: The `np.cov` function is used with the input transposed to ensure features are rows. The `ddof=1` parameter ensures the divisor is \\( n-1 \\) as required for the sample covariance.\n5. **Rounding and Conversion**: The result is rounded to four decimal places and converted to a nested list to meet the output requirements.\n\nThis approach efficiently handles various input scenarios and ensures accurate computation of the covariance matrix with proper numerical precision."}
{"task_id": 419, "completion_id": 0, "completion": "To solve this problem, we need to implement Bayesian linear regression with a conjugate Gaussian prior and known observation variance to compute the MAP (Maximum A Posteriori) predictive mean for new data points. The solution involves several steps, including preprocessing the data, handling prior parameters, computing the posterior distribution, and making predictions.\n\n### Approach\n1. **Intercept Handling**: If required, add an intercept column of ones to both the training data `X` and the new data `X_new`.\n2. **Prior Parameter Handling**: Process the prior mean `mu` and prior covariance matrix `V` to ensure they are in the correct form.\n3. **Posterior Computation**: Compute the posterior distribution parameters using the given formulas, which involve matrix inversions and multiplications.\n4. **Prediction**: Use the posterior mean to predict the target values for the new data points and round the results to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    # Add intercept column if required\n    if fit_intercept:\n        X = np.concatenate([np.ones((X.shape[0], 1), X], axis=1)\n        X_new = np.concatenate([np.ones((X_new.shape[0], 1), dtype=X.dtype), X_new], axis=1)\n    \n    n_features = X.shape[1]\n    \n    # Process mu into a vector\n    if np.isscalar(mu):\n        mu = np.full(n_features, mu)\n    else:\n        mu = np.asarray(mu).flatten()\n    \n    # Process V into a matrix\n    if V is None:\n        V_mat = np.eye(n_features)\n    elif np.isscalar(V):\n        V_mat = V * np.eye(n_features)\n    else:\n        V_arr = np.asarray(V)\n        if V_arr.ndim == 1:\n            V_mat = np.diag(V_arr)\n        else:\n            V_mat = V_arr\n    \n    # Compute V inverse\n    V_inv = np.linalg.inv(V_mat)\n    \n    # Compute X^T X and X^T y\n    X_T = X.T\n    X_T_X = X_T @ X\n    X_T_y = X_T @ y\n    \n    # Compute posterior covariance and mean\n    term = V_inv + X_T_X\n    Sigma_p = np.linalg.inv(term)\n    mu_p = Sigma_p @ (V_inv @ mu + X_T_y)\n    \n    # Compute predictions for X_new\n    y_pred = X_new @ mu_p\n    \n    # Round to 4 decimal places and convert to list\n    return [round(float(val), 4) for val in y_pred]\n```\n\n### Explanation\n1. **Intercept Handling**: If `fit_intercept` is `True`, a column of ones is added to both the training data `X` and the new data `X_new` to account for the intercept term in the linear regression model.\n2. **Prior Mean (`mu`)**: The prior mean `mu` is converted to a vector of appropriate length. If `mu` is a scalar, it is replicated to form a vector with the same number of features as the input data.\n3. **Prior Covariance (`V`)**: The prior covariance matrix `V` is constructed based on its input type. It can be a scalar (forming a diagonal matrix), a 1-D array (forming a diagonal matrix), or a 2-D matrix.\n4. **Posterior Computation**: The posterior covariance matrix `Sigma_p` and mean `mu_p` are computed using the given formulas. This involves matrix inversion and multiplication to combine the prior information with the data.\n5. **Prediction**: The MAP predictions for `X_new` are computed using the posterior mean `mu_p`, and the results are rounded to four decimal places before being returned as a list.\n\nThis approach efficiently combines prior knowledge with observed data to make predictions, leveraging Bayesian inference to provide a robust solution."}
{"task_id": 420, "completion_id": 0, "completion": "To solve this problem, we need to generate two Boolean masks that split the rows of a given 2-D NumPy array based on a specified column and threshold value. These masks will be used to divide the data into two subsets for tree-based machine learning algorithms.\n\n### Approach\n1. **Check Column Validity**: First, verify if the provided column index is within the valid range of the array's columns. If the column index is out of bounds, return two empty lists.\n2. **Generate Masks**: If the column index is valid, compute the Boolean masks by comparing each element in the specified column with the threshold value. The left mask will include rows where the column value is strictly less than the threshold, and the right mask will include rows where the column value is greater than or equal to the threshold.\n3. **Convert to Lists**: Convert the resulting NumPy Boolean arrays to Python lists using the `tolist()` method to ensure compatibility with non-NumPy code.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_split_mask(X: np.ndarray, column: int, value: float) -> tuple[list[bool], list[bool]]:\n    \"\"\"Generate Boolean masks that split rows of *X* on a given threshold.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    column : int\n        Index of the column (feature) used for the split.\n    value : float\n        Threshold value for the split.\n\n    Returns\n    -------\n    tuple[list[bool], list[bool]]\n        A pair *(left_mask, right_mask)* where `left_mask[i]` is `True` iff\n        `X[i, column] < value` and `right_mask[i]` is `True` iff\n        `X[i, column] >= value`.\n    \"\"\"\n    if column < 0 or column >= X.shape[1]:\n        return ([], [])\n    left_mask = (X[:, column] < value).tolist()\n    right_mask = (X[:, column] >= value).tolist()\n    return (left_mask, right_mask)\n```\n\n### Explanation\n1. **Column Validity Check**: The function first checks if the column index is within the valid range of the array's columns. If not, it returns two empty lists.\n2. **Mask Generation**: Using NumPy's array slicing and comparison operations, the function efficiently generates Boolean arrays indicating which rows meet the split criteria.\n3. **Conversion to Lists**: The Boolean arrays are converted to Python lists to ensure the output is in a format that can be easily used without NumPy dependencies.\n\nThis approach ensures that the function is both efficient and compatible with non-NumPy code, adhering to the problem constraints and requirements."}
{"task_id": 421, "completion_id": 0, "completion": "To solve this problem, we need to implement the DBSCAN clustering algorithm from scratch. DBSCAN groups together points that are closely packed (core points) and marks points in low-density regions as noise. The algorithm works by iterating through each point, determining if it is a core point, and expanding clusters from core points to include all density-reachable points.\n\n### Approach\n1. **Initialization**: Start by initializing labels for all points as unvisited (-2) and a visited array to track processed points.\n2. **Iterate through Points**: For each point, if it is unvisited, mark it as visited and compute its neighbors within a given distance (eps).\n3. **Determine Core Points**: If a point has fewer neighbors than the minimum required (min_samples), label it as noise (-1). Otherwise, start a new cluster and expand it.\n4. **Expand Clusters**: Use a queue to process all neighbors of the core point. For each neighbor, if it is a core point, add its neighbors to the queue and continue expanding the cluster until all density-reachable points are included.\n5. **Labeling**: Assign the same cluster label to all points that are part of the same density-connected region.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\"\"\"\n    n_samples = data.shape[0]\n    labels = np.full(n_samples, -2, dtype=int)  # -2 indicates unvisited\n    visited = np.zeros(n_samples, dtype=bool)\n    cluster_id = 0\n    \n    for i in range(n_samples):\n        if visited[i]:\n            continue\n        visited[i] = True\n        \n        # Calculate distances from data[i] to all other points\n        distances = np.linalg.norm(data - data[i], axis=1)\n        neighbors = np.where(distances <= eps)[0].tolist()\n        \n        if len(neighbors) < min_samples:\n            labels[i] = -1\n        else:\n            cluster_id += 1\n            labels[i] = cluster_id\n            \n            # Expand the cluster using a queue approach\n            queue = neighbors.copy()\n            j = 0\n            while j < len(queue):\n                current_point = queue[j]\n                if not visited[current_point]:\n                    visited[current_point] = True\n                    current_distances = np.linalg.norm(data - data[current_point], axis=1)\n                    current_neighbors = np.where(current_distances <= eps)[0].tolist()\n                    if len(current_neighbors) >= min_samples:\n                        queue.extend(current_neighbors)\n                if labels[current_point] in (-2, -1):\n                    labels[current_point] = cluster_id\n                j += 1\n    \n    return labels.tolist()\n```\n\n### Explanation\n1. **Initialization**: The `labels` array is initialized to -2 (unvisited), and `visited` is a boolean array to track processed points.\n2. **Processing Points**: Each point is processed in the order they appear. If a point is unvisited, it is marked as visited.\n3. **Neighbor Calculation**: For each point, compute the Euclidean distances to all other points to determine neighbors within `eps`.\n4. **Core Point Check**: If a point has enough neighbors, it is a core point. A new cluster is started, and the point's neighbors are added to a queue.\n5. **Cluster Expansion**: The queue processes each neighbor. If a neighbor is a core point, its neighbors are added to the queue. All reachable points are labeled with the current cluster ID.\n6. **Noise Handling**: Points that do not meet the density requirement are labeled as noise (-1).\n\nThis approach ensures that clusters are formed correctly by expanding from core points and including all density-reachable points, while noise points are identified and labeled appropriately."}
{"task_id": 422, "completion_id": 0, "completion": "To solve this problem, we need to implement the SoftPlus activation function and its first two derivatives efficiently in Python. The implementation should handle various input types and ensure numerical stability to avoid overflow issues, especially for large values of x. The results must be rounded to four decimal places and returned as standard Python types.\n\n### Approach\n1. **Input Handling**: Convert the input `x` into a NumPy array to handle both scalar and array inputs uniformly. This allows us to use vectorized operations for efficient computation.\n2. **Numerical Stability**: Use numerically stable formulas to compute the SoftPlus function and its derivatives to prevent overflow and underflow issues:\n   - **SoftPlus Function**: For large positive values of x, compute `ln(1 + e^x)` using the identity `max(x, 0) + ln(1 + exp(-|x|))`, which avoids overflow.\n   - **First Derivative (Sigmoid)**: Compute the sigmoid function using `1 / (1 + exp(-x))` for numerical stability.\n   - **Second Derivative**: Compute the derivative of the sigmoid function using the product of the sigmoid and its complement, `sigmoid(x) * (1 - sigmoid(x))`.\n3. **Order Check**: Return -1 for any order not in {0, 1, 2}.\n4. **Rounding and Conversion**: Round the results to four decimal places and convert the result to standard Python types using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(x: \"np.ndarray | list | tuple | float | int\", order: int = 0):\n    \"\"\"SoftPlus activation function and its derivatives.\n\n    Parameters\n    ----------\n    x : float | int | list | tuple | np.ndarray\n        Input data on which SoftPlus or its derivatives will be computed.\n    order : int, optional\n        0  -> SoftPlus(x)\n        1  -> SoftPlus'(x)\n        2  -> SoftPlus''(x)\n        Any other value should make the function return -1.\n\n    Returns\n    -------\n    float | list\n        Result rounded to 4 decimal places. The shape mirrors the input. If the\n        order is invalid, returns -1.\n    \"\"\"\n    if order not in {0, 1, 2}:\n        return -1\n    \n    x_arr = np.asarray(x)\n    \n    if order == 0:\n        # Compute SoftPlus(x) using stable formula\n        result = np.maximum(x_arr, 0) + np.log1p(np.exp(-np.abs(x_arr)))\n    elif order == 1:\n        # First derivative: sigmoid function\n        result = 1.0 / (1.0 + np.exp(-x_arr))\n    else:\n        # Second derivative: sigmoid * (1 - sigmoid)\n        sig = 1.0 / (1.0 + np.exp(-x_arr))\n        result = sig * (1.0 - sig)\n    \n    # Round and convert to Python types\n    return np.round(result, 4).tolist()\n```\n\n### Explanation\n- **Input Conversion**: The input is converted to a NumPy array to handle both scalar and array inputs uniformly.\n- **Numerical Stability**: The SoftPlus function is computed using a numerically stable formula to avoid overflow for large positive values. The sigmoid function and its derivative are computed using stable exponential operations.\n- **Order Handling**: The function checks the order and computes the appropriate result. For invalid orders, it returns -1.\n- **Rounding and Conversion**: The results are rounded to four decimal places and converted to standard Python types using `tolist()`, ensuring the output shape mirrors the input shape.\n\nThis approach ensures that the function is efficient, handles edge cases gracefully, and maintains numerical stability across a wide range of input values."}
{"task_id": 423, "completion_id": 0, "completion": "To solve this problem, we need to compute the Root Mean Squared Logarithmic Error (RMSLE) between two sequences of non-negative numbers. RMSLE is a metric used to evaluate regression models, emphasizing the ratio of predicted to actual values rather than their absolute differences.\n\n### Approach\n1. **Input Validation**: Ensure both input sequences are of the same length and contain only non-negative numbers. If any of these conditions are not met, return -1.\n2. **Logarithmic Transformation**: Compute the natural logarithm of each element incremented by 1 for both actual and predicted values. This helps in handling zero values gracefully and emphasizes relative differences.\n3. **Difference Calculation**: Calculate the difference between the logarithmic values of predicted and actual values.\n4. **Squaring and Averaging**: Square each of these differences, compute their mean, and then take the square root of the mean to get the RMSLE.\n5. **Rounding**: Round the result to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    # Validate input lengths\n    if len(actual) != len(predicted):\n        return -1\n    \n    # Check all elements are non-negative\n    if not (all(x >= 0 for x in actual) and all(x >= 0 for x in predicted)):\n        return -1\n    \n    # Convert lists to numpy arrays for vectorized operations\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    \n    # Compute log differences\n    log_diff = np.log1p(predicted_np) - np.log1p(actual_np)\n    \n    # Square the differences, compute mean, then take square root\n    squared_diff = np.square(log_diff)\n    mean_squared_diff = np.mean(squared_diff)\n    rmsle = np.sqrt(mean_squared_diff)\n    \n    # Round to four decimal places\n    return round(rmsle, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of the input lists are equal. If not, it returns -1. It then checks if all elements in both lists are non-negative. Any negative value in either list results in returning -1.\n2. **Logarithmic Transformation**: Using `np.log1p`, we compute the natural logarithm of each element after adding 1 to it. This handles zero values without causing domain errors.\n3. **Difference Calculation**: The difference between the log-transformed predicted and actual values is computed element-wise.\n4. **Squaring and Averaging**: The differences are squared, averaged, and then the square root of the average is taken to compute the RMSLE.\n5. **Rounding**: The result is rounded to four decimal places using Python's `round` function to meet the specified precision requirement.\n\nThis approach ensures that the function efficiently handles valid inputs and computes the RMSLE accurately while adhering to the problem constraints."}
{"task_id": 424, "completion_id": 0, "completion": "To solve this problem, we need to compute the Shannon entropy of a label sequence, which measures the uncertainty or impurity of the sequence. The entropy is calculated using the probabilities of each distinct label in the sequence.\n\n### Approach\n1. **Calculate Frequencies**: Use numpy's `unique` function to determine the frequency of each distinct label in the input list.\n2. **Compute Probabilities**: Convert these frequencies into probabilities by dividing each frequency by the total number of elements in the input list.\n3. **Filter Zero Probabilities**: Exclude any zero probabilities to avoid undefined logarithmic operations.\n4. **Compute Entropy**: Calculate the entropy using the formula \\( H = -\\sum_{i=1}^{k} p_i \\log_2 p_i \\), where \\( p_i \\) are the non-zero probabilities.\n5. **Round Result**: Round the computed entropy to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef entropy(y: list[int]) -> float:\n    \"\"\"Compute the Shannon entropy of a label sequence.\n\n    Args:\n        y: A non-empty list of non-negative integers representing class labels.\n\n    Returns:\n        The Shannon entropy of the sequence, rounded to 4 decimal places.\n    \"\"\"\n    counts = np.unique(y, return_counts=True)[1]\n    probs = counts / len(y)\n    non_zero_probs = probs[probs > 0]\n    h = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n    return round(h, 4)\n```\n\n### Explanation\n1. **Calculate Frequencies**: The `np.unique` function with `return_counts=True` returns the counts of each unique label in the input list.\n2. **Compute Probabilities**: By dividing each count by the total number of elements, we obtain the relative frequency (probability) of each label.\n3. **Filter Zero Probabilities**: This step ensures we do not take the logarithm of zero, which is undefined. Only non-zero probabilities are considered.\n4. **Compute Entropy**: Using vectorized operations, we compute the sum of each probability multiplied by its logarithm base 2, then take the negative of this sum to get the entropy.\n5. **Round Result**: The result is rounded to four decimal places using Python's built-in `round` function to meet the specified precision requirement.\n\nThis approach efficiently handles the calculation using numpy's optimized functions, ensuring both correctness and performance."}
{"task_id": 425, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Shannon entropy of a one-dimensional collection of non-negative integers using the natural logarithm. The Shannon entropy is a measure of the uncertainty or randomness in the data. The steps involve converting the input data into a probability distribution and then applying the entropy formula.\n\n### Approach\n1. **Convert Input to Probability Distribution**: Use the input data to determine the frequency of each unique value. This is efficiently done using the `numpy.unique` function which returns both the unique values and their counts.\n2. **Compute Probabilities**: Convert the counts into probabilities by dividing each count by the total number of elements in the input data.\n3. **Calculate Entropy**: Apply the Shannon entropy formula, which involves summing the product of each probability and the natural logarithm of that probability, then taking the negative of this sum.\n4. **Edge Case Handling**: If all elements in the input are the same, the entropy is defined as 0.0. This is naturally handled by the entropy formula but can be checked explicitly for clarity.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef f_entropy(p: list[int] | np.ndarray) -> float:\n    values, counts = np.unique(p, return_counts=True)\n    if len(values) == 1:\n        return 0.0\n    probs = counts / len(p)\n    entropy = -np.sum(probs * np.log(probs))\n    return round(entropy, 4)\n```\n\n### Explanation\n1. **Unique Values and Counts**: The `numpy.unique` function with `return_counts=True` gives both the unique values in the input and how many times each value appears.\n2. **Probability Calculation**: Each count is divided by the total number of elements to get the probability distribution of the data.\n3. **Entropy Calculation**: The entropy is computed using the formula \\(-\\sum p_i \\ln p_i\\). The `numpy.log` function is used here with base \\(e\\) (natural logarithm).\n4. **Edge Case Handling**: If there is only one unique value in the input, the entropy is immediately returned as 0.0, as all probabilities are 1, leading to \\(\\ln(1) = 0\\).\n\nThis approach efficiently handles both lists and NumPy arrays, ensuring accurate and efficient computation of the Shannon entropy. The result is rounded to four decimal places to meet the specified precision requirement."}
{"task_id": 426, "completion_id": 0, "completion": "To solve this problem, we need to compute the accuracy score of a classification model by comparing the true labels with the predicted labels. The accuracy score is defined as the fraction of correct predictions out of the total predictions. The solution must handle edge cases such as empty input lists or lists of different lengths appropriately.\n\n### Approach\n1. **Input Validation**: Check if either of the input lists is empty or if their lengths differ. If any of these conditions are met, return -1.0 as specified.\n2. **Count Correct Predictions**: Iterate through the pairs of true and predicted labels, counting how many times they match.\n3. **Calculate Accuracy**: Divide the count of correct predictions by the total number of predictions to get the accuracy.\n4. **Round the Result**: Round the computed accuracy to four decimal places and return it as a float.\n\n### Solution Code\n```python\ndef classification_accuracy(y_true: list, y_pred: list) -> float:\n    \"\"\"Compute the accuracy between the true and predicted class labels.\n\n    Args:\n        y_true (list): Ground-truth class labels.\n        y_pred (list): Predicted class labels produced by a model.\n\n    Returns:\n        float: The accuracy score rounded to 4 decimal places.\n              Returns -1 when the inputs have different lengths or when\n              either of the inputs is empty.\n    \"\"\"\n    if len(y_true) == 0 or len(y_pred) == 0:\n        return -1.0\n    if len(y_true) != len(y_pred):\n        return -1.0\n    correct = 0\n    for true, pred in zip(y_true, y_pred):\n        if true == pred:\n            correct += 1\n    accuracy = correct / len(y_true)\n    return round(accuracy, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if either `y_true` or `y_pred` is empty. If so, it returns -1.0. Next, it checks if the lengths of the two lists are different, returning -1.0 in that case as well.\n2. **Counting Correct Predictions**: Using a loop, the function iterates through each pair of corresponding elements from `y_true` and `y_pred`. Each time the elements match, a counter is incremented.\n3. **Calculating Accuracy**: The number of correct predictions is divided by the total number of predictions (the length of either list, as they are validated to be the same and non-empty).\n4. **Rounding**: The resulting accuracy is rounded to four decimal places using Python's `round` function to ensure the result meets the specified precision requirement.\n\nThis approach ensures that the function handles all edge cases efficiently and computes the accuracy score correctly while adhering to the problem constraints."}
{"task_id": 427, "completion_id": 0, "completion": "To solve this problem, we need to compute the identity activation function and its derivative for a given input, which can be a 1-D or n-D numeric array in the form of a Python list, tuple, or NumPy ndarray. The identity activation function simply returns the input unchanged, and its derivative is an array of ones with the same shape as the input. The results must be returned as Python lists.\n\n### Approach\n1. **Input Conversion**: Convert the input to a NumPy array if it is a Python list or tuple. This allows us to leverage NumPy's vectorized operations for efficient computation.\n2. **Dimension Handling**: Ensure the input is at least 1-D using `np.atleast_1d` to handle any scalar inputs correctly, converting them to 1-D arrays.\n3. **Activation and Derivative Computation**: Compute the identity activation (which is the input itself) and its derivative (an array of ones with the same shape as the input).\n4. **Conversion to Python Lists**: Convert the results from NumPy arrays back to Python lists using the `tolist()` method to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef identity_activation(z):\n    \"\"\"Compute identity activation and its derivative.\n\n    Args:\n        z (list | tuple | numpy.ndarray): Input data of any numeric type and\n            shape.\n\n    Returns:\n        tuple[list, list]: A tuple (activation, derivative) where both elements\n            are Python lists matching the shape of the input.\n    \"\"\"\n    if isinstance(z, (list, tuple)):\n        arr = np.array(z)\n    else:\n        arr = z\n    arr = np.atleast_1d(arr)\n    activation = arr\n    derivative = np.ones_like(arr)\n    return (activation.tolist(), derivative.tolist())\n```\n\n### Explanation\n1. **Input Conversion**: The input is checked if it is a list or tuple and converted to a NumPy array if necessary. This ensures uniform handling of the input data.\n2. **Dimension Handling**: Using `np.atleast_1d` ensures that even scalar inputs (0-D arrays) are treated as 1-D arrays, which is necessary for consistent list conversion.\n3. **Activation Computation**: The identity activation is straightforward as it returns the input array unchanged.\n4. **Derivative Computation**: The derivative of the identity function is an array of ones with the same shape as the input, computed using `np.ones_like`.\n5. **Conversion to Lists**: The results are converted back to Python lists using `tolist()` to ensure the output format meets the specified requirements.\n\nThis approach efficiently handles various input types and dimensions, ensuring correct and consistent results for both activation and derivative computations."}
{"task_id": 428, "completion_id": 0, "completion": "To solve this problem, we need to generate a Hann window (also known as the Hanning window) of a specified length. The Hann window is used in digital signal processing to reduce spectral leakage by ensuring the endpoints smoothly reach zero. The window can be generated in two modes: symmetric and periodic, depending on the use case.\n\n### Approach\n1. **Special Cases Handling**: \n   - If the window length is less than or equal to 0, return an empty list.\n   - If the window length is exactly 1, return a list containing a single element [1.0].\n\n2. **Determine Window Length**:\n   - For a symmetric window, use the specified window length directly.\n   - For a periodic window, generate a symmetric window of length `window_len + 1` and then discard the last sample to ensure periodicity.\n\n3. **Generate the Hann Window**:\n   - Use the mathematical formula for the Hann window to compute each sample.\n   - For each sample index `n`, the value is calculated using the formula: \n     \\[\n     \\text{hann}(n) = 0.5 - 0.5 \\times \\cos\\left(\\frac{2 \\pi n}{N-1}\\right)\n     \\]\n     where \\(N\\) is the determined window length.\n\n4. **Adjust for Periodicity**:\n   - If generating a periodic window, truncate the last sample of the symmetric window.\n\n5. **Rounding and Conversion**:\n   - Round each value to 4 decimal places.\n   - Convert the resulting NumPy array to a Python list and return it.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    N = window_len if symmetric else window_len + 1\n    samples = np.arange(N)\n    hann_values = 0.5 - 0.5 * np.cos(2 * math.pi * samples / (N - 1))\n    \n    if not symmetric:\n        hann_values = hann_values[:window_len]\n    \n    hann_rounded = np.round(hann_values, 4)\n    return hann_rounded.tolist()\n```\n\n### Explanation\n- **Special Cases**: The function first checks for invalid or trivial inputs (window length \u2264 0 or exactly 1) and handles them directly.\n- **Window Length Adjustment**: Depending on whether a symmetric or periodic window is needed, the effective window length is adjusted. For periodic windows, the length is increased by 1 before generation and then truncated.\n- **Mathematical Calculation**: The Hann window values are computed using vectorized operations for efficiency, leveraging NumPy's array operations.\n- **Truncation and Rounding**: The periodic window is truncated to the desired length, and all values are rounded to 4 decimal places for precision, then converted to a Python list for the final output.\n\nThis approach ensures that the generated Hann window meets the requirements for both symmetric and periodic use cases efficiently and accurately."}
{"task_id": 429, "completion_id": 0, "completion": "To solve this problem, we need to determine the optimal arm and its corresponding reward for a given context in a contextual Bernoulli bandit setting. The solution involves validating the input context and then computing the optimal reward and arm based on the given probabilities.\n\n### Approach\n1. **Validate Context**: Check if the provided context is a valid one-hot vector. A valid one-hot vector must be one-dimensional, have the same length as the number of contexts (rows) in the probability matrix, contain only 0s and 1s, and exactly one 1.\n2. **Determine Active Context**: Identify the active context by finding the index of the 1 in the one-hot vector.\n3. **Compute Optimal Arm and Reward**: For the identified context, find the arm with the highest success probability. If multiple arms have the same highest probability, select the arm with the smallest index. Round the highest probability to four decimal places to get the optimal reward.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef oracle_payoff(context_probs: np.ndarray, context: np.ndarray) -> tuple[float, int]:\n    \"\"\"Oracle that returns the optimal expected reward and the corresponding arm.\n\n    Parameters\n    ----------\n    context_probs : np.ndarray\n        A 2-D array of shape (D, K) where each row represents a context and each\n        column an arm.  Entry (i, j) is the success probability of arm *j* when\n        the active context is *i*.\n    context : np.ndarray\n        A one-hot vector of length *D* identifying the current context.\n\n    Returns\n    -------\n    tuple[float, int]\n        (optimal_rwd, optimal_arm): the best achievable expected reward (rounded\n        to four decimals) and the index of the arm that achieves it.  If the\n        input context is invalid the function returns (-1, \u20111).\n    \"\"\"\n    # Check if context is a valid one-hot vector\n    if context.ndim != 1:\n        return (-1, -1)\n    D = context_probs.shape[0]\n    if context.size != D:\n        return (-1, -1)\n    if not np.isin(context, [0, 1]).all():\n        return (-1, -1)\n    if np.sum(context) != 1:\n        return (-1, -1)\n    \n    # Find the active context\n    active_context_idx = np.argmax(context)\n    \n    # Get the probabilities for the active context\n    probs = context_probs[active_context_idx, :]\n    \n    # Find the optimal arm and reward\n    max_prob = np.max(probs)\n    optimal_arm = np.where(probs == max_prob)[0][0]\n    optimal_rwd = np.round(max_prob, 4)\n    \n    return (optimal_rwd, optimal_arm)\n```\n\n### Explanation\n1. **Validation Checks**: The context is checked to ensure it is a valid one-hot vector. This includes checking the dimensionality, length, presence of only 0s and 1s, and exactly one 1.\n2. **Active Context Identification**: The index of the 1 in the context vector is found using `np.argmax`, which efficiently locates the position of the maximum value (which should be 1 in a valid context).\n3. **Optimal Arm and Reward Calculation**: For the identified context, the probabilities of each arm are extracted. The maximum probability is found, and the smallest index of the arm with this maximum probability is determined. The maximum probability is then rounded to four decimal places to provide the optimal reward.\n\nThis approach ensures that we efficiently and correctly determine the optimal arm and its reward while handling invalid inputs appropriately."}
{"task_id": 430, "completion_id": 0, "completion": "To solve this problem, we need to determine a topological ordering of a directed graph if it is a Directed Acyclic Graph (DAG) or detect if it contains a cycle. The solution must be deterministic, ensuring that the order of processing nodes and their neighbors follows the given vertices list strictly.\n\n### Approach\n1. **Adjacency List Construction**: Build an adjacency list where each node's neighbors are ordered based on their positions in the given vertices list. This ensures deterministic processing of neighbors.\n2. **Depth-First Search (DFS) with Cycle Detection**: Use DFS to explore the graph. Track visited nodes and the current recursion stack to detect cycles. If a cycle is detected, return `None`.\n3. **Topological Order Collection**: Nodes are added to the result list in reverse order of their finishing times during DFS. The final topological order is obtained by reversing this result list.\n\n### Solution Code\n```python\nfrom typing import List, Dict, Set, Tuple\n\ndef topological_sort(vertices: list, edges: list[tuple]) -> list | None:\n    \"\"\"Determine a topological ordering of a directed graph using deterministic DFS.\"\"\"\n    # Build adjacency list with neighbors ordered based on vertices' order\n    pos = {v: idx for idx, v in enumerate(vertices)}\n    adj: Dict = {u: [] for u in vertices}\n    for src, dest in edges:\n        adj[src].append(dest)\n    # Sort each adjacency list according to the vertices' order\n    for u in adj:\n        adj[u].sort(key=lambda x: pos[x])\n    \n    visited: Set = set()\n    recursion_stack: Set = set()\n    result: List = []\n    \n    def dfs(u) -> bool:\n        \"\"\"Returns False if a cycle is detected, True otherwise.\"\"\"\n        if u in recursion_stack:\n            return False\n        if u in visited:\n            return True\n        visited.add(u)\n        recursion_stack.add(u)\n        for v in adj[u]:\n            if not dfs(v):\n                return False\n        recursion_stack.remove(u)\n        result.append(u)\n        return True\n    \n    for u in vertices:\n        if u not in visited:\n            if not dfs(u):\n                return None\n    \n    # Reverse to get the correct topological order\n    return result[::-1]\n```\n\n### Explanation\n1. **Adjacency List Construction**: The adjacency list is built by iterating through the edges and sorting each node's neighbors based on their positions in the vertices list. This ensures that neighbors are processed in the deterministic order required.\n2. **DFS with Cycle Detection**: The DFS function keeps track of visited nodes and the current recursion stack. If a node is revisited while still in the recursion stack, a cycle is detected, and the function returns `None`.\n3. **Topological Order Collection**: Nodes are added to the result list once all their descendants have been processed. The final result is the reverse of this list, giving the correct topological order.\n\nThis approach ensures that the topological sort is both correct and deterministic, adhering strictly to the specified order of vertices and their neighbors."}
{"task_id": 431, "completion_id": 0, "completion": "To solve this problem, we need to compute the Euclidean (L2) distance between two numeric vectors. The function should handle vectors provided as Python lists, tuples, or 1-D NumPy arrays, validate their lengths, compute the distance, and return the result rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: Convert the input vectors to NumPy arrays to handle element-wise operations uniformly, regardless of the input type (list, tuple, or array).\n2. **Length Validation**: Check if the lengths of the two vectors are the same. If not, return -1.\n3. **Distance Calculation**: Compute the Euclidean distance using the formula \\(\\sqrt{\\sum_{i=1}^{n}(x_{1i}-x_{2i})^{2}}\\). This involves element-wise subtraction, squaring the differences, summing the squares, and taking the square root.\n4. **Rounding**: Round the computed distance to four decimal places and return it as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dist(x1, x2):\n    \"\"\"Compute the Euclidean distance between two vectors.\n\n    The vectors may be supplied as Python lists, tuples, or NumPy arrays. If the\n    vectors are of unequal length, the function must return -1.\n\n    Parameters\n    ----------\n    x1 : list | tuple | numpy.ndarray\n        First 1-D vector containing numeric (int/float) values.\n    x2 : list | tuple | numpy.ndarray\n        Second 1-D vector containing numeric (int/float) values.\n\n    Returns\n    -------\n    float\n        Euclidean distance rounded to 4 decimal places, or -1 if the input\n        vectors are of different lengths.\n    \"\"\"\n    x1 = np.asarray(x1)\n    x2 = np.asarray(x2)\n    \n    if len(x1) != len(x2):\n        return -1\n    \n    squared_diff = (x1 - x2) ** 2\n    sum_squared = np.sum(squared_diff)\n    distance = np.sqrt(sum_squared)\n    \n    return round(distance, 4)\n```\n\n### Explanation\n1. **Input Conversion**: The vectors are converted to NumPy arrays using `np.asarray()`, ensuring consistent handling of element-wise operations.\n2. **Length Check**: The lengths of the converted arrays are checked. If they differ, the function returns -1 immediately.\n3. **Distance Calculation**: The difference between the vectors is computed, squared, summed, and the square root of the sum gives the Euclidean distance.\n4. **Rounding**: The result is rounded to four decimal places using Python's `round()` function to meet the specified precision requirement.\n\nThis approach efficiently handles different input types, ensures correct validation, and accurately computes the Euclidean distance with the required precision."}
{"task_id": 433, "completion_id": 0, "completion": "To solve this problem, we need to compute the fan-in and fan-out values for a given weight-tensor shape according to specific rules. These values are essential for initializing neural network parameters using schemes like Xavier/Glorot. The solution involves distinguishing between 2D convolutional layers and other layer types based on the shape of the weight tensor.\n\n### Approach\n1. **Identify Convolutional Layer**: If the input shape has exactly 4 dimensions, it is treated as a 2D convolutional kernel. The layout for such a kernel is `(out_channels, in_channels, kernel_height, kernel_width)`.\n   - Compute the receptive field size as the product of the last two dimensions (kernel height and width).\n   - Calculate fan-in as `in_channels * receptive_field_size`.\n   - Calculate fan-out as `out_channels * receptive_field_size`.\n\n2. **Handle Other Layers**: For any other layer type (e.g., dense layers, embedding matrices), directly use the first two dimensions of the shape for fan-in and fan-out respectively.\n\n### Solution Code\n```python\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    if len(shape) == 4:\n        out_channels, in_channels, kernel_height, kernel_width = shape\n        receptive_field_size = kernel_height * kernel_width\n        fan_in = in_channels * receptive_field_size\n        fan_out = out_channels * receptive_field_size\n    else:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    return (float(fan_in), float(fan_out))\n```\n\n### Explanation\n- **Convolutional Layer Handling**: When the input shape has 4 dimensions, the code extracts `out_channels`, `in_channels`, `kernel_height`, and `kernel_width`. The receptive field size is computed as the product of `kernel_height` and `kernel_width`. Fan-in and fan-out are then calculated using the respective channels multiplied by the receptive field size.\n- **Other Layers Handling**: For any other input shape, the first two elements are directly used as fan-in and fan-out. This applies to dense layers, embedding matrices, and higher-dimensional tensors where the first two dimensions represent input and output features respectively.\n\nThis approach ensures that the correct fan-in and fan-out values are computed efficiently based on the given shape, adhering to the rules specified for different types of neural network layers."}
{"task_id": 434, "completion_id": 0, "completion": "To solve this problem, we need to implement a numerically stable softmax function that converts logits into a probability distribution while avoiding numerical instability issues such as overflow or underflow. The solution should handle both 1-D and 2-D input arrays, returning a list of lists with probabilities rounded to six decimal places.\n\n### Approach\n1. **Input Handling**: Convert 1-D input arrays to 2-D arrays to ensure uniform processing.\n2. **Numerical Stability**: Subtract the maximum value of each row from all elements in that row before exponentiation to prevent overflow/underflow.\n3. **Exponentiation and Summation**: Compute exponentials of the adjusted values and sum them row-wise.\n4. **Probability Calculation**: Divide each exponentiated value by the row sum to get probabilities.\n5. **Rounding and Conversion**: Round each probability to six decimal places and convert the result to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softmax(logits: np.ndarray) -> list[list[float]]:\n    \"\"\"Compute a row-wise numerically stable softmax.\"\"\"\n    # Handle 1-D case by reshaping to (1, n)\n    if logits.ndim == 1:\n        logits = logits.reshape(1, -1)\n    \n    # Compute max along each row, keeping dimensions for broadcasting\n    max_vals = np.max(logits, axis=1, keepdims=True)\n    \n    # Subtract max for numerical stability\n    shifted = logits - max_vals\n    \n    # Exponentiate the shifted logits\n    exps = np.exp(shifted)\n    \n    # Sum of exponentials per row\n    sums = np.sum(exps, axis=1, keepdims=True)\n    \n    # Compute probabilities\n    probabilities = exps / sums\n    \n    # Round to 6 decimal places\n    rounded = np.round(probabilities, decimals=6)\n    \n    # Convert to a list of lists\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input is a 1-D array and reshapes it to a 2-D array with a single row to ensure consistent processing.\n2. **Max Subtraction**: For each row, the maximum value is computed and subtracted from all elements in that row. This step ensures that the largest value in each row is zero, preventing overflow during exponentiation.\n3. **Exponentiation**: Each element in the adjusted matrix is exponentiated. Since the maximum adjusted value is zero, the largest exponentiated value is 1, avoiding overflow.\n4. **Summation and Normalization**: The exponentiated values are summed for each row, and each value is divided by this sum to obtain probabilities.\n5. **Rounding and Conversion**: The probabilities are rounded to six decimal places and converted to a list of lists to meet the output requirements.\n\nThis approach efficiently handles numerical stability and ensures the output is in the correct format, making it suitable for use in deep learning models where converting logits to probabilities is a common requirement."}
{"task_id": 435, "completion_id": 0, "completion": "To solve this problem, we need to implement a simplified version of a Gradient Boosting Regressor using ordinary least-squares (OLS) linear regression as the weak learner and the squared error as the loss function. The algorithm iteratively improves predictions by fitting residual errors from previous iterations.\n\n### Approach\n1. **Convert Inputs**: Convert the input features `X` and target `y` into NumPy arrays for efficient computation.\n2. **Handle Special Cases**: If `n_estimators` is non-positive or `learning_rate` is zero, return predictions as the mean of `y` for all samples.\n3. **Initialize Predictions**: Start with initial predictions set to the mean of `y`.\n4. **Iterative Boosting**: For each boosting iteration:\n   - Compute residuals as the difference between actual values and current predictions.\n   - Fit an OLS regression model on the residuals, including an intercept term.\n   - Update predictions using the weak learner's predictions scaled by the learning rate.\n5. **Final Output**: Round the final predictions to four decimal places and convert them to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    X = np.array(X, dtype=np.float64)\n    y = np.array(y, dtype=np.float64)\n    m = X.shape[0]\n    mean_val = y.mean()\n    if n_estimators <= 0 or learning_rate == 0:\n        return [round(mean_val, 4)] * m\n    y_hat = np.full(m, mean_val, dtype=np.float64)\n    for _ in range(n_estimators):\n        r = y - y_hat\n        X_aug = np.hstack([np.ones((m, 1)), X])\n        theta = np.linalg.lstsq(X_aug, r, rcond=None)[0]\n        h = X_aug @ theta\n        y_hat += learning_rate * h\n    y_hat_rounded = np.round(y_hat, 4)\n    return y_hat_rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The input features `X` and target `y` are converted to NumPy arrays to leverage efficient numerical operations.\n2. **Special Case Handling**: If the number of estimators is non-positive or the learning rate is zero, the model returns the mean of the target values for all predictions.\n3. **Initialization**: The initial prediction for each sample is set to the mean of the target values.\n4. **Boosting Loop**: For each iteration:\n   - **Residual Calculation**: Compute the residuals as the difference between the actual target values and the current predictions.\n   - **OLS Model Fitting**: Fit an OLS regression model on the residuals. This involves augmenting the feature matrix with a column of ones to account for the intercept term.\n   - **Prediction Update**: Update the predictions using the weak learner's predictions scaled by the learning rate. This step gradually improves the model by correcting the residuals from previous iterations.\n5. **Final Predictions**: The final predictions are rounded to four decimal places and converted to a list to meet the output requirements.\n\nThis approach ensures that the model iteratively improves its predictions by focusing on the errors made in previous iterations, leading to a robust and accurate regressor even with a small number of estimators."}
{"task_id": 437, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that evaluates the logistic sigmoid activation function and its first or second derivatives based on the specified order. The function should handle scalar, list, and NumPy array inputs, returning the result in the appropriate type (float for scalar inputs and list for others) rounded to four decimal places. If an invalid order is specified, the function should return -1.\n\n### Approach\n1. **Input Handling**: The function first checks if the input order is valid (0, 1, or 2). If not, it returns -1 immediately.\n2. **Conversion to NumPy Array**: Convert the input to a NumPy array to handle both scalar and non-scalar inputs uniformly.\n3. **Sigmoid Calculation**: Compute the sigmoid function using vectorized operations for efficiency.\n4. **Derivative Calculation**: Depending on the order, compute the first or second derivative using the sigmoid values.\n5. **Rounding**: Round the results to four decimal places.\n6. **Return Type Handling**: Convert the result back to a float if the input was a scalar, otherwise return a list using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in {0, 1, 2}:\n        return -1\n    \n    original_is_scalar = np.isscalar(x)\n    x_arr = np.asarray(x)\n    s = 1 / (1 + np.exp(-x_arr))\n    \n    if order == 0:\n        res = s\n    elif order == 1:\n        res = s * (1 - s)\n    else:  # order == 2\n        res = s * (1 - s) * (1 - 2 * s)\n    \n    res_rounded = np.round(res, 4)\n    \n    if original_is_scalar:\n        return float(res_rounded)\n    else:\n        return res_rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function checks if the order is 0, 1, or 2. If not, it returns -1 immediately.\n2. **Conversion to Array**: The input is converted to a NumPy array to handle both scalar and non-scalar inputs uniformly.\n3. **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n4. **Derivative Calculation**: \n   - First derivative is calculated using \\( \\sigma(x)(1 - \\sigma(x)) \\).\n   - Second derivative uses \\( \\sigma(x)(1 - \\sigma(x))(1 - 2\\sigma(x)) \\).\n5. **Rounding**: Results are rounded to four decimal places using NumPy's `round` function.\n6. **Return Type**: The result is returned as a float for scalar inputs and as a list for non-scalar inputs using `tolist()` to preserve the original structure.\n\nThis approach ensures efficient computation using vectorized operations and handles different input types appropriately, providing the correct return type and structure."}
{"task_id": 438, "completion_id": 0, "completion": "To solve this problem, we need to convert a vector of class labels into a one-hot encoded matrix. The function should handle both list and NumPy array inputs and allow for an optional number of classes. If the number of classes is not provided, it should be inferred from the data. If the provided number of classes is insufficient, the function should return -1.\n\n### Approach\n1. **Input Handling**: Convert the input to a NumPy array if it's a list to ensure consistent processing.\n2. **Edge Case Handling**: If the input is an empty list, return an empty list.\n3. **Determine Number of Classes**: If the number of classes (`num_classes`) is not provided, calculate it as the maximum label in the input plus one. If `num_classes` is provided but is smaller than the required number of classes (determined by the maximum label plus one), return -1.\n4. **Matrix Creation**: Create a zero matrix of appropriate dimensions and set the corresponding positions to 1 based on the input labels.\n5. **Conversion to List**: Convert the resulting NumPy matrix to a list of lists before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    y_np = np.asarray(y)\n    if y_np.size == 0:\n        return []\n    \n    max_label = y_np.max()\n    \n    if num_classes is not None:\n        if num_classes < max_label + 1:\n            return -1\n        num_classes_ = num_classes\n    else:\n        num_classes_ = max_label + 1\n    \n    n_samples = y_np.shape[0]\n    matrix = np.zeros((n_samples, num_classes_), dtype=int)\n    rows = np.arange(n_samples)\n    matrix[rows, y_np] = 1\n    \n    return matrix.tolist()\n```\n\n### Explanation\n1. **Input Conversion**: The input `y` is converted to a NumPy array for uniform processing, whether it's a list or a NumPy array.\n2. **Empty Input Handling**: If the input is empty, an empty list is returned immediately.\n3. **Class Number Determination**: The maximum label in the input is found. If `num_classes` is not provided, it is set to this maximum label plus one. If `num_classes` is provided but too small, the function returns -1.\n4. **Matrix Construction**: A zero matrix is created with dimensions based on the number of samples and the determined number of classes. The appropriate positions in the matrix are set to 1 using vectorized indexing for efficiency.\n5. **Conversion to List**: The resulting matrix is converted to a list of lists using the `tolist()` method to meet the output requirements.\n\nThis approach ensures efficient handling of both list and array inputs, correctly infers or validates the number of classes, and efficiently constructs the one-hot encoded matrix using NumPy operations."}
{"task_id": 439, "completion_id": 0, "completion": "To solve this problem, we need to implement the Apriori algorithm to discover all frequent itemsets in a transactional database. The Apriori algorithm follows a breadth-first strategy to efficiently find itemsets that meet a minimum support threshold. \n\n### Approach\n1. **Generate Frequent 1-Itemsets**: Start by identifying all single-item itemsets that meet the minimum support threshold.\n2. **Generate Candidates**: For each subsequent level (k-itemsets), generate candidate itemsets by self-joining the frequent itemsets from the previous level.\n3. **Prune Candidates**: Remove candidates that have any subset which is not frequent in the previous level.\n4. **Check Support**: Calculate the support for each candidate and retain those that meet the minimum support threshold.\n5. **Repeat**: Continue generating and pruning candidates until no new frequent itemsets are found.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\nimport itertools\n\ndef apriori_frequent_itemsets(transactions: list[list], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\"\"\"\n    if not transactions:\n        return []\n    \n    n_transactions = len(transactions)\n    required_count = math.ceil(min_sup * n_transactions - 1e-12)\n    transaction_sets = [frozenset(trans) for trans in transactions]\n    \n    # Generate frequent 1-itemsets\n    item_counts = defaultdict(int)\n    for trans in transactions:\n        unique_items = set(trans)\n        for item in unique_items:\n            item_counts[item] += 1\n    \n    frequent_items = []\n    frequent_1 = [tuple([item]) for item in sorted(item for item, cnt in item_counts.items() if cnt >= required_count)]\n    frequent_items.extend(frequent_1)\n    prev_frequent = frequent_1\n    \n    k = 2\n    while True:\n        # Generate candidates via self-join\n        candidates = []\n        prev_sorted = sorted(prev_frequent)\n        len_prev = len(prev_sorted)\n        for i in range(len_prev):\n            itemset_i = prev_sorted[i]\n            for j in range(i + 1, len_prev):\n                itemset_j = prev_sorted[j]\n                if itemset_i[:-1] == itemset_j[:-1]:\n                    if itemset_i[-1] < itemset_j[-1]:\n                        new_candidate = itemset_i + (itemset_j[-1],)\n                        candidates.append(new_candidate)\n                else:\n                    break  # Since sorted, no more matches\n        \n        # Prune candidates with infrequent subsets\n        prev_frequent_set = set(prev_sorted)\n        pruned_candidates = []\n        for candidate in candidates:\n            valid = True\n            for i in range(len(candidate)):\n                subset = candidate[:i] + candidate[i+1:]\n                if subset not in prev_frequent_set:\n                    valid = False\n                    break\n            if valid:\n                pruned_candidates.append(candidate)\n        \n        # Compute support for pruned candidates\n        current_frequent = []\n        for candidate in pruned_candidates:\n            count = 0\n            for t_set in transaction_sets:\n                all_in = True\n                for item in candidate:\n                    if item not in t_set:\n                        all_in = False\n                        break\n                if all_in:\n                    count += 1\n            if count >= required_count:\n                current_frequent.append(candidate)\n        \n        if not current_frequent:\n            break\n        frequent_items.extend(current_frequent)\n        prev_frequent = current_frequent\n        k += 1\n    \n    # Sort by length and lex order\n    frequent_items.sort(key=lambda x: (len(x), x))\n    return frequent_items\n```\n\n### Explanation\n1. **Preprocessing**: Convert each transaction to a set to efficiently check for item presence. Calculate the required support count based on the minimum support threshold and the number of transactions.\n2. **Frequent 1-Itemsets**: Count occurrences of each item across all transactions and filter those that meet the support threshold.\n3. **Candidate Generation**: For each level `k`, generate candidate itemsets by joining frequent itemsets from the previous level that share the first `k-2` items.\n4. **Pruning**: Remove candidates that have any subset which is not frequent in the previous level.\n5. **Support Calculation**: For each candidate, count how many transactions contain all items in the candidate and retain those that meet the support threshold.\n6. **Sorting**: The final list of frequent itemsets is sorted by the length of the itemsets and lexicographically within each length group.\n\nThis approach efficiently explores potential frequent itemsets using the Apriori principle, ensuring that only viable candidates are considered at each step, thus reducing the search space and computational complexity."}
{"task_id": 440, "completion_id": 0, "completion": "To solve this problem, we need to aggregate the probability predictions from an ensemble of estimators (such as a Random Forest) by averaging the probabilities for each class across all estimators, then determining the class with the highest average probability for each sample. The solution involves converting the input list to a NumPy array for efficient computation, averaging the probabilities, rounding them, and then determining the predicted class indices.\n\n### Approach\n1. **Convert Input to NumPy Array**: Convert the nested list of predictions into a NumPy array to leverage efficient array operations.\n2. **Average Probabilities**: Compute the mean of the probabilities across all estimators for each sample and class.\n3. **Round Probabilities**: Round the averaged probabilities to four decimal places.\n4. **Determine Predicted Classes**: Use the argmax function to find the class index with the highest probability for each sample, breaking ties by selecting the smallest index in case of equality.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    np_preds = np.array(predictions)\n    avg_probs = np.mean(np_preds, axis=0)\n    rounded_probs = np.round(avg_probs, 4)\n    predicted_classes = np.argmax(rounded_probs, axis=1)\n    return (rounded_probs.tolist(), predicted_classes.tolist())\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input list is converted into a NumPy array, which allows efficient numerical operations. The shape of the array is `(n_estimators, n_samples, n_classes)`.\n2. **Averaging Probabilities**: Using `np.mean` with `axis=0`, we average the probabilities across all estimators, resulting in an array of shape `(n_samples, n_classes)`.\n3. **Rounding**: The averaged probabilities are rounded to four decimal places using `np.round`, ensuring each value is formatted correctly.\n4. **Argmax for Class Indices**: The `np.argmax` function is applied along the class axis (axis=1) to find the index of the highest probability for each sample. This function inherently handles ties by selecting the first occurrence of the maximum value, which aligns with the problem's requirements.\n\nThis approach efficiently handles the aggregation and computation using NumPy, ensuring both correctness and performance. The final results are converted back to Python lists to meet the output constraints."}
{"task_id": 441, "completion_id": 0, "completion": "To solve this problem, we need to generate a Hamming window of a specified length, either in a symmetric or periodic form. The Hamming window is used in digital signal processing to reduce spectral leakage. The symmetric form is used for FIR-filter design, while the periodic form is used to ensure continuous frames when using FFT.\n\n### Approach\n1. **Edge Cases Handling**: \n   - If the window length is 0, return an empty list.\n   - If the window length is 1, return `[1.0]` since a single sample does not require tapering.\n\n2. **Symmetric Window**:\n   - For the symmetric case, generate the window using the formula directly for the given length.\n\n3. **Periodic Window**:\n   - For the periodic case, generate a symmetric window of length `window_len + 1` and then remove the last sample to ensure continuity between successive frames.\n\n4. **Implementation**:\n   - Use NumPy for efficient array operations to compute the window values based on the mathematical formula.\n   - Round the values to 4 decimal places and convert the result to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hamming(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hamming window.\n\n    The function should return *window_len* samples of a Hamming window.\n    When *symmetric* is *False*, return the periodic variant (last sample of\n    the symmetric window removed).  Each value must be rounded to 4 decimals\n    and the result converted to a regular Python list.\n    \"\"\"\n    if window_len == 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    m = window_len if symmetric else window_len + 1\n    n = np.arange(m)\n    window = 0.54 - 0.46 * np.cos(2 * np.pi * n / (m - 1))\n    \n    if not symmetric:\n        window = window[:window_len]\n    \n    window_rounded = np.round(window, decimals=4)\n    return window_rounded.tolist()\n```\n\n### Explanation\n1. **Edge Cases**: The function immediately returns an empty list for a window length of 0 and `[1.0]` for a window length of 1.\n2. **Window Length Calculation**: Depending on whether the window is symmetric or periodic, the length `m` is adjusted. For periodic windows, we use `window_len + 1` to generate the symmetric window and then truncate it.\n3. **Window Generation**: Using NumPy, we efficiently compute the Hamming window values using the provided formula. The cosine function is applied element-wise across the array of indices.\n4. **Truncation for Periodic Case**: The periodic window is formed by taking the first `window_len` elements of the symmetric window of length `window_len + 1`.\n5. **Rounding and Conversion**: The values are rounded to 4 decimal places using NumPy's rounding function and then converted to a Python list for the final output.\n\nThis approach ensures that both symmetric and periodic Hamming windows are generated efficiently and correctly, adhering to the mathematical definitions and edge case requirements."}
{"task_id": 442, "completion_id": 0, "completion": "To solve this problem, we need to implement a k-Nearest Neighbors (k-NN) classifier using a KD-Tree for efficient neighbor queries. The KD-Tree is a space-partitioning data structure that allows for efficient nearest neighbor searches in multidimensional spaces. The solution involves building the KD-Tree from the training data and using it to find the k nearest neighbors for each test sample, followed by majority voting to predict the class labels.\n\n### Approach\n1. **Building the KD-Tree**:\n   - The KD-Tree is constructed recursively by selecting the median point along the current split axis (determined by the depth of the tree) and partitioning the remaining points into left and right subtrees. This ensures a balanced tree structure.\n   - Each node in the tree represents a split along a specific axis and stores the median point's index, split axis, and left and right children.\n\n2. **Querying the KD-Tree**:\n   - For each test sample, traverse the tree to find the k nearest neighbors. Use a max-heap to maintain the k closest points encountered during the traversal.\n   - During traversal, compare each node's point with the query point and update the heap if the point is closer than the current farthest point in the heap. Handle ties by preferring points with smaller indices.\n\n3. **Majority Voting**:\n   - After collecting the k nearest neighbors for a test sample, determine the majority class label using `np.argmax(np.bincount(labels))` to handle ties by selecting the smallest class index.\n\n### Solution Code\n```python\nimport numpy as np\nimport heapq\n\ndef knn_kdtree(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> list[int]:\n    n_train, n_features = X_train.shape\n\n    def build_tree(points_indices, depth):\n        if not points_indices:\n            return None\n        split_axis = depth % n_features\n        sorted_indices = sorted(points_indices, key=lambda idx: X_train[idx, split_axis])\n        median_pos = len(sorted_indices) // 2\n        median_idx = sorted_indices[median_pos]\n        left = build_tree(sorted_indices[:median_pos], depth + 1)\n        right = build_tree(sorted_indices[median_pos+1:], depth + 1)\n        return (split_axis, median_idx, left, right)\n\n    tree = build_tree(list(range(n_train)), 0)\n\n    def query_knn(node, query_point, k):\n        heap = []\n        def recurse(node):\n            if node is None:\n                return\n            split_axis, current_idx, left, right = node\n            current_point = X_train[current_idx]\n            distance_sq = np.sum((current_point - query_point) ** 2)\n            current_max = None\n\n            if len(heap) < k:\n                heapq.heappush(heap, (-distance_sq, -current_idx))\n            else:\n                current_max_neg_dist, current_max_neg_idx = heap[0]\n                current_max_dist = -current_max_neg_dist\n                current_max_idx = -current_max_neg_idx\n                if (distance_sq < current_max_dist) or (distance_sq == current_max_dist and current_idx < current_max_idx):\n                    heapq.heappop(heap)\n                    heapq.heappush(heap, (-distance_sq, -current_idx))\n\n            query_value = query_point[split_axis]\n            split_value = current_point[split_axis]\n            if query_value <= split_value:\n                near_subtree = left\n                far_subtree = right\n            else:\n                near_subtree = right\n                far_subtree = left\n\n            recurse(near_subtree)\n\n            current_max_sq = -heap[0][0] if heap else float('inf')\n            axis_distance_sq = (query_value - split_value) ** 2\n            if axis_distance_sq <= current_max_sq or len(heap) < k:\n                recurse(far_subtree)\n\n        recurse(node)\n        return [ -idx for (dist_sq, idx) in heap ]\n\n    predictions = []\n    for test_sample in X_test:\n        neighbors = query_knn(tree, test_sample, k)\n        labels = y_train[neighbors]\n        if labels.size == 0:\n            predictions.append(0)\n            continue\n        counts = np.bincount(labels)\n        majority = np.argmax(counts)\n        predictions.append(int(majority))\n    return predictions\n```\n\n### Explanation\n- **Building the KD-Tree**: The tree is built recursively by selecting the median point along the current split axis (determined by the depth modulo the number of features). This ensures the tree remains balanced, allowing efficient searches.\n- **Querying the KD-Tree**: For each test sample, the tree is traversed to collect the k nearest neighbors. A max-heap is used to keep track of the closest points, updating the heap when closer points are found or when ties are broken by preferring smaller indices.\n- **Majority Voting**: After collecting the k nearest neighbors, the majority class label is determined using `np.bincount` and `np.argmax`, ensuring ties are broken by selecting the smallest class index.\n\nThis approach efficiently handles high-dimensional data and ensures accurate classification by leveraging the KD-Tree structure for fast neighbor searches."}
{"task_id": 443, "completion_id": 0, "completion": "To solve this problem, we need to split a dataset into two subsets based on a specific feature and a threshold value. The split is determined by whether the feature values meet or exceed the threshold (for numeric thresholds) or match the threshold exactly (for non-numeric thresholds).\n\n### Approach\n1. **Determine Threshold Type**: Check if the threshold is a boolean, numeric (including both Python and numpy numeric types), or non-numeric (e.g., string).\n2. **Create Mask**: Based on the threshold type, create a boolean mask to identify rows that belong to the left subset:\n   - For boolean thresholds, use equality check.\n   - For numeric thresholds, use a greater-than-or-equal comparison.\n   - For non-numeric thresholds, use equality check.\n3. **Split Data**: Use the boolean mask to split the input array into left and right subsets. Ensure that even if no rows meet the condition, the resulting arrays maintain the correct shape with zero rows but the same number of columns as the input.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef divide_on_feature(X: np.ndarray, feature_i: int, threshold):\n    \"\"\"Split *X* into two NumPy matrices according to feature *feature_i* and *threshold*.\n\n    A row is sent to the *left* subset iff\n    \u2022 threshold is numeric  ->  X[row, feature_i] >= threshold\n    \u2022 threshold is non-numeric -> X[row, feature_i] == threshold\n\n    Args:\n        X: 2-D NumPy array. Each row is a sample, each column a feature.\n        feature_i: Index of the column that is tested.\n        threshold: Value that determines the split (numeric or categorical).\n\n    Returns:\n        (left_subset, right_subset): tuple of NumPy arrays with the same\n        number of columns as *X*.\n    \"\"\"\n    # Check if the threshold is a boolean first to handle it as non-numeric\n    if isinstance(threshold, bool):\n        left_mask = X[:, feature_i] == threshold\n    elif isinstance(threshold, (int, float, np.integer, np.floating)):\n        left_mask = X[:, feature_i] >= threshold\n    else:\n        left_mask = X[:, feature_i] == threshold\n    \n    left_subset = X[left_mask]\n    right_subset = X[~left_mask]\n    \n    return (left_subset, right_subset)\n```\n\n### Explanation\n1. **Threshold Type Check**: The function first checks if the threshold is a boolean. If so, it uses an equality check. For numeric thresholds (including numpy types), it uses a greater-than-or-equal comparison. For all other types, it uses an equality check.\n2. **Mask Creation**: A boolean mask is created based on the threshold type to identify rows that belong to the left subset.\n3. **Data Splitting**: The input array is split into left and right subsets using the boolean mask. The `~` operator inverts the mask to get the right subset. This ensures that the resulting arrays maintain the correct shape even if they contain no rows.\n\nThis approach efficiently handles both numeric and non-numeric thresholds, ensuring the correct split of the dataset while maintaining the integrity of the data structure."}
{"task_id": 444, "completion_id": 0, "completion": "To solve this problem, we need to compute the Radial Basis Function (RBF) kernel matrix between two collections of vectors. The RBF kernel is a popular choice in various machine learning algorithms, including Gaussian Processes and Support Vector Machines. The kernel measures the similarity between pairs of vectors using a scale parameter, which can be isotropic (single value) or anisotropic (different values for each feature).\n\n### Approach\n1. **Input Validation**: \n   - Convert input vectors into NumPy arrays for efficient computation.\n   - Ensure both input matrices have the same number of features (columns).\n   - Check the validity of the scale parameter (\u03c3). If \u03c3 is a list, ensure its length matches the number of features and all values are positive. If \u03c3 is a single value, ensure it is positive. If \u03c3 is None, compute it as the square root of half the number of features.\n\n2. **Kernel Computation**:\n   - Compute the pairwise differences between all vectors in the two matrices.\n   - Scale these differences by the \u03c3 values.\n   - Square the scaled differences, sum them up for each vector pair, and apply the exponential function to the negative half of these sums.\n\n3. **Result Formatting**:\n   - Round the resulting kernel matrix values to four decimal places and convert them to a nested Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\"\"\"\n    # Convert X and Y to numpy arrays\n    X_np = np.array(X)\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.array(Y)\n    \n    # Check that X and Y are 2D arrays with matching feature dimensions\n    if X_np.ndim != 2 or Y_np.ndim != 2:\n        return -1\n    if X_np.shape[1] != Y_np.shape[1]:\n        return -1\n    \n    C = X_np.shape[1]  # Number of features\n    \n    # Validate and process sigma\n    if sigma is None:\n        sigma_val = np.sqrt(C / 2.0)\n    elif isinstance(sigma, (float, int)):\n        if sigma <= 0:\n            return -1\n        sigma_val = sigma\n    elif isinstance(sigma, list):\n        if len(sigma) != C:\n            return -1\n        try:\n            sigma_arr = np.array(sigma, dtype=np.float64)\n        except (ValueError, TypeError):\n            return -1\n        if (sigma_arr <= 0).any():\n            return -1\n        sigma_val = sigma_arr\n    else:\n        # Sigma is of invalid type\n        return -1\n    \n    # Compute pairwise differences\n    diff = X_np[:, np.newaxis, :] - Y_np[np.newaxis, :, :]\n    \n    # Scale differences by sigma_val and compute squared sum\n    scaled = diff / sigma_val\n    squared = scaled ** 2\n    sum_sq = squared.sum(axis=2)\n    \n    # Compute RBF kernel and round to 4 decimal places\n    k = np.exp(-0.5 * sum_sq)\n    rounded = np.round(k, decimals=4)\n    \n    return rounded.tolist()\n```\n\n### Explanation\n1. **Input Handling**: The input vectors are converted to NumPy arrays to leverage efficient array operations. If the second matrix Y is not provided, it defaults to the first matrix X.\n\n2. **Validation Checks**: \n   - Ensure both matrices are 2D and have the same number of features.\n   - Validate the scale parameter \u03c3 to ensure it meets the specified criteria (positive values, correct dimensions).\n\n3. **Kernel Calculation**:\n   - Compute the pairwise differences between vectors using broadcasting.\n   - Scale the differences by \u03c3, square them, sum over features, and apply the exponential function to get the kernel values.\n\n4. **Result Formatting**: The kernel matrix values are rounded to four decimal places and converted to a nested list to meet the output requirements.\n\nThis approach efficiently handles both isotropic and anisotropic scale parameters, ensuring accurate and efficient computation of the RBF kernel matrix."}
{"task_id": 446, "completion_id": 0, "completion": "To solve this problem, we need to implement a function called `decision_stump` that finds the best single-level split (decision stump) for a given feature matrix and label vector based on information gain. The goal is to split the data such that the decrease in entropy (information gain) is maximized.\n\n### Approach\n1. **Entropy Calculation**: Compute the entropy of a set of labels, which measures the impurity or uncertainty. The entropy is calculated using the formula \\( H = -\\sum (p_i \\log_2 p_i) \\), where \\( p_i \\) is the proportion of class \\( i \\) in the set.\n\n2. **Majority Label Determination**: Determine the majority label in a subset of labels. If there is a tie, the smallest label is chosen.\n\n3. **Parent Entropy Check**: Calculate the entropy of the entire dataset (parent node). If the entropy is zero (all labels are the same), return the no-split case.\n\n4. **Feature and Threshold Evaluation**: For each feature, generate possible thresholds as midpoints between consecutive unique values. For each threshold, split the data and compute the information gain. Track the best split based on the highest information gain, resolving ties by feature index and threshold value.\n\n5. **Result Determination**: If no split improves information gain, return the no-split case. Otherwise, return the best split details.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    n_samples, n_features = X.shape\n\n    def entropy(labels):\n        if len(labels) == 0:\n            return 0.0\n        _, counts = np.unique(labels, return_counts=True)\n        probs = counts / counts.sum()\n        return -np.sum(probs * np.log2(probs))\n\n    def majority_label(labels):\n        if len(labels) == 0:\n            return 0  # Should not occur in valid splits\n        values, counts = np.unique(labels, return_counts=True)\n        max_count = np.max(counts)\n        candidates = values[counts == max_count]\n        return np.min(candidates)\n\n    parent_entropy = entropy(y)\n    if parent_entropy == 0.0:\n        majority = majority_label(y)\n        return (-1, None, majority, majority)\n\n    overall_majority = majority_label(y)\n    current_max_gain = -np.inf\n    best_feature_index = -1\n    best_threshold = None\n    left_maj = None\n    right_maj = None\n\n    for j in range(n_features):\n        feature_values = X[:, j]\n        unique_values = np.unique(feature_values)\n        if len(unique_values) < 2:\n            continue\n        sorted_values = np.sort(unique_values)\n        thresholds = (sorted_values[:-1] + sorted_values[1:]) / 2\n        for t in thresholds:\n            left_mask = feature_values <= t\n            n_left = np.sum(left_mask)\n            if n_left == 0 or n_left == n_samples:\n                continue\n            left_y = y[left_mask]\n            right_y = y[~left_mask]\n            h_left = entropy(left_y)\n            h_right = entropy(right_y)\n            p_left = n_left / n_samples\n            p_right = (n_samples - n_left) / n_samples\n            gain = parent_entropy - (p_left * h_left + p_right * h_right)\n\n            if gain > current_max_gain:\n                current_max_gain = gain\n                best_feature_index = j\n                best_threshold = t\n                left_maj = majority_label(left_y)\n                right_maj = majority_label(right_y)\n            elif gain == current_max_gain:\n                if j < best_feature_index:\n                    best_feature_index = j\n                    best_threshold = t\n                    left_maj = majority_label(left_y)\n                    right_maj = majority_label(right_y)\n                elif j == best_feature_index and t < best_threshold:\n                    best_threshold = t\n                    left_maj = majority_label(left_y)\n                    right_maj = majority_label(right_y)\n\n    if current_max_gain <= 0.0:\n        return (-1, None, overall_majority, overall_majority)\n    else:\n        rounded_threshold = round(best_threshold, 4)\n        return (best_feature_index, rounded_threshold, left_maj, right_maj)\n```\n\n### Explanation\n1. **Entropy Calculation**: The entropy function calculates the entropy of a given set of labels, which helps measure the impurity. Lower entropy indicates a more homogeneous set.\n\n2. **Majority Label**: This function determines the most frequent label in a subset, breaking ties by choosing the smallest label.\n\n3. **Parent Entropy Check**: If all labels are the same, the entropy is zero, and we return the no-split case immediately.\n\n4. **Feature and Threshold Evaluation**: For each feature, possible thresholds are generated as midpoints between consecutive unique values. Each threshold is evaluated by splitting the data and calculating the information gain. The best split is tracked based on the highest gain, with tiebreakers for feature index and threshold.\n\n5. **Result Determination**: If no valid split improves information gain, the no-split case is returned. Otherwise, the best split details are returned with the threshold rounded to four decimal places."}
{"task_id": 447, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef one(shape, **kwargs):\n    \"\"\"Create a nested Python list filled with ones.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired shape of the output list. Works exactly as the first argument of\n        ``numpy.ones``.\n    **kwargs : dict\n        Optional keyword arguments forwarded to ``numpy.ones`` (e.g. ``dtype``).\n\n    Returns\n    -------\n    list\n        A list (possibly nested) containing only ones that mirrors the provided\n        shape.\n    \"\"\"\n    return np.ones(shape, **kwargs).tolist()\n```"}
{"task_id": 448, "completion_id": 0, "completion": "To solve this problem, we need to implement a baseline regressor that predicts the mean of the training targets for all test samples. The solution should handle edge cases such as empty training targets and empty test data efficiently.\n\n### Approach\n1. **Check for Empty Training Targets**: If the training target array `y_train` is empty, return an empty list as no predictions can be made.\n2. **Compute the Mean**: Calculate the arithmetic mean of the training targets `y_train`.\n3. **Generate Predictions**: Create an array of the same length as the number of samples in the test feature matrix `X_test`, filled with the computed mean value.\n4. **Round Predictions**: Round each element in the predictions array to the nearest 4th decimal place.\n5. **Return as List**: Convert the rounded predictions array to a Python list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mean_baseline_predict(X_train, y_train, X_test):\n    \"\"\"Baseline regressor that predicts the mean of the training targets.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples_train, n_features_train).\n    y_train : np.ndarray\n        One-dimensional array of training targets of length n_samples_train.\n    X_test : np.ndarray\n        Test feature matrix of shape (n_samples_test, n_features_test).\n\n    Returns\n    -------\n    list[float]\n        A list containing the predictions for every row in ``X_test`` rounded to\n        4 decimal places.  If ``y_train`` is empty, the function returns an\n        empty list.\n    \"\"\"\n    if y_train.size == 0:\n        return []\n    mean_val = np.mean(y_train)\n    n_samples = X_test.shape[0]\n    predictions = np.full(n_samples, mean_val)\n    rounded_predictions = np.round(predictions, 4)\n    return rounded_predictions.tolist()\n```\n\n### Explanation\n1. **Check for Empty Training Targets**: The code first checks if `y_train` is empty using `y_train.size == 0`. If it is, an empty list is returned immediately.\n2. **Compute the Mean**: The arithmetic mean of `y_train` is computed using `np.mean(y_train)`.\n3. **Generate Predictions**: Using `np.full(n_samples, mean_val)`, an array of length equal to the number of test samples (from `X_test.shape[0]`) is created, filled with the computed mean value.\n4. **Round Predictions**: Each element in the predictions array is rounded to 4 decimal places using `np.round(predictions, 4)`.\n5. **Return as List**: The rounded predictions are converted to a Python list using `tolist()` and returned.\n\nThis approach ensures that all edge cases are handled efficiently and the predictions are accurate and formatted correctly as per the problem requirements."}
{"task_id": 449, "completion_id": 0, "completion": "To solve this problem, we need to implement a Gaussian Discriminant Analysis (GDA) model for binary classification. The model will learn parameters from the training data and use them to predict labels for the test data. The key steps involve computing class means, covariance matrix, and deriving a decision boundary based on these parameters.\n\n### Approach\n1. **Split Training Data**: Separate the training data into two classes based on their labels.\n2. **Compute Class Means**: Calculate the mean vectors for each class.\n3. **Covariance Matrix**: Compute the covariance matrix using the centered data from both classes.\n4. **Normal Vector**: Determine the normal vector to the hyperplane by subtracting the class means and normalizing it.\n5. **Hyperplane Parameters**: Calculate the bias term to position the hyperplane as the perpendicular bisector between the class means.\n6. **Sign Determination**: Check if the class-1 mean lies on the positive side of the hyperplane.\n7. **Prediction**: Use the learned parameters to predict labels for the test data based on their position relative to the hyperplane.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gda(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Train a Gaussian Discriminant Analysis model and predict labels.\"\"\"\n    # Split the training data into two classes based on labels\n    X0 = X_train[y_train == 0]\n    X1 = X_train[y_train == 1]\n    \n    # Compute the mean vectors for each class\n    Mu0 = np.mean(X0, axis=0)\n    Mu1 = np.mean(X1, axis=0)\n    \n    # Stack the centered data from both classes\n    X_sub_Mu = np.vstack([X0 - Mu0, X1 - Mu1])\n    \n    # Number of features (m) used in covariance calculation\n    m = X_train.shape[1]\n    \n    # Compute the covariance matrix (not used in prediction)\n    Sigma = (1.0 / m) * (X_sub_Mu.T @ X_sub_Mu)\n    \n    # Calculate the normal vector (Mu1 - Mu0) and normalize it\n    normal_vec = Mu1 - Mu0\n    norm = np.sqrt(np.sum(normal_vec ** 2))\n    if norm != 0:\n        normal_vec /= norm\n    \n    # Assign weights and compute the bias term\n    w = normal_vec\n    midpoint = (Mu0 + Mu1) / 2\n    b = -np.dot(w, midpoint)\n    \n    # Determine the sign based on the position of Mu1 relative to the hyperplane\n    sign = int(np.dot(w, Mu1) + b > 0)\n    \n    # Predict the labels for the test data\n    linear = X_test @ w + b\n    predictions = (linear > 0).astype(int) * sign\n    \n    return predictions.tolist()\n```\n\n### Explanation\n1. **Data Splitting**: The training data is divided into two subsets based on their labels (0 or 1).\n2. **Mean Calculation**: The mean vectors for each class are computed to determine the central tendency of each class.\n3. **Covariance Matrix**: The covariance matrix is computed using the centered data (each data point minus its class mean), scaled by the number of features. This matrix captures the spread of the data.\n4. **Normal Vector**: The direction of the hyperplane is determined by the vector connecting the class means, normalized to unit length.\n5. **Bias Term**: The bias term positions the hyperplane midway between the class means, ensuring it is the perpendicular bisector.\n6. **Sign Check**: The sign is determined to check if class-1 mean lies on the positive side of the hyperplane, ensuring predictions are correctly oriented.\n7. **Prediction**: Test data points are classified based on their position relative to the hyperplane, adjusted by the sign to ensure correct orientation.\n\nThis approach ensures that the model learns the optimal decision boundary from the training data and applies it to predict labels for the test data accurately."}
{"task_id": 450, "completion_id": 0, "completion": "To solve this problem, we need to determine the final class label for each sample based on the predictions from multiple estimators in an ensemble method. The final class label is chosen using majority voting, where the class with the highest count of predictions is selected. In case of a tie, the smallest class label is chosen.\n\n### Approach\n1. **Understanding the Input**: The input is a 2-D NumPy array where each row represents a sample, and each column contains the prediction from an individual estimator.\n2. **Majority Vote Calculation**: For each sample (row), we count the occurrences of each class label using NumPy's `bincount` function. This function returns an array where each index represents a class label, and the value at that index is the count of predictions for that class.\n3. **Handling Ties**: The `argmax` function is applied to the result of `bincount` to determine the class label with the highest count. In case of a tie (multiple classes with the same highest count), `argmax` returns the smallest class label because it picks the first occurrence of the maximum value in the array.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef majority_vote(preds: np.ndarray) -> list[int]:\n    \"\"\"Aggregate ensemble predictions using majority voting.\n\n    Parameters\n    ----------\n    preds : np.ndarray\n        A 2-D NumPy array of shape `(n_samples, n_estimators)` where each entry\n        is an *integer* class label predicted by one estimator for one sample.\n\n    Returns\n    -------\n    list[int]\n        A list containing the chosen class for every sample. If multiple\n        classes have the same maximal count (a tie), return the smallest class\n        label.\n    \"\"\"\n    return [np.bincount(row).argmax() for row in preds]\n```\n\n### Explanation\n1. **Loop Through Each Sample**: For each row in the input array `preds`, which represents a sample's predictions from all estimators, we process the row to determine the majority class.\n2. **Count Class Occurrences**: Using `np.bincount`, we count how many times each class label appears in the row. The resulting array from `bincount` has indices corresponding to class labels and values corresponding to their counts.\n3. **Determine Majority Class**: The `argmax` function is applied to the counts array to find the class label with the highest count. In case of a tie, `argmax` returns the smallest class label due to the way it handles the first occurrence of the maximum value.\n\nThis approach efficiently computes the majority vote for each sample, ensuring correctness and handling ties appropriately as required."}
{"task_id": 451, "completion_id": 0, "completion": "To solve this problem, we need to implement a k-nearest neighbors (k-NN) classifier using a KD-Tree for efficient nearest neighbor search. The KD-Tree allows us to partition the feature space, making the search for the nearest neighbors more efficient compared to a brute-force approach.\n\n### Approach\n1. **Edge Case Handling**: Check if the training data is empty or if k is non-positive, returning an empty list in such cases.\n2. **Building the KD-Tree**: Construct a balanced KD-Tree from the training data. Each node in the tree represents a split along a specific dimension, determined by cycling through dimensions as we go deeper into the tree. The median point along the current dimension is chosen to split the data, ensuring the tree remains balanced.\n3. **k-Nearest Neighbor Search**: For each test point, traverse the KD-Tree to find the k nearest neighbors. Use a max-heap to keep track of the nearest neighbors efficiently. The search involves checking the nearest subtree first and then the farther subtree if necessary, based on the distance to the split plane.\n4. **Majority Voting**: After collecting the k nearest neighbors, determine the most frequent class label. In case of a tie, return the smallest label.\n\n### Solution Code\n```python\nimport math\nimport heapq\nfrom collections import Counter\nfrom typing import List\nimport numpy as np\n\nclass Node:\n    __slots__ = ['split_axis', 'split_value', 'point', 'label', 'left', 'right']\n    def __init__(self, split_axis, split_value, point, label, left, right):\n        self.split_axis = split_axis\n        self.split_value = split_value\n        self.point = point\n        self.label = label\n        self.left = left\n        self.right = right\n\ndef knn_kdtree_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> List[int]:\n    if len(X_train) == 0 or k <= 0:\n        return []\n    \n    n_samples, n_features = X_train.shape\n    y_train = np.asarray(y_train)\n    \n    def build_kd_tree(points: List[List[float]], labels: List[int], depth: int = 0) -> Optional[Node]:\n        if not points:\n            return None\n        axis = depth % n_features\n        combined = list(zip(points, labels))\n        combined.sort(key=lambda x: x[0][axis])\n        sorted_points = [p for p, l in combined]\n        sorted_labels = [l for p, l in combined]\n        median_idx = len(sorted_points) // 2\n        median_point = sorted_points[median_idx]\n        median_label = sorted_labels[median_idx]\n        left_points = sorted_points[:median_idx]\n        left_labels = sorted_labels[:median_idx]\n        right_points = sorted_points[median_idx + 1:]\n        right_labels = sorted_labels[median_idx + 1:]\n        left_child = build_kd_tree(left_points, left_labels, depth + 1)\n        right_child = build_kd_tree(right_points, right_labels, depth + 1)\n        return Node(axis, median_point[axis], median_point, median_label, left_child, right_child)\n    \n    points = X_train.tolist()\n    labels = y_train.tolist()\n    root = build_kd_tree(points, labels)\n    \n    predictions = []\n    \n    def search_knn(node: Optional[Node], query: List[float], heap: list, k: int):\n        if not node:\n            return\n        distance = math.sqrt(sum((x - y) ** 2 for x, y in zip(query, node.point)))\n        if len(heap) < k:\n            heapq.heappush(heap, (-distance, node.label))\n        else:\n            current_max = -heap[0][0]\n            if distance < current_max:\n                heapq.heappushpop(heap, (-distance, node.label))\n        if query[node.split_axis] < node.split_value:\n            near, far = node.left, node.right\n        else:\n            near, far = node.right, node.left\n        search_knn(near, query, heap, k)\n        current_max_dist = -heap[0][0] if len(heap) >= k else float('inf')\n        split_dist = abs(query[node.split_axis] - node.split_value)\n        if split_dist < current_max_dist:\n            search_knn(far, query, heap, k)\n    \n    for test_point in X_test:\n        query = test_point.tolist()\n        heap = []\n        search_knn(root, query, heap, k)\n        if not heap:\n            predictions.append(0)\n            continue\n        counter = Counter(label for _, label in heap)\n        max_count = max(counter.values())\n        candidates = [label for label, count in counter.items() if count == max_count]\n        predictions.append(min(candidates))\n    \n    return predictions\n```\n\n### Explanation\n1. **Node Class**: Represents each node in the KD-Tree, storing the split dimension, split value, the point itself, its label, and left/right children.\n2. **Tree Construction**: The `build_kd_tree` function recursively constructs the tree by selecting the median point along the current dimension, splitting the data into left and right subtrees.\n3. **Search Function**: The `search_knn` function traverses the tree, maintaining a max-heap to track the k nearest neighbors. It explores the nearest subtree first and checks the farther subtree if necessary based on the distance to the split plane.\n4. **Majority Voting**: After collecting neighbors, the most frequent label is determined, with ties broken by selecting the smallest label.\n\nThis approach efficiently handles the k-NN classification using a KD-Tree, ensuring both construction and querying are performed optimally."}
{"task_id": 452, "completion_id": 0, "completion": "To solve this problem, we need to split a given dataset into two subsets based on a specified feature column and a threshold value. The split criteria depend on whether the threshold is numeric (int or float) or non-numeric (categorical). For numeric thresholds, samples with values greater than or equal to the threshold form the first subset, while for non-numeric thresholds, samples with values exactly equal to the threshold form the first subset. The order of samples in each subset must be preserved, and both subsets should be returned as Python lists.\n\n### Approach\n1. **Input Handling**: Convert the input dataset to a NumPy array if it is provided as a list of lists. This allows uniform handling of both list and array inputs.\n2. **Threshold Type Check**: Determine if the threshold is numeric (exactly int or float) or non-numeric. This check is crucial as it dictates the split criteria.\n3. **Splitting the Dataset**: Iterate through each sample in the dataset. For each sample, check the value at the specified feature column. Depending on the threshold type, apply the appropriate condition (>= for numeric, == for non-numeric) to split the samples into two groups.\n4. **Conversion to Python Lists**: Convert the resulting subsets from NumPy arrays to Python lists while preserving the order of samples.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import List\n\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\"\"\"\n    X_array = np.array(X) if isinstance(X, list) else X\n    left = []\n    right = []\n    is_numeric = type(threshold) in (int, float)\n    \n    for row in X_array:\n        value = row[feature_i]\n        if is_numeric:\n            if value >= threshold:\n                left.append(row)\n            else:\n                right.append(row)\n        else:\n            if value == threshold:\n                left.append(row)\n            else:\n                right.append(row)\n    \n    left_list = [r.tolist() for r in left]\n    right_list = [r.tolist() for r in right]\n    return [left_list, right_list]\n```\n\n### Explanation\n1. **Input Conversion**: The dataset `X` is converted to a NumPy array if it is provided as a list. This ensures consistent handling of rows and columns.\n2. **Threshold Check**: The type of the threshold is checked to determine if it is numeric (int or float). This check uses the exact type rather than `isinstance` to avoid treating boolean values as numeric.\n3. **Iterating Through Samples**: Each row of the dataset is examined. The value at the specified feature column is compared against the threshold using the appropriate condition (>= for numeric, == for non-numeric) to split the data into two lists, `left` and `right`.\n4. **Conversion to Lists**: The resulting subsets, stored as lists of NumPy arrays, are converted to Python lists using `tolist()` to meet the output requirements.\n\nThis approach ensures that the dataset is split correctly according to the given threshold while preserving the original order of samples and converting the results to the required format."}
{"task_id": 453, "completion_id": 0, "completion": "To solve this problem, we need to compute the target Q-values for training a Deep Q-Network (DQN). The targets are derived from the current Q-values, the next state Q-values, the rewards received, and whether the next state is terminal. The solution involves validating the input parameters and then updating the Q-values for the actions taken in each state.\n\n### Approach\n1. **Input Validation**: Check if the discount factor `gamma` is within the valid range [0, 1]. Verify that the shapes of the Q-value matrices `Q` and `Q_next` are consistent. Ensure that the lengths of the lists `actions`, `rewards`, and `dones` match the batch size inferred from the Q-value matrices.\n2. **Target Calculation**: For each state in the batch, compute the target Q-value. If the next state is terminal (`dones[i]` is True), the target is the immediate reward. Otherwise, the target is the immediate reward plus the discounted maximum Q-value of the next state.\n3. **Update Q-values**: Copy the current Q-values and update only the entries corresponding to the actions taken with the computed targets.\n4. **Rounding and Conversion**: Round the updated Q-values to four decimal places and convert the result to a nested list structure.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # Check if gamma is within the valid range\n    if gamma < 0 or gamma > 1:\n        return -1\n    \n    # Check Q and Q_next are 2D arrays with the same shape\n    if Q.ndim != 2 or Q_next.ndim != 2 or Q.shape != Q_next.shape:\n        return -1\n    \n    batch_size = Q.shape[0]\n    \n    # Check actions, rewards, dones have length equal to batch_size\n    if len(actions) != batch_size or len(rewards) != batch_size or len(dones) != batch_size:\n        return -1\n    \n    # Create a copy of Q to modify\n    y = Q.copy()\n    \n    for i in range(batch_size):\n        done = dones[i]\n        reward = rewards[i]\n        action = actions[i]\n        \n        if done:\n            target = reward\n        else:\n            next_max = np.max(Q_next[i])\n            target = reward + gamma * next_max\n        \n        # Update the corresponding action's Q-value\n        y[i, action] = target\n    \n    # Round to 4 decimal places and convert to list of lists\n    y_rounded = np.round(y, 4)\n    return y_rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the discount factor `gamma` is within the valid range. It then ensures that the Q-value matrices `Q` and `Q_next` have the same shape and that the lengths of `actions`, `rewards`, and `dones` match the batch size.\n2. **Target Calculation**: For each state in the batch, the target Q-value is computed based on whether the next state is terminal. If it is terminal, the target is just the reward; otherwise, it includes the discounted maximum Q-value of the next state.\n3. **Update Q-values**: The function updates only the Q-value corresponding to the action taken in each state, leaving other Q-values unchanged.\n4. **Rounding and Conversion**: The updated Q-values are rounded to four decimal places and converted to a nested list structure to meet the output requirements.\n\nThis approach ensures that the Q-values are correctly updated for training the DQN while adhering to the constraints and requirements specified."}
{"task_id": 454, "completion_id": 0, "completion": "To solve this problem, we need to generate one-hot label arrays for training a Deep Convolutional Generative Adversarial Network (DCGAN). These labels are used to train the discriminator and generator models. The labels must be structured such that real images are labeled as class 0 ([1, 0]) and generated images as class 1 ([0, 1]). \n\n### Approach\n1. **Understanding the Labels**: The problem specifies three types of labels needed:\n   - **valid_half**: Labels for half the batch of real images, each row being [1, 0].\n   - **fake_half**: Labels for the other half of generated images, each row being [0, 1].\n   - **valid_full**: Labels for a full batch where the generator tries to fool the discriminator, all rows being [1, 0].\n\n2. **Creating Arrays with NumPy**:\n   - **valid_half and fake_half**: These arrays will each have a shape of (n//2, 2), where n is the batch size. For valid_half, the first column is set to 1, and for fake_half, the second column is set to 1.\n   - **valid_full**: This array has a shape of (n, 2) with all rows set to [1, 0].\n\n3. **Data Type Conversion**: The arrays are converted to Python lists using NumPy's `tolist()` method to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef generate_dcgan_labels(batch_size: int) -> tuple[list[list[int]], list[list[int]], list[list[int]]]:\n    n_half = batch_size // 2\n    \n    valid_half = np.zeros((n_half, 2), dtype=np.int32)\n    valid_half[:, 0] = 1\n    \n    fake_half = np.zeros((n_half, 2), dtype=np.int32)\n    fake_half[:, 1] = 1\n    \n    valid_full = np.zeros((batch_size, 2), dtype=np.int32)\n    valid_full[:, 0] = 1\n    \n    return (valid_half.tolist(), fake_half.tolist(), valid_full.tolist())\n```\n\n### Explanation\n- **valid_half**: This array is initialized with zeros and the first column is set to 1 for all rows, resulting in one-hot vectors [1, 0] for half the batch size.\n- **fake_half**: Similarly initialized with zeros, but the second column is set to 1, resulting in one-hot vectors [0, 1] for the other half of the batch.\n- **valid_full**: This array uses the full batch size, with the first column set to 1 for all rows, ensuring all labels are [1, 0].\n\nBy converting these NumPy arrays to Python lists, we ensure compatibility with typical training loops in machine learning frameworks that may require list inputs. This approach efficiently constructs the necessary labels using vectorized operations for optimal performance."}
{"task_id": 455, "completion_id": 0, "completion": "To solve this problem, we need to implement a k-Nearest Neighbors (k-NN) classifier that supports three different neighbor search strategies: \"naive\", \"heap\", and \"kdtree\". Each strategy has its own approach to finding the nearest neighbors, which affects the efficiency and performance of the classifier.\n\n### Approach\n\n1. **Naive Strategy**: This method involves comparing each query point with every training point. The distances are computed, sorted, and the k smallest distances are selected. This approach is straightforward but can be inefficient for large datasets due to its O(n log n) complexity for sorting all distances.\n\n2. **Heap Strategy**: This method optimizes the naive approach by using a max-heap to keep track of the k smallest distances. By maintaining a heap of size k, we avoid sorting all distances, reducing the complexity to O(n log k).\n\n3. **KD-Tree Strategy**: This method constructs a KD-Tree from the training data, which allows for efficient nearest neighbor searches. The tree is built by recursively splitting the data along the axis with the largest variance, using the median value as the split point. During the search, the tree is traversed to find the nearest neighbors, and branches are pruned based on the current best distances to improve efficiency.\n\n### Solution Code\n\n```python\nimport math\nimport heapq\nimport numpy as np\nfrom collections import Counter\n\ndef knn_classifier(X_train, y_train, X_query, k, method=\"naive\"):\n    \"\"\"k-Nearest Neighbour classifier.\"\"\"\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_query = np.array(X_query)\n    predictions = []\n    \n    if method == \"naive\":\n        for q in X_query:\n            distances = []\n            for i, x in enumerate(X_train):\n                dist = np.sum((x - q) ** 2)\n                distances.append((dist, y_train[i]))\n            distances.sort(key=lambda x: (x[0], x[1]))\n            top_k = distances[:k]\n            labels = [label for (d, label) in top_k]\n            counts = Counter(labels)\n            max_count = max(counts.values(), default=0)\n            candidates = [label for label, cnt in counts.items() if cnt == max_count]\n            predictions.append(min(candidates) if candidates else 0)\n    elif method == \"heap\":\n        for q in X_query:\n            heap = []\n            for i, x in enumerate(X_train):\n                dist = np.sum((x - q) ** 2)\n                if len(heap) < k:\n                    heapq.heappush(heap, (-dist, y_train[i]))\n                else:\n                    current_max = -heap[0][0]\n                    if dist < current_max:\n                        heapq.heappushpop(heap, (-dist, y_train[i]))\n            labels = [label for (neg_d, label) in heap]\n            counts = Counter(labels)\n            max_count = max(counts.values(), default=0)\n            candidates = [label for label, cnt in counts.items() if cnt == max_count]\n            predictions.append(min(candidates) if candidates else 0)\n    elif method == \"kdtree\":\n        points = list(zip(X_train, y_train))\n        \n        def build_kd_tree(points):\n            if not points:\n                return None\n            if len(points) == 1:\n                return {'type': 'leaf', 'point': points[0]}\n            features = np.array([p[0] for p in points])\n            variances = np.var(features, axis=0)\n            split_axis = np.argmax(variances)\n            sorted_points = sorted(points, key=lambda x: x[0][split_axis])\n            median_idx = len(sorted_points) // 2\n            split_val = sorted_points[median_idx][0][split_axis]\n            left_points = sorted_points[:median_idx]\n            right_points = sorted_points[median_idx:]\n            left_child = build_kd_tree(left_points)\n            right_child = build_kd_tree(right_points)\n            return {\n                'type': 'internal',\n                'axis': split_axis,\n                'split_val': split_val,\n                'left': left_child,\n                'right': right_child,\n            }\n        \n        tree = build_kd_tree(points)\n        \n        def search(node, query, heap, k):\n            if node is None:\n                return\n            if node['type'] == 'leaf':\n                point = node['point']\n                dist = np.sum((point[0] - query) ** 2)\n                if len(heap) < k:\n                    heapq.heappush(heap, (-dist, point[1]))\n                else:\n                    current_max = -heap[0][0]\n                    if dist < current_max:\n                        heapq.heappushpop(heap, (-dist, point[1]))\n                return\n            axis = node['axis']\n            split_val = node['split_val']\n            if query[axis] <= split_val:\n                near_subtree = node['left']\n                far_subtree = node['right']\n            else:\n                near_subtree = node['right']\n                far_subtree = node['left']\n            search(near_subtree, query, heap, k)\n            plane_dist = (query[axis] - split_val) ** 2\n            current_max = -heap[0][0] if len(heap) >= k else float('inf')\n            if plane_dist < current_max or len(heap) < k:\n                search(far_subtree, query, heap, k)\n        \n        for q in X_query:\n            heap = []\n            if tree is not None:\n                search(tree, q, heap, k)\n            labels = [label for (neg_d, label) in heap]\n            counts = Counter(labels)\n            max_count = max(counts.values(), default=0)\n            candidates = [label for label, cnt in counts.items() if cnt == max_count]\n            predictions.append(min(candidates) if candidates else 0)\n    else:\n        raise ValueError(\"Invalid method\")\n    \n    return predictions\n```\n\n### Explanation\n\n1. **Naive Strategy**: For each query point, compute the squared Euclidean distance to every training point, sort these distances, and select the k smallest. The majority vote of the labels of these k points determines the prediction, with ties broken by selecting the smallest label.\n\n2. **Heap Strategy**: Similar to the naive approach but uses a max-heap to maintain the k smallest distances efficiently. This avoids sorting all distances, improving performance for large datasets.\n\n3. **KD-Tree Strategy**: Constructs a balanced KD-Tree by recursively splitting the training data along the axis of maximum variance. During the search, the tree is traversed to find the nearest neighbors efficiently by pruning branches that cannot contain closer points than those already found.\n\nEach method ensures that the k nearest neighbors are found using different strategies, balancing between simplicity and efficiency based on the dataset size and structure. The KD-Tree method is particularly efficient for higher-dimensional data, reducing the search complexity significantly compared to the naive approach."}
{"task_id": 456, "completion_id": 0, "completion": "To solve this problem, we need to compute the sample covariance matrix or cross-covariance matrix between two datasets, X and Y. The covariance matrix is a key tool in statistics for understanding the relationships between different features in a dataset. The cross-covariance matrix extends this concept to measure how features from two different datasets vary together.\n\n### Approach\n1. **Input Validation**: Check if the input matrices meet the required criteria. Specifically, ensure that the number of observations (rows) in X is at least 2 and that Y (if provided) has the same number of rows as X.\n2. **Reshape 1D Arrays**: Convert any 1-dimensional arrays to 2-dimensional to ensure proper matrix operations.\n3. **Centering the Data**: Subtract the mean of each feature from the corresponding feature values to center the data around the origin.\n4. **Compute Covariance Matrix**: Use matrix multiplication to compute the covariance matrix, dividing by the appropriate degrees of freedom (n-1) to get the unbiased estimate.\n5. **Rounding and Conversion**: Round the resulting matrix to 4 decimal places and convert it to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_covariance_matrix(X: np.ndarray, Y: np.ndarray | None = None) -> list[list[float]]:\n    \"\"\"Compute the sample covariance (or cross-covariance) matrix.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        An (n_samples \u00d7 n_features) matrix containing the first dataset.\n    Y : np.ndarray | None, optional\n        An (n_samples \u00d7 m_features) matrix for cross-covariance. If None,\n        the covariance matrix of X with itself is returned.\n\n    Returns\n    -------\n    list[list[float]]\n        The covariance matrix rounded to 4 decimal places, or -1 for\n        invalid input.\n    \"\"\"\n    n_samples = X.shape[0]\n    if n_samples < 2:\n        return -1\n    \n    if Y is not None:\n        if Y.shape[0] != n_samples:\n            return -1\n    else:\n        Y = X\n    \n    # Ensure X and Y are 2D arrays\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    if Y.ndim == 1:\n        Y = Y.reshape(-1, 1)\n    \n    # Compute centered matrices\n    X_centered = X - X.mean(axis=0, keepdims=True)\n    Y_centered = Y - Y.mean(axis=0, keepdims=True)\n    \n    # Calculate covariance matrix\n    covariance_matrix = (X_centered.T @ Y_centered) / (n_samples - 1)\n    \n    # Round and convert to list of lists\n    rounded_cov = np.round(covariance_matrix, 4)\n    return rounded_cov.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the number of rows in X is at least 2 and if Y (when provided) has the same number of rows as X. If these conditions are not met, the function returns -1.\n2. **Reshaping**: If either X or Y is a 1-dimensional array, it is reshaped to a 2-dimensional array to ensure proper matrix operations.\n3. **Centering**: The mean of each feature is subtracted from the corresponding feature values. This centers the data around the origin, which is necessary for accurate covariance calculation.\n4. **Matrix Multiplication**: The covariance matrix is computed using matrix multiplication of the centered data matrices, scaled by the degrees of freedom (n-1) to get an unbiased estimate.\n5. **Rounding and Conversion**: The resulting matrix is rounded to 4 decimal places and converted to a list of lists to meet the output requirements.\n\nThis approach ensures that the function handles both covariance and cross-covariance calculations efficiently, even when dealing with 1-dimensional input arrays."}
{"task_id": 457, "completion_id": 0, "completion": "To solve this problem, we need to compute the Elastic-Net penalty for a given set of weights and a mixing hyper-parameter \u03b1. The Elastic-Net penalty is a combination of the L1 and L2 norms of the weight vector, controlled by the parameter \u03b1. The solution involves validating the parameter \u03b1, converting the input weights to a NumPy array for efficient computation, and then calculating the penalty using vectorized operations.\n\n### Approach\n1. **Validation of \u03b1**: Check if the mixing parameter \u03b1 lies within the interval [0, 1]. If not, return -1.\n2. **Conversion to NumPy Array**: Convert the input weights (which could be a list, tuple, or NumPy array) into a NumPy array to leverage vectorized operations for efficient computation.\n3. **Compute L2 Term**: Calculate the L2 norm term using the sum of squares of the weights, scaled by 0.5 * \u03b1.\n4. **Compute L1 Term**: Calculate the L1 norm term using the sum of absolute values of the weights, scaled by (1 - \u03b1).\n5. **Sum Terms and Round**: Combine the L2 and L1 terms to get the Elastic-Net penalty, then round the result to 4 decimal places and return it as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_penalty(weights, alpha):\n    \"\"\"Compute the Elastic-Net penalty for a set of weights.\n\n    The penalty is a linear combination of the L1 and L2 norms of the\n    weight vector controlled by *alpha*.\n\n    Args:\n        weights (list | tuple | numpy.ndarray): 1-D collection of weights.\n        alpha (float): Mixing parameter satisfying 0 <= alpha <= 1. If\n            alpha = 1, the penalty reduces to 0.5*L2; if alpha = 0 it\n            becomes purely L1.\n\n    Returns:\n        float | int: The Elastic-Net penalty rounded to 4 decimals, or -1\n        if *alpha* is outside the valid interval.\n    \"\"\"\n    if alpha < 0 or alpha > 1:\n        return -1\n    w = np.asarray(weights)\n    sum_squares = np.sum(w ** 2)\n    l2_term = 0.5 * alpha * sum_squares\n    l1_term = (1 - alpha) * np.sum(np.abs(w))\n    penalty = l2_term + l1_term\n    return round(penalty, 4)\n```\n\n### Explanation\n- **Validation**: The function first checks if \u03b1 is within the valid range [0, 1]. If not, it returns -1 immediately.\n- **Conversion to NumPy Array**: The input weights are converted to a NumPy array to utilize vectorized operations, ensuring efficient computation of sums and absolute values.\n- **L2 Term Calculation**: The sum of the squares of the weights is computed and scaled by 0.5 * \u03b1 to get the L2 term.\n- **L1 Term Calculation**: The sum of the absolute values of the weights is computed and scaled by (1 - \u03b1) to get the L1 term.\n- **Combining Terms**: The L2 and L1 terms are summed to get the Elastic-Net penalty, which is then rounded to 4 decimal places and returned as a float. This ensures the result is precise and formatted correctly."}
{"task_id": 458, "completion_id": 0, "completion": "To solve this problem, we need to convert a one-dimensional array-like object of non-negative integer labels into a two-dimensional one-hot encoded matrix. The one-hot encoding is a binary matrix where each row has a single `1` at the index corresponding to the original class label, and `0` elsewhere. \n\n### Approach\n1. **Input Handling**: Convert the input labels into a numpy array for easier manipulation, regardless of whether the input is a list, tuple, or numpy array.\n2. **Check for Negative Labels**: If any label is negative, return -1 immediately as negative labels are invalid.\n3. **Determine Number of Classes**: If the number of classes (`num_classes`) is not provided, infer it as the maximum label value plus one. If `num_classes` is provided but is insufficient (smaller than the maximum label value plus one), return -1.\n4. **Generate One-Hot Encoded Matrix**: Create a list of lists where each row corresponds to a label, filled with `0`s except for a `1` at the index of the label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    labels_np = np.array(labels)\n    \n    # Handle empty labels case\n    if labels_np.size == 0:\n        return []\n    \n    # Check for any negative labels\n    if (labels_np < 0).any():\n        return -1\n    \n    max_label = labels_np.max()\n    \n    # Determine num_classes and validate if provided\n    if num_classes is not None:\n        if num_classes < max_label + 1:\n            return -1\n    else:\n        num_classes = max_label + 1\n    \n    # Generate one-hot encoded lists\n    result = []\n    for label in labels_np:\n        row = [0] * num_classes\n        row[label] = 1\n        result.append(row)\n    \n    return result\n```\n\n### Explanation\n1. **Input Conversion**: The input labels are converted to a numpy array to handle all input types uniformly.\n2. **Negative Label Check**: Using numpy operations, we efficiently check for any negative values in the labels.\n3. **Number of Classes Calculation**: The number of classes is either inferred from the maximum label value or validated against the provided value. If the provided value is too small, the function returns -1.\n4. **Matrix Construction**: For each label, a row of zeros is created with a `1` at the index corresponding to the label. This row is then added to the result list.\n\nThis approach ensures that the function efficiently handles all edge cases, including empty input and invalid class numbers, while converting the labels into the required one-hot encoded format."}
{"task_id": 459, "completion_id": 0, "completion": "To solve this problem, we need to implement a k-Nearest Neighbors (k-NN) classifier using a max-heap to efficiently keep track of the k closest training samples for each validation sample. The goal is to classify each validation sample by majority vote of its k nearest neighbors and compute the accuracy of the predictions.\n\n### Approach\n1. **Compute Distances**: For each validation sample, compute the Euclidean distance to every training sample.\n2. **Maintain Max-Heap**: Use a max-heap (implemented as a min-heap with negative distances) to keep track of the k smallest distances encountered. This allows efficient insertion and removal of elements in O(log k) time.\n3. **Majority Vote**: Extract the labels of the k nearest neighbors from the heap and use a majority vote to determine the predicted label. In case of ties, the smallest label ID is chosen.\n4. **Compute Accuracy**: Compare the predicted labels with the true labels to compute the accuracy, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nimport heapq\n\ndef knn_validate_heap(X_train: np.ndarray,\n                      Y_train: np.ndarray,\n                      X_val: np.ndarray,\n                      Y_val: np.ndarray,\n                      k: int) -> tuple[float, list[int]]:\n    predicted_labels = []\n    n_val = X_val.shape[0]\n    n_train = X_train.shape[0]\n    \n    for i in range(n_val):\n        x_val = X_val[i]\n        heap = []\n        for j in range(n_train):\n            x_train = X_train[j]\n            distance = np.linalg.norm(x_val - x_train)\n            if len(heap) < k:\n                heapq.heappush(heap, (-distance, j))\n            else:\n                current_max_neg = heap[0][0]\n                if distance < -current_max_neg:\n                    heapq.heappushpop(heap, (-distance, j))\n        \n        neighbors = [item[1] for item in heap]\n        labels = Y_train[neighbors]\n        counts = np.bincount(labels)\n        majority = np.argmax(counts)\n        predicted_labels.append(int(majority))\n    \n    accuracy = np.mean(np.array(predicted_labels) == Y_val)\n    accuracy_rounded = round(accuracy, 4)\n    return (accuracy_rounded, predicted_labels)\n```\n\n### Explanation\n1. **Distance Calculation**: For each validation sample, we compute the Euclidean distance to every training sample using `np.linalg.norm`.\n2. **Heap Management**: We use a max-heap (implemented with negative distances) to maintain the k smallest distances. If the heap size exceeds k, we replace the largest element with a smaller distance if found.\n3. **Majority Voting**: After collecting the k nearest neighbors, we use `np.bincount` to count occurrences of each label and `np.argmax` to determine the most frequent label, which handles ties by selecting the smallest label ID.\n4. **Accuracy Calculation**: The accuracy is computed by comparing the predicted labels with the true labels and rounding the result to four decimal places.\n\nThis approach efficiently manages the nearest neighbors using a heap, ensuring optimal performance for maintaining the k smallest distances while iterating through the training samples. The majority vote mechanism ensures correct classification even in the case of ties."}
{"task_id": 460, "completion_id": 0, "completion": "To solve this problem, we need to compute the Pearson correlation matrix between the columns of two data matrices, X and Y. If Y is not provided, we compute the correlation matrix of X with itself. The correlation coefficient is undefined if any column in X or Y has a zero standard deviation, in which case we return -1.\n\n### Approach\n1. **Check Dimensions**: Ensure that if Y is provided, it has the same number of rows as X. If not, return -1.\n2. **Mean and Standard Deviation Calculation**: Compute the mean and standard deviation for each column in X and Y. If any column has a zero standard deviation, return -1.\n3. **Centering Data**: Subtract the column means from the respective columns to center the data around zero.\n4. **Covariance Matrix Calculation**: Compute the covariance matrix using the centered data.\n5. **Correlation Matrix Calculation**: Derive the correlation matrix by normalizing the covariance matrix with the product of the standard deviations of the respective columns.\n6. **Rounding and Conversion**: Round the correlation values to four decimal places and convert the result to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X: np.ndarray, Y: np.ndarray | None = None) -> list | int:\n    \"\"\"Compute the Pearson correlation matrix between the columns of *X* and *Y*.\n\n    The function falls back to \u03c1(X, X) when *Y* is omitted.  Zero standard\n    deviation in any column leads to an undefined correlation; the function\n    must then return `-1`.\n\n    Args:\n        X: 2-D numpy array of shape (n_samples, n_features_X).\n        Y: Optional 2-D numpy array of shape (n_samples, n_features_Y).\n\n    Returns:\n        A list of lists with the correlation coefficients rounded to four\n        decimals, or `-1` if the computation is not possible.\n    \"\"\"\n    n_samples = X.shape[0]\n    \n    # Check if Y has the same number of rows as X when provided\n    if Y is not None and Y.shape[0] != n_samples:\n        return -1\n    \n    # Compute X's statistics\n    X_means = X.mean(axis=0)\n    X_centered = X - X_means\n    X_std = np.sqrt((X_centered ** 2).mean(axis=0))\n    if np.any(X_std == 0):\n        return -1\n    \n    # Handle Y\n    if Y is None:\n        Y_centered = X_centered\n        Y_std = X_std\n    else:\n        Y_means = Y.mean(axis=0)\n        Y_centered = Y - Y_means\n        Y_std = np.sqrt((Y_centered ** 2).mean(axis=0))\n        if np.any(Y_std == 0):\n            return -1\n    \n    # Compute covariance matrix\n    cov_matrix = (X_centered.T @ Y_centered) / n_samples\n    \n    # Compute correlation matrix\n    corr_matrix = cov_matrix / (X_std[:, np.newaxis] * Y_std[np.newaxis, :])\n    \n    # Round and convert to list of lists\n    rounded_corr = np.round(corr_matrix, 4)\n    return rounded_corr.tolist()\n```\n\n### Explanation\n1. **Check Dimensions**: The function first checks if Y is provided and ensures it has the same number of rows as X. If not, it returns -1.\n2. **Mean and Standard Deviation**: The mean of each column in X is computed, and the data is centered by subtracting these means. The standard deviation of each column is then calculated. If any column has a zero standard deviation, the function returns -1.\n3. **Handling Y**: If Y is not provided, it defaults to X. Otherwise, the same process of centering and standard deviation calculation is applied to Y.\n4. **Covariance Matrix**: The covariance matrix is computed using matrix multiplication of the centered data matrices.\n5. **Correlation Matrix**: The covariance matrix is normalized by the product of the standard deviations of the respective columns to form the correlation matrix.\n6. **Rounding and Conversion**: The correlation values are rounded to four decimal places and converted to a list of lists for the final output."}
{"task_id": 461, "completion_id": 0, "completion": "To solve this problem, we need to implement a gradient boosting regressor using decision stumps (depth-1 regression trees) for one-dimensional data. The algorithm uses square loss as the objective function and updates the predictions iteratively by fitting decision stumps to the residuals of the current predictions.\n\n### Approach\n1. **Initial Prediction**: Start with the mean of the target values as the initial prediction for all samples.\n2. **Iterative Boosting**: For each boosting iteration (up to `n_estimators` times):\n   - **Residual Calculation**: Compute the residuals as the difference between the true target values and the current predictions.\n   - **Early Stopping**: If all residuals are zero, stop the iterations early.\n   - **Split Candidates**: Generate potential split points from the unique values of the feature, excluding the maximum value to avoid empty splits.\n   - **Best Split Selection**: For each candidate split, compute the sum of squared errors (SSE) and select the split that minimizes SSE. If no valid splits are found, use the mean residual for the entire dataset.\n   - **Update Predictions**: Adjust the current predictions using the decision stump predictions scaled by the learning rate.\n3. **Final Predictions**: Return the final predictions rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    x = np.array(x)\n    y = np.array(y)\n    n_samples = len(x)\n    if n_samples == 0:\n        return []\n    y_pred = np.full(n_samples, np.mean(y))\n    if n_estimators <= 0:\n        return np.round(y_pred, 4).tolist()\n    for _ in range(n_estimators):\n        r = y - y_pred\n        if np.all(r == 0):\n            break\n        unique_x = np.unique(x)\n        split_candidates = unique_x[:-1] if len(unique_x) > 1 else []\n        best_sse = float('inf')\n        best_t = None\n        best_vL = None\n        best_vR = None\n        sum_r = np.sum(r)\n        sum_r_sq = np.sum(r ** 2)\n        if len(split_candidates) == 0:\n            v = np.mean(r)\n            stump_pred = np.full(n_samples, v)\n        else:\n            for t in split_candidates:\n                left_mask = x <= t\n                count_left = np.sum(left_mask)\n                if count_left == 0 or count_left == n_samples:\n                    continue\n                sum_left = np.sum(r[left_mask])\n                sum_left_sq = np.sum(r[left_mask] ** 2)\n                count_right = n_samples - count_left\n                sum_right = sum_r - sum_left\n                sum_right_sq = sum_r_sq - sum_left_sq\n                sse_left = sum_left_sq - (sum_left ** 2) / count_left\n                sse_right = sum_right_sq - (sum_right ** 2) / count_right\n                total_sse = sse_left + sse_right\n                if (total_sse < best_sse) or (total_sse == best_sse and (best_t is None or t < best_t)):\n                    best_sse = total_sse\n                    best_t = t\n                    best_vL = sum_left / count_left\n                    best_vR = sum_right / count_right\n            if best_t is not None:\n                stump_pred = np.where(x <= best_t, best_vL, best_vR)\n            else:\n                v = np.mean(r)\n                stump_pred = np.full(n_samples, v)\n        y_pred += learning_rate * stump_pred\n    return np.round(y_pred, 4).tolist()\n```\n\n### Explanation\n1. **Initial Prediction**: The initial prediction for all samples is set to the mean of the target values.\n2. **Residual Calculation**: For each iteration, residuals are computed as the difference between the actual target values and the current predictions.\n3. **Split Candidates**: Potential split points are derived from the unique values of the feature, excluding the maximum value to ensure non-empty splits.\n4. **Best Split Selection**: For each candidate split, the sum of squared errors (SSE) is calculated. The split with the minimum SSE is chosen. If no valid splits are found, the mean residual is used.\n5. **Prediction Update**: The predictions are updated using the decision stump predictions scaled by the learning rate.\n6. **Early Stopping**: If all residuals become zero, the iterations stop early to save computation.\n\nThis approach efficiently builds an ensemble of decision stumps to iteratively improve predictions, leveraging gradient boosting principles for regression tasks."}
{"task_id": 462, "completion_id": 0, "completion": "To solve this problem, we need to implement a sinusoidal cepstral lifter that post-processes a matrix of Mel-Frequency Cepstral Coefficients (MFCCs). The liftering process smooths the spectral envelope by attenuating higher-order coefficients while leaving the lower ones nearly unchanged. The liftering operation is defined using a sinusoidal function applied column-wise to the MFCC matrix.\n\n### Approach\n1. **Special Cases Handling**: When the lifter parameter \\( D \\) is 0 or 1, the input MFCC matrix should be returned unchanged. This is because \\( D = 0 \\) leads to a division by zero error in the sinusoidal function, and \\( D = 1 \\) results in all lifter values being 1, which does not change the input.\n2. **Lifter Calculation**: For other values of \\( D \\), compute the lifter values using the given sinusoidal formula. The lifter values are calculated for each column index \\( n \\) from 0 to \\( C-1 \\) (where \\( C \\) is the number of coefficients).\n3. **Element-wise Multiplication**: Multiply each column of the MFCC matrix by the corresponding lifter value. This is done efficiently using NumPy's broadcasting capabilities.\n4. **Data Type Preservation**: Ensure the output matrix has the same data type as the input matrix by casting the result back to the original data type after multiplication.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cepstral_lifter(mfccs: np.ndarray, D: int) -> np.ndarray:\n    \"\"\"Apply a sinusoidal cepstral lifter to a matrix of MFCC coefficients.\n\n    Args:\n        mfccs: NumPy array of shape (G, C) containing MFCC coefficients.\n        D: Non-negative integer lifter parameter.  A value of 0 (or 1) should\n            return the input unchanged.\n\n    Returns:\n        NumPy array with the same shape and dtype as *mfccs* containing the\n        liftered coefficients.\n    \"\"\"\n    if D == 0 or D == 1:\n        return mfccs.copy()\n    \n    C = mfccs.shape[1]\n    n = np.arange(C)\n    lifter = 1 + (D / 2) * np.sin(np.pi * n / D)\n    lifted = mfccs * lifter\n    return lifted.astype(mfccs.dtype)\n```\n\n### Explanation\n1. **Special Cases Handling**: The code first checks if \\( D \\) is 0 or 1. If so, it returns a copy of the input matrix to avoid modifying the original data.\n2. **Lifter Calculation**: For other values of \\( D \\), the code computes the lifter values using the formula \\( 1 + \\frac{D}{2} \\sin\\left(\\frac{\\pi n}{D}\\right) \\) for each column index \\( n \\).\n3. **Element-wise Multiplication**: The MFCC matrix is multiplied element-wise by the computed lifter values using NumPy broadcasting, ensuring each column is scaled appropriately.\n4. **Data Type Preservation**: The result is cast back to the original data type of the input matrix to maintain consistency, even if the input was of an integer type (though MFCCs are typically floating-point values).\n\nThis approach efficiently handles the liftering process while ensuring the output maintains the same shape and data type as the input, adhering to the problem constraints."}
{"task_id": 463, "completion_id": 0, "completion": "To solve this problem, we need to compute the radius of a Ball-tree node, which is defined as the maximum Euclidean distance between the centroid of the node and any data point stored in the node. The result should be rounded to the nearest four decimal places. If the data matrix is empty, the radius is defined to be 0.0.\n\n### Approach\n1. **Check for Empty Data Matrix**: If the data matrix (X) is empty, return 0.0 immediately.\n2. **Compute Distances**: For each point in the data matrix, compute the Euclidean distance from the centroid. This involves summing the squared differences between each coordinate of the centroid and the point, then taking the square root of the sum.\n3. **Track Maximum Distance**: Keep track of the maximum distance encountered during the iteration over all points.\n4. **Round Result**: Round the maximum distance to four decimal places using Python's built-in `round` function.\n\n### Solution Code\n```python\nimport math\n\ndef compute_ball_radius(centroid: list[float], X: list[list[float]]) -> float:\n    \"\"\"Compute the maximum Euclidean distance between centroid and points in X, rounded to 4 decimals.\"\"\"\n    if not X:\n        return 0.0\n    \n    max_distance = 0.0\n    for point in X:\n        sum_squares = 0.0\n        for c, x in zip(centroid, point):\n            sum_squares += (c - x) ** 2\n        distance = math.sqrt(sum_squares)\n        if distance > max_distance:\n            max_distance = distance\n    \n    return round(max_distance, 4)\n```\n\n### Explanation\n1. **Check for Empty Data Matrix**: The function first checks if the data matrix (X) is empty. If it is, the function returns 0.0 immediately.\n2. **Iterate Over Points**: For each point in the data matrix, the function calculates the Euclidean distance from the centroid. This is done by iterating over each coordinate of the centroid and the corresponding point, computing the squared differences, summing them up, and then taking the square root of the sum to get the Euclidean distance.\n3. **Track Maximum Distance**: During the iteration, the function keeps track of the maximum distance encountered. If a newly computed distance is greater than the current maximum, it updates the maximum.\n4. **Round Result**: Finally, the maximum distance is rounded to four decimal places using Python's `round` function and returned as the result.\n\nThis approach ensures that we efficiently compute the required radius while adhering to the problem constraints and requirements."}
{"task_id": 464, "completion_id": 0, "completion": "To solve this problem, we need to implement a basic k-Nearest Neighbors (k-NN) classifier to predict the labels of a validation set using the majority vote of the k closest training points. The solution involves calculating distances between each validation sample and all training samples, identifying the k nearest neighbors, and using their labels to predict the validation sample's label through majority voting.\n\n### Approach\n1. **Compute Distances**: Calculate the squared Euclidean distances between each validation sample and all training samples. This avoids the computational cost of taking square roots while maintaining the order of distances.\n2. **Find Nearest Neighbors**: For each validation sample, determine the indices of the k smallest distances using argsort.\n3. **Majority Vote**: For each validation sample, gather the labels of the k nearest neighbors and determine the most frequent label using bincount and argmax, which also handles ties by selecting the smallest label.\n4. **Calculate Accuracy**: Compare the predicted labels with the true labels of the validation set to compute the classification accuracy, rounded to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_validate(X_train: np.ndarray,\n                 Y_train: np.ndarray,\n                 X_val: np.ndarray,\n                 Y_val: np.ndarray,\n                 k: int) -> tuple[float, list[int]]:\n    # Compute pairwise squared Euclidean distances\n    diff = X_val[:, np.newaxis, :] - X_train  # Shape (n_val, n_train, n_features)\n    squared_distances = np.sum(diff ** 2, axis=2)  # Shape (n_val, n_train)\n    \n    # Get indices of the k smallest distances for each validation sample\n    k_nearest_indices = np.argsort(squared_distances, axis=1)[:, :k]  # Shape (n_val, k)\n    \n    # Get the labels of the k nearest neighbors\n    nearest_labels = Y_train[k_nearest_indices]  # Shape (n_val, k)\n    \n    # Predict labels using majority vote\n    predicted_labels = []\n    for labels_row in nearest_labels:\n        counts = np.bincount(labels_row)\n        predicted = np.argmax(counts)\n        predicted_labels.append(predicted)\n    \n    # Compute accuracy\n    correct_predictions = np.array(predicted_labels) == Y_val\n    accuracy = correct_predictions.mean()\n    accuracy_rounded = round(accuracy, 4)\n    \n    return (accuracy_rounded, predicted_labels)\n```\n\n### Explanation\n1. **Distance Calculation**: By broadcasting, we efficiently compute the squared Euclidean distances between each validation sample and all training samples. This results in a matrix where each entry (i, j) represents the squared distance between the i-th validation sample and j-th training sample.\n2. **Nearest Neighbor Indices**: Using argsort on each row of the distance matrix, we obtain the indices of the k smallest distances, which correspond to the k nearest neighbors.\n3. **Label Aggregation**: For each validation sample, we gather the labels of its k nearest neighbors and determine the majority label using bincount to count occurrences and argmax to resolve ties by selecting the smallest label.\n4. **Accuracy Calculation**: The accuracy is computed by comparing the predicted labels with the true labels and taking the mean of correct predictions, which is then rounded to 4 decimal places for the final result.\n\nThis approach efficiently leverages vectorized operations in NumPy to handle the distance calculations and sorting, ensuring the solution is both concise and performant."}
{"task_id": 465, "completion_id": 0, "completion": "To solve this problem, we need to implement a higher-order function that generates a Gaussian Radial Basis Function (RBF) kernel. This function will take a parameter `gamma` and return another function that computes the RBF similarity between two vectors using the given `gamma`.\n\n### Approach\nThe Gaussian RBF kernel function measures the similarity between two vectors using the formula:\n\n\\[ f(x1, x2) = \\exp(-\\gamma \\cdot ||x1 - x2||^2) \\]\n\nwhere \\( ||x1 - x2|| \\) is the Euclidean distance between the vectors \\( x1 \\) and \\( x2 \\). The steps to implement this are as follows:\n\n1. **Convert Inputs to Arrays**: Convert the input vectors \\( x1 \\) and \\( x2 \\) into NumPy arrays to handle element-wise operations efficiently.\n2. **Compute Squared Euclidean Distance**: Calculate the squared Euclidean distance between the two vectors by subtracting them, squaring each element, and summing the results.\n3. **Apply Gamma and Exponential**: Multiply the squared distance by \\(-\\gamma\\) and compute the exponential of the result.\n4. **Round the Result**: Round the computed value to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rbf_kernel(gamma: float, **kwargs):\n    \"\"\"Create and return a Gaussian RBF kernel function.\n\n    The returned function takes two 1-D vectors and computes the Gaussian\n    Radial Basis Function (RBF) similarity between them.\n\n    Args:\n        gamma: A non-negative real number that controls the width of the RBF\n               kernel. A larger gamma makes the kernel decay faster.\n        **kwargs: Optional keyword arguments kept for interface compatibility\n                   (not used in the computation).\n\n    Returns:\n        Function ``f(x1, x2)`` such that ``f`` returns the value of\n        ``exp( -gamma * ||x1 - x2||^2 )`` rounded to four decimal places.\n    \"\"\"\n    def f(x1, x2):\n        x1 = np.asarray(x1)\n        x2 = np.asarray(x2)\n        squared_distance = np.sum((x1 - x2) ** 2)\n        result = np.exp(-gamma * squared_distance)\n        return round(result, 4)\n    return f\n```\n\n### Explanation\n- **Conversion to Arrays**: The function converts the input vectors to NumPy arrays to leverage efficient array operations.\n- **Squared Distance Calculation**: The squared Euclidean distance is computed by element-wise subtraction, squaring each element, and summing the results. This avoids computing the square root, which is unnecessary for the squared norm.\n- **Exponential Application**: The computed squared distance is scaled by \\(-\\gamma\\) and passed through the exponential function to get the RBF similarity value.\n- **Rounding**: The result is rounded to four decimal places using Python's `round` function to ensure the output meets the specified precision.\n\nThis approach efficiently computes the RBF kernel similarity while handling edge cases such as `gamma` being zero, which naturally results in a similarity of 1.0 after rounding."}
{"task_id": 466, "completion_id": 0, "completion": "To solve this problem, we need to create a function that removes all punctuation characters from a given string and trims any leading or trailing whitespace from the result. If the input string consists solely of punctuation and/or whitespace, the function should return an empty string.\n\n### Approach\n1. **Remove Punctuation**: Use Python's `str.translate()` method with a precomputed translation table that maps all punctuation characters to `None`, effectively removing them from the string.\n2. **Trim Whitespace**: After removing punctuation, use the `str.strip()` method to remove any leading and trailing whitespace from the resulting string.\n3. **Handle Edge Cases**: If the input string is empty or contains only punctuation and whitespace, the combination of the above steps will naturally result in an empty string, which meets the problem's requirements.\n\n### Solution Code\n```python\nimport string\n\n# Build the translation table once so it can be reused efficiently by every\n# call to ``strip_punctuation``.\n_PUNC_TABLE: dict[int, None] = str.maketrans('', '', string.punctuation)\n\ndef strip_punctuation(line: str) -> str:\n    \"\"\"Remove punctuation characters from a string.\n\n    The function deletes all characters listed in ``string.punctuation`` and\n    trims surrounding whitespace.\n\n    Args:\n        line (str): Text that may contain punctuation symbols.\n\n    Returns:\n        str: Text with punctuation removed and stripped of leading/trailing\n        whitespace.\n    \"\"\"\n    return line.translate(_PUNC_TABLE).strip()\n```\n\n### Explanation\n- **Translation Table**: The translation table `_PUNC_TABLE` is created once using `str.maketrans('', '', string.punctuation)`. This table maps each punctuation character to `None`, which instructs the `translate()` method to remove these characters.\n- **String Processing**: The input string is processed using `translate(_PUNC_TABLE)`, which removes all punctuation. The result is then stripped of leading and trailing whitespace using `strip()`.\n- **Efficiency**: By precomputing the translation table, the function ensures efficient processing for each call, making it suitable for repeated use in applications requiring high performance.\n\nThis approach ensures that the function efficiently handles the removal of punctuation and whitespace trimming in a straightforward and Pythonic manner."}
{"task_id": 467, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that calculates both the L1 and L2 regularization penalties and their gradients for a given list of model weights. Regularization is a technique used to prevent overfitting in machine learning models by penalizing large weights. The L1 regularization (lasso) adds a penalty proportional to the absolute value of the weights, while the L2 regularization (ridge) adds a penalty proportional to the square of the weights.\n\n### Approach\n1. **Check Norm Type**: Determine if the specified norm is either \"l1\" or \"l2\". If not, return -1 as specified.\n2. **Convert Weights to Array**: Convert the input list of weights to a NumPy array for efficient numerical operations.\n3. **Calculate Penalty and Gradient**:\n   - **L1 Regularization**: Compute the penalty as the sum of absolute values of the weights multiplied by the regularization strength `C`. The gradient for each weight is `C` multiplied by the sign of the weight.\n   - **L2 Regularization**: Compute the penalty as half of the sum of squares of the weights multiplied by `C`. The gradient for each weight is `C` multiplied by the weight itself.\n4. **Rounding**: Round the penalty and each element of the gradient to four decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef regularization(weights: list[float | int], C: float = 0.01, norm: str = \"l2\") -> tuple[float, list[float]]:\n    \"\"\"Return the regularisation penalty and gradient for a list of weights.\n\n    Parameters\n    ----------\n    weights : list[float | int]\n        Vector of model weights.\n    C : float, default 0.01\n        Regularisation strength.\n    norm : str, {\"l1\", \"l2\"}, default \"l2\"\n        Type of regularisation to apply.\n\n    Returns\n    -------\n    tuple[float, list[float]]\n        (penalty, gradient). Round every value to 4 decimals.\n        If *norm* is unsupported, return -1 instead.\n    \"\"\"\n    if norm not in (\"l1\", \"l2\"):\n        return -1\n    w = np.array(weights)\n    if norm == \"l1\":\n        penalty = C * np.abs(w).sum()\n        gradient = C * np.sign(w)\n    else:\n        penalty = 0.5 * C * (w ** 2).sum()\n        gradient = C * w\n    penalty_rounded = round(penalty.item(), 4)\n    gradient_rounded = [round(g, 4) for g in gradient.tolist()]\n    return (penalty_rounded, gradient_rounded)\n```\n\n### Explanation\n- **Check Norm Type**: The function first checks if the provided norm is either \"l1\" or \"l2\". If not, it returns -1.\n- **Conversion to NumPy Array**: The weights list is converted to a NumPy array to leverage efficient vector operations.\n- **Penalty Calculation**:\n  - For L1, the penalty is computed using the sum of absolute values of the weights.\n  - For L2, the penalty is computed using half the sum of squares of the weights.\n- **Gradient Calculation**:\n  - For L1, the gradient is derived using the sign of each weight.\n  - For L2, the gradient is directly proportional to the weight values.\n- **Rounding**: Both the penalty and each element of the gradient are rounded to four decimal places to meet the problem's precision requirements.\n\nThis approach ensures that the function efficiently computes the regularization penalties and gradients while adhering to the specified constraints and precision requirements."}
{"task_id": 468, "completion_id": 0, "completion": "To solve this problem, we need to implement a binary logistic regression model from scratch using NumPy and gradient descent optimization. The model will learn parameters (weights and bias) from the training data and use these parameters to predict class labels for the test data.\n\n### Approach\n1. **Data Preprocessing**: \n   - Add a bias term (a column of 1s) to both the training and test feature matrices. This allows the model to learn the bias term as part of the weight vector.\n   - Ensure the training labels are in the correct shape (a column vector).\n\n2. **Model Initialization**:\n   - Initialize the weight vector (including the bias term) to zeros. This ensures the algorithm starts from a deterministic point.\n\n3. **Gradient Descent Optimization**:\n   - Compute the predicted probabilities using the logistic sigmoid function.\n   - Calculate the gradient of the loss with respect to the weights.\n   - Update the weights iteratively using the gradient descent rule.\n\n4. **Prediction**:\n   - Use the learned weights to compute probabilities for the test data.\n   - Convert these probabilities to binary class labels (0 or 1) based on a threshold of 0.5.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        learning_rate: float = 0.1,\n        num_iters: int = 10000) -> list[int]:\n    # Ensure y_train is a column vector\n    if y_train.ndim == 1:\n        y_train = y_train.reshape(-1, 1)\n    \n    # Add bias term to X_train and X_test\n    X_train_bias = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n    X_test_bias = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n    \n    # Initialize weights (including bias) to 0.0\n    n_features = X_train_bias.shape[1]\n    W = np.zeros((n_features, 1))\n    \n    m = X_train_bias.shape[0]  # Number of training examples\n    \n    # Gradient descent optimization\n    for _ in range(num_iters):\n        # Compute predictions\n        Z = X_train_bias @ W\n        h = 1 / (1 + np.exp(-Z))\n        \n        # Compute gradient\n        error = h - y_train\n        dW = (X_train_bias.T @ error) / m\n        \n        # Update weights\n        W -= learning_rate * dW\n    \n    # Predict on test set\n    Z_test = X_test_bias @ W\n    probabilities = 1 / (1 + np.exp(-Z_test))\n    y_pred = (probabilities >= 0.5).astype(int)\n    \n    return y_pred.flatten().tolist()\n```\n\n### Explanation\n1. **Data Preprocessing**: The training and test datasets are augmented with a column of ones to account for the bias term. This allows the model to learn the bias as part of the weight vector during training.\n\n2. **Weight Initialization**: The weights (including the bias) are initialized to zero, ensuring the algorithm starts from a deterministic point.\n\n3. **Gradient Descent**: For each iteration, the model computes the predicted probabilities using the logistic sigmoid function. The gradient of the loss with respect to the weights is calculated using the formula provided. The weights are updated iteratively using this gradient and the learning rate.\n\n4. **Prediction**: After training, the model computes the probabilities for the test data. These probabilities are converted to binary class labels using a threshold of 0.5. The final predictions are returned as a list of integers (0 or 1).\n\nThis approach ensures that the model learns the optimal parameters through gradient descent and efficiently predicts class labels for new data points."}
{"task_id": 469, "completion_id": 0, "completion": "To solve this problem, we need to create a factory function that returns a callable to compute the linear kernel (dot product) between two vectors. This callable should handle inputs as 1-D lists or NumPy arrays, convert them to NumPy arrays, check their lengths, and compute their dot product. If the vectors have different lengths, the callable should return -1.\n\n### Approach\n1. **Factory Function**: The factory function `linear_kernel` takes any keyword arguments (ignored) and returns a callable function `f`.\n2. **Input Conversion**: The inner function `f` converts the input vectors to NumPy arrays of type float and flattens them to ensure they are 1-D.\n3. **Length Check**: The inner function checks if the flattened arrays have the same length. If not, it returns -1.\n4. **Dot Product Calculation**: If the lengths match, the dot product is computed using `np.inner` and converted to a Python float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_kernel(**kwargs) -> callable:\n    \"\"\"Return a callable that computes the linear kernel (dot product).\n\n    The factory keeps a flexible signature (accepting **kwargs) so that it can\n    be used interchangeably with other kernel constructors that may require\n    hyper-parameters.\n\n    Returns:\n        Callable[[array_like, array_like], float]: A function `f(x1, x2)` that\n        returns the dot product of `x1` and `x2`.  If the vectors are of\n        different lengths, the callable must return \u20111.\n    \"\"\"\n    def f(x1, x2):\n        x1 = np.asarray(x1, dtype=float).flatten()\n        x2 = np.asarray(x2, dtype=float).flatten()\n        if x1.size != x2.size:\n            return -1\n        return float(np.inner(x1, x2))\n    return f\n```\n\n### Explanation\n1. **Factory Function**: `linear_kernel` is designed to ignore any keyword arguments, making it compatible with other kernel functions that might require hyper-parameters.\n2. **Conversion and Flattening**: The inner function `f` converts inputs to NumPy arrays and flattens them to handle any shape discrepancies, ensuring they are treated as 1-D vectors.\n3. **Length Check**: By comparing the sizes of the flattened arrays, we ensure that the vectors are of the same length before computing the dot product.\n4. **Dot Product Calculation**: Using `np.inner`, we compute the dot product efficiently and convert the result to a Python float to meet the output requirements.\n\nThis approach ensures that the function handles various input types correctly, checks vector lengths efficiently, and returns the appropriate result in the required format."}
{"task_id": 470, "completion_id": 0, "completion": "To solve this problem, we need to determine the absolute, canonical directory of a given file path. This involves resolving any symbolic links, normalizing the path by removing redundant segments like \"..\" and \".\", and ensuring the result is an absolute path without a trailing separator (except for the root directory).\n\n### Approach\nThe approach involves two main steps:\n1. **Resolve the Canonical Path**: Convert the given file path to its canonical absolute form using `os.path.realpath()`. This function resolves symbolic links and normalizes the path by eliminating redundant segments.\n2. **Extract the Directory Part**: Use `os.path.dirname()` to extract the directory part of the canonical path. This function handles the path string to return the parent directory of the given path, effectively removing the last component.\n\n### Solution Code\n```python\nimport os\n\ndef get_scriptdir(file_path: str) -> str:\n    \"\"\"Return the absolute directory that contains *file_path*.\n\n    The function normalises *file_path* by resolving any symbolic links\n    and removing ``..`` / ``.`` path segments, then returns only the\n    directory component of this canonical path.\n\n    Args:\n        file_path: Path string pointing to a file.  May be absolute or\n                    relative and may contain symlinks or redundant path\n                    elements.\n\n    Returns:\n        The absolute directory path that contains *file_path*.  The\n        returned path never ends with a path separator (unless it is the\n        root directory).\n    \"\"\"\n    canonical_path = os.path.realpath(file_path)\n    directory = os.path.dirname(canonical_path)\n    return directory\n```\n\n### Explanation\n1. **Resolving the Canonical Path**: The function `os.path.realpath()` is used to resolve the given file path into its absolute form. This process includes expanding symbolic links and normalizing the path by removing any \".\" and \"..\" segments, as well as trailing separators.\n2. **Extracting the Directory**: The function `os.path.dirname()` is then applied to the resolved path. This function splits the path into head and tail components, where the head is everything except the last part of the path. This effectively gives the parent directory of the resolved path, ensuring that even if the input path was a directory (with or without a trailing separator), the result is the parent directory. The root directory is an exception where the result will include a trailing separator, which is allowed as per the problem constraints.\n\nThis approach ensures that the solution is efficient and leverages Python's built-in functions to handle path resolution and directory extraction correctly for various edge cases."}
{"task_id": 471, "completion_id": 0, "completion": "To solve this problem, we need to implement the Expected SARSA TD(0) algorithm for updating a Q-table in a tabular setting. The algorithm involves computing the expected value of the next state using an \u03b5-soft policy derived from the current Q-table and updating the Q-value of the current state-action pair accordingly.\n\n### Approach\n1. **Copy the Q-table**: Create a deep copy of the Q-table to avoid modifying the original input directly.\n2. **Compute the target value**:\n   - If the next state is terminal (i.e., `next_state` is `None`), the target is just the immediate reward.\n   - If the next state is non-terminal, compute the expected value of the Q-values for the next state using the \u03b5-soft policy. This involves determining the greedy action (the first action with the maximum Q-value) and calculating the probabilities for each action based on the \u03b5-soft policy.\n3. **Update the Q-value**: Adjust the Q-value for the current state-action pair using the learning rate and the computed target.\n4. **Round the values**: Ensure all values in the updated Q-table are rounded to four decimal places for consistency.\n\n### Solution Code\n```python\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    # Make a copy of the Q-table to modify\n    new_q = [row.copy() for row in q_table]\n    \n    # Compute the target\n    if next_state is None:\n        target = reward\n    else:\n        s_prime = next_state\n        q_s_prime = new_q[s_prime]\n        num_actions = len(q_s_prime)\n        max_q = max(q_s_prime)\n        a_greedy = q_s_prime.index(max_q)  # first occurrence of max_q\n        \n        expectation = 0.0\n        for a_prime in range(num_actions):\n            if a_prime == a_greedy:\n                prob = (1 - epsilon) + (epsilon / num_actions)\n            else:\n                prob = epsilon / num_actions\n            expectation += prob * q_s_prime[a_prime]\n        \n        target = reward + gamma * expectation\n    \n    # Update the Q-value\n    new_q[state][action] += lr * (target - new_q[state][action])\n    \n    # Round all entries to four decimal places\n    for row in new_q:\n        for i in range(len(row)):\n            row[i] = round(row[i], 4)\n    \n    return new_q\n```\n\n### Explanation\n1. **Copying the Q-table**: This step ensures that we do not modify the original Q-table, allowing the function to return a new updated table without side effects.\n2. **Handling terminal state**: If the next state is terminal, the future value is zero, so the target is just the immediate reward.\n3. **Non-terminal state processing**: For non-terminal states, the algorithm calculates the expected value using the \u03b5-soft policy. The greedy action is identified as the first action with the highest Q-value, and probabilities for each action are computed based on whether they are greedy or not.\n4. **Updating the Q-value**: The Q-value for the current state-action pair is adjusted using the learning rate and the difference between the target and the current Q-value.\n5. **Rounding**: All values in the Q-table are rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the necessary updates while adhering to the constraints and requirements of the Expected SARSA algorithm."}
{"task_id": 472, "completion_id": 0, "completion": "To solve this problem, we need to implement the numerically stable softmax function along with its element-wise gradient. The softmax function converts raw scores into a valid probability distribution, and the gradient is computed for each element based on the softmax probabilities. The solution must handle both one-dimensional and two-dimensional inputs, applying the transformations row-wise for two-dimensional inputs.\n\n### Approach\n1. **Convert Input to Numpy Array**: This allows for efficient numerical operations and easy manipulation of both 1D and 2D arrays.\n2. **Compute Max Values**: Subtract the maximum value along the last axis to ensure numerical stability when exponentiating. This is done using `np.max` with `keepdims=True` to maintain the correct shape for broadcasting.\n3. **Exponentiate and Normalize**: Compute the exponentials of the shifted values and normalize them by their sum along the last axis to get the softmax probabilities.\n4. **Compute Gradients**: Use the formula for the gradient of the softmax function, which is `prob * (1 - prob)` for each element.\n5. **Round and Convert to Lists**: Round the results to four decimal places and convert the numpy arrays back to Python lists to maintain the required output format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softmax(x: list | list[list]):\n    \"\"\"Return the softmax probabilities and their element-wise gradient.\n\n    Parameters\n    ----------\n    x : list | list[list]\n        A one- or two-dimensional numeric list.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(probabilities, gradient)`` where both elements keep the same\n        shape as *x* and are rounded to 4 decimal places.\n    \"\"\"\n    x_np = np.array(x)\n    max_vals = np.max(x_np, axis=-1, keepdims=True)\n    shifted = x_np - max_vals\n    exps = np.exp(shifted)\n    sum_exps = np.sum(exps, axis=-1, keepdims=True)\n    probs = exps / sum_exps\n    gradients = probs * (1 - probs)\n    probs_rounded = np.round(probs, 4)\n    gradients_rounded = np.round(gradients, 4)\n    return (probs_rounded.tolist(), gradients_rounded.tolist())\n```\n\n### Explanation\n1. **Numerical Stability**: By subtracting the maximum value from each element before exponentiation, we prevent potential numerical overflow issues.\n2. **Axis Handling**: Using `axis=-1` ensures that operations are performed along the last axis, which corresponds to row-wise operations for 2D arrays and the single axis for 1D arrays.\n3. **Broadcasting**: The use of `keepdims=True` ensures that the max values and sums retain their dimensions, allowing correct element-wise operations through broadcasting.\n4. **Efficiency**: Converting the input to a numpy array allows efficient vectorized operations, which are significantly faster than iterating through Python lists.\n5. **Gradient Calculation**: The gradient for each element is derived directly from the softmax probabilities, making the computation straightforward and efficient.\n\nThis approach efficiently handles both 1D and 2D inputs, ensuring numerical stability and correct computation of both probabilities and gradients, while converting the results back to the required Python list format."}
{"task_id": 474, "completion_id": 0, "completion": "To solve this problem, we need to generate a 2-D toy dataset suitable for a binary or multi-class classification problem. The dataset should be perfectly separable with each class arranged in a rectangular cluster. The solution must address the issues of hard-coded parameters and missing validation labels in the original code, while ensuring reproducibility and proper formatting.\n\n### Approach\n1. **Parameter Handling**: The function should accept parameters for the number of classes, training and validation samples per class, vertical interval between clusters, and an optional random seed.\n2. **Data Generation**: For each class, generate training and validation samples with specific ranges for x and y coordinates based on the class index. The labels are determined using a formula derived from the class index.\n3. **Reproducibility**: Use the provided seed to initialize the random number generator for reproducibility.\n4. **Rounding**: Round all coordinates to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\"\"\"\n\n    if seed is not None:\n        random.seed(seed)\n\n    X_train = []\n    X_val = []\n    Y_train = []\n    Y_val = []\n\n    for i in range(m):\n        label = (i - 0.5) * 2\n        x_floor = i // 2\n        x_low = (x_floor + 0.1) * 10\n        x_high = (x_floor + 0.9) * 10\n\n        y_mod = i % 2\n        y_low = (y_mod * interval + 0.1) * 10\n        y_high = (y_mod * interval + 0.9) * 10\n\n        # Generate training samples\n        for _ in range(n_train):\n            x = random.uniform(x_low, x_high)\n            y = random.uniform(y_low, y_high)\n            X_train.append([x, y])\n            Y_train.append(label)\n\n        # Generate validation samples\n        for _ in range(n_val):\n            x = random.uniform(x_low, x_high)\n            y = random.uniform(y_low, y_high)\n            X_val.append([x, y])\n            Y_val.append(label)\n\n    # Convert to numpy arrays and round coordinates\n    X_train = np.round(np.array(X_train), 4)\n    X_val = np.round(np.array(X_val), 4)\n    Y_train = np.array(Y_train)\n    Y_val = np.array(Y_val)\n\n    return X_train, X_val, Y_train, Y_val\n```\n\n### Explanation\n1. **Parameter Handling**: The function accepts parameters for the number of classes (`m`), training and validation samples per class (`n_train`, `n_val`), vertical interval (`interval`), and an optional seed for reproducibility.\n2. **Random Seed Initialization**: If a seed is provided, it initializes the random number generator using `random.seed(seed)` to ensure reproducible results.\n3. **Class Loop**: For each class index `i`, the function calculates the x and y coordinate ranges based on the class index and the given interval. The label for each class is determined using the formula `(i - 0.5) * 2`.\n4. **Sample Generation**: For each class, training and validation samples are generated using uniform distribution within the calculated ranges. These samples are collected into lists.\n5. **Array Conversion and Rounding**: The collected samples are converted to NumPy arrays and rounded to four decimal places to meet the output constraints.\n\nThis approach ensures that the generated data is reproducible, correctly formatted, and suitable for training and validating classification models."}
{"task_id": 475, "completion_id": 0, "completion": "To solve this problem, we need to implement the Adam optimization algorithm for a single update step. Adam is an optimization technique that adapts the learning rate for each parameter using estimates of the first and second moments of the gradients. This approach combines the advantages of two other extensions of stochastic gradient descent: AdaGrad and RMSProp.\n\n### Approach\n1. **Initialization of Moments**: If the previous first moment (m_prev) or the previous second moment (v_prev) is None, initialize them as zeros with the same shape as the gradient (grad).\n2. **Update Moments**: Compute the new first and second moments using exponential moving averages. The first moment is a weighted average of the gradient, and the second moment is a weighted average of the squared gradient.\n3. **Bias Correction**: Adjust the moments to account for their initialization bias. This correction is necessary because the initial moments (especially at early time steps) are biased towards zero.\n4. **Parameter Update**: Use the bias-corrected moments to update the parameters. The learning rate is adjusted by the ratio of the bias-corrected first moment to the square root of the bias-corrected second moment, with a small constant added for numerical stability.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    # Handle None values for previous moments\n    m_prev = np.zeros_like(grad) if m_prev is None else m_prev\n    v_prev = np.zeros_like(grad) if v_prev is None else v_prev\n    \n    # Update first and second moments\n    m_new = b1 * m_prev + (1 - b1) * grad\n    v_new = b2 * v_prev + (1 - b2) * (grad ** 2)\n    \n    # Compute bias-corrected moments\n    m_hat = m_new / (1 - b1 ** t)\n    v_hat = v_new / (1 - b2 ** t)\n    \n    # Update parameters\n    w_new = w - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n    \n    return w_new, m_new, v_new\n```\n\n### Explanation\n1. **Initialization**: The moments (m_prev and v_prev) are initialized to zero if they are None, ensuring they match the shape of the gradient.\n2. **Moment Updates**: The first moment (m_new) is updated using a weighted average of the previous first moment and the current gradient. The second moment (v_new) is updated similarly using the squared gradient.\n3. **Bias Correction**: The moments are adjusted to counteract their initialization bias, especially important in the initial steps. The correction factor is derived from the exponential decay rates (b1 and b2) raised to the power of the current time step (t).\n4. **Parameter Update**: The parameters are updated by adjusting them in the direction of the first moment, scaled by the learning rate and normalized by the square root of the second moment. A small constant (eps) is added to the denominator to prevent division by zero and ensure numerical stability.\n\nThis approach efficiently combines adaptive learning rates with momentum, making Adam a robust choice for optimization in various machine learning tasks."}
{"task_id": 477, "completion_id": 0, "completion": "To solve this problem, we need to implement the A* search algorithm on a 2-D grid to find the shortest path from a start coordinate to a destination coordinate using 4-directional movement. The grid consists of free cells (0) and obstacles (1), and each move to an adjacent free cell costs 1. If the destination is unreachable, we return -1.\n\n### Approach\n1. **Initial Checks**: Check if the start or destination coordinates are out of bounds or blocked by obstacles. If either is invalid, return -1 immediately.\n2. **Edge Case Handling**: If the start and destination coordinates are the same, return 0 as no movement is needed.\n3. **Priority Queue Initialization**: Use a priority queue (min-heap) to explore nodes based on the sum of the cost to reach the node (g) and the heuristic Manhattan distance to the destination (h). The priority queue is ordered by this sum (f = g + h).\n4. **Cost Tracking**: Maintain a matrix to track the minimum cost (g) to reach each cell. This helps in pruning paths that reach the same cell with higher costs than previously found.\n5. **Neighbor Exploration**: For each node, explore its neighbors (up, down, left, right). If moving to a neighbor is valid and results in a lower cost, update the cost matrix and push the neighbor into the priority queue.\n6. **Termination**: If the destination is reached, return the accumulated cost. If the priority queue is exhausted without reaching the destination, return -1.\n\n### Solution Code\n```python\nimport heapq\nfrom typing import List, Tuple\n\ndef astar(grid: List[List[int]], start: Tuple[int, int], dest: Tuple[int, int]) -> int:\n    \"\"\"A* search on a 2-D grid.\"\"\"\n    if start == dest:\n        return 0\n\n    rows = len(grid)\n    if rows == 0:\n        return -1\n    cols = len(grid[0]) if rows > 0 else 0\n    if cols == 0:\n        return -1\n\n    start_r, start_c = start\n    dest_r, dest_c = dest\n\n    # Check if start or destination is out of bounds or blocked\n    if not (0 <= start_r < rows and 0 <= start_c < cols) or grid[start_r][start_c] != 0:\n        return -1\n    if not (0 <= dest_r < rows and 0 <= dest_c < cols) or grid[dest_r][dest_c] != 0:\n        return -1\n\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    costs = [[float('inf')] * cols for _ in range(rows)]\n    costs[start_r][start_c] = 0\n\n    heap = []\n    initial_h = abs(start_r - dest_r) + abs(start_c - dest_c)\n    heapq.heappush(heap, (initial_h, 0, start_r, start_c))\n\n    while heap:\n        current_f, current_g, r, c = heapq.heappop(heap)\n\n        if (r, c) == (dest_r, dest_c):\n            return current_g\n\n        if current_g > costs[r][c]:\n            continue\n\n        for dr, dc in directions:\n            new_r = r + dr\n            new_c = c + dc\n            if 0 <= new_r < rows and 0 <= new_c < cols and grid[new_r][new_c] == 0:\n                new_g = current_g + 1\n                if new_g < costs[new_r][new_c]:\n                    costs[new_r][new_c] = new_g\n                    new_h = abs(new_r - dest_r) + abs(new_c - dest_c)\n                    new_f = new_g + new_h\n                    heapq.heappush(heap, (new_f, new_g, new_r, new_c))\n\n    return -1\n```\n\n### Explanation\n- **Initial Checks**: Ensure start and destination are valid and accessible.\n- **Priority Queue**: The priority queue ensures nodes are processed in order of their potential to lead to the shortest path, based on the sum of current path cost and heuristic estimate.\n- **Cost Matrix**: Tracks the minimum cost to reach each cell, preventing redundant processing of higher-cost paths.\n- **Neighbor Exploration**: For each node, explore all valid adjacent cells, updating their costs and pushing them into the queue if a better path is found.\n- **Termination**: The algorithm terminates when the destination is reached (returning the path cost) or when all possible paths are exhausted (returning -1).\n\nThis approach efficiently finds the shortest path using the A* algorithm, leveraging the Manhattan distance heuristic to guide the search optimally."}
{"task_id": 478, "completion_id": 0, "completion": "To solve this problem, we need to compute the feature-wise (column-wise) population standard deviation of a 2-D numerical dataset. The dataset can be provided as a list of lists or a NumPy array, where each row represents a sample and each column represents a feature. The solution must handle edge cases such as empty datasets and constant features, ensuring the result is a list of standard deviations rounded to four decimal places.\n\n### Approach\n1. **Convert Input to NumPy Array**: Convert the input dataset to a NumPy array of type float to facilitate numerical operations. This conversion also helps in handling empty inputs efficiently.\n2. **Check for Empty Dataset**: If the converted NumPy array is empty (i.e., has zero elements), return an empty list immediately.\n3. **Compute Column Means**: Calculate the mean of each column (feature) using vectorized operations for efficiency.\n4. **Calculate Squared Differences**: Subtract the mean of each column from every element in that column, then square the results.\n5. **Sum Squared Differences**: Sum the squared differences for each column to get the total squared deviation from the mean.\n6. **Compute Variance**: Divide the total squared deviation by the number of samples (rows) to get the population variance for each column.\n7. **Compute Standard Deviation**: Take the square root of the variance values to obtain the population standard deviation for each feature.\n8. **Round and Convert to List**: Round each standard deviation to four decimal places and convert the result from a NumPy array to a regular Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_std_dev(X):\n    \"\"\"Return the population standard deviation for every feature in *X*.\n\n    Args:\n        X (list[list[int | float]] | np.ndarray): 2-D data set where rows are\n            samples and columns are features.\n\n    Returns:\n        list[float]: A list containing the standard deviation of each feature,\n            rounded to 4 decimal places.\n            If *X* is empty an empty list is returned.\n    \"\"\"\n    X_np = np.array(X, dtype=float)\n    if X_np.size == 0:\n        return []\n    mu = np.mean(X_np, axis=0)\n    squared_diff = (X_np - mu) ** 2\n    sum_sq = np.sum(squared_diff, axis=0)\n    variance = sum_sq / X_np.shape[0]\n    std_dev = np.sqrt(variance)\n    rounded_std = np.around(std_dev, decimals=4)\n    return rounded_std.tolist()\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input is converted to a NumPy array to leverage efficient numerical operations. This conversion handles both lists of lists and existing NumPy arrays uniformly.\n- **Empty Dataset Check**: If the array is empty (size zero), the function immediately returns an empty list, adhering to the edge case requirement.\n- **Mean Calculation**: Using `np.mean` with `axis=0` computes the mean for each column efficiently.\n- **Squared Differences**: By subtracting the mean from each element and squaring the result, we compute the squared deviations in a vectorized manner.\n- **Sum and Variance**: Summing the squared deviations along the columns and dividing by the number of samples gives the population variance.\n- **Standard Deviation**: Taking the square root of the variance yields the standard deviation, which is then rounded and converted to a list for the final result.\n\nThis approach ensures that all operations are efficient and handle edge cases appropriately, providing the correct result for both typical and edge-case inputs."}
{"task_id": 479, "completion_id": 0, "completion": "To solve this problem, we need to simulate an \u03b5-greedy algorithm for the stationary multi-armed bandit problem. The goal is to determine the final estimated expected rewards for each arm after following the \u03b5-greedy strategy, which balances exploration and exploitation.\n\n### Approach\n1. **Initialization**: \n   - Seed the random number generator if a seed is provided to ensure reproducibility.\n   - Initialize the estimated values (V) for each arm to a given prior value (ev_prior).\n   - Initialize a counter (C) for each arm to track the number of times each arm has been pulled.\n\n2. **Iterate through Time Steps**:\n   - For each time step, decide whether to explore (choose a random arm) with probability \u03b5 or exploit (choose the arm with the highest current estimated value).\n   - Retrieve the reward for the chosen arm at the current time step.\n   - Update the counter for the chosen arm.\n   - Adjust the estimated value of the chosen arm using the incremental sample mean formula to incorporate the new reward.\n\n3. **Final Output**:\n   - After processing all time steps, return the estimated values for each arm, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\"\"\"\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    if not rewards:\n        return []\n    \n    n_arms = len(rewards[0])\n    V = np.full(n_arms, ev_prior, dtype=np.float64)\n    C = np.zeros(n_arms, dtype=np.int64)\n    \n    for t in range(len(rewards)):\n        current_rewards = rewards[t]\n        \n        if np.random.rand() < epsilon:\n            chosen_arm = np.random.randint(n_arms)\n        else:\n            chosen_arm = np.argmax(V)\n        \n        reward = current_rewards[chosen_arm]\n        \n        C[chosen_arm] += 1\n        V[chosen_arm] += (reward - V[chosen_arm]) / C[chosen_arm]\n    \n    return [round(val, 4) for val in V.tolist()]\n```\n\n### Explanation\n- **Initialization**: The estimated values (V) start with a prior value, and the counters (C) are initialized to zero. This setup ensures that each arm begins with the same initial estimate and no prior pulls.\n- **Exploration vs. Exploitation**: At each time step, the algorithm either explores by randomly selecting an arm (with probability \u03b5) or exploits by selecting the arm with the highest current estimate (with probability 1-\u03b5). This balance allows the algorithm to discover potentially better arms while maximizing rewards based on current knowledge.\n- **Incremental Update**: After selecting an arm, the reward is observed, the counter for that arm is incremented, and the estimated value is updated using the incremental mean formula. This ensures that the estimate adapts dynamically as more rewards are observed.\n- **Final Output**: The estimated values are rounded to four decimal places to meet the problem's precision requirements, providing a clear and concise result.\n\nThis approach efficiently balances exploration and exploitation, adapting to the observed rewards to provide accurate estimates for each arm's expected reward."}
{"task_id": 480, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mel_spectrogram(\n    x: \"np.ndarray\",\n    window_duration: float = 0.025,\n    stride_duration: float = 0.01,\n    mean_normalize: bool = True,\n    window: str = \"hamming\",\n    n_filters: int = 20,\n    center: bool = True,\n    alpha: float = 0.95,\n    fs: int = 44000,\n):\n    \"\"\"Return the Mel spectrogram of *x* together with the energy of every frame.\"\"\"\n\n    # Apply pre-emphasis\n    x = x.copy()\n    if alpha != 0.0:\n        x[1:] -= alpha * x[:-1]\n\n    # Calculate frame and stride lengths\n    frame_width = int(round(window_duration * fs))\n    stride = int(round(stride_duration * fs))\n    if frame_width == 0:\n        return [], []\n\n    # Pad the signal if centering is required\n    if center:\n        pad_width = frame_width // 2\n        padded_x = np.pad(x, (pad_width, pad_width), mode='constant')\n    else:\n        padded_x = x\n\n    # Frame the signal into overlapping windows\n    num_frames = (len(padded_x) - frame_width) // stride + 1\n    if num_frames <= 0:\n        return [], []\n    indices = np.arange(num_frames) * stride\n    frames = np.array([padded_x[i:i+frame_width] for i in indices])\n\n    # Apply window function\n    if window == \"hamming\":\n        win = np.hamming(frame_width)\n    elif window == \"hann\":\n        win = np.hanning(frame_width)\n    elif window == \"blackman_harris\":\n        win = np.blackmanharris(frame_width)\n    else:\n        raise ValueError(f\"Invalid window function: {window}\")\n    windowed_frames = frames * win\n\n    # Compute power spectrum\n    power_spectrum = np.abs(np.fft.rfft(windowed_frames, axis=1)) ** 2\n    power_spectrum /= frame_width\n\n    # Calculate energy per frame\n    energy_per_frame = power_spectrum.sum(axis=1)\n\n    # Build Mel filter bank\n    n_bins = power_spectrum.shape[1]\n    freqs = np.arange(n_bins) * (fs / frame_width)\n\n    def hz_to_mel(f):\n        return 2595 * np.log10(1 + f / 700)\n    def mel_to_hz(m):\n        return 700 * (10 ** (m / 2595) - 1)\n\n    # Generate Mel points and convert to Hz\n    mel_min, mel_max = hz_to_mel(0), hz_to_mel(fs / 2)\n    mel_points = np.linspace(mel_min, mel_max, n_filters + 2)\n    hz_points = mel_to_hz(mel_points)\n\n    # Create filter bank matrix\n    fbank = np.zeros((n_filters, n_bins))\n    for i in range(n_filters):\n        left, center, right = hz_points[i], hz_points[i+1], hz_points[i+2]\n        # Rising slope\n        rising = (freqs >= left) & (freqs < center)\n        fbank[i, rising] = (freqs[rising] - left) / (center - left)\n        # Falling slope\n        falling = (freqs >= center) & (freqs <= right)\n        fbank[i, falling] = (right - freqs[falling]) / (right - center)\n\n    # Project power spectra onto filter bank\n    filter_energies = np.dot(power_spectrum, fbank.T)\n\n    # Mean normalization\n    if mean_normalize:\n        filter_energies -= filter_energies.mean(axis=0, keepdims=True)\n\n    # Replace zeros with epsilon\n    filter_energies[filter_energies == 0] = np.finfo(float).eps\n\n    # Round and convert to lists\n    filter_energies = np.round(filter_energies, 4).tolist()\n    energy_per_frame = np.round(energy_per_frame, 4).tolist()\n\n    return filter_energies, energy_per_frame\n```"}
{"task_id": 481, "completion_id": 0, "completion": "To solve this problem, we need to implement the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm from scratch. The goal is to cluster points based on their density, marking noise points that do not belong to any cluster.\n\n### Approach\nThe DBSCAN algorithm works by identifying core points, which have at least a specified number of neighbors within a given radius (eps). These core points form the basis of clusters, which are expanded to include all reachable points (both core and border points). Border points are non-core points that are within the neighborhood of a core point. Noise points are those that are neither core nor border points.\n\nThe steps to implement DBSCAN are as follows:\n1. **Initialization**: Start with all points unprocessed and no clusters.\n2. **Core Point Identification**: For each point, check if it has enough neighbors within the radius to be a core point.\n3. **Cluster Expansion**: For each core point, expand the cluster by adding all reachable points (core and border) using a breadth-first search (BFS) approach.\n4. **Labeling**: Assign cluster labels to core and border points, and label noise points as -1.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if len(data) == 0:\n        return []\n    \n    labels = [-2] * len(data)\n    cluster_id = 0\n    \n    for i in range(len(data)):\n        if labels[i] != -2:\n            continue\n        \n        # Calculate all points within eps distance of data[i]\n        distances = np.linalg.norm(data - data[i], axis=1)\n        neighbors = np.where(distances <= eps)[0]\n        \n        if len(neighbors) < min_samples:\n            labels[i] = -1\n            continue\n        \n        # Start a new cluster\n        labels[i] = cluster_id\n        queue = deque(neighbors.tolist())\n        \n        while queue:\n            j = queue.popleft()\n            \n            if labels[j] == -1:\n                labels[j] = cluster_id\n            elif labels[j] == -2:\n                labels[j] = cluster_id\n                # Check if j is a core point\n                j_distances = np.linalg.norm(data - data[j], axis=1)\n                j_neighbors = np.where(j_distances <= eps)[0]\n                \n                if len(j_neighbors) >= min_samples:\n                    # Add all j's neighbors to the queue if they are unprocessed or noise\n                    for n in j_neighbors:\n                        if labels[n] == -2 or labels[n] == -1:\n                            if labels[n] == -2:\n                                queue.append(n)\n                            labels[n] = cluster_id\n        cluster_id += 1\n    \n    # Ensure any remaining unprocessed points are marked as noise\n    # In the algorithm, all reachable points are processed during expansion, so remaining -2 are impossible\n    # However, during the loop, all points are processed, so no need for this step.\n    return labels\n```\n\n### Explanation\n1. **Initialization**: We start with all points labeled as unprocessed (-2) and initialize the cluster ID to 0.\n2. **Core Point Check**: For each point, we compute the Euclidean distance to all other points to find neighbors within the given radius (eps). If a point has fewer neighbors than the required minimum (min_samples), it is marked as noise (-1).\n3. **Cluster Expansion**: If a point is identified as a core point, we start a new cluster and use a BFS approach to expand the cluster. This involves processing all reachable points (both core and border) from the initial core point, ensuring all reachable points are added to the cluster.\n4. **Queue Processing**: The BFS queue processes each neighbor, checking if it is a core point. If it is, its neighbors are added to the queue for further expansion. This ensures all density-reachable points are included in the cluster.\n\nThis approach efficiently clusters points based on density and correctly labels noise points, adhering to the DBSCAN algorithm's requirements."}
{"task_id": 482, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that converts a vector of integer class labels into a 2-D array where each row is the one-hot encoded representation of the corresponding label. The function should handle both specified and inferred numbers of classes, ensuring that all labels are valid and within the specified range.\n\n### Approach\n1. **Input Conversion**: Convert the input labels into a NumPy array for easier manipulation.\n2. **Validation Checks**: \n   - Ensure all labels are non-negative.\n   - If the number of classes (`num_classes`) is provided, check that all labels are within the valid range [0, num_classes - 1].\n3. **Determine Number of Classes**: If `num_classes` is not provided, calculate it as the maximum label value plus one.\n4. **One-Hot Encoding**: Create a zero-initialized 2-D array and set the appropriate positions to 1 based on the labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    y = np.asarray(y)\n    if np.any(y < 0):\n        raise ValueError(\"Labels must be non-negative.\")\n    \n    if num_classes is not None:\n        if not isinstance(num_classes, int) or num_classes < 0:\n            raise ValueError(\"num_classes must be a non-negative integer\")\n        if np.any(y >= num_classes):\n            raise ValueError(\"Labels must be less than num_classes.\")\n    else:\n        if y.size == 0:\n            num_classes = 0\n        else:\n            num_classes = np.max(y) + 1\n    \n    n_samples = y.shape[0]\n    categorical = np.zeros((n_samples, num_classes), dtype=int)\n    \n    if n_samples > 0 and num_classes > 0:\n        categorical[np.arange(n_samples), y] = 1\n    \n    return categorical\n```\n\n### Explanation\n1. **Input Conversion**: The input labels are converted to a NumPy array using `np.asarray(y)` to ensure uniform handling.\n2. **Validation for Non-Negative Labels**: A check ensures all labels are non-negative, raising a `ValueError` if any negative label is found.\n3. **Handling `num_classes`**:\n   - If `num_classes` is provided, it validates that all labels are within the range [0, num_classes - 1].\n   - If `num_classes` is not provided, it is inferred as the maximum label value plus one. Special handling for empty input ensures the function works correctly with zero samples.\n4. **One-Hot Encoding**: A zero matrix of appropriate shape is created. Using advanced indexing, the positions corresponding to each label are set to 1, efficiently creating the one-hot encoded matrix.\n\nThis approach ensures that the function is both efficient and robust, handling edge cases such as empty input and invalid labels appropriately."}
{"task_id": 483, "completion_id": 0, "completion": "To solve this problem, we need to convert a 1-D array-like object of non-negative integer labels into their one-hot encoded representation. The one-hot encoded format is a binary vector where each class label is represented by a vector with a single 1 at the index corresponding to the class label and 0s elsewhere.\n\n### Approach\n1. **Determine the Number of Classes**: If the number of classes (`num_classes`) is not provided, it is inferred as the maximum value in the labels plus one. This ensures that all class labels are covered.\n2. **Handle Input Types**: Convert the input labels to a numpy array for uniform processing, whether the input is a list or a numpy array.\n3. **Generate One-Hot Vectors**: For each label, create a vector of zeros with length `num_classes` and set the position corresponding to the label to 1. This results in a list of lists where each inner list is a one-hot encoded vector.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(labels, num_classes=None):\n    \"\"\"Convert integer class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    labels : sequence of int or 1-D np.ndarray\n        The class labels to transform.\n    num_classes : int, optional\n        Desired length of the one-hot vectors; if omitted the function should\n        use ``max(labels)+1``.\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded representation of ``labels``.\n    \"\"\"\n    labels_np = np.array(labels)\n    if num_classes is None:\n        num_classes = labels_np.max() + 1 if labels_np.size > 0 else 0\n    result = []\n    for label in labels_np:\n        vec = [0] * num_classes\n        vec[label] = 1\n        result.append(vec)\n    return result\n```\n\n### Explanation\n1. **Input Conversion**: The input labels are converted to a numpy array to handle both list and numpy array inputs uniformly.\n2. **Determine Classes**: If `num_classes` is not provided, it is calculated as the maximum value in the labels plus one. This handles the case where the labels are non-consecutive or start from a non-zero value.\n3. **Generate Vectors**: For each label, a vector of zeros is created with length `num_classes`. The position corresponding to the label is set to 1, creating the one-hot encoded vector. These vectors are collected into a list and returned.\n\nThis approach efficiently converts class labels into their one-hot encoded form, ensuring compatibility with machine learning pipelines that require input in this format. The solution handles both provided and inferred `num_classes`, ensuring flexibility and correctness."}
{"task_id": 484, "completion_id": 0, "completion": "To solve this problem, we need to implement Lasso regression (linear regression with L1 regularization) using coordinate descent for polynomial features. The goal is to fit a model to one-dimensional training data and make predictions on test data, considering polynomial feature expansion up to a specified degree.\n\n### Approach\n1. **Polynomial Feature Expansion**: Expand the input data into polynomial features of the specified degree. This involves creating a design matrix where each row corresponds to a data point and each column corresponds to a polynomial term (from x^0 to x^degree).\n\n2. **Coordinate Descent**: Optimize the Lasso regression coefficients using coordinate descent. In each iteration, each weight is updated individually while keeping others fixed. The update involves computing the residual, applying the soft-thresholding operator for regularization, and adjusting the weight accordingly.\n\n3. **Prediction**: After training the model, use the learned weights to predict values for the test data, which is also expanded into polynomial features. The predictions are then rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef lasso_regression_predict(\n    X_train: list[float | int],\n    y_train: list[float | int],\n    X_test: list[float | int],\n    degree: int,\n    reg_factor: float = 0.1,\n    n_iterations: int = 1000,\n) -> list[float]:\n    \"\"\"Fit a one-dimensional Lasso regression model and predict on new data.\"\"\"\n    # Convert training data to numpy arrays\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    \n    # Generate polynomial features for training data\n    phi = X_train.reshape(-1, 1) ** np.arange(degree + 1)\n    \n    # Precompute z_j for each feature (sum of squares)\n    z = np.sum(phi ** 2, axis=0)\n    \n    # Initialize weights and current predictions\n    w = np.zeros(degree + 1)\n    current_predictions = phi @ w  # Initially zero\n    \n    # Coordinate descent iterations\n    for _ in range(n_iterations):\n        for j in range(degree + 1):\n            residual_part = y_train - current_predictions\n            rho_j = np.dot(phi[:, j], residual_part) + w[j] * z[j]\n            \n            if j == 0:\n                new_wj = rho_j / z[j]\n            else:\n                # Apply soft thresholding\n                soft = np.sign(rho_j) * np.maximum(np.abs(rho_j) - reg_factor, 0.0)\n                new_wj = soft / z[j]\n            \n            delta = new_wj - w[j]\n            current_predictions += phi[:, j] * delta\n            w[j] = new_wj\n    \n    # Generate polynomial features for test data\n    X_test = np.array(X_test)\n    phi_test = X_test.reshape(-1, 1) ** np.arange(degree + 1)\n    \n    # Compute predictions and round to 4 decimal places\n    y_pred = phi_test @ w\n    rounded_predictions = np.round(y_pred, 4)\n    \n    return rounded_predictions.tolist()\n```\n\n### Explanation\n1. **Polynomial Feature Expansion**: The input data (both training and test) is transformed into a design matrix where each row represents a data point expanded into polynomial terms. For example, a degree of 2 transforms each data point x into [1, x, x^2].\n\n2. **Coordinate Descent**: For each iteration, each weight (coefficient) is updated individually. The residual (difference between actual and predicted values) is computed, and the weight update is determined using the soft-thresholding operator to apply L1 regularization. This ensures sparsity in the model coefficients, effectively performing feature selection.\n\n3. **Prediction**: After training the model, the test data is expanded into the same polynomial features, and predictions are made using the learned weights. The results are rounded to four decimal places for the final output.\n\nThis approach efficiently optimizes the model parameters using coordinate descent, balancing model complexity and regularization to prevent overfitting."}
{"task_id": 485, "completion_id": 0, "completion": "To solve this problem, we need to implement a univariate regression tree using the CART algorithm. The tree will be built by recursively splitting the data based on the feature that minimizes the sum of squared errors (SSE) and will stop splitting when the maximum depth is reached or a node has fewer than two samples. Each leaf node will predict the mean of the target values in that node. The predictions for test points are made by traversing the tree.\n\n### Approach\n1. **Check for Empty Training Data**: If the training data is empty, return an empty list of predictions.\n2. **Sort Training Data**: Sort the training data based on the feature values to facilitate efficient splitting.\n3. **Recursive Tree Construction**: Build the tree recursively by finding the best split at each node, which minimizes the SSE. The best split is determined by iterating through possible split points between sorted feature values.\n4. **Leaf Node Creation**: Create leaf nodes when the maximum depth is reached or a node has fewer than two samples, storing the mean of the target values.\n5. **Prediction**: Traverse the tree for each test point to predict the target value based on the learned splits and leaf node means.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\"\"\"\n\n    if not X_train:\n        return []\n\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n\n    # Sort the training data based on X values\n    sorted_indices = np.argsort(X_train)\n    X_sorted = X_train[sorted_indices]\n    y_sorted = y_train[sorted_indices]\n\n    def build_tree(X, y, current_depth):\n        # Base cases: max depth reached or node has fewer than two samples\n        if current_depth >= max_depth or len(X) < 2:\n            return {'mean': np.mean(y) if len(y) > 0 else 0.0}\n\n        best_sse = float('inf')\n        best_threshold = None\n        best_i = -1\n\n        # Iterate through possible split points\n        for i in range(len(X) - 1):\n            if X[i] >= X[i + 1]:\n                continue  # Skip if not a valid split\n\n            # Calculate SSE for this split\n            left_y = y[:i + 1]\n            right_y = y[i + 1:]\n            mean_left = np.mean(left_y)\n            mean_right = np.mean(right_y)\n\n            sse_left = np.sum((left_y - mean_left) ** 2)\n            sse_right = np.sum((right_y - mean_right) ** 2)\n            total_sse = sse_left + sse_right\n\n            if total_sse < best_sse:\n                best_sse = total_sse\n                best_threshold = (X[i] + X[i + 1]) / 2\n                best_i = i\n\n        # Check if a valid split was found\n        if best_i == -1:\n            return {'mean': np.mean(y)}\n\n        # Split the data into left and right\n        left_X = X[:best_i + 1]\n        left_y = y[:best_i + 1]\n        right_X = X[best_i + 1:]\n        right_y = y[best_i + 1:]\n\n        # Recursively build left and right subtrees\n        left_child = build_tree(left_X, left_y, current_depth + 1)\n        right_child = build_tree(right_X, right_y, current_depth + 1)\n\n        return {'threshold': best_threshold, 'left': left_child, 'right': right_child}\n\n    # Build the regression tree\n    root = build_tree(X_sorted, y_sorted, 0)\n\n    # Function to predict a single value\n    def predict_single(x):\n        node = root\n        while True:\n            if 'mean' in node:\n                return node['mean']\n            else:\n                if x <= node['threshold']:\n                    node = node['left']\n                else:\n                    node = node['right']\n\n    # Generate predictions for all test points\n    return [predict_single(x) for x in X_test]\n```\n\n### Explanation\n1. **Handling Empty Data**: The function immediately returns an empty list if the training data is empty.\n2. **Sorting Data**: The training data is sorted based on the feature values to ensure efficient splitting.\n3. **Tree Construction**: The `build_tree` function recursively constructs the tree. For each node, it checks if it should be a leaf node based on depth or sample size. If not, it finds the best split by minimizing SSE.\n4. **Splitting**: For each potential split point between sorted feature values, the SSE is calculated. The split with the lowest SSE is chosen.\n5. **Leaf Nodes**: When a node can no longer be split, it becomes a leaf node storing the mean of its target values.\n6. **Prediction**: Each test point is predicted by traversing the tree from the root to the appropriate leaf node based on the feature value, using the stored thresholds and means.\n\nThis approach ensures that the tree is built efficiently and predictions are made accurately by leveraging the structure of the binary tree formed through recursive splitting."}
{"task_id": 486, "completion_id": 0, "completion": "To solve this problem, we need to compute the Mean Squared Error (MSE) between two equally-sized numeric sequences. The MSE is a common regression metric that measures the average of the squares of the errors between the observed and predicted values. The function should handle various input types and return an error code under specific conditions.\n\n### Approach\n1. **Input Handling**: Convert the input sequences (lists, tuples, or NumPy arrays) into NumPy arrays to leverage efficient vector operations.\n2. **Validation**: Check if the converted arrays are 1-dimensional and of the same non-zero length. If not, return -1.\n3. **MSE Calculation**: Compute the squared differences between the elements of the two arrays, take the mean of these squared differences, and round the result to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"Compute the mean squared error (MSE) between two numeric sequences.\n\n    The function should return -1 if the two inputs do not have the same\n    non-zero length. Otherwise, it returns the MSE rounded to 4 decimal\n    places.\n\n    Args:\n        y_true: Ground-truth values (list, tuple or NumPy array).\n        y_pred: Predicted values (list, tuple or NumPy array).\n\n    Returns:\n        A float rounded to 4 decimals representing the MSE, or -1 on error.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Check if both arrays are 1-dimensional\n    if y_true.ndim != 1 or y_pred.ndim != 1:\n        return -1\n    \n    n = len(y_true)\n    if n != len(y_pred) or n == 0:\n        return -1\n    \n    squared_errors = (y_true - y_pred) ** 2\n    mse = np.mean(squared_errors)\n    \n    return round(mse, 4)\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The inputs are converted to NumPy arrays to facilitate element-wise operations.\n2. **Dimensionality Check**: The function ensures both arrays are 1-dimensional to handle only valid sequences.\n3. **Length and Non-Zero Check**: The function checks if both arrays have the same length and are non-empty to avoid invalid computations.\n4. **MSE Calculation**: The squared differences between corresponding elements are computed, averaged, and rounded to 4 decimal places to provide the final result.\n\nThis approach efficiently handles different input types and ensures robust error checking, providing a reliable and accurate MSE calculation."}
{"task_id": 487, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm from scratch. The goal is to partition a given dataset into `k` clusters, where each sample belongs to the cluster with the nearest centroid. The algorithm iteratively updates the centroids until convergence or a maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: The initial centroids are set to the first `k` samples of the dataset. This makes the algorithm deterministic and easy to test.\n2. **Assignment Step**: Each sample is assigned to the nearest centroid using the Euclidean distance. This is efficiently computed using vectorized operations to handle all samples and centroids simultaneously.\n3. **Update Step**: New centroids are computed as the mean of the samples assigned to each cluster. If a cluster has no samples, its centroid remains unchanged.\n4. **Convergence Check**: The algorithm stops iterating when the Euclidean distance (Frobenius norm) between the old and new centroids is below a specified tolerance or when the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef k_means(X: np.ndarray, k: int, max_iters: int = 300, tol: float = 1e-4) -> list[int]:\n    \"\"\"Cluster the data contained in *X* into *k* groups using the K-Means algorithm.\"\"\"\n    # Initialize centroids using the first k samples\n    centroids = X[:k].copy()\n    \n    for _ in range(max_iters):\n        # Calculate squared distances between each sample and each centroid\n        squared_diffs = (X[:, np.newaxis, :] - centroids) ** 2\n        distances_sq = np.sum(squared_diffs, axis=2)\n        \n        # Assign each sample to the nearest centroid\n        labels = np.argmin(distances_sq, axis=1)\n        \n        # Update centroids\n        new_centroids = centroids.copy()\n        for j in range(k):\n            mask = (labels == j)\n            if np.any(mask):\n                new_centroids[j] = X[mask].mean(axis=0)\n        \n        # Check for convergence\n        if np.linalg.norm(centroids - new_centroids) < tol:\n            break\n        centroids = new_centroids\n    \n    return labels.tolist()\n```\n\n### Explanation\n1. **Initialization**: The centroids are initialized to the first `k` samples of the dataset. This ensures a deterministic starting point.\n2. **Distance Calculation**: For each sample, the squared Euclidean distance to each centroid is computed. Using squared distances avoids unnecessary square root operations, improving efficiency without affecting the assignment step.\n3. **Assignment**: Each sample is assigned to the cluster of the nearest centroid using the computed distances.\n4. **Centroid Update**: New centroids are calculated as the mean of the samples assigned to each cluster. If a cluster has no samples, the centroid remains unchanged.\n5. **Convergence Check**: The algorithm stops if the Frobenius norm (Euclidean distance for matrices) between the old and new centroids is below the tolerance threshold, indicating that the centroids have stabilized.\n\nThis approach efficiently handles the iterative nature of K-Means clustering, ensuring convergence within a reasonable number of iterations while maintaining clarity and correctness."}
{"task_id": 488, "completion_id": 0, "completion": "To solve this problem, we need to calculate the population variance of each feature (column) in a given 2D dataset. The dataset can be provided as either a list of lists or a NumPy array. The solution must handle various edge cases, such as invalid input structures, and return the variances rounded to four decimal places.\n\n### Approach\n1. **Input Validation**: Convert the input to a NumPy array and check if it is a valid 2D structure. If the conversion fails or the array is not 2D, return -1.\n2. **Check Validity**: Ensure the dataset has at least one sample (row). If not, return -1.\n3. **Compute Variances**: Use NumPy's built-in function to compute the population variance for each column. This function uses the formula for population variance directly, which simplifies the implementation.\n4. **Round Results**: Round each variance value to four decimal places and return the results as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_variance(X):\n    \"\"\"Return the population variance of every feature (column) in X.\n\n    Parameters\n    ----------\n    X : list[list[int | float]] | np.ndarray\n        A 2-D data set where each row is a sample and each column is a feature.\n\n    Returns\n    -------\n    list[float] | int\n        A list containing the variance of each feature rounded to four\n        decimals.  Returns -1 if the input is empty or not 2-D.\n    \"\"\"\n    # Convert input to numpy array if it's a list; check validity\n    if isinstance(X, list):\n        if not X:\n            return -1\n        for row in X:\n            if not isinstance(row, list):\n                return -1\n    try:\n        X = np.array(X)\n    except:\n        return -1\n    \n    # Check if the array is 2-dimensional\n    if X.ndim != 2:\n        return -1\n    \n    # Check if there are zero samples (rows)\n    if X.shape[0] < 1:\n        return -1\n    \n    # Calculate population variance for each column\n    variances = np.var(X, axis=0)\n    \n    # Round to 4 decimal places and convert to list\n    rounded_variances = [round(var, 4) for var in variances.tolist()]\n    \n    return rounded_variances\n```\n\n### Explanation\n1. **Input Conversion and Validation**: The input is first checked to ensure it is a valid 2D structure. If it is a list, each element is verified to be a list. Conversion to a NumPy array helps in handling both list and array inputs uniformly.\n2. **Dimensionality Check**: The converted array is checked to ensure it is 2-dimensional. If not, the function returns -1.\n3. **Sample Check**: The function verifies that there is at least one sample (row) in the dataset. If not, it returns -1.\n4. **Variance Calculation**: Using NumPy's `var` function with `axis=0` computes the population variance for each column efficiently.\n5. **Rounding and Conversion**: The results are rounded to four decimal places and converted to a list to meet the output requirements.\n\nThis approach ensures that all edge cases are handled, and the solution is both efficient and easy to understand."}
{"task_id": 489, "completion_id": 0, "completion": "To solve this problem, we need to convert a batch of one-hot encoded vectors into their nominal (integer) class labels. Each one-hot encoded vector is a 1-D array where exactly one element is 1 and the rest are 0. The function must validate each row to ensure it meets this criterion before converting the valid vectors to their respective class indices.\n\n### Approach\n1. **Input Handling**: Convert the input (whether a list of lists or a NumPy array) into a NumPy array for uniform processing.\n2. **Dimensionality Check**: Ensure the input is a 2-D array. If not, return -1 immediately.\n3. **Validity Check**: For each row in the array, check if the sum of elements is exactly 1 and the maximum element is 1. This ensures that each row is a valid one-hot vector.\n4. **Conversion**: If all rows are valid, convert each row to the index of the element with value 1 using the `argmax` method. If any row is invalid, return -1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_nominal(one_hot):\n    \"\"\"Convert a batch of one-hot encoded vectors to nominal class labels.\"\"\"\n    arr = np.asarray(one_hot)\n    # Check if the array is 2-dimensional\n    if arr.ndim != 2:\n        return -1\n    # Calculate sum and max of each row\n    row_sums = arr.sum(axis=1)\n    row_maxs = arr.max(axis=1)\n    # Check all rows are valid one-hot vectors\n    if not (np.all(row_sums == 1) and np.all(row_maxs == 1)):\n        return -1\n    # Get the indices of the maximum elements (which are 1s)\n    indices = arr.argmax(axis=1).tolist()\n    return indices\n```\n\n### Explanation\n1. **Input Conversion**: The input is converted to a NumPy array to handle both list and array inputs uniformly.\n2. **Dimensionality Check**: The function checks if the converted array is 2-dimensional. If not, it returns -1 immediately as the input format is incorrect.\n3. **Sum and Max Check**: For each row, the sum of elements must be 1 and the maximum element must be 1. This ensures that there is exactly one 1 and the rest are 0s.\n4. **Index Extraction**: Using the `argmax` method, the function efficiently finds the index of the 1 in each valid row and converts the result to a Python list of integers.\n\nThis approach efficiently validates the input and converts valid one-hot vectors to their nominal indices, ensuring correctness and handling edge cases appropriately."}
{"task_id": 490, "completion_id": 0, "completion": "To solve this problem, we need to convert a one-dimensional vector into a square diagonal matrix. The input can be a Python list, tuple, or a one-dimensional NumPy array, and the output should be a list of lists where the elements of the input vector are placed on the principal diagonal, and all off-diagonal elements are zeros. If the input vector is empty, the function should return an empty list.\n\n### Approach\n1. **Handle Empty Input**: Check if the input vector is empty. If it is, return an empty list immediately.\n2. **Convert to NumPy Array**: Convert the input (whether it's a list, tuple, or already a NumPy array) into a NumPy array for consistent processing.\n3. **Check Size**: Verify the size of the converted array. If it's empty, return an empty list.\n4. **Create Diagonal Matrix**: Use NumPy's `diag` function to create a diagonal matrix from the input vector. This function places the elements of the input vector on the main diagonal and fills the off-diagonal elements with zeros.\n5. **Convert to List of Lists**: Convert the resulting NumPy matrix into a list of lists using the `tolist()` method for the required output format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | np.ndarray) -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    arr = x if isinstance(x, np.ndarray) else np.array(x)\n    if arr.size == 0:\n        return []\n    return np.diag(arr).tolist()\n```\n\n### Explanation\n- **Handling Empty Input**: The function first checks if the input is empty by converting it into a NumPy array and checking its size. If the size is zero, it returns an empty list.\n- **Conversion to NumPy Array**: The input is converted to a NumPy array if it isn't already one. This ensures uniform processing regardless of the input type.\n- **Diagonal Matrix Creation**: Using `np.diag`, the function creates a diagonal matrix from the input vector. This function efficiently places the elements on the main diagonal and initializes other elements to zero.\n- **Conversion to List of Lists**: The resulting matrix is converted to a list of lists using `tolist()`, which is the required output format.\n\nThis approach ensures that the function efficiently handles all valid input types and correctly converts them into the desired diagonal matrix format."}
{"task_id": 491, "completion_id": 0, "completion": "To solve this problem, we need to implement the FP-Growth algorithm to mine all frequent item-sets from a collection of transactions that appear at least a specified number of times (min_sup). The solution involves constructing an FP-Tree and recursively mining it to discover all frequent item-sets.\n\n### Approach\nThe FP-Growth algorithm works in two main stages:\n1. **FP-Tree Construction**: \n   - Count the frequency of each item in the transactions.\n   - Filter out items that do not meet the minimum support (min_sup).\n   - Sort the remaining items in each transaction by descending frequency and lexicographical order.\n   - Construct the FP-Tree by inserting each transaction into the tree, sharing common prefixes and maintaining a header table for quick access to nodes.\n\n2. **Recursive Mining**:\n   - For each item in the header table (processed in ascending order of frequency), generate the conditional pattern base.\n   - Build a conditional FP-Tree from the conditional pattern base.\n   - If the conditional FP-Tree has a single path, generate all non-empty combinations of items in that path. Otherwise, recursively mine the conditional FP-Tree.\n\n### Solution Code\n```python\nfrom collections import Counter, defaultdict\nimport itertools\n\nclass FPNode:\n    def __init__(self, item, count=0, parent=None):\n        self.item = item\n        self.count = count\n        self.parent = parent\n        self.children = defaultdict(FPNode)\n        self.node_link = None  # Not used in this implementation but kept for structure\n\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\"\"\"\n    # Step 1: Count item frequencies\n    item_counts = Counter()\n    for trans in transactions:\n        for item in trans:\n            item_counts[item] += 1\n\n    # Filter items by minimum support\n    frequent_items = {item: cnt for item, cnt in item_counts.items() if cnt >= min_sup}\n    if not frequent_items:\n        return []\n\n    # Sort frequent items by descending frequency, then lexicographical order\n    sorted_items = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n    rank = {item: idx for idx, item in enumerate(sorted_items)}\n\n    # Build FP-Tree\n    root = FPNode(None)\n    header_table = defaultdict(list)\n\n    for trans in transactions:\n        filtered = [item for item in trans if item in frequent_items]\n        filtered.sort(key=lambda x: rank[x])\n        current = root\n        for item in filtered:\n            if item in current.children:\n                current = current.children[item]\n                current.count += 1\n            else:\n                new_node = FPNode(item, 1, current)\n                current.children[item] = new_node\n                header_table[item].append(new_node)\n                current = new_node\n\n    # Mining the FP-Tree\n    frequent_itemsets = []\n\n    def is_single_path(node):\n        if not node.children:\n            return True\n        if len(node.children) > 1:\n            return False\n        return is_single_path(next(iter(node.children.values())))\n\n    def mine_tree(header, min_sup, suffix, frequent_itemsets):\n        # Process items in ascending order of frequency (sorted by count then lex)\n        items = sorted(header.keys(), key=lambda x: (frequent_items[x], x))\n\n        for item in items:\n            new_suffix = tuple(sorted(list(suffix) + [item]))\n            frequent_itemsets.append(new_suffix)\n\n            conditional_patterns = []\n            for node in header[item]:\n                path = []\n                count = node.count\n                parent = node.parent\n                while parent.item is not None:\n                    path.append(parent.item)\n                    parent = parent.parent\n                path.reverse()\n                conditional_patterns.append((path, count))\n\n            # Count frequencies in conditional patterns\n            cond_counts = Counter()\n            for path, cnt in conditional_patterns:\n                for it in path:\n                    cond_counts[it] += cnt\n\n            # Filter items that meet min_sup\n            cond_freq = {it: cnt for it, cnt in cond_counts.items() if cnt >= min_sup}\n            if not cond_freq:\n                continue\n\n            # Sort conditional frequent items by descending count, then lex\n            cond_sorted = sorted(cond_freq.keys(), key=lambda x: (-cond_counts[x], x))\n\n            # Build conditional FP-Tree\n            cond_root = FPNode(None)\n            cond_header = defaultdict(list)\n            for path, cnt in conditional_patterns:\n                filtered_path = [it for it in path if it in cond_freq]\n                filtered_path.sort(key=lambda x: (-cond_counts[x], x))\n\n                current = cond_root\n                for it in filtered_path:\n                    if it in current.children:\n                        current = current.children[it]\n                        current.count += cnt\n                    else:\n                        new_node = FPNode(it, cnt, current)\n                        current.children[it] = new_node\n                        cond_header[it].append(new_node)\n                        current = new_node\n\n            if is_single_path(cond_root):\n                # Generate all subsets of the path\n                path_items = []\n                current_node = cond_root\n                while current_node.children:\n                    next_node = next(iter(current_node.children.values()))\n                    path_items.append(next_node.item)\n                    current_node = next_node\n\n                for i in range(1, len(path_items) + 1):\n                    for subset in itertools.combinations(path_items, i):\n                        combined = tuple(sorted(list(subset) + [item] + list(suffix)))\n                        frequent_itemsets.append(combined)\n            else:\n                mine_tree(cond_header, min_sup, new_suffix, frequent_itemsets)\n\n    mine_tree(header_table, min_sup, tuple(), frequent_itemsets)\n\n    # Deduplicate and sort\n    unique = list({tuple(sorted(itemset)) for itemset in frequent_itemsets})\n    unique.sort(key=lambda x: (len(x), x))\n\n    return unique\n```\n\n### Explanation\n1. **FP-Tree Construction**:\n   - The algorithm first counts the frequency of each item across all transactions.\n   - Items that do not meet the minimum support threshold are filtered out.\n   - Remaining items are sorted by frequency (descending) and lexicographical order.\n   - Transactions are inserted into the FP-Tree, creating nodes for items and linking them through shared prefixes.\n\n2. **Mining Process**:\n   - The algorithm recursively processes each item from the header table, starting with the least frequent.\n   - For each item, the conditional pattern base is generated, which consists of paths from the FP-Tree that end with the item.\n   - A conditional FP-Tree is built from these paths. If this tree has a single path, all non-empty subsets of the path are combined with the current item to form frequent item-sets. Otherwise, the tree is mined recursively.\n\nThe solution efficiently discovers all frequent item-sets by leveraging the FP-Tree structure, ensuring that the result is sorted by length and lexicographical order as required."}
{"task_id": 492, "completion_id": 0, "completion": "To solve this problem, we need to generate a polynomial feature matrix from a given data matrix such that the resulting matrix includes all monomials of the original features up to a specified degree. The monomials should be ordered first by ascending degree and then lexicographically within each degree.\n\n### Approach\n1. **Check for Negative Degree**: If the specified degree is negative, return -1 immediately.\n2. **Convert Input to NumPy Array**: This allows for easier manipulation and mathematical operations.\n3. **Generate Exponent Tuples**: For each degree from 0 up to the specified degree, generate all possible combinations of feature indices. Each combination represents the exponents of the features such that their sum equals the current degree.\n4. **Compute Monomials**: For each sample (row) in the input data matrix, compute the product of each feature raised to the corresponding exponent from the generated tuples.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    if degree < 0:\n        return -1\n    X = np.array(X, dtype=float)\n    n_samples, m = X.shape\n    exponents = []\n    for current_degree in range(degree + 1):\n        if current_degree == 0:\n            exponents.append((0,) * m)\n        else:\n            for comb in combinations_with_replacement(range(m), current_degree):\n                exp = [0] * m\n                for idx in comb:\n                    exp[idx] += 1\n                exponents.append(tuple(exp))\n    result = []\n    for row in X:\n        new_row = []\n        for exp in exponents:\n            product = 1.0\n            for i in range(m):\n                product *= row[i] ** exp[i]\n            new_row.append(product)\n        result.append(new_row)\n    return result\n```\n\n### Explanation\n1. **Negative Degree Check**: The function first checks if the input degree is negative and returns -1 if true.\n2. **Conversion to NumPy Array**: The input data matrix is converted to a NumPy array to leverage vectorized operations.\n3. **Generating Exponent Tuples**: For each degree from 0 to the specified degree, combinations of feature indices are generated. Each combination is converted into an exponent tuple by counting the occurrences of each feature index.\n4. **Computing Monomials**: For each row in the input matrix, the function computes the product of each feature raised to the corresponding exponent from the generated tuples. These products form the polynomial features for each row.\n\nThis approach ensures that all monomials are generated in the correct order and efficiently computes the polynomial features for each sample, adhering to the specified constraints."}
{"task_id": 493, "completion_id": 0, "completion": "To solve this problem, we need to compute the mean-squared error (MSE) of a given 1-D sample of real numbers. The MSE is the average of the squared differences between each element and the mean of the sample. If the input sample is empty, the function should return -1.\n\n### Approach\n1. **Check for Empty Input**: Convert the input to a NumPy array and check if it is empty. If it is, return -1.\n2. **Calculate the Mean**: Compute the mean of the elements in the sample.\n3. **Compute Squared Differences**: For each element in the sample, compute the squared difference from the mean.\n4. **Average the Squared Differences**: Find the average of these squared differences to get the MSE.\n5. **Round the Result**: Round the computed MSE to four decimal places and return the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    y_arr = np.array(y)\n    if y_arr.size == 0:\n        return -1\n    mean_val = np.mean(y_arr)\n    mse_value = np.mean((y_arr - mean_val) ** 2)\n    return round(mse_value, 4)\n```\n\n### Explanation\n1. **Handling Empty Input**: The input is converted to a NumPy array. If the array has no elements (size is 0), the function immediately returns -1.\n2. **Mean Calculation**: Using NumPy's `mean` function, we efficiently compute the mean of the array elements.\n3. **Squared Differences**: By subtracting the mean from each element and squaring the result, we get the squared differences.\n4. **Averaging Squared Differences**: The mean of these squared differences is computed to obtain the MSE.\n5. **Rounding**: The result is rounded to four decimal places using Python's `round` function to meet the specified precision requirement.\n\nThis approach ensures that the function efficiently handles both Python lists and NumPy arrays, correctly processes empty inputs, and computes the MSE accurately while adhering to the required precision."}
{"task_id": 494, "completion_id": 0, "completion": "To solve this problem, we need to aggregate predictions from an ensemble of models into a single prediction per sample. The aggregation method depends on whether the task is classification or regression. For classification, we use majority voting, and for regression, we use averaging.\n\n### Approach\n1. **Classification (Majority Voting)**:\n   - For each sample, collect predictions from all models.\n   - Determine the class label that appears most frequently (mode). In case of a tie, select the smallest label.\n   - This is achieved by using `np.unique` to find unique labels and their counts, then selecting the label with the highest count, breaking ties by choosing the smallest label.\n\n2. **Regression (Averaging)**:\n   - For each sample, compute the arithmetic mean of predictions from all models.\n   - Round the result to 4 decimal places for the final prediction.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import List, Union\n\ndef aggregate_predictions(\n    preds: Union[List[List[Union[int, float]]], np.ndarray],\n    classifier: bool = True,\n) -> List[Union[int, float]]:\n    \"\"\"Aggregate ensemble predictions.\"\"\"\n    preds_array = np.array(preds)\n    \n    if classifier:\n        n_samples = preds_array.shape[1]\n        aggregated = []\n        for i in range(n_samples):\n            sample_preds = preds_array[:, i]\n            unique_values, counts = np.unique(sample_preds, return_counts=True)\n            max_count = np.max(counts)\n            candidates = unique_values[counts == max_count]\n            final_label = np.min(candidates)\n            aggregated.append(final_label)\n        return aggregated\n    else:\n        means = np.mean(preds_array, axis=0)\n        rounded = np.round(means, 4)\n        return rounded.tolist()\n```\n\n### Explanation\n- **Classification**:\n  - Convert the input to a NumPy array for efficient computation.\n  - For each sample (column in the array), determine the unique prediction values and their counts using `np.unique`.\n  - Identify the prediction with the highest count. If multiple predictions have the same highest count, select the smallest value using `np.min`.\n\n- **Regression**:\n  - Convert the input to a NumPy array.\n  - Compute the mean of predictions for each sample across all models using `np.mean` along axis 0.\n  - Round the resulting means to 4 decimal places using `np.round` and convert the result to a list for the final output.\n\nThis approach efficiently handles both classification and regression tasks, ensuring correct aggregation based on the specified task type."}
{"task_id": 495, "completion_id": 0, "completion": "To solve this problem, we need to standardize each column of a 2-D NumPy array such that each feature (column) has a mean of 0 and a variance of 1. If a column has zero variance (all values are identical), it should be replaced with zeros. The solution must return a new array without modifying the original input and convert the result to a Python list with elements rounded to 4 decimal places.\n\n### Approach\n1. **Compute Column Means and Standard Deviations**: Calculate the mean and population standard deviation (using `ddof=0`) for each column of the input array.\n2. **Standardize the Data**: For each column, subtract the mean and divide by the standard deviation. If the standard deviation is zero (indicating all values in the column are identical), replace the entire column with zeros.\n3. **Handle Division by Zero**: Use NumPy operations to efficiently handle columns with zero standard deviation by setting those columns to zero after the initial standardization step.\n4. **Round and Convert to List**: Round each element of the resulting array to 4 decimal places and convert the array to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef standardize(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Standardize every column of a 2-D NumPy array.\n\n    Each feature (column) should have mean 0 and variance 1 after the\n    transformation.  Columns with zero variance must become all zeros.\n\n    The input array **must not** be modified in place; return a new result.\n\n    Args:\n        X: 2-D NumPy array (shape = n_samples \u00d7 n_features).\n\n    Returns:\n        Standardized dataset as a Python list rounded to 4 decimals.\n    \"\"\"\n    mean = np.mean(X, axis=0)\n    std = np.std(X, axis=0, ddof=0)\n    standardized = (X - mean) / std\n    standardized[:, std == 0] = 0.0\n    rounded = np.round(standardized, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Mean and Standard Deviation Calculation**: The mean and standard deviation are computed for each column using `np.mean` and `np.std` with appropriate parameters. This ensures we handle each feature independently.\n2. **Broadcasting for Standardization**: By leveraging NumPy's broadcasting capabilities, we efficiently subtract the mean and divide by the standard deviation for each column. This avoids explicit loops and makes the code concise and efficient.\n3. **Handling Zero Variance Columns**: After standardization, any column with a standard deviation of zero is identified using a boolean mask, and all elements in those columns are set to zero to avoid division by zero errors.\n4. **Rounding and Conversion**: The result is rounded to 4 decimal places using `np.round` and converted to a Python list using `tolist()` to meet the output requirements.\n\nThis approach ensures that the input array remains unmodified, handles edge cases such as zero variance columns, and efficiently computes the standardized values using vectorized operations."}
{"task_id": 496, "completion_id": 0, "completion": "To solve this problem, we need to compute the affine activation function along with its first and second derivatives for a given input vector. The affine activation function is defined as \\(f(x) = \\text{slope} \\cdot x + \\text{intercept}\\), and its derivatives are straightforward since the first derivative is a constant (the slope) and the second derivative is zero.\n\n### Approach\n1. **Input Handling**: Convert the input to a 1-D NumPy array. If the input is a scalar, treat it as a single-element vector.\n2. **Compute Activation**: Calculate the affine activation values for each element in the input vector.\n3. **Compute Derivatives**: The first derivative is a constant value (slope) for all elements, and the second derivative is zero for all elements.\n4. **Rounding**: Round each element of the activation values and derivatives to four decimal places.\n5. **Conversion to Lists**: Convert the resulting NumPy arrays to Python lists and return them as a tuple.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to a 1-D numpy array, treating scalars as length-one vectors\n    x_arr = np.atleast_1d(np.asarray(x))\n    \n    # Compute the affine activation\n    y = slope * x_arr + intercept\n    \n    # First derivative is slope for all elements\n    grad = np.full_like(y, slope)\n    \n    # Second derivative is zero for all elements\n    grad2 = np.zeros_like(y)\n    \n    # Round each element to 4 decimal places and convert to lists\n    y_list = np.round(y, 4).tolist()\n    grad_list = np.round(grad, 4).tolist()\n    grad2_list = np.round(grad2, 4).tolist()\n    \n    return (y_list, grad_list, grad2_list)\n```\n\n### Explanation\n1. **Input Conversion**: The input is converted to a 1-D NumPy array using `np.atleast_1d` to handle scalars and ensure the input is treated as a vector.\n2. **Activation Calculation**: The affine function \\(f(x) = \\text{slope} \\cdot x + \\text{intercept}\\) is applied element-wise using vectorized operations for efficiency.\n3. **Derivatives**: The first derivative is simply the slope for all elements, and the second derivative is zero for all elements. These are generated using NumPy functions for consistency and performance.\n4. **Rounding and Conversion**: The results are rounded to four decimal places using `np.round` and converted to Python lists to meet the output requirements.\n\nThis approach ensures that the function is efficient, handles various input types correctly, and returns the results in the specified format."}
{"task_id": 497, "completion_id": 0, "completion": "To solve this problem, we need to compute the exponential activation function or its first two derivatives for a given input. The exponential function and its derivatives have the unique property that all higher-order derivatives are the same as the function itself. This allows us to use the same computation for the function and its first two derivatives, simplifying our implementation.\n\n### Approach\n1. **Check Order Validity**: First, we check if the provided `order` is valid (0, 1, or 2). If not, we return -1.\n2. **Convert Input to Numpy Array**: Convert the input `x` (which can be a list or a numpy array) into a numpy array for efficient computation.\n3. **Compute Exponential Values**: Use numpy's exponential function to compute the values for all elements in the input array.\n4. **Round Values**: Round each computed value to 6 decimal places to meet the precision requirement.\n5. **Convert Back to List**: Convert the result back to a Python list to maintain the same structure as the input.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef exponential_activation(x, order: int = 0):\n    \"\"\"Compute the exponential activation or one of its first two derivatives.\n\n    Parameters\n    ----------\n    x : list | numpy.ndarray\n        A (possibly nested) list or NumPy array containing real numbers.\n    order : int, optional\n        Indicates what to compute:\n        * 0 \u2013 activation itself (default)\n        * 1 \u2013 first derivative\n        * 2 \u2013 second derivative\n\n    Returns\n    -------\n    list | int\n        Python list mirroring the shape of *x* with every entry replaced by\n        the corresponding result rounded to 6 decimals, or -1 for invalid\n        *order*.\n    \"\"\"\n    if order not in {0, 1, 2}:\n        return -1\n    x_array = np.asarray(x)\n    result = np.exp(x_array)\n    rounded_result = np.around(result, decimals=6)\n    return rounded_result.tolist()\n```\n\n### Explanation\n- **Order Check**: The function first checks if the `order` is 0, 1, or 2. If not, it returns -1 immediately.\n- **Conversion to Numpy Array**: Using `np.asarray(x)`, we convert the input into a numpy array, which allows us to leverage numpy's efficient vectorized operations.\n- **Exponential Calculation**: The numpy `exp` function is applied element-wise to compute the exponential values.\n- **Rounding**: Each value is rounded to 6 decimal places using `np.around` to ensure the required precision.\n- **Conversion to List**: Finally, the result is converted back to a Python list using `tolist()` to maintain the original input structure, whether it was a nested list or a multi-dimensional numpy array.\n\nThis approach efficiently handles both lists and numpy arrays, ensuring the output structure matches the input while meeting the precision and functionality requirements."}
{"task_id": 498, "completion_id": 0, "completion": "To solve this problem, we need to implement the Leaky ReLU activation function and its derivatives for various input types, including scalars, lists, and NumPy arrays. The function should return the activation value, its first derivative, or its second derivative based on the specified order. \n\n### Approach\n1. **Input Handling**: Convert the input to a NumPy array to handle scalars, lists, and NumPy arrays uniformly.\n2. **Order Validation**: Check if the specified order is valid (0, 1, or 2). If not, return an empty list.\n3. **Computation**:\n   - **Order 0**: Compute the Leaky ReLU activation using a conditional approach where positive values remain unchanged and non-positive values are scaled by a factor `alpha`.\n   - **Order 1**: Compute the first derivative, which is 1 for positive values and `alpha` for non-positive values.\n   - **Order 2**: Return zeros as the second derivative is always zero.\n4. **Output Conversion**: Convert the result to a Python list. If the result is a scalar (0-dimensional array), wrap it in a single-element list to adhere to the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(x: np.ndarray | list | int | float, alpha: float = 0.3, order: int = 0) -> list:\n    \"\"\"Compute Leaky ReLU, its first derivative, or its second derivative.\n\n    Args:\n        x: Values to be transformed. Accepts NumPy array, list, or scalar.\n        alpha: Slope used when an element is non-positive (default 0.3).\n        order: 0 \u2013 return the function value; 1 \u2013 return the first derivative; 2 \u2013 return the second derivative.\n\n    Returns:\n        Python list with the same shape as *x* that contains the requested quantity.\n    \"\"\"\n    if order not in {0, 1, 2}:\n        return []\n    \n    x_arr = np.array(x)\n    \n    if order == 0:\n        result = np.where(x_arr > 0, x_arr, alpha * x_arr)\n    elif order == 1:\n        result = np.where(x_arr > 0, 1.0, alpha)\n    else:  # order == 2\n        result = np.zeros_like(x_arr)\n    \n    if result.ndim == 0:\n        return [result.tolist()]\n    return result.tolist()\n```\n\n### Explanation\n- **Input Handling**: The input `x` is converted to a NumPy array to ensure uniform processing regardless of the input type (scalar, list, or NumPy array).\n- **Order Validation**: The function checks if the order is 0, 1, or 2. Any other value results in an empty list.\n- **Computation**:\n  - For order 0, the Leaky ReLU activation is computed using `np.where` to apply the activation function element-wise.\n  - For order 1, the first derivative is computed similarly, returning 1 for positive values and `alpha` otherwise.\n  - For order 2, the second derivative is zero everywhere, so an array of zeros with the same shape as the input is created.\n- **Output Conversion**: The result is converted to a Python list. If the result is a scalar (0-dimensional array), it is wrapped in a single-element list to meet the problem's requirement of returning scalars in a one-element list. Otherwise, the result is directly converted to a list, preserving the input shape."}
{"task_id": 499, "completion_id": 0, "completion": "To solve this problem, we need to implement a Principal Component Analysis (PCA) routine from scratch to reduce the dimensionality of a given dataset. PCA is a statistical technique that transforms the data into a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n\n### Approach\n1. **Standardize the Data**: Each feature (column) of the dataset is standardized to have zero mean and unit population variance. If a feature has zero variance, it remains unchanged after centering.\n2. **Compute Covariance Matrix**: Using the standardized data, compute the sample covariance matrix with Bessel's correction (dividing by \\(n-1\\)).\n3. **Eigen Decomposition**: Perform eigen decomposition on the covariance matrix to obtain eigenvalues and eigenvectors.\n4. **Sort Eigenvalues and Eigenvectors**: Sort the eigenvalues in descending order and arrange the corresponding eigenvectors accordingly.\n5. **Adjust Eigenvector Signs**: Ensure each eigenvector's entry with the largest absolute value is positive to remove sign ambiguity.\n6. **Project Data**: Project the standardized data onto the first \\(k\\) eigenvectors (principal components).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\"\"\"\n    n_samples, n_features = data.shape\n    if k < 1 or k > n_features:\n        return -1\n    \n    # Step 1: Standardize the features (zero mean, unit population variance)\n    data_centered = data - np.mean(data, axis=0)\n    var_pop = np.var(data_centered, axis=0, ddof=0)\n    std = np.sqrt(var_pop)\n    std[std == 0] = 1.0  # Avoid division by zero where variance is zero\n    data_std = data_centered / std\n    \n    # Step 2: Compute the sample covariance matrix with Bessel's correction\n    cov_matrix = np.cov(data_std, rowvar=False, ddof=1)\n    \n    # Step 3: Eigen decomposition of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    # Step 4: Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues_sorted = eigenvalues[sorted_indices]\n    eigenvectors_sorted = eigenvectors[:, sorted_indices]\n    \n    # Step 5: Fix the sign of eigenvectors\n    for i in range(eigenvectors_sorted.shape[1]):\n        eigenvector = eigenvectors_sorted[:, i]\n        max_abs_idx = np.argmax(np.abs(eigenvector))\n        if eigenvector[max_abs_idx] < 0:\n            eigenvectors_sorted[:, i] = -eigenvector\n    \n    # Step 6: Project the standardized data onto the first k eigenvectors\n    projection = data_std @ eigenvectors_sorted[:, :k]\n    \n    # Round and convert to list\n    return np.round(projection, 4).tolist()\n```\n\n### Explanation\n1. **Standardization**: Each feature is centered (mean subtracted) and scaled by its population standard deviation. This ensures that each feature has zero mean and unit variance, making the covariance matrix computation meaningful.\n2. **Covariance Matrix**: The covariance matrix is computed using the standardized data. This matrix captures the covariance between each pair of features.\n3. **Eigen Decomposition**: The eigenvalues and eigenvectors of the covariance matrix are computed. Eigenvectors represent the directions of maximum variance, and eigenvalues represent the magnitude of variance in those directions.\n4. **Sorting**: Eigenvalues are sorted in descending order to prioritize the principal components that explain the most variance.\n5. **Sign Adjustment**: The sign of each eigenvector is adjusted to ensure the largest absolute value element is positive, ensuring deterministic output.\n6. **Projection**: The data is projected onto the first \\(k\\) principal components, reducing the dimensionality while retaining the most significant variance in the data.\n\nThis approach efficiently reduces the dimensionality of the dataset while preserving the essential structure and relationships in the data."}
{"task_id": 500, "completion_id": 0, "completion": "To solve this problem, we need to implement a small-scale Gradient Boosting binary classifier using decision stumps as weak learners. The classifier will use the squared-error loss function and predict binary labels by thresholding the final score at 0.5.\n\n### Approach\n1. **Initialization**: Compute the initial prediction as the mean of the training labels (denoted as `c`).\n2. **Residual Calculation**: At each boosting stage, compute the residuals as the difference between the true labels and the current predictions.\n3. **Decision Stump Training**: For each feature, find the optimal threshold that splits the data into two groups, minimizing the sum of squared errors (SSE) of the residuals. The optimal split for each feature is determined by evaluating all unique values of the feature as potential thresholds.\n4. **Model Update**: Update the model predictions by adding the contribution of the best decision stump found, scaled by the learning rate.\n5. **Prediction**: After training, compute the final score for each test sample by summing the contributions of all decision stumps and apply a threshold of 0.5 to convert the score into a binary label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int = 20, learning_rate: float = 0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\"\"\"\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    n_samples, n_features = X_train.shape\n    c = np.mean(y_train)\n    F = np.full(n_samples, c)\n    stumps = []\n    \n    for m in range(n_estimators):\n        residuals = y_train - F\n        \n        best_sse = float('inf')\n        best_j = None\n        best_t = None\n        best_left_val = None\n        best_right_val = None\n        \n        for j in range(n_features):\n            feature_values = X_train[:, j]\n            unique_values = np.unique(feature_values)\n            \n            for t in unique_values:\n                left_mask = feature_values <= t\n                n_left = left_mask.sum()\n                n_right = n_samples - n_left\n                if n_left == 0 or n_right == 0:\n                    continue\n                \n                left_res = residuals[left_mask]\n                right_res = residuals[~left_mask]\n                \n                left_val = left_res.mean()\n                right_val = right_res.mean()\n                \n                sse = np.sum((left_res - left_val)**2) + np.sum((right_res - right_val)**2)\n                \n                if sse < best_sse:\n                    best_sse = sse\n                    best_j = j\n                    best_t = t\n                    best_left_val = left_val\n                    best_right_val = right_val\n        \n        if best_j is not None:\n            stumps.append((best_j, best_t, best_left_val, best_right_val))\n            h_pred = np.where(X_train[:, best_j] <= best_t, best_left_val, best_right_val)\n            F += learning_rate * h_pred\n    \n    predictions = []\n    for x in X_test:\n        f = c\n        for (j, t, lv, rv) in stumps:\n            if x[j] <= t:\n                f += learning_rate * lv\n            else:\n                f += learning_rate * rv\n        predictions.append(1 if f >= 0.5 else 0)\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The initial prediction `F` is set to the mean of the training labels `c`, which serves as the baseline prediction.\n2. **Residual Calculation**: At each boosting iteration, residuals are computed as the difference between the actual labels and the current predictions.\n3. **Decision Stump Training**: For each feature, potential thresholds are evaluated. The best threshold is chosen based on the SSE of the residuals. The decision stump (weak learner) splits the data into two groups and predicts the mean residual for each group.\n4. **Model Update**: The model's predictions are updated by adding the contribution of the best decision stump found, scaled by the learning rate. This iterative process continues for the specified number of estimators.\n5. **Prediction**: For each test sample, the final score is computed by summing the initial prediction and the contributions of all decision stumps. The score is thresholded at 0.5 to determine the binary class label (0 or 1).\n\nThis approach efficiently builds an additive model using gradient boosting, leveraging decision stumps to iteratively improve predictions and minimize the squared-error loss."}
{"task_id": 501, "completion_id": 0, "completion": "To solve this problem, we need to create a utility function that returns a constant learning rate regardless of the current training step. This function is essential for optimization algorithms where a fixed learning rate is required throughout the training process.\n\n### Approach\nThe approach to solving this problem is straightforward. The function will take two parameters: `initial_lr`, which is the fixed learning rate to be returned, and `step`, which is the current training step but is ignored. The function will simply return the `initial_lr` value without any modifications, ensuring that the learning rate remains constant across all training steps. No validation or adjustment is performed on the `initial_lr`, even if it is negative, as per the problem requirements.\n\n### Solution Code\n\n```python\ndef constant_scheduler(initial_lr: float, step: int) -> float:\n    \"\"\"Constant learning-rate scheduler.\n\n    Parameters\n    ----------\n    initial_lr : float\n        The fixed learning rate that should be returned on every call.\n    step : int\n        Current training step (ignored).\n\n    Returns\n    -------\n    float\n        The same value as `initial_lr`.\n    \"\"\"\n    return initial_lr\n```\n\n### Explanation\nThe `constant_scheduler` function is designed to maintain a fixed learning rate throughout the training process. Here's a breakdown of how it works:\n\n1. **Parameters**:\n   - `initial_lr` (float): This is the learning rate that will be returned every time the function is called. It remains unchanged regardless of the training step.\n   - `step` (int): This parameter is included for compatibility with other learning rate schedulers but is not used in the calculation. It is ignored by the function.\n\n2. **Return Value**:\n   - The function returns `initial_lr` directly, ensuring that the learning rate does not change over time. This means that every call to the function will yield the same learning rate value provided initially.\n\nThis approach ensures simplicity and efficiency, as there are no complex calculations or validations involved. The function meets the requirement of providing a constant learning rate, making it suitable for scenarios where a fixed learning rate is necessary for the optimization process."}
{"task_id": 502, "completion_id": 0, "completion": "To solve this problem, we need to implement the Hard-Sigmoid activation function and its first and second derivatives efficiently using NumPy. The Hard-Sigmoid function is a piece-wise linear approximation of the logistic sigmoid, which is computationally cheaper and introduces non-linearity, making it suitable for deep-learning models.\n\n### Approach\nThe Hard-Sigmoid function is defined as follows:\n- 0 for z < -2.5\n- 0.2z + 0.5 for -2.5 \u2264 z \u2264 2.5\n- 1 for z > 2.5\n\nThe first derivative of the Hard-Sigmoid function is:\n- 0.2 for -2.5 \u2264 z \u2264 2.5\n- 0 otherwise\n\nThe second derivative is always 0 for all values of z.\n\nThe approach to implement this involves using vectorized operations in NumPy for efficiency:\n1. **Activation (order 0)**: Compute the linear part and clip the values to the range [0, 1] using `np.clip`.\n2. **First Derivative (order 1)**: Use a boolean mask to check where the values lie between -2.5 and 2.5 and apply 0.2 in those regions, 0 otherwise.\n3. **Second Derivative (order 2)**: Return an array of zeros with the same shape as the input.\n\nThe results are then rounded to 4 decimal places and converted to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hard_sigmoid(z: np.ndarray, order: int = 0) -> list[float]:\n    \"\"\"Compute Hard-Sigmoid activation or its derivatives.\"\"\"\n    if order not in {0, 1, 2}:\n        return []\n    \n    if order == 0:\n        result = 0.2 * z + 0.5\n        result = np.clip(result, 0, 1)\n    elif order == 1:\n        mask = (z >= -2.5) & (z <= 2.5)\n        result = np.where(mask, 0.2, 0.0)\n    else:  # order == 2\n        result = np.zeros_like(z)\n    \n    rounded = np.round(result, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n- **Activation (order 0)**: The linear transformation `0.2 * z + 0.5` is applied to all elements of the input array. The `np.clip` function ensures values outside the range [-2.5, 2.5] are clamped to 0 and 1 respectively.\n- **First Derivative (order 1)**: A boolean mask identifies elements within the range [-2.5, 2.5]. The `np.where` function applies 0.2 where the mask is true and 0 otherwise.\n- **Second Derivative (order 2)**: Directly returns an array of zeros using `np.zeros_like`, which matches the shape and type of the input array.\n\nThis approach efficiently handles element-wise operations using NumPy, ensuring optimal performance and correctness for any input array. The results are rounded to 4 decimal places and returned as a Python list, meeting the problem requirements."}
{"task_id": 503, "completion_id": 0, "completion": "To solve this problem, we need to update a dictionary of hyperparameters with new values from another dictionary, but only for keys that already exist in the original hyperparameters. This ensures that any new keys in the updates dictionary are ignored, preserving the structure of the original hyperparameters.\n\n### Approach\n1. **Check for None**: If the original hyperparameters dictionary (`hparams`) is `None`, return `-1` immediately as specified.\n2. **Deep Copy**: Create a deep copy of the original hyperparameters to avoid modifying the input dictionary directly.\n3. **Apply Updates**: If there are updates provided, iterate through each key-value pair in the updates dictionary. For each key, check if it exists in the copied hyperparameters. If it does, update the value in the copied hyperparameters with the new value from the updates.\n4. **Return Result**: Return the modified deep copy of the hyperparameters.\n\n### Solution Code\n```python\nfrom copy import deepcopy\n\ndef selective_update(hparams: dict | None, updates: dict | None) -> dict | int:\n    \"\"\"Update only existing keys in *hparams* with values from *updates*.\"\"\"\n\n    if hparams is None:\n        return -1\n\n    copied_hparams = deepcopy(hparams)\n\n    if updates is not None:\n        for key, value in updates.items():\n            if key in copied_hparams:\n                copied_hparams[key] = value\n\n    return copied_hparams\n```\n\n### Explanation\n- **Deep Copy**: Using `deepcopy` ensures that nested structures within the hyperparameters are fully copied, preventing any unintended side effects on the original data.\n- **Key Check**: By checking if each key in the updates exists in the copied hyperparameters before applying the update, we ensure that only existing keys are modified. This maintains the integrity of the original hyperparameters' structure.\n- **Efficiency**: The solution efficiently processes each key in the updates dictionary exactly once, making the approach linear in complexity relative to the size of the updates dictionary, which is optimal for this task."}
{"task_id": 505, "completion_id": 0, "completion": "To solve this problem, we need to implement a single weight-update step for a binary Restricted Boltzmann Machine (RBM) using the Contrastive Divergence (CD-k) algorithm. The goal is to update the weight matrix based on the given mini-batch of visible vectors, current weights, biases, learning rate, and the number of Gibbs sampling steps.\n\n### Approach\n1. **Check for Empty Mini-batch**: If the input mini-batch is empty, return an empty list immediately.\n2. **Compute Positive Phase Hidden Probabilities**: Calculate the hidden layer probabilities using the visible input data.\n3. **Gibbs Sampling Steps**: Perform `k` Gibbs sampling steps to approximate the negative phase. Each step involves updating the visible and hidden probabilities without stochastic sampling.\n4. **Compute Gradients**: Calculate the positive and negative gradients using the probabilities from the positive phase and the `k`-th Gibbs step.\n5. **Update Weights**: Adjust the weight matrix using the computed gradients and learning rate.\n6. **Format the Result**: Round the updated weights to 4 decimal places and convert them to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\"\"\"\n    if X.size == 0:\n        return []\n    \n    m = X.shape[0]\n    # Positive phase: compute hidden probabilities given visible input X\n    h0_input = X.dot(W) + hbias\n    h0_prob = 1 / (1 + np.exp(-h0_input))\n    \n    # Negative phase: run k Gibbs steps\n    if k == 0:\n        v_k = X\n        h_k = h0_prob\n    else:\n        current_h = h0_prob\n        for _ in range(k):\n            v_input = current_h.dot(W.T) + vbias\n            v = 1 / (1 + np.exp(-v_input))\n            h_input = v.dot(W) + hbias\n            current_h = 1 / (1 + np.exp(-h_input))\n        v_k = v\n        h_k = current_h\n    \n    # Compute gradients\n    pos_grad = X.T.dot(h0_prob)\n    neg_grad = v_k.T.dot(h_k)\n    \n    # Update weights\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n    \n    # Round and convert to list of lists\n    return np.round(W_new, 4).tolist()\n```\n\n### Explanation\n1. **Check for Empty Mini-batch**: The function first checks if the input mini-batch `X` is empty. If it is, an empty list is returned immediately.\n2. **Positive Phase**: The hidden probabilities (`h0_prob`) are computed using the sigmoid activation function applied to the linear transformation of the visible input `X` and the hidden biases `hbias`.\n3. **Gibbs Sampling Steps**: For each of the `k` steps, the visible probabilities are computed from the hidden probabilities, and then the hidden probabilities are updated again. This process is repeated `k` times to approximate the negative phase.\n4. **Gradients Calculation**: The positive gradient is calculated using the outer product of the visible input `X` and the initial hidden probabilities `h0_prob`. The negative gradient is calculated using the outer product of the visible and hidden probabilities obtained after `k` Gibbs steps.\n5. **Weight Update**: The weight matrix is updated using the difference between the positive and negative gradients, scaled by the learning rate and the number of samples in the mini-batch.\n6. **Result Formatting**: The updated weight matrix is rounded to 4 decimal places and converted to a list of lists for the final output.\n\nThis approach efficiently computes the necessary updates to the weight matrix using the Contrastive Divergence algorithm, ensuring that the model learns from the data distribution effectively."}
{"task_id": 508, "completion_id": 0, "completion": "To solve this problem, we need to extract values from a dictionary based on keys that contain a specific substring, while preserving the insertion order of the dictionary. The solution should be case-sensitive and return an empty list if no keys match the substring.\n\n### Approach\n1. **Iterate Through Keys**: Loop through each key in the dictionary in insertion order. In Python 3.7 and above, dictionaries preserve insertion order by default.\n2. **Check for Substring**: For each key, check if the given substring is present within the key. This check is case-sensitive.\n3. **Collect Values**: If a key contains the substring, add the corresponding value to the result list.\n4. **Return Result**: Return the collected values in the order they were encountered, which matches the insertion order of the keys.\n\n### Solution Code\n```python\nfrom typing import Any, Dict, List\n\ndef params_with_name(params: Dict[str, Any], name: str) -> List[Any]:\n    \"\"\"Return all values whose keys contain a given substring.\n\n    The function must preserve the insertion order of the dictionary.\n\n    Args:\n        params: Dictionary mapping parameter names (strings) to parameter values (any type).\n        name: Substring to be searched for inside the dictionary keys.\n\n    Returns:\n        List of values whose corresponding keys contain *name* (order preserved).\n    \"\"\"\n    return [value for key, value in params.items() if name in key]\n```\n\n### Explanation\n- **Iteration and Order Preservation**: The solution uses a list comprehension to iterate over the dictionary's items. This ensures that the order of the values in the result list matches the insertion order of the keys in the dictionary.\n- **Substring Check**: The `in` operator is used to check if the substring `name` is present in each key. This check is case-sensitive, as required.\n- **Efficiency**: The approach efficiently processes each key-value pair exactly once, resulting in a time complexity of O(n), where n is the number of entries in the dictionary. This is optimal for this problem since each key must be checked individually.\n\nThis solution is concise and leverages Python's built-in features to ensure correctness and efficiency."}
{"task_id": 509, "completion_id": 0, "completion": "To solve this problem, we need to compute the Chebyshev distance between two real-valued vectors. The Chebyshev distance is defined as the maximum absolute difference between corresponding elements of the vectors. The solution must handle both Python lists and NumPy 1-D arrays, verify that the vectors are of the same length, and return the result rounded to four decimal places as a Python float.\n\n### Approach\n1. **Check Lengths**: Verify that the two input vectors have the same length. If not, return -1.0.\n2. **Handle Empty Vectors**: If both vectors are empty (length 0), return 0.0 as the Chebyshev distance for empty vectors is considered 0 by convention.\n3. **Convert to NumPy Arrays**: Convert the input vectors to NumPy arrays to leverage efficient element-wise operations.\n4. **Compute Differences**: Calculate the absolute differences between corresponding elements of the two vectors.\n5. **Find Maximum Difference**: Determine the maximum value from the computed differences.\n6. **Round Result**: Round the maximum difference to four decimal places and return it as a Python float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    if len(x) != len(y):\n        return -1.0\n    if len(x) == 0:\n        return 0.0\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    diff = np.abs(x_arr - y_arr)\n    max_diff = np.max(diff)\n    return round(max_diff, 4)\n```\n\n### Explanation\n1. **Check Lengths**: The function first checks if the lengths of the two vectors are equal. If not, it returns -1.0 immediately.\n2. **Handle Empty Vectors**: If both vectors are empty, the function returns 0.0, as there are no elements to compare and the distance is conventionally considered zero.\n3. **Convert to Arrays**: The vectors are converted to NumPy arrays to utilize NumPy's efficient element-wise operations.\n4. **Compute Differences**: Using NumPy's element-wise subtraction and absolute value functions, the differences between corresponding elements are computed.\n5. **Find Maximum**: The maximum value of these differences is found using NumPy's max function.\n6. **Round Result**: The maximum difference is rounded to four decimal places using Python's `round` function to ensure the result is a standard Python float.\n\nThis approach efficiently handles both Python lists and NumPy arrays, ensuring correctness and adherence to the problem constraints."}
{"task_id": 510, "completion_id": 0, "completion": "To solve this problem, we need to implement the Variational Auto-Encoder (VAE) loss function, which consists of two components: the reconstruction loss and the KL divergence. The reconstruction loss is calculated using binary cross-entropy, and the KL divergence measures the difference between the learned latent variable distribution and a standard Gaussian distribution. The final loss is the average of the sum of these two components over a mini-batch, rounded to six decimal places.\n\n### Approach\n1. **Clipping for Numerical Stability**: To avoid numerical instability when computing logarithms, each element of the predicted probabilities (`y_pred`) is clipped to a small interval around [0, 1] using machine epsilon.\n2. **Reconstruction Loss**: Compute the binary cross-entropy loss between the true input (`y`) and the clipped predicted probabilities (`y_pred_clipped`). This is done element-wise and then summed over all features for each sample in the batch.\n3. **KL Divergence**: Calculate the KL divergence using the mean (`t_mean`) and log-variance (`t_log_var`) of the latent variables. The formula provided sums over the latent dimensions for each sample in the batch.\n4. **Total Loss**: Sum the reconstruction loss and KL divergence for each sample, then average these values over the entire batch. The result is rounded to six decimal places to ensure precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    epsilon = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    \n    # Compute reconstruction loss per sample\n    bce = y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped)\n    reconstruction_loss = -np.sum(bce, axis=1)\n    \n    # Compute KL divergence per sample\n    kl = -0.5 * np.sum(1 + t_log_var - np.square(t_mean) - np.exp(t_log_var), axis=1)\n    \n    # Average total loss over the batch\n    total_loss = reconstruction_loss + kl\n    average_loss = np.mean(total_loss)\n    \n    return round(average_loss.item(), 6)\n```\n\n### Explanation\n1. **Clipping**: The predicted probabilities are clipped to avoid values of 0 or 1, which can cause numerical instability when taking logarithms.\n2. **Reconstruction Loss Calculation**: The binary cross-entropy (BCE) is calculated element-wise and summed over all features for each sample. This gives the reconstruction loss for each sample in the batch.\n3. **KL Divergence Calculation**: The KL divergence is computed using the analytical formula provided, which sums over the latent dimensions for each sample. This measures how much the learned latent distribution deviates from a standard Gaussian distribution.\n4. **Averaging**: The total loss for each sample is the sum of the reconstruction loss and KL divergence. The average of these total losses over the batch is computed and rounded to six decimal places to produce the final result.\n\nThis approach ensures numerical stability and correctness while efficiently computing the necessary components of the VAE loss function."}
{"task_id": 511, "completion_id": 0, "completion": "To solve this problem, we need to generate a list of success probabilities for a Bernoulli multi-armed bandit (MAB) environment. Each arm in the MAB has a success probability that is drawn from a uniform distribution and then normalized to ensure they form a valid probability distribution. The solution must handle edge cases such as when the sum of the drawn samples is zero and ensure the probabilities are rounded to four decimal places.\n\n### Approach\n1. **Check Validity of Input**: If the number of arms (`n_arms`) is less than 1, return an empty list immediately.\n2. **Generate Samples**: Draw `n_arms` samples from a uniform distribution in the interval [0, 1).\n3. **Handle Zero Sum Case**: If the sum of the samples is zero (which can happen if all samples are zero), assign each arm an equal probability of 1/n_arms.\n4. **Normalize Samples**: If the sum is non-zero, normalize each sample by dividing it by the sum of all samples to ensure the probabilities sum to 1.\n5. **Round Probabilities**: Round each normalized probability to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_bernoulli_mab(n_arms: int = 10) -> list[float]:\n    \"\"\"Generates a list of success probabilities for a Bernoulli MAB environment.\"\"\"\n    if n_arms < 1:\n        return []\n    samples = np.random.uniform(0, 1, n_arms)\n    sum_samples = samples.sum()\n    if sum_samples == 0:\n        probs = np.ones(n_arms) / n_arms\n    else:\n        probs = samples / sum_samples\n    return [round(p, 4) for p in probs]\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if `n_arms` is less than 1. If so, it returns an empty list as specified.\n2. **Sample Generation**: Using numpy's `uniform` function, we generate `n_arms` samples from the interval [0, 1).\n3. **Sum Check**: The sum of these samples is computed. If the sum is zero (indicative of all samples being zero), each arm is assigned an equal probability of 1/n_arms to avoid division by zero and ensure a valid distribution.\n4. **Normalization**: If the sum is non-zero, each sample is divided by the sum to normalize the probabilities, ensuring they sum to 1.\n5. **Rounding**: Each probability is rounded to four decimal places to meet the specified precision, resulting in the final list of probabilities.\n\nThis approach ensures that the generated probabilities form a valid distribution, handles edge cases gracefully, and adheres to the precision requirements."}
{"task_id": 512, "completion_id": 0, "completion": "To solve this problem, we need to compute the output spatial dimensions of a 2-D pooling layer given the kernel size, input image shape, and stride values. The solution must ensure that the kernel fits within the input dimensions and that the resulting dimensions are integers. If these conditions are not met, the configuration is considered invalid.\n\n### Approach\n1. **Extract Parameters**: Extract the kernel dimensions, input image dimensions, and stride values from the given input tuples.\n2. **Check Stride Validity**: Ensure that both stride values are positive integers, as a non-positive stride is invalid.\n3. **Kernel Fit Check**: Verify that the kernel dimensions do not exceed the corresponding input dimensions.\n4. **Divisibility Check**: Check if the difference between the input dimension and kernel dimension is exactly divisible by the corresponding stride value. This ensures that the resulting dimensions are integers.\n5. **Compute Output Dimensions**: If all checks pass, compute the output dimensions using the given formula. If any check fails, return -1.\n\n### Solution Code\n```python\ndef pooling_shape(\n    pool_shape: tuple[int, int],\n    image_shape: tuple[int, int, int, int],\n    stride: tuple[int, int],\n) -> tuple[int, int] | int:\n    \"\"\"Calculate the spatial output dimensions of a 2-D pooling layer.\"\"\"\n\n    k_h, k_w = pool_shape\n    _, _, h, w = image_shape\n    s_h, s_w = stride\n\n    # Check if strides are positive\n    if s_h <= 0 or s_w <= 0:\n        return -1\n\n    # Check kernel fits within the input dimensions\n    if k_h > h or k_w > w:\n        return -1\n\n    # Check if (h - k_h) is divisible by s_h and (w - k_w) by s_w\n    if (h - k_h) % s_h != 0 or (w - k_w) % s_w != 0:\n        return -1\n\n    # Calculate output dimensions\n    out_h = (h - k_h) // s_h + 1\n    out_w = (w - k_w) // s_w + 1\n\n    return (out_h, out_w)\n```\n\n### Explanation\n1. **Parameter Extraction**: The function starts by unpacking the kernel dimensions (`k_h`, `k_w`), input image dimensions (`h`, `w`), and strides (`s_h`, `s_w`).\n2. **Stride Validity Check**: The strides must be positive integers to ensure valid sliding window operations. If either stride is non-positive, the configuration is invalid.\n3. **Kernel Fit Check**: The kernel must fit within the input dimensions. If the kernel is larger than the input in any dimension, the configuration is invalid.\n4. **Divisibility Check**: The difference between input and kernel dimensions must be exactly divisible by the corresponding stride to ensure integer output dimensions. If not, the configuration is invalid.\n5. **Output Calculation**: If all checks pass, the output dimensions are computed using integer division to ensure valid results. The result is returned as a tuple. If any check fails, -1 is returned indicating an invalid configuration.\n\nThis approach ensures that all edge cases are handled, such as invalid strides, oversized kernels, and non-integer output dimensions, providing a robust solution to determine valid pooling layer output dimensions."}
{"task_id": 513, "completion_id": 0, "completion": "To solve this problem, we need to implement a Factorization Machine (FM) regression model to predict values for a given set of samples. The FM model combines linear regression with pair-wise feature interactions, which allows it to capture complex relationships between features. \n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Convert Inputs to NumPy Arrays**: Convert the input lists to NumPy arrays for efficient numerical computations.\n2. **Compute Linear Term**: Calculate the linear part of the prediction, which includes the bias term and the dot product of the feature matrix with the linear weights.\n3. **Compute Interaction Term**: This term captures the pair-wise interactions between features using latent factors. It involves two main parts:\n   - The sum of squares of the dot product of the feature matrix with the latent factors.\n   - The sum of the squares of the element-wise product of the feature matrix and the latent factors.\n4. **Combine Terms**: Sum the linear term and the interaction term to get the final predictions.\n5. **Round Results**: Round the predictions to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\"\"\"\n    X_np = np.array(X)\n    w_np = np.array(w)\n    V_np = np.array(V)\n    \n    # Compute linear term: w0 + X * w\n    linear_term = w0 + X_np.dot(w_np)\n    \n    # Compute interaction term components\n    XV = X_np.dot(V_np)  # Shape (n_samples, k)\n    sum_XV_sq = (XV ** 2).sum(axis=1)  # Sum over latent factors\n    \n    X_sq = X_np ** 2\n    V_sq = V_np ** 2\n    sum_XV_sq_terms = (X_sq.dot(V_sq)).sum(axis=1)  # Sum over latent factors\n    \n    interaction_term = 0.5 * (sum_XV_sq - sum_XV_sq_terms)\n    \n    # Combine terms and round to 4 decimal places\n    predictions = linear_term + interaction_term\n    rounded_predictions = np.round(predictions, 4)\n    \n    return rounded_predictions.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The input lists `X`, `w`, and `V` are converted to NumPy arrays to leverage efficient vectorized operations.\n2. **Linear Term Calculation**: The linear term is computed by adding the bias term `w0` to the dot product of the feature matrix `X` with the linear weights `w`.\n3. **Interaction Term Calculation**:\n   - **Dot Product with Latent Factors**: The product of the feature matrix `X` and the latent factors `V` is computed. This product is then squared and summed over the latent factors.\n   - **Element-wise Squares**: The feature matrix and latent factors are squared element-wise. The product of these squared matrices is summed over the features and then over the latent factors.\n   - **Interaction Term**: The interaction term is derived by taking the difference between the two computed sums, scaling it by 0.5.\n4. **Combining Terms**: The final predictions are obtained by adding the linear term and the interaction term.\n5. **Rounding**: The predictions are rounded to four decimal places using NumPy's rounding function for precision.\n\nThis approach ensures that the solution is efficient and handles arbitrary dimensions for the input matrices, providing accurate predictions as required."}
{"task_id": 514, "completion_id": 0, "completion": "To solve this problem, we need to create a function that updates the current phase of a machine-learning workflow between \"training\" and \"testing\" based on optional Boolean switches. The function must handle invalid inputs appropriately and toggle the phase if no switches are provided.\n\n### Approach\n1. **Input Validation**: Check if the current phase is either \"training\" or \"testing\". If not, return -1.\n2. **Check for Conflicting Switches**: Ensure that at most one of the switches (`set_train` or `set_test`) is provided. If both are provided, return -1.\n3. **Validate Switch Types**: Ensure that any provided switches are Boolean values. If a non-Boolean value is provided, return -1.\n4. **Determine New Phase**:\n   - If `set_train` is provided, set the phase to \"training\" if `set_train` is True, otherwise set it to \"testing\".\n   - If `set_test` is provided, set the phase to \"testing\" if `set_test` is True, otherwise set it to \"training\".\n   - If neither switch is provided, toggle the current phase between \"training\" and \"testing\".\n\n### Solution Code\n```python\ndef switch_phase(current_phase: str,\n                 set_train: bool | None = None,\n                 set_test: bool | None = None) -> str | int:\n    \"\"\"Update the training/testing phase.\"\"\"\n    # Validate current_phase is either \"training\" or \"testing\"\n    if current_phase not in (\"training\", \"testing\"):\n        return -1\n    \n    # Check if both set_train and set_test are provided (not None)\n    if set_train is not None and set_test is not None:\n        return -1\n    \n    # Validate that any provided switches are boolean\n    if (set_train is not None and not isinstance(set_train, bool)) or \\\n       (set_test is not None and not isinstance(set_test, bool)):\n        return -1\n    \n    # Determine new phase based on switches\n    if set_train is not None:\n        new_phase = \"training\" if set_train else \"testing\"\n    elif set_test is not None:\n        new_phase = \"testing\" if set_test else \"training\"\n    else:\n        # Toggle the phase\n        new_phase = \"testing\" if current_phase == \"training\" else \"training\"\n    \n    return new_phase\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the `current_phase` is valid. If not, it returns -1.\n2. **Conflicting Switches Check**: The function ensures that both `set_train` and `set_test` cannot be provided simultaneously. If they are, it returns -1.\n3. **Switch Type Validation**: The function checks that any provided switches are Boolean values. Non-Boolean values result in returning -1.\n4. **Phase Determination**:\n   - If `set_train` is provided, the phase is set based on its Boolean value.\n   - If `set_test` is provided, the phase is set based on its Boolean value.\n   - If neither switch is provided, the phase is toggled from the current value.\n\nThis approach ensures the function adheres to the specified rules and handles all edge cases, providing a robust solution for switching between training and testing phases in a machine-learning workflow."}
{"task_id": 515, "completion_id": 0, "completion": "To solve this problem, we need to flatten a nested dictionary such that each key in the resulting dictionary represents the path to the value in the original nested structure, with keys separated by dots. This is useful for logging, grid-search, or command-line interfaces where a flat structure is more convenient.\n\n### Approach\n1. **Recursive Flattening**: The function processes each key-value pair in the input dictionary. If a value is itself a dictionary, the function recursively flattens this sub-dictionary.\n2. **Key Concatenation**: When a nested dictionary is encountered, the keys from the nested structure are concatenated with the parent keys using a dot (`.`).\n3. **Base Case Handling**: If the input dictionary is empty, the function returns an empty dictionary. Non-dictionary values are added directly to the result without modification.\n4. **Immutable Input**: The input dictionary is not mutated; a new dictionary is created and returned.\n\n### Solution Code\n```python\nfrom typing import Any, Dict\n\ndef flatten_parameters(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Flatten a nested parameter dictionary.\n\n    A dot (`.`) is used to join nested keys.  The input dictionary must not be\n    mutated.\n\n    Args:\n        params: A potentially nested dictionary where some values can be other\n                dictionaries.\n\n    Returns:\n        A new dictionary without nested structures.\n    \"\"\"\n    result = {}\n    for key, value in params.items():\n        if isinstance(value, dict):\n            sub_flattened = flatten_parameters(value)\n            for sub_key, sub_value in sub_flattened.items():\n                new_key = f\"{key}.{sub_key}\"\n                result[new_key] = sub_value\n        else:\n            result[key] = value\n    return result\n```\n\n### Explanation\n- **Recursive Processing**: The function iterates over each key-value pair in the input dictionary. If a value is a dictionary, the function calls itself recursively to flatten this sub-dictionary.\n- **Key Concatenation**: For each key in the flattened sub-dictionary, the parent key is prepended to the sub-key, separated by a dot, forming the new key in the result.\n- **Non-Dictionary Values**: Values that are not dictionaries are added directly to the result with their original keys.\n- **Efficiency**: The function efficiently processes each key-value pair exactly once, ensuring linear time complexity relative to the number of key-value pairs in the input and nested dictionaries.\n\nThis approach ensures that the input dictionary remains unmodified while producing a new flattened dictionary that accurately represents the nested structure through dot-separated keys."}
{"task_id": 516, "completion_id": 0, "completion": "To solve this problem, we need to compute the Euclidean distance between two vectors, which can be provided as either Python lists or NumPy arrays. The Euclidean distance is a measure of the straight-line distance between two points in Euclidean space and is calculated using the formula:\n\n\\[ d(\\mathbf a, \\mathbf b) = \\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^2} \\]\n\n### Approach\n1. **Input Conversion and Validation**: Convert the input vectors to one-dimensional NumPy arrays to handle both lists and arrays uniformly. Check if the lengths of the two vectors are the same. If not, return -1.\n2. **Distance Calculation**: Compute the Euclidean distance using vectorized operations for efficiency. This involves subtracting the vectors, squaring the differences, summing the squared values, taking the square root of the sum, and rounding the result to four decimal places.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef euclidean_distance(a, b):\n    \"\"\"Compute the Euclidean distance between two numeric vectors.\n\n    The vectors may be provided as Python lists or NumPy arrays and may contain\n    integers or floats.  If the vectors have different numbers of total\n    elements, the function must return -1.\n\n    Args:\n        a (list[int | float] | np.ndarray): First vector.\n        b (list[int | float] | np.ndarray): Second vector.\n\n    Returns:\n        float: Euclidean distance rounded to 4 decimal places, or -1 if the\n               vectors are of unequal length.\n    \"\"\"\n    a_array = np.asarray(a).flatten()\n    b_array = np.asarray(b).flatten()\n    \n    if a_array.size != b_array.size:\n        return -1\n    \n    difference = a_array - b_array\n    squared_diff = np.square(difference)\n    sum_squared = np.sum(squared_diff)\n    distance = math.sqrt(sum_squared)\n    return round(distance, 4)\n```\n\n### Explanation\n1. **Input Conversion**: The inputs are converted to one-dimensional NumPy arrays using `np.asarray().flatten()`, ensuring that any nested structures or multi-dimensional arrays are flattened into a single dimension.\n2. **Length Check**: The sizes of the flattened arrays are compared. If they differ, the function returns -1 immediately.\n3. **Distance Calculation**: The Euclidean distance is computed using vectorized operations for efficiency. The difference between the arrays is calculated, squared, summed, and then the square root of the sum is taken. The result is rounded to four decimal places using Python's `round` function.\n\nThis approach ensures that the function handles various input types efficiently and correctly, leveraging NumPy's optimized operations for performance."}
{"task_id": 517, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that computes the sigmoid activation function in a numerically stable manner for various input types, including scalars, lists, tuples, and NumPy arrays. The function should return the result rounded to four decimal places and preserve the input structure.\n\n### Approach\n1. **Input Handling**: The function first checks if the input is a scalar (int or float). If it is, the input is converted to a NumPy array to leverage vectorized operations. For other input types (lists, tuples, or NumPy arrays), the input is directly converted to a NumPy array.\n2. **Numerically Stable Sigmoid Calculation**: The sigmoid function is computed using a numerically stable approach to avoid overflow for very large positive or negative values. This is done by splitting the computation based on the sign of the input values:\n   - For non-negative values, compute \\( \\frac{1}{1 + e^{-z}} \\).\n   - For negative values, compute \\( \\frac{e^z}{1 + e^z} \\).\n3. **Rounding**: The computed sigmoid values are rounded to four decimal places using NumPy's rounding function to ensure consistency.\n4. **Output Structure**: The result is converted back to the original input structure. If the input was a scalar, the result is returned as a float. Otherwise, the result is returned as a nested Python list using NumPy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    is_scalar = isinstance(z, (int, float))\n    if is_scalar:\n        z = np.array([z])\n    else:\n        z = np.asarray(z)\n    \n    # Compute the sigmoid in a numerically stable way\n    sigmoid_values = np.where(z >= 0, \n                              1 / (1 + np.exp(-z)), \n                              np.exp(z) / (1 + np.exp(z)))\n    \n    # Round to four decimal places\n    rounded = np.round(sigmoid_values, 4)\n    \n    if is_scalar:\n        return float(rounded[0])\n    else:\n        return rounded.tolist()\n```\n\n### Explanation\n- **Input Handling**: The function checks if the input is a scalar and converts it to a NumPy array for uniform processing. Non-scalar inputs are converted to NumPy arrays to handle element-wise operations efficiently.\n- **Sigmoid Calculation**: Using `np.where`, the function applies different computations based on the sign of each element in the input array. This avoids overflow by preventing the evaluation of large exponentials directly.\n- **Rounding**: The results are rounded to four decimal places using `np.round`, ensuring consistent rounding across all elements.\n- **Output Structure**: The function returns a float if the input was a scalar. For other input types, the result is converted to a nested list using `tolist()`, preserving the structure of the original input.\n\nThis approach ensures that the function is both efficient and numerically stable, handling all specified input types correctly and returning results in the required format."}
{"task_id": 518, "completion_id": 0, "completion": "To solve this problem, we need to create a decorator that automatically converts one-hot encoded label matrices into 1-D integer arrays before passing them to a machine learning metric function. This conversion is necessary because most metric functions expect labels in a 1-D integer format rather than one-hot encoded matrices.\n\n### Approach\nThe decorator, named `unhot`, will wrap around any metric function that takes two arguments: `actual` and `predicted`, both of which are NumPy arrays. The decorator's task is to check if these arrays are one-hot encoded (2-D arrays with the second dimension greater than 1). If they are, it converts them to 1-D integer arrays using `argmax(axis=1)`. If the arrays are already 1-D or have a shape of (n, 1), they are left unchanged. The converted arrays are then passed to the metric function, ensuring compatibility with functions expecting 1-D inputs.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n\n    def wrapper(actual: np.ndarray, predicted: np.ndarray):\n        # Process actual array\n        if actual.ndim == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        \n        # Process predicted array\n        if predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        \n        return function(actual, predicted)\n    \n    return wrapper\n```\n\n### Explanation\n1. **Decorator Definition**: The `unhot` decorator takes a metric function as input and returns a wrapped version of this function.\n2. **Wrapper Function**: The wrapper function processes the `actual` and `predicted` arrays:\n   - **Check for One-Hot Encoding**: For each array, it checks if the array is 2-D and if the second dimension is greater than 1. If true, it converts the array to a 1-D integer array using `argmax(axis=1)`.\n   - **Pass Through**: If the array is already 1-D or has a shape of (n, 1), it is passed through without modification.\n3. **Call Original Function**: The processed arrays are then passed to the original metric function, which computes the result using the converted arrays.\n\nThis approach ensures that the metric function always receives 1-D integer arrays, regardless of whether the inputs were originally one-hot encoded. This allows the metric function to work correctly without needing to handle one-hot encoding internally."}
{"task_id": 520, "completion_id": 0, "completion": "To solve this problem, we need to compute the gradient of the sigmoid function given its output values. The gradient is computed element-wise using the formula \\(g'(z) = z \\cdot (1 - z)\\), where \\(z\\) is the output of the sigmoid function. The solution must handle various input types, including scalars, nested lists, and NumPy arrays, and return the result in the same structure as the input, with each value rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: The function should accept Python scalars, nested lists, or NumPy arrays. Each input type is processed differently to maintain the structure of the output.\n2. **Gradient Calculation**: For each element in the input, compute the gradient using the formula \\(z \\cdot (1 - z)\\).\n3. **Rounding**: Use NumPy's rounding function to ensure each result is rounded to four decimal places.\n4. **Recursive Processing**: For nested lists, recursively process each element to maintain the nested structure in the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        grad = z * (1 - z)\n        return np.round(grad, 4)\n    elif isinstance(z, list):\n        return [sigmoid_grad(elem) for elem in z]\n    else:\n        return np.round(z * (1 - z), 4).item()\n```\n\n### Explanation\n1. **NumPy Arrays**: If the input is a NumPy array, the gradient is computed using vectorized operations for efficiency. The result is rounded and returned as a NumPy array.\n2. **Nested Lists**: If the input is a list (including nested lists), each element is processed recursively. This ensures that the output maintains the same nested structure as the input.\n3. **Scalars**: If the input is a scalar (Python float or int), the gradient is computed and converted to a Python float after rounding, ensuring the output type matches the input type.\n\nThis approach efficiently handles all specified input types and ensures the output structure matches the input structure, with each element correctly transformed and rounded."}
{"task_id": 521, "completion_id": 0, "completion": "To solve this problem, we need to compute the gradient (derivative) of the Rectified Linear Unit (ReLU) activation function for various input types, including scalars, nested Python lists, and NumPy arrays. The ReLU gradient is defined as 0 for all inputs less than or equal to 0 and 1 for all inputs greater than 0. The solution must handle each input type appropriately and return the result in the correct format.\n\n### Approach\n1. **Scalar Handling**: Check if the input is a scalar (either a Python `int`/`float` or a 0-dimensional NumPy array). If it is a scalar, return 1 if the value is positive, otherwise return 0.\n2. **NumPy Array Handling**: If the input is a non-scalar NumPy array, convert all elements to 0 or 1 based on their values and then convert the result to a Python list using the `tolist()` method.\n3. **Nested List Handling**: If the input is a nested Python list, recursively process each element to apply the ReLU gradient function, ensuring the output structure matches the input structure.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu_grad(z):\n    \"\"\"Compute the element-wise derivative of the ReLU activation.\n\n    The function should return 0 for every entry that is less than or equal to\n    zero and 1 for every entry that is strictly greater than zero.  Scalars\n    must yield a single integer; arrays/lists must yield a list of identical\n    shape.\n    \"\"\"\n    if isinstance(z, np.ndarray) and z.ndim == 0:\n        return 1 if z.item() > 0 else 0\n    elif isinstance(z, (int, float)):\n        return 1 if z > 0 else 0\n    elif isinstance(z, np.ndarray):\n        return (z > 0).astype(int).tolist()\n    elif isinstance(z, list):\n        return [relu_grad(elem) for elem in z]\n    else:\n        raise TypeError(\"Input must be a scalar, list, or numpy array\")\n```\n\n### Explanation\n1. **Scalar Check**: The function first checks if the input is a scalar. This includes Python integers, floats, and 0-dimensional NumPy arrays. For these, it directly returns 1 if the value is positive and 0 otherwise.\n2. **NumPy Array Processing**: If the input is a non-scalar NumPy array, it uses vectorized operations to check element-wise positivity, converts the boolean results to integers (0s and 1s), and then converts the result to a Python list.\n3. **Nested List Processing**: For nested lists, the function recursively processes each element. Each element is checked individually, whether it is a scalar, another list, or a NumPy array, ensuring the output structure matches the input structure exactly.\n\nThis approach efficiently handles all specified input types and ensures the output format matches the input structure, adhering to the problem constraints."}
{"task_id": 522, "completion_id": 0, "completion": "To solve this problem, we need to generate a tensor (nested list) filled with values drawn from the standard normal distribution (mean = 0, variance = 1). The generated values must be reproducible, rounded to four decimal places, and returned as a pure Python nested list.\n\n### Approach\n1. **Resetting the Random Seed**: To ensure reproducibility, we reset the NumPy random seed to 42 every time the function is called. This ensures that the sequence of generated random numbers is the same across different function calls.\n2. **Generating the Tensor**: Using NumPy's `np.random.normal` function, we generate values from the standard normal distribution with the specified shape provided by `weight_shape`. This function handles both scalar and iterable shapes (like tuples or lists) directly.\n3. **Rounding Values**: Each generated value is rounded to four decimal places using NumPy's `np.around` function.\n4. **Conversion to Python List**: The resulting NumPy array is converted to a pure Python nested list using the `tolist()` method to meet the output requirements.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef standard_normal(weight_shape):\n    \"\"\"Return a tensor of the given shape filled with samples from N(0,1).\n\n    The function *must* reset the NumPy random seed to 42 every time it is\n    called so that the output is reproducible.  It should then draw numbers\n    from the standard normal distribution, round every value to four decimal\n    places and finally convert the NumPy array to a Python list before\n    returning it.\n\n    Args:\n        weight_shape (int | tuple[int, ...] | list[int]): Shape of the tensor\n            to be returned.\n\n    Returns:\n        list: A Python list containing the generated (rounded) values.\n    \"\"\"\n    np.random.seed(42)\n    tensor = np.random.normal(loc=0, scale=1, size=weight_shape)\n    tensor_rounded = np.around(tensor, decimals=4)\n    return tensor_rounded.tolist()\n```\n\n### Explanation\n1. **Resetting the Seed**: By setting `np.random.seed(42)` at the beginning of the function, we ensure that each call to the function starts with the same random seed, leading to reproducible results.\n2. **Generating Values**: The `np.random.normal` function generates values from the standard normal distribution (mean 0, standard deviation 1) with the specified shape. This function is versatile and accepts integers, tuples, or lists for the shape parameter.\n3. **Rounding**: The `np.around` function is used to round each value to four decimal places, ensuring precision in the output.\n4. **Conversion to List**: The `tolist()` method converts the NumPy array into a nested Python list, which is the required output format.\n\nThis approach efficiently combines NumPy's capabilities for random number generation and array manipulation with Python's list structures to meet the problem's requirements."}
{"task_id": 523, "completion_id": 0, "completion": "To solve this problem, we need to initialize a weight tensor for neural networks using the He normal distribution. The He normal distribution is particularly effective for layers followed by ReLU activation functions, as it helps in faster convergence during training. The key challenge is to correctly compute the fan-in based on the shape of the weight tensor and then generate the initial weights accordingly.\n\n### Approach\n1. **Determine Fan-in**: The fan-in (number of input units) varies based on the shape of the weight tensor:\n   - For a 2-D shape (typical of dense layers), the fan-in is the first dimension.\n   - For a 4-D shape (typical of convolutional layers), the fan-in is the product of the first three dimensions (kernel height, kernel width, and input channels).\n   - For any other shape, treat it as a 2-D tensor and use the first dimension as the fan-in.\n\n2. **Compute Standard Deviation**: Using the fan-in, compute the standard deviation (\u03c3) as the square root of (2 divided by the fan-in).\n\n3. **Generate Weights**: Draw samples from a normal distribution with mean 0 and the computed standard deviation, using the given shape.\n\n4. **Round and Convert**: Round each element to four decimal places and convert the resulting NumPy array into a nested Python list structure.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef he_normal(weight_shape: tuple[int, ...]) -> list:\n    \"\"\"Initialise a weight tensor using the He normal distribution.\n\n    Args:\n        weight_shape: Shape of the desired weight tensor. For dense layers this\n            is usually (fan_in, fan_out). For 2-D convolutions it is\n            (kernel_h, kernel_w, in_channels, out_channels).\n\n    Returns:\n        Nested Python lists containing the initialised weights, rounded to four\n        decimal places.\n    \"\"\"\n    if len(weight_shape) == 2:\n        fan_in = weight_shape[0]\n    elif len(weight_shape) == 4:\n        # Convolutional layer: kernel_h, kernel_w, in_channels, out_channels\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    else:\n        # Assume it's a 2-D tensor, take the first dimension as fan_in\n        fan_in = weight_shape[0]\n    \n    # Calculate the standard deviation\n    std_dev = np.sqrt(2 / fan_in)\n    \n    # Generate the weights from the He normal distribution\n    weights = np.random.normal(loc=0, scale=std_dev, size=weight_shape)\n    \n    # Round to four decimal places and convert to nested lists\n    return np.round(weights, 4).tolist()\n```\n\n### Explanation\n- **Fan-in Calculation**: The function first checks the length of the input shape to determine the appropriate fan-in. For 2-D shapes, it uses the first dimension. For 4-D convolutional shapes, it multiplies the first three dimensions. For other shapes, it defaults to the first dimension of the input shape.\n- **Standard Deviation**: Using the computed fan-in, the standard deviation is derived using the formula \u221a(2 / fan_in).\n- **Weight Generation**: Samples are drawn from a normal distribution with the computed standard deviation and the specified shape.\n- **Conversion**: The generated weights are rounded to four decimal places and converted to a nested list structure to meet the output requirements.\n\nThis approach ensures that the weights are initialized correctly according to the He normal distribution, facilitating efficient neural network training."}
{"task_id": 525, "completion_id": 0, "completion": "To solve this problem, we need to implement Ridge Regression (L2-regularized linear regression) using a closed-form solution without relying on any third-party machine learning libraries. The solution involves mean-centering the data, computing optimal weights using matrix operations, and recovering the bias term.\n\n### Approach\n1. **Mean-Centering the Data**: \n   - Subtract the mean of each feature (column) from the original feature values.\n   - Subtract the mean of the target values from each target value.\n\n2. **Compute Optimal Weights**:\n   - Use the closed-form solution for Ridge Regression, which involves regularizing the matrix inversion to handle potential singularities. The formula used is \\( w = (X^T X + \\lambda I)^{-1} X^T y \\), where \\( \\lambda \\) is the regularization strength and \\( I \\) is the identity matrix.\n\n3. **Recover the Bias Term**:\n   - The bias term is calculated using the original means of the features and the target values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_regression(\n        X: np.ndarray,\n        y: np.ndarray,\n        lambda_: float = 0.0) -> tuple[list[float], float]:\n    \"\"\"Fit a Ridge (L2-regularised) linear regression model.\"\"\"\n\n    # Ensure y is a 1D array\n    y = y.ravel()\n\n    m, n = X.shape\n\n    # Compute the mean of each feature and the target\n    x_means = np.mean(X, axis=0)\n    y_mean = np.mean(y)\n\n    # Center the features and target\n    X_centered = X - x_means\n    y_centered = y - y_mean\n\n    # Compute the regularized matrix\n    Xt_X = X_centered.T @ X_centered\n    regularization = lambda_ * np.eye(n)\n    M = Xt_X + regularization\n\n    # Compute the Moore-Penrose pseudo-inverse\n    M_pinv = np.linalg.pinv(M)\n\n    # Calculate the optimal weights\n    w = M_pinv @ (X_centered.T @ y_centered)\n\n    # Compute the bias term\n    bias = y_mean - np.dot(x_means, w)\n\n    # Round to 4 decimal places\n    weights = np.round(w, 4).tolist()\n    bias = np.round(bias, 4).item()\n\n    return (weights, bias)\n```\n\n### Explanation\n1. **Data Centering**: By subtracting the mean of each feature and the target from their respective values, we ensure that the model is trained on centered data, which helps in stabilizing the numerical computations and makes the intercept term easier to recover later.\n\n2. **Regularized Matrix Inversion**: The matrix \\( X^T X \\) is regularized by adding \\( \\lambda I \\) to avoid singularity issues. The Moore-Penrose pseudo-inverse is used to handle cases where the matrix might still be singular, ensuring a solution always exists.\n\n3. **Bias Recovery**: The bias term is computed using the original means of the features and the target, adjusting the model to account for the initial centering of the data.\n\nThis approach efficiently computes the model parameters using matrix operations, ensuring both numerical stability and adherence to the theoretical formulation of Ridge Regression."}
{"task_id": 527, "completion_id": 0, "completion": "To solve this problem, we need to determine if a given file should be considered for extraction based on its file extension and the user-specified archive formats. The solution involves checking the file's extension against a list of allowed formats in a case-insensitive manner and ensuring the function handles various input scenarios without raising exceptions.\n\n### Approach\n1. **Input Validation**: Check if the provided `file_path` is a string and handle cases where `archive_format` is `None` or an empty list immediately by returning `False`.\n2. **Process `archive_format` Parameter**: Convert `archive_format` into a list of allowed formats, normalizing them to lowercase and filtering out any non-supported formats.\n3. **Collect Extensions**: Gather all valid file extensions for the allowed formats (tar and zip).\n4. **Check File Extensions**: Normalize the file name to lowercase and check if it ends with any of the valid extensions for the allowed formats.\n\n### Solution Code\n```python\nimport os\nfrom typing import List, Union\n\ndef extract_archive(file_path: str, path: str = '.', archive_format: Union[str, List[str], None] = 'auto') -> bool:\n    \"\"\"Checks whether *file_path* belongs to an allowed archive format.\"\"\"\n    \n    # Check if file_path is a valid string\n    if not isinstance(file_path, str):\n        return False\n    \n    # Handle None or empty list cases for archive_format\n    if archive_format is None:\n        return False\n    if isinstance(archive_format, list) and len(archive_format) == 0:\n        return False\n    \n    # Process archive_format to determine allowed formats\n    allowed_formats = []\n    if archive_format == 'auto':\n        allowed_formats = ['tar', 'zip']\n    elif isinstance(archive_format, str):\n        allowed_formats = [archive_format.lower()]\n    elif isinstance(archive_format, list):\n        allowed_formats = [fmt.lower() for fmt in archive_format if isinstance(fmt, str)]\n    else:\n        return False  # Invalid type for archive_format\n    \n    # Filter allowed formats to only 'tar' and 'zip'\n    allowed_formats = [fmt for fmt in allowed_formats if fmt in {'tar', 'zip'}]\n    if not allowed_formats:\n        return False\n    \n    # Define the valid extensions for each format\n    format_extensions = {\n        'tar': ['.tar', '.tar.gz', '.tgz', '.tar.bz', '.tar.bz2', '.tbz'],\n        'zip': ['.zip']\n    }\n    \n    # Collect all possible extensions from allowed formats\n    all_extensions = []\n    for fmt in allowed_formats:\n        all_extensions.extend(format_extensions.get(fmt, []))\n    \n    # Normalize the file name to lowercase\n    file_name = os.path.basename(file_path).lower()\n    \n    # Check if any extension matches the end of the file name\n    for ext in all_extensions:\n        if file_name.endswith(ext):\n            return True\n    \n    return False\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if `file_path` is a string and if `archive_format` is `None` or an empty list, returning `False` immediately if any of these conditions are met.\n2. **Processing `archive_format`**: The function converts the `archive_format` parameter into a list of allowed formats, ensuring they are lowercase and filtering out any formats that are not 'tar' or 'zip'.\n3. **Extension Collection**: The valid file extensions for each allowed format (tar and zip) are collected into a list.\n4. **File Name Check**: The file name is normalized to lowercase, and each valid extension is checked against the file name to see if it ends with any of the valid extensions. If a match is found, the function returns `True`; otherwise, it returns `False`.\n\nThis approach ensures that the function efficiently checks the file extensions against the allowed formats while handling various edge cases and input scenarios gracefully."}
{"task_id": 528, "completion_id": 0, "completion": "To solve this problem, we need to generate a grid of predictions for a 2D linear classifier. The classifier is defined by weights and a bias term, and we need to label each grid point based on the classifier's decision rule. The grid should span the bounding box of the given data and have a specified resolution.\n\n### Approach\n1. **Data Conversion**: Convert the input data into a NumPy array to facilitate numerical operations.\n2. **Determine Bounds**: Compute the minimum and maximum values for each feature (x\u2081 and x\u2082) from the input data to define the grid's range.\n3. **Generate Grid Points**: Create two 1D arrays (`x1_plot` and `x2_plot`) that are linearly spaced between the computed minimum and maximum values for each feature.\n4. **Create Meshgrid**: Use these 1D arrays to generate a meshgrid, which provides all possible combinations of (x\u2081, x\u2082) points in the grid.\n5. **Classify Grid Points**: For each point in the meshgrid, compute the linear combination of the weights and bias, then apply the sign function to determine the label (1 or -1).\n6. **Convert to List**: Convert the resulting NumPy array of labels into a nested Python list of lists as required.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    X_np = np.array(X)\n    x1_min, x1_max = X_np[:, 0].min(), X_np[:, 0].max()\n    x2_min, x2_max = X_np[:, 1].min(), X_np[:, 1].max()\n    \n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    \n    X1, X2 = np.meshgrid(x1_plot, x2_plot, indexing='ij')\n    Z = W[0] * X1 + W[1] * X2 + b\n    labels = np.where(Z >= 0, 1, -1)\n    \n    return labels.tolist()\n```\n\n### Explanation\n1. **Data Conversion**: The input data `X` is converted to a NumPy array to leverage NumPy's efficient numerical operations.\n2. **Determine Bounds**: The minimum and maximum values for each feature (x\u2081 and x\u2082) are computed to ensure the grid spans the entire range of the input data.\n3. **Generate Grid Points**: Using `np.linspace`, we create two 1D arrays (`x1_plot` and `x2_plot`) that cover the range from the minimum to maximum values of each feature with the specified resolution (`grid_n`).\n4. **Create Meshgrid**: The `np.meshgrid` function with `indexing='ij'` generates two 2D arrays (X1 and X2) representing all possible (x\u2081, x\u2082) pairs in the grid.\n5. **Classify Grid Points**: The linear combination of weights, features, and bias is computed for each grid point. The `np.where` function is used to apply the sign function, resulting in a 2D array of labels (1 or -1).\n6. **Convert to List**: The resulting NumPy array of labels is converted to a nested Python list using the `tolist()` method to meet the output requirements.\n\nThis approach efficiently generates the decision boundary grid using vectorized operations, ensuring both correctness and performance."}
{"task_id": 529, "completion_id": 0, "completion": "To solve this problem, we need to generate an ASCII progress bar that visualizes the progress of a long-running loop. The progress bar should follow specific formatting rules based on whether the total number of steps (target) is known or unknown. The solution must handle both cases and produce the exact format described.\n\n### Approach\n1. **Case when Target is Unknown**:\n   - If the target is `None`, the progress bar is replaced with a right-aligned 7-character representation of the current step followed by `/Unknown`.\n\n2. **Case when Target is Known**:\n   - Calculate the number of digits in the target to format the current step and target correctly.\n   - Compute the filled portion of the progress bar based on the ratio of the current step to the target, ensuring it does not exceed the specified width.\n   - Construct the progress bar string with appropriate characters (`=`, `>`, `.`) to represent the progress and remaining steps.\n\n### Solution Code\n```python\nimport math\n\ndef generate_progress_bar(current: int, target: int | None, width: int = 30) -> str:\n    \"\"\"Return an ASCII progress bar string similar to Keras' *Progbar*.\"\"\"\n    if target is None:\n        return f\"{current:7d}/Unknown\"\n    else:\n        d = len(str(target))\n        start_part = f\"{current:>{d}}/{target} [\"\n        if target == 0:\n            prog_width = 0\n        else:\n            prog_width = int(width * current / target)\n            prog_width = min(prog_width, width)  # Ensure we don't exceed the bar width\n\n        filled_part = ''\n        if prog_width > 0:\n            filled_part += '=' * (prog_width - 1)\n            if current < target:\n                filled_part += '>'\n            else:\n                filled_part += '='\n        dots_part = '.' * (width - prog_width)\n        bar_part = filled_part + dots_part\n        return f\"{start_part}{bar_part}]\"\n```\n\n### Explanation\n1. **Handling Unknown Target**:\n   - If the target is `None`, the function returns a string with the current step right-aligned in a 7-character field followed by `/Unknown`.\n\n2. **Handling Known Target**:\n   - **Digit Calculation**: The number of digits in the target is determined to format the current step and target correctly.\n   - **Progress Calculation**: The filled width (`prog_width`) is calculated as the integer ratio of the current step to the target, scaled by the bar width. This value is capped to ensure it does not exceed the bar width.\n   - **Bar Construction**: The filled portion of the bar is constructed using `=` characters, with an arrow `>` if the process is ongoing, or an additional `=` if completed. Dots `.` fill the remaining space to maintain the bar's total width.\n\nThis approach ensures the progress bar is constructed accurately according to the specified rules, providing a clear visual representation of progress for both known and unknown targets."}
{"task_id": 530, "completion_id": 0, "completion": "To solve this problem, we need to compute the mean Binary Cross-Entropy (BCE) loss for a batch of predictions. The BCE loss is a common metric used in training binary classifiers and GAN discriminators. The key steps involve clipping predicted probabilities to ensure numerical stability and applying the BCE formula to each sample before averaging the results.\n\n### Approach\n1. **Clip Predictions**: To avoid numerical instability caused by logarithms of 0 or 1, we clip the predicted probabilities to the range [1e-7, 1 - 1e-7].\n2. **Compute Loss Per Sample**: For each sample, the BCE loss is calculated using the formula:\n   \\[\n   \\ell = -\\left[ y \\cdot \\log(p) + (1 - y) \\cdot \\log(1 - p) \\right]\n   \\]\n   where \\( y \\) is the true label and \\( p \\) is the clipped predicted probability.\n3. **Average Loss**: Compute the mean of the individual losses across all samples.\n4. **Round Result**: The final result is rounded to four decimal places for consistency.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculates the mean Binary Cross-Entropy loss.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        Ground-truth binary labels (0 or 1).\n    y_pred : np.ndarray\n        Predicted probabilities for the positive class. Must have the same shape\n        as *y_true*.\n\n    Returns\n    -------\n    float\n        Mean BCE loss rounded to 4 decimal places.\n    \"\"\"\n    clipped_preds = np.clip(y_pred, 1e-7, 1 - 1e-7)\n    term1 = y_true * np.log(clipped_preds)\n    term2 = (1 - y_true) * np.log(1 - clipped_preds)\n    mean_loss = -np.mean(term1 + term2)\n    return round(mean_loss, 4)\n```\n\n### Explanation\n1. **Clipping Predictions**: The `np.clip` function ensures that all predicted values are within the range [1e-7, 1 - 1e-7], preventing log(0) errors.\n2. **Loss Calculation**: The BCE loss for each sample is computed using element-wise operations. This leverages NumPy's efficient array operations to handle the entire batch of predictions simultaneously.\n3. **Mean Loss**: The average of all individual losses is computed using `np.mean`, which efficiently handles the array of losses.\n4. **Rounding**: The result is rounded to four decimal places using Python's built-in `round` function to meet the specified precision requirement.\n\nThis approach ensures numerical stability and efficiently computes the mean BCE loss using vectorized operations, making it both robust and performant for large datasets."}
{"task_id": 531, "completion_id": 0, "completion": "To solve this problem, we need to generate a generalized cosine window based on given coefficients and a symmetric flag. This window is used in signal processing for shaping finite-length signals before applying transformations like DFT or designing digital filters. The key challenge is to correctly compute the phase variable (theta) based on whether the window is symmetric or periodic, and then use this to compute the window values using cosine terms weighted by the given coefficients.\n\n### Approach\n1. **Determine Theta Values**: The phase variable theta is computed differently based on the symmetric flag:\n   - **Symmetric Window**: Theta values are sampled as exactly `L` equally spaced points from -\u03c0 to \u03c0.\n   - **Periodic Window**: Theta values are sampled as `L+1` equally spaced points from -\u03c0 to \u03c0, and the last value is discarded to leave `L` points.\n\n2. **Compute Window Values**: Using the computed theta values, sum up the cosine terms weighted by the given coefficients. Each term in the sum is of the form `a_k * cos(k * theta)` for each coefficient `a_k`.\n\n3. **Rounding**: Round the computed window values to four decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef generalized_cosine(window_len: int, coefs: list[float], symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a generalized cosine window.\n\n    The function creates either a periodic or a symmetric generalized\n    cosine window according to the *symmetric* flag and returns it rounded to\n    four decimal places.\n\n    Args:\n        window_len (int): Desired length L of the final window (must be strictly positive).\n        coefs (list[float]): List of coefficients a_k.\n        symmetric (bool, optional): If True produce a symmetric window of length L;\n            otherwise produce a periodic window suitable for the FFT. Defaults to False.\n\n    Returns:\n        list[float]: Window values rounded to four decimals.\n    \"\"\"\n    # Compute theta based on the symmetric flag\n    if symmetric:\n        theta = np.linspace(-np.pi, np.pi, window_len)\n    else:\n        theta = np.linspace(-np.pi, np.pi, window_len + 1)[:-1]\n    \n    # Initialize the window with zeros\n    window = np.zeros(window_len)\n    \n    # Sum the contributions from each coefficient\n    for k, a_k in enumerate(coefs):\n        window += a_k * np.cos(k * theta)\n    \n    # Round to four decimal places and convert to a list\n    return [round(val, 4) for val in window.tolist()]\n```\n\n### Explanation\n- **Theta Calculation**: The theta values are generated using `np.linspace` which creates evenly spaced values over a specified interval. For symmetric windows, we generate exactly `L` points from -\u03c0 to \u03c0. For periodic windows, we generate `L+1` points and discard the last one to avoid redundancy in the periodic boundary conditions of the FFT.\n- **Window Computation**: The window values are computed by iterating over each coefficient and summing the contributions of each cosine term. This is efficiently done using vectorized operations in NumPy.\n- **Rounding**: The computed values are rounded to four decimal places using Python's `round` function and converted to a list for the final output.\n\nThis approach ensures that the window is correctly generated according to the specified parameters and efficiently handles the computation using NumPy's array operations."}
{"task_id": 532, "completion_id": 0, "completion": "To solve this problem, we need to calculate the total number of trainable parameters in a fully-connected neural network based on the given layer sizes. The trainable parameters include the weights and biases between each pair of consecutive layers.\n\n### Approach\n1. **Check for Valid Layers**: If the input list contains fewer than two elements, return 0 since no trainable parameters can exist.\n2. **Initialize Total Parameters**: Start with a total count of 0.\n3. **Iterate Through Layer Pairs**: For each consecutive pair of layers, compute the number of weights and biases. The number of weights is the product of the number of neurons in the current layer and the next layer. The number of biases is equal to the number of neurons in the next layer.\n4. **Sum Parameters**: Accumulate the total number of parameters by adding the weights and biases for each pair of consecutive layers.\n5. **Return the Total**: After processing all layer pairs, return the accumulated total.\n\n### Solution Code\n```python\ndef count_fc_parameters(layers: list[int]) -> int:\n    \"\"\"Calculate total trainable parameters (weights + biases) in a fully-connected network.\n\n    Args:\n        layers: List of integers where each element represents the number of\n            neurons in the respective layer (input layer first).\n\n    Returns:\n        An integer \u2013 total count of trainable parameters. Returns 0 when fewer\n        than two layers are supplied.\n    \"\"\"\n\n    if len(layers) < 2:\n        return 0\n    total = 0\n    for i in range(len(layers) - 1):\n        n_in = layers[i]\n        n_out = layers[i + 1]\n        total += n_in * n_out + n_out\n    return total\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the input list has fewer than two layers, in which case it immediately returns 0.\n- **Loop Through Layers**: By iterating through each consecutive pair of layers (from the first to the second-to-last layer), the function calculates the number of weights and biases for each pair.\n- **Calculate Parameters**: For each pair of layers, the weights are determined by multiplying the number of neurons in the current layer by the next layer's neurons, and the biases are simply the number of neurons in the next layer.\n- **Accumulate Total**: The total number of parameters is accumulated by summing the weights and biases for each layer pair, which is then returned as the result.\n\nThis approach efficiently computes the total number of trainable parameters in linear time relative to the number of layers, making it both optimal and straightforward."}
{"task_id": 533, "completion_id": 0, "completion": "To solve this problem, we need to compute the probability density function (PDF) of a normal (Gaussian) distribution at one or several points. The function should handle both scalar and array-like inputs and return the results rounded to four decimal places. \n\n### Approach\n1. **Understanding the PDF Formula**: The PDF of a normal distribution is given by the formula \\( f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\). This formula is applied to each input point to compute the density.\n2. **Handling Input Types**: The function should accept both scalar values (int or float) and array-like inputs (list or NumPy array). We convert the input to a NumPy array to leverage vectorized operations for efficient computation.\n3. **Vectorized Computation**: Using NumPy allows us to compute the PDF values for all elements in the input array simultaneously, which is efficient and concise.\n4. **Rounding and Output Format**: After computing the PDF values, we round them to four decimal places. The result is returned as a float if the input is a scalar, or as a Python list if the input is array-like.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_likelihood(x, mean, sigma):\n    \"\"\"Compute the probability density of a normal distribution.\n\n    Parameters\n    ----------\n    x : float | list[float] | np.ndarray\n        Point(s) where the density will be evaluated.\n    mean : float\n        The mean (\u00b5) of the distribution.\n    sigma : float\n        The standard deviation (\u03c3) of the distribution.\n\n    Returns\n    -------\n    float | list[float]\n        Density value(s) rounded to 4 decimal places. A single float is\n        returned for a scalar x, otherwise a list is returned.\n    \"\"\"\n    arr = np.asarray(x)\n    coefficient = 1 / (sigma * np.sqrt(2 * np.pi))\n    exponent = -(arr - mean) ** 2 / (2 * sigma ** 2)\n    pdf_values = coefficient * np.exp(exponent)\n    rounded = np.round(pdf_values, 4)\n    if arr.ndim == 0:\n        return rounded.item()\n    else:\n        return rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `x` is converted to a NumPy array to handle both scalar and array-like inputs uniformly.\n2. **Coefficient Calculation**: The coefficient part of the PDF formula \\( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\) is computed once and reused for all elements.\n3. **Exponent Calculation**: The exponent part \\( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\) is computed using vectorized operations, which is efficient for array inputs.\n4. **Exponential and Multiplication**: The computed exponent is applied to the exponential function, and the result is multiplied by the coefficient to get the PDF values.\n5. **Rounding**: The PDF values are rounded to four decimal places using NumPy's `round` function.\n6. **Return Type Handling**: If the input was a scalar (0-dimensional array), the result is returned as a float. For array-like inputs, the result is converted to a Python list using `tolist()`.\n\nThis approach ensures that the function efficiently handles both scalar and array-like inputs while providing the required precision and correct return types."}
{"task_id": 534, "completion_id": 0, "completion": "To solve this problem, we need to compute the element-wise sigmoid activation for a given input, which can be a scalar, a 1-D list/array, or a 2-D list/array. The solution must handle these inputs correctly, preserve their shape, round the results to 4 decimal places, and return the appropriate type (float for scalar inputs and a list for non-scalar inputs).\n\n### Approach\n1. **Identify Scalar Inputs**: Check if the input is a Python scalar (int or float). If it is, compute the sigmoid value, round it to 4 decimal places, and return it as a float.\n2. **Handle Non-scalar Inputs**: For non-scalar inputs (lists or NumPy arrays), convert them to a NumPy array if they are lists. Compute the sigmoid activation element-wise using vectorized operations for efficiency.\n3. **Round Results**: Use NumPy's `around` function to round the results to 4 decimal places, ensuring consistent rounding behavior.\n4. **Convert Back to List**: Convert the resulting NumPy array back to a Python list using the `tolist()` method to preserve the original structure and shape.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_(Z):\n    \"\"\"Compute the logistic sigmoid activation element-wise.\n\n    Args:\n        Z: A scalar, 1-D list/array, or 2-D list/array of real numbers.\n\n    Returns:\n        Sigmoid-transformed value(s) rounded to 4 decimals.  Returns a float for\n        scalar input; otherwise returns a Python list preserving the input\n        shape.\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        s = 1 / (1 + np.exp(-Z))\n        rounded = np.around(s, 4)\n        return float(rounded)\n    else:\n        arr = np.array(Z)\n        s = 1 / (1 + np.exp(-arr, dtype=np.float64))  # Ensure floating point calculation\n        rounded = np.around(s, 4)\n        return rounded.tolist()\n```\n\n### Explanation\n- **Scalar Handling**: If the input is a Python scalar (int or float), the sigmoid function is applied directly, and the result is rounded to 4 decimal places using NumPy's `around` function. The result is returned as a float.\n- **Non-scalar Handling**: For lists or NumPy arrays, the input is converted to a NumPy array (if not already one) to leverage vectorized operations. The sigmoid function is applied element-wise, and the results are rounded. The result is then converted back to a Python list using `tolist()`, preserving the original structure and dimensions.\n- **Efficiency**: Using NumPy's vectorized operations ensures efficient computation even for large arrays, and the `around` function ensures consistent rounding behavior across all elements.\n\nThis approach ensures that the function works correctly for all specified input types, maintains the input shape, and returns the appropriate data types as required."}
{"task_id": 536, "completion_id": 0, "completion": "To solve this problem, we need to implement a linear regression function from scratch using two different training strategies: the closed-form Normal Equation and batch Gradient Descent (BGD). The function should be able to predict target values for a given test dataset after training on the provided training data.\n\n### Approach\n1. **Data Preparation**: Prepend a column of ones to both the training and test feature matrices to account for the bias term in the linear regression model.\n2. **Parameter Learning**:\n   - **Normal Equation**: Compute the parameters using the closed-form solution \\(\\theta = (X^T X)^{-1} X^T y\\). This method directly solves for the optimal parameters using matrix operations.\n   - **Gradient Descent**: Iteratively update the parameters using the gradient of the cost function. The parameters are updated in the direction that minimizes the mean squared error (MSE) using the formula \\(\\theta \\leftarrow \\theta - \\alpha/m \\cdot X^T (X\\theta - y)\\).\n3. **Prediction**: Use the learned parameters to predict target values for the test dataset, then round these predictions to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression(X_train, y_train, X_test, method=\"normal\", alpha=0.01, num_iter=1000):\n    # Prepend a column of ones to X_train and X_test for the bias term\n    X_train_aug = np.hstack([np.ones((X_train.shape[0], 1), X_train])\n    X_test_aug = np.hstack([np.ones((X_test.shape[0], 1), X_test])\n    \n    # Compute theta using the specified method\n    if method == \"normal\":\n        XtX = X_train_aug.T @ X_train_aug\n        XtX_inv = np.linalg.inv(XtX)\n        theta = XtX_inv @ X_train_aug.T @ y_train\n    else:\n        m = X_train.shape[0]\n        theta = np.zeros(X_train_aug.shape[1])\n        alpha_over_m = alpha / m\n        for _ in range(num_iter):\n            error = X_train_aug.dot(theta) - y_train\n            gradient = X_train_aug.T.dot(error)\n            theta -= alpha_over_m * gradient\n    \n    # Predict on X_test and round the results\n    predictions = X_test_aug.dot(theta)\n    return np.round(predictions, 4).tolist()\n```\n\n### Explanation\n1. **Data Preparation**: By prepending a column of ones to the feature matrices, we allow the model to learn an intercept term (bias) which is crucial for fitting the data accurately.\n2. **Normal Equation**: This method leverages matrix inversion to directly compute the optimal parameters. It is efficient and guarantees the exact solution if the matrix \\(X^T X\\) is invertible.\n3. **Gradient Descent**: This iterative method updates the parameters by moving in the direction opposite to the gradient of the cost function. The learning rate (\\(\\alpha\\)) controls the step size, and the number of iterations determines how many times the parameters are updated. This method is useful when dealing with large datasets where matrix inversion might be computationally expensive.\n4. **Prediction**: After learning the parameters, we use them to make predictions on the test data. The predictions are then rounded to four decimal places to meet the specified output constraints.\n\nThis approach ensures that we can handle both small and moderately sized datasets efficiently, using either a direct mathematical solution or an iterative optimization method depending on the user's preference."}
{"task_id": 537, "completion_id": 0, "completion": "To solve this problem, we need to implement a Gaussian Naive Bayes classifier from scratch. The classifier will learn the parameters from the training data and use these parameters to predict the class labels for the test data. The solution involves two main phases: training and prediction.\n\n### Approach\n\n1. **Training Phase**:\n   - **Parameter Estimation**: Compute the mean and standard deviation of each feature for both classes (0 and 1) using the training data. To avoid division by zero, a small constant (epsilon) is added to the standard deviations.\n   - **Class Priors**: Calculate the prior probabilities for each class based on their relative frequencies in the training data.\n\n2. **Prediction Phase**:\n   - **Posterior Probability Calculation**: For each test sample, compute the posterior probability for both classes using the Gaussian likelihood and the class priors. The likelihood is calculated under the assumption that features are independent.\n   - **Decision Making**: Predict the class with the higher posterior probability for each test sample. To avoid numerical underflow, log probabilities are used instead of multiplying probabilities directly.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\"\"\"\n    # Separate training data by class\n    mask0 = y_train == 0\n    X0 = X_train[mask0]\n    X1 = X_train[~mask0]\n    \n    # Calculate mean and standard deviation for each feature in each class\n    mean0 = np.mean(X0, axis=0)\n    std0 = np.std(X0, axis=0, ddof=1)  # Using sample standard deviation\n    mean1 = np.mean(X1, axis=0)\n    std1 = np.std(X1, axis=0, ddof=1)\n    \n    # Add epsilon to prevent division by zero\n    epsilon = 1e-9\n    std0 += epsilon\n    std1 += epsilon\n    \n    # Compute class priors\n    prior0 = X0.shape[0] / X_train.shape[0]\n    prior1 = X1.shape[0] / X_train.shape[0]\n    \n    # Predict each test sample\n    predictions = []\n    for x in X_test:\n        # Compute log likelihoods for class 0\n        log_likelihood0 = (-0.5 * np.log(2 * np.pi)) - np.log(std0) - ((x - mean0) ** 2) / (2 * std0 ** 2)\n        log_prob0 = np.log(prior0) + np.sum(log_likelihood0)\n        \n        # Compute log likelihoods for class 1\n        log_likelihood1 = (-0.5 * np.log(2 * np.pi)) - np.log(std1) - ((x - mean1) ** 2) / (2 * std1 ** 2)\n        log_prob1 = np.log(prior1) + np.sum(log_likelihood1)\n        \n        # Determine the predicted class\n        predictions.append(0 if log_prob0 > log_prob1 else 1)\n    \n    return predictions\n```\n\n### Explanation\n\n1. **Training Phase**:\n   - **Separation by Class**: The training data is split into two groups based on their class labels (0 or 1).\n   - **Mean and Standard Deviation Calculation**: For each feature in each class group, compute the mean and standard deviation. A small constant (epsilon) is added to each standard deviation to avoid division by zero during prediction.\n   - **Prior Calculation**: The prior probabilities for each class are computed as the ratio of the number of samples in each class to the total number of training samples.\n\n2. **Prediction Phase**:\n   - **Log Probability Calculation**: For each test sample, the log likelihood for each feature is computed using the Gaussian formula. The log likelihoods are summed up and combined with the log prior to get the log posterior probability for each class.\n   - **Class Decision**: The class with the higher log posterior probability is chosen as the predicted class for the test sample. This approach ensures numerical stability by using log probabilities to prevent underflow.\n\nThis method efficiently computes the necessary parameters and uses them to make predictions, adhering to the principles of the Gaussian Naive Bayes algorithm."}
{"task_id": 539, "completion_id": 0, "completion": "To solve this problem, we need to compute the binary cross-entropy cost, which is a common loss function used in binary classification tasks. This cost function measures the performance of a classification model whose output is a probability between 0 and 1. The goal is to minimize this cost during training.\n\n### Approach\nThe binary cross-entropy cost is defined by the formula:\n\\[ J = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\cdot \\ln(a_i) + (1 - y_i) \\cdot \\ln(1 - a_i)] \\]\nwhere \\( m \\) is the number of training examples, \\( a_i \\) is the predicted probability for the \\( i \\)-th example, and \\( y_i \\) is the actual label (0 or 1) for the \\( i \\)-th example.\n\nTo avoid numerical instability (such as taking the logarithm of zero), we clip the predicted probabilities \\( a_i \\) to the range \\([10^{-15}, 1 - 10^{-15}]\\).\n\nThe steps to implement this are as follows:\n1. **Clip the Probabilities**: Use NumPy's `clip` function to ensure all predicted probabilities are within a safe range.\n2. **Compute Log Values**: Calculate the log of the clipped probabilities and the log of one minus the clipped probabilities.\n3. **Calculate Cross-Entropy Terms**: For each example, compute the terms \\( y_i \\cdot \\ln(a_i) \\) and \\( (1 - y_i) \\cdot \\ln(1 - a_i) \\).\n4. **Sum and Average**: Sum all the cross-entropy terms and average them by the number of examples \\( m \\).\n5. **Return Result**: Convert the result to a Python float to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\"\"\"\n    m = Y.size\n    AL_clipped = np.clip(AL, 1e-15, 1 - 1e-15)\n    cross_entropy = - (Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped))\n    cost = np.sum(cross_entropy) / m\n    return float(cost)\n```\n\n### Explanation\n1. **Clipping Probabilities**: The `np.clip` function ensures that all elements in `AL` are between \\(10^{-15}\\) and \\(1 - 10^{-15}\\), preventing logarithms of zero or one which would cause numerical issues.\n2. **Logarithm Calculation**: Using `np.log`, we compute the logarithms of the clipped probabilities and their complements efficiently.\n3. **Element-wise Operations**: By leveraging NumPy's element-wise operations, we compute the cross-entropy terms for all examples simultaneously, which is both efficient and concise.\n4. **Summation and Averaging**: The sum of all cross-entropy terms is divided by the number of examples \\( m \\) to get the average cost.\n5. **Conversion to Float**: The result is converted to a Python float to ensure the output type matches the requirement.\n\nThis approach efficiently handles the computation while ensuring numerical stability and correctness for various input shapes."}
{"task_id": 540, "completion_id": 0, "completion": "To solve this problem, we need to compute the element-wise derivative of the sigmoid function for a given NumPy array and return the results as a Python list with the same shape, rounded to four decimal places. The sigmoid function's derivative can be efficiently computed using the function's output, which simplifies the calculation.\n\n### Approach\n1. **Compute the Sigmoid Function**: For each element in the input array, compute the sigmoid function using the formula \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\).\n2. **Compute the Derivative**: Using the result from the sigmoid function, compute the derivative using the formula \\( \\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z)) \\).\n3. **Round the Results**: Round each element of the derivative result to four decimal places.\n4. **Convert to List**: Convert the resulting NumPy array to a Python list while maintaining the original shape using NumPy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef derivative_sigmoid(Z: np.ndarray) -> list:\n    \"\"\"Return the element-wise derivative of the sigmoid function.\n\n    Args:\n        Z: NumPy array (any shape) containing numeric values.\n\n    Returns:\n        A Python list with the same shape as *Z*, containing the\n        derivative of the sigmoid evaluated at each element, rounded to\n        4 decimal places.\n    \"\"\"\n    sigmoid = 1 / (1 + np.exp(-Z))\n    derivative = sigmoid * (1 - sigmoid)\n    rounded = np.round(derivative, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Sigmoid Calculation**: The sigmoid function is computed using NumPy's `exp` function to handle element-wise operations efficiently. This step converts each element \\( z \\) of the input array to its corresponding sigmoid value.\n2. **Derivative Calculation**: The derivative of the sigmoid function is derived using the property that \\( \\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z)) \\). This step leverages the precomputed sigmoid values for efficiency.\n3. **Rounding**: The results are rounded to four decimal places using NumPy's `round` function to ensure precision constraints are met.\n4. **Conversion to List**: The final result is converted to a Python list using the `tolist()` method, ensuring the output matches the input shape and type requirements.\n\nThis approach efficiently computes the derivative using vectorized operations, ensuring both performance and correctness for any input shape."}
{"task_id": 541, "completion_id": 0, "completion": "To solve this problem, we need to compute the gradient of the Binary Cross-Entropy (BCE) loss with respect to the logits (inputs to the discriminator in a GAN) for arbitrary-shaped inputs. The formula for this gradient is straightforward, but we need to ensure that the function handles scalars, 1-D arrays, and 2-D arrays correctly and returns the results as a Python list with elements rounded to four decimal places.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert the input labels `y` and predicted probabilities `y_hat` into NumPy arrays to leverage NumPy's element-wise operations.\n2. **Compute Gradient**: Use the formula for the gradient, which is the element-wise subtraction of the true labels from the predicted probabilities.\n3. **Round Values**: Round each element of the resulting gradient array to four decimal places.\n4. **Handle Scalar Inputs**: Check if the result is a scalar (0-dimensional array) and convert it to a single-element list if necessary.\n5. **Return as Python List**: Convert the NumPy array back to a Python list using the `.tolist()` method, ensuring the result maintains the original shape.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef return_input_grads(y, y_hat):\n    \"\"\"Compute the gradient of Binary Cross-Entropy loss with respect to the\n    network inputs (logits).\n\n    Parameters\n    ----------\n    y : array-like\n        Ground-truth binary labels (0 or 1).  Can be a scalar, 1-D list/array\n        or 2-D nested list/array.\n    y_hat : array-like\n        Predicted probabilities (after sigmoid, values in the range 0-1) with\n        the same shape as *y*.\n\n    Returns\n    -------\n    list\n        Gradient of the BCE loss w.r.t. each logit, rounded to 4 decimal\n        places, returned as a regular Python list (with the original shape).\n    \"\"\"\n    y_array = np.array(y)\n    y_hat_array = np.array(y_hat)\n    gradient = y_hat_array - y_array\n    rounded = np.round(gradient, 4)\n    if rounded.ndim == 0:\n        return [rounded.tolist()]\n    return rounded.tolist()\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: The inputs `y` and `y_hat` are converted to NumPy arrays to handle both Python lists and NumPy arrays uniformly.\n- **Element-wise Gradient Calculation**: The gradient is computed using the formula `y_hat - y`, which is straightforward and efficient with NumPy's element-wise operations.\n- **Rounding**: The `np.round` function is used to round each element of the gradient array to four decimal places.\n- **Handling Scalar Inputs**: If the result is a scalar (0-dimensional array), it is wrapped in a list to ensure the return type is always a list, even for scalar inputs.\n- **Conversion to Python List**: The `.tolist()` method converts the NumPy array back to a Python list, preserving the original structure (scalar, 1-D, or 2-D).\n\nThis approach ensures that the function works efficiently for all input shapes and returns the results in the required format."}
{"task_id": 542, "completion_id": 0, "completion": "To solve this problem, we need to implement the RMSProp optimization algorithm, which adapts the learning rate for each parameter based on the moving average of the squared gradients. This helps in converging faster and more effectively, especially in the context of non-convex optimization problems.\n\n### Approach\n1. **Initialization of Running Averages (s):** If the running averages list `s` is not provided or is empty, initialize it with zeros having the same shapes as the corresponding gradients.\n2. **Update Running Averages:** For each parameter, compute the new running average of squared gradients using an exponentially decaying average.\n3. **Parameter Update:** Adjust each parameter using the computed running average to scale the learning rate, ensuring numerical stability by adding a small constant (epsilon) to prevent division by zero.\n4. **Rounding:** The updated parameters are rounded to 6 decimal places as specified.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmsprop_update(\n    params: list[np.ndarray],\n    grads: list[np.ndarray],\n    s: list[np.ndarray] | None = None,\n    alpha: float = 0.01,\n    beta: float = 0.9,\n    epsilon: float = 1e-8,\n) -> tuple[list[np.ndarray], list[np.ndarray]]:\n    \"\"\"Implement a single optimization step of the RMSProp algorithm.\"\"\"\n    # Initialize the running averages if not provided or empty\n    if s is None:\n        s = []\n    if not s:\n        s = [np.zeros_like(g) for g in grads]\n    \n    new_params = []\n    new_s = []\n    \n    for param, grad, s_i in zip(params, grads, s):\n        # Update the running average of squared gradients\n        s_updated = beta * s_i + (1 - beta) * (grad ** 2)\n        # Compute the update term\n        update = alpha * grad / (np.sqrt(s_updated) + epsilon)\n        # Update the parameter and round to 6 decimal places\n        param_updated = np.round(param - update, 6)\n        new_params.append(param_updated)\n        new_s.append(s_updated)\n    \n    return (new_params, new_s)\n```\n\n### Explanation\n1. **Initialization:** The running averages `s` are initialized to zeros if not provided. This ensures that each parameter has a corresponding running average initialized properly.\n2. **Running Average Update:** For each parameter, the running average of squared gradients is updated using the formula \\( s = \\beta \\cdot s + (1 - \\beta) \\cdot g^2 \\). This captures the exponentially decaying average of past squared gradients.\n3. **Parameter Adjustment:** Each parameter is adjusted by scaling the gradient with the inverse of the square root of the running average plus a small epsilon to avoid division by zero. This scaling adapts the learning rate individually for each parameter.\n4. **Rounding:** The updated parameters are rounded to 6 decimal places to meet the specified precision requirement, ensuring consistency in the output.\n\nThis approach efficiently handles parameters of any shape (scalars, vectors, matrices, etc.) and ensures numerical stability through the epsilon term. The algorithm adapts the learning rate dynamically, leading to faster convergence in training models."}
{"task_id": 543, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Gini impurity of a list of class labels. The Gini impurity is a measure of how often a randomly chosen element would be misclassified if it were labeled randomly according to the distribution of labels in the dataset. \n\n### Approach\n1. **Check for Empty Input**: If the input list is empty, return 0.0 immediately as there are no elements to classify.\n2. **Count Class Frequencies**: Use a counter to determine the frequency of each class label in the list.\n3. **Calculate Proportions**: For each class label, compute the proportion of the total elements that belong to that class.\n4. **Sum of Squared Proportions**: Compute the sum of the squares of these proportions.\n5. **Compute Gini Impurity**: Subtract the sum of squared proportions from 1 to get the Gini impurity.\n6. **Round the Result**: Round the resulting Gini impurity to 4 decimal places to meet the output requirements.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef cal_gini_index(data: list) -> float:\n    \"\"\"Calculate the Gini index of a list of class labels.\n\n    Args:\n        data: A list of hashable values representing class labels.\n\n    Returns:\n        The Gini index rounded to 4 decimal places. If the input list is empty\n        return 0.0.\n    \"\"\"\n    if not data:\n        return 0.0\n    total = len(data)\n    counts = Counter(data)\n    sum_p_sq = sum((count / total) ** 2 for count in counts.values())\n    gini = 1 - sum_p_sq\n    return round(gini, 4)\n```\n\n### Explanation\n1. **Handling Empty Input**: The function first checks if the input list is empty. If it is, the function returns 0.0 immediately.\n2. **Counting Labels**: Using `Counter` from the `collections` module, we count the occurrences of each class label efficiently.\n3. **Calculating Proportions**: For each class label, the proportion of elements belonging to that class is calculated by dividing the count of the class by the total number of elements.\n4. **Summing Squared Proportions**: The sum of the squares of these proportions is computed. This step is crucial as it forms the core part of the Gini impurity calculation.\n5. **Computing Gini Impurity**: The Gini impurity is derived by subtracting the sum of squared proportions from 1.\n6. **Rounding**: The result is rounded to 4 decimal places to ensure the output meets the specified precision requirements.\n\nThis approach efficiently handles the calculation using standard libraries and ensures accuracy and correctness for both small and large datasets."}
{"task_id": 544, "completion_id": 0, "completion": "To solve this problem, we need to partition a dataset into two subsets based on a threshold value applied to a specific feature. This is similar to how decision trees split data nodes, where each sample is directed to one of two child nodes based on a feature's value relative to a threshold.\n\n### Approach\n1. **Problem Analysis**: The task requires splitting the dataset such that one subset contains samples with a feature value greater than or equal to the threshold, and the other subset contains the remaining samples. The order of samples in each subset must match their order in the original dataset, and the original dataset must remain unmodified.\n2. **Intuition**: By iterating through each sample in the dataset, we can check the value of the specified feature. Depending on whether this value meets or exceeds the threshold, the sample is added to one subset or the other. This ensures that the order is preserved and the original dataset is not altered.\n3. **Algorithm**: \n   - Initialize two empty lists to hold the two subsets.\n   - Iterate through each sample in the dataset.\n   - For each sample, check the value of the specified feature. If the value is greater than or equal to the threshold, add the sample to the first subset. Otherwise, add it to the second subset.\n   - Return the two subsets as a tuple.\n\n### Solution Code\n```python\ndef split_tree(data: list, fea: int, value: float):\n    \"\"\"Split a data set into two ordered subsets according to a threshold.\n\n    Args:\n        data (list): List of samples. Each sample is a list or tuple of feature values.\n        fea (int): Index of the feature used for the split.\n        value (float): Threshold for the split.\n\n    Returns:\n        tuple[list, list]: Two lists (set1, set2) where set1 contains the samples\n        with feature value >= threshold and set2 contains the rest.\n    \"\"\"\n    set1 = []\n    set2 = []\n    for sample in data:\n        if sample[fea] >= value:\n            set1.append(sample)\n        else:\n            set2.append(sample)\n    return (set1, set2)\n```\n\n### Explanation\n- **Initialization**: Two empty lists, `set1` and `set2`, are created to store the partitioned samples.\n- **Iteration**: Each sample in the input dataset is processed in order. The feature value at the specified index `fea` is compared against the threshold `value`.\n- **Partitioning**: Samples with a feature value greater than or equal to the threshold are added to `set1`, while others are added to `set2`.\n- **Result**: The function returns a tuple containing the two subsets, ensuring the original order of samples is preserved in both subsets.\n\nThis approach efficiently splits the dataset in linear time, O(n), where n is the number of samples, ensuring optimal performance while adhering to the problem constraints."}
{"task_id": 545, "completion_id": 0, "completion": "To solve this problem, we need to compute two statistics for a given binary decision tree: the number of leaf nodes and the maximum depth of the tree. A leaf node is defined as a node where the `results` attribute is not `None`, and the maximum depth is the length of the longest path from the root to a leaf node.\n\n### Approach\n1. **Base Cases**:\n   - If the tree is empty (root is `None`), return `(0, 0)`.\n   - If the current node is a leaf node (`results` is not `None`), return `(1, 1)` since a leaf node contributes to both the leaf count and depth.\n\n2. **Recursive Cases**:\n   - For non-leaf nodes, recursively compute the statistics for the left and right subtrees.\n   - Sum the leaf counts from both subtrees.\n   - Calculate the maximum depth by taking the maximum depth from both subtrees and adding 1 (for the current node). If there are no leaves in either subtree, the depth is 0.\n\n### Solution Code\n```python\ndef tree_stats(root: \"node | None\") -> tuple[int, int]:\n    \"\"\"Return the number of leaf nodes and the maximum depth of the tree.\n\n    A **leaf** is defined as a node whose ``results`` attribute is not ``None``.\n\n    Parameters\n    ----------\n    root : node | None\n        Root of the decision tree.  If *None*, the tree is considered empty.\n\n    Returns\n    -------\n    tuple[int, int]\n        ``(leaf_count, max_depth)`` where\n        * *leaf_count* is the total number of leaves in the tree, and\n        * *max_depth* is the length of the longest path from *root* to a leaf.\n    \"\"\"\n    if root is None:\n        return (0, 0)\n    if root.results is not None:\n        return (1, 1)\n    left_count, left_depth = tree_stats(root.left)\n    right_count, right_depth = tree_stats(root.right)\n    total_count = left_count + right_count\n    if total_count == 0:\n        return (0, 0)\n    current_depth = 1 + max(left_depth, right_depth)\n    return (total_count, current_depth)\n```\n\n### Explanation\n- **Base Cases Handling**:\n  - An empty tree directly returns `(0, 0)`.\n  - A leaf node returns `(1, 1)` as it contributes to both the leaf count and depth.\n\n- **Recursive Calculation**:\n  - For non-leaf nodes, the function recursively processes the left and right children.\n  - The total leaf count is the sum of leaves from both subtrees.\n  - The maximum depth is computed as 1 plus the maximum depth of the left or right subtree, ensuring that only valid paths (those ending in leaves) contribute to the depth.\n\nThis approach ensures that both statistics are computed efficiently in a single traversal of the tree, adhering to the problem constraints."}
{"task_id": 546, "completion_id": 0, "completion": "To solve this problem, we need to implement the CART (Classification And Regression Tree) algorithm for classification tasks using the Gini impurity as the splitting criterion. The goal is to build a binary decision tree recursively by evaluating possible splits based on Gini gain and selecting the best split that maximizes this gain.\n\n### Approach\n1. **Base Case Handling**: If the input data is empty, return an empty dictionary. If all samples in the current subset belong to the same class, create a leaf node with that class label.\n2. **Gini Impurity Calculation**: Compute the Gini impurity for the current node. The Gini impurity measures the likelihood of misclassification if a random label is chosen according to the distribution of labels in the subset.\n3. **Evaluate Splits**: For each feature and each unique value of that feature, split the data into two subsets (left and right). Ignore splits that result in empty subsets.\n4. **Gini Gain Calculation**: For each valid split, compute the Gini gain, which is the reduction in impurity achieved by the split. The best split is the one with the highest Gini gain. In case of ties, the split that appears first when iterating over features in increasing order and values in sorted order is chosen.\n5. **Recursive Tree Construction**: If no split yields a positive gain, create a leaf node with the majority class. Otherwise, create an internal node with the best split and recursively build the left and right subtrees.\n\n### Solution Code\n```python\nfrom collections import Counter\nfrom typing import Any, Dict, List\n\ndef build_tree(data: List[List[Any]]) -> Dict[str, Any]:\n    \"\"\"Build a CART decision tree using the Gini impurity criterion.\"\"\"\n\n    def compute_gini(samples):\n        if not samples:\n            return 0.0\n        counts = Counter(sample[-1] for sample in samples)\n        total = len(samples)\n        impurity = 1.0\n        for count in counts.values():\n            prob = count / total\n            impurity -= prob ** 2\n        return impurity\n\n    def get_majority_label(samples):\n        labels = [sample[-1] for sample in samples]\n        counts = Counter(labels)\n        if not counts:\n            return None\n        max_count = max(counts.values())\n        candidates = [label for label, cnt in counts.items() if cnt == max_count]\n        for label in labels:\n            if label in candidates:\n                return label\n        return None  # Should not be reached if samples is non-empty\n\n    if not data:\n        return {}\n\n    current_gini = compute_gini(data)\n    if current_gini == 0.0:\n        return {\"label\": data[0][-1]}\n\n    best_gain = -1.0\n    best_feature = None\n    best_value = None\n    best_left = None\n    best_right = None\n\n    n_features = len(data[0]) - 1 if data else 0\n\n    for feature_index in range(n_features):\n        unique_values = list({sample[feature_index] for sample in data})\n        unique_values.sort()\n        for value in unique_values:\n            left = [sample for sample in data if sample[feature_index] == value]\n            right = [sample for sample in data if sample[feature_index] != value]\n            if not left or not right:\n                continue\n            left_gini = compute_gini(left)\n            right_gini = compute_gini(right)\n            total = len(data)\n            weighted_gini = (len(left) / total) * left_gini + (len(right) / total) * right_gini\n            gain = current_gini - weighted_gini\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = feature_index\n                best_value = value\n                best_left = left\n                best_right = right\n\n    if best_gain <= 0.0:\n        majority_label = get_majority_label(data)\n        return {\"label\": majority_label}\n    else:\n        left_subtree = build_tree(best_left)\n        right_subtree = build_tree(best_right)\n        return {\n            \"feature\": best_feature,\n            \"value\": best_value,\n            \"left\": left_subtree,\n            \"right\": right_subtree\n        }\n```\n\n### Explanation\n- **Gini Impurity Calculation**: The `compute_gini` function calculates the Gini impurity of a dataset by considering the distribution of class labels. A lower Gini impurity indicates a purer subset.\n- **Majority Label Determination**: The `get_majority_label` function determines the majority class in a dataset, breaking ties by selecting the first occurring class in the data.\n- **Tree Construction**: The main `build_tree` function recursively constructs the decision tree. It first checks for base cases (empty data or pure subset), then evaluates all possible splits to find the best one based on Gini gain. If no valid split improves purity, a leaf node is created with the majority class. Otherwise, an internal node is created, and the process repeats for the left and right subsets.\n\nThis approach ensures that the decision tree is built efficiently, considering all possible splits and selecting the optimal ones to maximize classification accuracy."}
{"task_id": 547, "completion_id": 0, "completion": "To solve this problem, we need to compute the forward probability vector \\(\\alpha_t\\) for a Hidden Markov Model (HMM) at a given time step \\(t\\). The forward probability vector \\(\\alpha_t(i)\\) represents the probability of being in state \\(i\\) after observing the first \\(t+1\\) observations. \n\n### Approach\n1. **Input Validation**: \n   - Ensure the time step \\(t\\) is within the valid range.\n   - Check the consistency of the dimensions of the state-transition matrix \\(A\\), emission matrix \\(B\\), and initial state distribution \\(S\\).\n   - Verify that each observation in the sequence is a valid index for the emission matrix.\n\n2. **Conversion to Numpy Arrays**: \n   - Convert the input matrices and vectors into numpy arrays for efficient matrix operations.\n\n3. **Initialization**:\n   - Compute the initial forward probability vector \\(\\alpha_0\\) using the initial state distribution and the emission probabilities for the first observation.\n\n4. **Iterative Calculation**:\n   - For each subsequent time step up to \\(t\\), update the forward probability vector using the state-transition matrix and the emission probabilities for the current observation.\n\n5. **Rounding**:\n   - Round the resulting probabilities to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef forward_step(A: list[list[float]],\n                 B: list[list[float]],\n                 S: list[float],\n                 obs: list[int],\n                 t: int) -> list[float] | int:\n    \"\"\"Compute the forward probability vector \u03b1_t for a Hidden Markov Model.\"\"\"\n    # Check if t is within the valid range\n    if t < 0 or t >= len(obs):\n        return -1\n    \n    # Validate the initial state distribution S\n    n = len(S)\n    if n == 0:\n        return -1\n    \n    # Validate state-transition matrix A (must be n x n)\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    \n    # Validate emission matrix B (must be n x m, m >= 1)\n    if len(B) != n:\n        return -1\n    m = len(B[0]) if n > 0 else 0\n    for row in B:\n        if len(row) != m:\n            return -1\n    if m == 0:\n        return -1\n    \n    # Validate all observations are valid integers within the emission matrix column range\n    for o in obs:\n        if not isinstance(o, int):\n            return -1\n        if o < 0 or o >= m:\n            return -1\n    \n    # Convert input lists to numpy arrays for efficient computation\n    A_np = np.array(A, dtype=np.float64)\n    B_np = np.array(B, dtype=np.float64)\n    S_np = np.array(S, dtype=np.float64)\n    \n    # Compute \u03b1_0\n    current_obs = obs[0]\n    alpha_prev = S_np * B_np[:, current_obs]\n    \n    # Compute \u03b1_t for t >= 1\n    for i in range(1, t + 1):\n        current_obs = obs[i]\n        alpha_prev = np.dot(alpha_prev, A_np) * B_np[:, current_obs]\n    \n    # Round to 4 decimal places and return as a list\n    alpha_rounded = [round(x, 4) for x in alpha_prev.tolist()]\n    return alpha_rounded\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the time step \\(t\\) is valid and ensures all matrices and vectors have consistent dimensions. Each observation is checked to be a valid index for the emission matrix.\n2. **Conversion to Numpy Arrays**: The input lists are converted to numpy arrays to leverage efficient matrix operations.\n3. **Initialization**: The initial forward probability vector \\(\\alpha_0\\) is computed by element-wise multiplication of the initial state distribution \\(S\\) with the emission probabilities of the first observation.\n4. **Iterative Calculation**: For each subsequent time step, the forward probability vector is updated using matrix multiplication with the state-transition matrix followed by element-wise multiplication with the emission probabilities of the current observation.\n5. **Rounding**: The resulting probabilities are rounded to four decimal places to meet the output specifications.\n\nThis approach ensures that the forward probabilities are computed efficiently and correctly, adhering to the mathematical definitions and constraints of the Hidden Markov Model."}
{"task_id": 548, "completion_id": 0, "completion": "To solve this problem, we need to compute the \u03be matrix for a given time step in a Hidden Markov Model (HMM). The \u03be matrix represents the probability of transitioning between states at a specific time step, given the entire observation sequence and model parameters. This matrix is crucial for the Baum-Welch algorithm used in training HMMs.\n\n### Approach\n1. **Validation Check**: First, check if the given time index `t` is valid. If `t` is out of bounds, return -1.\n2. **Forward Pass**: Compute the forward probabilities (\u03b1) up to time `t` using the initial state distribution, transition matrix, and emission probabilities.\n3. **Backward Pass**: Compute the backward probabilities (\u03b2) starting from the end of the observation sequence down to time `t+1`.\n4. **Compute \u03be Matrix**: Using the forward and backward probabilities, calculate the numerator for each state transition at time `t`, then normalize these values to form the \u03be matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_xi(A: list[list[float]],\n               B: list[list[float]],\n               pi: list[float],\n               obs: list[int],\n               t: int) -> list[list[float]]:\n    \"\"\"Return the \u03be (xi) matrix for a single time step in a discrete HMM.\"\"\"\n    n = len(obs)\n    if t < 0 or t >= n - 1:\n        return -1\n    \n    A = np.array(A)\n    B = np.array(B)\n    pi = np.array(pi)\n    n_states = A.shape[0]\n    \n    # Compute forward variable up to time t\n    alpha_t = pi * B[:, obs[0]]\n    for time in range(1, t + 1):\n        alpha_t = (alpha_t @ A) * B[:, obs[time]]\n    \n    # Compute backward variables\n    T = n\n    beta = np.zeros((T, n_states))\n    beta[-1] = 1.0  # Initialize last beta to 1\n    \n    for time in reversed(range(T - 1)):\n        beta[time] = A @ (B[:, obs[time + 1]] * beta[time + 1])\n    \n    # Get beta at t+1\n    beta_t_plus_1 = beta[t + 1]\n    \n    # Calculate B_j for observation at t+1\n    B_j = B[:, obs[t + 1]]\n    \n    # Compute numerator matrix\n    numerator = (alpha_t[:, np.newaxis] * A) * B_j * beta_t_plus_1\n    denominator = numerator.sum()\n    \n    if denominator == 0:\n        # This case should not occur as per problem constraints\n        return [[0.0 for _ in range(n_states)] for _ in range(n_states)]\n    \n    xi = numerator / denominator\n    \n    # Round to 4 decimals and convert to list of lists\n    xi_rounded = np.round(xi, 4).tolist()\n    return xi_rounded\n```\n\n### Explanation\n1. **Validation Check**: The function first checks if the time index `t` is within valid bounds. If not, it returns -1.\n2. **Forward Pass**: The forward probabilities (\u03b1) are computed iteratively up to time `t` using the initial state distribution (\u03c0), transition matrix (A), and emission probabilities (B) for the observed sequence.\n3. **Backward Pass**: The backward probabilities (\u03b2) are computed starting from the end of the observation sequence. This involves initializing the last time step's \u03b2 values to 1 and iterating backwards to compute \u03b2 for each preceding time step using the transition and emission probabilities.\n4. **Compute \u03be Matrix**: The numerator for each element of the \u03be matrix is computed using the forward probability at time `t`, the transition probabilities, the emission probabilities for the next observation, and the backward probability at time `t+1`. These values are then normalized by their sum to form the final \u03be matrix, which is rounded to four decimal places and returned as a list of lists.\n\nThis approach efficiently combines the forward and backward probabilities to compute the \u03be matrix, which is essential for understanding state transitions in HMMs."}
{"task_id": 549, "completion_id": 0, "completion": "To solve this problem, we need to compute the squared Euclidean distance between two numerical vectors. The squared Euclidean distance is defined as the sum of the squared differences between corresponding elements of the two vectors. The solution must handle different input types and ensure the vectors are of the same length.\n\n### Approach\n1. **Input Conversion**: Convert the input vectors (which can be lists, tuples, or NumPy arrays) into NumPy arrays to facilitate element-wise operations.\n2. **Length Check**: Verify that the two vectors have the same length. If not, return -1.\n3. **Compute Squared Differences**: Subtract the elements of the second vector from the first, square each difference, and sum the results.\n4. **Rounding**: Round the computed sum to four decimal places and return it as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cal_dis(x, y):\n    \"\"\"Compute the squared Euclidean distance between two vectors.\n\n    Parameters\n    ----------\n    x : list, tuple, or np.ndarray\n        First vector of numbers.\n    y : list, tuple, or np.ndarray\n        Second vector of numbers.\n\n    Returns\n    -------\n    float\n        Squared Euclidean distance rounded to 4 decimal places, or -1 if the\n        vectors are of different lengths.\n    \"\"\"\n    x = np.array(x)\n    y = np.array(y)\n    if len(x) != len(y):\n        return -1.0\n    distance_sq = np.sum((x - y) ** 2)\n    return round(distance_sq, 4)\n```\n\n### Explanation\n1. **Input Conversion**: The vectors `x` and `y` are converted to NumPy arrays using `np.array()`, which allows for efficient element-wise operations.\n2. **Length Check**: The lengths of the converted vectors are compared. If they differ, the function returns `-1.0` immediately.\n3. **Squared Differences Calculation**: Using NumPy's vectorized operations, the difference between the arrays is computed, each element is squared, and the sum of these squares is calculated. This approach is both efficient and concise.\n4. **Rounding**: The result of the sum is rounded to four decimal places using Python's built-in `round()` function, ensuring the output meets the specified precision requirement.\n\nThis approach efficiently handles various input types, ensures correctness through vectorized operations, and adheres to the problem constraints."}
{"task_id": 551, "completion_id": 0, "completion": "To solve this problem, we need to determine the best split for a categorical CART node based on the Gini impurity. The goal is to find the split that minimizes the weighted Gini impurity and check if the reduction in impurity meets a specified threshold. If not, we return that no valid split exists.\n\n### Approach\n1. **Calculate Parent Gini Impurity**: Compute the Gini impurity for the entire dataset before any split. This serves as the baseline impurity.\n2. **Iterate Through Features and Values**: For each feature and each unique value in that feature, consider splitting the dataset into two subsets: one where the feature equals the value and the other where it does not.\n3. **Compute Gini Impurity for Subsets**: For each split, compute the Gini impurity for both resulting subsets. Calculate the weighted average of these impurities based on the size of each subset.\n4. **Track Best Split**: Keep track of the split that results in the lowest weighted Gini impurity.\n5. **Check Validity of Best Split**: After evaluating all possible splits, check if the best split reduces the impurity by at least the specified threshold (epsilon). If it does, return the best split details; otherwise, indicate no valid split exists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef best_gini_split(X: np.ndarray,\n                    y: np.ndarray,\n                    epsilon: float = 1e-3) -> tuple[int, object, float]:\n    n_samples, n_features = X.shape\n    if n_samples == 0:\n        return (-1, None, 0.0)\n    \n    # Compute parent Gini impurity\n    parent_counts = np.unique(y, return_counts=True)[1]\n    sum_sq = np.sum((parent_counts / n_samples) ** 2)\n    parent_gini = 1.0 - sum_sq\n    parent_gini = float(parent_gini)\n    \n    best_gini = parent_gini\n    best_feature = -1\n    best_value = None\n    \n    for feature in range(n_features):\n        feature_col = X[:, feature]\n        unique_values = np.unique(feature_col)\n        \n        for value in unique_values:\n            mask = (feature_col == value)\n            left_y = y[mask]\n            right_y = y[~mask]\n            \n            # Compute Gini for left subset\n            if left_y.size == 0:\n                left_gini = 1.0\n            else:\n                left_counts = np.unique(left_y, return_counts=True)[1]\n                left_sum_sq = np.sum((left_counts / left_y.size) ** 2)\n                left_gini = 1.0 - left_sum_sq\n            \n            # Compute Gini for right subset\n            if right_y.size == 0:\n                right_gini = 1.0\n            else:\n                right_counts = np.unique(right_y, return_counts=True)[1]\n                right_sum_sq = np.sum((right_counts / right_y.size) ** 2)\n                right_gini = 1.0 - right_sum_sq\n            \n            # Calculate weighted Gini impurity\n            weighted_gini = (left_y.size / n_samples) * left_gini + (right_y.size / n_samples) * right_gini\n            \n            # Update best split if current is better\n            if weighted_gini < best_gini:\n                best_gini = weighted_gini\n                best_feature = feature\n                best_value = value.item() if isinstance(value, np.generic) else value\n    \n    # Check if the improvement meets the epsilon threshold\n    if (parent_gini - best_gini) >= epsilon:\n        return (best_feature, best_value, float(best_gini))\n    else:\n        return (-1, None, parent_gini)\n```\n\n### Explanation\n1. **Parent Gini Calculation**: The parent Gini impurity is calculated using the class distribution of the entire dataset. This gives a baseline measure of impurity before any splits.\n2. **Feature and Value Iteration**: For each feature and each unique value within that feature, the dataset is split into two subsets. The left subset contains samples where the feature matches the value, and the right subset contains the remaining samples.\n3. **Subset Gini Calculation**: The Gini impurity for each subset is computed. If a subset is empty, its impurity is considered 1.0. The weighted average impurity of the subsets is calculated based on their sizes relative to the total dataset.\n4. **Track Best Split**: The algorithm keeps track of the split with the lowest weighted impurity. If multiple splits have the same impurity, the first encountered split is retained.\n5. **Validity Check**: After evaluating all possible splits, the best split's impurity reduction is compared to the threshold. If the reduction is sufficient, the split details are returned; otherwise, it is indicated that no valid split exists.\n\nThis approach ensures that we efficiently evaluate all potential splits and determine the optimal one based on the Gini impurity criterion, adhering to the constraints and requirements of the problem."}
{"task_id": 552, "completion_id": 0, "completion": "To solve this problem, we need to compute the posterior state distribution \u03b3\u209c for a given Hidden Markov Model (HMM) at a specific time index t using the forward-backward algorithm. The algorithm involves calculating forward probabilities (\u03b1) and backward probabilities (\u03b2), combining them, and normalizing to get the posterior probabilities.\n\n### Approach\n1. **Validation Check**: Ensure the time index t is within the valid range of the observation sequence length.\n2. **Forward Probabilities (\u03b1)**: Compute the probability of being in a state at time t given the observations up to that time. This is done by iterating through each time step and updating the probabilities based on the previous state probabilities, transition matrix, and emission probabilities.\n3. **Backward Probabilities (\u03b2)**: Compute the probability of observing the remaining sequence from time t+1 to the end given the current state. This is done by iterating backward from the last observation to the current time step, updating the probabilities based on the transition matrix, emission probabilities, and future state probabilities.\n4. **Combine \u03b1 and \u03b2**: Multiply the forward and backward probabilities at time t and normalize the result to get the posterior probabilities \u03b3\u209c.\n5. **Rounding**: Round each component of the resulting probabilities to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    T = len(observations)\n    if t < 0 or t >= T:\n        return []\n    \n    pi = np.array(hmm['pi'])\n    A = np.array(hmm['A'])\n    B = np.array(hmm['B'])\n    n_states = len(pi)\n    \n    # Compute forward probabilities (alpha)\n    alpha = np.zeros((T, n_states))\n    alpha[0] = pi * B[:, observations[0]]\n    \n    for time in range(1, T):\n        for j in range(n_states):\n            alpha[time, j] = B[j, observations[time]] * np.dot(alpha[time - 1], A[:, j])\n    \n    # Compute backward probabilities (beta)\n    beta = np.ones((T, n_states))\n    for time in reversed(range(T - 1)):  # time goes from T-2 down to 0\n        for i in range(n_states):\n            beta[time, i] = np.dot(A[i, :], B[:, observations[time + 1]] * beta[time + 1, :])\n    \n    # Compute gamma at time t\n    gamma = alpha[t] * beta[t]\n    gamma_sum = gamma.sum()\n    if gamma_sum == 0:\n        return [0.0] * n_states  # Edge case if all probabilities are zero (unlikely with valid HMM)\n    gamma_normalized = gamma / gamma_sum\n    \n    # Round to four decimal places and convert to list\n    rounded_gamma = [round(prob, 4) for prob in gamma_normalized.tolist()]\n    return rounded_gamma\n```\n\n### Explanation\n1. **Validation Check**: The function first checks if the time index t is within the valid range [0, len(observations) - 1]. If not, it returns an empty list.\n2. **Forward Probabilities (\u03b1)**: The forward probabilities are initialized with the initial state probabilities multiplied by the emission probabilities of the first observation. For each subsequent time step, the probabilities are updated using the previous probabilities, transition matrix, and emission probabilities.\n3. **Backward Probabilities (\u03b2)**: The backward probabilities start from the last observation with all probabilities set to 1. For each previous time step, the probabilities are updated using the transition matrix, emission probabilities of the next observation, and the future backward probabilities.\n4. **Combining \u03b1 and \u03b2**: The posterior probabilities \u03b3\u209c are computed by element-wise multiplication of the forward and backward probabilities at time t, followed by normalization to ensure they sum to 1.\n5. **Rounding**: The resulting probabilities are rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the required probabilities using dynamic programming for both forward and backward passes, ensuring accurate and efficient computation even for longer sequences."}
{"task_id": 553, "completion_id": 0, "completion": "To solve this problem, we need to implement a logistic regression model using batch gradient descent (BGD) to classify binary labels for a given test dataset. The model will be trained on the provided training data and then used to predict labels for the test data.\n\n### Approach\n1. **Add Bias Column**: Append a column of ones to both the training and test datasets to account for the bias term in the logistic regression model.\n2. **Initialize Weights**: Start with a weight vector initialized to zeros, including the bias term.\n3. **Batch Gradient Descent**: Iteratively update the weights using the gradient of the log-loss function. The update rule involves computing the gradient of the log-likelihood and adjusting the weights in the direction that minimizes the loss.\n4. **Prediction**: After training the model, use the learned weights to compute probabilities for the test data. Convert these probabilities to binary labels (0 or 1) based on a threshold of 0.5.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    alpha: float = 0.01,\n    max_iter: int = 1000,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model with batch gradient descent and\n    predict class labels for a test set.\"\"\"\n    \n    # Add a bias column (of ones) to the training and test data\n    X_train_bias = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n    X_test_bias = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n    \n    # Initialize the weight vector with zeros\n    n_samples = X_train.shape[0]\n    w = np.zeros(X_train_bias.shape[1])\n    \n    # Perform batch gradient descent for max_iter iterations\n    for _ in range(max_iter):\n        z = X_train_bias @ w\n        sigma = 1 / (1 + np.exp(-z))\n        gradient = (X_train_bias.T @ (y_train - sigma)) / n_samples\n        w += alpha * gradient\n    \n    # Compute probabilities for the test set and convert to class labels\n    test_probs = 1 / (1 + np.exp(-X_test_bias @ w))\n    predictions = (test_probs >= 0.5).astype(int).tolist()\n    \n    return predictions\n```\n\n### Explanation\n1. **Bias Column Addition**: The first step involves adding a column of ones to the training and test datasets to include a bias term in the model. This allows the model to learn an intercept term, which is crucial for fitting the data properly.\n2. **Weight Initialization**: The weights are initialized to zeros, ensuring that we start from a neutral point and gradually adjust based on the gradient updates.\n3. **Gradient Descent**: For each iteration, the gradient of the log-loss is computed using the entire training dataset. The weights are updated in the direction that minimizes the loss, scaled by the learning rate `alpha`. The gradient is derived from the difference between the actual labels and the predicted probabilities, multiplied by the feature matrix.\n4. **Prediction**: After training, the model computes the probabilities for each test instance by applying the sigmoid function to the linear combination of the test data and learned weights. These probabilities are thresholded at 0.5 to produce binary predictions, which are then converted to a standard Python list of integers.\n\nThis approach efficiently trains the logistic regression model using batch gradient descent and applies it to make predictions on new data, ensuring both accuracy and computational efficiency."}
{"task_id": 555, "completion_id": 0, "completion": "To solve this problem, we need to implement the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm from scratch. This algorithm groups points that are densely packed together and marks points in low-density regions as noise. The key steps involve computing pairwise distances, identifying core points, and expanding clusters using a breadth-first search (BFS).\n\n### Approach\n1. **Compute Pairwise Distances**: Calculate the Euclidean distance between every pair of points to determine their proximity.\n2. **Identify Neighborhoods**: For each point, determine which other points lie within a given radius (\u03b5) to form its neighborhood.\n3. **Determine Core Points**: A point is considered a core point if its neighborhood contains at least a minimum number of points (min_samples).\n4. **Expand Clusters Using BFS**: Starting from an unlabeled core point, expand the cluster by including all reachable points through core points. Points that are not part of any cluster after processing are marked as noise.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\"\"\"\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n    \n    # Compute pairwise Euclidean distances\n    distances = np.sqrt(np.sum((data[:, np.newaxis, :] - data) ** 2, axis=2))\n    \n    # Build the neighborhood list for each sample\n    neighborhood = []\n    for i in range(n_samples):\n        mask = distances[i] <= eps\n        neighbors = np.where(mask)[0].tolist()\n        neighborhood.append(neighbors)\n    \n    # Identify core points\n    core_mask = [len(neighbors) >= min_samples for neighbors in neighborhood]\n    \n    labels = [0] * n_samples  # 0 represents unlabeled\n    cluster_id = 0\n    \n    for i in range(n_samples):\n        if labels[i] != 0:\n            continue  # Already processed\n        if not core_mask[i]:\n            continue  # Not a core point, skip\n        \n        # Start a new cluster\n        cluster_id += 1\n        labels[i] = cluster_id\n        \n        queue = deque()\n        queue.append(i)\n        \n        while queue:\n            current_point = queue.popleft()\n            for neighbor in neighborhood[current_point]:\n                if labels[neighbor] == 0:\n                    labels[neighbor] = cluster_id\n                    if core_mask[neighbor]:\n                        queue.append(neighbor)\n    \n    # Assign -1 to noise points (unlabeled points)\n    for i in range(n_samples):\n        if labels[i] == 0:\n            labels[i] = -1\n    \n    return labels\n```\n\n### Explanation\n1. **Pairwise Distance Calculation**: We compute the Euclidean distance between every pair of points using vectorized operations for efficiency.\n2. **Neighborhood Identification**: For each point, we determine which other points are within the \u03b5 radius, forming its neighborhood.\n3. **Core Point Detection**: Points with neighborhoods containing at least `min_samples` points are marked as core points.\n4. **Cluster Expansion**: Using BFS from each unlabeled core point, we expand the cluster by including all reachable points through core points. Non-core points in the neighborhood are included in the cluster but do not expand it further.\n5. **Noise Assignment**: Points that remain unlabeled after processing all core points are marked as noise (-1).\n\nThis approach efficiently groups points into clusters based on density and correctly identifies noise points, adhering to the DBSCAN algorithm's principles."}
{"task_id": 559, "completion_id": 0, "completion": "To solve this problem, we need to collect the values of all leaf nodes in a binary tree in left-to-right order. A leaf node is defined as a node with no children. The challenge is to traverse the tree and collect these values efficiently.\n\n### Approach\nThe approach to solve this problem involves using a depth-first traversal of the binary tree. The key idea is to recursively visit each node, check if it is a leaf node, and collect its value if it is. If a node is not a leaf, we recursively process its left and right subtrees. By processing the left subtree first and then the right subtree, we ensure that the values are collected in the desired left-to-right order.\n\n### Solution Code\n```python\nfrom typing import List, Optional\n\nclass Node:\n    \"\"\"Binary-tree node used by the task.\"\"\"\n    def __init__(self, val: Optional[int] = None, right: 'Optional[Node]' = None,\n                 left: 'Optional[Node]' = None):\n        self.val = val\n        self.right = right\n        self.left = left\n\ndef leaf_values(root: Optional[Node]) -> List[int]:\n    \"\"\"Collect all leaf values from left to right.\"\"\"\n    if root is None:\n        return []\n    if root.left is None and root.right is None:\n        return [root.val]\n    left_leaves = leaf_values(root.left)\n    right_leaves = leaf_values(root.right)\n    return left_leaves + right_leaves\n```\n\n### Explanation\n1. **Base Case for Empty Tree**: If the root is `None`, the tree is empty, and we return an empty list.\n2. **Leaf Node Check**: If a node has no left or right children, it is a leaf node. We return a list containing its value.\n3. **Recursive Traversal**: For non-leaf nodes, we recursively collect leaf values from the left and right subtrees. The results from the left subtree are concatenated with the results from the right subtree to maintain the left-to-right order.\n\nThis approach ensures that each node is processed exactly once, leading to a time complexity of O(n), where n is the number of nodes in the tree. The space complexity is also O(n) in the worst case due to the recursion stack depth for a skewed tree. However, for a balanced tree, the space complexity would be O(log n)."}
{"task_id": 560, "completion_id": 0, "completion": "To solve this problem, we need to find the k-nearest neighbors of a query point in a given set of points using a k-d tree. The k-d tree is a space-partitioning data structure that allows efficient nearest neighbor searches in multidimensional space. The solution involves constructing a balanced k-d tree and then performing a search to find the k closest points to the query point.\n\n### Approach\n\n1. **Building the k-d Tree**:\n   - The k-d tree is built recursively by selecting the median point along the current axis and partitioning the points into left and right subtrees. This process alternates between axes at each level of the tree to ensure balance.\n\n2. **Nearest Neighbor Search**:\n   - The search involves traversing the tree to find the closest points to the query. During traversal, we maintain a max-heap (priority queue) to keep track of the k closest points found so far. This allows efficient updates and replacements as closer points are found.\n   - The algorithm checks both subtrees if there is a possibility that the other subtree might contain points closer than the current farthest point in the heap, based on the distance to the splitting plane.\n\n### Solution Code\n\n```python\nimport heapq\n\ndef k_nearest_neighbors(points: list[list[float]], query: list[float], k: int = 3) -> list[int]:\n    \"\"\"Find the *k* nearest neighbours of *query* among *points* using a k-d tree.\"\"\"\n    \n    n = len(points)\n    if n == 0:\n        return []\n    k = min(k, n)\n    if k <= 0:\n        return []\n    \n    num_dimensions = len(points[0])\n    \n    def build_kd_tree(indices, depth):\n        if not indices:\n            return None\n        axis = depth % num_dimensions\n        sorted_indices = sorted(indices, key=lambda i: points[i][axis])\n        median = len(sorted_indices) // 2\n        median_index = sorted_indices[median]\n        return {\n            'index': median_index,\n            'axis': axis,\n            'left': build_kd_tree(sorted_indices[:median], depth + 1),\n            'right': build_kd_tree(sorted_indices[median+1:], depth + 1),\n        }\n    \n    tree = build_kd_tree(list(range(n)), 0)\n    heap = []\n    \n    def search(node):\n        if not node:\n            return\n        point = points[node['index']]\n        dist_sq = sum((q - p)**2 for q, p in zip(query, point))\n        \n        if len(heap) < k:\n            heapq.heappush(heap, (-dist_sq, -node['index'], node['index']))\n        else:\n            current_max_neg_dist, current_max_neg_idx, current_max_idx = heap[0]\n            current_max_dist = -current_max_neg_dist\n            if (dist_sq < current_max_dist) or (dist_sq == current_max_dist and node['index'] < current_max_idx):\n                heapq.heappop(heap)\n                heapq.heappush(heap, (-dist_sq, -node['index'], node['index']))\n        \n        axis = node['axis']\n        if query[axis] < point[axis]:\n            search(node['left'])\n            current_max_dist_sq = -heap[0][0] if len(heap) >= k else float('inf')\n            split_dist = (point[axis] - query[axis])**2\n            if split_dist <= current_max_dist_sq:\n                search(node['right'])\n        else:\n            search(node['right'])\n            current_max_dist_sq = -heap[0][0] if len(heap) >= k else float('inf')\n            split_dist = (query[axis] - point[axis])**2\n            if split_dist <= current_max_dist_sq:\n                search(node['left'])\n    \n    search(tree)\n    \n    results = []\n    while heap:\n        neg_dist_sq, neg_idx, idx = heapq.heappop(heap)\n        results.append((-neg_dist_sq, idx))\n    results.sort(key=lambda x: (x[0], x[1]))\n    return [x[1] for x in results]\n```\n\n### Explanation\n\n1. **Building the k-d Tree**:\n   - The tree is constructed recursively by selecting the median point along the current axis (determined by the depth of the tree). This median point becomes the root of the current subtree, and the process is repeated for the left and right subtrees.\n\n2. **Searching for Nearest Neighbors**:\n   - The search starts at the root of the tree and traverses down to the leaf nodes, adding each node's point to a max-heap if it is among the k closest points found so far.\n   - After traversing the initial path, the algorithm backtracks to check other subtrees if there is a possibility that they might contain closer points. This is determined by comparing the distance from the query to the splitting plane with the current maximum distance in the heap.\n\n3. **Heap Management**:\n   - The max-heap is used to maintain the k closest points. Points are stored with their negative squared distances to simulate a max-heap using Python's min-heap implementation. This allows efficient replacement of the farthest point when a closer one is found.\n\n4. **Result Extraction**:\n   - After the search, the points in the heap are sorted by their squared distance and index to ensure the correct order, and the indices of the k closest points are returned."}
{"task_id": 561, "completion_id": 0, "completion": "To solve this problem, we need to implement the Expectation-Maximization (EM) algorithm for a Gaussian Mixture Model (GMM) to cluster data points into K components. The solution must ensure deterministic output by sorting the components based on their means and re-labeling them accordingly.\n\n### Approach\n1. **Initialization**: For K > 1, randomly initialize the parameters (means, weights, and covariance matrices) of the Gaussian components. The means are initialized using K random data points. Covariance matrices are initialized using the covariance of the entire dataset with a small regularization term to ensure they are positive definite.\n2. **Expectation (E) Step**: Compute the responsibilities (posterior probabilities) of each data point belonging to each component using the current parameters.\n3. **Maximization (M) Step**: Update the parameters (weights, means, covariance matrices) based on the computed responsibilities.\n4. **Convergence Check**: Stop iterating when the change in mixture weights falls below a specified tolerance or the maximum number of iterations is reached.\n5. **Component Reordering**: After convergence, sort the components by the first coordinate of their means to ensure deterministic output and re-label the cluster indices accordingly.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef gmm_em(data: np.ndarray, k: int, max_iter: int = 200, epsilon: float = 1e-4) -> list[int]:\n    \"\"\"Perform Gaussian Mixture Model clustering using the EM algorithm and return deterministic cluster labels.\"\"\"\n    if k == 1:\n        return [0] * len(data)\n    \n    n_samples, d = data.shape\n    # Deterministic seed based on data and k\n    data_sum = np.sum(data)\n    seed = int(abs(data_sum * 1e6)) + k\n    np.random.seed(seed)\n    \n    # Initialize parameters\n    indices = np.random.choice(n_samples, k, replace=False)\n    mu = data[indices]\n    pi = np.ones(k) / k\n    data_cov = np.cov(data, rowvar=False) + 1e-6 * np.eye(d)\n    sigma = np.tile(data_cov, (k, 1, 1))\n    prev_pi = pi.copy()\n    \n    for _ in range(max_iter):\n        # E-step: Compute responsibilities\n        log_probs = np.zeros((n_samples, k))\n        for ki in range(k):\n            mu_k = mu[ki]\n            sigma_k = sigma[ki]\n            # Handle potential singular matrix by adding regularization\n            try:\n                sigma_inv = np.linalg.inv(sigma_k)\n                log_det = np.log(np.linalg.det(sigma_k))\n            except np.linalg.LinAlgError:\n                sigma_k_reg = sigma_k + 1e-6 * np.eye(d)\n                sigma_inv = np.linalg.inv(sigma_k_reg)\n                log_det = np.log(np.linalg.det(sigma_k_reg))\n            diff = data - mu_k\n            quadratic = np.einsum('ij,ij->i', diff @ sigma_inv, diff)\n            log_prob = np.log(pi[ki]) - 0.5 * (d * np.log(2 * np.pi) + log_det + quadratic)\n            log_probs[:, ki] = log_prob\n        \n        # Compute gamma (responsibilities)\n        max_log_probs = np.max(log_probs, axis=1, keepdims=True)\n        log_probs -= max_log_probs\n        gamma = np.exp(log_probs)\n        gamma /= gamma.sum(axis=1, keepdims=True)\n        \n        # M-step: Update parameters\n        n_k = gamma.sum(axis=0)\n        pi = n_k / n_samples\n        \n        # Prevent division by zero by adding a small epsilon\n        n_k = np.maximum(n_k, 1e-6)\n        mu = (gamma.T @ data) / n_k[:, np.newaxis]\n        \n        for ki in range(k):\n            diff = data - mu[ki]\n            sigma[ki] = (gamma[:, ki] * diff.T) @ diff / n_k[ki]\n            sigma[ki] += 1e-6 * np.eye(d)  # Regularization\n        \n        # Check for convergence\n        delta = np.max(np.abs(pi - prev_pi))\n        if delta < epsilon:\n            break\n        prev_pi = pi.copy()\n    \n    # Final E-step to get the correct responsibilities with the last parameters\n    log_probs = np.zeros((n_samples, k))\n    for ki in range(k):\n        mu_k = mu[ki]\n        sigma_k = sigma[ki]\n        sigma_inv = np.linalg.inv(sigma_k)\n        log_det = np.log(np.linalg.det(sigma_k))\n        diff = data - mu_k\n        quadratic = np.einsum('ij,ij->i', diff @ sigma_inv, diff)\n        log_prob = np.log(pi[ki]) - 0.5 * (d * np.log(2 * np.pi) + log_det + quadratic)\n        log_probs[:, ki] = log_prob\n    max_log_probs = np.max(log_probs, axis=1, keepdims=True)\n    log_probs -= max_log_probs\n    gamma = np.exp(log_probs)\n    gamma /= gamma.sum(axis=1, keepdims=True)\n    \n    # Assign clusters\n    original_assignments = np.argmax(gamma, axis=1)\n    permutation = np.argsort(mu[:, 0])\n    inv_perm = np.zeros(k, dtype=int)\n    for new_label, original_label in enumerate(permutation):\n        inv_perm[original_label] = new_label\n    new_assignments = inv_perm[original_assignments]\n    \n    return new_assignments.tolist()\n```\n\n### Explanation\n1. **Initialization**: The means are initialized using K random data points. Covariance matrices are initialized as the dataset's covariance plus a small regularization term to ensure they are positive definite.\n2. **E-Step**: Computes the responsibilities using the current parameters, applying the log-sum-exp trick for numerical stability.\n3. **M-Step**: Updates the parameters using the responsibilities. Regularization is added to covariance matrices to avoid singularities.\n4. **Convergence Check**: Iterates until the change in mixture weights is below the tolerance or the maximum iterations are reached.\n5. **Reordering Components**: After convergence, components are sorted by the first coordinate of their means, and labels are reassigned to ensure deterministic output.\n\nThis approach ensures that the output is deterministic for identical inputs by reordering components based on their means, thus providing consistent cluster labels."}
{"task_id": 562, "completion_id": 0, "completion": "To solve this problem, we need to implement the Spectral Clustering algorithm from scratch without using any third-party machine-learning libraries. The algorithm involves constructing a similarity graph, computing the Laplacian matrix, finding eigenvectors, and performing k-means clustering in the eigenvector space. Let's break down the approach and solution step-by-step.\n\n### Approach\n\n1. **Build the Similarity Matrix (W):** \n   - Compute the pairwise squared Euclidean distances between all points using the Gaussian kernel to form a fully-connected similarity graph. The similarity between points \\(i\\) and \\(j\\) is given by \\(w_{ij} = \\exp(-\\gamma \\|x_i - x_j\\|^2)\\).\n\n2. **Construct the Laplacian Matrix (L):** \n   - For the unnormalized Laplacian, compute \\(L = D - W\\), where \\(D\\) is the diagonal degree matrix.\n   - For the normalized Laplacian, compute \\(L_{sym} = D^{-1/2} (D - W) D^{-1/2}\\).\n\n3. **Compute Eigenvectors:**\n   - Find the eigenvectors corresponding to the smallest `n_cluster` eigenvalues of the Laplacian matrix. If using the normalized Laplacian, row-normalize the eigenvector matrix.\n\n4. **K-means Clustering:**\n   - Perform k-means clustering on the eigenvectors. Initialize centroids as the first `n_cluster` samples and relabel clusters based on the smallest original indices.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\"\"\"\n    \n    if n_cluster == 1:\n        return [0] * len(data)\n    \n    # Step 1: Build the similarity matrix W using Gaussian kernel\n    x_squared = np.sum(data ** 2, axis=1)\n    x_dot = data @ data.T\n    squared_distances = x_squared[:, np.newaxis] + x_squared[np.newaxis, :] - 2 * x_dot\n    W = np.exp(-gamma * squared_distances)\n    \n    # Step 2: Construct the Laplacian matrix\n    degrees = W.sum(axis=1)\n    if method == 'unnormalized':\n        L = np.diag(degrees) - W\n    elif method == 'normalized':\n        D_inv_sqrt = 1.0 / np.sqrt(degrees)\n        L = np.diag(degrees) - W\n        L = D_inv_sqrt[:, np.newaxis] * L * D_inv_sqrt[np.newaxis, :]\n    else:\n        raise ValueError(\"Method must be 'unnormalized' or 'normalized'\")\n    \n    # Step 3: Compute eigenvectors corresponding to the n_cluster smallest eigenvalues\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    k_eigenvectors = eigenvectors[:, :n_cluster]\n    \n    if method == 'normalized':\n        row_norms = np.linalg.norm(k_eigenvectors, axis=1, keepdims=True)\n        row_norms[row_norms == 0] = 1  # Avoid division by zero\n        k_eigenvectors = k_eigenvectors / row_norms\n    \n    # Step 4: Run deterministic k-means and relabel clusters\n    def run_kmeans(X, k):\n        centroids = X[:k].copy()\n        while True:\n            distances = np.linalg.norm(X[:, np.newaxis, :] - centroids, axis=2)\n            labels = np.argmin(distances, axis=1)\n            new_centroids = np.zeros_like(centroids)\n            for i in range(k):\n                mask = (labels == i)\n                if mask.any():\n                    new_centroids[i] = X[mask].mean(axis=0)\n                else:\n                    new_centroids[i] = centroids[i]\n            if np.allclose(new_centroids, centroids):\n                break\n            centroids = new_centroids.copy()\n        \n        # Relabel clusters based on the smallest original index in each cluster\n        cluster_indices = {}\n        for idx, label in enumerate(labels):\n            cluster_indices.setdefault(label, []).append(idx)\n        min_indices = {label: min(indices) for label, indices in cluster_indices.items()}\n        sorted_labels = sorted(min_indices.keys(), key=lambda x: min_indices[x])\n        label_map = {old: new for new, old in enumerate(sorted_labels)}\n        return [label_map[label] for label in labels]\n    \n    labels = run_kmeans(k_eigenvectors, n_cluster)\n    return labels\n```\n\n### Explanation\n\n1. **Similarity Matrix Construction:** The Gaussian kernel is used to compute the similarity matrix \\(W\\) based on pairwise squared distances between data points. This matrix captures the similarity between each pair of points.\n\n2. **Laplacian Matrix Construction:** Depending on the method chosen (normalized or unnormalized), the Laplacian matrix is computed. This matrix helps in capturing the structure of the data graph.\n\n3. **Eigenvector Computation:** The eigenvectors corresponding to the smallest eigenvalues of the Laplacian matrix are computed. These eigenvectors form a low-dimensional embedding of the data.\n\n4. **K-means Clustering:** The k-means algorithm is applied on the eigenvector embeddings. The initial centroids are deterministically chosen as the first `n_cluster` points. After convergence, clusters are relabeled based on the smallest original indices to ensure deterministic output labels.\n\nThis approach efficiently combines spectral techniques with k-means clustering to partition the data into meaningful clusters."}
{"task_id": 563, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\"\"\"\n    N = X_data.shape[0]\n    alpha = np.zeros(N)\n    b = 0.0\n    G = X_data @ X_data.T  # Gram matrix\n    \n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            g = np.dot(alpha * y_data, G[:, i])\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n        if not updated:\n            break\n    \n    w = np.dot(alpha * y_data, X_data)\n    w_rounded = np.round(w, 4)\n    b_rounded = np.round(b, 4)\n    \n    return (w_rounded.tolist(), float(b_rounded))\n```"}
{"task_id": 564, "completion_id": 0, "completion": "To solve this problem, we need to traverse a decision tree to predict the outcome for a given sample based on the tree's structure. The traversal rules differ depending on whether the tree is used for classification or regression. \n\n### Approach\n1. **Classification Tree**:\n   - If the current node is a leaf (contains a `y` value), return `y`.\n   - Otherwise, inspect the feature value at the index specified by the node's `label`.\n   - Traverse to the first child node whose `x` value matches the feature value. If no match is found, default to the second child in the list.\n\n2. **Regression Tree**:\n   - If the current node is a leaf (contains a `y` value), return `y`.\n   - Otherwise, inspect the feature value at the index specified by the node's `label`.\n   - Compare the feature value with the node's split-point `s`. If the value is less than or equal to `s`, traverse to the first child; otherwise, traverse to the second child.\n\n### Solution Code\n```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    current = root\n    while True:\n        if hasattr(current, 'y'):\n            return current.y\n        if task_type == 'classification':\n            feature_val = features[current.label]\n            found_child = None\n            for child in current.child:\n                if child.x == feature_val:\n                    found_child = child\n                    break\n            current = current.child[1] if found_child is None else found_child\n        else:\n            feature_val = features[current.label]\n            if feature_val <= current.s:\n                current = current.child[0]\n            else:\n                current = current.child[1]\n```\n\n### Explanation\n1. **Initialization**: Start at the root node of the tree.\n2. **Leaf Check**: At each node, check if it is a leaf node (contains a `y` value). If so, return the `y` value immediately.\n3. **Classification Handling**:\n   - Extract the feature value using the node's `label`.\n   - Search through the node's children to find the first child whose `x` value matches the feature value. If no match is found, default to the second child.\n4. **Regression Handling**:\n   - Extract the feature value using the node's `label`.\n   - Compare the feature value with the node's split-point `s` to determine which child to traverse to (first child if value \u2264 `s`, otherwise the second child).\n\nThis approach ensures that the tree is traversed efficiently according to the specified rules, leading to the correct leaf node where the prediction value is stored."}
{"task_id": 565, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that selects the feature from a dataset that maximizes the information gain using the ID3 algorithm. The information gain is calculated using the entropy of the labels and the conditional entropy after splitting the data by each feature's values.\n\n### Approach\n1. **Entropy Calculation**: Compute the entropy of the labels (H(y)) using the formula \\( H(y) = -\\sum p \\log_2 p \\), where \\( p \\) is the probability of each class in the labels.\n2. **Conditional Entropy Calculation**: For each feature, split the dataset based on the feature's unique values. For each subset of data corresponding to a feature value, compute the entropy of the labels and then calculate the weighted average entropy (H(y|A)) for that feature.\n3. **Information Gain Calculation**: Subtract the conditional entropy (H(y|A)) from the entropy of the labels (H(y)) to get the information gain (IG) for each feature.\n4. **Feature Selection**: Select the feature with the highest information gain. In case of ties, choose the feature with the smallest index.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\"\"\"\n    n_samples, n_features = X.shape\n    if n_samples == 0:\n        return (0, 0.0)\n    \n    # Compute H(y)\n    counts_y = np.bincount(y)\n    probs_y = counts_y[counts_y > 0] / n_samples\n    H_y = -np.sum(probs_y * np.log2(probs_y))\n    \n    max_ig = -float('inf')\n    best_feature = 0\n    \n    for j in range(n_features):\n        feature_col = X[:, j]\n        values, value_counts = np.unique(feature_col, return_counts=True)\n        cond_entropy = 0.0\n        \n        for v, cnt in zip(values, value_counts):\n            mask = (feature_col == v)\n            y_subset = y[mask]\n            counts_sub = np.bincount(y_subset)\n            probs_sub = counts_sub[counts_sub > 0] / cnt\n            entropy_sub = -np.sum(probs_sub * np.log2(probs_sub))\n            cond_entropy += (cnt / n_samples) * entropy_sub\n        \n        ig = H_y - cond_entropy\n        # Update if current IG is higher, or same IG but smaller index\n        if (ig > max_ig) or (ig == max_ig and j < best_feature):\n            max_ig = ig\n            best_feature = j\n    \n    max_ig_rounded = round(max_ig, 6)\n    return (best_feature, max_ig_rounded)\n```\n\n### Explanation\n1. **Entropy Calculation (H(y))**: The entropy of the labels is calculated by first counting the occurrences of each label using `np.bincount`, converting these counts to probabilities, and then computing the entropy using these probabilities.\n2. **Conditional Entropy (H(y|A))**: For each feature, the dataset is split into subsets based on the feature's unique values. For each subset, the entropy of the labels is computed and then averaged by the proportion of samples in each subset relative to the entire dataset.\n3. **Information Gain (IG)**: The information gain for each feature is computed by subtracting the conditional entropy from the label entropy. The feature with the highest information gain is selected, with ties broken by choosing the smallest feature index.\n\nThis approach ensures that we efficiently compute the information gain for each feature and select the optimal one based on the ID3 algorithm's criteria."}
{"task_id": 566, "completion_id": 0, "completion": "To solve this problem, we need to implement a simplified version of the Isolation Forest algorithm for anomaly detection. The goal is to identify outliers in a given dataset using a set of randomly constructed isolation trees. Each tree is built by recursively splitting the data using random features and thresholds until a certain depth or sample size limit is reached. The anomaly score for each data point is computed based on the average path length across all trees, and the top outliers are identified based on these scores.\n\n### Approach\n1. **Subsampling**: For each tree, draw a subsample of the data. If the sample size is larger than the dataset, use the entire dataset.\n2. **Tree Construction**: Build each tree by recursively splitting the subsample using a random feature and threshold until a depth limit or node size limit is reached.\n3. **Path Length Calculation**: For each data point, compute the path length in each tree, which is the number of edges traversed plus a correction factor based on the number of samples in the leaf node.\n4. **Anomaly Score Calculation**: Compute the average path length for each data point and convert it to an anomaly score using a predefined formula.\n5. **Outlier Detection**: Identify outliers as the top fraction of data points with the highest anomaly scores.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    np.random.seed(42)\n    \n    if epsilon == 0:\n        return []\n    \n    n_samples, n_features = data.shape\n    s = min(sample_size, n_samples)\n    \n    # Handle cases where s is 0 or 1 to avoid invalid phi calculation\n    if s <= 1:\n        phi = 1.0\n    else:\n        phi = 2 * math.log(s - 1) - 2 * (s - 1) / s\n    \n    trees = []\n    \n    for _ in range(n_trees):\n        if sample_size >= n_samples:\n            subsample = data\n        else:\n            indices = np.random.choice(n_samples, size=sample_size, replace=False)\n            subsample = data[indices]\n        \n        # Determine the height limit for this tree\n        if s == 0:\n            height_limit = 0\n        else:\n            height_limit = math.ceil(math.log2(s)) if s > 0 else 0\n        \n        # Recursive function to build the tree\n        def build_tree(node_data, current_depth):\n            if current_depth >= height_limit or len(node_data) <= 1:\n                return {'type': 'leaf', 'n': len(node_data)}\n            else:\n                feature_idx = np.random.randint(n_features)\n                min_val = np.min(node_data[:, feature_idx])\n                max_val = np.max(node_data[:, feature_idx])\n                threshold = np.random.uniform(min_val, max_val)\n                left_mask = node_data[:, feature_idx] <= threshold\n                left_data = node_data[left_mask]\n                right_data = node_data[~left_mask]\n                left_child = build_tree(left_data, current_depth + 1)\n                right_child = build_tree(right_data, current_depth + 1)\n                return {\n                    'type': 'split',\n                    'feature': feature_idx,\n                    'threshold': threshold,\n                    'left': left_child,\n                    'right': right_child\n                }\n        \n        tree = build_tree(subsample, 0)\n        trees.append(tree)\n    \n    path_lengths = np.zeros((n_samples, n_trees))\n    \n    for tree_idx, tree in enumerate(trees):\n        for data_idx in range(n_samples):\n            x = data[data_idx]\n            current_node = tree\n            depth = 0\n            while True:\n                if current_node['type'] == 'leaf':\n                    n = current_node['n']\n                    if n <= 1:\n                        c = 0.0\n                    else:\n                        c = 2 * math.log(n - 1) + 0.5772156649 - 2 * (n - 1) / n\n                    path_lengths[data_idx, tree_idx] = depth + c\n                    break\n                else:\n                    feat = current_node['feature']\n                    thresh = current_node['threshold']\n                    if x[feat] <= thresh:\n                        current_node = current_node['left']\n                    else:\n                        current_node = current_node['right']\n                    depth += 1\n    \n    avg_h = np.mean(path_lengths, axis=1)\n    scores = 2 ** (-avg_h / phi)\n    \n    if epsilon >= 1:\n        selected_indices = np.arange(n_samples)\n    else:\n        m = math.ceil(epsilon * n_samples)\n        sorted_indices = np.argsort(-scores)  # Descending order of scores\n        selected_indices = sorted_indices[:m]\n    \n    selected_indices.sort()\n    return selected_indices.tolist()\n```\n\n### Explanation\n1. **Subsampling**: For each tree, a subsample of the data is drawn. If the sample size is larger than the dataset, the entire dataset is used.\n2. **Tree Construction**: Each tree is built recursively by selecting a random feature and threshold, splitting the data until a depth limit or node size limit is reached.\n3. **Path Length Calculation**: The path length for each data point is calculated by traversing the tree and adding a correction factor based on the number of samples in the leaf node.\n4. **Anomaly Score**: The average path length across all trees is converted to an anomaly score using a predefined formula. Higher scores indicate higher likelihood of being an outlier.\n5. **Outlier Detection**: The top fraction of data points with the highest scores are identified as outliers, and their indices are returned in ascending order."}
{"task_id": 567, "completion_id": 0, "completion": "To solve this problem, we need to determine the final class label for each sample using majority voting from an ensemble of decision trees. The final label is determined by the most frequent prediction (mode) among the trees. In case of a tie, the smallest label is chosen.\n\n### Approach\n1. **Convert Input to NumPy Array**: Convert the input list of lists into a NumPy array for efficient manipulation.\n2. **Transpose the Array**: Transpose the array so that each row represents the predictions for a single sample across all trees.\n3. **Process Each Sample**: For each sample (row in the transposed array):\n   - Convert all predictions to integers to handle any floating-point representations of class labels.\n   - Use NumPy's `unique` function to count the occurrences of each class label.\n   - Identify the class label(s) with the highest count (mode).\n   - In case of a tie, select the smallest class label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef majority_vote(predictions: list[list[int | float]]) -> list[int]:\n    \"\"\"Ensemble majority voting.\"\"\"\n    arr = np.array(predictions)\n    arr = arr.astype(int)  # Convert all elements to integers to handle float labels like 1.0\n    samples = arr.T  # Transpose to get each sample's votes as rows\n    result = []\n    for sample in samples:\n        values, counts = np.unique(sample, return_counts=True)\n        max_count = counts.max()\n        candidates = values[counts == max_count]\n        result.append(np.min(candidates))\n    return result\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input list of lists is converted to a NumPy array to facilitate efficient transposition and manipulation.\n2. **Transposition**: The array is transposed so that each row now corresponds to a sample's predictions from all trees.\n3. **Integer Conversion**: All elements are converted to integers to handle cases where class labels might be represented as floats (e.g., 1.0 instead of 1).\n4. **Counting Occurrences**: Using `np.unique`, we get the unique class labels and their counts for each sample.\n5. **Determine Mode**: The maximum count of predictions is identified, and all labels with this count are considered candidates.\n6. **Resolve Ties**: If there are multiple candidates (a tie), the smallest label is selected using `np.min`.\n\nThis approach efficiently handles the majority voting process using NumPy's optimized functions, ensuring both correctness and performance even for large datasets."}
{"task_id": 568, "completion_id": 0, "completion": "To solve this problem, we need to implement a Maximum Entropy (MaxEnt) classifier using the Generalized Iterative Scaling (GIS) algorithm. This classifier will learn the weights of features derived from the training data and use these weights to predict labels for test samples.\n\n### Approach\n1. **Feature Collection**: Identify all unique (feature index, feature value, label) triplets from the training data. These triplets form the basis of the features used in the model.\n2. **Empirical Distribution**: Compute the empirical expectation for each feature, which is the average occurrence of each feature in the training data.\n3. **Precompute Indices**: For efficiency, precompute the indices of training samples where each (feature index, feature value) pair occurs.\n4. **Compute C**: Determine the maximum number of active features for any (sample, label) pair in the training data. This helps in stabilizing the GIS algorithm.\n5. **GIS Iterations**: Iteratively update the weights of the features using the GIS algorithm until convergence or the maximum number of iterations is reached. Each iteration involves computing the model's expected feature values, adjusting the weights based on the difference between empirical and model expectations, and checking for convergence.\n6. **Prediction**: For each test sample, compute the scores for each possible label using the learned weights and predict the label with the highest score.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"Implements a Maximum Entropy classifier using Generalized Iterative Scaling.\"\"\"\n    # Collect all (i, v, l) features and their counts\n    feature_counts = defaultdict(int)\n    N, num_features = X_train.shape\n    for x, y in zip(X_train, y_train):\n        for i in range(num_features):\n            v = x[i]\n            feature_counts[(i, v, y)] += 1\n    \n    # Create a list of unique features and map each to its index\n    features = list(feature_counts.keys())\n    num_feature_pairs = len(features)\n    if num_feature_pairs == 0:\n        # Edge case: no features, return arbitrary labels\n        return [0] * X_test.shape[0]\n    feature_to_j = {key: j for j, key in enumerate(features)}\n    \n    # Compute empirical expectations\n    emp_exp = np.array([feature_counts[key] / N for key in features])\n    \n    # Precompute indices for each (i, v) pair\n    iv_to_x_indices = defaultdict(list)\n    for x_idx in range(N):\n        x = X_train[x_idx]\n        for i in range(num_features):\n            v = x[i]\n            iv_to_x_indices[(i, v)].append(x_idx)\n    \n    # Compute the maximum number of active features C\n    unique_labels = np.unique(y_train)\n    C = 0\n    for x in X_train:\n        for y in unique_labels:\n            current_sum = 0\n            for i in range(num_features):\n                v = x[i]\n                if (i, v, y) in feature_to_j:\n                    current_sum += 1\n            if current_sum > C:\n                C = current_sum\n    if C == 0:\n        C = 1  # Avoid division by zero\n    \n    # Initialize weights\n    weights = np.zeros(num_feature_pairs)\n    \n    # GIS iterations\n    K = len(unique_labels)\n    unique_labels_list = unique_labels.tolist()\n    for _ in range(n_iter):\n        # Compute probabilities P(y | x) for all x in training data\n        prob = np.zeros((N, K))\n        for x_idx in range(N):\n            x = X_train[x_idx]\n            scores = np.zeros(K)\n            for yk_idx, yk in enumerate(unique_labels):\n                score = 0.0\n                for i in range(num_features):\n                    v = x[i]\n                    key = (i, v, yk)\n                    if key in feature_to_j:\n                        j = feature_to_j[key]\n                        score += weights[j]\n                scores[yk_idx] = score\n            # Compute softmax with numerical stability\n            max_score = np.max(scores)\n            exp_scores = np.exp(scores - max_score)\n            Z = np.sum(exp_scores)\n            if Z == 0:\n                prob[x_idx] = np.ones(K) / K\n            else:\n                prob[x_idx] = exp_scores / Z\n        \n        # Compute model expectations\n        model_exp = np.zeros(num_feature_pairs)\n        for j in range(num_feature_pairs):\n            i, v, l = features[j]\n            # Find the index of l in unique_labels\n            l_idx = unique_labels_list.index(l)\n            x_indices = iv_to_x_indices.get((i, v), [])\n            if x_indices:\n                model_exp[j] = np.sum(prob[x_indices, l_idx]) / N\n        \n        # Compute delta and update weights\n        # Add small epsilon to avoid division by zero or log(0)\n        delta = (1.0 / C) * np.log(emp_exp / (model_exp + 1e-10))\n        weights += delta\n        \n        # Check convergence\n        max_delta = np.max(np.abs(delta))\n        if max_delta < epsilon:\n            break\n    \n    # Predict labels for test data\n    predictions = []\n    for x in X_test:\n        scores = np.zeros(K)\n        for yk_idx, yk in enumerate(unique_labels):\n            score = 0.0\n            for i in range(num_features):\n                v = x[i]\n                key = (i, v, yk)\n                if key in feature_to_j:\n                    j = feature_to_j[key]\n                    score += weights[j]\n            scores[yk_idx] = score\n        pred_idx = np.argmax(scores)\n        predictions.append(unique_labels[pred_idx])\n    \n    return predictions\n```\n\n### Explanation\n1. **Feature Collection**: The code collects all unique (feature index, feature value, label) triplets from the training data and counts their occurrences.\n2. **Empirical Distribution**: The empirical expectation for each feature is calculated by dividing the count of each feature by the number of training samples.\n3. **Precompute Indices**: For each (feature index, feature value) pair, the code precomputes the indices of training samples where this pair occurs to speed up subsequent calculations.\n4. **Compute C**: The maximum number of active features for any (sample, label) pair is computed to stabilize the GIS algorithm.\n5. **GIS Iterations**: The code iteratively updates the feature weights by comparing the model's expected feature values to the empirical expectations. The weights are adjusted using the GIS update rule until convergence or the maximum number of iterations is reached.\n6. **Prediction**: For each test sample, the code computes the score for each possible label using the learned weights and selects the label with the highest score as the prediction. This is done using the softmax function to ensure numerical stability."}
{"task_id": 569, "completion_id": 0, "completion": "To solve this problem, we need to analyze a collection of text documents to determine the document-frequency ratio of each word and generate a set of the most frequent words based on a specified cut-off. The document-frequency ratio is the fraction of documents in which each word appears, and the results should be ordered by the most common to the least common words.\n\n### Approach\n1. **Count Document Frequencies**: For each document, convert its words into a set to ensure each word is counted once per document. Use a counter to track how many documents each word appears in.\n2. **Calculate Ratios**: For each word, compute its document-frequency ratio by dividing the count of documents it appears in by the total number of documents. Round this ratio to four decimal places.\n3. **Sort Words by Frequency**: Sort the words based on their document-frequency ratios in descending order. If two words have the same ratio, sort them lexicographically to ensure a deterministic order.\n4. **Generate Result Set**: Depending on the cut-off value, generate a set containing the top-k most frequent words. If the cut-off is `None`, include all words; if it is zero, return an empty set.\n\n### Solution Code\n```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\"\"\"\n\n    doc_counts = Counter()\n    total_docs = len(data)\n    for doc in data:\n        _, words = doc\n        unique_words = set(words)\n        for word in unique_words:\n            doc_counts[word] += 1\n\n    ratios = []\n    for word, count in doc_counts.items():\n        ratio = count / total_docs if total_docs != 0 else 0.0\n        ratio_rounded = round(ratio, 4)\n        ratios.append((word, ratio_rounded))\n\n    # Sort by descending ratio, then ascending word to ensure deterministic order\n    sorted_ratios = sorted(ratios, key=lambda x: (-x[1], x[0]))\n\n    if cut_off is None:\n        words_set = {word for word, _ in sorted_ratios}\n    else:\n        if cut_off <= 0:\n            words_set = set()\n        else:\n            k = min(cut_off, len(sorted_ratios))\n            words_set = {word for word, _ in sorted_ratios[:k]}\n\n    return (sorted_ratios, words_set)\n```\n\n### Explanation\n1. **Document Frequency Counting**: We iterate through each document, converting the list of words into a set to avoid counting duplicates within the same document. This helps in accurately determining how many documents each word appears in.\n2. **Ratio Calculation**: For each word, we calculate the ratio by dividing the count of documents it appears in by the total number of documents. This ratio is then rounded to four decimal places.\n3. **Sorting**: Words are sorted primarily by their document-frequency ratio in descending order. If two words have the same ratio, they are sorted lexicographically to maintain a consistent order.\n4. **Result Set Generation**: Based on the cut-off value, we generate a set of the top-k words. If no cut-off is specified, all words are included. If the cut-off is zero, an empty set is returned. If the cut-off exceeds the number of distinct words, all words are included.\n\nThis approach ensures that we efficiently compute the required statistics and handle edge cases such as empty input data and varying cut-off values."}
{"task_id": 570, "completion_id": 0, "completion": "To solve this problem, we need to implement a Bernoulli Naive Bayes classifier that predicts the most likely class labels for test documents based on a training set and a given set of keywords. The classifier uses binary features indicating the presence or absence of keywords in the documents.\n\n### Approach\n1. **Training Phase**:\n   - **Count Occurrences**: For each class, count how many times each keyword appears in the documents of that class.\n   - **Calculate Probabilities**: For each keyword and class, compute the probability that a document in that class contains the keyword. This is done by dividing the count of documents containing the keyword by the total number of documents in the class.\n\n2. **Prediction Phase**:\n   - **Compute Likelihoods**: For each test document, compute the likelihood for each class using the Bernoulli model. The likelihood is the product of the probabilities of the keywords present in the document and the complement of probabilities for keywords not present.\n   - **Select Maximum Likelihood**: For each test document, select the class with the highest likelihood.\n\n### Solution Code\n```python\nfrom collections import defaultdict\n\ndef naive_bayes_predict(train_data: list[tuple[str, list[str]]],\n                        keywords: set[str],\n                        test_samples: list[list[str]]) -> list[str]:\n    \"\"\"Build an unsmoothed Bernoulli Naive Bayes text classifier and predict labels.\"\"\"\n\n    # Training phase: calculate counts for each class and keyword\n    class_counts = defaultdict(lambda: defaultdict(int))  # class_counts[label][word] = count\n    n_c = defaultdict(int)  # n_c[label] = number of documents in class\n\n    for label, tokens in train_data:\n        n_c[label] += 1\n        tokens_set = set(tokens)\n        for word in keywords:\n            if word in tokens_set:\n                class_counts[label][word] += 1\n\n    # Prediction phase\n    predictions = []\n    for sample in test_samples:\n        sample_set = set(sample)\n        max_likelihood = -1.0\n        best_label = None\n\n        for label in n_c:\n            likelihood = 1.0\n            for word in keywords:\n                # Calculate P(word|label)\n                count = class_counts[label].get(word, 0)\n                total_docs = n_c[label]\n                p = count / total_docs if total_docs != 0 else 0.0\n\n                # Update likelihood based on presence/absence of the word\n                if word in sample_set:\n                    likelihood *= p\n                else:\n                    likelihood *= (1 - p)\n\n            # Check if current label has higher likelihood\n            if likelihood > max_likelihood or best_label is None:\n                max_likelihood = likelihood\n                best_label = label\n\n        predictions.append(best_label)\n\n    return predictions\n```\n\n### Explanation\n1. **Training Phase**:\n   - **Count Occurrences**: We iterate over each document in the training data, converting the document's tokens into a set for efficient lookup. For each keyword, we check if it is present in the document and update the count for the corresponding class.\n   - **Calculate Probabilities**: The probability of a keyword given a class is calculated by dividing the count of documents containing the keyword by the total number of documents in the class.\n\n2. **Prediction Phase**:\n   - **Compute Likelihoods**: For each test document, we convert its tokens into a set. For each class, we compute the likelihood by iterating over all keywords. If a keyword is present in the document, we use its probability; if absent, we use the complement of its probability.\n   - **Select Maximum Likelihood**: The class with the highest computed likelihood is selected as the predicted label for the test document. This process is repeated for all test documents to generate the final list of predictions."}
{"task_id": 571, "completion_id": 0, "completion": "To solve this problem, we need to compute the weight vector and bias term of a linear Support Vector Machine (SVM) using the optimized Lagrange multipliers obtained from the Sequential Minimal Optimization (SMO) algorithm. The solution involves applying mathematical formulas to derive these parameters efficiently using matrix operations.\n\n### Approach\n1. **Compute the Weight Vector (w):**\n   - The weight vector is calculated using the formula \\( \\mathbf{w} = X^\\top (\\boldsymbol{\\alpha} \\odot \\mathbf{y}) \\), where \\( X \\) is the training matrix, \\( \\boldsymbol{\\alpha} \\) is the vector of Lagrange multipliers, and \\( \\mathbf{y} \\) is the label vector. This is efficiently computed using matrix multiplication.\n   \n2. **Compute the Bias Term (b):**\n   - The bias term is derived as the average of the residuals \\( (y_i - \\mathbf{w}^\\top \\mathbf{x}_i) \\) for all samples. This is done by first computing the dot product of the weight vector with each sample, subtracting these values from the corresponding labels, and then taking the mean of the resulting residuals.\n\n3. **Rounding:**\n   - Both the weight vector and the bias term are rounded to four decimal places to meet the specified precision requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    # Compute the weight vector using the formula w = X^T (alpha * y)\n    w = X.T @ (alpha * y)\n    # Round each element of w to four decimal places and convert to a list\n    w_rounded = np.round(w, 4)\n    w_list = w_rounded.tolist()\n    \n    # Compute the bias term using the average of (y_i - w^T x_i)\n    residuals = y - X @ w\n    b = np.round(residuals.mean(), 4)\n    \n    return (w_list, float(b))\n```\n\n### Explanation\n- **Weight Vector Calculation:** The weight vector is obtained by transposing the training matrix \\( X \\) and multiplying it with the element-wise product of the Lagrange multipliers \\( \\boldsymbol{\\alpha} \\) and the labels \\( \\mathbf{y} \\). This operation efficiently sums the contributions of each sample scaled by their respective Lagrange multipliers and labels.\n  \n- **Bias Term Calculation:** The bias term is computed by first determining the residuals (differences between the actual labels and the predicted values using the weight vector). The mean of these residuals gives the bias term, which is then rounded to four decimal places.\n\nThis approach ensures that the computations are both efficient and numerically precise, leveraging NumPy's optimized matrix operations for performance and accuracy. The results are formatted to meet the specified precision requirements and returned in the required format."}
{"task_id": 572, "completion_id": 0, "completion": "To solve this problem, we need to implement the Local Outlier Factor (LOF) algorithm from scratch to detect anomalous samples in a dataset. The LOF algorithm identifies outliers based on the local density of data points compared to their neighbors. \n\n### Approach\nThe LOF algorithm involves several key steps:\n1. **Compute Pairwise Euclidean Distances**: Calculate the Euclidean distance between every pair of points in the dataset.\n2. **Determine k-Distance and Neighbors**: For each point, find the distance to its k-th nearest neighbor (k-distance) and the indices of those k nearest neighbors.\n3. **Reachability Distance**: For each point, compute the reachability distance to its neighbors, which is the maximum of the k-distance of the neighbor and the actual distance.\n4. **Local Reachability Density (LRD)**: Calculate the LRD for each point, which is the inverse of the average reachability distance of its k neighbors.\n5. **Local Outlier Factor (LOF)**: Compute the LOF for each point as the ratio of the average LRD of its neighbors to its own LRD.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    n = data.shape[0]\n    # Compute pairwise Euclidean distance matrix\n    distance_matrix = np.sqrt(np.sum((data[:, np.newaxis] - data) ** 2, axis=2))\n    \n    # Calculate k-distance and neighbors for each point\n    k_distances = []\n    neighbors = []\n    for p in range(n):\n        row = distance_matrix[p]\n        sorted_indices = np.argsort(row)\n        sorted_distances = row[sorted_indices]\n        k_distance = sorted_distances[k]\n        k_neighbors = sorted_indices[1:k+1].tolist()  # Exclude the point itself\n        k_distances.append(k_distance)\n        neighbors.append(k_neighbors)\n    \n    # Compute Local Reachability Density (LRD)\n    LRD = np.zeros(n)\n    for p in range(n):\n        sum_reach = 0.0\n        for q in neighbors[p]:\n            reach_dist = max(k_distances[q], distance_matrix[p, q])\n            sum_reach += reach_dist\n        LRD[p] = k / sum_reach if sum_reach != 0 else 0.0\n    \n    # Compute Local Outlier Factor (LOF)\n    LOF = np.zeros(n)\n    for p in range(n):\n        sum_LRD = 0.0\n        for q in neighbors[p]:\n            sum_LRD += LRD[q]\n        avg_LRD = sum_LRD / k\n        LOF[p] = avg_LRD / LRD[p] if LRD[p] != 0 else 0.0\n    \n    # Round LOF scores to 4 decimal places\n    LOF_rounded = np.round(LOF, 4)\n    \n    # Get indices of outliers where LOF > epsilon, sorted ascending\n    outlier_indices = np.where(LOF_rounded > epsilon)[0].tolist()\n    outlier_indices.sort()\n    \n    return (LOF_rounded.tolist(), outlier_indices)\n```\n\n### Explanation\n1. **Pairwise Euclidean Distance Matrix**: This matrix is computed using broadcasting to efficiently calculate the Euclidean distances between all pairs of points.\n2. **k-Distance and Neighbors**: For each point, the distances are sorted, and the k-th smallest distance (excluding the point itself) is determined. The indices of the k nearest neighbors are also stored.\n3. **Reachability Distance**: For each neighbor of a point, the reachability distance is computed as the maximum of the neighbor's k-distance and the actual distance to the neighbor.\n4. **Local Reachability Density (LRD)**: The LRD is the inverse of the average reachability distance of the k neighbors, providing a measure of how dense the neighborhood around a point is.\n5. **Local Outlier Factor (LOF)**: The LOF is the ratio of the average LRD of the neighbors to the LRD of the point itself, highlighting points with significantly lower density than their neighbors.\n\nThis approach ensures that we identify outliers based on local density deviations, providing a robust method for anomaly detection in various datasets."}
{"task_id": 573, "completion_id": 0, "completion": "To solve this problem, we need to implement the Perceptron Learning Algorithm for binary classification. The goal is to train a linear classifier using the Rosenblatt Perceptron learning rule, which adjusts the weights and bias iteratively based on classification errors. The algorithm stops early if the data becomes linearly separable within the specified number of epochs, otherwise, it completes the maximum number of epochs.\n\n### Approach\n1. **Initialization**: Start with zero-initialized weight vector and bias.\n2. **Epoch Iteration**: For each epoch, iterate through each training example and update the weights and bias whenever a misclassification occurs.\n3. **Error Counting**: Track the number of misclassifications in each epoch. If an epoch results in zero errors, the algorithm stops early.\n4. **Convergence Check**: If the data is linearly separable, the algorithm will converge early; otherwise, it will run for the maximum number of epochs.\n5. **Rounding**: After training, round the weights and bias to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_train(X_train: np.ndarray,\n                     y_train: np.ndarray,\n                     eta: float = 0.1,\n                     n_iter: int = 1000) -> tuple[list[float], float, list[int]]:\n    \"\"\"Implement the Perceptron learning algorithm.\"\"\"\n    m, n = X_train.shape\n    w = np.zeros(n, dtype=np.float64)\n    b = 0.0\n    error_history = []\n    \n    for _ in range(n_iter):\n        error_count = 0\n        for i in range(m):\n            x_i = X_train[i]\n            y_i = y_train[i]\n            prediction = np.dot(w, x_i) + b\n            if y_i * prediction <= 0:\n                w += eta * y_i * x_i\n                b += eta * y_i\n                error_count += 1\n        error_history.append(error_count)\n        if error_count == 0:\n            break\n    \n    # Rounding the weights and bias to 4 decimal places\n    w_rounded = [round(num, 4) for num in w]\n    b_rounded = round(b, 4)\n    \n    return (w_rounded, b_rounded, error_history)\n```\n\n### Explanation\n1. **Initialization**: The weight vector `w` is initialized as a zero vector with the same number of features as the input data. The bias `b` is initialized to zero.\n2. **Epoch Loop**: The algorithm runs for a maximum of `n_iter` epochs. In each epoch, it processes each training example.\n3. **Prediction and Update**: For each training example, the algorithm checks if the example is misclassified. If it is, the weights and bias are updated using the learning rate `eta`, and the error count is incremented.\n4. **Early Stopping**: After processing all examples in an epoch, if no errors are encountered, the algorithm stops early.\n5. **Rounding**: After training, the weights and bias are rounded to four decimal places to ensure the required precision in the output.\n\nThis approach efficiently handles both linearly separable and non-separable data, providing the learned parameters and error history as required."}
{"task_id": 574, "completion_id": 0, "completion": "To solve this problem, we need to generate a synthetic 2-D dataset for multiple classes, with separate training and validation sets. Each class's data points are uniformly distributed within a specific square region. The solution must ensure reproducibility when a seed is provided and return the data in a structured format with coordinates rounded to four decimal places.\n\n### Approach\n1. **Seed Initialization**: If a seed is provided, initialize the random number generator with this seed to ensure reproducibility.\n2. **Class Loop**: For each class, determine the base coordinate for the square region based on the class index.\n3. **Point Generation**: For each class, generate the required number of training and validation points. Each point's coordinates are sampled uniformly from the class's square region.\n4. **Data Collection**: Collect all generated points and their corresponding labels into lists.\n5. **Conversion and Rounding**: Convert the collected lists into NumPy arrays and round the coordinate values to four decimal places for compactness and determinism.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    X_train = []\n    X_val = []\n    y_train = []\n    y_val = []\n    \n    for c in range(m):\n        base = 10 * (c // 2)\n        low = base + 1\n        high = base + 9\n        \n        # Generate training data for class c\n        for _ in range(n_train):\n            x = random.uniform(low, high)\n            y = random.uniform(low, high)\n            X_train.append([x, y])\n            y_train.append(c)\n        \n        # Generate validation data for class c\n        for _ in range(n_val):\n            x = random.uniform(low, high)\n            y = random.uniform(low, high)\n            X_val.append([x, y])\n            y_val.append(c)\n    \n    # Convert to numpy arrays and round coordinates\n    X_train = np.array(X_train).round(4)\n    X_val = np.array(X_val).round(4)\n    y_train = np.array(y_train)\n    y_val = np.array(y_val)\n    \n    return (X_train, X_val, y_train, y_val)\n```\n\n### Explanation\n1. **Seed Initialization**: The function checks if a seed is provided and initializes the random number generator with it, ensuring that subsequent runs with the same seed produce identical results.\n2. **Class Loop**: For each class index `c`, the base coordinate for the square region is calculated using integer division. This base determines the lower bound of the square.\n3. **Point Generation**: Training and validation points are generated using the `random.uniform` function to sample coordinates within the square region defined for the class. Each point's coordinates are added to the respective lists along with the class label.\n4. **Data Conversion**: The collected points and labels are converted to NumPy arrays. The coordinates are rounded to four decimal places to meet the problem's requirements for compactness and determinism.\n5. **Return Values**: The function returns a tuple of NumPy arrays containing the training and validation data points and their corresponding labels.\n\nThis approach ensures that the generated data is reproducible, structured, and meets the specified constraints for each class's distribution and precision."}
{"task_id": 577, "completion_id": 0, "completion": "To solve this problem, we need to generate a synthetic 2-D dataset for a classification task. The dataset will be split into training and validation sets with specific properties for each class. The solution involves generating data points according to uniform distributions and ensuring the correct allocation of samples to training and validation sets.\n\n### Approach\n1. **Setting the Seed**: If a seed is provided, the Python random number generator is initialized with this seed to ensure reproducibility.\n2. **Class-specific Distributions**: For each class, the x and y coordinates are sampled from uniform distributions defined based on the class index.\n3. **Data Generation**: For each class, generate the required number of training and validation samples. Each sample's coordinates are generated using the class-specific distributions and rounded to four decimal places.\n4. **Splitting Data**: The generated samples for each class are split into training and validation sets based on the specified counts.\n5. **Label Assignment**: Labels are assigned to each sample based on the class index using the formula provided.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int, n_train: int, n_val: int, seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a tiny 2-D synthetic data set for classification.\"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    X_train = []\n    X_val = []\n    y_train = []\n    y_val = []\n    \n    for i in range(m):\n        # Calculate the ranges for x and y coordinates\n        a_x = ((i // 2) + 0.1) * 10\n        b_x = ((i // 2) + 0.9) * 10\n        a_y = ((i % 2) * 0.5 + 0.1) * 10\n        b_y = ((i % 2) * 0.5 + 0.9) * 10\n        \n        # Generate samples for this class\n        samples = []\n        for _ in range(n_train + n_val):\n            # Generate x coordinate\n            rx = random.random()\n            x = a_x + (b_x - a_x) * rx\n            x_rounded = round(x, 4)\n            \n            # Generate y coordinate\n            ry = random.random()\n            y = a_y + (b_y - a_y) * ry\n            y_rounded = round(y, 4)\n            \n            samples.append([x_rounded, y_rounded])\n        \n        # Split into training and validation\n        X_train.extend(samples[:n_train])\n        X_val.extend(samples[n_train:])\n        \n        # Compute the label for this class\n        label = (i - 0.5) * 2\n        y_train.extend([label] * n_train)\n        y_val.extend([label] * n_val)\n    \n    # Convert lists to numpy arrays\n    return (\n        np.array(X_train),\n        np.array(X_val),\n        np.array(y_train),\n        np.array(y_val)\n    )\n```\n\n### Explanation\n1. **Seed Initialization**: The random seed is set to ensure reproducible results if specified.\n2. **Class-specific Distributions**: For each class `i`, the x-coordinate is generated from a uniform distribution defined by `(i // 2)`, and the y-coordinate is defined by `(i % 2)`.\n3. **Data Generation**: For each class, the required number of samples (both training and validation) are generated. Each sample's coordinates are computed using the uniform distribution parameters for the class and rounded to four decimal places.\n4. **Data Splitting**: After generating all samples for a class, the first `n_train` samples are allocated to the training set, and the remaining `n_val` samples go to the validation set.\n5. **Label Assignment**: Labels are calculated using the formula `(i - 0.5) * 2` and assigned to the corresponding samples in the training and validation sets.\n\nThis approach ensures that the data is generated according to the specified rules and split correctly into training and validation sets, with labels assigned appropriately. The use of the Python random module guarantees reproducibility when a seed is provided."}
{"task_id": 578, "completion_id": 0, "completion": "To solve this problem, we need to generate consecutive pairs (bigrams) from a given list. Each pair consists of two consecutive elements from the list. The solution should efficiently handle lists of varying lengths, including edge cases where the list is too short to form any pairs.\n\n### Approach\nThe approach involves the following steps:\n1. **Check List Length**: If the list has fewer than two elements, return an empty list immediately since no pairs can be formed.\n2. **Generate Consecutive Pairs**: Use the `zip` function to pair each element with its subsequent element. This is done by zipping the original list with a version of the list that starts from the second element. This method efficiently creates the required pairs without explicit loops.\n\n### Solution Code\n```python\nfrom typing import List, Tuple\n\ndef make_pairs(corpus: list) -> list[tuple]:\n    \"\"\"Return every consecutive pair (bigram) found in *corpus*.\n\n    Args:\n        corpus: A list containing any kind of elements.\n\n    Returns:\n        A list of tuples where each tuple is a pair of consecutive\n        elements from *corpus*.\n    \"\"\"\n    if len(corpus) < 2:\n        return []\n    return list(zip(corpus, corpus[1:]))\n```\n\n### Explanation\n1. **Check List Length**: The function first checks if the length of the input list `corpus` is less than 2. If true, it returns an empty list as no pairs can be formed.\n2. **Generate Pairs with Zip**: The `zip` function is used to combine two lists:\n   - The original list `corpus`.\n   - A slice of the original list starting from the second element (`corpus[1:]`).\n   This combination results in tuples of consecutive elements. Converting the result of the `zip` function to a list gives the desired list of tuples.\n\nThis approach is efficient and concise, leveraging Python's built-in functions to handle the iteration and pairing logic, ensuring clarity and optimal performance."}
{"task_id": 579, "completion_id": 0, "completion": "To solve this problem, we need to implement the classical K-Means clustering algorithm from scratch. The goal is to cluster a set of data points into a specified number of groups, ensuring deterministic results by using the first k points as initial cluster centers. The algorithm iteratively assigns points to the nearest cluster and updates the cluster centers until convergence or a maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Use the first k data points as the initial cluster centers.\n2. **Assignment Step**: For each data point, compute the Euclidean distance to each cluster center and assign the point to the nearest cluster. In case of a tie, the point is assigned to the cluster with the smallest index.\n3. **Update Step**: Recompute each cluster center as the mean of the points assigned to it. If a cluster has no points, its center remains unchanged.\n4. **Iteration**: Repeat the assignment and update steps until the cluster assignments stabilize or the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\"\"\"\n    m, d = X.shape\n    centers = X[:k].copy()  # Initial cluster centers\n    prev_labels = None\n\n    for _ in range(max_iters):\n        # Compute distances from each point to each center\n        distances = np.sqrt(((X[:, np.newaxis, :] - centers) ** 2).sum(axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        # Check if labels have stabilized\n        if prev_labels is not None and np.array_equal(labels, prev_labels):\n            break\n\n        # Update each cluster center\n        new_centers = np.zeros_like(centers)\n        for i in range(k):\n            mask = (labels == i)\n            if np.any(mask):\n                new_centers[i] = X[mask].mean(axis=0)\n            else:\n                new_centers[i] = centers[i]\n        centers = new_centers.copy()\n        prev_labels = labels.copy()\n\n    # Round the centers to 4 decimal places\n    rounded_centers = np.round(centers, 4)\n    return labels.tolist(), rounded_centers.tolist()\n```\n\n### Explanation\n1. **Initialization**: The initial cluster centers are set to the first k points in the dataset. This ensures deterministic results.\n2. **Distance Calculation**: For each point, the Euclidean distance to each cluster center is computed using vectorized operations for efficiency.\n3. **Assignment**: Each point is assigned to the nearest cluster center using the calculated distances. The `argmin` function handles ties by selecting the smallest cluster index.\n4. **Centroid Update**: Each cluster's center is updated to the mean of its assigned points. If a cluster has no points, its center remains unchanged to avoid division by zero.\n5. **Convergence Check**: The algorithm stops iterating once the cluster assignments no longer change between iterations or the maximum number of iterations is reached.\n6. **Rounding**: The final cluster centers are rounded to four decimal places to ensure consistent formatting for comparison.\n\nThis approach efficiently handles the iterative nature of K-Means clustering, ensuring deterministic results and correct handling of edge cases such as empty clusters."}
{"task_id": 581, "completion_id": 0, "completion": "To solve this problem, we need to implement Elastic-Net regularized linear regression using batch gradient descent. Elastic-Net combines both L1 and L2 regularization to prevent overfitting and handle multicollinearity in the dataset. The goal is to learn the weights and bias that minimize the loss function through iterative updates using gradient descent.\n\n### Approach\n1. **Initialization**: Start with all weights and the bias term set to zero.\n2. **Edge Cases Handling**: If the learning rate or the number of iterations is zero, return the initial parameters immediately.\n3. **Iterative Updates**: For each iteration:\n   - Compute the predicted values using the current weights and bias.\n   - Calculate the residuals (difference between actual and predicted values).\n   - Compute the gradients for the weights and bias, incorporating both L1 and L2 regularization terms.\n   - Update the weights and bias using the computed gradients, learning rate, and the number of samples.\n4. **Regularization**: Use L1 regularization (which can drive some weights to zero) and L2 regularization (which penalizes large weights) to ensure a balanced model complexity.\n5. **Rounding**: After completing all iterations, round the weights and bias to four decimal places for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    m, n = X.shape\n    if learning_rate == 0.0 or iterations == 0:\n        return ([round(0.0, 4) for _ in range(n)], round(0.0, 4))\n    \n    w = np.zeros(n)\n    b = 0.0\n    \n    for _ in range(iterations):\n        y_pred = X.dot(w) + b\n        residual = y - y_pred\n        \n        sign_w = np.where(w == 0, -1, np.sign(w))\n        grad_w = (-2 * X.T.dot(residual)) + (l1_penalty * sign_w) + (2 * l2_penalty * w)\n        grad_b = -2 * np.sum(residual)\n        \n        w -= (learning_rate * grad_w) / m\n        b -= (learning_rate * grad_b) / m\n    \n    weights_rounded = [round(weight, 4) for weight in w]\n    bias_rounded = round(b, 4)\n    return (weights_rounded, bias_rounded)\n```\n\n### Explanation\n1. **Initialization**: We start with weights `w` as a zero vector and bias `b` as 0.0.\n2. **Edge Case Check**: If the learning rate or iterations are zero, return initial zeros.\n3. **Prediction and Residual Calculation**: For each iteration, compute the predicted values and residuals.\n4. **Gradient Calculation**: Compute gradients for weights using the formula that includes both L1 and L2 regularization terms. The sign function for L1 regularization is adjusted to return -1 for zero weights to ensure correct initial updates.\n5. **Parameter Updates**: Adjust weights and bias using the computed gradients, learning rate, and the number of samples to average the gradients.\n6. **Rounding**: Finally, round the weights and bias to four decimal places to meet the output requirements.\n\nThis approach efficiently combines the benefits of both L1 and L2 regularization, ensuring a robust model that generalizes well to new data."}
{"task_id": 582, "completion_id": 0, "completion": "To solve this problem, we need to construct the adjacency matrix of an undirected k-nearest-neighbor (k-NN) graph from a given set of points in a Euclidean space. The steps involve computing the nearest neighbors for each point, creating directed edges, and then converting the directed graph into an undirected one by symmetrizing the adjacency matrix.\n\n### Approach\n1. **Handle Special Case**: If there is only one point, return a 1x1 matrix with 0.\n2. **Determine k**: Compute k as the minimum of the floor of the square root of the number of points and 10.\n3. **Compute Squared Distances**: Calculate the squared Euclidean distances between all pairs of points to avoid the computational cost of taking square roots.\n4. **Find Nearest Neighbors**: For each point, determine its k nearest neighbors using the computed distances, resolving ties by index order.\n5. **Construct Directed Adjacency Matrix**: Create a directed adjacency matrix where each point has edges to its k nearest neighbors.\n6. **Symmetrize the Matrix**: Convert the directed adjacency matrix into an undirected one by ensuring an edge exists if either direction exists in the directed matrix.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef nearest_neighbor_graph(X):\n    \"\"\"Build the undirected k-nearest-neighbour adjacency matrix.\n\n    Parameters\n    ----------\n    X : Sequence[Sequence[float]]\n        2-D array-like object of shape (n_samples, n_features).\n\n    Returns\n    -------\n    list[list[int]]\n        Adjacency matrix W where W[i][j] is 1 iff an undirected edge exists\n        between sample i and sample j, otherwise 0.\n    \"\"\"\n    X_np = np.array(X)\n    n = X_np.shape[0]\n    if n == 1:\n        return [[0]]\n    \n    k = min(int(math.floor(math.sqrt(n))), 10)\n    \n    # Compute squared distances matrix\n    squared_distances = np.sum((X_np[:, np.newaxis, :] - X_np) ** 2, axis=2)\n    \n    directed_adj = np.zeros((n, n), dtype=int)\n    for i in range(n):\n        sorted_indices = np.argsort(squared_distances[i])\n        neighbors = sorted_indices[1 : k + 1]  # Exclude the point itself\n        directed_adj[i, neighbors] = 1\n    \n    undirected_adj = (directed_adj | directed_adj.T).astype(int)\n    return undirected_adj.tolist()\n```\n\n### Explanation\n1. **Handling Special Case**: If there's only one point, the adjacency matrix is trivially `[[0]]`.\n2. **Compute k**: The value of k is determined as the minimum between the floor of the square root of the number of points and 10 to balance between locality and computational complexity.\n3. **Squared Distances**: Using squared distances instead of actual Euclidean distances saves computational time while maintaining the order of distances.\n4. **Nearest Neighbors**: For each point, the indices of the k nearest neighbors are found by sorting the distances and selecting the top k indices after excluding the point itself.\n5. **Directed to Undirected Graph**: The undirected adjacency matrix is formed by taking the logical OR of the directed adjacency matrix and its transpose, ensuring edges exist if either direction was present in the directed graph.\n\nThis approach efficiently constructs the k-NN graph using vectorized operations for distance computation and matrix operations for symmetrization, ensuring correctness and performance."}
{"task_id": 584, "completion_id": 0, "completion": "To solve this problem, we need to create a function that transforms a 2-D frame (representing a grayscale image) into a 3-D state tensor by stacking the same frame four times along a new third dimension. This is a common preprocessing step in Deep Q-Learning for Atari agents to infer motion from a sequence of frames.\n\n### Approach\n1. **Convert Input to Numpy Array**: The input 2-D frame is converted into a numpy array for efficient manipulation.\n2. **Expand Dimensions**: Add a new third dimension to the 2-D array using numpy's `newaxis` to transform it into a 3-D array with shape (height, width, 1).\n3. **Tile Along Third Dimension**: Use numpy's `tile` function to replicate the array four times along the third dimension, resulting in a 3-D array of shape (height, width, 4).\n4. **Convert to List**: Convert the resulting 3-D numpy array back to a nested list structure using the `tolist()` method to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef make_initial_state(state: list[list[int | float]]) -> list[list[list[int | float]]]:\n    \"\"\"Stack a single 2-D Atari frame four times to form the initial state.\n\n    Args:\n        state: 2-D list representing the grayscale frame (height \u00d7 width).\n\n    Returns:\n        3-D list with shape (height \u00d7 width \u00d7 4) where the frame is repeated\n        along the last axis.\n    \"\"\"\n    arr = np.array(state)\n    expanded = arr[:, :, np.newaxis]\n    stacked = np.tile(expanded, (1, 1, 4))\n    return stacked.tolist()\n```\n\n### Explanation\n1. **Conversion to Numpy Array**: The input list of lists is converted into a numpy array to leverage numpy's efficient array operations.\n2. **Dimension Expansion**: By adding a new axis using `newaxis`, the 2-D array is reshaped into a 3-D array with a singleton third dimension. This prepares the array for tiling along this new dimension.\n3. **Tiling**: The `np.tile` function replicates the array four times along the third dimension. The parameters `(1, 1, 4)` specify that the array should be replicated once along the first and second dimensions, and four times along the third dimension.\n4. **Conversion to List**: Finally, the 3-D numpy array is converted back to a nested list structure using `tolist()`, ensuring the output format matches the problem's requirements.\n\nThis approach efficiently transforms the input frame into the desired 3-D structure using numpy operations, which are both concise and performant."}
{"task_id": 585, "completion_id": 0, "completion": "To solve this problem, we need to replicate the behavior of Python 2's built-in `cmp` function in Python 3. This function compares two objects and returns an integer indicating their relative order. The function should return 1 if the first argument is greater than the second, 0 if they are equal, and -1 if the first argument is less than the second.\n\n### Approach\nThe approach to solving this problem involves using Python's standard comparison operators (`<`, `>`, and `==`). The key steps are as follows:\n1. Check if the first argument is less than the second. If true, return -1.\n2. Check if the first argument is greater than the second. If true, return 1.\n3. If neither of the above conditions is met, the arguments are considered equal, so return 0.\n\nThis approach leverages Python's built-in comparison operators, which work across various data types (e.g., numbers, strings, lists, tuples) as long as they support these operators. The implementation assumes that the objects being compared support these operators, which aligns with the problem constraints.\n\n### Solution Code\n```python\ndef cmp(a: object, b: object) -> int:\n    \"\"\"Compares two values and returns their ordering.\n\n    The function mimics Python 2's ``cmp``. It must return:\n    \u2022  1  if ``a`` is greater than ``b``\n    \u2022  0  if ``a`` equals ``b``\n    \u2022  -1 if ``a`` is less than ``b``\n\n    Args:\n        a: Any Python object supporting comparison operations.\n        b: Any Python object supporting comparison operations.\n\n    Returns:\n        int: One of -1, 0, or 1 indicating the comparison result.\n    \"\"\"\n    if a < b:\n        return -1\n    elif a > b:\n        return 1\n    else:\n        return 0\n```\n\n### Explanation\nThe function `cmp` starts by comparing the two arguments using the `<` operator. If `a` is less than `b`, it returns -1. If not, it proceeds to check if `a` is greater than `b` using the `>` operator, returning 1 if true. If neither condition is met, it concludes that the arguments are equal and returns 0. This approach efficiently determines the relative order of the two arguments using straightforward comparisons, ensuring compatibility with any objects that support the standard comparison operators."}
{"task_id": 586, "completion_id": 0, "completion": "To solve this problem, we need to implement a single-point LOWESS (Locally Weighted Scatterplot Smoothing) predictor. This method involves fitting a weighted linear regression model to a subset of the training data points that are nearest to the query point, using tricubic weights to emphasize closer points more than those farther away. The solution must also handle normalization of the data and edge cases where the weighted regression might fail.\n\n### Approach\n1. **Input Validation**: Ensure the input arrays `x` and `y` have the same length and the `window` parameter is within the valid range.\n2. **Normalization**: Normalize both `x` and `y` to the range [0, 1] to handle different scales in the input data.\n3. **Query Point Normalization**: Normalize the query point `x_query` using the same parameters as the training data.\n4. **Nearest Neighbors Selection**: Identify the `window` nearest neighbors to the normalized query point based on their normalized x-values.\n5. **Tricubic Weights Calculation**: Compute weights for the selected neighbors using the tricubic weight function.\n6. **Weighted Linear Regression**: Fit a weighted linear regression model to the selected neighbors. If the regression fails (e.g., due to all weights being zero or all x-values being the same), fall back to the weighted mean of the y-values.\n7. **Denormalization**: Convert the predicted normalized y-value back to the original scale and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef lowess_predict(x: np.ndarray, y: np.ndarray, x_query: float, window: int) -> float:\n    \"\"\"Predict a single y value using LOWESS with linear (degree-1) local fitting.\"\"\"\n    \n    # Validate inputs\n    if len(x) != len(y):\n        raise ValueError(\"x and y must have the same length.\")\n    if window < 2 or window > len(x):\n        raise ValueError(\"window must be between 2 and len(x).\")\n    \n    # Normalize x and y to [0, 1]\n    x_min, x_max = x.min(), x.max()\n    x_range = x_max - x_min\n    if x_range == 0:\n        x_normalized = np.full_like(x, 0.5, dtype=np.float64)\n    else:\n        x_normalized = (x - x_min) / x_range\n    \n    y_min, y_max = y.min(), y.max()\n    y_range = y_max - y_min\n    if y_range == 0:\n        y_normalized = np.full_like(y, 0.5, dtype=np.float64)\n    else:\n        y_normalized = (y - y_min) / y_range\n    \n    # Normalize x_query\n    if x_range == 0:\n        x_query_normalized = 0.5\n    else:\n        x_query_normalized = (x_query - x_min) / x_range\n    \n    # Select window nearest neighbors based on normalized x distances\n    distances = np.abs(x_normalized - x_query_normalized)\n    selected_indices = np.argsort(distances)[:window]\n    selected_x = x_normalized[selected_indices]\n    selected_y = y_normalized[selected_indices]\n    selected_distances = distances[selected_indices]\n    \n    # Compute tricubic weights\n    d_max = np.max(selected_distances)\n    if d_max == 0:\n        weights = np.ones_like(selected_distances)\n    else:\n        scaled_dist = selected_distances / d_max\n        weights = (1 - scaled_dist ** 3) ** 3\n    \n    sum_weights = np.sum(weights)\n    \n    if sum_weights == 0:\n        # All weights are zero; use unweighted mean of selected y\n        a = np.mean(selected_y)\n        b = 0.0\n    else:\n        x_weighted_mean = np.sum(weights * selected_x) / sum_weights\n        y_weighted_mean = np.sum(weights * selected_y) / sum_weights\n        \n        # Compute numerator and denominator for slope\n        numerator = np.sum(weights * (selected_x - x_weighted_mean) * (selected_y - y_weighted_mean))\n        denominator = np.sum(weights * (selected_x - x_weighted_mean) ** 2)\n        \n        if denominator == 0:\n            b = 0.0\n            a = y_weighted_mean\n        else:\n            b = numerator / denominator\n            a = y_weighted_mean - b * x_weighted_mean\n    \n    # Predict normalized y and denormalize\n    y_hat_normalized = a + b * x_query_normalized\n    \n    if y_range == 0:\n        y_hat = y_min\n    else:\n        y_hat = y_min + y_hat_normalized * y_range\n    \n    return float(np.round(y_hat, 4))\n```\n\n### Explanation\n1. **Input Validation**: Ensures the input data is valid and meets the constraints for the `window` parameter.\n2. **Normalization**: Converts the input features and target variables to a [0, 1] range to handle different scales.\n3. **Query Point Handling**: Normalizes the query point using the same parameters as the training data to maintain consistency.\n4. **Nearest Neighbors Selection**: Uses the normalized x-values to find the closest points to the query, ensuring the model focuses on the most relevant data.\n5. **Weight Calculation**: Uses tricubic weights to give more importance to points closer to the query, reducing the influence of distant points.\n6. **Weighted Regression**: Fits a linear regression model using the calculated weights. If the model cannot be fit (due to division by zero), it defaults to the weighted mean of the y-values.\n7. **Denormalization**: Converts the prediction back to the original scale and rounds the result for readability.\n\nThis approach ensures robustness and handles edge cases gracefully, providing accurate predictions even when data is not ideally distributed."}
{"task_id": 587, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that manages a replay memory buffer by executing a sequence of operations and returning the results of specific operations. The replay memory buffer is a list that stores transitions, and the operations include appending transitions, popping the most recent transition, sampling transitions, and checking the buffer size.\n\n### Approach\n1. **Initialization**: Start with an empty buffer and set a fixed random seed for reproducibility.\n2. **Processing Operations**: Iterate through each operation and execute the corresponding command:\n   - **Append**: Add a transition to the end of the buffer.\n   - **Pop**: Remove and return the most recent transition (LIFO order). Return `None` if the buffer is empty.\n   - **Sample**: Return a list of distinct elements chosen uniformly at random. If the sample size exceeds the buffer size, return an empty list.\n   - **Size**: Return the current number of elements in the buffer.\n3. **Collect Results**: Accumulate the results of pop, sample, and size operations in the order they are executed.\n\n### Solution Code\n```python\nimport random\nfrom typing import Any, List, Tuple\n\ndef manage_replay_memory(operations: List[Tuple]) -> List[Any]:\n    \"\"\"Perform operations on an in-memory replay buffer.\"\"\"\n    random.seed(0)\n    buffer = []\n    outputs = []\n    \n    for op in operations:\n        command = op[0]\n        if command == 'append':\n            transition = op[1]\n            buffer.append(transition)\n        elif command == 'pop':\n            if not buffer:\n                outputs.append(None)\n            else:\n                outputs.append(buffer.pop())\n        elif command == 'sample':\n            batch_size = op[1]\n            if len(buffer) < batch_size:\n                outputs.append([])\n            else:\n                sampled = random.sample(buffer, batch_size)\n                outputs.append(sampled)\n        elif command == 'size':\n            outputs.append(len(buffer))\n    return outputs\n```\n\n### Explanation\n- **Initialization**: The function starts by setting a fixed random seed to ensure deterministic behavior for sampling. An empty buffer and results list are initialized.\n- **Appending Transitions**: The `append` operation adds a transition to the end of the buffer without generating any output.\n- **Popping Transitions**: The `pop` operation removes the most recently added transition from the buffer. If the buffer is empty, it returns `None`.\n- **Sampling Transitions**: The `sample` operation uses `random.sample` to select a specified number of transitions. If the buffer size is insufficient, an empty list is returned.\n- **Checking Size**: The `size` operation returns the current number of elements in the buffer.\n- **Result Collection**: Results from pop, sample, and size operations are collected in the order they are executed and returned as the final output list.\n\nThis approach ensures that all operations are handled efficiently and deterministically, with reproducible results for sampling due to the fixed random seed."}
{"task_id": 588, "completion_id": 0, "completion": "To solve this problem, we need to determine the maximum depth of a binary tree. The depth of a node is defined as the number of edges from the root to that node, with the root node having a depth of 0. If the tree is empty, the depth is defined as -1.\n\n### Approach\nThe approach to solve this problem involves using recursion to compute the height of the tree, which directly corresponds to the maximum depth of the tree. The height of a tree is the number of edges on the longest path from the root to a leaf node. For any given node, the height of its subtree is 1 plus the maximum height of its left and right subtrees. This recursive approach efficiently computes the maximum depth by leveraging the tree's structure.\n\n### Solution Code\n```python\nfrom typing import Optional\n\nclass Node:\n    \"\"\"A simple binary tree node.\"\"\"\n\n    def __init__(self,\n                 data=None,\n                 split_variable=None,\n                 split_variable_value=None,\n                 left: Optional['Node'] = None,\n                 right: Optional['Node'] = None,\n                 depth: int = 0,\n                 criterion_value=None):\n        self.data = data\n        self.split_variable = split_variable\n        self.split_variable_value = split_variable_value\n        self.left = left\n        self.right = right\n        self.depth = depth\n        self.criterion_value = criterion_value\n\n\ndef tree_max_depth(root: Optional[Node]) -> int:\n    \"\"\"Computes the maximum depth of a binary tree.\n\n    The depth of the root node is defined to be 0.  The depth of an empty\n    tree is defined to be -1.\n\n    Args:\n        root: The root of the binary tree (Node or None).\n\n    Returns:\n        The maximum depth as an integer.\n    \"\"\"\n    if root is None:\n        return -1\n    return 1 + max(tree_max_depth(root.left), tree_max_depth(root.right))\n```\n\n### Explanation\n1. **Base Case**: If the tree is empty (`root` is `None`), the function returns -1, as specified.\n2. **Recursive Case**: For a non-empty tree, the function recursively calculates the depth of the left and right subtrees. The maximum depth of the tree rooted at the current node is 1 plus the maximum of the depths of the left and right subtrees. This works because each child node's depth is one more than its parent's depth, and we take the maximum depth from both subtrees.\n\nThis approach efficiently computes the maximum depth in O(n) time complexity, where n is the number of nodes in the tree, as each node is visited exactly once. The space complexity is O(h), where h is the height of the tree, due to the recursion stack."}
{"task_id": 590, "completion_id": 0, "completion": "To solve this problem, we need to determine which squares in a Minesweeper game can be safely revealed and which must be flagged as bombs based on the current state of the board. The solution involves applying logical rules to each revealed square to deduce the next moves.\n\n### Approach\n1. **Iterate Over Each Cell**: For each cell in the board, check if it is revealed (not hidden) and not flagged.\n2. **Check Neighbors**: For each revealed cell, identify its 8-connected neighbors (up, down, left, right, and the four diagonals).\n3. **Apply Rules**:\n   - If the cell's value is 0, all hidden and un-flagged neighbors can be safely revealed.\n   - If the cell's value is 8, all hidden and un-flagged neighbors must be flagged as bombs.\n   - For other values, calculate the number of already flagged neighbors (bombs) and the number of hidden, un-flagged neighbors (unexplored). Depending on these counts, either reveal the unexplored neighbors or flag them as bombs.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef minesweeper_step(ground: list[list[float | int]],\n                     flags: list[list[bool]]) -> tuple[list[tuple[int, int]],\n                                                    list[tuple[int, int]]]:\n    \"\"\"Perform one logical deduction step on a Minesweeper board.\"\"\"\n    rows = len(ground)\n    if rows == 0:\n        return [], []\n    cols = len(ground[0])\n    clicks_set = set()\n    new_flags_set = set()\n\n    for r in range(rows):\n        for c in range(cols):\n            if not np.isnan(ground[r][c]) and not flags[r][c]:\n                val = ground[r][c]\n                neighbors = []\n                for dr in [-1, 0, 1]:\n                    for dc in [-1, 0, 1]:\n                        if dr == 0 and dc == 0:\n                            continue\n                        nr, nc = r + dr, c + dc\n                        if 0 <= nr < rows and 0 <= nc < cols:\n                            neighbors.append((nr, nc))\n                candidates = [(nr, nc) for (nr, nc) in neighbors\n                              if np.isnan(ground[nr][nc]) and not flags[nr][nc]]\n                if val == 0:\n                    clicks_set.update(candidates)\n                elif val == 8:\n                    new_flags_set.update(candidates)\n                else:\n                    bombs = sum(flags[nr][nc] for (nr, nc) in neighbors)\n                    unexplored = len(candidates)\n                    if bombs == val:\n                        clicks_set.update(candidates)\n                    elif unexplored == val:\n                        new_flags_set.update(candidates)\n\n    clicks = sorted(clicks_set)\n    new_flags = sorted(new_flags_set)\n    return clicks, new_flags\n```\n\n### Explanation\n1. **Initialization**: We start by initializing sets to collect the coordinates of squares that can be clicked or flagged, ensuring uniqueness.\n2. **Iterate Over Cells**: For each cell, we check if it is revealed and not flagged. If so, we proceed to analyze its neighbors.\n3. **Neighbor Collection**: For each valid cell, we generate its 8-connected neighbors and check if they are within the bounds of the board.\n4. **Candidate Neighbors**: We identify neighbors that are hidden (not yet revealed) and not flagged as potential candidates for further action.\n5. **Rule Application**: Based on the value of the current cell:\n   - If the value is 0, all candidate neighbors are added to the safe clicks.\n   - If the value is 8, all candidate neighbors are marked as flags.\n   - For other values, we count the number of flagged neighbors (bombs) and the number of hidden, un-flagged neighbors (unexplored). Depending on these counts, we either reveal the unexplored neighbors or flag them.\n\nThis approach ensures that we efficiently apply the logical rules in a single pass over the board, collecting all possible safe moves and flags while avoiding duplicates. The results are then sorted to meet the problem's output requirements."}
{"task_id": 591, "completion_id": 0, "completion": "To solve this problem, we need to compute the output of a fully-connected feed-forward neural network with given weights and an input vector. The network uses ReLU activation for hidden layers and a linear activation for the output layer. The solution involves a forward pass through the network, applying the appropriate transformations at each layer.\n\n### Approach\n1. **Input Processing**: Start with the input vector and convert it into a numpy array for efficient numerical operations.\n2. **Layer-wise Processing**:\n   - **Bias Addition**: For each layer, prepend a bias term (1.0) to the input vector.\n   - **Weight Multiplication**: Use the given weights to compute the dot product of the augmented input vector with the weight matrix of the current layer.\n   - **Activation Function**: Apply the ReLU activation function to the output of hidden layers. The output layer uses a linear activation (no transformation).\n3. **Result Formatting**: After processing all layers, round the result to four decimal places and return it as a float if the output layer has one neuron, or as a list of floats otherwise.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    current = np.array(x, dtype=np.float64)\n    for i, layer in enumerate(weights):\n        augmented = np.insert(current, 0, 1.0)\n        layer_weights = np.array(layer, dtype=np.float64)\n        output = np.dot(augmented, layer_weights.T)\n        if i != len(weights) - 1:\n            output = np.maximum(output, 0)\n        current = output\n    rounded = np.round(current, 4)\n    return rounded.item() if len(rounded) == 1 else rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: Convert the input vector `x` to a numpy array for efficient computation.\n2. **Layer Processing**:\n   - **Bias Addition**: For each layer, prepend a 1.0 to the current input vector to include the bias term.\n   - **Weight Matrix Multiplication**: Convert the layer's weights to a numpy array and compute the dot product with the augmented input vector.\n   - **Activation Function**: Apply ReLU activation to hidden layers (all except the last layer) to introduce non-linearity.\n3. **Result Handling**: After processing all layers, round the final output to four decimal places. Return a float if the output layer has a single neuron, otherwise return a list of floats.\n\nThis approach ensures that each layer's computations are performed efficiently using numpy's optimized operations, and the result is formatted correctly according to the problem's requirements."}
{"task_id": 592, "completion_id": 0, "completion": "To solve this problem, we need to analyze the neighboring cells of a given cell in a Minesweeper game to determine which cells are unopened and how many are flagged as mines. The solution involves checking all possible neighboring cells, validating their positions, and then collecting the required information.\n\n### Approach\n1. **Identify Neighbors**: Generate all possible neighboring cells around the given cell (row, col) using directional deltas.\n2. **Check Bounds**: Ensure each neighbor is within the valid bounds of the game board.\n3. **Collect Data**: For each valid neighbor, check if it is flagged and if it is unopened.\n4. **Sort and Return**: Sort the unopened neighbors by row and column in ascending order and return the result along with the count of flagged neighbors.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef analyze_cell(clicked: np.ndarray, flags: np.ndarray, row: int, col: int) -> tuple[list[tuple[int, int]], int]:\n    \"\"\"Return unopened neighbouring cells and flagged-mine count for a board position.\"\"\"\n    rows, cols = clicked.shape\n    unopened = []\n    flagged_count = 0\n    deltas = [(-1, -1), (-1, 0), (-1, 1),\n              (0, -1),          (0, 1),\n              (1, -1),  (1, 0), (1, 1)]\n    \n    for dr, dc in deltas:\n        r = row + dr\n        c = col + dc\n        if 0 <= r < rows and 0 <= c < cols:\n            if flags[r, c]:\n                flagged_count += 1\n            if not clicked[r, c]:\n                unopened.append((r, c))\n    \n    unopened_sorted = sorted(unopened)\n    return (unopened_sorted, flagged_count)\n```\n\n### Explanation\n1. **Neighbor Generation**: The directional deltas represent the eight possible directions around a cell (up, down, left, right, and the four diagonals).\n2. **Bounds Check**: For each generated neighbor, we check if it lies within the valid indices of the board using the board's dimensions (rows and columns).\n3. **Flagged Count**: We increment the count for each neighbor that is marked as a mine (flagged).\n4. **Unopened Collection**: We collect the coordinates of neighbors that have not been opened (clicked is False).\n5. **Sorting**: The collected unopened neighbors are sorted lexicographically to ensure the correct order.\n\nThis approach efficiently checks all possible neighbors, ensures valid bounds, and collects the necessary information in a structured manner, providing the required results in optimal time."}
{"task_id": 593, "completion_id": 0, "completion": "To solve this problem, we need to deduce which cells in a Minesweeper game are safe and which contain mines based on given constraints. The constraints are simplified using logical rules until no further deductions can be made. The solution involves applying four specific rules iteratively to derive the maximum information possible.\n\n### Approach\n1. **Solved Constraints**: Check if any constraint directly indicates that all variables are safe (value 0) or all are mines (value equals the number of variables).\n2. **Propagation of Knowledge**: Remove known safe cells and mines from other constraints, adjusting the value of the constraints accordingly.\n3. **Subset Rule**: If one constraint's variables are a subset of another's, replace the larger constraint with the difference of variables and adjust the value.\n4. **Duplicate Removal**: Remove any duplicate constraints to avoid redundant processing.\n\nThe approach involves iteratively applying these rules until no further changes can be made, ensuring that all possible deductions are made.\n\n### Solution Code\n```python\nfrom copy import deepcopy\n\ndef deduce_mines(constraints: list[list]) -> tuple[list[str], list[str]]:\n    \"\"\"Simplify Minesweeper constraints.\"\"\"\n    safe = set()\n    mines = set()\n    current_constraints = [ (tuple(sorted(c[0])), c[1] ) for c in constraints ]\n    \n    changed = True\n    while changed:\n        changed = False\n        \n        # Rule 1: Solved constraints\n        to_remove = []\n        new_safe = set()\n        new_mines = set()\n        for idx, (vars_, val) in enumerate(current_constraints):\n            if val == 0:\n                new_safe.update(vars_)\n                to_remove.append(idx)\n            elif val == len(vars_):\n                new_mines.update(vars_)\n                to_remove.append(idx)\n        if new_safe:\n            safe.update(new_safe)\n            changed = True\n        if new_mines:\n            mines.update(new_mines)\n            changed = True\n        current_constraints = [ c for idx, c in enumerate(current_constraints) if idx not in to_remove ]\n        \n        # Rule 2: Propagation\n        new_constraints = []\n        for (vars_, val) in current_constraints:\n            remaining_vars = [var for var in vars_ if var not in safe and var not in mines]\n            mine_count = sum(1 for var in vars_ if var in mines)\n            new_val = val - mine_count\n            new_vars = tuple(sorted(remaining_vars))\n            new_constraints.append( (new_vars, new_val) )\n        filtered_new = []\n        for vars_, val in new_constraints:\n            if len(vars_) > 0:\n                filtered_new.append( (vars_, val) )\n            elif val == 0:\n                pass\n        if current_constraints != filtered_new:\n            changed = True\n            current_constraints = filtered_new\n        \n        # Rule 3: Subset rule\n        subset_changes = True\n        while subset_changes:\n            subset_changes = False\n            new_constraints_subset = []\n            replaced = set()\n            for i in range(len(current_constraints)):\n                if i in replaced:\n                    continue\n                C_i_vars, C_i_val = current_constraints[i]\n                S_i = set(C_i_vars)\n                found = False\n                for j in range(len(current_constraints)):\n                    if j == i or j in replaced:\n                        continue\n                    C_j_vars, C_j_val = current_constraints[j]\n                    S_j = set(C_j_vars)\n                    if S_j.issubset(S_i) and S_j != S_i:\n                        new_vars = list(S_i - S_j)\n                        new_vars.sort()\n                        new_vars_tuple = tuple(new_vars)\n                        new_val = C_i_val - C_j_val\n                        if len(new_vars) > 0:\n                            new_constraints_subset.append( (new_vars_tuple, new_val) )\n                        else:\n                            if new_val != 0:\n                                pass\n                        replaced.add(i)\n                        found = True\n                        subset_changes = True\n                        changed = True\n                        break\n                if not found:\n                    new_constraints_subset.append(current_constraints[i])\n            if subset_changes:\n                current_constraints = new_constraints_subset\n        \n        # Rule 4: Duplicate removal\n        seen = set()\n        unique_constraints = []\n        for c in current_constraints:\n            key = (c[0], c[1])\n            if key not in seen:\n                seen.add(key)\n                unique_constraints.append(c)\n        if len(unique_constraints) < len(current_constraints):\n            changed = True\n        current_constraints = unique_constraints\n    \n    safe_sorted = sorted( safe - mines )\n    mines_sorted = sorted( mines - safe )\n    return (safe_sorted, mines_sorted)\n```\n\n### Explanation\n1. **Solved Constraints**: Directly identify cells that are safe or mines if the constraints explicitly state so (value 0 or equal to the number of variables).\n2. **Propagation**: Remove known safe cells and mines from other constraints, adjusting the values accordingly to reflect the remaining unknowns.\n3. **Subset Rule**: Simplify constraints by replacing supersets with the difference of variables and adjusting the value based on the subset's value.\n4. **Duplicate Removal**: Eliminate redundant constraints to streamline further processing.\n\nThis approach ensures that all possible logical deductions are made iteratively until no further information can be extracted, providing the final lists of safe cells and mines."}
{"task_id": 594, "completion_id": 0, "completion": "To solve this problem, we need to convert a given 2-D maze into an undirected graph represented as an adjacency list. Each open cell (0) in the maze becomes a node in the graph, and edges exist between nodes if their corresponding cells are orthogonally adjacent (up, down, left, right).\n\n### Approach\n1. **Identify Open Cells**: Traverse the maze to identify all open cells (cells with value 0). These cells will be the nodes of our graph.\n2. **Check Neighbors**: For each open cell, check its four possible neighbors (up, down, left, right). If a neighbor is also an open cell and within the maze boundaries, it is added to the list of neighbors for the current cell.\n3. **Sort Neighbors**: Ensure that the list of neighbors for each node is sorted in lexicographic order (first by row, then by column) to maintain a deterministic output.\n4. **Construct Adjacency List**: Build the adjacency list as a dictionary where each key is a coordinate tuple of an open cell, and the corresponding value is the sorted list of its neighboring open cells.\n\n### Solution Code\n```python\ndef maze_to_graph(maze: list[list[int]]) -> dict[tuple[int, int], list[tuple[int, int]]:\n    \"\"\"Convert a binary maze into an adjacency-list graph.\n\n    A 0 in the maze denotes an open cell, while 1 denotes a wall.  Two open\n    cells are adjacent if they share an edge (up, down, left, right).\n\n    Args:\n        maze: Rectangular 2-D list containing only 0s and 1s.\n\n    Returns:\n        A dictionary mapping each open cell coordinate (row, col) to a sorted\n        list of neighbouring open cell coordinates.  Walls are not included in\n        the dictionary.  If there are no open cells the function returns an\n        empty dictionary.\n    \"\"\"\n    adjacency = {}\n    n_rows = len(maze)\n    if n_rows == 0:\n        return adjacency\n    n_cols = len(maze[0]) if n_rows > 0 else 0\n    \n    for row in range(n_rows):\n        for col in range(n_cols):\n            if maze[row][col] == 0:\n                neighbors = []\n                directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n                for dr, dc in directions:\n                    new_row = row + dr\n                    new_col = col + dc\n                    if 0 <= new_row < n_rows and 0 <= new_col < n_cols:\n                        if maze[new_row][new_col] == 0:\n                            neighbors.append((new_row, new_col))\n                neighbors.sort()\n                adjacency[(row, col)] = neighbors\n    return adjacency\n```\n\n### Explanation\n1. **Initialization**: We start by initializing an empty dictionary `adjacency` to hold our graph. We then determine the number of rows and columns in the maze.\n2. **Edge Cases**: If the maze is empty (no rows), we immediately return the empty dictionary. This handles cases where the input maze is invalid or empty.\n3. **Traverse Maze**: We iterate over each cell in the maze. For each cell that is an open cell (0), we check its four possible neighbors.\n4. **Neighbor Check**: For each direction (up, down, left, right), we calculate the new cell coordinates and check if they are within bounds and if the cell is open. Valid neighbors are added to the list.\n5. **Sorting**: The list of neighbors for each cell is sorted to ensure lexicographic order.\n6. **Return Result**: The constructed adjacency list is returned, which maps each open cell to its sorted list of neighboring open cells.\n\nThis approach efficiently converts the maze into the required graph structure, ensuring correctness and determinism in the output."}
{"task_id": 595, "completion_id": 0, "completion": "To solve this problem, we need to implement the K-Means clustering algorithm from scratch to compute the centroids of clusters given a dataset and the number of clusters. The solution must follow specific steps for initialization, iteration, convergence checking, and result formatting.\n\n### Approach\n1. **Initialization**: The centroids are initialized using the first `k` samples from the dataset to ensure deterministic results.\n2. **Iteration**: For up to `max_iters` iterations, each sample is assigned to the nearest centroid using squared Euclidean distance. Centroids are then updated to the mean of the samples assigned to them. If a cluster becomes empty, its centroid remains unchanged.\n3. **Convergence Check**: The algorithm stops early if all centroids move less than `1e-4` in an iteration.\n4. **Sorting and Formatting**: After convergence, centroids are sorted lexicographically and each coordinate is rounded to 4 decimal places before returning as a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Compute the centroids using K-Means clustering algorithm.\"\"\"\n    if k == 0:\n        return []\n    \n    centroids = X[:k].copy()\n    \n    for _ in range(max_iters):\n        # Calculate squared distances between each sample and each centroid\n        distances = np.sum((X[:, np.newaxis] - centroids) ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        new_centroids = np.zeros_like(centroids)\n        for i in range(k):\n            mask = (labels == i)\n            if np.any(mask):\n                new_centroids[i] = X[mask].mean(axis=0)\n            else:\n                new_centroids[i] = centroids[i]\n        \n        # Check for convergence\n        delta = np.linalg.norm(new_centroids - centroids, axis=1)\n        if np.all(delta < 1e-4):\n            break\n        \n        centroids = new_centroids\n    \n    # Sort centroids lexicographically\n    if centroids.size == 0:\n        sorted_centroids = centroids\n    else:\n        keys = [centroids[:, col] for col in reversed(range(centroids.shape[1]))]\n        order = np.lexsort(keys)\n        sorted_centroids = centroids[order]\n    \n    # Round to 4 decimal places and convert to list of lists\n    rounded_centroids = np.round(sorted_centroids, 4)\n    return rounded_centroids.tolist()\n```\n\n### Explanation\n1. **Initialization**: The centroids are initialized using the first `k` samples from the dataset to ensure deterministic results.\n2. **Distance Calculation**: For each sample, the squared Euclidean distance to each centroid is computed using broadcasting to efficiently handle the operations.\n3. **Assignment**: Each sample is assigned to the nearest centroid based on the computed distances.\n4. **Centroid Update**: New centroids are computed as the mean of the samples assigned to each cluster. If a cluster has no samples, the centroid remains unchanged.\n5. **Convergence Check**: The algorithm checks if the movement of all centroids is below the threshold `1e-4`. If so, it stops early.\n6. **Sorting and Rounding**: After convergence, centroids are sorted lexicographically and each coordinate is rounded to 4 decimal places, ensuring the result is formatted as required.\n\nThis approach efficiently handles the K-Means clustering process, ensuring deterministic results and proper formatting of the output."}
{"task_id": 596, "completion_id": 0, "completion": "To solve this problem, we need to convert a binary maze represented as a 2-D NumPy array into an undirected, unweighted graph using an adjacency list. Each free cell (represented by 1) in the maze becomes a vertex in the graph, and edges exist between vertices if their corresponding cells are adjacent (sharing a common edge).\n\n### Approach\n1. **Identify Free Cells**: Iterate through the maze to identify all cells that contain a 1. These cells are the vertices of our graph.\n2. **Check Neighbors**: For each free cell, check its four possible neighbors (up, down, left, right) to determine if they are within the maze boundaries and also free cells.\n3. **Build Adjacency List**: For each free cell, collect all valid adjacent free cells as its neighbors. Sort these neighbors in lexicographic order to ensure the output meets the specified constraints.\n4. **Return Result**: If there are no free cells, return an empty dictionary. Otherwise, return the adjacency list with sorted neighbor lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef convert_maze_to_graph(maze: np.ndarray) -> dict:\n    \"\"\"Convert a binary maze to an adjacency-list representation.\"\"\"\n    adjacency = {}\n    rows, cols = maze.shape\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Up, Down, Left, Right\n    \n    for row in range(rows):\n        for col in range(cols):\n            if maze[row, col] == 1:\n                neighbors = []\n                for dr, dc in directions:\n                    nr, nc = row + dr, col + dc\n                    if 0 <= nr < rows and 0 <= nc < cols:\n                        if maze[nr, nc] == 1:\n                            neighbors.append((nr, nc))\n                # Sort neighbors lexicographically\n                adjacency[(row, col)] = sorted(neighbors)\n    \n    return adjacency\n```\n\n### Explanation\n- **Identify Free Cells**: The code iterates through each cell in the maze. If a cell contains a 1, it is added to the adjacency list as a vertex.\n- **Check Neighbors**: For each free cell, the code checks the four possible directions (up, down, left, right). Each neighbor's coordinates are checked to ensure they are within the maze boundaries and are also free cells.\n- **Build Adjacency List**: Valid neighbors are collected in a list, which is then sorted lexicographically. This sorted list is assigned to the corresponding vertex in the adjacency list.\n- **Return Result**: The adjacency list is returned, containing all free cells as keys with their sorted lists of adjacent free cells. If there are no free cells, the dictionary will naturally be empty.\n\nThis approach efficiently converts the maze into the required graph structure while ensuring the adjacency lists are correctly sorted and formatted."}
{"task_id": 597, "completion_id": 0, "completion": "To solve this problem, we need to determine the best split for a regression tree based on the minimal weighted Mean Squared Error (MSE). The solution involves evaluating all possible splits for each feature and selecting the one that minimizes the weighted MSE, considering both the left and right subsets of the split.\n\n### Approach\n1. **Data Preparation**: Convert the input data (features and target) into NumPy arrays for efficient processing.\n2. **Feature Iteration**: For each feature, sort the data based on the feature values.\n3. **Threshold Identification**: Identify unique thresholds from the sorted feature values, excluding the maximum value to ensure valid splits.\n4. **Prefix Sums Calculation**: Precompute cumulative sums of the target values and their squares to efficiently compute variances for potential splits.\n5. **Split Evaluation**: For each threshold, compute the weighted MSE for the split, considering the left and right subsets. Track the best split based on minimal MSE, feature index, and threshold.\n6. **Result Formatting**: Return the best feature index, threshold, and the rounded MSE to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef best_split_mse(X: list | np.ndarray, y: list | np.ndarray) -> list:\n    X = np.asarray(X)\n    y = np.asarray(y)\n    n_samples, n_features = X.shape\n    best_feature = -1\n    best_threshold = None\n    best_mse = float('inf')\n\n    for j in range(n_features):\n        X_j = X[:, j]\n        sorted_indices = np.argsort(X_j)\n        sorted_x = X_j[sorted_indices]\n        sorted_y = y[sorted_indices]\n\n        unique_thresholds = np.unique(sorted_x)\n        if len(unique_thresholds) < 2:\n            continue\n        unique_thresholds_candidates = unique_thresholds[:-1]\n\n        prefix_sum = np.cumsum(sorted_y)\n        prefix_sq_sum = np.cumsum(sorted_y ** 2)\n        total_sum = prefix_sum[-1]\n        total_sq_sum = prefix_sq_sum[-1]\n        n = n_samples\n\n        for t in unique_thresholds_candidates:\n            split_idx = np.searchsorted(sorted_x, t, side='right')\n            if split_idx == 0 or split_idx == n:\n                continue\n\n            len_L = split_idx\n            sum_y_L = prefix_sum[split_idx - 1]\n            sum_sq_L = prefix_sq_sum[split_idx - 1]\n            mean_L = sum_y_L / len_L\n            var_L = (sum_sq_L / len_L) - (mean_L ** 2)\n\n            len_R = n - split_idx\n            sum_y_R = total_sum - sum_y_L\n            sum_sq_R = total_sq_sum - sum_sq_L\n            mean_R = sum_y_R / len_R\n            var_R = (sum_sq_R / len_R) - (mean_R ** 2)\n\n            mse = (len_L / n) * var_L + (len_R / n) * var_R\n\n            if mse < best_mse:\n                best_mse = mse\n                best_feature = j\n                best_threshold = t\n            elif mse == best_mse:\n                if j < best_feature:\n                    best_feature = j\n                    best_threshold = t\n                elif j == best_feature and t < best_threshold:\n                    best_threshold = t\n\n    best_mse_rounded = round(best_mse, 4)\n    return [best_feature, best_threshold, best_mse_rounded]\n```\n\n### Explanation\n1. **Data Conversion**: The input data is converted to NumPy arrays for efficient manipulation.\n2. **Sorting and Thresholds**: For each feature, the data is sorted, and unique thresholds are identified. Only thresholds that ensure non-empty splits are considered.\n3. **Prefix Sums**: Cumulative sums of target values and their squares are precomputed to efficiently calculate variances for potential splits.\n4. **MSE Calculation**: For each threshold, the data is split, and the weighted MSE is computed using the precomputed sums. The best split is tracked based on the minimal MSE, feature index, and threshold.\n5. **Result**: The best feature index, threshold, and rounded MSE are returned, ensuring the solution meets the problem constraints and requirements."}
{"task_id": 598, "completion_id": 0, "completion": "To solve this problem, we need to standardize a numeric data matrix feature-wise such that each feature (column) has zero mean and unit variance. This transformation is known as z-score standardization. We must handle cases where the standard deviation is zero and replace any NaN or infinite values resulting from the transformation with zeros.\n\n### Approach\n1. **Convert Input to NumPy Array**: Convert the input list of lists into a NumPy array for efficient numerical operations.\n2. **Compute Mean and Standard Deviation**: Calculate the mean (\u03bc) and population standard deviation (\u03c3) for each column. Use `ddof=0` for the standard deviation to ensure it's the population standard deviation.\n3. **Adjust Standard Deviation**: If the standard deviation (\u03c3) of a column is zero, replace it with 1 to avoid division by zero. This adjustment ensures that the entire column is set to zero after standardization.\n4. **Standardize Data**: Subtract the mean from each element and divide by the adjusted standard deviation.\n5. **Handle NaN and Infinite Values**: Replace any NaN or infinite values resulting from the transformation with zeros.\n6. **Round and Convert Back**: Round the standardized values to four decimal places and convert the result back to a nested Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef standardize(data: list[list[int | float]]) -> list[list[float]]:\n    \"\"\"Standardise a numeric data matrix feature-wise.\"\"\"\n    data_np = np.array(data, dtype=np.float64)\n    mu = np.mean(data_np, axis=0)\n    sigma = np.std(data_np, axis=0, ddof=0)\n    sigma_adj = np.where(sigma == 0, 1.0, sigma)\n    standardized = (data_np - mu) / sigma_adj\n    standardized = np.nan_to_num(standardized, nan=0.0, posinf=0.0, neginf=0.0)\n    rounded = np.round(standardized, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input data is converted to a NumPy array to leverage efficient vectorized operations.\n2. **Mean and Standard Deviation Calculation**: The mean (\u03bc) and standard deviation (\u03c3) are computed for each column. Using `axis=0` ensures these calculations are column-wise.\n3. **Adjusting Standard Deviation**: Columns with a standard deviation of zero are adjusted to use 1 instead. This avoids division by zero and ensures such columns are set to zero after the transformation.\n4. **Standardization**: Each element in the column is transformed using the formula \\((x - \u03bc) / \u03c3_{\\text{adj}}\\), where \u03c3_adj is the adjusted standard deviation.\n5. **Handling NaN and Infinite Values**: The `np.nan_to_num` function replaces NaN and infinite values with zeros, ensuring the result is clean.\n6. **Rounding and Conversion**: The standardized values are rounded to four decimal places and converted back to a nested list to match the required output format.\n\nThis approach efficiently handles all edge cases, including constant columns and potential NaN or infinite values, ensuring the result is accurate and meets the specified requirements."}
