{"task_id": 0, "completion_id": 0, "solution": "import numpy as np\ndef ridge_regression(data: np.ndarray, alpha: float=0.0) -> list[float]:\n    \"\"\"Compute the closed-form solution for Ridge Regression.\"\"\"\n    X = data[:, :-1]\n    y = data[:, -1]\n    X_b = np.hstack([np.ones((X.shape[0], 1)), X])\n    n_features = X_b.shape[1]\n    A = X_b.T @ X_b + alpha * np.eye(n_features)\n    det = np.linalg.det(A)\n    if np.abs(det) < 1e-12:\n        return -1\n    w = np.linalg.inv(A) @ (X_b.T @ y)\n    return np.round(w, 4).tolist()"}
{"task_id": 1, "completion_id": 0, "solution": "import numpy as np\ndef perceptron_dual(X_train: np.ndarray, y_train: np.ndarray, eta: float=1.0, n_iter: int=10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\"\"\"\n    (n_samples, n_features) = X_train.shape\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    updates = 0\n    gram = X_train @ X_train.T\n    i = 0\n    while updates < n_iter:\n        if i >= n_samples:\n            break\n        activation = np.dot(alpha * y_train, gram[:, i])\n        margin = y_train[i] * (activation + b)\n        if margin <= 0:\n            alpha[i] += eta\n            b += eta * y_train[i]\n            updates += 1\n            i = 0\n        else:\n            i += 1\n    w = np.dot(alpha * y_train, X_train)\n    w_rounded = [round(x, 4) for x in w]\n    b_rounded = round(b, 4)\n    return (w_rounded, b_rounded)"}
{"task_id": 2, "completion_id": 0, "solution": "import numpy as np\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    return {'identity': {'link': lambda mu: mu, 'inv_link': lambda eta: eta, 'deriv': lambda mu: np.ones_like(mu)}, 'log': {'link': np.log, 'inv_link': np.exp, 'deriv': lambda mu: 1.0 / mu}, 'logit': {'link': lambda mu: np.log(mu / (1 - mu)), 'inv_link': lambda eta: 1 / (1 + np.exp(-eta)), 'deriv': lambda mu: 1 / (mu * (1 - mu))}}"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\"\"\"\n    np.random.seed(0)\n    n_samples = data.shape[0]\n    depths = [0] * n_samples\n    indices = list(range(n_samples))\n\n    def process_node(current_indices, current_depth):\n        if len(current_indices) < 3:\n            for i in current_indices:\n                depths[i] = current_depth\n            return\n        f = np.random.randint(data.shape[1])\n        feature_values = data[current_indices, f]\n        down = np.min(feature_values)\n        up = np.max(feature_values)\n        v = np.random.uniform(down, up)\n        left = []\n        right = []\n        for i in current_indices:\n            if data[i, f] <= v:\n                left.append(i)\n            else:\n                right.append(i)\n        if not left or not right:\n            for i in current_indices:\n                depths[i] = current_depth\n            return\n        process_node(left, current_depth + 1)\n        process_node(right, current_depth + 1)\n    process_node(indices, 0)\n    return depths"}
{"task_id": 7, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\"\"\"\n    n_trans = len(transactions)\n    if n_trans == 0 or minsup <= 0:\n        return {}\n    min_support = math.ceil(minsup * n_trans)\n    if min_support <= 0:\n        return {}\n    preprocessed = [set(t) for t in transactions]\n    item_counter = Counter()\n    for t in preprocessed:\n        for item in t:\n            item_counter[item] += 1\n    F_prev = {(item,): count for (item, count) in item_counter.items() if count >= min_support}\n    frequent_itemsets = dict(F_prev)\n    k = 2\n    while F_prev:\n        itemsets = sorted(F_prev.keys())\n        Ck = []\n        for i in range(len(itemsets)):\n            itemset1 = itemsets[i]\n            for j in range(i + 1, len(itemsets)):\n                itemset2 = itemsets[j]\n                if itemset1[:-1] == itemset2[:-1] and itemset1[-1] < itemset2[-1]:\n                    new_candidate = itemset1 + (itemset2[-1],)\n                    valid = True\n                    for subset in combinations(new_candidate, k - 1):\n                        if subset not in F_prev:\n                            valid = False\n                            break\n                    if valid:\n                        Ck.append(new_candidate)\n        Ck = list(set(Ck))\n        Fk = {}\n        for candidate in Ck:\n            count = 0\n            for t in preprocessed:\n                if all((item in t for item in candidate)):\n                    count += 1\n            if count >= min_support:\n                Fk[candidate] = count\n        if not Fk:\n            break\n        frequent_itemsets.update(Fk)\n        F_prev = Fk\n        k += 1\n    return frequent_itemsets"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data: np.ndarray, k: int, epsilon: float=0.001, max_iter: int=2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\"\"\"\n    if data.size == 0 or k == 0:\n        return ([], [])\n    centers = data[:k].copy()\n    for _ in range(max_iter):\n        distances = np.sum((data[:, np.newaxis] - centers) ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centers = np.zeros_like(centers)\n        for j in range(k):\n            mask = labels == j\n            if np.sum(mask) == 0:\n                new_centers[j] = data[np.random.randint(data.shape[0])]\n            else:\n                new_centers[j] = data[mask].mean(axis=0)\n        delta = np.max(np.linalg.norm(new_centers - centers, axis=1))\n        if delta < epsilon:\n            centers = new_centers.copy()\n            break\n        centers = new_centers.copy()\n    centers = np.round(centers, 4)\n    distances = np.sum((data[:, np.newaxis] - centers) ** 2, axis=2)\n    labels = np.argmin(distances, axis=1).tolist()\n    centers_list = [center.tolist() for center in centers]\n    return (centers_list, labels)"}
{"task_id": 13, "completion_id": 0, "solution": "import numpy as np\nfrom collections import defaultdict\ndef knn_recommend(data: np.ndarray, user_ind: int, k: int, criterion: str='cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\"\"\"\n    n_items = data.shape[1]\n    S = np.zeros((n_items, n_items))\n    for i in range(n_items):\n        for j in range(i + 1, n_items):\n            common_rated = (data[:, i] > 0) & (data[:, j] > 0)\n            if not np.any(common_rated):\n                S[i, j] = S[j, i] = 0.0\n                continue\n            v1 = data[common_rated, i]\n            v2 = data[common_rated, j]\n            if criterion == 'cosine':\n                std1 = np.std(v1, ddof=1)\n                if std1 > 0.001:\n                    v1 = v1 - np.mean(v1)\n                std2 = np.std(v2, ddof=1)\n                if std2 > 0.001:\n                    v2 = v2 - np.mean(v2)\n                norm1 = np.linalg.norm(v1)\n                norm2 = np.linalg.norm(v2)\n                if norm1 == 0 or norm2 == 0:\n                    sim = 0.0\n                else:\n                    sim = np.dot(v1, v2) / (norm1 * norm2)\n            elif criterion == 'pearson':\n                if len(v1) < 2:\n                    sim = 0.0\n                else:\n                    corr = np.corrcoef(v1, v2)[0, 1]\n                    sim = 0.0 if np.isnan(corr) else corr\n            else:\n                sim = 0.0\n            S[i, j] = S[j, i] = sim\n    user_ratings = data[user_ind]\n    rated_items = np.where(user_ratings > 0)[0].tolist()\n    if len(rated_items) == n_items:\n        return []\n    unrated = [t for t in range(n_items) if user_ratings[t] == 0]\n    scores = []\n    for t in unrated:\n        sum_num = 0.0\n        sum_den = 0.0\n        for i in rated_items:\n            sim = S[t, i]\n            sum_num += user_ratings[i] * sim\n            sum_den += abs(sim)\n        score = sum_num / sum_den if sum_den != 0 else 0.0\n        scores.append((score, t))\n    sorted_scores = sorted(scores, key=lambda x: (-x[0], x[1]))\n    result = [t for (score, t) in sorted_scores[:k]]\n    return result"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef best_split(X: np.ndarray, g: np.ndarray, h: np.ndarray, gamma: float, lam: float) -> tuple[int, float] | None:\n    (n_samples, n_features) = X.shape\n    best_gain = -np.inf\n    best_feature = None\n    best_threshold = None\n    for feature in range(n_features):\n        X_feature = X[:, feature]\n        sorted_indices = np.argsort(X_feature)\n        X_sorted = X_feature[sorted_indices]\n        g_sorted = g[sorted_indices]\n        h_sorted = h[sorted_indices]\n        cum_g = np.concatenate([[0], np.cumsum(g_sorted)])\n        cum_h = np.concatenate([[0], np.cumsum(h_sorted)])\n        total_g = cum_g[-1]\n        total_h = cum_h[-1]\n        denominator_parent = total_h + lam\n        if denominator_parent == 0:\n            continue\n        parent_loss = -total_g ** 2 / (2 * denominator_parent) + gamma\n        unique_vals = np.unique(X_sorted)\n        for t in unique_vals:\n            split_idx = np.searchsorted(X_sorted, t, side='right')\n            left_count = split_idx\n            right_count = n_samples - split_idx\n            if left_count < 2 or right_count < 2:\n                continue\n            sum_g_left = cum_g[split_idx]\n            sum_h_left = cum_h[split_idx]\n            sum_g_right = total_g - sum_g_left\n            sum_h_right = total_h - sum_h_left\n            denominator_left = sum_h_left + lam\n            denominator_right = sum_h_right + lam\n            if denominator_left == 0 or denominator_right == 0:\n                continue\n            left_loss = -sum_g_left ** 2 / (2 * denominator_left) + gamma\n            right_loss = -sum_g_right ** 2 / (2 * denominator_right) + gamma\n            current_gain = parent_loss - (left_loss + right_loss)\n            if current_gain > best_gain:\n                best_gain = current_gain\n                best_feature = feature\n                best_threshold = t\n    if best_gain > 0:\n        return (best_feature, best_threshold)\n    else:\n        return None"}
{"task_id": 20, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)):\n        sig = 1 / (1 + np.exp(-x))\n        grad = sig * (1 - sig)\n        return (round(sig, 4), round(grad, 4))\n    else:\n        if isinstance(x, list):\n            arr = np.array(x)\n        else:\n            arr = x\n        if arr.ndim == 0:\n            sig = 1 / (1 + np.exp(-arr))\n            grad = sig * (1 - sig)\n            return (round(sig.item(), 4), round(grad.item(), 4))\n        else:\n            sig = 1 / (1 + np.exp(-arr))\n            grad = sig * (1 - sig)\n            sig_rounded = np.round(sig, 4).tolist()\n            grad_rounded = np.round(grad, 4).tolist()\n            return (sig_rounded, grad_rounded)"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_1d_predict(x_train: list[float], y_train: list[int], x_test: list[float], epsilon: float=0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\"\"\"\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    N = len(x_train)\n    if not np.all(np.isin(y_train, [-1, 1])):\n        raise ValueError('y_train must contain only 1 or -1')\n    sorted_x = np.sort(x_train)\n    if len(sorted_x) == 1:\n        thetas = [sorted_x[0]]\n    else:\n        thetas = [(sorted_x[i] + sorted_x[i + 1]) / 2 for i in range(len(sorted_x) - 1)]\n    thetas = np.array(thetas)\n    D = np.ones(N) / N\n    stumps = []\n    while True:\n        best_err = np.inf\n        best_d = None\n        best_theta = None\n        best_preds = None\n        for theta in thetas:\n            for d in [0, 1]:\n                if d == 0:\n                    preds = np.where(x_train <= theta, 1, -1)\n                else:\n                    preds = np.where(x_train > theta, 1, -1)\n                err = np.sum(D * (preds != y_train))\n                if err < best_err:\n                    best_err = err\n                    best_d = d\n                    best_theta = theta\n                    best_preds = preds\n        if best_err == 0:\n            alpha = np.inf\n        else:\n            alpha = 0.5 * np.log((1 - best_err) / best_err)\n        stumps.append((alpha, best_d, best_theta))\n        correct = best_preds == y_train\n        factors = np.where(correct, np.exp(-alpha), np.exp(alpha))\n        D *= factors\n        D /= D.sum()\n        current_preds = []\n        for x in x_train:\n            sum_alpha = 0.0\n            for (a, d_stump, theta_stump) in stumps:\n                if d_stump == 0:\n                    h = 1 if x <= theta_stump else -1\n                else:\n                    h = 1 if x > theta_stump else -1\n                sum_alpha += a * h\n            current_pred = 1 if sum_alpha >= 0 else -1\n            current_preds.append(current_pred)\n        training_error = np.mean(np.array(current_preds) != y_train)\n        if training_error <= epsilon:\n            break\n    y_pred = []\n    for x in x_test:\n        sum_alpha = 0.0\n        for (a, d, theta) in stumps:\n            if d == 0:\n                h = 1 if x <= theta else -1\n            else:\n                h = 1 if x > theta else -1\n            sum_alpha += a * h\n        pred = 1 if sum_alpha >= 0 else -1\n        y_pred.append(pred)\n    return y_pred"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_svm_predict(X_train: np.ndarray, y_train: np.ndarray, alpha: np.ndarray, b: float, gamma: float, X_test: np.ndarray) -> list[int]:\n    X_train_sq = np.sum(X_train ** 2, axis=1)\n    X_test_sq = np.sum(X_test ** 2, axis=1)\n    cross_term = X_train @ X_test.T\n    squared_distances = X_train_sq[:, np.newaxis] + X_test_sq[np.newaxis, :] - 2 * cross_term\n    K = np.exp(-gamma * squared_distances)\n    decision_values = np.dot(alpha * y_train, K) + b\n    predicted_labels = np.where(decision_values >= 0, 1, -1).astype(int).tolist()\n    return predicted_labels"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    X_np = np.array(X)\n    (m, n) = X_np.shape\n    if k < 1 or k > min(m, n):\n        return -1\n    (U, S, Vt) = np.linalg.svd(X_np, full_matrices=False)\n    U_k = U[:, :k]\n    S_k = S[:k]\n    Vt_k = Vt[:k, :]\n    X_hat = U_k @ np.diag(S_k) @ Vt_k\n    mse = np.sum(S[k:] ** 2) / (m * n)\n    X_hat_rounded = np.round(X_hat, 4)\n    X_hat_list = X_hat_rounded.tolist()\n    mse_rounded = round(mse, 4)\n    return (X_hat_list, mse_rounded)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef predict_boosting_tree(x_train: list[float] | np.ndarray, y_train: list[float] | np.ndarray, x_query: float, epsilon: float=0.01) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\"\"\"\n    x_train = np.asarray(x_train)\n    y_train = np.asarray(y_train)\n    sorted_indices = np.argsort(x_train)\n    x_sorted = x_train[sorted_indices]\n    y_sorted = y_train[sorted_indices]\n    split_candidates = []\n    n = len(x_sorted)\n    for i in range(n - 1):\n        split = (x_sorted[i] + x_sorted[i + 1]) / 2\n        split_candidates.append(split)\n    valid_splits = []\n    for s in split_candidates:\n        mask_left = x_sorted <= s\n        mask_right = x_sorted > s\n        if np.any(mask_left) and np.any(mask_right):\n            valid_splits.append(s)\n    ensemble = []\n    residuals = y_sorted.copy()\n    current_rss = np.sum(residuals ** 2)\n    if valid_splits:\n        while current_rss > epsilon:\n            best_split = None\n            best_c1 = None\n            best_c2 = None\n            best_new_rss = np.inf\n            for s in valid_splits:\n                mask_left = x_sorted <= s\n                mask_right = ~mask_left\n                c1 = np.mean(residuals[mask_left])\n                c2 = np.mean(residuals[mask_right])\n                predictions = np.where(mask_left, c1, c2)\n                new_residuals = residuals - predictions\n                new_rss = np.sum(new_residuals ** 2)\n                if new_rss < best_new_rss:\n                    best_new_rss = new_rss\n                    best_split = s\n                    best_c1 = c1\n                    best_c2 = c2\n            if best_split is None:\n                break\n            ensemble.append((best_split, best_c1, best_c2))\n            mask_left = x_sorted <= best_split\n            predictions = np.where(mask_left, best_c1, best_c2)\n            residuals -= predictions\n            current_rss = best_new_rss\n            if current_rss <= epsilon:\n                break\n    prediction = 0.0\n    for stump in ensemble:\n        (s, c1, c2) = stump\n        if x_query <= s:\n            prediction += c1\n        else:\n            prediction += c2\n    return round(prediction, 4)"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef random_binary_tensor(shape: tuple[int, ...], sparsity: float=0.5, seed: int | None=None) -> np.ndarray | int:\n    \"\"\"Generates a binary tensor with specified shape and sparsity.\n\n    The tensor is filled with 0.0s and 1.0s, where each element has a probability\n    equal to 'sparsity' of being 1.0. If a seed is provided, the results are reproducible.\n\n    Args:\n        shape: The dimensions of the output tensor.\n        sparsity: Probability of an element being 1.0; must be in [0, 1].\n        seed: Optional seed for the random number generator.\n\n    Returns:\n        A numpy.ndarray with elements 0.0 and 1.0, or -1 if sparsity is invalid.\n    \"\"\"\n    if not 0.0 <= sparsity <= 1.0:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    threshold = 1.0 - sparsity\n    random_tensor = np.random.rand(*shape)\n    binary_tensor = np.where(random_tensor >= threshold, 1.0, 0.0)\n    return binary_tensor"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\"\"\"\n    n_samples = data.shape[0]\n    k = n_samples // 2\n    col = data[:, d]\n    partitioned_indices = np.argpartition(col, k)\n    pivot_idx = partitioned_indices[k]\n    med_val = col[pivot_idx]\n    left_mask = col < med_val\n    right_mask = col > med_val\n    left_indices = np.flatnonzero(left_mask).tolist()\n    right_indices = np.flatnonzero(right_mask).tolist()\n    left_indices.sort()\n    right_indices.sort()\n    return (pivot_idx, left_indices, right_indices)"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool=False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    assert x.ndim == 1, 'x must be one-dimensional'\n    assert stride >= 1, 'stride must be >= 1'\n    assert len(x) >= frame_width, 'x is shorter than frame_width'\n    n_frames = (len(x) - frame_width) // stride + 1\n    new_shape = (n_frames, frame_width)\n    new_strides = (stride * x.strides[0], x.strides[0])\n    framed = as_strided(x, shape=new_shape, strides=new_strides)\n    if not writeable:\n        framed.flags.writeable = False\n    return framed"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef is_stochastic(X: list[list[float]] | 'np.ndarray') -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    if isinstance(X, list):\n        for row in X:\n            if not isinstance(row, list):\n                return False\n        if X:\n            first_len = len(X[0])\n            for row in X:\n                if len(row) != first_len:\n                    return False\n        try:\n            arr = np.array(X)\n        except:\n            return False\n    elif isinstance(X, np.ndarray):\n        arr = X\n    else:\n        return False\n    if arr.ndim != 2:\n        return False\n    if not np.issubdtype(arr.dtype, np.number):\n        return False\n    if (arr < 0).any() or (arr > 1).any():\n        return False\n    row_sums = arr.sum(axis=1)\n    if not np.allclose(row_sums, 1.0, atol=1e-08):\n        return False\n    return True"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef rms_prop(X: np.ndarray, y: np.ndarray, epsilon: float=0.0001, max_iter: int=10000, eta: float=0.01, rho: float=0.9, batch_size: int=32, eps_station: float=1e-08) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\"\"\"\n    (n, d) = (X.shape[0], X.shape[1])\n    w = np.zeros(d)\n    s = np.zeros_like(w)\n    start = 0\n    for _ in range(max_iter):\n        if n < batch_size:\n            X_batch = X\n            y_batch = y\n            m = n\n        else:\n            indices = (start + np.arange(batch_size)) % n\n            X_batch = X[indices]\n            y_batch = y[indices]\n            m = batch_size\n            start = (start + batch_size) % n\n        residual_batch = X_batch @ w - y_batch\n        g_batch = X_batch.T @ residual_batch / m\n        s = rho * s + (1 - rho) * g_batch ** 2\n        w -= eta * g_batch / (np.sqrt(s) + eps_station)\n        residual_full = X @ w - y\n        g_full = X.T @ residual_full / n\n        g_norm = np.linalg.norm(g_full)\n        if g_norm < epsilon:\n            break\n    return np.round(w, 4).tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef softplus(x, deriv: bool=False):\n    \"\"\"Compute the SoftPlus activation function or its gradient (sigmoid) with numerical stability.\"\"\"\n    is_scalar = not isinstance(x, (list, np.ndarray))\n    x_np = np.array([x]) if is_scalar else np.asarray(x)\n    if deriv:\n        sigmoid = np.where(x_np >= 0, 1.0 / (1.0 + np.exp(-x_np)), np.exp(x_np) / (1.0 + np.exp(x_np)))\n        output = sigmoid\n    else:\n        mask = x_np > 0\n        softplus_vals = np.zeros_like(x_np)\n        x_pos = x_np[mask]\n        softplus_vals[mask] = x_pos + np.log1p(np.exp(-x_pos))\n        x_neg = x_np[~mask]\n        softplus_vals[~mask] = np.log1p(np.exp(x_neg))\n        output = softplus_vals\n    output_rounded = np.round(output, 4)\n    return output_rounded.item() if is_scalar else output_rounded.tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    if len(sequence) < N:\n        return round(0.0, 4)\n    corpus_ngrams = [tuple(corpus[i:i + N]) for i in range(len(corpus) - N + 1)]\n    count_ngrams = Counter(corpus_ngrams)\n    count_prefix = None\n    if N > 1:\n        corpus_prefix_ngrams = [tuple(corpus[i:i + (N - 1)]) for i in range(len(corpus) - (N - 1) + 1)]\n        count_prefix = Counter(corpus_prefix_ngrams)\n    total_log = 0.0\n    for i in range(len(sequence) - N + 1):\n        current_ngram = tuple(sequence[i:i + N])\n        numerator = count_ngrams.get(current_ngram, 0)\n        if numerator == 0:\n            return float('-inf')\n        if N == 1:\n            denominator = len(corpus)\n        else:\n            prefix = current_ngram[:-1]\n            denominator = count_prefix.get(prefix, 0)\n        if denominator == 0:\n            return float('-inf')\n        prob = numerator / denominator\n        total_log += np.log(prob)\n    return round(total_log, 4)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\ndef lasso_regression(X: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray', degree: int, reg_factor: float, n_iterations: int=1000, tol: float=1e-06) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model with polynomial features using coordinate descent.\"\"\"\n    X = np.asarray(X).flatten()\n    y = np.asarray(y).flatten()\n    m = X.shape[0]\n    if m == 0:\n        return [0.0] * (degree + 1)\n    X_poly = np.vander(X, degree + 1, increasing=True)\n    w = np.zeros(degree + 1)\n    residual = y - X_poly @ w\n    for _ in range(n_iterations):\n        max_delta = 0.0\n        sum_other_terms = X_poly[:, 1:] @ w[1:]\n        w0_new = np.mean(y - sum_other_terms)\n        delta_w0 = abs(w0_new - w[0])\n        if delta_w0 > max_delta:\n            max_delta = delta_w0\n        residual -= w0_new - w[0]\n        w[0] = w0_new\n        for j in range(1, degree + 1):\n            Xj = X_poly[:, j]\n            Xj_squared_sum = Xj @ Xj\n            if Xj_squared_sum == 0:\n                new_wj = 0.0\n            else:\n                rho_j = Xj @ residual + w[j] * Xj_squared_sum\n                if abs(rho_j) <= reg_factor:\n                    new_wj = 0.0\n                else:\n                    new_wj = np.sign(rho_j) * (abs(rho_j) - reg_factor) / Xj_squared_sum\n            delta = abs(new_wj - w[j])\n            if delta > max_delta:\n                max_delta = delta\n            residual += (w[j] - new_wj) * Xj\n            w[j] = new_wj\n        if max_delta < tol:\n            break\n    return [round(coef, 4) for coef in w]"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"Compute the backward probability vector beta_t for the given HMM parameters and observation sequence.\"\"\"\n    N = len(A)\n    T = len(obs)\n    if t == T - 1:\n        return [round(1.0, 4) for _ in range(N)]\n    beta_prev = [1.0 for _ in range(N)]\n    for s in range(T - 2, t - 1, -1):\n        o = obs[s + 1]\n        beta_current = []\n        for i in range(N):\n            total = 0.0\n            for j in range(N):\n                total += A[i][j] * B[j][o] * beta_prev[j]\n            beta_current.append(total)\n        beta_prev = beta_current\n    return [round(x, 4) for x in beta_prev]"}
{"task_id": 65, "completion_id": 0, "solution": "import numpy as np\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or (not pi) or (not obs):\n        return 0.0\n    A = np.array(A, dtype=np.float64)\n    B = np.array(B, dtype=np.float64)\n    pi = np.array(pi, dtype=np.float64)\n    obs = np.array(obs, dtype=int)\n    T = len(obs)\n    N = len(pi)\n    if T == 0:\n        return 0.0\n    beta_prev = np.ones(N, dtype=np.float64)\n    for t in reversed(range(T - 1)):\n        o = obs[t + 1]\n        temp = B[:, o] * beta_prev\n        beta_prev = A.dot(temp)\n    total = np.sum(pi * B[:, obs[0]] * beta_prev)\n    return round(total, 6)"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef forward_algorithm(S: list[float], A: list[list[float]], B: list[list[float]], observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    n = len(S)\n    if n == 0:\n        return -1.0\n    if len(A) != n:\n        return -1.0\n    for row in A:\n        if len(row) != n:\n            return -1.0\n    if len(B) != n:\n        return -1.0\n    if n == 0:\n        return -1.0\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1.0\n    if not observations:\n        return -1.0\n    for obs in observations:\n        if not isinstance(obs, int) or obs < 0 or obs >= m:\n            return -1.0\n    T = len(observations)\n    alpha_prev = [S[i] * B[i][observations[0]] for i in range(n)]\n    for t in range(1, T):\n        current_obs = observations[t]\n        alpha_current = []\n        for j in range(n):\n            total = 0.0\n            for i in range(n):\n                total += alpha_prev[i] * A[i][j]\n            alpha_current.append(total * B[j][current_obs])\n        alpha_prev = alpha_current\n    final_prob = sum(alpha_prev)\n    return round(final_prob, 4)"}
{"task_id": 70, "completion_id": 0, "solution": "from collections import Counter, defaultdict\nclass TreeNode:\n\n    def __init__(self, item, count, parent):\n        self.item = item\n        self.count = count\n        self.parent = parent\n        self.children = {}\n        self.node_link = None\ndef build_fp_tree(transactions_with_counts, min_support):\n    item_counts = defaultdict(int)\n    for (transaction, count) in transactions_with_counts:\n        for item in transaction:\n            item_counts[item] += count\n    frequent_items = {item for (item, cnt) in item_counts.items() if cnt >= min_support}\n    if not frequent_items:\n        return (None, None)\n    sorted_items = sorted(frequent_items, key=lambda x: (-item_counts[x], x))\n    root = TreeNode(None, 0, None)\n    header_table = {}\n    for item in sorted_items:\n        header_table[item] = [item_counts[item], None]\n    for (transaction, count) in transactions_with_counts:\n        sorted_transaction = [item for item in sorted_items if item in transaction]\n        if not sorted_transaction:\n            continue\n        current_node = root\n        for item in sorted_transaction:\n            if item in current_node.children:\n                current_node = current_node.children[item]\n                current_node.count += count\n            else:\n                new_node = TreeNode(item, count, current_node)\n                current_node.children[item] = new_node\n                current_node = new_node\n                if header_table[item][1] is None:\n                    header_table[item][1] = new_node\n                else:\n                    last_node = header_table[item][1]\n                    while last_node.node_link is not None:\n                        last_node = last_node.node_link\n                    last_node.node_link = new_node\n    return (root, header_table)\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    if not transactions or min_support < 1:\n        return []\n    item_counts = defaultdict(int)\n    for transaction in transactions:\n        unique_items = set(transaction)\n        for item in unique_items:\n            item_counts[item] += 1\n    frequent_items = {item: count for (item, count) in item_counts.items() if count >= min_support}\n    if not frequent_items:\n        return []\n    sorted_items = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n    filtered_transactions = []\n    for trans in transactions:\n        unique_trans = list(set(trans))\n        filtered = [item for item in unique_trans if item in frequent_items]\n        if filtered:\n            filtered.sort(key=lambda x: sorted_items.index(x))\n            filtered_transactions.append(filtered)\n    if not filtered_transactions:\n        return []\n    transactions_with_counts = [(t, 1) for t in filtered_transactions]\n    (root, header_table) = build_fp_tree(transactions_with_counts, min_support)\n    if not header_table:\n        return []\n    header_list = sorted(header_table.items(), key=lambda x: (-x[1][0], x[0]))\n    frequent_itemsets = []\n\n    def mine_tree(header_list, prefix, frequent_itemsets):\n        for (item, (support, head_node)) in reversed(header_list):\n            new_prefix = prefix.copy()\n            new_prefix.append(item)\n            new_prefix_sorted = sorted(new_prefix)\n            frequent_itemsets.append(tuple(new_prefix_sorted))\n            conditional_patterns = []\n            node = head_node\n            while node is not None:\n                path = []\n                current = node.parent\n                while current.parent is not None:\n                    path.append(current.item)\n                    current = current.parent\n                path.reverse()\n                if path:\n                    conditional_patterns.append((path, node.count))\n                node = node.node_link\n            if conditional_patterns:\n                (cond_root, cond_header_table) = build_fp_tree(conditional_patterns, min_support)\n                if cond_header_table is not None:\n                    cond_header_list = sorted(cond_header_table.items(), key=lambda x: (-x[1][0], x[0]))\n                    mine_tree(cond_header_list, new_prefix_sorted, frequent_itemsets)\n    mine_tree(header_list, [], frequent_itemsets)\n    unique_itemsets = set(frequent_itemsets)\n    sorted_itemsets = [list(items) for items in unique_itemsets]\n    sorted_itemsets.sort(key=lambda x: (len(x), x))\n    return sorted_itemsets"}
{"task_id": 75, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\"\"\"\n    n_train = len(X_train)\n    n_test = len(X_test)\n    if not isinstance(k, int) or k <= 0 or k > n_train:\n        return [-1] * n_test\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n    predictions = []\n    for x in X_test_np:\n        distances = np.sum((X_train_np - x) ** 2, axis=1)\n        nearest_indices = np.argsort(distances)[:k]\n        nearest_labels = y_train_np[nearest_indices]\n        label_counts = {}\n        for label in nearest_labels:\n            label_counts[label] = label_counts.get(label, 0) + 1\n        max_count = max(label_counts.values())\n        candidates = [label for (label, count) in label_counts.items() if count == max_count]\n        predicted_label = min(candidates)\n        predictions.append(predicted_label)\n    return predictions"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_loss(y: list | np.ndarray, y_pred: list | np.ndarray) -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y)\n    y_pred = np.asarray(y_pred)\n    eps = np.finfo(y_pred.dtype).eps\n    log_probs = np.log(y_pred + eps)\n    loss = -np.sum(y * log_probs)\n    return round(loss.item(), 4)"}
{"task_id": 77, "completion_id": 0, "solution": "import numpy as np\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> tuple[list[list[float]], list]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\"\"\"\n    L = len(parameters) // 2\n    caches = []\n    A_prev = X\n    for l in range(1, L + 1):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        Z = np.dot(W, A_prev) + b\n        if l == L:\n            A = 1 / (1 + np.exp(-Z))\n        else:\n            A = np.maximum(0, Z)\n        linear_cache = (A_prev, W, b)\n        activation_cache = Z\n        caches.append((linear_cache, activation_cache))\n        A_prev = A\n    AL_rounded = np.round(A_prev, 4)\n    return (AL_rounded.tolist(), caches)"}
{"task_id": 81, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    m = A2.size\n    log_probs = Y * np.log(A2_clipped) + (1 - Y) * np.log(1 - A2_clipped)\n    cost = -np.sum(log_probs) / m\n    return round(cost.item(), 6)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef epsilon_soft(Q: list[float] | np.ndarray, epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list must contain the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    Q_array = np.asarray(Q)\n    n = Q_array.size\n    if n == 0:\n        return []\n    a_star = np.argmax(Q_array)\n    base = epsilon / n\n    probs = np.full(n, base, dtype=np.float64)\n    probs[a_star] += 1 - epsilon\n    rounded_probs = np.round(probs, 4)\n    return rounded_probs.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "from collections import Counter\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    samples = zip(*predictions)\n    aggregated = []\n    for sample_votes in samples:\n        counts = Counter(sample_votes)\n        max_count = max(counts.values())\n        candidates = [k for (k, v) in counts.items() if v == max_count]\n        candidates_sorted = sorted(candidates)\n        aggregated.append(candidates_sorted[0])\n    return aggregated"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        s = np.logaddexp(0.0, z)\n        rounded = np.round(s, 4)\n        return float(rounded)\n    else:\n        z_array = np.asarray(z)\n        s = np.logaddexp(0.0, z_array)\n        s_rounded = np.round(s, 4)\n        return s_rounded.tolist()"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Any\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\"\"\"\n    ev_estimates = None\n    if hasattr(policy, 'ev_estimates'):\n        ev_estimates = policy.ev_estimates\n    elif isinstance(policy, dict) and 'ev_estimates' in policy:\n        ev_estimates = policy['ev_estimates']\n    else:\n        return np.nan\n    if not isinstance(ev_estimates, dict) or len(ev_estimates) == 0:\n        return np.nan\n    arm_evs = None\n    if hasattr(bandit, 'arm_evs'):\n        arm_evs = bandit.arm_evs\n    elif isinstance(bandit, dict) and 'arm_evs' in bandit:\n        arm_evs = bandit['arm_evs']\n    else:\n        return np.nan\n    if not isinstance(arm_evs, (list, tuple)):\n        return np.nan\n    n_arms = len(arm_evs)\n    for i in range(n_arms):\n        if i not in ev_estimates:\n            return np.nan\n    squared_errors = []\n    for i in range(n_arms):\n        true_val = arm_evs[i]\n        estimate_val = ev_estimates[i]\n        squared_error = (estimate_val - true_val) ** 2\n        squared_errors.append(squared_error)\n    average_mse = sum(squared_errors) / n_arms\n    return round(average_mse, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "import math\nfrom collections import Counter, defaultdict\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float=1.0) -> float:\n    training_vocab = set()\n    for sentence in corpus:\n        tokens = sentence.split()\n        training_vocab.update(tokens)\n    V_size = len(training_vocab) + 1\n    ngram_counts = defaultdict(Counter)\n    for sentence in corpus:\n        tokens = sentence.split()\n        for n in range(1, N + 1):\n            for i in range(len(tokens) - n + 1):\n                ngram = tuple(tokens[i:i + n])\n                ngram_counts[n][ngram] += 1\n    sequence_tokens = sequence.split()\n    processed_tokens = [word if word in training_vocab else '<UNK>' for word in sequence_tokens]\n    if len(processed_tokens) < N:\n        return 0.0\n    total_log_prob = 0.0\n    for i in range(len(processed_tokens) - N + 1):\n        current_ngram = tuple(processed_tokens[i:i + N])\n        context = current_ngram[:-1]\n        count_ngram = ngram_counts[N].get(current_ngram, 0)\n        if N == 1:\n            count_context = sum(ngram_counts[1].values())\n        else:\n            count_context = ngram_counts[N - 1].get(context, 0)\n        numerator = count_ngram + K\n        denominator = count_context + K * V_size\n        prob = numerator / denominator\n        total_log_prob += math.log(prob)\n    return round(total_log_prob, 4)"}
{"task_id": 108, "completion_id": 0, "solution": "import numpy as np\ndef als_factorization(X: np.ndarray, K: int, alpha: float=1.0, max_iter: int=200, tol: float=0.0001) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\"\"\"\n    np.random.seed(0)\n    (N, M) = X.shape\n    W = np.random.randn(N, K)\n    H = np.random.randn(K, M)\n    best_loss = float('inf')\n    best_X_hat = None\n    for _ in range(max_iter):\n        H_HT = H @ H.T\n        reg_W = H_HT + alpha * np.eye(K)\n        inv_reg_W = np.linalg.inv(reg_W)\n        W = X @ H.T @ inv_reg_W\n        WT_W = W.T @ W\n        reg_H = WT_W + alpha * np.eye(K)\n        inv_reg_H = np.linalg.inv(reg_H)\n        H = inv_reg_H @ (W.T @ X)\n        X_hat = W @ H\n        frobenius_loss = np.linalg.norm(X - X_hat, 'fro') ** 2\n        reg_term = alpha * (np.linalg.norm(W, 'fro') ** 2 + np.linalg.norm(H, 'fro') ** 2)\n        current_loss = frobenius_loss + reg_term\n        if current_loss < best_loss:\n            best_loss = current_loss\n            best_X_hat = X_hat.copy()\n        if best_loss <= tol:\n            break\n    return best_X_hat.round(4).tolist()"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef kmeans(X: np.ndarray, K: int, max_iter: int=100, random_state: int | None=None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\"\"\"\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n    (m, n) = X.shape\n    centroids = []\n    indices = np.arange(m)\n    first_idx = np.random.choice(indices)\n    centroids.append(X[first_idx].copy())\n    for _ in range(1, K):\n        current_centroids = np.array(centroids)\n        dist_sq = np.sum((X[:, np.newaxis, :] - current_centroids) ** 2, axis=2)\n        min_dist_sq = np.min(dist_sq, axis=1)\n        sum_D = min_dist_sq.sum()\n        if sum_D == 0:\n            probabilities = np.ones(m) / m\n        else:\n            probabilities = min_dist_sq / sum_D\n        next_idx = np.random.choice(indices, p=probabilities)\n        centroids.append(X[next_idx].copy())\n    centroids = np.array(centroids)\n    prev_labels = None\n    for _ in range(max_iter):\n        dist_sq = ((X[:, np.newaxis, :] - centroids) ** 2).sum(axis=2)\n        labels = np.argmin(dist_sq, axis=1)\n        if prev_labels is not None and np.array_equal(labels, prev_labels):\n            break\n        new_centroids = []\n        for i in range(K):\n            mask = labels == i\n            if np.sum(mask) == 0:\n                new_centroid = centroids[i].copy()\n            else:\n                new_centroid = X[mask].mean(axis=0)\n            new_centroids.append(new_centroid)\n        centroids = np.array(new_centroids)\n        prev_labels = labels.copy()\n    centroids_rounded = np.round(centroids, decimals=4)\n    centroids_list = centroids_rounded.tolist()\n    centroids_list.sort(key=lambda x: (x[0], x))\n    return centroids_list"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\nTIME_STEPS = 20\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Encodes a string into a fixed-length list of integer token ids.\"\"\"\n    truncated_text = text[:time_steps]\n    encoded = [vocabulary.get(c, 0) for c in truncated_text]\n    pad_length = time_steps - len(encoded)\n    encoded += [0] * pad_length\n    return encoded\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer token ids back to a string, ignoring padding tokens (0).\"\"\"\n    return ''.join([inverse_vocab[idx] for idx in indices if idx != 0])\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encodes the input text, runs model prediction, and decodes the output.\"\"\"\n    encoded_sequence = string_to_int(text, TIME_STEPS, input_vocabulary)\n    input_batch = np.array([encoded_sequence])\n    prediction = model.predict(input_batch)\n    predicted_ids = np.argmax(prediction, axis=-1).squeeze()\n    return int_to_string(predicted_ids, inv_output_vocabulary)"}
{"task_id": 113, "completion_id": 0, "solution": "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str]\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        output_str = ''.join(output_chars)\n        print(f'input:  {example}')\n        print(f'output: {output_str}')\n        predictions.append(output_str)\n    return predictions"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\"\"\"\n    m = X.shape[0]\n    y = y.ravel()\n    w = w.ravel()\n    z = X.dot(w)\n    p = 1 / (1 + np.exp(-z))\n    epsilon = 1e-20\n    p_clipped = np.clip(p, epsilon, 1 - epsilon)\n    loss_terms = y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped)\n    loss = -np.sum(loss_terms) / m\n    loss_rounded = np.round(loss, 4).item()\n    diff = p - y\n    gradient = X.T.dot(diff) / m\n    gradient_rounded = np.round(gradient, 4)\n    gradient_list = gradient_rounded.reshape(-1, 1).tolist()\n    return (loss_rounded, gradient_list)"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\"\"\"\n    (m, n) = X_train.shape\n    n_clf = max(n_clf, 1)\n    D = np.ones(m) / m\n    stumps = []\n    for _ in range(n_clf):\n        best_error = np.inf\n        best_j = -1\n        best_threshold = 0.0\n        best_sign = 1\n        for j in range(n):\n            sorted_indices = np.argsort(X_train[:, j])\n            X_sorted_j = X_train[sorted_indices, j]\n            y_sorted = y_train[sorted_indices]\n            weights_sorted = D[sorted_indices]\n            pos_mask = y_sorted == 1\n            neg_mask = y_sorted == -1\n            cum_pos = np.cumsum(pos_mask * weights_sorted)\n            cum_neg = np.cumsum(neg_mask * weights_sorted)\n            total_pos = cum_pos[-1] if len(cum_pos) > 0 else 0\n            total_neg = cum_neg[-1] if len(cum_neg) > 0 else 0\n            for k in range(len(X_sorted_j) - 1):\n                if X_sorted_j[k] == X_sorted_j[k + 1]:\n                    continue\n                sum_pos_left = cum_pos[k]\n                sum_neg_left = cum_neg[k]\n                sum_pos_right = total_pos - sum_pos_left\n                sum_neg_right = total_neg - sum_neg_left\n                error1 = sum_neg_left + sum_pos_right\n                error2 = sum_pos_left + sum_neg_right\n                current_error = min(error1, error2)\n                if current_error < best_error:\n                    best_error = current_error\n                    best_j = j\n                    best_threshold = (X_sorted_j[k] + X_sorted_j[k + 1]) / 2\n                    best_sign = 1 if error1 < error2 else -1\n        eps = 1e-15\n        error_t = max(eps, min(best_error, 1 - eps))\n        alpha = 0.5 * np.log((1 - error_t) / error_t)\n        stumps.append((alpha, best_j, best_threshold, best_sign))\n        h_train = np.where(X_train[:, best_j] <= best_threshold, best_sign, -best_sign)\n        D *= np.exp(-alpha * y_train * h_train)\n        D /= D.sum()\n    predictions = []\n    for x in X_test:\n        sum_alpha_h = 0.0\n        for (alpha, j, threshold, sign) in stumps:\n            if x[j] <= threshold:\n                h = sign\n            else:\n                h = -sign\n            sum_alpha_h += alpha * h\n        predictions.append(1 if sum_alpha_h >= 0 else -1)\n    return predictions"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\"\"\"\n    if n_components == 0:\n        return []\n    (m, d) = X.shape\n    classes = np.unique(y)\n    global_mean = np.mean(X, axis=0)\n    S_W = np.zeros((d, d))\n    for c in classes:\n        X_c = X[y == c]\n        class_mean = np.mean(X_c, axis=0)\n        X_c_centered = X_c - class_mean\n        S_W += X_c_centered.T @ X_c_centered\n    S_B = np.zeros((d, d))\n    for c in classes:\n        X_c = X[y == c]\n        class_mean = np.mean(X_c, axis=0)\n        n_c = X_c.shape[0]\n        diff = class_mean - global_mean\n        S_B += n_c * np.outer(diff, diff)\n    A = np.linalg.pinv(S_W) @ S_B\n    (eigenvalues, eigenvectors) = np.linalg.eigh(A)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    for i in range(sorted_eigenvectors.shape[1]):\n        eigenvector = sorted_eigenvectors[:, i]\n        for j in range(eigenvector.shape[0]):\n            if not np.isclose(eigenvector[j], 0, atol=1e-08):\n                if eigenvector[j] < 0:\n                    sorted_eigenvectors[:, i] = -eigenvector\n                break\n    W = sorted_eigenvectors[:, :n_components]\n    X_proj = X @ W\n    X_proj_rounded = np.round(X_proj, 4)\n    return X_proj_rounded.tolist()"}
{"task_id": 140, "completion_id": 0, "solution": "from collections import deque\nimport math\ndef escape_fire_maze(grid: list[str]) -> int:\n    n = len(grid)\n    if n == 0:\n        return -1\n    fire_time = [[math.inf] * n for _ in range(n)]\n    q = deque()\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_time[i][j] = 0\n                q.append((i, j))\n    dirs = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    while q:\n        (i, j) = q.popleft()\n        for (di, dj) in dirs:\n            (ni, nj) = (i + di, j + dj)\n            if 0 <= ni < n and 0 <= nj < n:\n                if grid[ni][nj] != '#' and fire_time[ni][nj] > fire_time[i][j] + 1:\n                    fire_time[ni][nj] = fire_time[i][j] + 1\n                    q.append((ni, nj))\n    if fire_time[0][0] == 0 or fire_time[n - 1][n - 1] == 0:\n        return -1\n    if n == 1:\n        return 0\n    visited = [[math.inf] * n for _ in range(n)]\n    q_agent = deque()\n    q_agent.append((0, 0, 0))\n    visited[0][0] = 0\n    while q_agent:\n        (i, j, t) = q_agent.popleft()\n        for (di, dj) in dirs:\n            (ni, nj) = (i + di, j + dj)\n            if 0 <= ni < n and 0 <= nj < n:\n                if grid[ni][nj] == '#':\n                    continue\n                new_time = t + 1\n                if fire_time[ni][nj] > new_time:\n                    if ni == n - 1 and nj == n - 1:\n                        return new_time\n                    if visited[ni][nj] > new_time:\n                        visited[ni][nj] = new_time\n                        q_agent.append((ni, nj, new_time))\n    return -1"}
{"task_id": 141, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int, metric: str='euclidean') -> np.ndarray:\n    \"\"\"Implement k-Nearest Neighbors classifier.\"\"\"\n    if metric not in ['euclidean', 'manhattan', 'cosine']:\n        metric = 'euclidean'\n    if metric == 'euclidean':\n        distances = np.sqrt(((X_test[:, np.newaxis] - X_train) ** 2).sum(axis=2))\n    elif metric == 'manhattan':\n        distances = np.abs(X_test[:, np.newaxis] - X_train).sum(axis=2)\n    elif metric == 'cosine':\n        dot_product = np.dot(X_test, X_train.T)\n        norm_test = np.linalg.norm(X_test, axis=1, keepdims=True)\n        norm_train = np.linalg.norm(X_train, axis=1)\n        cosine_sim = dot_product / (norm_test * norm_train)\n        distances = 1 - cosine_sim\n    nearest_indices = np.argpartition(distances, k, axis=1)[:, :k]\n    y_neighbors = y_train[nearest_indices]\n    y_pred = []\n    for neighbors in y_neighbors:\n        (labels, counts) = np.unique(neighbors, return_counts=True)\n        max_count = np.max(counts)\n        candidates = labels[counts == max_count]\n        y_pred.append(np.min(candidates))\n    return np.array(y_pred)"}
{"task_id": 146, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X: np.ndarray, y: np.ndarray, X_test: np.ndarray, k: int=3, metric: str='euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\"\"\"\n    predictions = []\n    n_train = X.shape[0]\n    k_adj = min(k, n_train)\n    for x in X_test:\n        if metric == 'euclidean':\n            distances = np.sqrt(np.sum((X - x) ** 2, axis=1))\n        elif metric == 'manhattan':\n            distances = np.sum(np.abs(X - x), axis=1)\n        elif metric == 'cosine':\n            dot_product = np.dot(X, x)\n            norm_x = np.linalg.norm(x)\n            norm_X = np.linalg.norm(X, axis=1)\n            denominator = norm_X * norm_x + 1e-12\n            cosine_sim = dot_product / denominator\n            distances = 1 - cosine_sim\n        else:\n            raise ValueError(f\"Invalid metric '{metric}'. Choose 'euclidean', 'manhattan', or 'cosine'.\")\n        sorted_indices = np.argsort(distances)\n        k_nearest_indices = sorted_indices[:k_adj]\n        k_nearest_labels = y[k_nearest_indices]\n        (unique_labels, counts) = np.unique(k_nearest_labels, return_counts=True)\n        max_count = np.max(counts)\n        candidates = unique_labels[counts == max_count]\n        predicted_label = np.min(candidates)\n        predictions.append(predicted_label)\n    return np.array(predictions).tolist()"}
{"task_id": 155, "completion_id": 0, "solution": "import numpy as np\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    x = np.asarray(state)\n    x = np.dot(x, weights['W1']) + weights['b1']\n    x = np.maximum(x, 0)\n    x = np.dot(x, weights['W2']) + weights['b2']\n    x = np.maximum(x, 0)\n    x = np.dot(x, weights['W3']) + weights['b3']\n    x = np.tanh(x)\n    return np.round(x, 4).tolist()"}
{"task_id": 160, "completion_id": 0, "solution": "import numpy as np\ndef fit_lasso(X: np.ndarray, y: np.ndarray, lambda_param: float=1.0, max_iters: int=100, fit_intercept: bool=True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\"\"\"\n    (m, n_original) = X.shape\n    if fit_intercept:\n        intercept_col = np.ones((m, 1))\n    else:\n        intercept_col = np.zeros((m, 1))\n    X_new = np.hstack((intercept_col, X))\n    n_new = X_new.shape[1]\n    w = np.zeros(n_new)\n    m = X_new.shape[0]\n    for _ in range(max_iters):\n        if fit_intercept:\n            residuals = y - X_new[:, 1:] @ w[1:]\n            w[0] = np.mean(residuals)\n        for j in range(1, n_new):\n            old_wj = w[j]\n            w[j] = 0.0\n            r_j = y - X_new @ w\n            xj = X_new[:, j]\n            rho_j = xj.dot(r_j)\n            sum_xj_sq = xj.dot(xj)\n            if sum_xj_sq == 0.0:\n                w_j_new = 0.0\n            else:\n                tau = lambda_param * m\n                soft_val = np.sign(rho_j) * np.maximum(np.abs(rho_j) - tau, 0.0)\n                w_j_new = soft_val / sum_xj_sq\n            w[j] = w_j_new\n    bias = round(float(w[0]), 4)\n    weights = [round(float(weight), 4) for weight in w[1:]]\n    return (weights, bias)"}
{"task_id": 165, "completion_id": 0, "solution": "import numpy as np\ndef hamming_distance(x: list[int] | np.ndarray, y: list[int] | np.ndarray) -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    N = len(x)\n    if N == 0:\n        return 0.0\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    sum_diff = np.sum(x_arr != y_arr)\n    normalized = sum_diff / N\n    return round(normalized, 4)"}
{"task_id": 169, "completion_id": 0, "solution": "import numpy as np\ndef generate_window(window: str, N: int, coefficients: list[float] | None=None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\"\"\"\n    if not isinstance(N, int) or N < 1:\n        raise ValueError('N must be a positive integer')\n    if N == 1:\n        return [1.0]\n    valid_windows = ['hamming', 'hann', 'blackman_harris', 'generalized_cosine']\n    if window not in valid_windows:\n        raise ValueError('Unknown window type')\n    n = np.arange(N)\n    if window == 'hamming':\n        arg = 2 * np.pi * n / (N - 1)\n        w = 0.54 - 0.46 * np.cos(arg)\n    elif window == 'hann':\n        arg = 2 * np.pi * n / (N - 1)\n        w = 0.5 - 0.5 * np.cos(arg)\n    elif window == 'blackman_harris':\n        (a0, a1, a2, a3) = (0.35875, 0.48829, 0.14128, 0.01168)\n        arg = 2 * np.pi * n / (N - 1)\n        term1 = a1 * np.cos(arg)\n        term2 = a2 * np.cos(2 * arg)\n        term3 = a3 * np.cos(3 * arg)\n        w = a0 - term1 + term2 - term3\n    elif window == 'generalized_cosine':\n        if coefficients is None:\n            raise ValueError('coefficients must be provided for generalized_cosine window')\n        arg = 2 * np.pi * n / (N - 1)\n        w = np.zeros(N)\n        for (k, a) in enumerate(coefficients):\n            w += a * np.cos(k * arg)\n    rounded = np.round(w, 4)\n    return rounded.tolist()"}
{"task_id": 171, "completion_id": 0, "solution": "import numpy as np\ndef logistic_regression_train_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], epochs: int=5000, learning_rate: float=0.1, batch_size: int=32) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n    \"\"\"\n    unique_labels = list(set(y_train))\n    if len(unique_labels) == 1:\n        return [unique_labels[0]] * len(X_test)\n    unique_labels = sorted(unique_labels)\n    (label0, label1) = unique_labels\n    label_map = {label0: 0, label1: 1}\n    inverse_map = {0: label0, 1: label1}\n    y_internal = np.array([label_map[y] for y in y_train])\n    X_train_np = np.hstack([np.ones((len(X_train), 1)), np.array(X_train)])\n    X_test_np = np.hstack([np.ones((len(X_test), 1)), np.array(X_test)])\n    weights = np.zeros(X_train_np.shape[1])\n    m = X_train_np.shape[0]\n    for epoch in range(epochs):\n        indices = np.random.permutation(m)\n        X_shuffled = X_train_np[indices]\n        y_shuffled = y_internal[indices]\n        for i in range(0, m, batch_size):\n            X_batch = X_shuffled[i:i + batch_size]\n            y_batch = y_shuffled[i:i + batch_size]\n            z = X_batch.dot(weights)\n            h = 1 / (1 + np.exp(-z))\n            error = h - y_batch\n            gradient = X_batch.T.dot(error) / len(X_batch)\n            weights -= learning_rate * gradient\n    z_test = X_test_np.dot(weights)\n    probabilities = 1 / (1 + np.exp(-z_test))\n    predictions_internal = (probabilities >= 0.5).astype(int)\n    return [inverse_map[pred] for pred in predictions_internal.tolist()]"}
{"task_id": 176, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: list[list[int | float]], y_train: list[int], X_test: list[list[int | float]], n_estimators: int=10) -> list[int]:\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    (n_samples, n_features) = X_train.shape\n    y_converted = np.where(y_train == 1, 1, -1)\n    weights = np.ones(n_samples) / n_samples\n    stumps = []\n    for _ in range(n_estimators):\n        best_error = float('inf')\n        best_j = -1\n        best_threshold = None\n        best_polarity = None\n        best_h = None\n        for j in range(n_features):\n            unique_values = np.unique(X_train[:, j])\n            for threshold in unique_values:\n                for polarity in [1, -1]:\n                    if polarity == 1:\n                        h = np.where(X_train[:, j] < threshold, 1, -1)\n                    else:\n                        h = np.where(X_train[:, j] >= threshold, 1, -1)\n                    error = np.sum(weights * (h != y_converted))\n                    current_candidate = (error, j, threshold, polarity)\n                    if current_candidate < (best_error, best_j, best_threshold, best_polarity):\n                        (best_error, best_j, best_threshold, best_polarity) = current_candidate\n                        best_h = h.copy()\n        if best_error < 1e-10:\n            alpha = 0.5 * np.log((1.0 - best_error) / (best_error + 1e-10))\n            stumps.append((best_j, best_threshold, best_polarity, alpha))\n            break\n        alpha = 0.5 * np.log((1.0 - best_error) / (best_error + 1e-10))\n        exponent = -alpha * y_converted * best_h\n        weights *= np.exp(exponent)\n        weights /= np.sum(weights)\n        stumps.append((best_j, best_threshold, best_polarity, alpha))\n        if best_error < 1e-10:\n            break\n    predictions = []\n    for x in X_test:\n        sum_votes = 0.0\n        for (j, threshold, polarity, alpha_t) in stumps:\n            feature_val = x[j]\n            if polarity == 1:\n                h = 1 if feature_val < threshold else -1\n            else:\n                h = 1 if feature_val >= threshold else -1\n            sum_votes += alpha_t * h\n        pred = 1 if sum_votes > 0 else 0\n        predictions.append(pred)\n    return predictions"}
{"task_id": 178, "completion_id": 0, "solution": "import numpy as np\ndef particle_swarm_optimisation(n_dims: int, num_particles: int, num_iterations: int, seed: int=1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\"\"\"\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1.0\n    w = 0.5\n    c1 = 1.5\n    c2 = 1.5\n    rng = np.random.default_rng(seed)\n    positions = rng.uniform(low=-1, high=1, size=(num_particles, n_dims))\n    velocities = np.zeros((num_particles, n_dims))\n    pbest_positions = positions.copy()\n    current_values = np.sum(positions ** 2, axis=1)\n    gbest_idx = np.argmin(current_values)\n    gbest_value = current_values[gbest_idx]\n    gbest_position = pbest_positions[gbest_idx].copy()\n    for _ in range(num_iterations):\n        for i in range(num_particles):\n            r1 = rng.uniform(0, 1, size=n_dims)\n            r2 = rng.uniform(0, 1, size=n_dims)\n            velocities[i] = w * velocities[i] + c1 * r1 * (pbest_positions[i] - positions[i]) + c2 * r2 * (gbest_position - positions[i])\n            new_pos = np.clip(positions[i] + velocities[i], -1, 1)\n            new_val = np.sum(new_pos ** 2)\n            positions[i] = new_pos\n            if new_val < current_values[i]:\n                current_values[i] = new_val\n                pbest_positions[i] = new_pos.copy()\n                if new_val < gbest_value:\n                    gbest_value = new_val\n                    gbest_position = new_pos.copy()\n    return round(gbest_value, 4)"}
{"task_id": 180, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef knn_predict(X_train: list[list[float]], y_train: list, X_test: list[list[float]], k: int=5, task: str='classification') -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\"\"\"\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n    n_train = X_train_np.shape[0]\n    if k == 0 or k > n_train:\n        effective_k = n_train\n    else:\n        effective_k = k\n    predictions = []\n    for test_point in X_test_np:\n        distances = np.sqrt(np.sum((X_train_np - test_point) ** 2, axis=1))\n        sorted_indices = np.argsort(distances)\n        neighbors_indices = sorted_indices[:effective_k]\n        neighbor_labels = y_train_np[neighbors_indices]\n        if task == 'classification':\n            counts = Counter(neighbor_labels)\n            if not counts:\n                predictions.append(None)\n                continue\n            max_count = max(counts.values())\n            candidates = [label for (label, cnt) in counts.items() if cnt == max_count]\n            prediction = min(candidates)\n        else:\n            mean_value = np.mean(neighbor_labels)\n            prediction = round(mean_value, 4)\n        predictions.append(prediction)\n    return predictions"}
{"task_id": 184, "completion_id": 0, "solution": "import numpy as np\ndef decision_tree_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_depth: int | None=None) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\"\"\"\n\n    class TreeNode:\n\n        def __init__(self, value=None, feature=None, threshold=None, left=None, right=None):\n            self.value = value\n            self.feature = feature\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n\n        def is_leaf(self):\n            return self.value is not None\n\n    def gini(y):\n        counts = np.bincount(y)\n        if len(counts) == 0:\n            return 0.0\n        probs = counts / len(y)\n        return 1.0 - np.sum(probs ** 2)\n\n    def get_majority_class(y):\n        counts = np.bincount(y)\n        return np.argmax(counts)\n\n    def build_tree(X, y, current_depth, max_depth):\n        unique_classes = np.unique(y)\n        if len(unique_classes) == 1:\n            return TreeNode(value=int(unique_classes[0]))\n        if max_depth is not None and current_depth >= max_depth:\n            return TreeNode(value=int(get_majority_class(y)))\n        parent_gini = gini(y)\n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n        best_left_mask = None\n        best_right_mask = None\n        (n_samples, n_features) = X.shape\n        for feature_idx in range(n_features):\n            feature_values = X[:, feature_idx]\n            sorted_indices = np.argsort(feature_values)\n            sorted_features = feature_values[sorted_indices]\n            unique_values = np.unique(sorted_features)\n            if len(unique_values) < 2:\n                continue\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n            for thresh in thresholds:\n                left_mask = feature_values <= thresh\n                right_mask = ~left_mask\n                y_left = y[left_mask]\n                y_right = y[right_mask]\n                if len(y_left) == 0 or len(y_right) == 0:\n                    continue\n                g_left = gini(y_left)\n                g_right = gini(y_right)\n                (n_left, n_right) = (len(y_left), len(y_right))\n                weighted_gini = (n_left * g_left + n_right * g_right) / (n_left + n_right)\n                gain = parent_gini - weighted_gini\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature_idx\n                    best_threshold = thresh\n                    best_left_mask = left_mask\n                    best_right_mask = right_mask\n        if best_gain <= 0 or best_feature is None:\n            return TreeNode(value=int(get_majority_class(y)))\n        (X_left, y_left) = (X[best_left_mask], y[best_left_mask])\n        (X_right, y_right) = (X[best_right_mask], y[best_right_mask])\n        left_child = build_tree(X_left, y_left, current_depth + 1, max_depth)\n        right_child = build_tree(X_right, y_right, current_depth + 1, max_depth)\n        return TreeNode(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n    if X_train.size == 0:\n        return [0] * len(X_test)\n    root = build_tree(X_train, y_train, current_depth=0, max_depth=max_depth)\n    predictions = []\n    for sample in X_test:\n        node = root\n        while True:\n            if node.is_leaf():\n                predictions.append(node.value)\n                break\n            feature_val = sample[node.feature]\n            if feature_val <= node.threshold:\n                node = node.left\n            else:\n                node = node.right\n    return predictions"}
{"task_id": 190, "completion_id": 0, "solution": "import numpy as np\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y)\n    (n_samples, n_features) = X.shape\n    (classes, counts) = np.unique(y, return_counts=True)\n    total = len(y)\n    if total == 0:\n        return (-1, None, 0.0)\n    p = counts / total\n    g_whole = 1.0 - np.sum(p ** 2)\n    g_whole_rounded = round(g_whole, 4)\n    all_constant = True\n    for j in range(n_features):\n        if np.unique(X[:, j]).size > 1:\n            all_constant = False\n            break\n    if all_constant:\n        return (-1, None, g_whole_rounded)\n    best_feature = -1\n    best_threshold = None\n    best_gini = float('inf')\n    for j in range(n_features):\n        feature = X[:, j]\n        unique_values = np.unique(feature)\n        for v in unique_values:\n            left_mask = feature <= v\n            n_left = np.sum(left_mask)\n            n_right = n_samples - n_left\n            if n_left == 0 or n_right == 0:\n                continue\n            y_left = y[left_mask]\n            y_right = y[~left_mask]\n            left_counts = np.unique(y_left, return_counts=True)[1]\n            p_left = left_counts / n_left\n            g_left = 1.0 - np.sum(p_left ** 2)\n            right_counts = np.unique(y_right, return_counts=True)[1]\n            p_right = right_counts / n_right\n            g_right = 1.0 - np.sum(p_right ** 2)\n            g_split = n_left / n_samples * g_left + n_right / n_samples * g_right\n            if g_split < best_gini - 1e-12:\n                best_gini = g_split\n                best_feature = j\n                best_threshold = v\n            elif abs(g_split - best_gini) < 1e-12:\n                if best_feature == -1 or j < best_feature or (j == best_feature and v < best_threshold):\n                    best_gini = g_split\n                    best_feature = j\n                    best_threshold = v\n    if best_feature == -1:\n        return (-1, None, g_whole_rounded)\n    else:\n        return (best_feature, best_threshold, round(best_gini, 4))"}
{"task_id": 191, "completion_id": 0, "solution": "import numpy as np\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n    valid_metrics = {'euclidean', 'manhattan', 'chebyshev', 'cosine'}\n    if name not in valid_metrics:\n\n        def invalid_metric(x, y):\n            return -1\n        return invalid_metric\n\n    def validated_metric(x, y):\n        try:\n            if not isinstance(x, (list, tuple, np.ndarray)) or not isinstance(y, (list, tuple, np.ndarray)):\n                return -1\n            x_arr = np.array(x, dtype=float)\n            y_arr = np.array(y, dtype=float)\n            if x_arr.ndim != 1 or y_arr.ndim != 1:\n                return -1\n            if x_arr.size == 0 or y_arr.size == 0 or x_arr.shape != y_arr.shape:\n                return -1\n            if name == 'cosine':\n                norm_x = np.linalg.norm(x_arr)\n                norm_y = np.linalg.norm(y_arr)\n                if norm_x == 0.0 or norm_y == 0.0:\n                    return -1\n            if name == 'euclidean':\n                distance = np.sqrt(np.sum((x_arr - y_arr) ** 2))\n            elif name == 'manhattan':\n                distance = np.sum(np.abs(x_arr - y_arr))\n            elif name == 'chebyshev':\n                distance = np.max(np.abs(x_arr - y_arr))\n            elif name == 'cosine':\n                dot_product = np.dot(x_arr, y_arr)\n                norm_product = np.linalg.norm(x_arr) * np.linalg.norm(y_arr)\n                distance = 1 - dot_product / norm_product\n            else:\n                return -1\n            return round(distance, 4)\n        except:\n            return -1\n    return validated_metric"}
{"task_id": 197, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\"\"\"\n    n_actions_per_dim = []\n    action_dim = 0\n    action_ids = None\n    if hasattr(env.action_space, 'spaces'):\n        for subspace in env.action_space.spaces:\n            if hasattr(subspace, 'n'):\n                n_actions_per_dim.append(subspace.n)\n                action_dim += 1\n            elif hasattr(subspace, 'shape'):\n                dims = int(np.prod(subspace.shape))\n                n_actions_per_dim.extend([np.inf] * dims)\n                action_dim += dims\n            else:\n                raise ValueError('Unsupported subspace type in multi-dimensional action space.')\n    elif hasattr(env.action_space, 'n'):\n        n_actions_per_dim.append(env.action_space.n)\n        action_dim = 1\n    elif hasattr(env.action_space, 'shape'):\n        dims = int(np.prod(env.action_space.shape))\n        n_actions_per_dim.extend([np.inf] * dims)\n        action_dim = dims\n    else:\n        raise ValueError('Unsupported action space type.')\n    has_continuous = any((np.isinf(n) for n in n_actions_per_dim))\n    if has_continuous:\n        action_ids = None\n    else:\n        ranges = [range(n) for n in n_actions_per_dim]\n        action_ids = list(product(*ranges))\n    return (n_actions_per_dim, action_ids, action_dim)"}
{"task_id": 198, "completion_id": 0, "solution": "import numpy as np\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    T = phi[0].shape[1] if phi else 0\n    beta = np.zeros((V, T), dtype=np.float64)\n    for d in range(len(phi)):\n        doc_phi = phi[d]\n        doc_words = corpus[d]\n        N_d = len(doc_words)\n        for n in range(N_d):\n            v = doc_words[n]\n            beta[v, :] += doc_phi[n, :]\n    sums = beta.sum(axis=0)\n    sums = np.where(sums == 0, 1.0, sums)\n    beta_normalized = beta / sums.reshape(1, -1)\n    beta_rounded = np.round(beta_normalized, 4)\n    result = [[float(round(val, 4)) for val in row] for row in beta_rounded]\n    return result"}
{"task_id": 202, "completion_id": 0, "solution": "import numpy as np\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\"\"\"\n    n_samples = X.shape[0]\n    current_medoids = list(range(k))\n    current_cost = 0.0\n    for x in X:\n        min_dist = np.min([np.linalg.norm(x - X[med]) for med in current_medoids])\n        current_cost += min_dist\n    while True:\n        best_gain = 0.0\n        best_new_medoids = current_medoids.copy()\n        for m in current_medoids:\n            non_medoids = [s for s in range(n_samples) if s not in current_medoids]\n            for s in non_medoids:\n                new_medoids = current_medoids.copy()\n                idx = new_medoids.index(m)\n                new_medoids[idx] = s\n                new_cost = 0.0\n                for x in X:\n                    min_dist = np.inf\n                    for med in new_medoids:\n                        dist = np.linalg.norm(x - X[med])\n                        if dist < min_dist:\n                            min_dist = dist\n                    new_cost += min_dist\n                gain = current_cost - new_cost\n                if gain > best_gain:\n                    best_gain = gain\n                    best_new_medoids = new_medoids\n        if best_gain > 0:\n            current_medoids = best_new_medoids\n            current_cost -= best_gain\n        else:\n            break\n    labels = []\n    for x in X:\n        min_dist = np.inf\n        best_label = 0\n        for (label_idx, med) in enumerate(current_medoids):\n            dist = np.linalg.norm(x - X[med])\n            if dist < min_dist:\n                min_dist = dist\n                best_label = label_idx\n            elif dist == min_dist and label_idx < best_label:\n                best_label = label_idx\n        labels.append(best_label)\n    return labels"}
{"task_id": 216, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_test = np.array(x_test)\n    if x_train.ndim == 1:\n        x_train = x_train.reshape(-1, 1)\n    if x_test.ndim == 1:\n        x_test = x_test.reshape(-1, 1)\n    if x_train.shape[1] != x_test.shape[1] or n_iterations < 1:\n        return -1\n\n    def generate_poly(x, degree):\n        (n_samples, n_features) = x.shape\n        poly = np.ones((n_samples, 1))\n        for feature_idx in range(n_features):\n            feature_col = x[:, feature_idx]\n            for power in range(1, degree + 1):\n                new_col = (feature_col ** power).reshape(-1, 1)\n                poly = np.hstack((poly, new_col))\n        return poly\n    x_train_poly = generate_poly(x_train, degree)\n    x_test_poly = generate_poly(x_test, degree)\n    x_train_non_bias = x_train_poly[:, 1:]\n    if x_train_non_bias.size == 0:\n        x_train_std = x_train_poly\n        x_test_std = x_test_poly\n    else:\n        means = np.mean(x_train_non_bias, axis=0)\n        stds = np.std(x_train_non_bias, axis=0, ddof=0)\n        stds[stds == 0] = 1.0\n        x_train_std = x_train_poly.copy()\n        x_train_std[:, 1:] = (x_train_non_bias - means) / stds\n        x_test_non_bias = x_test_poly[:, 1:]\n        x_test_std = x_test_poly.copy()\n        x_test_std[:, 1:] = (x_test_non_bias - means) / stds\n    (n_samples_train, n_features) = x_train_std.shape\n    w = np.zeros(n_features)\n    for _ in range(n_iterations):\n        y_pred = x_train_std @ w\n        error = y_pred - y_train\n        gradient_mse = x_train_std.T @ error / n_samples_train\n        reg_grad = np.zeros_like(w)\n        if n_features > 1:\n            mask = np.arange(n_features) >= 1\n            w_non_bias = w[mask]\n            sign_w = np.sign(w_non_bias)\n            l1_part = l1_ratio * sign_w\n            l2_part = 2 * (1 - l1_ratio) * w_non_bias\n            reg_grad[mask] = reg_factor * (l1_part + l2_part)\n        total_gradient = gradient_mse + reg_grad\n        w -= learning_rate * total_gradient\n    y_pred_test = x_test_std @ w\n    return np.round(y_pred_test, 4).tolist()"}
{"task_id": 217, "completion_id": 0, "solution": "import numpy as np\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    s_predicted = 1 / (1 + np.exp(-predicted))\n    hessian = s_predicted * (1 - s_predicted)\n    prob = s_predicted\n    s_grad = 1 / (1 + np.exp(actual * predicted))\n    grad = actual * s_grad\n    grad_list = np.round(grad, 6).tolist()\n    hessian_list = np.round(hessian, 6).tolist()\n    prob_list = np.round(prob, 6).tolist()\n    return (grad_list, hessian_list, prob_list)"}
{"task_id": 218, "completion_id": 0, "solution": "def blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\"\"\"\n    player_sum = sum(player)\n    player_aces = player.count(1)\n    if player_aces > 0 and player_sum + 10 <= 21:\n        player_score = player_sum + 10\n    else:\n        player_score = player_sum\n    if player_score > 21:\n        return -1.0\n    dealer_sum = sum(dealer)\n    dealer_aces = dealer.count(1)\n    if dealer_aces > 0 and dealer_sum + 10 <= 21:\n        dealer_score = dealer_sum + 10\n    else:\n        dealer_score = dealer_sum\n    if dealer_score > 21:\n        return 1.0\n    player_natural = len(player) == 2 and player_score == 21\n    dealer_natural = len(dealer) == 2 and dealer_score == 21\n    if player_natural:\n        if dealer_natural:\n            return 0.0\n        else:\n            return 1.5 if dealer_score != 21 else 0.0\n    elif dealer_natural:\n        return 0.0 if player_score == 21 else -1.0\n    elif player_score > dealer_score:\n        return 1.0\n    elif player_score < dealer_score:\n        return -1.0\n    else:\n        return 0.0"}
{"task_id": 221, "completion_id": 0, "solution": "import numpy as np\ndef squared_log_error(actual, predicted):\n    \"\"\"Return element-wise squared log errors.\"\"\"\n    return (np.log1p(actual) - np.log1p(predicted)) ** 2\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\"\"\"\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n    if actual.shape != predicted.shape:\n        return -1.0\n    if (actual < 0).any() or (predicted < 0).any():\n        return -1.0\n    sle = squared_log_error(actual, predicted)\n    msle = np.mean(sle)\n    return round(msle, 4)"}
{"task_id": 222, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        if isinstance(z, np.ndarray):\n            z_scalar = z.item()\n        else:\n            z_scalar = z\n        if z_scalar >= 0:\n            s = 1.0 / (1.0 + np.exp(-z_scalar))\n        else:\n            s = np.exp(z_scalar) / (1.0 + np.exp(z_scalar))\n        return round(s, 4)\n    else:\n        arr = np.array(z, dtype=np.float64)\n        result = np.where(arr >= 0, 1.0 / (1.0 + np.exp(-arr)), np.exp(arr) / (1.0 + np.exp(arr)))\n        rounded = np.round(result, 4)\n        return rounded.tolist()"}
{"task_id": 224, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(element, a) for element in z]\n    else:\n        return z if z >= 0 else a * z"}
{"task_id": 226, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\"\"\"\n    (n_samples, n_features) = X_train.shape\n    weights = np.ones(n_samples) / n_samples\n    ensemble = []\n    for _ in range(n_clf):\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n        best_error = np.inf\n        for feature in range(n_features):\n            values = X_train[:, feature]\n            unique_values = np.unique(values)\n            if len(unique_values) < 2:\n                continue\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n            for threshold in thresholds:\n                pred_p1 = np.where(values <= threshold, -1, 1)\n                error_p1 = np.sum(weights * (pred_p1 != y_train))\n                pred_pm1 = np.where(values <= threshold, 1, -1)\n                error_pm1 = np.sum(weights * (pred_pm1 != y_train))\n                if error_p1 < error_pm1:\n                    (current_error, current_polarity) = (error_p1, 1)\n                else:\n                    (current_error, current_polarity) = (error_pm1, -1)\n                if current_error < best_error:\n                    best_error = current_error\n                    best_feature = feature\n                    best_threshold = threshold\n                    best_polarity = current_polarity\n        error = best_error\n        alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n        pred = np.where(X_train[:, best_feature] <= best_threshold, -best_polarity, best_polarity)\n        misclassified = pred != y_train\n        weights *= np.exp(alpha * (2 * misclassified - 1))\n        weights /= np.sum(weights)\n        ensemble.append((alpha, best_feature, best_threshold, best_polarity))\n    y_pred = np.zeros(X_test.shape[0])\n    for (alpha, feature, threshold, polarity) in ensemble:\n        x_feature = X_test[:, feature]\n        stump_pred = np.where(x_feature <= threshold, -polarity, polarity)\n        y_pred += alpha * stump_pred\n    y_pred = np.sign(y_pred)\n    y_pred[y_pred == 0] = 1\n    return y_pred.astype(int).tolist()"}
{"task_id": 241, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal(shape: tuple[int, ...], scale: float=0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n    d0 = shape[0]\n    remaining_dims = int(np.prod(shape[1:]))\n    A = np.random.normal(size=(d0, remaining_dims))\n    (U, S, Vh) = np.linalg.svd(A, full_matrices=False)\n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = Vh\n    tensor = Q.reshape(shape) * scale\n    return tensor.tolist()"}
{"task_id": 243, "completion_id": 0, "solution": "import numpy as np\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\"\"\"\n    sum_s = sum(state)\n    z1 = max(0.0, sum_s)\n    state_value = 64.0 * z1\n    action_probs = [round(1 / 3, 4)] * 3\n    return (action_probs, round(state_value, 4))"}
{"task_id": 249, "completion_id": 0, "solution": "import numpy as np\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\"\"\"\n    x = np.array(state, dtype=np.float32)\n    for layer in ['1', '2', '3']:\n        W = np.array(params[f'W{layer}'], dtype=np.float32)\n        b = np.array(params[f'b{layer}'], dtype=np.float32)\n        x = x @ W + b\n        x = np.maximum(x, 0)\n    Wa = np.array(params['Wa'], dtype=np.float32)\n    ba = np.array(params['ba'], dtype=np.float32)\n    actor_scores = x @ Wa + ba\n    exp_scores = np.exp(actor_scores - np.max(actor_scores))\n    probabilities = exp_scores / exp_scores.sum()\n    prob_list = probabilities.tolist()\n    Wc = np.array(params['Wc'], dtype=np.float32)\n    bc = np.array(params['bc'], dtype=np.float32)\n    critic_value = (x @ Wc + bc).item()\n    rounded_probs = []\n    for p in prob_list:\n        p_rounded = round(p, 4)\n        if p_rounded in (0.0, 1.0):\n            rounded_probs.append(round(p_rounded, 1))\n        else:\n            rounded_probs.append(round(p_rounded, 4))\n    value_rounded = round(critic_value, 4)\n    if value_rounded in (0.0, 1.0):\n        value_final = round(value_rounded, 1)\n    else:\n        value_final = round(value_rounded, 4)\n    return (rounded_probs, value_final)"}
{"task_id": 253, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w_np = np.asarray(w)\n    if gradient:\n        grad = alpha * (l1_ratio * np.sign(w_np) + (1 - l1_ratio) * w_np)\n        grad_rounded = np.round(grad, 4)\n        return grad_rounded.tolist()\n    else:\n        l1_term = l1_ratio * np.sum(np.abs(w_np))\n        l2_sq_term = 0.5 * (1 - l1_ratio) * np.sum(w_np ** 2)\n        penalty = alpha * (l1_term + l2_sq_term)\n        return round(penalty, 4)"}
{"task_id": 256, "completion_id": 0, "solution": "import numpy as np\nfrom copy import deepcopy\ndef grad_check_RNN(model, loss_func, param_name: str, n_t: int, X, epsilon: float=1e-07):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\"\"\"\n    if param_name in {'X', 'y'}:\n        return None\n    if param_name == 'Ba':\n        key = 'ba'\n    elif param_name == 'Bx':\n        key = 'bx'\n    else:\n        key = param_name\n    if key not in model.parameters:\n        return None\n    param = model.parameters[key]\n    original_param = param.copy()\n    param_float64 = param.astype(np.float64)\n    model.parameters[key] = param_float64\n    grads = np.zeros_like(param_float64, dtype=np.float64)\n    for idx in np.ndindex(param_float64.shape):\n        original_val = param_float64[idx].item()\n        param_float64[idx] = original_val + epsilon\n        model.flush_gradients()\n        predictions_plus = []\n        for t in range(n_t):\n            X_t = X[:, :, t].astype(np.float64)\n            pred = model.forward(X_t)\n            predictions_plus.append(pred)\n        L_plus = loss_func(predictions_plus)\n        param_float64[idx] = original_val - epsilon\n        model.flush_gradients()\n        predictions_minus = []\n        for t in range(n_t):\n            X_t = X[:, :, t].astype(np.float64)\n            pred = model.forward(X_t)\n            predictions_minus.append(pred)\n        L_minus = loss_func(predictions_minus)\n        param_float64[idx] = original_val\n        grad = (L_plus - L_minus) / (2 * epsilon)\n        grads[idx] = grad\n    model.parameters[key] = original_param\n    return grads.T"}
{"task_id": 257, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\"\"\"\n    (m, n) = X_train.shape\n    n_clf = max(n_clf, 1)\n    weights = np.ones(m) / m\n    stumps = []\n    for _ in range(n_clf):\n        best_error = np.inf\n        best_feature = None\n        best_threshold = None\n        best_direction = None\n        for j in range(n):\n            feature_values = X_train[:, j]\n            sorted_indices = np.argsort(feature_values)\n            sorted_feature = feature_values[sorted_indices]\n            sorted_y = y_train[sorted_indices]\n            sorted_weights = weights[sorted_indices]\n            for i in range(len(sorted_feature) - 1):\n                th = (sorted_feature[i] + sorted_feature[i + 1]) / 2\n                error_plus = np.sum(sorted_weights[:i + 1][sorted_y[:i + 1] != -1]) + np.sum(sorted_weights[i + 1:][sorted_y[i + 1:] != 1])\n                error_minus = np.sum(sorted_weights[:i + 1][sorted_y[:i + 1] != 1]) + np.sum(sorted_weights[i + 1:][sorted_y[i + 1:] != -1])\n                current_error = min(error_plus, error_minus)\n                current_direction = 1 if error_plus < error_minus else -1\n                if current_error < best_error:\n                    best_error = current_error\n                    best_feature = j\n                    best_threshold = th\n                    best_direction = current_direction\n            th = sorted_feature[0] - 1e-05\n            error_plus = np.sum(sorted_weights[sorted_y != 1])\n            error_minus = np.sum(sorted_weights[sorted_y != -1])\n            current_error = min(error_plus, error_minus)\n            current_direction = 1 if error_plus < error_minus else -1\n            if current_error < best_error:\n                best_error = current_error\n                best_feature = j\n                best_threshold = th\n                best_direction = current_direction\n            th = sorted_feature[-1] + 1e-05\n            error_plus = np.sum(sorted_weights[sorted_y != -1])\n            error_minus = np.sum(sorted_weights[sorted_y != 1])\n            current_error = min(error_plus, error_minus)\n            current_direction = 1 if error_plus < error_minus else -1\n            if current_error < best_error:\n                best_error = current_error\n                best_feature = j\n                best_threshold = th\n                best_direction = current_direction\n        epsilon = max(best_error, 1e-10)\n        epsilon = min(epsilon, 1 - 1e-10)\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        pred = np.where(X_train[:, best_feature] >= best_threshold, best_direction, -best_direction)\n        incorrect = pred != y_train\n        weights *= np.exp(alpha * incorrect)\n        weights /= np.sum(weights)\n        stumps.append((best_feature, best_threshold, best_direction, alpha))\n    predictions = []\n    for x in X_test:\n        sum_alpha = 0.0\n        for (j, th, d, a) in stumps:\n            if x[j] >= th:\n                h = d\n            else:\n                h = -d\n            sum_alpha += a * h\n        predictions.append(1 if sum_alpha >= 0 else -1)\n    return predictions"}
{"task_id": 261, "completion_id": 0, "solution": "import numpy as np\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    if len(shape) == 2:\n        (fan_in, fan_out) = (shape[0], shape[1])\n    else:\n        receptive_field_size = np.prod(shape[2:])\n        fan_in = shape[1] * receptive_field_size\n        fan_out = shape[0] * receptive_field_size\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n    return np.random.normal(0.0, std, shape)"}
{"task_id": 266, "completion_id": 0, "solution": "from typing import Any, List, Tuple\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\"\"\"\n    vertices_set = set(V)\n    adj_dict = {v: set() for v in V}\n    for (u, v) in E:\n        if u in vertices_set and v in vertices_set:\n            adj_dict[u].add(v)\n            adj_dict[v].add(u)\n    vertex_to_index = {v: idx for (idx, v) in enumerate(V)}\n    result = []\n    for v in V:\n        adj_vertices = list(adj_dict[v])\n        adj_vertices.sort(key=lambda x: vertex_to_index[x])\n        result.append(adj_vertices)\n    return result"}
{"task_id": 267, "completion_id": 0, "solution": "import numpy as np\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\"\"\"\n    best_error = np.inf\n    best_feature = 0\n    best_threshold = 0.0\n    best_polarity = 1\n    (n_samples, n_features) = X.shape\n    for j in range(n_features):\n        x_j = X[:, j]\n        sorted_indices = np.argsort(x_j)\n        sorted_x = x_j[sorted_indices]\n        sorted_y = y[sorted_indices]\n        sorted_weights = sample_weights[sorted_indices]\n        (unique_thetas, split_indices) = np.unique(sorted_x, return_index=True)\n        n = len(sorted_x)\n        if n == 0:\n            continue\n        mask_neq_1 = sorted_y != 1\n        mask_neq_minus1 = sorted_y != -1\n        contrib_neq_1 = mask_neq_1 * sorted_weights\n        contrib_neq_minus1 = mask_neq_minus1 * sorted_weights\n        cum_neq_1 = np.zeros(n + 1)\n        cum_neq_1[1:] = np.cumsum(contrib_neq_1)\n        cum_neq_minus1 = np.zeros(n + 1)\n        cum_neq_minus1[1:] = np.cumsum(contrib_neq_minus1)\n        total_neq_1 = cum_neq_1[-1]\n        total_neq_minus1 = cum_neq_minus1[-1]\n        for (theta, split_idx) in zip(unique_thetas, split_indices):\n            error_p1 = cum_neq_1[split_idx] + (total_neq_minus1 - cum_neq_minus1[split_idx])\n            error_pm1 = cum_neq_minus1[split_idx] + (total_neq_1 - cum_neq_1[split_idx])\n            if error_p1 < best_error:\n                best_error = error_p1\n                best_feature = j\n                best_threshold = theta\n                best_polarity = 1\n            if error_pm1 < best_error:\n                best_error = error_pm1\n                best_feature = j\n                best_threshold = theta\n                best_polarity = -1\n    best_threshold = round(best_threshold, 4)\n    best_error = round(best_error, 4)\n    return {'feature_index': best_feature, 'threshold': best_threshold, 'polarity': best_polarity, 'weighted_error': best_error}"}
{"task_id": 273, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nimport bisect\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\"\"\"\n    sorted_pairs = sorted(zip(feature, target), key=lambda x: x[0])\n    sorted_features = [x[0] for x in sorted_pairs]\n    sorted_targets = [x[1] for x in sorted_pairs]\n    unique_sorted = []\n    prev = None\n    for val in sorted_features:\n        if val != prev:\n            unique_sorted.append(val)\n            prev = val\n    candidate_thresholds = []\n    for i in range(len(unique_sorted) - 1):\n        mid = (unique_sorted[i] + unique_sorted[i + 1]) / 2\n        candidate_thresholds.append(mid)\n    if not candidate_thresholds:\n        return (None, 0.0)\n    parent_counts = Counter(sorted_targets)\n    total_parent = len(sorted_targets)\n    parent_entropy = 0.0\n    for count in parent_counts.values():\n        p = count / total_parent\n        if p > 0:\n            parent_entropy -= p * math.log2(p)\n    if parent_entropy == 0.0:\n        return (None, 0.0)\n    max_gain = -float('inf')\n    best_threshold = None\n    for t in candidate_thresholds:\n        split_idx = bisect.bisect_left(sorted_features, t)\n        left_targets = sorted_targets[:split_idx]\n        right_targets = sorted_targets[split_idx:]\n        left_total = len(left_targets)\n        left_entropy = 0.0\n        if left_total > 0:\n            left_counts = Counter(left_targets)\n            for count in left_counts.values():\n                p = count / left_total\n                if p > 0:\n                    left_entropy -= p * math.log2(p)\n        right_total = len(right_targets)\n        right_entropy = 0.0\n        if right_total > 0:\n            right_counts = Counter(right_targets)\n            for count in right_counts.values():\n                p = count / right_total\n                if p > 0:\n                    right_entropy -= p * math.log2(p)\n        n = total_parent\n        weighted_avg = left_total / n * left_entropy + right_total / n * right_entropy\n        gain = parent_entropy - weighted_avg\n        if gain > max_gain or (gain == max_gain and t < best_threshold):\n            max_gain = gain\n            best_threshold = t\n    if max_gain <= 0.0:\n        return (None, 0.0)\n    else:\n        return (round(best_threshold, 4), round(max_gain, 4))"}
{"task_id": 286, "completion_id": 0, "solution": "import random\nfrom typing import Callable, Tuple, List\ndef get_initializer(name: str):\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    global_symbols = globals()\n    initializer = global_symbols.get(name)\n    if initializer is not None and callable(initializer):\n        return initializer\n    raise ValueError('Invalid initialization function.')"}
{"task_id": 287, "completion_id": 0, "solution": "import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    (N, C, H, W) = X.shape\n    (p_h, p_w) = pool_shape\n    (s_h, s_w) = stride\n    H_out = (H - p_h) // s_h + 1\n    W_out = (W - p_w) // s_w + 1\n    X_strides = X.strides\n    new_strides = (X_strides[0], X_strides[1], X_strides[2] * s_h, X_strides[3] * s_w, X_strides[2], X_strides[3])\n    X_windows = as_strided(X, shape=(N, C, H_out, W_out, p_h, p_w), strides=new_strides)\n    pooled_output = np.mean(X_windows, axis=(-2, -1))\n    grad_input = np.zeros_like(X)\n    grad_windows = as_strided(grad_input, shape=(N, C, H_out, W_out, p_h, p_w), strides=new_strides)\n    scaled_grad = accum_grad[..., np.newaxis, np.newaxis] / (p_h * p_w)\n    np.add(grad_windows, scaled_grad, out=grad_windows)\n    pooled_output_rounded = np.round(pooled_output, 4)\n    grad_input_rounded = np.round(grad_input, 4)\n    return (pooled_output_rounded.tolist(), grad_input_rounded.tolist())"}
{"task_id": 290, "completion_id": 0, "solution": "import numpy as np\nfrom dataclasses import dataclass\n@dataclass\nclass Leaf:\n    value: object\n@dataclass\nclass Node:\n    feature: int\n    threshold: float\n    left: object\n    right: object\ndef compare_trees(tree_a, tree_b):\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-08, rtol=0.0)\n    elif isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-08, rtol=0.0):\n            return False\n        return compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    else:\n        return False"}
{"task_id": 292, "completion_id": 0, "solution": "import numpy as np\ndef single_point_crossover(parent1: list[list[int | float]], parent2: list[list[int | float]], cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\"\"\"\n    parent1_rows = len(parent1)\n    parent2_rows = len(parent2)\n    if parent1_rows != parent2_rows:\n        return -1\n    if parent1_rows == 0:\n        return ([], [])\n    parent1_cols = len(parent1[0])\n    for row in parent1:\n        if len(row) != parent1_cols:\n            return -1\n    parent2_cols = len(parent2[0])\n    for row in parent2:\n        if len(row) != parent2_cols:\n            return -1\n    if parent1_cols != parent2_cols:\n        return -1\n    np_parent1 = np.array(parent1)\n    np_parent2 = np.array(parent2)\n    child1 = np.hstack((np_parent1[:, :cutoff], np_parent2[:, cutoff:]))\n    child2 = np.hstack((np_parent2[:, :cutoff], np_parent1[:, cutoff:]))\n    return (child1.tolist(), child2.tolist())"}
{"task_id": 294, "completion_id": 0, "solution": "from typing import Any, Dict, List, Tuple\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\"\"\"\n    adj = {}\n    v2i = G['_V2I']\n    for vertex in v2i:\n        adj[vertex] = []\n    for edges in G['_G']:\n        for edge in edges:\n            (s, t, w) = edge\n            adj[s].append((t, w))\n            if not G['is_directed']:\n                adj[t].append((s, w))\n    for u in adj:\n        adj[u].sort(key=lambda x: x[0])\n    return adj"}
{"task_id": 296, "completion_id": 0, "solution": "import numpy as np\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    x = np.asarray(x)\n    return bool(np.all((x == 0) | (x == 1)))"}
{"task_id": 298, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom typing import List\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\"\"\"\n    if N < 1:\n        raise ValueError('N must be \u2265 1')\n    if N == 1:\n        unigram_counts = Counter()\n        for sentence in corpus:\n            tokens = sentence.split()\n            unigram_counts.update(tokens)\n        total_tokens = sum(unigram_counts.values())\n    else:\n        ngram_counts = Counter()\n        prefix_counts = Counter()\n        for sentence in corpus:\n            original_tokens = sentence.split()\n            padded_tokens = ['<bol>'] * (N - 1) + original_tokens + ['<eol>']\n            for i in range(len(padded_tokens) - N + 1):\n                ngram = tuple(padded_tokens[i:i + N])\n                ngram_counts[ngram] += 1\n            for i in range(len(padded_tokens) - (N - 1) + 1):\n                prefix = tuple(padded_tokens[i:i + (N - 1)])\n                prefix_counts[prefix] += 1\n    query_tokens = query.split()\n    if N > 1:\n        padded_query = ['<bol>'] * (N - 1) + query_tokens + ['<eol>']\n    else:\n        padded_query = query_tokens\n    if len(padded_query) < N:\n        return float('-inf')\n    query_ngrams = [tuple(padded_query[i:i + N]) for i in range(len(padded_query) - N + 1)]\n    log_sum = 0.0\n    for ngram in query_ngrams:\n        if N == 1:\n            unigram = ngram[0]\n            count = unigram_counts.get(unigram, 0)\n            if count == 0 or total_tokens == 0:\n                return float('-inf')\n            prob = count / total_tokens\n        else:\n            count_ngram = ngram_counts.get(ngram, 0)\n            if count_ngram == 0:\n                return float('-inf')\n            prefix = ngram[:-1]\n            count_prefix = prefix_counts.get(prefix, 0)\n            if count_prefix == 0:\n                return float('-inf')\n            prob = count_ngram / count_prefix\n        log_sum += math.log(prob)\n    return round(log_sum, 4)"}
{"task_id": 302, "completion_id": 0, "solution": "import numpy as np\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n = X.shape[0]\n    diff = X[:, np.newaxis, :] - X[np.newaxis, :, :]\n    distance_matrix = np.sqrt((diff ** 2).sum(axis=2))\n    W = 1 / (1 + distance_matrix)\n    np.fill_diagonal(W, 0)\n    D = np.diag(W.sum(axis=1))\n    L = D - W\n    (eigenvalues, eigenvectors) = np.linalg.eigh(L)\n    E = eigenvectors[:, :k]\n    centroids = E[:k, :].copy()\n    prev_labels = None\n    for _ in range(100):\n        distances = np.sqrt(((E[:, np.newaxis, :] - centroids) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=1)\n        if prev_labels is not None and np.array_equal(labels, prev_labels):\n            break\n        prev_labels = labels.copy()\n        new_centroids = np.zeros_like(centroids)\n        counts = np.zeros(k, dtype=int)\n        for i in range(n):\n            cluster = labels[i]\n            new_centroids[cluster] += E[i]\n            counts[cluster] += 1\n        for c in range(k):\n            if counts[c] == 0:\n                random_idx = np.random.randint(n)\n                new_centroids[c] = E[random_idx]\n            else:\n                new_centroids[c] = new_centroids[c] / counts[c]\n        centroids = new_centroids\n    return labels.tolist()"}
{"task_id": 303, "completion_id": 0, "solution": "def err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str='') -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    prev_index = max(ix - 1, 0)\n    (prev_mine, prev_label) = params[prev_index]\n    (curr_mine, curr_label) = params[ix]\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    debug_lines = ['-' * 25 + ' DEBUG ' + '-' * 25, f'Mine (prev) [{prev_label}]:', prev_mine, f'Theirs (prev) [{prev_label}]:', prev_gold, f'Mine [{curr_label}]:', curr_mine, f'Theirs [{curr_label}]:', curr_gold + warn_str, '-' * 23 + ' END DEBUG ' + '-' * 23]\n    return '\\n'.join(debug_lines)"}
{"task_id": 304, "completion_id": 0, "solution": "import numpy as np\ndef bayesian_posterior_mean(X: np.ndarray, y: np.ndarray, alpha: float=1.0, beta: float=1.0, mu=0.0, V=None, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility.\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    if fit_intercept:\n        X_augmented = np.hstack([np.ones((X.shape[0], 1), dtype=X.dtype), X])\n    else:\n        X_augmented = X.copy()\n    (n_samples, n_features) = X_augmented.shape\n    if np.isscalar(mu):\n        mu_prior = np.full(n_features, mu, dtype=np.float64)\n    else:\n        mu_prior = np.asarray(mu, dtype=np.float64)\n        if mu_prior.shape != (n_features,):\n            raise ValueError('mu must be a scalar or have length equal to the number of features.')\n    if V is None:\n        V_prior = np.eye(n_features)\n    elif np.isscalar(V):\n        V_prior = V * np.eye(n_features)\n    elif isinstance(V, (list, tuple)):\n        if len(V) != n_features:\n            raise ValueError('V as list/tuple must have length equal to the number of features.')\n        V_prior = np.diag(V)\n    elif isinstance(V, np.ndarray):\n        if V.ndim == 1:\n            if V.size != n_features:\n                raise ValueError('V as 1D array must have length equal to the number of features.')\n            V_prior = np.diag(V)\n        elif V.ndim == 2:\n            if V.shape != (n_features, n_features):\n                raise ValueError('V as 2D array must be a square matrix matching feature count.')\n            V_prior = V\n        else:\n            raise ValueError('V must be 1D or 2D numpy array.')\n    else:\n        raise TypeError('V must be None, scalar, list/tuple, or numpy array.')\n    V_inv = np.linalg.inv(V_prior)\n    XTX = X_augmented.T @ X_augmented\n    XTy = X_augmented.T @ y\n    Sigma_b = np.linalg.inv(V_inv + XTX)\n    mu_b = Sigma_b @ (V_inv @ mu_prior + XTy)\n    return [round(coef, 4) for coef in mu_b.tolist()]"}
{"task_id": 308, "completion_id": 0, "solution": "import numpy as np\ndef mel_filterbank(N: int, n_filters: int=20, fs: int=44000, min_freq: int=0, max_freq: int | None=None, normalize: bool=True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\"\"\"\n    if max_freq is None:\n        max_freq = fs / 2\n    min_mel = 2595 * np.log10(1 + min_freq / 700)\n    max_mel = 2595 * np.log10(1 + max_freq / 700)\n    mels = np.linspace(min_mel, max_mel, num=n_filters + 2)\n    f_Hz = 700 * (10 ** (mels / 2595) - 1)\n    bin_freqs = np.arange(N // 2 + 1) * fs / N\n    filterbank = []\n    for i in range(n_filters):\n        f_left = f_Hz[i]\n        f_center = f_Hz[i + 1]\n        f_right = f_Hz[i + 2]\n        mask = (bin_freqs >= f_left) & (bin_freqs <= f_right)\n        left_slope = (bin_freqs - f_left) / (f_center - f_left)\n        right_slope = (f_right - bin_freqs) / (f_right - f_center)\n        weights = np.minimum(left_slope, right_slope)\n        weights = np.clip(weights, 0.0, None)\n        weights = np.where(mask, weights, 0.0)\n        if normalize:\n            scale = 2.0 / (f_right - f_left)\n            weights *= scale\n        rounded_weights = [round(w, 4) for w in weights.tolist()]\n        filterbank.append(rounded_weights)\n    return filterbank"}
{"task_id": 312, "completion_id": 0, "solution": "import numpy as np\ndef blackman_harris(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    (a0, a1, a2, a3) = (0.35875, 0.48829, 0.14128, 0.01168)\n    if symmetric:\n        N = window_len - 1\n        n = np.arange(window_len)\n    else:\n        N = window_len\n        n = np.arange(window_len + 1)\n    window = a0 - a1 * np.cos(2 * np.pi * n / N) + a2 * np.cos(4 * np.pi * n / N) - a3 * np.cos(6 * np.pi * n / N)\n    if not symmetric:\n        window = window[:-1]\n    return np.round(window, 4).tolist()"}
{"task_id": 313, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n\n    def digamma(x):\n        shift = 0.0\n        x = float(x)\n        while x < 6:\n            shift -= 1.0 / x\n            x += 1.0\n        ln_x = math.log(x)\n        inv_x = 1.0 / x\n        inv_x2 = inv_x * inv_x\n        inv_x4 = inv_x2 * inv_x2\n        inv_x6 = inv_x4 * inv_x2\n        result = ln_x - 0.5 * inv_x - 1.0 / 12.0 * inv_x2 + 1.0 / 120.0 * inv_x4 - 1.0 / 252.0 * inv_x6\n        return result + shift\n    gamma_row = gamma[d]\n    sum_gamma = sum(gamma_row)\n    gamma_dt = gamma_row[t]\n    term1 = digamma(gamma_dt)\n    term2 = digamma(sum_gamma)\n    result = term1 - term2\n    return round(result, 4)"}
{"task_id": 317, "completion_id": 0, "solution": "from typing import List, Tuple, Dict\ndef err_fmt(params: List[Tuple[str, str]], golds: Dict[str, str], ix: int, warn_str: str='') -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\"\"\"\n    prev_ix = max(ix - 1, 0)\n    (prev_pred, prev_label) = params[prev_ix]\n    prev_gold = golds[prev_label]\n    (curr_pred, curr_label) = params[ix]\n    curr_gold = golds[curr_label]\n    header = '-' * 25 + ' DEBUG ' + '-' * 25 + '\\n'\n    prev_mine = f'Mine (prev) [{prev_label}]:\\n{prev_pred}\\n'\n    prev_theirs = f'Theirs (prev) [{prev_label}]:\\n{prev_gold}\\n'\n    two_blanks = '\\n\\n'\n    current_mine = f'Mine [{curr_label}]:\\n{curr_pred}\\n'\n    current_theirs = f'Theirs [{curr_label}]:\\n{curr_gold}{warn_str}\\n'\n    footer = '-' * 23 + ' END DEBUG ' + '-' * 23 + '\\n'\n    return header + prev_mine + prev_theirs + two_blanks + current_mine + current_theirs + footer"}
{"task_id": 318, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    (n_samples, n_features) = X_train.shape\n    D = np.ones(n_samples) / n_samples\n    stumps = []\n    for _ in range(n_estimators):\n        best_error = np.inf\n        best_j = -1\n        best_t = None\n        best_p = None\n        for j in range(n_features):\n            sorted_indices = np.argsort(X_train[:, j])\n            X_j_sorted = X_train[sorted_indices, j]\n            y_sorted = y_train[sorted_indices]\n            D_sorted = D[sorted_indices]\n            mask_pos = y_sorted == 1\n            mask_neg = ~mask_pos\n            left_pos = np.cumsum(D_sorted * mask_pos)\n            left_neg = np.cumsum(D_sorted * mask_neg)\n            total_pos = left_pos[-1] if len(left_pos) > 0 else 0\n            total_neg = left_neg[-1] if len(left_neg) > 0 else 0\n            for k in range(len(X_j_sorted) - 1):\n                if X_j_sorted[k] == X_j_sorted[k + 1]:\n                    continue\n                t_candidate = (X_j_sorted[k] + X_j_sorted[k + 1]) / 2\n                current_left_pos = left_pos[k]\n                current_left_neg = left_neg[k]\n                current_right_pos = total_pos - current_left_pos\n                current_right_neg = total_neg - current_left_neg\n                error_p1 = current_left_pos + current_right_neg\n                error_m1 = current_left_neg + current_right_pos\n                if error_p1 < error_m1:\n                    current_error = error_p1\n                    current_p = 1\n                else:\n                    current_error = error_m1\n                    current_p = -1\n                if current_error < best_error:\n                    best_error = current_error\n                    best_j = j\n                    best_t = t_candidate\n                    best_p = current_p\n        if best_j == -1:\n            weighted_error_p1 = np.sum(D[y_train == -1])\n            weighted_error_m1 = np.sum(D[y_train == 1])\n            if weighted_error_p1 < weighted_error_m1:\n                best_p = 1\n                best_error = weighted_error_p1\n            else:\n                best_p = -1\n                best_error = weighted_error_m1\n            best_j = 0\n            best_t = -np.inf\n        epsilon_m = best_error\n        epsilon_m = max(epsilon_m, 1e-15)\n        epsilon_m = min(epsilon_m, 1 - 1e-15)\n        alpha_m = 0.5 * math.log((1 - epsilon_m) / epsilon_m)\n        h_m = best_p * np.where(X_train[:, best_j] >= best_t, 1, -1)\n        D *= np.exp(-alpha_m * y_train * h_m)\n        D_sum = D.sum()\n        if D_sum > 0:\n            D /= D_sum\n        else:\n            D = np.ones_like(D) / n_samples\n        stumps.append((best_j, best_t, best_p, alpha_m))\n    sum_alpha_h = np.zeros(X_test.shape[0])\n    for (j, t, p, alpha) in stumps:\n        h = p * np.where(X_test[:, j] >= t, 1, -1)\n        sum_alpha_h += alpha * h\n    y_pred = [1 if s >= 0 else -1 for s in sum_alpha_h]\n    return y_pred"}
{"task_id": 329, "completion_id": 0, "solution": "import numpy as np\ndef dft_bins(N: int, fs: int=44000, positive_only: bool=True) -> list[float]:\n    if N <= 0 or fs <= 0:\n        return []\n    if positive_only:\n        max_k = N // 2\n        k_values = np.arange(max_k + 1)\n        freqs = k_values * (fs / N)\n    else:\n        freqs = np.fft.fftfreq(N) * fs\n    rounded_freqs = np.round(freqs, 4)\n    return rounded_freqs.tolist()"}
{"task_id": 331, "completion_id": 0, "solution": "import numpy as np\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\"\"\"\n    y_true = np.asarray(y_true)\n    predictions = np.asarray(predictions)\n    oob_mask = np.asarray(oob_mask).astype(bool)\n    oob_counts = oob_mask.sum(axis=1)\n    valid_samples = oob_counts > 0\n    if not np.any(valid_samples):\n        return -1.0\n    oob_sums = (predictions * oob_mask).sum(axis=1)\n    averages = oob_sums[valid_samples] / oob_counts[valid_samples]\n    squared_errors = (y_true[valid_samples] - averages) ** 2\n    mse = squared_errors.mean()\n    return round(mse, 4)"}
{"task_id": 332, "completion_id": 0, "solution": "from collections import Counter\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\"\"\"\n    item_counter = Counter()\n    for transaction in transactions:\n        unique_items = set(transaction)\n        item_counter.update(unique_items)\n    frequent_items = {item: count for (item, count) in item_counter.items() if count >= min_support}\n    if not frequent_items:\n        return {'support': 0, 'children': {}}\n    sorted_items = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n    order_dict = {item: idx for (idx, item) in enumerate(sorted_items)}\n    root = {'support': 0, 'children': {}}\n    for transaction in transactions:\n        unique_trans = set(transaction)\n        filtered = [item for item in unique_trans if item in frequent_items]\n        if not filtered:\n            continue\n        filtered_sorted = sorted(filtered, key=lambda x: order_dict[x])\n        current_node = root\n        for item in filtered_sorted:\n            children = current_node['children']\n            if item in children:\n                children[item]['support'] += 1\n                current_node = children[item]\n            else:\n                new_node = {'support': 1, 'children': {}}\n                children[item] = new_node\n                current_node = new_node\n    return root"}
{"task_id": 336, "completion_id": 0, "solution": "import numpy as np\ndef linear_kernel(X: list[list[int | float]] | 'np.ndarray', Y: list[list[int | float]] | 'np.ndarray' | None=None, c0: int | float=0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\"\"\"\n    if isinstance(X, list) and len(X) == 0:\n        return []\n    x = np.array(X)\n    if x.ndim == 1:\n        x = x.reshape(1, -1)\n    if x.shape[0] == 0:\n        return []\n    if Y is None:\n        y = x.copy()\n    else:\n        y = np.array(Y)\n        if y.ndim == 1:\n            y = y.reshape(1, -1)\n    if x.shape[1] != y.shape[1]:\n        return -1\n    dot_product = np.dot(x, y.T)\n    k_matrix = dot_product + c0\n    k_rounded = np.round(k_matrix, 4)\n    return k_rounded.tolist()"}
{"task_id": 340, "completion_id": 0, "solution": "import numpy as np\ndef adam_update(param: np.ndarray, grad: np.ndarray, t: int, m: np.ndarray, v: np.ndarray, lr: float=0.001, beta1: float=0.9, beta2: float=0.999, eps: float=1e-08) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\"\"\"\n    new_m = beta1 * m + (1 - beta1) * grad\n    new_v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = new_m / (1 - beta1 ** t)\n    v_hat = new_v / (1 - beta2 ** t)\n    denominator = np.sqrt(v_hat) + eps\n    delta = lr * m_hat / denominator\n    if np.all(grad == 0):\n        updated_param = param.copy()\n    else:\n        updated_param = param - delta\n    updated_param_list = np.round(updated_param, 8).tolist()\n    new_m_list = np.round(new_m, 8).tolist()\n    new_v_list = np.round(new_v, 8).tolist()\n    return (updated_param_list, new_m_list, new_v_list)"}
{"task_id": 343, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_update(theta_samples: np.ndarray, rewards: list[float] | np.ndarray, retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\"\"\"\n    N = theta_samples.shape[0]\n    k = int(retain_prcnt * N)\n    rewards_arr = np.array(rewards)\n    sorted_indices = np.argsort(-rewards_arr)\n    elite_indices = sorted_indices[:k]\n    elite_samples = theta_samples[elite_indices]\n    mean = np.mean(elite_samples, axis=0)\n    variance = np.var(elite_samples, axis=0)\n    rounded_mean = np.round(mean, 4)\n    rounded_variance = np.round(variance, 4)\n    return (rounded_mean.tolist(), rounded_variance.tolist())"}
{"task_id": 353, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    if not labels:\n        return 0.0\n    counts = Counter(labels)\n    total = len(labels)\n    entropy_val = 0.0\n    for cnt in counts.values():\n        p = cnt / total\n        entropy_val += p * math.log2(p)\n    entropy_val = -entropy_val\n    return round(entropy_val, 5)"}
{"task_id": 354, "completion_id": 0, "solution": "import numpy as np\ndef conv2D(X: 'np.ndarray', W: 'np.ndarray', stride: int, pad, dilation: int=0):\n    (n_ex, in_rows, in_cols, in_ch) = X.shape\n    (kr, kc, in_ch_W, out_ch) = W.shape\n    if in_ch != in_ch_W:\n        raise ValueError('Mismatch in input channels between X and W')\n    kr_eff = (kr - 1) * dilation + 1\n    kc_eff = (kc - 1) * dilation + 1\n    if pad == 'same':\n        pad_row_total = max(0, (in_rows - 1) * stride + kr_eff - in_rows)\n        pad_col_total = max(0, (in_cols - 1) * stride + kc_eff - in_cols)\n        pr1 = pad_row_total // 2\n        pr2 = pad_row_total - pr1\n        pc1 = pad_col_total // 2\n        pc2 = pad_col_total - pc1\n    elif isinstance(pad, int):\n        pr1 = pr2 = pad\n        pc1 = pc2 = pad\n    elif isinstance(pad, tuple):\n        if len(pad) == 2:\n            (pr, pc) = pad\n            pr1 = pr2 = pr\n            pc1 = pc2 = pc\n        elif len(pad) == 4:\n            (pr1, pr2, pc1, pc2) = pad\n        else:\n            pr1 = pr2 = pc1 = pc2 = 0\n    else:\n        pr1 = pr2 = pc1 = pc2 = 0\n    X_pad = np.pad(X, ((0, 0), (pr1, pr2), (pc1, pc2), (0, 0)), mode='constant')\n    (H_pad, W_pad) = (X_pad.shape[1], X_pad.shape[2])\n    H_out = (H_pad - kr_eff) // stride + 1\n    W_out = (W_pad - kc_eff) // stride + 1\n    if H_out <= 0 or W_out <= 0:\n        return np.zeros((n_ex, H_out, W_out, out_ch)).tolist()\n    (s_batch, s_H, s_W, s_ch) = X_pad.strides\n    new_shape = (n_ex, H_out, W_out, kr, kc, in_ch)\n    new_strides = (s_batch, s_H * stride, s_W * stride, s_H * (dilation + 1), s_W * (dilation + 1), s_ch)\n    patches = np.lib.stride_tricks.as_strided(X_pad, shape=new_shape, strides=new_strides, writeable=False)\n    patches_reshaped = patches.reshape(-1, kr * kc * in_ch)\n    W_reshaped = W.reshape(-1, out_ch)\n    result = np.matmul(patches_reshaped, W_reshaped)\n    output = result.reshape(n_ex, H_out, W_out, out_ch)\n    return output.tolist()"}
{"task_id": 355, "completion_id": 0, "solution": "import numpy as np\ndef pca_transform(data: np.ndarray, n_components: int, solver: str='svd') -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\"\"\"\n    centered_data = data - np.mean(data, axis=0)\n    if solver == 'svd':\n        (U, s, Vt) = np.linalg.svd(centered_data, full_matrices=False)\n        directions = Vt\n    elif solver == 'eigen':\n        cov_matrix = np.cov(centered_data, rowvar=False, ddof=1)\n        (eigenvalues, eigenvectors) = np.linalg.eigh(cov_matrix)\n        sorted_idx = np.argsort(eigenvalues)[::-1]\n        sorted_eigenvalues = eigenvalues[sorted_idx]\n        sorted_eigenvectors = eigenvectors[:, sorted_idx]\n        directions = sorted_eigenvectors.T\n    else:\n        raise ValueError(\"solver must be 'svd' or 'eigen'.\")\n    n_features = data.shape[1]\n    n_components = min(n_components, n_features)\n    directions = directions[:n_components]\n    for i in range(directions.shape[0]):\n        direction = directions[i]\n        non_zero_indices = np.flatnonzero(direction)\n        if non_zero_indices.size > 0:\n            first_non_zero = non_zero_indices[0]\n            if direction[first_non_zero] < 0:\n                directions[i] *= -1\n    scores = centered_data @ directions.T\n    if solver == 'svd':\n        total_variance = (s ** 2).sum()\n        explained_var_ratios = s[:n_components] ** 2 / total_variance\n    else:\n        total_variance = sorted_eigenvalues.sum()\n        explained_var_ratios = sorted_eigenvalues[:n_components] / total_variance\n    scores_rounded = np.round(scores, 4).tolist()\n    explained_var_ratios_rounded = np.round(explained_var_ratios, 4).tolist()\n    return (scores_rounded, explained_var_ratios_rounded)"}
{"task_id": 356, "completion_id": 0, "solution": "def leaf_predict(leaf: 'Leaf', classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        probs = leaf.value\n        max_prob = max(probs)\n        return probs.index(max_prob)\n    else:\n        return leaf.value"}
{"task_id": 357, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data: np.ndarray, K: int, max_iters: int=100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    centroids = data[:K].copy()\n    for _ in range(max_iters):\n        distances = np.sqrt(((data[:, np.newaxis] - centroids) ** 2).sum(axis=2))\n        closest = np.argmin(distances, axis=1)\n        new_centroids = centroids.copy()\n        for j in range(K):\n            mask = closest == j\n            if mask.any():\n                new_centroids[j] = data[mask].mean(axis=0)\n        if np.allclose(centroids, new_centroids, atol=1e-06):\n            break\n        centroids = new_centroids.copy()\n    sorted_centroids = sorted(centroids.tolist())\n    rounded_centroids = [[round(val, 4) for val in centroid] for centroid in sorted_centroids]\n    return rounded_centroids"}
{"task_id": 362, "completion_id": 0, "solution": "import numpy as np\ndef row_stochastic_matrix(data: list[list[int | float]] | 'np.ndarray') -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    arr = np.array(data, dtype=float)\n    row_sums = arr.sum(axis=1)\n    if np.any(row_sums == 0):\n        return -1\n    normalized = arr / row_sums[:, np.newaxis]\n    rounded = np.round(normalized, 4)\n    return rounded.tolist()"}
{"task_id": 363, "completion_id": 0, "solution": "from typing import Any, Dict, Iterable, Tuple as PyTuple\nclass Space:\n    pass\nclass Box(Space):\n\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\nclass Tuple(Space):\n\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\nclass Dict(Space):\n\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n\n    def all_subspaces_are_box(space):\n        if isinstance(space, Tuple):\n            return all((isinstance(s, Box) for s in space.spaces))\n        elif isinstance(space, Dict):\n            return all((isinstance(s, Box) for s in space.spaces.values()))\n        else:\n            return False\n    cont_action = all_subspaces_are_box(env.action_space) if tuple_action else isinstance(env.action_space, Box)\n    cont_obs = all_subspaces_are_box(env.observation_space) if tuple_obs else isinstance(env.observation_space, Box)\n    return (cont_action, cont_obs)"}
{"task_id": 369, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\"\"\"\n    sum_updates = sum(updates)\n    running_pred = -learning_rate * sum_updates\n    if regression:\n        return np.round(running_pred, 4)\n    else:\n        logits = running_pred\n        max_logits = np.max(logits, axis=1, keepdims=True)\n        shifted_logits = logits - max_logits\n        exp_logits = np.exp(shifted_logits)\n        softmax = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        return np.argmax(softmax, axis=1).astype(int)"}
{"task_id": 371, "completion_id": 0, "solution": "import numpy as np\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int=0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\"\"\"\n    (n_examples, l_in, in_channels) = X.shape\n    kernel_width = W.shape[0]\n    out_channels = W.shape[2]\n    if pad == 'same':\n        out_len = np.ceil(l_in / stride).astype(int)\n        effective_kernel = (kernel_width - 1) * (dilation + 1) + 1\n        total_pad = max(0, (out_len - 1) * stride + effective_kernel - l_in)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n    elif isinstance(pad, int):\n        p_left = pad\n        p_right = pad\n    elif isinstance(pad, tuple) and len(pad) == 2:\n        (p_left, p_right) = pad\n    else:\n        raise ValueError(\"Invalid pad argument. Must be 'same', int, or 2-tuple.\")\n    X_padded = np.pad(X, [(0, 0), (p_left, p_right), (0, 0)], mode='constant')\n    padded_length = X_padded.shape[1]\n    effective_kernel_size = (kernel_width - 1) * (dilation + 1) + 1\n    output_length = (padded_length - effective_kernel_size) // stride + 1\n    output_length = max(0, output_length)\n    if output_length == 0:\n        return np.zeros((n_examples, 0, out_channels)).tolist()\n    output_positions = np.arange(output_length) * stride\n    kernel_offsets = np.arange(kernel_width) * (dilation + 1)\n    indices = output_positions[:, None] + kernel_offsets[None, :]\n    valid_indices = (indices >= 0) & (indices < padded_length)\n    indices = np.where(valid_indices, indices, 0)\n    output = np.zeros((n_examples, output_length, out_channels))\n    for ex_idx in range(n_examples):\n        example = X_padded[ex_idx]\n        window = example[indices]\n        window = window * valid_indices[..., None]\n        example_output = np.einsum('okc,kco->oc', window, W)\n        output[ex_idx] = example_output\n    return output.tolist()"}
{"task_id": 373, "completion_id": 0, "solution": "import numpy as np\ndef gini(y: list[int] | 'np.ndarray') -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    if len(y) == 0:\n        return 0.0\n    (_, counts) = np.unique(y, return_counts=True)\n    probs = counts / len(y)\n    sum_squares = np.sum(probs ** 2)\n    return round(1.0 - sum_squares, 4)"}
{"task_id": 374, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float=0.01, n_iterations: int=10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\"\"\"\n    if X.shape[0] != y.shape[0]:\n        return -1\n    (m, n) = X.shape\n    y = y.ravel()\n    theta = np.zeros(n)\n    b = 0.0\n    for _ in range(n_iterations):\n        y_hat = X.dot(theta) + b\n        error = y_hat - y\n        dw = 2 / m * X.T.dot(error)\n        db = 2 / m * np.sum(error)\n        theta -= learning_rate * dw\n        b -= learning_rate * db\n    theta_rounded = [round(val, 4) for val in theta]\n    b_rounded = round(b, 4)\n    return (theta_rounded, b_rounded)"}
{"task_id": 376, "completion_id": 0, "solution": "import numpy as np\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    n = len(X)\n    for row in X:\n        if len(row) != n:\n            return False\n    x_np = np.array(X)\n    return np.allclose(x_np, x_np.T, rtol=0, atol=1e-08)"}
{"task_id": 377, "completion_id": 0, "solution": "import numpy as np\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"Compute the negative gradient for logistic loss in binary classification.\"\"\"\n    y_arr = np.array(y)\n    if np.any((y_arr != 0) & (y_arr != 1)):\n        return -1\n    f_arr = np.array(f)\n    y_prime = 2 * y_arr - 1\n    denominator = 1 + np.exp(y_prime * f_arr)\n    g = y_prime / denominator\n    g_rounded = np.round(g, 4)\n    return g_rounded.tolist()"}
{"task_id": 380, "completion_id": 0, "solution": "import numpy as np\ndef selu(x, order: int=0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    x_np = np.asarray(x)\n    if order == 0:\n        elu = np.where(x_np > 0, x_np, alpha * (np.exp(x_np) - 1))\n        result = scale * elu\n    elif order == 1:\n        derivative = np.where(x_np > 0, 1, alpha * np.exp(x_np))\n        result = scale * derivative\n    elif order == 2:\n        result = np.where(x_np > 0, 0, scale * alpha * np.exp(x_np))\n    else:\n        raise ValueError('Order must be 0, 1, or 2.')\n    rounded_result = np.round(result, 4)\n    return rounded_result.tolist()"}
{"task_id": 387, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float=0.1) -> list[float]:\n    x = np.array(x)\n    y = np.array(y)\n    y_hat = np.full_like(y, np.mean(y), dtype=np.float64)\n    for _ in range(n_estimators):\n        residuals = y - y_hat\n        sorted_indices = np.argsort(x)\n        x_sorted = x[sorted_indices]\n        r_sorted = residuals[sorted_indices]\n        split_candidates = []\n        for i in range(len(x_sorted) - 1):\n            if x_sorted[i] != x_sorted[i + 1]:\n                split = (x_sorted[i] + x_sorted[i + 1]) / 2\n                split_candidates.append(split)\n        if not split_candidates:\n            global_mean = np.mean(residuals)\n            y_hat += learning_rate * global_mean\n            continue\n        best_t = None\n        best_loss = float('inf')\n        best_left_mean = 0.0\n        best_right_mean = 0.0\n        for t in split_candidates:\n            left_mask = x_sorted <= t\n            right_mask = ~left_mask\n            left_mean = np.mean(r_sorted[left_mask])\n            right_mean = np.mean(r_sorted[right_mask])\n            pred = np.where(left_mask, left_mean, right_mean)\n            loss = np.sum((r_sorted - pred) ** 2)\n            if loss < best_loss:\n                best_loss = loss\n                best_t = t\n                best_left_mean = left_mean\n                best_right_mean = right_mean\n        mask = x <= best_t\n        r_hat = np.where(mask, best_left_mean, best_right_mean)\n        y_hat += learning_rate * r_hat\n    return np.round(y_hat, 4).tolist()"}
{"task_id": 394, "completion_id": 0, "solution": "import numpy as np\ndef rmsprop_update(w: np.ndarray, grad: np.ndarray, Eg: np.ndarray | None=None, learning_rate: float=0.01, rho: float=0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n    Eg_next = rho * Eg + (1 - rho) * grad ** 2\n    denominator = np.sqrt(Eg_next + 1e-08)\n    w_next = w - learning_rate * grad / denominator\n    return (np.round(w_next, 4).tolist(), np.round(Eg_next, 4).tolist())"}
{"task_id": 398, "completion_id": 0, "solution": "import numpy as np\ndef elu(x, alpha: float=0.1, derivative: bool=False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    x_arr = np.asarray(x)\n    if derivative:\n        result = np.where(x_arr >= 0, 1.0, alpha * np.exp(x_arr))\n    else:\n        result = np.where(x_arr >= 0, x_arr, alpha * (np.exp(x_arr) - 1))\n    return [round(val, 4) for val in result.tolist()]"}
{"task_id": 411, "completion_id": 0, "solution": "import numpy as np\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\"\"\"\n\n    def to_python_scalar(value):\n        if isinstance(value, np.generic):\n            return value.item()\n        return value\n\n    def get_tuple_flag(lst):\n        if not lst:\n            return False\n        return isinstance(lst[0], (list, tuple, np.ndarray))\n\n    def is_continuous(value):\n        if isinstance(value, np.generic):\n            value = value.item()\n        if isinstance(value, float):\n            return not value.is_integer()\n        if isinstance(value, int):\n            return False\n        return False\n\n    def process_dimensions(lst, is_tuple):\n        if not lst:\n            return ([], [])\n        dimensions = []\n        if is_tuple:\n            num_dim = len(lst[0])\n            dimensions = [[] for _ in range(num_dim)]\n            for elem in lst:\n                for i in range(num_dim):\n                    val = elem[i]\n                    py_val = to_python_scalar(val)\n                    dimensions[i].append(py_val)\n        else:\n            dimensions = [[to_python_scalar(elem) for elem in lst]]\n        n_per_dim = []\n        ids_per_dim = []\n        for dim in dimensions:\n            unique = sorted(set(dim))\n            n_per_dim.append(len(unique))\n            ids_per_dim.append(list(unique))\n        return (n_per_dim, ids_per_dim)\n    tuple_actions = get_tuple_flag(actions)\n    if tuple_actions:\n        action_dim = len(actions[0]) if actions else 0\n    else:\n        action_dim = 1 if actions else 0\n    multidim_actions = action_dim > 1\n    flattened_actions = []\n    if tuple_actions and actions:\n        for elem in actions:\n            flattened_actions.extend(elem)\n    else:\n        flattened_actions = list(actions)\n    continuous_actions = any((is_continuous(val) for val in flattened_actions))\n    (n_actions_per_dim, action_ids) = process_dimensions(actions, tuple_actions)\n    tuple_observations = get_tuple_flag(observations)\n    if tuple_observations:\n        obs_dim = len(observations[0]) if observations else 0\n    else:\n        obs_dim = 1 if observations else 0\n    multidim_observations = obs_dim > 1\n    flattened_observations = []\n    if tuple_observations and observations:\n        for elem in observations:\n            flattened_observations.extend(elem)\n    else:\n        flattened_observations = list(observations)\n    continuous_observations = any((is_continuous(val) for val in flattened_observations))\n    (n_obs_per_dim, obs_ids) = process_dimensions(observations, tuple_observations)\n    return {'tuple_actions': tuple_actions, 'tuple_observations': tuple_observations, 'multidim_actions': multidim_actions, 'multidim_observations': multidim_observations, 'continuous_actions': continuous_actions, 'continuous_observations': continuous_observations, 'n_actions_per_dim': n_actions_per_dim, 'action_dim': action_dim, 'n_obs_per_dim': n_obs_per_dim, 'obs_dim': obs_dim, 'action_ids': action_ids, 'obs_ids': obs_ids}"}
{"task_id": 413, "completion_id": 0, "solution": "import numpy as np\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int=10, learning_rate: float=0.1, max_depth: int=3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\"\"\"\n\n    def build_tree(X, y, depth, max_depth):\n        if depth >= max_depth or len(y) <= 1:\n            return {'value': np.mean(y)}\n        if np.var(y) == 0:\n            return {'value': np.mean(y)}\n        best_feature = None\n        best_threshold = None\n        best_mse = np.inf\n        (n_samples, n_features) = X.shape\n        for feature in range(n_features):\n            sorted_indices = np.argsort(X[:, feature])\n            sorted_X_feature = X[sorted_indices, feature]\n            sorted_y = y[sorted_indices]\n            sum_total = sorted_y.sum()\n            sum_sq_total = (sorted_y ** 2).sum()\n            sum_left = 0.0\n            sum_sq_left = 0.0\n            for i in range(1, len(sorted_y)):\n                sum_left += sorted_y[i - 1]\n                sum_sq_left += sorted_y[i - 1] ** 2\n                sum_right = sum_total - sum_left\n                sum_sq_right = sum_sq_total - sum_sq_left\n                n_left = i\n                n_right = len(sorted_y) - i\n                if n_left == 0 or n_right == 0:\n                    continue\n                mse_left = sum_sq_left - sum_left ** 2 / n_left\n                mse_right = sum_sq_right - sum_right ** 2 / n_right\n                total_mse = mse_left + mse_right\n                if total_mse < best_mse:\n                    best_mse = total_mse\n                    best_feature = feature\n                    best_threshold = (sorted_X_feature[i - 1] + sorted_X_feature[i]) / 2.0\n        if best_feature is None:\n            return {'value': np.mean(y)}\n        left_mask = X[:, best_feature] <= best_threshold\n        right_mask = ~left_mask\n        if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n            return {'value': np.mean(y)}\n        left_subtree = build_tree(X[left_mask], y[left_mask], depth + 1, max_depth)\n        right_subtree = build_tree(X[right_mask], y[right_mask], depth + 1, max_depth)\n        return {'feature': best_feature, 'threshold': best_threshold, 'left': left_subtree, 'right': right_subtree}\n\n    def predict_tree(node, X):\n        predictions = np.zeros(X.shape[0])\n        for i in range(X.shape[0]):\n            current_node = node\n            while True:\n                if 'value' in current_node:\n                    predictions[i] = current_node['value']\n                    break\n                else:\n                    feat = current_node['feature']\n                    thr = current_node['threshold']\n                    if X[i, feat] <= thr:\n                        current_node = current_node['left']\n                    else:\n                        current_node = current_node['right']\n        return predictions\n    initial_pred = np.mean(y_train)\n    current_train_pred = np.full_like(y_train, initial_pred, dtype=np.float64)\n    test_pred = np.full(X_test.shape[0], initial_pred, dtype=np.float64)\n    for _ in range(n_estimators):\n        residuals = y_train - current_train_pred\n        tree = build_tree(X_train, residuals, 0, max_depth)\n        pred_train = predict_tree(tree, X_train)\n        current_train_pred += learning_rate * pred_train\n        pred_test = predict_tree(tree, X_test)\n        test_pred += learning_rate * pred_test\n    test_pred_rounded = np.round(test_pred, 4)\n    return test_pred_rounded.tolist()"}
{"task_id": 416, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"Compute the PDF of a multivariate Gaussian distribution for given data points.\n    \n    Args:\n        X (np.ndarray): Data points, shape (n_samples, n_features) or (n_samples,) for 1D.\n        mean (np.ndarray): Mean vector of the distribution, shape (n_features,).\n        cov (np.ndarray): Covariance matrix, shape (n_features, n_features).\n        \n    Returns:\n        list: PDF values for each data point, rounded to four decimal places.\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    d = len(mean)\n    cov_inv = np.linalg.inv(cov)\n    cov_det = np.linalg.det(cov)\n    denominator = (2 * np.pi) ** (d / 2) * np.sqrt(cov_det)\n    norm = 1.0 / denominator\n    diff = X - mean\n    temp = diff @ cov_inv\n    quadratic = np.sum(temp * diff, axis=1)\n    exponents = -0.5 * quadratic\n    pdf_values = norm * np.exp(exponents)\n    return [round(val, 4) for val in pdf_values]"}
{"task_id": 419, "completion_id": 0, "solution": ""}
{"task_id": 423, "completion_id": 0, "solution": "import numpy as np\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    if len(actual) != len(predicted):\n        return -1\n    if not (all((x >= 0 for x in actual)) and all((x >= 0 for x in predicted))):\n        return -1\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    log_diff = np.log1p(predicted_np) - np.log1p(actual_np)\n    squared_diff = np.square(log_diff)\n    mean_squared_diff = np.mean(squared_diff)\n    rmsle = np.sqrt(mean_squared_diff)\n    return round(rmsle, 4)"}
{"task_id": 428, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef hann(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    N = window_len if symmetric else window_len + 1\n    samples = np.arange(N)\n    hann_values = 0.5 - 0.5 * np.cos(2 * math.pi * samples / (N - 1))\n    if not symmetric:\n        hann_values = hann_values[:window_len]\n    hann_rounded = np.round(hann_values, 4)\n    return hann_rounded.tolist()"}
{"task_id": 433, "completion_id": 0, "solution": "def glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    if len(shape) == 4:\n        (out_channels, in_channels, kernel_height, kernel_width) = shape\n        receptive_field_size = kernel_height * kernel_width\n        fan_in = in_channels * receptive_field_size\n        fan_out = out_channels * receptive_field_size\n    else:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    return (float(fan_in), float(fan_out))"}
{"task_id": 435, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_regressor(X: list[list[float]], y: list[float], n_estimators: int=10, learning_rate: float=0.1) -> list[float]:\n    X = np.array(X, dtype=np.float64)\n    y = np.array(y, dtype=np.float64)\n    m = X.shape[0]\n    mean_val = y.mean()\n    if n_estimators <= 0 or learning_rate == 0:\n        return [round(mean_val, 4)] * m\n    y_hat = np.full(m, mean_val, dtype=np.float64)\n    for _ in range(n_estimators):\n        r = y - y_hat\n        X_aug = np.hstack([np.ones((m, 1)), X])\n        theta = np.linalg.lstsq(X_aug, r, rcond=None)[0]\n        h = X_aug @ theta\n        y_hat += learning_rate * h\n    y_hat_rounded = np.round(y_hat, 4)\n    return y_hat_rounded.tolist()"}
{"task_id": 437, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x, order: int=0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in {0, 1, 2}:\n        return -1\n    original_is_scalar = np.isscalar(x)\n    x_arr = np.asarray(x)\n    s = 1 / (1 + np.exp(-x_arr))\n    if order == 0:\n        res = s\n    elif order == 1:\n        res = s * (1 - s)\n    else:\n        res = s * (1 - s) * (1 - 2 * s)\n    res_rounded = np.round(res, 4)\n    if original_is_scalar:\n        return float(res_rounded)\n    else:\n        return res_rounded.tolist()"}
{"task_id": 438, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None=None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    y_np = np.asarray(y)\n    if y_np.size == 0:\n        return []\n    max_label = y_np.max()\n    if num_classes is not None:\n        if num_classes < max_label + 1:\n            return -1\n        num_classes_ = num_classes\n    else:\n        num_classes_ = max_label + 1\n    n_samples = y_np.shape[0]\n    matrix = np.zeros((n_samples, num_classes_), dtype=int)\n    rows = np.arange(n_samples)\n    matrix[rows, y_np] = 1\n    return matrix.tolist()"}
{"task_id": 439, "completion_id": 0, "solution": "import math\nfrom collections import defaultdict\nimport itertools\ndef apriori_frequent_itemsets(transactions: list[list], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\"\"\"\n    if not transactions:\n        return []\n    n_transactions = len(transactions)\n    required_count = math.ceil(min_sup * n_transactions - 1e-12)\n    transaction_sets = [frozenset(trans) for trans in transactions]\n    item_counts = defaultdict(int)\n    for trans in transactions:\n        unique_items = set(trans)\n        for item in unique_items:\n            item_counts[item] += 1\n    frequent_items = []\n    frequent_1 = [tuple([item]) for item in sorted((item for (item, cnt) in item_counts.items() if cnt >= required_count))]\n    frequent_items.extend(frequent_1)\n    prev_frequent = frequent_1\n    k = 2\n    while True:\n        candidates = []\n        prev_sorted = sorted(prev_frequent)\n        len_prev = len(prev_sorted)\n        for i in range(len_prev):\n            itemset_i = prev_sorted[i]\n            for j in range(i + 1, len_prev):\n                itemset_j = prev_sorted[j]\n                if itemset_i[:-1] == itemset_j[:-1]:\n                    if itemset_i[-1] < itemset_j[-1]:\n                        new_candidate = itemset_i + (itemset_j[-1],)\n                        candidates.append(new_candidate)\n                else:\n                    break\n        prev_frequent_set = set(prev_sorted)\n        pruned_candidates = []\n        for candidate in candidates:\n            valid = True\n            for i in range(len(candidate)):\n                subset = candidate[:i] + candidate[i + 1:]\n                if subset not in prev_frequent_set:\n                    valid = False\n                    break\n            if valid:\n                pruned_candidates.append(candidate)\n        current_frequent = []\n        for candidate in pruned_candidates:\n            count = 0\n            for t_set in transaction_sets:\n                all_in = True\n                for item in candidate:\n                    if item not in t_set:\n                        all_in = False\n                        break\n                if all_in:\n                    count += 1\n            if count >= required_count:\n                current_frequent.append(candidate)\n        if not current_frequent:\n            break\n        frequent_items.extend(current_frequent)\n        prev_frequent = current_frequent\n        k += 1\n    frequent_items.sort(key=lambda x: (len(x), x))\n    return frequent_items"}
{"task_id": 440, "completion_id": 0, "solution": "import numpy as np\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    np_preds = np.array(predictions)\n    avg_probs = np.mean(np_preds, axis=0)\n    rounded_probs = np.round(avg_probs, 4)\n    predicted_classes = np.argmax(rounded_probs, axis=1)\n    return (rounded_probs.tolist(), predicted_classes.tolist())"}
{"task_id": 444, "completion_id": 0, "solution": "import numpy as np\ndef rbf_kernel(X: list[list[int | float]], Y: list[list[int | float]] | None=None, sigma: float | list[float] | None=None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\"\"\"\n    X_np = np.array(X)\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.array(Y)\n    if X_np.ndim != 2 or Y_np.ndim != 2:\n        return -1\n    if X_np.shape[1] != Y_np.shape[1]:\n        return -1\n    C = X_np.shape[1]\n    if sigma is None:\n        sigma_val = np.sqrt(C / 2.0)\n    elif isinstance(sigma, (float, int)):\n        if sigma <= 0:\n            return -1\n        sigma_val = sigma\n    elif isinstance(sigma, list):\n        if len(sigma) != C:\n            return -1\n        try:\n            sigma_arr = np.array(sigma, dtype=np.float64)\n        except (ValueError, TypeError):\n            return -1\n        if (sigma_arr <= 0).any():\n            return -1\n        sigma_val = sigma_arr\n    else:\n        return -1\n    diff = X_np[:, np.newaxis, :] - Y_np[np.newaxis, :, :]\n    scaled = diff / sigma_val\n    squared = scaled ** 2\n    sum_sq = squared.sum(axis=2)\n    k = np.exp(-0.5 * sum_sq)\n    rounded = np.round(k, decimals=4)\n    return rounded.tolist()"}
{"task_id": 446, "completion_id": 0, "solution": "import numpy as np\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    (n_samples, n_features) = X.shape\n\n    def entropy(labels):\n        if len(labels) == 0:\n            return 0.0\n        (_, counts) = np.unique(labels, return_counts=True)\n        probs = counts / counts.sum()\n        return -np.sum(probs * np.log2(probs))\n\n    def majority_label(labels):\n        if len(labels) == 0:\n            return 0\n        (values, counts) = np.unique(labels, return_counts=True)\n        max_count = np.max(counts)\n        candidates = values[counts == max_count]\n        return np.min(candidates)\n    parent_entropy = entropy(y)\n    if parent_entropy == 0.0:\n        majority = majority_label(y)\n        return (-1, None, majority, majority)\n    overall_majority = majority_label(y)\n    current_max_gain = -np.inf\n    best_feature_index = -1\n    best_threshold = None\n    left_maj = None\n    right_maj = None\n    for j in range(n_features):\n        feature_values = X[:, j]\n        unique_values = np.unique(feature_values)\n        if len(unique_values) < 2:\n            continue\n        sorted_values = np.sort(unique_values)\n        thresholds = (sorted_values[:-1] + sorted_values[1:]) / 2\n        for t in thresholds:\n            left_mask = feature_values <= t\n            n_left = np.sum(left_mask)\n            if n_left == 0 or n_left == n_samples:\n                continue\n            left_y = y[left_mask]\n            right_y = y[~left_mask]\n            h_left = entropy(left_y)\n            h_right = entropy(right_y)\n            p_left = n_left / n_samples\n            p_right = (n_samples - n_left) / n_samples\n            gain = parent_entropy - (p_left * h_left + p_right * h_right)\n            if gain > current_max_gain:\n                current_max_gain = gain\n                best_feature_index = j\n                best_threshold = t\n                left_maj = majority_label(left_y)\n                right_maj = majority_label(right_y)\n            elif gain == current_max_gain:\n                if j < best_feature_index:\n                    best_feature_index = j\n                    best_threshold = t\n                    left_maj = majority_label(left_y)\n                    right_maj = majority_label(right_y)\n                elif j == best_feature_index and t < best_threshold:\n                    best_threshold = t\n                    left_maj = majority_label(left_y)\n                    right_maj = majority_label(right_y)\n    if current_max_gain <= 0.0:\n        return (-1, None, overall_majority, overall_majority)\n    else:\n        rounded_threshold = round(best_threshold, 4)\n        return (best_feature_index, rounded_threshold, left_maj, right_maj)"}
{"task_id": 452, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\"\"\"\n    X_array = np.array(X) if isinstance(X, list) else X\n    left = []\n    right = []\n    is_numeric = type(threshold) in (int, float)\n    for row in X_array:\n        value = row[feature_i]\n        if is_numeric:\n            if value >= threshold:\n                left.append(row)\n            else:\n                right.append(row)\n        elif value == threshold:\n            left.append(row)\n        else:\n            right.append(row)\n    left_list = [r.tolist() for r in left]\n    right_list = [r.tolist() for r in right]\n    return [left_list, right_list]"}
{"task_id": 453, "completion_id": 0, "solution": "import numpy as np\ndef update_q_values(Q: 'np.ndarray', Q_next: 'np.ndarray', actions: list[int], rewards: list[float], dones: list[bool], gamma: float) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    if gamma < 0 or gamma > 1:\n        return -1\n    if Q.ndim != 2 or Q_next.ndim != 2 or Q.shape != Q_next.shape:\n        return -1\n    batch_size = Q.shape[0]\n    if len(actions) != batch_size or len(rewards) != batch_size or len(dones) != batch_size:\n        return -1\n    y = Q.copy()\n    for i in range(batch_size):\n        done = dones[i]\n        reward = rewards[i]\n        action = actions[i]\n        if done:\n            target = reward\n        else:\n            next_max = np.max(Q_next[i])\n            target = reward + gamma * next_max\n        y[i, action] = target\n    y_rounded = np.round(y, 4)\n    return y_rounded.tolist()"}
{"task_id": 458, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None=None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    labels_np = np.array(labels)\n    if labels_np.size == 0:\n        return []\n    if (labels_np < 0).any():\n        return -1\n    max_label = labels_np.max()\n    if num_classes is not None:\n        if num_classes < max_label + 1:\n            return -1\n    else:\n        num_classes = max_label + 1\n    result = []\n    for label in labels_np:\n        row = [0] * num_classes\n        row[label] = 1\n        result.append(row)\n    return result"}
{"task_id": 461, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_regressor(x: list[float], y: list[float], n_estimators: int=200, learning_rate: float=0.5) -> list[float]:\n    x = np.array(x)\n    y = np.array(y)\n    n_samples = len(x)\n    if n_samples == 0:\n        return []\n    y_pred = np.full(n_samples, np.mean(y))\n    if n_estimators <= 0:\n        return np.round(y_pred, 4).tolist()\n    for _ in range(n_estimators):\n        r = y - y_pred\n        if np.all(r == 0):\n            break\n        unique_x = np.unique(x)\n        split_candidates = unique_x[:-1] if len(unique_x) > 1 else []\n        best_sse = float('inf')\n        best_t = None\n        best_vL = None\n        best_vR = None\n        sum_r = np.sum(r)\n        sum_r_sq = np.sum(r ** 2)\n        if len(split_candidates) == 0:\n            v = np.mean(r)\n            stump_pred = np.full(n_samples, v)\n        else:\n            for t in split_candidates:\n                left_mask = x <= t\n                count_left = np.sum(left_mask)\n                if count_left == 0 or count_left == n_samples:\n                    continue\n                sum_left = np.sum(r[left_mask])\n                sum_left_sq = np.sum(r[left_mask] ** 2)\n                count_right = n_samples - count_left\n                sum_right = sum_r - sum_left\n                sum_right_sq = sum_r_sq - sum_left_sq\n                sse_left = sum_left_sq - sum_left ** 2 / count_left\n                sse_right = sum_right_sq - sum_right ** 2 / count_right\n                total_sse = sse_left + sse_right\n                if total_sse < best_sse or (total_sse == best_sse and (best_t is None or t < best_t)):\n                    best_sse = total_sse\n                    best_t = t\n                    best_vL = sum_left / count_left\n                    best_vR = sum_right / count_right\n            if best_t is not None:\n                stump_pred = np.where(x <= best_t, best_vL, best_vR)\n            else:\n                v = np.mean(r)\n                stump_pred = np.full(n_samples, v)\n        y_pred += learning_rate * stump_pred\n    return np.round(y_pred, 4).tolist()"}
{"task_id": 471, "completion_id": 0, "solution": "def expected_sarsa_update(q_table: list[list[float]], state: int, action: int, reward: float, next_state: int | None, epsilon: float, lr: float, gamma: float) -> list[list[float]]:\n    new_q = [row.copy() for row in q_table]\n    if next_state is None:\n        target = reward\n    else:\n        s_prime = next_state\n        q_s_prime = new_q[s_prime]\n        num_actions = len(q_s_prime)\n        max_q = max(q_s_prime)\n        a_greedy = q_s_prime.index(max_q)\n        expectation = 0.0\n        for a_prime in range(num_actions):\n            if a_prime == a_greedy:\n                prob = 1 - epsilon + epsilon / num_actions\n            else:\n                prob = epsilon / num_actions\n            expectation += prob * q_s_prime[a_prime]\n        target = reward + gamma * expectation\n    new_q[state][action] += lr * (target - new_q[state][action])\n    for row in new_q:\n        for i in range(len(row)):\n            row[i] = round(row[i], 4)\n    return new_q"}
{"task_id": 474, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef generate_data(m: int, n_train: int, n_val: int, interval: float, seed: int | None=None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\"\"\"\n    if seed is not None:\n        random.seed(seed)\n    X_train = []\n    X_val = []\n    Y_train = []\n    Y_val = []\n    for i in range(m):\n        label = (i - 0.5) * 2\n        x_floor = i // 2\n        x_low = (x_floor + 0.1) * 10\n        x_high = (x_floor + 0.9) * 10\n        y_mod = i % 2\n        y_low = (y_mod * interval + 0.1) * 10\n        y_high = (y_mod * interval + 0.9) * 10\n        for _ in range(n_train):\n            x = random.uniform(x_low, x_high)\n            y = random.uniform(y_low, y_high)\n            X_train.append([x, y])\n            Y_train.append(label)\n        for _ in range(n_val):\n            x = random.uniform(x_low, x_high)\n            y = random.uniform(y_low, y_high)\n            X_val.append([x, y])\n            Y_val.append(label)\n    X_train = np.round(np.array(X_train), 4)\n    X_val = np.round(np.array(X_val), 4)\n    Y_train = np.array(Y_train)\n    Y_val = np.array(Y_val)\n    return (X_train, X_val, Y_train, Y_val)"}
{"task_id": 475, "completion_id": 0, "solution": "import numpy as np\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-08):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    m_prev = np.zeros_like(grad) if m_prev is None else m_prev\n    v_prev = np.zeros_like(grad) if v_prev is None else v_prev\n    m_new = b1 * m_prev + (1 - b1) * grad\n    v_new = b2 * v_prev + (1 - b2) * grad ** 2\n    m_hat = m_new / (1 - b1 ** t)\n    v_hat = v_new / (1 - b2 ** t)\n    w_new = w - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n    return (w_new, m_new, v_new)"}
{"task_id": 479, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Optional\ndef epsilon_greedy_bandit(rewards: List[List[float]], epsilon: float=0.05, ev_prior: float=0.5, seed: Optional[int]=None) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    if not rewards:\n        return []\n    n_arms = len(rewards[0])\n    V = np.full(n_arms, ev_prior, dtype=np.float64)\n    C = np.zeros(n_arms, dtype=np.int64)\n    for t in range(len(rewards)):\n        current_rewards = rewards[t]\n        if np.random.rand() < epsilon:\n            chosen_arm = np.random.randint(n_arms)\n        else:\n            chosen_arm = np.argmax(V)\n        reward = current_rewards[chosen_arm]\n        C[chosen_arm] += 1\n        V[chosen_arm] += (reward - V[chosen_arm]) / C[chosen_arm]\n    return [round(val, 4) for val in V.tolist()]"}
{"task_id": 481, "completion_id": 0, "solution": "import numpy as np\nfrom collections import deque\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if len(data) == 0:\n        return []\n    labels = [-2] * len(data)\n    cluster_id = 0\n    for i in range(len(data)):\n        if labels[i] != -2:\n            continue\n        distances = np.linalg.norm(data - data[i], axis=1)\n        neighbors = np.where(distances <= eps)[0]\n        if len(neighbors) < min_samples:\n            labels[i] = -1\n            continue\n        labels[i] = cluster_id\n        queue = deque(neighbors.tolist())\n        while queue:\n            j = queue.popleft()\n            if labels[j] == -1:\n                labels[j] = cluster_id\n            elif labels[j] == -2:\n                labels[j] = cluster_id\n                j_distances = np.linalg.norm(data - data[j], axis=1)\n                j_neighbors = np.where(j_distances <= eps)[0]\n                if len(j_neighbors) >= min_samples:\n                    for n in j_neighbors:\n                        if labels[n] == -2 or labels[n] == -1:\n                            if labels[n] == -2:\n                                queue.append(n)\n                            labels[n] = cluster_id\n        cluster_id += 1\n    return labels"}
{"task_id": 482, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    y = np.asarray(y)\n    if np.any(y < 0):\n        raise ValueError('Labels must be non-negative.')\n    if num_classes is not None:\n        if not isinstance(num_classes, int) or num_classes < 0:\n            raise ValueError('num_classes must be a non-negative integer')\n        if np.any(y >= num_classes):\n            raise ValueError('Labels must be less than num_classes.')\n    elif y.size == 0:\n        num_classes = 0\n    else:\n        num_classes = np.max(y) + 1\n    n_samples = y.shape[0]\n    categorical = np.zeros((n_samples, num_classes), dtype=int)\n    if n_samples > 0 and num_classes > 0:\n        categorical[np.arange(n_samples), y] = 1\n    return categorical"}
{"task_id": 485, "completion_id": 0, "solution": "import numpy as np\ndef regression_tree_predict(X_train: list[float | int], y_train: list[float | int], X_test: list[float | int], max_depth: int=3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\"\"\"\n    if not X_train:\n        return []\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    sorted_indices = np.argsort(X_train)\n    X_sorted = X_train[sorted_indices]\n    y_sorted = y_train[sorted_indices]\n\n    def build_tree(X, y, current_depth):\n        if current_depth >= max_depth or len(X) < 2:\n            return {'mean': np.mean(y) if len(y) > 0 else 0.0}\n        best_sse = float('inf')\n        best_threshold = None\n        best_i = -1\n        for i in range(len(X) - 1):\n            if X[i] >= X[i + 1]:\n                continue\n            left_y = y[:i + 1]\n            right_y = y[i + 1:]\n            mean_left = np.mean(left_y)\n            mean_right = np.mean(right_y)\n            sse_left = np.sum((left_y - mean_left) ** 2)\n            sse_right = np.sum((right_y - mean_right) ** 2)\n            total_sse = sse_left + sse_right\n            if total_sse < best_sse:\n                best_sse = total_sse\n                best_threshold = (X[i] + X[i + 1]) / 2\n                best_i = i\n        if best_i == -1:\n            return {'mean': np.mean(y)}\n        left_X = X[:best_i + 1]\n        left_y = y[:best_i + 1]\n        right_X = X[best_i + 1:]\n        right_y = y[best_i + 1:]\n        left_child = build_tree(left_X, left_y, current_depth + 1)\n        right_child = build_tree(right_X, right_y, current_depth + 1)\n        return {'threshold': best_threshold, 'left': left_child, 'right': right_child}\n    root = build_tree(X_sorted, y_sorted, 0)\n\n    def predict_single(x):\n        node = root\n        while True:\n            if 'mean' in node:\n                return node['mean']\n            elif x <= node['threshold']:\n                node = node['left']\n            else:\n                node = node['right']\n    return [predict_single(x) for x in X_test]"}
{"task_id": 490, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x: list | tuple | np.ndarray) -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    arr = x if isinstance(x, np.ndarray) else np.array(x)\n    if arr.size == 0:\n        return []\n    return np.diag(arr).tolist()"}
{"task_id": 491, "completion_id": 0, "solution": "from collections import Counter, defaultdict\nimport itertools\nclass FPNode:\n\n    def __init__(self, item, count=0, parent=None):\n        self.item = item\n        self.count = count\n        self.parent = parent\n        self.children = defaultdict(FPNode)\n        self.node_link = None\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\"\"\"\n    item_counts = Counter()\n    for trans in transactions:\n        for item in trans:\n            item_counts[item] += 1\n    frequent_items = {item: cnt for (item, cnt) in item_counts.items() if cnt >= min_sup}\n    if not frequent_items:\n        return []\n    sorted_items = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n    rank = {item: idx for (idx, item) in enumerate(sorted_items)}\n    root = FPNode(None)\n    header_table = defaultdict(list)\n    for trans in transactions:\n        filtered = [item for item in trans if item in frequent_items]\n        filtered.sort(key=lambda x: rank[x])\n        current = root\n        for item in filtered:\n            if item in current.children:\n                current = current.children[item]\n                current.count += 1\n            else:\n                new_node = FPNode(item, 1, current)\n                current.children[item] = new_node\n                header_table[item].append(new_node)\n                current = new_node\n    frequent_itemsets = []\n\n    def is_single_path(node):\n        if not node.children:\n            return True\n        if len(node.children) > 1:\n            return False\n        return is_single_path(next(iter(node.children.values())))\n\n    def mine_tree(header, min_sup, suffix, frequent_itemsets):\n        items = sorted(header.keys(), key=lambda x: (frequent_items[x], x))\n        for item in items:\n            new_suffix = tuple(sorted(list(suffix) + [item]))\n            frequent_itemsets.append(new_suffix)\n            conditional_patterns = []\n            for node in header[item]:\n                path = []\n                count = node.count\n                parent = node.parent\n                while parent.item is not None:\n                    path.append(parent.item)\n                    parent = parent.parent\n                path.reverse()\n                conditional_patterns.append((path, count))\n            cond_counts = Counter()\n            for (path, cnt) in conditional_patterns:\n                for it in path:\n                    cond_counts[it] += cnt\n            cond_freq = {it: cnt for (it, cnt) in cond_counts.items() if cnt >= min_sup}\n            if not cond_freq:\n                continue\n            cond_sorted = sorted(cond_freq.keys(), key=lambda x: (-cond_counts[x], x))\n            cond_root = FPNode(None)\n            cond_header = defaultdict(list)\n            for (path, cnt) in conditional_patterns:\n                filtered_path = [it for it in path if it in cond_freq]\n                filtered_path.sort(key=lambda x: (-cond_counts[x], x))\n                current = cond_root\n                for it in filtered_path:\n                    if it in current.children:\n                        current = current.children[it]\n                        current.count += cnt\n                    else:\n                        new_node = FPNode(it, cnt, current)\n                        current.children[it] = new_node\n                        cond_header[it].append(new_node)\n                        current = new_node\n            if is_single_path(cond_root):\n                path_items = []\n                current_node = cond_root\n                while current_node.children:\n                    next_node = next(iter(current_node.children.values()))\n                    path_items.append(next_node.item)\n                    current_node = next_node\n                for i in range(1, len(path_items) + 1):\n                    for subset in itertools.combinations(path_items, i):\n                        combined = tuple(sorted(list(subset) + [item] + list(suffix)))\n                        frequent_itemsets.append(combined)\n            else:\n                mine_tree(cond_header, min_sup, new_suffix, frequent_itemsets)\n    mine_tree(header_table, min_sup, tuple(), frequent_itemsets)\n    unique = list({tuple(sorted(itemset)) for itemset in frequent_itemsets})\n    unique.sort(key=lambda x: (len(x), x))\n    return unique"}
{"task_id": 492, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    if degree < 0:\n        return -1\n    X = np.array(X, dtype=float)\n    (n_samples, m) = X.shape\n    exponents = []\n    for current_degree in range(degree + 1):\n        if current_degree == 0:\n            exponents.append((0,) * m)\n        else:\n            for comb in combinations_with_replacement(range(m), current_degree):\n                exp = [0] * m\n                for idx in comb:\n                    exp[idx] += 1\n                exponents.append(tuple(exp))\n    result = []\n    for row in X:\n        new_row = []\n        for exp in exponents:\n            product = 1.0\n            for i in range(m):\n                product *= row[i] ** exp[i]\n            new_row.append(product)\n        result.append(new_row)\n    return result"}
{"task_id": 493, "completion_id": 0, "solution": "import numpy as np\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    y_arr = np.array(y)\n    if y_arr.size == 0:\n        return -1\n    mean_val = np.mean(y_arr)\n    mse_value = np.mean((y_arr - mean_val) ** 2)\n    return round(mse_value, 4)"}
{"task_id": 496, "completion_id": 0, "solution": "import numpy as np\ndef apply_affine(x, slope: float=1.0, intercept: float=0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    x_arr = np.atleast_1d(np.asarray(x))\n    y = slope * x_arr + intercept\n    grad = np.full_like(y, slope)\n    grad2 = np.zeros_like(y)\n    y_list = np.round(y, 4).tolist()\n    grad_list = np.round(grad, 4).tolist()\n    grad2_list = np.round(grad2, 4).tolist()\n    return (y_list, grad_list, grad2_list)"}
{"task_id": 499, "completion_id": 0, "solution": "import numpy as np\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\"\"\"\n    (n_samples, n_features) = data.shape\n    if k < 1 or k > n_features:\n        return -1\n    data_centered = data - np.mean(data, axis=0)\n    var_pop = np.var(data_centered, axis=0, ddof=0)\n    std = np.sqrt(var_pop)\n    std[std == 0] = 1.0\n    data_std = data_centered / std\n    cov_matrix = np.cov(data_std, rowvar=False, ddof=1)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(cov_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues_sorted = eigenvalues[sorted_indices]\n    eigenvectors_sorted = eigenvectors[:, sorted_indices]\n    for i in range(eigenvectors_sorted.shape[1]):\n        eigenvector = eigenvectors_sorted[:, i]\n        max_abs_idx = np.argmax(np.abs(eigenvector))\n        if eigenvector[max_abs_idx] < 0:\n            eigenvectors_sorted[:, i] = -eigenvector\n    projection = data_std @ eigenvectors_sorted[:, :k]\n    return np.round(projection, 4).tolist()"}
{"task_id": 500, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int=20, learning_rate: float=0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\"\"\"\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    (n_samples, n_features) = X_train.shape\n    c = np.mean(y_train)\n    F = np.full(n_samples, c)\n    stumps = []\n    for m in range(n_estimators):\n        residuals = y_train - F\n        best_sse = float('inf')\n        best_j = None\n        best_t = None\n        best_left_val = None\n        best_right_val = None\n        for j in range(n_features):\n            feature_values = X_train[:, j]\n            unique_values = np.unique(feature_values)\n            for t in unique_values:\n                left_mask = feature_values <= t\n                n_left = left_mask.sum()\n                n_right = n_samples - n_left\n                if n_left == 0 or n_right == 0:\n                    continue\n                left_res = residuals[left_mask]\n                right_res = residuals[~left_mask]\n                left_val = left_res.mean()\n                right_val = right_res.mean()\n                sse = np.sum((left_res - left_val) ** 2) + np.sum((right_res - right_val) ** 2)\n                if sse < best_sse:\n                    best_sse = sse\n                    best_j = j\n                    best_t = t\n                    best_left_val = left_val\n                    best_right_val = right_val\n        if best_j is not None:\n            stumps.append((best_j, best_t, best_left_val, best_right_val))\n            h_pred = np.where(X_train[:, best_j] <= best_t, best_left_val, best_right_val)\n            F += learning_rate * h_pred\n    predictions = []\n    for x in X_test:\n        f = c\n        for (j, t, lv, rv) in stumps:\n            if x[j] <= t:\n                f += learning_rate * lv\n            else:\n                f += learning_rate * rv\n        predictions.append(1 if f >= 0.5 else 0)\n    return predictions"}
{"task_id": 505, "completion_id": 0, "solution": "import numpy as np\ndef contrastive_divergence(X: np.ndarray, W: np.ndarray, hbias: np.ndarray, vbias: np.ndarray, learning_rate: float, k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\"\"\"\n    if X.size == 0:\n        return []\n    m = X.shape[0]\n    h0_input = X.dot(W) + hbias\n    h0_prob = 1 / (1 + np.exp(-h0_input))\n    if k == 0:\n        v_k = X\n        h_k = h0_prob\n    else:\n        current_h = h0_prob\n        for _ in range(k):\n            v_input = current_h.dot(W.T) + vbias\n            v = 1 / (1 + np.exp(-v_input))\n            h_input = v.dot(W) + hbias\n            current_h = 1 / (1 + np.exp(-h_input))\n        v_k = v\n        h_k = current_h\n    pos_grad = X.T.dot(h0_prob)\n    neg_grad = v_k.T.dot(h_k)\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n    return np.round(W_new, 4).tolist()"}
{"task_id": 509, "completion_id": 0, "solution": "import numpy as np\ndef chebyshev(x: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray') -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    if len(x) != len(y):\n        return -1.0\n    if len(x) == 0:\n        return 0.0\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    diff = np.abs(x_arr - y_arr)\n    max_diff = np.max(diff)\n    return round(max_diff, 4)"}
{"task_id": 510, "completion_id": 0, "solution": "import numpy as np\ndef vae_loss(y: np.ndarray, y_pred: np.ndarray, t_mean: np.ndarray, t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    epsilon = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    bce = y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped)\n    reconstruction_loss = -np.sum(bce, axis=1)\n    kl = -0.5 * np.sum(1 + t_log_var - np.square(t_mean) - np.exp(t_log_var), axis=1)\n    total_loss = reconstruction_loss + kl\n    average_loss = np.mean(total_loss)\n    return round(average_loss.item(), 6)"}
{"task_id": 513, "completion_id": 0, "solution": "import numpy as np\ndef fm_predict(X: list[list[int | float]], w0: float, w: list[float], V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\"\"\"\n    X_np = np.array(X)\n    w_np = np.array(w)\n    V_np = np.array(V)\n    linear_term = w0 + X_np.dot(w_np)\n    XV = X_np.dot(V_np)\n    sum_XV_sq = (XV ** 2).sum(axis=1)\n    X_sq = X_np ** 2\n    V_sq = V_np ** 2\n    sum_XV_sq_terms = X_sq.dot(V_sq).sum(axis=1)\n    interaction_term = 0.5 * (sum_XV_sq - sum_XV_sq_terms)\n    predictions = linear_term + interaction_term\n    rounded_predictions = np.round(predictions, 4)\n    return rounded_predictions.tolist()"}
{"task_id": 517, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    is_scalar = isinstance(z, (int, float))\n    if is_scalar:\n        z = np.array([z])\n    else:\n        z = np.asarray(z)\n    sigmoid_values = np.where(z >= 0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))\n    rounded = np.round(sigmoid_values, 4)\n    if is_scalar:\n        return float(rounded[0])\n    else:\n        return rounded.tolist()"}
{"task_id": 518, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Callable\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n\n    def wrapper(actual: np.ndarray, predicted: np.ndarray):\n        if actual.ndim == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        if predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        return function(actual, predicted)\n    return wrapper"}
{"task_id": 520, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        grad = z * (1 - z)\n        return np.round(grad, 4)\n    elif isinstance(z, list):\n        return [sigmoid_grad(elem) for elem in z]\n    else:\n        return np.round(z * (1 - z), 4).item()"}
{"task_id": 528, "completion_id": 0, "solution": "import numpy as np\ndef decision_boundary_grid(X: list[list[int | float]], W: list[int | float], b: float, grid_n: int=100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    X_np = np.array(X)\n    (x1_min, x1_max) = (X_np[:, 0].min(), X_np[:, 0].max())\n    (x2_min, x2_max) = (X_np[:, 1].min(), X_np[:, 1].max())\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    (X1, X2) = np.meshgrid(x1_plot, x2_plot, indexing='ij')\n    Z = W[0] * X1 + W[1] * X2 + b\n    labels = np.where(Z >= 0, 1, -1)\n    return labels.tolist()"}
{"task_id": 537, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\"\"\"\n    mask0 = y_train == 0\n    X0 = X_train[mask0]\n    X1 = X_train[~mask0]\n    mean0 = np.mean(X0, axis=0)\n    std0 = np.std(X0, axis=0, ddof=1)\n    mean1 = np.mean(X1, axis=0)\n    std1 = np.std(X1, axis=0, ddof=1)\n    epsilon = 1e-09\n    std0 += epsilon\n    std1 += epsilon\n    prior0 = X0.shape[0] / X_train.shape[0]\n    prior1 = X1.shape[0] / X_train.shape[0]\n    predictions = []\n    for x in X_test:\n        log_likelihood0 = -0.5 * np.log(2 * np.pi) - np.log(std0) - (x - mean0) ** 2 / (2 * std0 ** 2)\n        log_prob0 = np.log(prior0) + np.sum(log_likelihood0)\n        log_likelihood1 = -0.5 * np.log(2 * np.pi) - np.log(std1) - (x - mean1) ** 2 / (2 * std1 ** 2)\n        log_prob1 = np.log(prior1) + np.sum(log_likelihood1)\n        predictions.append(0 if log_prob0 > log_prob1 else 1)\n    return predictions"}
{"task_id": 539, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\"\"\"\n    m = Y.size\n    AL_clipped = np.clip(AL, 1e-15, 1 - 1e-15)\n    cross_entropy = -(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped))\n    cost = np.sum(cross_entropy) / m\n    return float(cost)"}
{"task_id": 552, "completion_id": 0, "solution": "import numpy as np\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    T = len(observations)\n    if t < 0 or t >= T:\n        return []\n    pi = np.array(hmm['pi'])\n    A = np.array(hmm['A'])\n    B = np.array(hmm['B'])\n    n_states = len(pi)\n    alpha = np.zeros((T, n_states))\n    alpha[0] = pi * B[:, observations[0]]\n    for time in range(1, T):\n        for j in range(n_states):\n            alpha[time, j] = B[j, observations[time]] * np.dot(alpha[time - 1], A[:, j])\n    beta = np.ones((T, n_states))\n    for time in reversed(range(T - 1)):\n        for i in range(n_states):\n            beta[time, i] = np.dot(A[i, :], B[:, observations[time + 1]] * beta[time + 1, :])\n    gamma = alpha[t] * beta[t]\n    gamma_sum = gamma.sum()\n    if gamma_sum == 0:\n        return [0.0] * n_states\n    gamma_normalized = gamma / gamma_sum\n    rounded_gamma = [round(prob, 4) for prob in gamma_normalized.tolist()]\n    return rounded_gamma"}
{"task_id": 555, "completion_id": 0, "solution": "import numpy as np\nfrom collections import deque\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\"\"\"\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n    distances = np.sqrt(np.sum((data[:, np.newaxis, :] - data) ** 2, axis=2))\n    neighborhood = []\n    for i in range(n_samples):\n        mask = distances[i] <= eps\n        neighbors = np.where(mask)[0].tolist()\n        neighborhood.append(neighbors)\n    core_mask = [len(neighbors) >= min_samples for neighbors in neighborhood]\n    labels = [0] * n_samples\n    cluster_id = 0\n    for i in range(n_samples):\n        if labels[i] != 0:\n            continue\n        if not core_mask[i]:\n            continue\n        cluster_id += 1\n        labels[i] = cluster_id\n        queue = deque()\n        queue.append(i)\n        while queue:\n            current_point = queue.popleft()\n            for neighbor in neighborhood[current_point]:\n                if labels[neighbor] == 0:\n                    labels[neighbor] = cluster_id\n                    if core_mask[neighbor]:\n                        queue.append(neighbor)\n    for i in range(n_samples):\n        if labels[i] == 0:\n            labels[i] = -1\n    return labels"}
{"task_id": 561, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef gmm_em(data: np.ndarray, k: int, max_iter: int=200, epsilon: float=0.0001) -> list[int]:\n    \"\"\"Perform Gaussian Mixture Model clustering using the EM algorithm and return deterministic cluster labels.\"\"\"\n    if k == 1:\n        return [0] * len(data)\n    (n_samples, d) = data.shape\n    data_sum = np.sum(data)\n    seed = int(abs(data_sum * 1000000.0)) + k\n    np.random.seed(seed)\n    indices = np.random.choice(n_samples, k, replace=False)\n    mu = data[indices]\n    pi = np.ones(k) / k\n    data_cov = np.cov(data, rowvar=False) + 1e-06 * np.eye(d)\n    sigma = np.tile(data_cov, (k, 1, 1))\n    prev_pi = pi.copy()\n    for _ in range(max_iter):\n        log_probs = np.zeros((n_samples, k))\n        for ki in range(k):\n            mu_k = mu[ki]\n            sigma_k = sigma[ki]\n            try:\n                sigma_inv = np.linalg.inv(sigma_k)\n                log_det = np.log(np.linalg.det(sigma_k))\n            except np.linalg.LinAlgError:\n                sigma_k_reg = sigma_k + 1e-06 * np.eye(d)\n                sigma_inv = np.linalg.inv(sigma_k_reg)\n                log_det = np.log(np.linalg.det(sigma_k_reg))\n            diff = data - mu_k\n            quadratic = np.einsum('ij,ij->i', diff @ sigma_inv, diff)\n            log_prob = np.log(pi[ki]) - 0.5 * (d * np.log(2 * np.pi) + log_det + quadratic)\n            log_probs[:, ki] = log_prob\n        max_log_probs = np.max(log_probs, axis=1, keepdims=True)\n        log_probs -= max_log_probs\n        gamma = np.exp(log_probs)\n        gamma /= gamma.sum(axis=1, keepdims=True)\n        n_k = gamma.sum(axis=0)\n        pi = n_k / n_samples\n        n_k = np.maximum(n_k, 1e-06)\n        mu = gamma.T @ data / n_k[:, np.newaxis]\n        for ki in range(k):\n            diff = data - mu[ki]\n            sigma[ki] = gamma[:, ki] * diff.T @ diff / n_k[ki]\n            sigma[ki] += 1e-06 * np.eye(d)\n        delta = np.max(np.abs(pi - prev_pi))\n        if delta < epsilon:\n            break\n        prev_pi = pi.copy()\n    log_probs = np.zeros((n_samples, k))\n    for ki in range(k):\n        mu_k = mu[ki]\n        sigma_k = sigma[ki]\n        sigma_inv = np.linalg.inv(sigma_k)\n        log_det = np.log(np.linalg.det(sigma_k))\n        diff = data - mu_k\n        quadratic = np.einsum('ij,ij->i', diff @ sigma_inv, diff)\n        log_prob = np.log(pi[ki]) - 0.5 * (d * np.log(2 * np.pi) + log_det + quadratic)\n        log_probs[:, ki] = log_prob\n    max_log_probs = np.max(log_probs, axis=1, keepdims=True)\n    log_probs -= max_log_probs\n    gamma = np.exp(log_probs)\n    gamma /= gamma.sum(axis=1, keepdims=True)\n    original_assignments = np.argmax(gamma, axis=1)\n    permutation = np.argsort(mu[:, 0])\n    inv_perm = np.zeros(k, dtype=int)\n    for (new_label, original_label) in enumerate(permutation):\n        inv_perm[original_label] = new_label\n    new_assignments = inv_perm[original_assignments]\n    return new_assignments.tolist()"}
{"task_id": 562, "completion_id": 0, "solution": "import numpy as np\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float=2.0, method: str='unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\"\"\"\n    if n_cluster == 1:\n        return [0] * len(data)\n    x_squared = np.sum(data ** 2, axis=1)\n    x_dot = data @ data.T\n    squared_distances = x_squared[:, np.newaxis] + x_squared[np.newaxis, :] - 2 * x_dot\n    W = np.exp(-gamma * squared_distances)\n    degrees = W.sum(axis=1)\n    if method == 'unnormalized':\n        L = np.diag(degrees) - W\n    elif method == 'normalized':\n        D_inv_sqrt = 1.0 / np.sqrt(degrees)\n        L = np.diag(degrees) - W\n        L = D_inv_sqrt[:, np.newaxis] * L * D_inv_sqrt[np.newaxis, :]\n    else:\n        raise ValueError(\"Method must be 'unnormalized' or 'normalized'\")\n    (eigenvalues, eigenvectors) = np.linalg.eigh(L)\n    k_eigenvectors = eigenvectors[:, :n_cluster]\n    if method == 'normalized':\n        row_norms = np.linalg.norm(k_eigenvectors, axis=1, keepdims=True)\n        row_norms[row_norms == 0] = 1\n        k_eigenvectors = k_eigenvectors / row_norms\n\n    def run_kmeans(X, k):\n        centroids = X[:k].copy()\n        while True:\n            distances = np.linalg.norm(X[:, np.newaxis, :] - centroids, axis=2)\n            labels = np.argmin(distances, axis=1)\n            new_centroids = np.zeros_like(centroids)\n            for i in range(k):\n                mask = labels == i\n                if mask.any():\n                    new_centroids[i] = X[mask].mean(axis=0)\n                else:\n                    new_centroids[i] = centroids[i]\n            if np.allclose(new_centroids, centroids):\n                break\n            centroids = new_centroids.copy()\n        cluster_indices = {}\n        for (idx, label) in enumerate(labels):\n            cluster_indices.setdefault(label, []).append(idx)\n        min_indices = {label: min(indices) for (label, indices) in cluster_indices.items()}\n        sorted_labels = sorted(min_indices.keys(), key=lambda x: min_indices[x])\n        label_map = {old: new for (new, old) in enumerate(sorted_labels)}\n        return [label_map[label] for label in labels]\n    labels = run_kmeans(k_eigenvectors, n_cluster)\n    return labels"}
{"task_id": 563, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef perceptron_dual_train(X_data: np.ndarray, y_data: np.ndarray, eta: float=1.0, max_iter: int=1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\"\"\"\n    N = X_data.shape[0]\n    alpha = np.zeros(N)\n    b = 0.0\n    G = X_data @ X_data.T\n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            g = np.dot(alpha * y_data, G[:, i])\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n        if not updated:\n            break\n    w = np.dot(alpha * y_data, X_data)\n    w_rounded = np.round(w, 4)\n    b_rounded = np.round(b, 4)\n    return (w_rounded.tolist(), float(b_rounded))"}
{"task_id": 564, "completion_id": 0, "solution": "def predict_sample(root: 'Node', features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    current = root\n    while True:\n        if hasattr(current, 'y'):\n            return current.y\n        if task_type == 'classification':\n            feature_val = features[current.label]\n            found_child = None\n            for child in current.child:\n                if child.x == feature_val:\n                    found_child = child\n                    break\n            current = current.child[1] if found_child is None else found_child\n        else:\n            feature_val = features[current.label]\n            if feature_val <= current.s:\n                current = current.child[0]\n            else:\n                current = current.child[1]"}
{"task_id": 565, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\"\"\"\n    (n_samples, n_features) = X.shape\n    if n_samples == 0:\n        return (0, 0.0)\n    counts_y = np.bincount(y)\n    probs_y = counts_y[counts_y > 0] / n_samples\n    H_y = -np.sum(probs_y * np.log2(probs_y))\n    max_ig = -float('inf')\n    best_feature = 0\n    for j in range(n_features):\n        feature_col = X[:, j]\n        (values, value_counts) = np.unique(feature_col, return_counts=True)\n        cond_entropy = 0.0\n        for (v, cnt) in zip(values, value_counts):\n            mask = feature_col == v\n            y_subset = y[mask]\n            counts_sub = np.bincount(y_subset)\n            probs_sub = counts_sub[counts_sub > 0] / cnt\n            entropy_sub = -np.sum(probs_sub * np.log2(probs_sub))\n            cond_entropy += cnt / n_samples * entropy_sub\n        ig = H_y - cond_entropy\n        if ig > max_ig or (ig == max_ig and j < best_feature):\n            max_ig = ig\n            best_feature = j\n    max_ig_rounded = round(max_ig, 6)\n    return (best_feature, max_ig_rounded)"}
{"task_id": 566, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    np.random.seed(42)\n    if epsilon == 0:\n        return []\n    (n_samples, n_features) = data.shape\n    s = min(sample_size, n_samples)\n    if s <= 1:\n        phi = 1.0\n    else:\n        phi = 2 * math.log(s - 1) - 2 * (s - 1) / s\n    trees = []\n    for _ in range(n_trees):\n        if sample_size >= n_samples:\n            subsample = data\n        else:\n            indices = np.random.choice(n_samples, size=sample_size, replace=False)\n            subsample = data[indices]\n        if s == 0:\n            height_limit = 0\n        else:\n            height_limit = math.ceil(math.log2(s)) if s > 0 else 0\n\n        def build_tree(node_data, current_depth):\n            if current_depth >= height_limit or len(node_data) <= 1:\n                return {'type': 'leaf', 'n': len(node_data)}\n            else:\n                feature_idx = np.random.randint(n_features)\n                min_val = np.min(node_data[:, feature_idx])\n                max_val = np.max(node_data[:, feature_idx])\n                threshold = np.random.uniform(min_val, max_val)\n                left_mask = node_data[:, feature_idx] <= threshold\n                left_data = node_data[left_mask]\n                right_data = node_data[~left_mask]\n                left_child = build_tree(left_data, current_depth + 1)\n                right_child = build_tree(right_data, current_depth + 1)\n                return {'type': 'split', 'feature': feature_idx, 'threshold': threshold, 'left': left_child, 'right': right_child}\n        tree = build_tree(subsample, 0)\n        trees.append(tree)\n    path_lengths = np.zeros((n_samples, n_trees))\n    for (tree_idx, tree) in enumerate(trees):\n        for data_idx in range(n_samples):\n            x = data[data_idx]\n            current_node = tree\n            depth = 0\n            while True:\n                if current_node['type'] == 'leaf':\n                    n = current_node['n']\n                    if n <= 1:\n                        c = 0.0\n                    else:\n                        c = 2 * math.log(n - 1) + 0.5772156649 - 2 * (n - 1) / n\n                    path_lengths[data_idx, tree_idx] = depth + c\n                    break\n                else:\n                    feat = current_node['feature']\n                    thresh = current_node['threshold']\n                    if x[feat] <= thresh:\n                        current_node = current_node['left']\n                    else:\n                        current_node = current_node['right']\n                    depth += 1\n    avg_h = np.mean(path_lengths, axis=1)\n    scores = 2 ** (-avg_h / phi)\n    if epsilon >= 1:\n        selected_indices = np.arange(n_samples)\n    else:\n        m = math.ceil(epsilon * n_samples)\n        sorted_indices = np.argsort(-scores)\n        selected_indices = sorted_indices[:m]\n    selected_indices.sort()\n    return selected_indices.tolist()"}
{"task_id": 568, "completion_id": 0, "solution": "import math\nfrom collections import defaultdict\nimport numpy as np\ndef maxent_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, epsilon: float=0.001, n_iter: int=100) -> list[int]:\n    \"\"\"Implements a Maximum Entropy classifier using Generalized Iterative Scaling.\"\"\"\n    feature_counts = defaultdict(int)\n    (N, num_features) = X_train.shape\n    for (x, y) in zip(X_train, y_train):\n        for i in range(num_features):\n            v = x[i]\n            feature_counts[i, v, y] += 1\n    features = list(feature_counts.keys())\n    num_feature_pairs = len(features)\n    if num_feature_pairs == 0:\n        return [0] * X_test.shape[0]\n    feature_to_j = {key: j for (j, key) in enumerate(features)}\n    emp_exp = np.array([feature_counts[key] / N for key in features])\n    iv_to_x_indices = defaultdict(list)\n    for x_idx in range(N):\n        x = X_train[x_idx]\n        for i in range(num_features):\n            v = x[i]\n            iv_to_x_indices[i, v].append(x_idx)\n    unique_labels = np.unique(y_train)\n    C = 0\n    for x in X_train:\n        for y in unique_labels:\n            current_sum = 0\n            for i in range(num_features):\n                v = x[i]\n                if (i, v, y) in feature_to_j:\n                    current_sum += 1\n            if current_sum > C:\n                C = current_sum\n    if C == 0:\n        C = 1\n    weights = np.zeros(num_feature_pairs)\n    K = len(unique_labels)\n    unique_labels_list = unique_labels.tolist()\n    for _ in range(n_iter):\n        prob = np.zeros((N, K))\n        for x_idx in range(N):\n            x = X_train[x_idx]\n            scores = np.zeros(K)\n            for (yk_idx, yk) in enumerate(unique_labels):\n                score = 0.0\n                for i in range(num_features):\n                    v = x[i]\n                    key = (i, v, yk)\n                    if key in feature_to_j:\n                        j = feature_to_j[key]\n                        score += weights[j]\n                scores[yk_idx] = score\n            max_score = np.max(scores)\n            exp_scores = np.exp(scores - max_score)\n            Z = np.sum(exp_scores)\n            if Z == 0:\n                prob[x_idx] = np.ones(K) / K\n            else:\n                prob[x_idx] = exp_scores / Z\n        model_exp = np.zeros(num_feature_pairs)\n        for j in range(num_feature_pairs):\n            (i, v, l) = features[j]\n            l_idx = unique_labels_list.index(l)\n            x_indices = iv_to_x_indices.get((i, v), [])\n            if x_indices:\n                model_exp[j] = np.sum(prob[x_indices, l_idx]) / N\n        delta = 1.0 / C * np.log(emp_exp / (model_exp + 1e-10))\n        weights += delta\n        max_delta = np.max(np.abs(delta))\n        if max_delta < epsilon:\n            break\n    predictions = []\n    for x in X_test:\n        scores = np.zeros(K)\n        for (yk_idx, yk) in enumerate(unique_labels):\n            score = 0.0\n            for i in range(num_features):\n                v = x[i]\n                key = (i, v, yk)\n                if key in feature_to_j:\n                    j = feature_to_j[key]\n                    score += weights[j]\n            scores[yk_idx] = score\n        pred_idx = np.argmax(scores)\n        predictions.append(unique_labels[pred_idx])\n    return predictions"}
{"task_id": 569, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\ndef statistic_key_word(data: List[Tuple[Any, Iterable[str]]], cut_off: int | None=None) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\"\"\"\n    doc_counts = Counter()\n    total_docs = len(data)\n    for doc in data:\n        (_, words) = doc\n        unique_words = set(words)\n        for word in unique_words:\n            doc_counts[word] += 1\n    ratios = []\n    for (word, count) in doc_counts.items():\n        ratio = count / total_docs if total_docs != 0 else 0.0\n        ratio_rounded = round(ratio, 4)\n        ratios.append((word, ratio_rounded))\n    sorted_ratios = sorted(ratios, key=lambda x: (-x[1], x[0]))\n    if cut_off is None:\n        words_set = {word for (word, _) in sorted_ratios}\n    elif cut_off <= 0:\n        words_set = set()\n    else:\n        k = min(cut_off, len(sorted_ratios))\n        words_set = {word for (word, _) in sorted_ratios[:k]}\n    return (sorted_ratios, words_set)"}
{"task_id": 571, "completion_id": 0, "solution": "import numpy as np\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    w = X.T @ (alpha * y)\n    w_rounded = np.round(w, 4)\n    w_list = w_rounded.tolist()\n    residuals = y - X @ w\n    b = np.round(residuals.mean(), 4)\n    return (w_list, float(b))"}
{"task_id": 572, "completion_id": 0, "solution": "import numpy as np\ndef local_outlier_factor(data, k, epsilon=1.5):\n    n = data.shape[0]\n    distance_matrix = np.sqrt(np.sum((data[:, np.newaxis] - data) ** 2, axis=2))\n    k_distances = []\n    neighbors = []\n    for p in range(n):\n        row = distance_matrix[p]\n        sorted_indices = np.argsort(row)\n        sorted_distances = row[sorted_indices]\n        k_distance = sorted_distances[k]\n        k_neighbors = sorted_indices[1:k + 1].tolist()\n        k_distances.append(k_distance)\n        neighbors.append(k_neighbors)\n    LRD = np.zeros(n)\n    for p in range(n):\n        sum_reach = 0.0\n        for q in neighbors[p]:\n            reach_dist = max(k_distances[q], distance_matrix[p, q])\n            sum_reach += reach_dist\n        LRD[p] = k / sum_reach if sum_reach != 0 else 0.0\n    LOF = np.zeros(n)\n    for p in range(n):\n        sum_LRD = 0.0\n        for q in neighbors[p]:\n            sum_LRD += LRD[q]\n        avg_LRD = sum_LRD / k\n        LOF[p] = avg_LRD / LRD[p] if LRD[p] != 0 else 0.0\n    LOF_rounded = np.round(LOF, 4)\n    outlier_indices = np.where(LOF_rounded > epsilon)[0].tolist()\n    outlier_indices.sort()\n    return (LOF_rounded.tolist(), outlier_indices)"}
{"task_id": 574, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef generate_data(m: int, n_train: int, n_val: int, seed: int | None=None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\"\"\"\n    if seed is not None:\n        random.seed(seed)\n    X_train = []\n    X_val = []\n    y_train = []\n    y_val = []\n    for c in range(m):\n        base = 10 * (c // 2)\n        low = base + 1\n        high = base + 9\n        for _ in range(n_train):\n            x = random.uniform(low, high)\n            y = random.uniform(low, high)\n            X_train.append([x, y])\n            y_train.append(c)\n        for _ in range(n_val):\n            x = random.uniform(low, high)\n            y = random.uniform(low, high)\n            X_val.append([x, y])\n            y_val.append(c)\n    X_train = np.array(X_train).round(4)\n    X_val = np.array(X_val).round(4)\n    y_train = np.array(y_train)\n    y_val = np.array(y_val)\n    return (X_train, X_val, y_train, y_val)"}
{"task_id": 579, "completion_id": 0, "solution": ""}
{"task_id": 581, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regression(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int, l1_penalty: float, l2_penalty: float) -> tuple[list[float], float]:\n    (m, n) = X.shape\n    if learning_rate == 0.0 or iterations == 0:\n        return ([round(0.0, 4) for _ in range(n)], round(0.0, 4))\n    w = np.zeros(n)\n    b = 0.0\n    for _ in range(iterations):\n        y_pred = X.dot(w) + b\n        residual = y - y_pred\n        sign_w = np.where(w == 0, -1, np.sign(w))\n        grad_w = -2 * X.T.dot(residual) + l1_penalty * sign_w + 2 * l2_penalty * w\n        grad_b = -2 * np.sum(residual)\n        w -= learning_rate * grad_w / m\n        b -= learning_rate * grad_b / m\n    weights_rounded = [round(weight, 4) for weight in w]\n    bias_rounded = round(b, 4)\n    return (weights_rounded, bias_rounded)"}
{"task_id": 591, "completion_id": 0, "solution": "import numpy as np\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    current = np.array(x, dtype=np.float64)\n    for (i, layer) in enumerate(weights):\n        augmented = np.insert(current, 0, 1.0)\n        layer_weights = np.array(layer, dtype=np.float64)\n        output = np.dot(augmented, layer_weights.T)\n        if i != len(weights) - 1:\n            output = np.maximum(output, 0)\n        current = output\n    rounded = np.round(current, 4)\n    return rounded.item() if len(rounded) == 1 else rounded.tolist()"}
{"task_id": 595, "completion_id": 0, "solution": "import numpy as np\ndef kmeans_centroids(X: 'np.ndarray', k: int, max_iters: int=100) -> list[list[float]]:\n    \"\"\"Compute the centroids using K-Means clustering algorithm.\"\"\"\n    if k == 0:\n        return []\n    centroids = X[:k].copy()\n    for _ in range(max_iters):\n        distances = np.sum((X[:, np.newaxis] - centroids) ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.zeros_like(centroids)\n        for i in range(k):\n            mask = labels == i\n            if np.any(mask):\n                new_centroids[i] = X[mask].mean(axis=0)\n            else:\n                new_centroids[i] = centroids[i]\n        delta = np.linalg.norm(new_centroids - centroids, axis=1)\n        if np.all(delta < 0.0001):\n            break\n        centroids = new_centroids\n    if centroids.size == 0:\n        sorted_centroids = centroids\n    else:\n        keys = [centroids[:, col] for col in reversed(range(centroids.shape[1]))]\n        order = np.lexsort(keys)\n        sorted_centroids = centroids[order]\n    rounded_centroids = np.round(sorted_centroids, 4)\n    return rounded_centroids.tolist()"}
