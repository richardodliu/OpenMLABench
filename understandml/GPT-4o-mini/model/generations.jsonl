{"task_id": 0, "completion_id": 0, "completion": "Here's a Python implementation of the Ridge Regression closed-form solver as described in your task. The function takes a NumPy array as input, where the last column is the target variable and the preceding columns are the features. It adds a bias term, computes the weight vector using the closed-form solution, and handles the case where the matrix is not invertible.\n\n```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"\n    Perform Ridge Regression using the closed-form solution.\n    \n    Parameters:\n    - data: A 2D numpy array where the last column is the target variable\n            and all preceding columns are the features.\n    - alpha: Regularization strength (non-negative float).\n    \n    Returns:\n    - A list of coefficients rounded to 4 decimal places, or -1 if the matrix is not invertible.\n    \"\"\"\n    # Split the data into features (X) and target (y)\n    X = data[:, :-1]  # All columns except the last\n    y = data[:, -1]   # Last column\n    \n    # Add a bias (intercept) term to X\n    X_bias = np.hstack((np.ones((X.shape[0], 1)), X))  # Add a column of ones\n    \n    # Compute the closed-form solution for Ridge Regression\n    I = np.eye(X_bias.shape[1])  # Identity matrix of size (number of features + 1)\n    \n    # Calculate the matrix to be inverted\n    matrix_to_invert = X_bias.T @ X_bias + alpha * I\n    \n    # Check if the matrix is invertible\n    if np.linalg.det(matrix_to_invert) == 0:\n        return -1\n    \n    # Calculate the weight vector\n    w_hat = np.linalg.inv(matrix_to_invert) @ (X_bias.T @ y)\n    \n    # Round the coefficients to 4 decimal places and convert to a list\n    return np.round(w_hat, 4).tolist()\n```\n\n### Explanation:\n1. **Data Preparation**: The function separates the features and target variable from the input data. It then adds a bias term (a column of ones) to the feature matrix.\n2. **Matrix Calculation**: It constructs the matrix \\((\\mathbf{X}^\\top \\mathbf{X} + \\alpha \\mathbf{I})\\) that needs to be inverted.\n3. **Invertibility Check**: Before attempting to invert the matrix, it checks if the determinant is zero. If it is, the function returns -1.\n4. **Weight Calculation**: If the matrix is invertible, it computes the weight vector using the closed-form solution.\n5. **Rounding and Return**: Finally, it rounds the coefficients to four decimal places and returns them as a list.\n\nYou can use this function by passing a NumPy array with your data, and it will return the Ridge Regression coefficients or -1 if the matrix is not invertible."}
{"task_id": 1, "completion_id": 0, "completion": "Here's the implementation of the dual-form perceptron learning algorithm as described in your task. The function `perceptron_dual` takes in the training samples and labels, and it updates the dual parameters according to the specified rules. Finally, it computes the weight vector `w` and the bias `b`, rounding them to four decimal places before returning.\n\n```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or \u20111) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of updates, by default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        The weight vector (as a list) and the bias; both rounded to 4 decimals.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    alpha = np.zeros(n_samples)  # Dual parameters\n    b = 0.0  # Bias term\n\n    for _ in range(n_iter):\n        updated = False\n        for i in range(n_samples):\n            # Compute the activation\n            activation = np.sum(alpha * y_train * np.dot(X_train, X_train[i])) + b\n            \n            # Check the margin\n            if y_train[i] * (activation) <= 0:\n                # Misclassified sample, update alpha and b\n                alpha[i] += eta\n                b += eta * y_train[i]\n                updated = True\n                # Restart the scan from the beginning\n                break\n        \n        # If no updates were made, we can stop\n        if not updated:\n            break\n\n    # Compute the weight vector w from the final alpha\n    w = np.sum((alpha * y_train).reshape(-1, 1) * X_train, axis=0)\n\n    # Round the results to 4 decimal places\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = round(b, 4)\n\n    return w_rounded, b_rounded\n```\n\n### Explanation of the Code:\n1. **Initialization**: The dual parameters `alpha` are initialized to zero, and the bias `b` is also initialized to zero.\n2. **Training Loop**: The outer loop runs for a maximum of `n_iter` iterations. The inner loop scans through each sample.\n3. **Activation Calculation**: For each sample, the activation is calculated using the current values of `alpha`, `y_train`, and the inner product of the training samples.\n4. **Margin Check**: If the margin condition is violated (i.e., the sample is misclassified), the dual parameter `alpha[i]` is updated, and the bias `b` is adjusted. The scan restarts from the beginning.\n5. **Stopping Condition**: If a full pass through the data occurs without any updates, the training stops.\n6. **Weight Vector Calculation**: After training, the weight vector `w` is computed as a linear combination of the training samples weighted by `alpha` and their corresponding labels `y_train`.\n7. **Rounding**: Finally, both `w` and `b` are rounded to four decimal places before being returned.\n\nThis implementation adheres to the specifications provided and should work correctly for linearly separable datasets."}
{"task_id": 2, "completion_id": 0, "completion": "Here's the implementation of the `glm_links` function that provides the required link functions for Generalized Linear Models (GLMs). Each link function includes the link, its inverse, and the derivative with respect to \u03bc. The functions are designed to work with both scalars and numpy arrays.\n\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    \n    def identity_link(mu):\n        return mu\n    \n    def identity_inv_link(eta):\n        return eta\n    \n    def identity_derivative(mu):\n        return np.ones_like(mu)\n\n    def log_link(mu):\n        return np.log(mu)\n    \n    def log_inv_link(eta):\n        return np.exp(eta)\n    \n    def log_derivative(mu):\n        return 1 / mu\n\n    def logit_link(mu):\n        return np.log(mu / (1 - mu))\n    \n    def logit_inv_link(eta):\n        return 1 / (1 + np.exp(-eta))\n    \n    def logit_derivative(mu):\n        return 1 / (mu * (1 - mu))\n\n    return {\n        \"identity\": {\n            \"link\": identity_link,\n            \"inv_link\": identity_inv_link,\n            \"derivative\": identity_derivative\n        },\n        \"log\": {\n            \"link\": log_link,\n            \"inv_link\": log_inv_link,\n            \"derivative\": log_derivative\n        },\n        \"logit\": {\n            \"link\": logit_link,\n            \"inv_link\": logit_inv_link,\n            \"derivative\": logit_derivative\n        }\n    }\n\n# Example usage\nlinks = glm_links()\nmu = np.array([0.2, 0.8])\neta_logit = links[\"logit\"][\"link\"](mu)        # \u2192 [-1.3863, 1.3863]\nmu2_logit = links[\"logit\"][\"inv_link\"](eta_logit)    # \u2192 [0.2, 0.8]\n\n# Print rounded results for demonstration\nprint(\"eta (logit):\", np.round(eta_logit, 4))\nprint(\"mu2 (logit):\", np.round(mu2_logit, 4))\n```\n\n### Explanation of the Code:\n1. **Identity Link**:\n   - `link`: Returns the input `mu` as is.\n   - `inv_link`: Returns the input `eta` as is.\n   - `derivative`: Returns an array of ones with the same shape as `mu`.\n\n2. **Log Link**:\n   - `link`: Computes the natural logarithm of `mu`.\n   - `inv_link`: Computes the exponential of `eta`.\n   - `derivative`: Computes the derivative \\( \\frac{1}{\\mu} \\).\n\n3. **Logit Link**:\n   - `link`: Computes the logit transformation \\( \\log(\\frac{\\mu}{1 - \\mu}) \\).\n   - `inv_link`: Computes the inverse logit transformation \\( \\frac{1}{1 + e^{-\\eta}} \\).\n   - `derivative`: Computes the derivative \\( \\frac{1}{\\mu(1 - \\mu)} \\).\n\n### Usage:\nThe functions can handle both scalars and numpy arrays, allowing for element-wise operations. The example usage demonstrates how to use the logit link function to transform `mu` to `eta` and back to `mu`. The results are printed rounded to four decimal places for clarity."}
{"task_id": 3, "completion_id": 0, "completion": "To implement the isolation tree path length calculation as described, we will create a recursive function that builds the isolation tree and tracks the depth of each sample. Below is the complete implementation of the `isolation_tree_path_length` function:\n\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)  # Set the random seed for reproducibility\n    n_samples, n_features = data.shape\n    depths = [0] * n_samples  # Initialize depths for each sample\n\n    def build_tree(indices, depth):\n        if len(indices) < 3:\n            # If fewer than 3 samples, mark the depth for these indices\n            for idx in indices:\n                depths[idx] = depth\n            return\n        \n        # Randomly select a feature index\n        feature_index = np.random.randint(n_features)\n        feature_values = data[indices, feature_index]\n        \n        # Determine the split value\n        down = np.min(feature_values)\n        up = np.max(feature_values)\n        v = np.random.uniform(down, up)\n        \n        # Split the indices based on the chosen value v\n        left_indices = indices[feature_values <= v]\n        right_indices = indices[feature_values > v]\n        \n        # Recursively build the left and right subtrees\n        build_tree(left_indices, depth + 1)\n        build_tree(right_indices, depth + 1)\n\n    # Start building the tree from the root\n    all_indices = np.arange(n_samples)\n    build_tree(all_indices, 0)\n    \n    return depths\n\n# Example usage\ndata = np.arange(5).reshape(-1, 1)\noutput = isolation_tree_path_length(data)\nprint(output)  # Output: [2, 2, 2, 1, 1]\n```\n\n### Explanation of the Code:\n1. **Initialization**: We set the random seed to ensure reproducibility and initialize a list `depths` to store the depth of each sample.\n\n2. **Recursive Function `build_tree`**:\n   - If the number of indices is less than 3, we mark the current depth for all indices and return.\n   - We randomly select a feature index and determine the minimum and maximum values of that feature for the current indices.\n   - We then draw a random split value `v` from the range `[down, up]`.\n   - We split the indices into two groups: those with feature values less than or equal to `v` (left child) and those greater than `v` (right child).\n   - We recursively call `build_tree` for both left and right child indices, increasing the depth by 1.\n\n3. **Starting the Tree Construction**: We call `build_tree` with all sample indices starting from depth 0.\n\n4. **Return the Depths**: Finally, we return the list of depths corresponding to each sample.\n\nThis implementation adheres to the specifications provided and should work correctly for any 2D NumPy array input."}
{"task_id": 7, "completion_id": 0, "completion": "To implement the function for frequent itemset mining, we will follow these steps:\n\n1. **Calculate the total number of transactions** and determine the minimum support count based on the `minsup` threshold.\n2. **Count the occurrences of individual items** in the transactions.\n3. **Count the occurrences of item pairs** (combinations of two items) in the transactions.\n4. **Store the results** in a dictionary, ensuring that the itemsets are stored as sorted tuples.\n5. **Return the dictionary** containing all frequent itemsets that meet or exceed the minimum support count.\n\nHere's the complete implementation:\n\n```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    N = len(transactions)\n    if N == 0:\n        return {}\n\n    # Calculate the minimum support count\n    min_support_count = math.ceil(minsup * N)\n\n    # Count single items\n    item_count = Counter()\n    for transaction in transactions:\n        unique_items = set(transaction)  # Ignore duplicates in the same transaction\n        item_count.update(unique_items)\n\n    # Prepare the result dictionary\n    frequent_itemsets = {}\n\n    # Add frequent single items\n    for item, count in item_count.items():\n        if count >= min_support_count:\n            frequent_itemsets[(item,)] = count\n\n    # Count pairs of items\n    pair_count = Counter()\n    for transaction in transactions:\n        unique_items = set(transaction)\n        for item1, item2 in combinations(sorted(unique_items), 2):\n            pair_count[(item1, item2)] += 1\n\n    # Add frequent pairs\n    for itemset, count in pair_count.items():\n        if count >= min_support_count:\n            frequent_itemsets[itemset] = count\n\n    return frequent_itemsets\n\n# Example usage\ntransactions = [\n    ['bread', 'milk'],\n    ['bread', 'diaper', 'beer', 'egg'],\n    ['milk', 'diaper', 'beer', 'coke'],\n    ['bread', 'milk', 'diaper', 'beer'],\n    ['bread', 'milk', 'diaper', 'coke']\n]\nminsup = 0.6\n\noutput = find_frequent_itemsets(transactions, minsup)\nprint(output)\n```\n\n### Explanation of the Code:\n- **Counting Single Items**: We use a `Counter` to count how many times each unique item appears across all transactions.\n- **Counting Pairs**: We generate all combinations of two items from each transaction and count their occurrences using another `Counter`.\n- **Support Check**: For both single items and pairs, we check if their counts meet or exceed the minimum support count and add them to the result dictionary.\n- **Return Value**: The function returns a dictionary where keys are tuples of items (sorted) and values are their respective support counts.\n\nThis implementation efficiently finds frequent itemsets while adhering to the constraints provided."}
{"task_id": 11, "completion_id": 0, "completion": "Here's a complete implementation of the K-Means clustering algorithm from scratch, following the requirements you've provided:\n\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    \n    # Step 1: Initialize cluster centers\n    centers = data[:k].copy()\n    n_samples = data.shape[0]\n    labels = np.zeros(n_samples, dtype=int)\n    \n    for iteration in range(max_iter):\n        # Step 2: Assign samples to the nearest center\n        for i in range(n_samples):\n            distances = np.sum((data[i] - centers) ** 2, axis=1)  # Squared Euclidean distance\n            labels[i] = np.argmin(distances)\n        \n        # Step 3: Recompute centers\n        new_centers = np.zeros_like(centers)\n        for j in range(k):\n            assigned_samples = data[labels == j]\n            if len(assigned_samples) > 0:\n                new_centers[j] = np.mean(assigned_samples, axis=0)\n            else:\n                # Reinitialize center with a random sample if empty\n                random_index = np.random.randint(0, n_samples)\n                new_centers[j] = data[random_index]\n        \n        # Step 4: Check for convergence\n        center_shifts = np.linalg.norm(new_centers - centers, axis=1)\n        if np.max(center_shifts) < epsilon:\n            break\n        \n        centers = new_centers\n    \n    # Round the centers to 4 decimal places\n    centers = np.round(centers, 4)\n    \n    return centers.tolist(), labels.tolist()\n```\n\n### Explanation of the Code:\n1. **Initialization**: The first `k` samples from the dataset are used as the initial cluster centers.\n2. **Assignment Step**: Each sample is assigned to the nearest cluster center based on the squared Euclidean distance.\n3. **Update Step**: The new centers are computed as the mean of all samples assigned to each cluster. If a cluster ends up empty, a random sample from the dataset is chosen as the new center.\n4. **Convergence Check**: The algorithm checks if the maximum shift of any center is less than `epsilon`. If so, it stops; otherwise, it continues for a maximum of `max_iter` iterations.\n5. **Rounding**: Finally, the centers are rounded to four decimal places before returning.\n\n### Usage:\nYou can call this function with your dataset and desired number of clusters to perform K-Means clustering. For example:\n\n```python\ndata = np.array([[1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0]])\nk = 3\ncenters, labels = kmeans(data, k)\nprint(\"Cluster Centers:\", centers)\nprint(\"Labels:\", labels)\n``` \n\nThis will output the cluster centers and the labels for each sample in the dataset."}
{"task_id": 13, "completion_id": 0, "completion": "Here's a complete implementation of the item-based k-NN collaborative filtering recommender in Python using NumPy. The function follows the steps outlined in your task description.\n\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user. It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    n_users, n_items = data.shape\n    \n    # Step 1: Build the item-item similarity matrix S\n    S = np.zeros((n_items, n_items))\n    \n    for i in range(n_items):\n        for j in range(i + 1, n_items):\n            # Find users who rated both items i and j\n            users = np.where((data[:, i] > 0) & (data[:, j] > 0))[0]\n            if len(users) == 0:\n                continue\n            \n            v1 = data[users, i]\n            v2 = data[users, j]\n            \n            if criterion == \"cosine\":\n                # Mean center if std > 1e-3\n                if np.std(v1) > 1e-3:\n                    v1 = v1 - np.mean(v1)\n                if np.std(v2) > 1e-3:\n                    v2 = v2 - np.mean(v2)\n                \n                # Compute cosine similarity\n                sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)) if np.linalg.norm(v1) > 0 and np.linalg.norm(v2) > 0 else 0\n            elif criterion == \"pearson\":\n                # Compute Pearson correlation\n                sim = np.corrcoef(v1, v2)[0, 1] if len(v1) > 1 else 0\n            \n            S[i, j] = S[j, i] = sim\n    \n    # Step 2: Collect the indices of the items rated by the active user\n    rated_items = np.where(data[user_ind] > 0)[0]\n    r = data[user_ind, rated_items]\n    \n    # Step 3: Compute predicted attractiveness for each unrated item\n    scores = {}\n    \n    for t in range(n_items):\n        if t in rated_items:\n            continue\n        \n        # Calculate score(t)\n        numerator = 0\n        denominator = 0\n        \n        for i in rated_items:\n            if S[t, i] != 0:\n                numerator += r[np.where(rated_items == i)[0][0]] * S[t, i]\n                denominator += abs(S[t, i])\n        \n        score_t = numerator / denominator if denominator != 0 else 0\n        scores[t] = score_t\n    \n    # Step 4: Sort items by score and return the top k\n    sorted_items = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n    recommended_items = [item for item, score in sorted_items[:k]]\n    \n    return recommended_items\n```\n\n### Explanation of the Code:\n1. **Similarity Matrix Construction**: The function iterates through all pairs of items and computes their similarity based on the specified criterion (cosine or Pearson). It uses only the users who rated both items to calculate the similarity.\n  \n2. **Rated Items Collection**: It collects the indices of items that the active user has rated and their corresponding ratings.\n\n3. **Score Calculation**: For each item that the active user has not rated, it calculates a predicted score based on the ratings of the active user and the similarity scores.\n\n4. **Sorting and Returning Recommendations**: Finally, it sorts the items based on their predicted scores and returns the top `k` items.\n\nThis implementation should work efficiently for the task described, and it adheres to the constraints and requirements provided."}
{"task_id": 19, "completion_id": 0, "completion": "To implement the `best_split` function for finding the optimal split in a gradient-boosting tree, we need to follow these steps:\n\n1. **Calculate the total gradients and Hessians** for the current node.\n2. **Iterate through each feature** and each unique value of that feature to consider it as a potential split threshold.\n3. **For each threshold**, split the data into left and right child nodes and compute the gradients and Hessians for each child.\n4. **Calculate the loss for the current node and the child nodes** using the provided formula.\n5. **Compute the gain** from the split and keep track of the best gain and corresponding feature and threshold.\n6. **Return the best feature and threshold** if the gain is positive; otherwise, return `None`.\n\nHere's the implementation of the `best_split` function:\n\n```python\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* containing the feature\n        values of all training instances that reach the current node.\n    g : np.ndarray\n        1-D array with the first-order gradients for each training instance.\n    h : np.ndarray\n        1-D array with the second-order gradients (Hessians).\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to the denominator when computing the\n        weight of a leaf.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        A pair *(feature_index, threshold)* describing the optimal split, or\n        ``None`` if no valid split yields a positive gain.\n    \"\"\"\n    \n    n_samples, n_features = X.shape\n    \n    # Calculate total gradients and Hessians for the current node\n    total_g = np.sum(g)\n    total_h = np.sum(h)\n    \n    # Calculate the loss for the current node\n    def loss(g_sum, h_sum):\n        return -0.5 * (g_sum ** 2) / (h_sum + lam) + gamma\n    \n    current_loss = loss(total_g, total_h)\n    \n    best_gain = 0\n    best_split_info = None\n    \n    # Iterate over each feature\n    for feature_index in range(n_features):\n        # Get unique values of the feature\n        unique_values = np.unique(X[:, feature_index])\n        \n        # Iterate over each unique value as a potential threshold\n        for threshold in unique_values:\n            # Create masks for left and right splits\n            left_mask = X[:, feature_index] <= threshold\n            right_mask = X[:, feature_index] > threshold\n            \n            # Ensure both sides have at least 2 samples\n            if np.sum(left_mask) < 2 or np.sum(right_mask) < 2:\n                continue\n            \n            # Calculate gradients and Hessians for left and right splits\n            g_left = np.sum(g[left_mask])\n            h_left = np.sum(h[left_mask])\n            g_right = np.sum(g[right_mask])\n            h_right = np.sum(h[right_mask])\n            \n            # Calculate the loss for left and right nodes\n            left_loss = loss(g_left, h_left)\n            right_loss = loss(g_right, h_right)\n            \n            # Calculate gain\n            gain = current_loss - left_loss - right_loss\n            \n            # Update best gain and split info if this gain is better\n            if gain > best_gain:\n                best_gain = gain\n                best_split_info = (feature_index, threshold)\n    \n    # Return the best split info if gain is positive, otherwise None\n    return best_split_info if best_gain > 0 else None\n```\n\n### Explanation:\n- The function first computes the total gradients and Hessians for the current node.\n- It then iterates through each feature and each unique value of that feature to evaluate potential splits.\n- For each split, it checks if both child nodes have at least two samples.\n- It calculates the loss for the current node and the child nodes, computes the gain, and keeps track of the best gain found.\n- Finally, it returns the best feature and threshold if a positive gain was found; otherwise, it returns `None`."}
{"task_id": 20, "completion_id": 0, "completion": "Here's the implementation of the `sigmoid_activation` function that meets all the specified requirements:\n\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    # Convert input to a NumPy array for uniform processing\n    x_array = np.asarray(x)\n    \n    # Compute the sigmoid function\n    sigmoid_x = 1 / (1 + np.exp(-x_array))\n    \n    # Compute the gradient of the sigmoid function\n    gradient_x = sigmoid_x * (1 - sigmoid_x)\n    \n    # Round results to 4 decimal places\n    sigmoid_x_rounded = np.round(sigmoid_x, 4)\n    gradient_x_rounded = np.round(gradient_x, 4)\n    \n    # Check if the input was a scalar\n    if np.isscalar(x):\n        return float(sigmoid_x_rounded), float(gradient_x_rounded)\n    else:\n        return sigmoid_x_rounded.tolist(), gradient_x_rounded.tolist()\n\n# Example usage:\n# print(sigmoid_activation(0))  # Scalar input\n# print(sigmoid_activation([0, 1, 2]))  # List input\n# print(sigmoid_activation(np.array([[0, 1], [2, 3]])))  # 2-D NumPy array input\n```\n\n### Explanation:\n1. **Input Handling**: The function first converts the input `x` into a NumPy array using `np.asarray()`, which allows for uniform processing regardless of whether the input is a scalar, list, or NumPy array.\n\n2. **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n\n3. **Gradient Calculation**: The gradient of the sigmoid function is calculated using the formula \\( \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) \\).\n\n4. **Rounding**: Both the sigmoid values and their gradients are rounded to 4 decimal places using `np.round()`.\n\n5. **Return Type**: The function checks if the input was a scalar using `np.isscalar()`. If it is, it returns the results as floats; otherwise, it converts the results to Python lists using `.tolist()`.\n\nThis implementation ensures that all requirements are met, including handling different input types and returning the results in the specified format."}
{"task_id": 21, "completion_id": 0, "completion": "Here's a complete implementation of the AdaBoost algorithm using one-dimensional decision stumps as specified in your request. The function `adaboost_1d_predict` trains the ensemble and returns predictions for a given test set.\n\n```python\nimport numpy as np\nimport math\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\n\n    Parameters\n    ----------\n    x_train : list[float]\n        Training sample values (one-dimensional).\n    y_train : list[int]\n        Labels corresponding to *x_train* (each value must be 1 or \u20131).\n    x_test : list[float]\n        Sample values to classify after training.\n    epsilon : float, default 0.0\n        Upper bound on the allowed training error.  Training stops once\n        the ensemble\u2019s training error \u2264 *epsilon*.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (1 or \u20131) for every value in *x_test*.\n    \"\"\"\n    n_samples = len(x_train)\n    \n    # Initialize weights\n    weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    alphas = []\n    \n    # Training loop\n    while True:\n        # Create possible stumps\n        thresholds = np.unique(np.sort(x_train))\n        best_stump = None\n        min_error = float('inf')\n        \n        for threshold in thresholds:\n            for d in [0, 1]:  # d = 0 or d = 1\n                # Predictions based on the stump\n                predictions = np.where((x_train <= threshold) if d == 0 else (x_train > threshold), 1, -1)\n                \n                # Calculate weighted error\n                error = np.sum(weights * (predictions != y_train))\n                \n                # Check if this stump is the best one\n                if error < min_error:\n                    min_error = error\n                    best_stump = (d, threshold)\n        \n        # Calculate alpha\n        alpha = 0.5 * math.log((1 - min_error) / (min_error + 1e-10))  # Avoid division by zero\n        \n        # Update weights\n        predictions = np.where((x_train <= best_stump[1]) if best_stump[0] == 0 else (x_train > best_stump[1]), 1, -1)\n        weights *= np.exp(-alpha * y_train * predictions)\n        weights /= np.sum(weights)  # Normalize weights\n        \n        # Store the stump and its alpha\n        classifiers.append(best_stump)\n        alphas.append(alpha)\n        \n        # Calculate training error of the ensemble\n        ensemble_predictions = np.zeros(n_samples)\n        for alpha, (d, threshold) in zip(alphas, classifiers):\n            ensemble_predictions += alpha * np.where((x_train <= threshold) if d == 0 else (x_train > threshold), 1, -1)\n        \n        # Check training error\n        training_error = np.mean(np.sign(ensemble_predictions) != y_train)\n        if training_error <= epsilon:\n            break\n    \n    # Make predictions on the test set\n    final_predictions = np.zeros(len(x_test))\n    for alpha, (d, threshold) in zip(alphas, classifiers):\n        final_predictions += alpha * np.where((x_test <= threshold) if d == 0 else (x_test > threshold), 1, -1)\n    \n    return np.sign(final_predictions).astype(int).tolist()\n\n# Example usage:\n# x_train = [1, 2, 3, 4, 5]\n# y_train = [1, 1, -1, -1, -1]\n# x_test = [1.5, 2.5, 3.5, 4.5]\n# predictions = adaboost_1d_predict(x_train, y_train, x_test, epsilon=0.1)\n# print(predictions)\n```\n\n### Explanation:\n1. **Initialization**: The weights are initialized uniformly.\n2. **Stump Generation**: For each unique threshold in the training data, two decision stumps are created (one for `d=0` and one for `d=1`).\n3. **Weighted Error Calculation**: The weighted error for each stump is calculated, and the stump with the minimum error is selected.\n4. **Weight Update**: The weights are updated based on the predictions of the selected stump, and they are normalized.\n5. **Stopping Condition**: The training continues until the ensemble's training error is less than or equal to `epsilon`.\n6. **Prediction**: Finally, predictions are made for the test set based on the trained ensemble.\n\nThis implementation adheres to the requirements specified in your task description."}
{"task_id": 25, "completion_id": 0, "completion": "To implement the Gaussian Kernel SVM prediction function, we will follow the formula provided for the decision function \\( g(z) \\). The key steps involve calculating the kernel values between the training samples and the test samples, applying the decision function, and then determining the predicted class labels based on the sign of \\( g(z) \\).\n\nHere's the complete implementation of the `gaussian_svm_predict` function:\n\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    # Calculate the squared Euclidean distance between each test sample and each training sample\n    # ||x_i - z||\u00b2 = ||x_i||\u00b2 + ||z||\u00b2 - 2 * <x_i, z>\n    # We can compute this efficiently using broadcasting\n    X_train_sq = np.sum(X_train**2, axis=1).reshape(-1, 1)  # shape (n, 1)\n    X_test_sq = np.sum(X_test**2, axis=1).reshape(1, -1)    # shape (1, m)\n    \n    # Compute the squared distances\n    squared_distances = X_train_sq + X_test_sq - 2 * np.dot(X_train, X_test.T)  # shape (n, m)\n\n    # Compute the Gaussian kernel values\n    K = np.exp(-gamma * squared_distances)  # shape (n, m)\n\n    # Compute the decision function g(z) for each test sample\n    g = np.dot(K.T, alpha * y_train) + b  # shape (m,)\n\n    # Predict labels based on the sign of g(z)\n    predictions = np.sign(g)\n\n    # Convert predictions to a list of integers\n    return predictions.astype(int).tolist()\n```\n\n### Explanation of the Code:\n1. **Squared Distances Calculation**: We compute the squared Euclidean distances between each test sample and each training sample using broadcasting. This avoids the need for explicit loops and is efficient.\n  \n2. **Gaussian Kernel Calculation**: We apply the Gaussian kernel formula to compute the kernel values \\( K \\) between the training and test samples.\n\n3. **Decision Function Calculation**: We compute the decision function \\( g(z) \\) for each test sample by taking the dot product of the kernel matrix \\( K \\) with the product of the Lagrange multipliers \\( \\alpha \\) and the training labels \\( y_{\\text{train}} \\), and then adding the bias term \\( b \\).\n\n4. **Predictions**: The predicted labels are determined by taking the sign of \\( g(z) \\). The result is converted to a list of integers.\n\nThis implementation is fully vectorized and should work efficiently for arbitrary numbers of training and test samples."}
{"task_id": 28, "completion_id": 0, "completion": "To implement the linear autoencoder reconstruction using truncated Singular Value Decomposition (SVD), we can follow these steps:\n\n1. Validate the input parameters.\n2. Convert the input list of lists into a NumPy array for easier manipulation.\n3. Perform SVD on the data matrix.\n4. Reconstruct the matrix using the first **k** singular values and vectors.\n5. Calculate the mean squared error (MSE) between the original and reconstructed matrices.\n6. Return the reconstructed matrix and the MSE, both rounded to four decimal places.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    # Validate k\n    if k < 1 or k > min(len(X), len(X[0])):\n        return -1\n    \n    # Convert the input list of lists to a NumPy array\n    X_np = np.array(X)\n    \n    # Perform SVD\n    U, S, Vt = np.linalg.svd(X_np, full_matrices=False)\n    \n    # Keep only the first k singular values and vectors\n    U_k = U[:, :k]\n    S_k = np.diag(S[:k])\n    Vt_k = Vt[:k, :]\n    \n    # Reconstruct the matrix\n    X_hat = U_k @ S_k @ Vt_k\n    \n    # Calculate the mean squared error\n    mse = np.mean((X_np - X_hat) ** 2)\n    \n    # Round the reconstructed matrix and mse to four decimals\n    X_hat_rounded = np.round(X_hat, 4).tolist()\n    mse_rounded = round(mse, 4)\n    \n    return (X_hat_rounded, mse_rounded)\n\n# Example usage:\n# X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n# k = 2\n# result = linear_autoencoder(X, k)\n# print(result)\n```\n\n### Explanation:\n- **Input Validation**: The function checks if `k` is within the valid range. If not, it returns `-1`.\n- **SVD Computation**: The `np.linalg.svd` function computes the singular value decomposition of the matrix.\n- **Matrix Reconstruction**: The reconstructed matrix is obtained by multiplying the truncated matrices.\n- **MSE Calculation**: The mean squared error is computed between the original and reconstructed matrices.\n- **Rounding**: Both the reconstructed matrix and the MSE are rounded to four decimal places before returning.\n\nThis implementation should work efficiently for the task described."}
{"task_id": 29, "completion_id": 0, "completion": "Here's a complete implementation of the `predict_boosting_tree` function that follows the specified algorithm for one-dimensional gradient boosting using decision stumps. The function sorts the training data, identifies candidate split points, and iteratively adds the best-performing stumps until the residual sum of squares (RSS) is below the specified tolerance.\n\n```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\n\n    The function must:  \n    1. Determine all possible split points (mid-points between consecutive *x_train* values).  \n    2. Iteratively add the stump that minimises the squared residuals until the total\n       residual sum of squares becomes smaller than *epsilon*.  \n    3. Return the prediction for *x_query* obtained by summing the constants contributed\n       by every learnt stump.  \n\n    Args:\n        x_train: One-dimensional training features.\n        y_train: Training targets (same length as *x_train*).\n        x_query: Feature value to predict.\n        epsilon: Stopping tolerance on the residual sum of squares (default 1e-2).\n\n    Returns:\n        Single floating point number \u2013 the predicted target for *x_query*, rounded to\n        four decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    x_train = np.asarray(x_train)\n    y_train = np.asarray(y_train)\n    \n    # Initialize residuals as the target values\n    residuals = y_train.copy()\n    predictions = np.zeros_like(y_train)\n    \n    # Sort the training data by x_train\n    sorted_indices = np.argsort(x_train)\n    x_train_sorted = x_train[sorted_indices]\n    residuals_sorted = residuals[sorted_indices]\n    \n    # Initialize the RSS\n    rss = np.sum(residuals_sorted ** 2)\n    \n    # While the RSS is larger than epsilon\n    while rss > epsilon:\n        # Determine candidate split points (mid-points)\n        candidate_splits = (x_train_sorted[:-1] + x_train_sorted[1:]) / 2\n        \n        best_rss = float('inf')\n        best_split = None\n        best_c1 = None\n        best_c2 = None\n        \n        # Evaluate each candidate split\n        for s in candidate_splits:\n            # Calculate c1 and c2\n            left_mask = x_train_sorted <= s\n            right_mask = x_train_sorted > s\n            \n            c1 = np.mean(residuals_sorted[left_mask]) if np.any(left_mask) else 0\n            c2 = np.mean(residuals_sorted[right_mask]) if np.any(right_mask) else 0\n            \n            # Calculate the new predictions\n            new_predictions = np.where(left_mask, c1, c2)\n            new_rss = np.sum((residuals_sorted - new_predictions) ** 2)\n            \n            # Check if this is the best split\n            if new_rss < best_rss:\n                best_rss = new_rss\n                best_split = s\n                best_c1 = c1\n                best_c2 = c2\n        \n        # Update the predictions and residuals\n        predictions += np.where(x_train_sorted <= best_split, best_c1, best_c2)\n        residuals = y_train - predictions\n        rss = np.sum(residuals ** 2)\n    \n    # Final prediction for the query point\n    final_prediction = 0\n    for s, c1, c2 in zip(candidate_splits, best_c1, best_c2):\n        if x_query <= s:\n            final_prediction += c1\n        else:\n            final_prediction += c2\n    \n    return round(final_prediction, 4)\n\n# Example usage:\n# x_train = [1, 2, 3, 4, 5]\n# y_train = [1, 2, 3, 4, 5]\n# x_query = 3.5\n# print(predict_boosting_tree(x_train, y_train, x_query))  # Example output\n```\n\n### Explanation:\n1. **Input Handling**: The function converts the input lists to NumPy arrays for easier manipulation.\n2. **Sorting**: The training data is sorted based on the feature values.\n3. **Residual Calculation**: The initial residuals are set to the target values.\n4. **Candidate Splits**: Midpoints between consecutive sorted feature values are calculated as potential split points.\n5. **Stump Selection**: For each candidate split, the function calculates the mean of the residuals on either side of the split and computes the RSS. The best split is selected based on the lowest RSS.\n6. **Updating Predictions**: The predictions are updated based on the selected stump, and the residuals are recalculated.\n7. **Final Prediction**: After the loop, the function computes the final prediction for the query point based on the learned stumps.\n\nThis implementation should work efficiently for one-dimensional data and provide accurate predictions based on the gradient boosting approach with decision stumps."}
{"task_id": 34, "completion_id": 0, "completion": "Here's a complete implementation of the `random_binary_tensor` function according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"\n    Generates a random binary tensor filled with 0.0s and 1.0s.\n\n    Parameters:\n    - shape (tuple[int, ...]): The desired shape of the output tensor.\n    - sparsity (float): The probability of generating a 1.0. Must be in the range [0, 1].\n    - seed (int | None): Optional seed for random number generation for reproducibility.\n\n    Returns:\n    - numpy.ndarray: A tensor of the specified shape filled with 0.0s and 1.0s.\n    - int: Returns -1 if sparsity is outside the range [0, 1].\n    \"\"\"\n    # Check if sparsity is within the valid range\n    if not (0 <= sparsity <= 1):\n        return -1\n    \n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate a random array with values in [0, 1)\n    random_array = np.random.rand(*shape)\n    \n    # Create a binary tensor based on the sparsity\n    binary_tensor = (random_array < sparsity).astype(float)\n    \n    return binary_tensor\n\n# Example usage:\n# tensor = random_binary_tensor((2, 3), sparsity=0.3, seed=42)\n# print(tensor)\n```\n\n### Explanation of the Code:\n1. **Input Validation**: The function first checks if the `sparsity` value is within the valid range [0, 1]. If not, it returns -1.\n2. **Random Seed**: If a seed is provided, it sets the random seed using `np.random.seed(seed)` to ensure reproducibility.\n3. **Random Array Generation**: It generates a random array of the specified shape using `np.random.rand(*shape)`, which produces values uniformly distributed in the range [0, 1).\n4. **Binary Tensor Creation**: The binary tensor is created by comparing the random values to the `sparsity` threshold. Values less than `sparsity` become 1.0, and others become 0.0. The result is cast to a float type using `.astype(float)`.\n5. **Return Value**: The function returns the generated binary tensor.\n\n### Example Usage:\nYou can uncomment the example usage at the end of the code to see how the function works with a specific shape, sparsity, and seed."}
{"task_id": 39, "completion_id": 0, "completion": "To implement the `get_split` function for performing a median split on a dataset using NumPy, we can utilize `numpy.argpartition` to efficiently find the median without fully sorting the data. Below is the complete implementation of the function:\n\n```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger values.\n    \"\"\"\n    n_samples = data.shape[0]\n    \n    # Find the index of the median using argpartition\n    median_index = n_samples // 2\n    partitioned_indices = np.argpartition(data[:, d], median_index)\n    \n    # The pivot is the index of the median value\n    pivot_index = partitioned_indices[median_index]\n    \n    # Get the value of the median\n    median_value = data[pivot_index, d]\n    \n    # Create left and right lists based on the median value\n    left_indices = []\n    right_indices = []\n    \n    for i in range(n_samples):\n        if data[i, d] < median_value:\n            left_indices.append(i)\n        elif data[i, d] > median_value:\n            right_indices.append(i)\n    \n    # Sort the left and right indices to ensure deterministic output\n    left_indices.sort()\n    right_indices.sort()\n    \n    return pivot_index, left_indices, right_indices\n```\n\n### Explanation of the Code:\n1. **Input Parameters**: The function takes a 2D NumPy array `data` and an integer `d` which specifies the column index for the split.\n2. **Finding the Median**:\n   - We calculate the median index as `n_samples // 2`.\n   - We use `np.argpartition` to partition the indices of the column `d` such that the first `median_index` elements are less than or equal to the median.\n3. **Identifying the Pivot**:\n   - The pivot index is determined from the partitioned indices.\n   - We retrieve the median value using this pivot index.\n4. **Creating Left and Right Lists**:\n   - We iterate through all samples and classify their indices into `left_indices` (for values less than the median) and `right_indices` (for values greater than the median).\n5. **Sorting the Indices**: Finally, we sort both lists to ensure the output is deterministic.\n6. **Return Value**: The function returns a tuple containing the pivot index, the sorted list of left indices, and the sorted list of right indices.\n\nThis implementation efficiently performs the median split in O(n) time complexity, as required."}
{"task_id": 40, "completion_id": 0, "completion": "Here's the implementation of the `to_frames` function that breaks a one-dimensional signal into overlapping frames using NumPy's stride manipulation. The function adheres to the specified requirements and constraints:\n\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Pre-condition checks\n    assert x.ndim == 1, \"Input must be a one-dimensional array.\"\n    assert stride > 0, \"Stride must be a positive integer.\"\n    assert len(x) >= frame_width, \"Length of input must be at least equal to frame_width.\"\n\n    # Calculate the number of frames\n    n_frames = (len(x) - frame_width) // stride + 1\n\n    # Calculate the shape and strides for the new view\n    shape = (n_frames, frame_width)\n    strides = (x.strides[0] * stride, x.strides[0])  # (stride in bytes, frame width in bytes)\n\n    # Create the strided view\n    framed_signal = as_strided(x, shape=shape, strides=strides)\n\n    # Set writability\n    framed_signal.setflags(write=writeable)\n\n    return framed_signal\n```\n\n### Explanation:\n1. **Input Validation**: The function checks if the input array `x` is one-dimensional, if `stride` is positive, and if the length of `x` is at least as long as `frame_width`. If any of these conditions are not met, an `AssertionError` is raised.\n\n2. **Frame Calculation**: The number of frames is calculated using the formula provided in the task description.\n\n3. **Strides and Shape**: The shape of the resulting array is defined as `(n_frames, frame_width)`, and the strides are calculated based on the original array's strides. The first stride corresponds to the hop length (in bytes), and the second corresponds to the frame width (in bytes).\n\n4. **Creating the View**: The `as_strided` function is used to create a view of the original array with the specified shape and strides.\n\n5. **Writability**: The `setflags` method is used to control whether the returned view is writable based on the `writeable` parameter.\n\nThis implementation efficiently creates overlapping frames from the input signal without copying the data, adhering to the constraints and requirements specified in the task."}
{"task_id": 48, "completion_id": 0, "completion": "Here's the implementation of the `is_stochastic` function that checks if a given matrix is row-stochastic according to the specified criteria:\n\n```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | np.ndarray) -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Convert to numpy array if X is a list of lists\n    if isinstance(X, list):\n        X = np.array(X)\n\n    # Check if the matrix is two-dimensional\n    if X.ndim != 2:\n        return False\n\n    # Check if all elements are between 0 and 1\n    if not np.all((X >= 0) & (X <= 1)):\n        return False\n\n    # Check if each row sums to 1 (within a tolerance)\n    row_sums = np.sum(X, axis=1)\n    if not np.all(np.isclose(row_sums, 1, atol=1e-8)):\n        return False\n\n    return True\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if the input `X` is a list of lists. If it is, it converts it to a NumPy array for easier manipulation.\n2. **Dimensionality Check**: It checks if the matrix is two-dimensional using `X.ndim`.\n3. **Element Value Check**: It verifies that all elements are within the range [0, 1] using a combination of comparison operations and `np.all`.\n4. **Row Sum Check**: It calculates the sum of each row using `np.sum` and checks if these sums are close to 1 using `np.isclose` with a specified tolerance of `1e-8`.\n5. **Return Value**: The function returns `True` if all conditions are satisfied; otherwise, it returns `False`.\n\nThis implementation ensures that the function adheres to the requirements without raising exceptions, providing a robust solution for validating row-stochastic matrices."}
{"task_id": 55, "completion_id": 0, "completion": "Here's the implementation of the RMSprop optimization algorithm for ordinary least-squares (OLS) linear regression as per your specifications:\n\n```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n    n, d = X.shape\n    w = np.zeros(d)  # Initialize weights\n    s = np.zeros(d)  # Initialize squared gradient running average\n\n    for iteration in range(max_iter):\n        # Shuffle the data for mini-batch processing\n        indices = np.random.permutation(n)\n        X_shuffled = X[indices]\n        y_shuffled = y[indices]\n\n        for start in range(0, n, batch_size):\n            end = min(start + batch_size, n)\n            X_batch = X_shuffled[start:end]\n            y_batch = y_shuffled[start:end]\n\n            # Compute the gradient\n            predictions = X_batch @ w\n            residuals = predictions - y_batch\n            g = (1 / len(X_batch)) * (X_batch.T @ residuals)\n\n            # Check for early stopping condition\n            if np.linalg.norm(g) < epsilon:\n                break\n\n            # Update the running average of squared gradients\n            s = rho * s + (1 - rho) * (g ** 2)\n\n            # Update weights\n            w -= eta * g / (np.sqrt(s) + eps_station)\n\n        else:\n            continue  # Only executed if the inner loop did NOT break\n        break  # Break the outer loop if the inner loop did break\n\n    return list(np.round(w, 4))\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4]])\n# y = np.array([1, 2, 3])\n# weights = rms_prop(X, y)\n# print(weights)\n```\n\n### Explanation:\n1. **Initialization**: The weight vector `w` and the squared gradient running average `s` are initialized to zero.\n2. **Mini-batch Processing**: The data is shuffled and processed in mini-batches. If the number of samples is less than the batch size, the entire dataset is used.\n3. **Gradient Calculation**: The gradient of the loss function is computed using the current weights and the mini-batch.\n4. **Early Stopping**: If the L2 norm of the gradient is less than the specified epsilon, the optimization stops early.\n5. **RMSprop Update**: The running average of the squared gradients is updated, and the weights are adjusted according to the RMSprop rule.\n6. **Return Value**: The final weight vector is rounded to four decimal places and returned as a list.\n\nThis implementation should work effectively for linear regression using the RMSprop optimization algorithm."}
{"task_id": 56, "completion_id": 0, "completion": "Here's the implementation of the SoftPlus activation function along with its gradient, adhering to the requirements specified in your task:\n\n```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"Compute the SoftPlus activation function or its derivative.\n    \n    Args:\n        x: A scalar, list, or NumPy array containing real values.\n        deriv: A boolean flag indicating whether to return the derivative.\n    \n    Returns:\n        A float if x is a scalar, or a list of floats if x is a list or array.\n    \"\"\"\n    # Convert input to a NumPy array for easier handling\n    x = np.asarray(x)\n    \n    if deriv:\n        # Return the derivative (sigmoid function)\n        result = 1 / (1 + np.exp(-x))\n    else:\n        # Compute SoftPlus using a numerically stable formulation\n        result = np.where(x > 0, x + np.log1p(np.exp(-x)), np.log1p(np.exp(x)))\n    \n    # Round the result to 4 decimal places\n    result = np.round(result, 4)\n    \n    # Return as a float if input was a scalar, otherwise return as a list\n    if np.isscalar(x):\n        return float(result)\n    else:\n        return result.tolist()\n\n# Example usage:\n# print(softplus(1))          # Should return SoftPlus(1)\n# print(softplus([1, 2, 3]))  # Should return SoftPlus([1, 2, 3])\n# print(softplus(1, deriv=True))  # Should return the derivative at 1\n# print(softplus([1, 2, 3], deriv=True))  # Should return the derivatives at [1, 2, 3]\n```\n\n### Explanation:\n1. **Input Handling**: The function accepts a scalar, list, or NumPy array and converts it to a NumPy array for easier manipulation.\n2. **SoftPlus Calculation**: The SoftPlus function is computed using a numerically stable formulation:\n   - For positive values of `x`, it uses the formula \\( x + \\log(1 + e^{-x}) \\).\n   - For negative values, it uses \\( \\log(1 + e^{x}) \\).\n3. **Derivative Calculation**: When `deriv` is `True`, it computes the sigmoid function \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n4. **Rounding**: The results are rounded to four decimal places.\n5. **Return Type**: The function returns a float if the input is a scalar, or a list if the input is a list or array.\n\nThis implementation ensures numerical stability and adheres to the specified output format."}
{"task_id": 58, "completion_id": 0, "completion": "To implement the unsmoothed Maximum-Likelihood Estimation (MLE) N-gram log-probability function, we need to follow these steps:\n\n1. **Count N-grams and their prefixes**: We will create a count of all N-grams in the corpus and their corresponding (N-1)-grams.\n2. **Calculate log-probabilities**: For each N-gram in the target sequence, we will compute its log-probability based on the counts we obtained from the corpus.\n3. **Handle edge cases**: If any required count is zero, we will return `float('-inf')`.\n\nHere's the complete implementation of the function:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\"\"\"\n    \n    # Step 1: Create N-gram counts\n    ngram_counts = Counter()\n    prefix_counts = Counter()\n    \n    # Generate N-grams and their prefixes\n    for i in range(len(corpus) - N + 1):\n        ngram = tuple(corpus[i:i + N])\n        prefix = tuple(corpus[i:i + N - 1])\n        ngram_counts[ngram] += 1\n        prefix_counts[prefix] += 1\n    \n    # Step 2: Calculate log-probability for the sequence\n    log_prob = 0.0\n    sequence_length = len(sequence)\n    \n    for i in range(sequence_length - N + 1):\n        ngram = tuple(sequence[i:i + N])\n        prefix = tuple(sequence[i:i + N - 1])\n        \n        # Get counts\n        ngram_count = ngram_counts[ngram]\n        prefix_count = prefix_counts[prefix]\n        \n        # If either count is zero, return negative infinity\n        if ngram_count == 0 or prefix_count == 0:\n            return float('-inf')\n        \n        # Calculate log probability\n        log_prob += np.log(ngram_count) - np.log(prefix_count)\n    \n    # Return the log probability rounded to 4 decimal places\n    return round(log_prob, 4)\n\n# Example usage:\n# corpus = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n# sequence = [\"the\", \"cat\", \"sat\"]\n# N = 2\n# print(unsmoothed_ngram_log_prob(corpus, sequence, N))  # Example output\n```\n\n### Explanation of the Code:\n- **Counting N-grams**: We use a sliding window approach to count N-grams and their prefixes. The `Counter` from the `collections` module is used to maintain counts efficiently.\n- **Log-Probability Calculation**: For each N-gram in the sequence, we check its count and the count of its prefix. If either is zero, we return `float('-inf')`. Otherwise, we compute the log-probability using the formula provided.\n- **Final Output**: The total log-probability is rounded to four decimal places before returning.\n\nThis implementation efficiently computes the required log-probability while handling edge cases as specified."}
{"task_id": 62, "completion_id": 0, "completion": "Here's a complete implementation of the univariate Lasso regression with polynomial features using coordinate descent as described in your task:\n\n```python\nimport numpy as np\n\ndef soft_threshold(rho, lam):\n    \"\"\"Applies the soft thresholding operator.\"\"\"\n    return np.sign(rho) * max(abs(rho) - lam, 0)\n\ndef lasso_regression(X: list[float] | np.ndarray, y: list[float] | np.ndarray, degree: int, reg_factor: float, n_iterations: int = 1000, tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\n\n    The function must build polynomial features up to *degree*, optimise the\n    Lasso objective with coordinate descent, and return the learned\n    coefficients rounded to four decimal places.\n\n    Parameters\n    ----------\n    X : list[float] | np.ndarray\n        One-dimensional input samples.\n    y : list[float] | np.ndarray\n        Target values with the same length as *X*.\n    degree : int\n        Highest exponent of *x* to include (gives *degree+1* coefficients in\n        total counting the bias).\n    reg_factor : float\n        L1 regularisation strength \u03bb.\n    n_iterations : int, default=1000\n        Maximum number of coordinate descent passes.\n    tol : float, default=1e-6\n        Early stopping criterion. The algorithm terminates when the largest\n        absolute change in any coefficient between two consecutive passes is\n        smaller than *tol*.\n\n    Returns\n    -------\n    list[float]\n        The learned weights [w0, w1, \u2026, w_degree] rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X = np.asarray(X)\n    y = np.asarray(y)\n    \n    # Build the design matrix with polynomial features\n    m = len(X)\n    X_poly = np.column_stack([X**i for i in range(degree + 1)])\n    \n    # Initialize weights\n    w = np.zeros(degree + 1)\n    \n    for iteration in range(n_iterations):\n        # Update bias (w[0])\n        w[0] = np.mean(y - X_poly[:, 1:] @ w[1:])\n        \n        # Store the maximum change for convergence check\n        max_change = 0\n        \n        # Update coefficients w[1] to w[degree]\n        for j in range(1, degree + 1):\n            rho = X_poly[:, j].T @ (y - (X_poly @ w)) + w[j] * (X_poly[:, j].T @ X_poly[:, j])\n            w_j_old = w[j]\n            w[j] = soft_threshold(rho, reg_factor) / (X_poly[:, j].T @ X_poly[:, j])\n            max_change = max(max_change, abs(w[j] - w_j_old))\n        \n        # Check for convergence\n        if max_change < tol:\n            break\n    \n    return list(np.round(w, 4))\n\n# Example usage:\n# X = [1, 2, 3, 4]\n# y = [2, 3, 5, 7]\n# degree = 2\n# reg_factor = 0.1\n# print(lasso_regression(X, y, degree, reg_factor))\n```\n\n### Explanation:\n1. **Soft Thresholding**: The `soft_threshold` function implements the soft thresholding operator used in Lasso regression.\n2. **Design Matrix**: The design matrix `X_poly` is created using polynomial features up to the specified degree.\n3. **Coordinate Descent**: The weights are updated iteratively:\n   - The bias term `w[0]` is updated based on the mean of the residuals.\n   - Each coefficient `w[j]` (for `j >= 1`) is updated using the soft thresholding rule.\n4. **Convergence Check**: The algorithm stops if the maximum change in weights is less than the specified tolerance or if the maximum number of iterations is reached.\n5. **Return Values**: The coefficients are rounded to four decimal places and returned as a list.\n\nThis implementation should work efficiently for the specified task and can be tested with various datasets."}
{"task_id": 63, "completion_id": 0, "completion": "Here's a Python function that calculates the backward probability vector \\(\\beta_t\\) for a given time index \\(t\\) in a Hidden Markov Model (HMM). The function takes the transition probability matrix \\(A\\), the emission probability matrix \\(B\\), the list of observation indices, and the time index \\(t\\) as inputs. It returns the backward probabilities rounded to four decimal places.\n\n```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"\n    Calculate the backward probability vector \u03b2_t for a given time index t in a Hidden Markov Model.\n\n    Parameters:\n    A (list[list[float]]): Transition probability matrix of shape (N, N).\n    B (list[list[float]]): Emission probability matrix of shape (N, M).\n    obs (list[int]): List of observation indices (length T).\n    t (int): Time index (0 \u2264 t < T).\n\n    Returns:\n    list[float]: Backward probability vector \u03b2_t rounded to 4 decimal places.\n    \"\"\"\n    N = len(A)  # Number of states\n    T = len(obs)  # Length of the observation sequence\n\n    # Initialize the backward probabilities for time T-1\n    beta = np.zeros(N)\n    \n    # Base case: \u03b2_{T-1}(i) = 1 for all i\n    if t == T - 1:\n        return [1.0] * N\n\n    # Calculate \u03b2_t for the given time index t\n    beta_t = np.zeros(N)\n    \n    # Iterate over all states\n    for i in range(N):\n        # Sum over all possible next states\n        for j in range(N):\n            # Get the observation at time t+1\n            observation_next = obs[t + 1]\n            # Calculate the contribution to \u03b2_t(i)\n            beta_t[i] += A[i][j] * B[j][observation_next] * backward_beta(A, B, obs, t + 1)[j]\n\n    # Round the results to 4 decimal places\n    return np.round(beta_t, 4).tolist()\n\n# Example usage:\n# A = [[0.7, 0.3], [0.4, 0.6]]\n# B = [[0.5, 0.5], [0.1, 0.9]]\n# obs = [0, 1, 0]\n# t = 1\n# print(backward_beta(A, B, obs, t))\n```\n\n### Explanation:\n1. **Initialization**: The function initializes the backward probabilities for the last time step \\(T-1\\) to 1 for all states, as per the definition of the backward variable.\n2. **Recursion**: For each state \\(i\\) at time \\(t\\), it calculates the sum of the products of the transition probabilities \\(a_{ij}\\), the emission probabilities \\(b_j(o_{t+1})\\), and the backward probabilities \\(\\beta_{t+1}(j)\\) for all possible next states \\(j\\).\n3. **Rounding**: The resulting backward probabilities are rounded to four decimal places before being returned.\n\n### Note:\n- The function uses recursion to compute the backward probabilities for the next time step. This can lead to performance issues for large sequences or state spaces. In practice, you might want to implement this using an iterative approach or memoization to avoid redundant calculations."}
{"task_id": 65, "completion_id": 0, "completion": "Here's the implementation of the backward algorithm for the Hidden Markov Model (HMM) as described in your task. The function computes the probability of a given observation sequence based on the provided HMM parameters.\n\n```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    # Check for empty inputs\n    if not A or not B or not pi or not obs:\n        return 0.0\n    \n    N = len(A)  # Number of states\n    T = len(obs)  # Length of observation sequence\n\n    # Initialize the beta matrix\n    beta = np.zeros((T, N))\n\n    # Base case: beta[T-1](i) = 1 for all states i\n    beta[T-1] = 1\n\n    # Backward recursion\n    for t in range(T - 2, -1, -1):\n        for i in range(N):\n            beta[t][i] = sum(A[i][j] * B[j][obs[t + 1]] * beta[t + 1][j] for j in range(N))\n\n    # Calculate the probability of the observation sequence\n    prob = sum(pi[i] * B[i][obs[0]] * beta[0][i] for i in range(N))\n\n    return round(prob, 6)\n\n# Example usage:\n# A = [[0.7, 0.3], [0.4, 0.6]]\n# B = [[0.5, 0.5], [0.1, 0.9]]\n# pi = [0.6, 0.4]\n# obs = [0, 1, 0]\n# print(backward_prob(A, B, pi, obs))  # Example call\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if any of the input lists are empty. If so, it returns `0.0`.\n2. **Initialization**: It initializes the `beta` matrix, where `beta[t][i]` will hold the backward probabilities.\n3. **Base Case**: The last backward variable `beta[T-1][i]` is set to `1` for all states `i`.\n4. **Backward Recursion**: The function iterates backward through the observation sequence, calculating the `beta` values using the recursive formula provided.\n5. **Final Probability Calculation**: Finally, it computes the total probability of the observation sequence by summing over all states at time `0`, weighted by the initial probabilities and the emission probabilities.\n6. **Return Value**: The result is rounded to six decimal places before being returned.\n\nYou can test the function with different HMM parameters and observation sequences to verify its correctness."}
{"task_id": 69, "completion_id": 0, "completion": "Here's a complete implementation of the forward algorithm for a Hidden Markov Model (HMM) in Python. The function validates the input dimensions, checks the observation indices, and computes the likelihood of the observation sequence using the forward algorithm.\n\n```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Validate input dimensions\n    n = len(S)  # Number of states\n    if n == 0 or len(A) != n or len(B) != n:\n        return -1  # Invalid dimensions for S, A, or B\n\n    for row in A:\n        if len(row) != n:\n            return -1  # Invalid dimension for state transition matrix A\n\n    m = len(B[0]) if n > 0 else 0  # Number of observation symbols\n    for row in B:\n        if len(row) != m:\n            return -1  # Invalid dimension for emission matrix B\n\n    if len(observations) == 0:\n        return -1  # Empty observation sequence\n\n    # Check that every observation index is in the valid range [0, m\u22121]\n    for obs in observations:\n        if obs < 0 or obs >= m:\n            return -1  # Out-of-range observation index\n\n    # Initialize the forward probabilities\n    alpha = np.zeros((len(observations), n))\n\n    # Base case: Initialize alpha for the first observation\n    for i in range(n):\n        alpha[0][i] = S[i] * B[i][observations[0]]\n\n    # Recursive case: Compute alpha for subsequent observations\n    for t in range(1, len(observations)):\n        for j in range(n):\n            alpha[t][j] = sum(alpha[t-1][i] * A[i][j] for i in range(n)) * B[j][observations[t]]\n\n    # The likelihood of the observation sequence is the sum of the last alpha values\n    likelihood = sum(alpha[len(observations) - 1][j] for j in range(n))\n\n    return round(likelihood, 4)\n\n# Example usage:\n# S = [0.6, 0.4]\n# A = [[0.7, 0.3], [0.4, 0.6]]\n# B = [[0.5, 0.5], [0.1, 0.9]]\n# observations = [0, 1, 0]\n# print(forward_algorithm(S, A, B, observations))  # Example call\n```\n\n### Explanation:\n1. **Input Validation**: The function checks if the dimensions of the initial state probabilities, state transition matrix, and emission matrix are valid. It also checks if the observation sequence is empty or if any observation index is out of range.\n\n2. **Forward Algorithm Implementation**:\n   - The forward probabilities (`alpha`) are initialized for the first observation.\n   - For each subsequent observation, the algorithm computes the probabilities based on the previous state probabilities and the transition and emission probabilities.\n   - Finally, the likelihood of the entire observation sequence is computed by summing the probabilities of the last time step.\n\n3. **Return Value**: The function returns the computed likelihood rounded to four decimal places or `-1` for invalid inputs."}
{"task_id": 70, "completion_id": 0, "completion": "Here's a complete implementation of the FP-Growth algorithm in Python, following the specifications you've provided. The function `fp_growth` will discover all frequent item-sets from the given transactions based on the minimum support threshold.\n\n```python\nfrom collections import Counter, defaultdict\n\nclass TreeNode:\n    def __init__(self, item, count, parent=None):\n        self.item = item\n        self.count = count\n        self.parent = parent\n        self.children = {}\n        self.node_link = None  # Link to the next node with the same item\n\ndef build_fp_tree(transactions, min_support):\n    # Count the frequency of each item\n    item_count = Counter(item for transaction in transactions for item in transaction)\n    \n    # Remove infrequent items\n    item_count = {item: count for item, count in item_count.items() if count >= min_support}\n    \n    if not item_count:\n        return None, None\n    \n    # Sort items by frequency and then lexicographically\n    sorted_items = [item for item, _ in sorted(item_count.items(), key=lambda x: (-x[1], x[0]))]\n    \n    # Build the FP-tree\n    root = TreeNode('root', 0)\n    for transaction in transactions:\n        # Filter out infrequent items and sort them\n        filtered_items = [item for item in sorted_items if item in transaction]\n        if filtered_items:\n            # Create a path in the FP-tree\n            current_node = root\n            for item in filtered_items:\n                if item in current_node.children:\n                    current_node.children[item].count += 1\n                else:\n                    new_node = TreeNode(item, 1, current_node)\n                    current_node.children[item] = new_node\n                \n                current_node = current_node.children[item]\n    \n    # Create a header table for the tree\n    header_table = {}\n    for item in sorted_items:\n        header_table[item] = None\n    \n    for item, node in header_table.items():\n        if node is None:\n            # Find the first node in the tree with this item\n            current_node = root\n            while current_node:\n                if item in current_node.children:\n                    if header_table[item] is None:\n                        header_table[item] = current_node.children[item]\n                    else:\n                        # Link the nodes\n                        current_node.children[item].node_link = header_table[item]\n                        header_table[item] = current_node.children[item]\n                current_node = current_node.children.get(item, None)\n    \n    return root, header_table\n\ndef mine_fp_tree(node, prefix, min_support, frequent_itemsets):\n    if node is not None:\n        # Add the current prefix to the frequent itemsets\n        if prefix:\n            frequent_itemsets.append(sorted(prefix))\n        \n        # Mine the tree recursively\n        for item, child_node in node.children.items():\n            new_prefix = prefix + [item]\n            mine_fp_tree(child_node, new_prefix, min_support, frequent_itemsets)\n\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings).  `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result must be deterministic:\n      \u2022 Inside each item-set the items have to be sorted alphabetically.\n      \u2022 The outer list has to be sorted by `(len(itemset), itemset)`.\n    If *transactions* is empty or no item-set meets the threshold return an\n    empty list.\n    \"\"\"\n    if not transactions:\n        return []\n    \n    # Build the FP-tree\n    fp_tree, header_table = build_fp_tree(transactions, min_support)\n    \n    # List to hold the frequent itemsets\n    frequent_itemsets = []\n    \n    # Mine the FP-tree\n    if fp_tree is not None:\n        for item in header_table.keys():\n            # Create a new prefix for the current item\n            prefix = [item]\n            # Mine the conditional FP-tree\n            conditional_transactions = []\n            current_node = header_table[item]\n            while current_node is not None:\n                count = current_node.count\n                path = []\n                parent = current_node.parent\n                while parent and parent.item != 'root':\n                    path.append(parent.item)\n                    parent = parent.parent\n                if path:\n                    conditional_transactions.extend([path] * count)\n                current_node = current_node.node_link\n            \n            # Build the conditional FP-tree\n            conditional_fp_tree, _ = build_fp_tree(conditional_transactions, min_support)\n            # Mine the conditional FP-tree\n            mine_fp_tree(conditional_fp_tree, prefix, min_support, frequent_itemsets)\n    \n    # Sort the frequent itemsets\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    \n    return frequent_itemsets\n```\n\n### Explanation:\n1. **TreeNode Class**: Represents a node in the FP-tree, storing the item, its count, and links to its children and the next node with the same item.\n2. **build_fp_tree Function**: Constructs the FP-tree from the transactions, filtering out infrequent items and linking nodes with the same item.\n3. **mine_fp_tree Function**: Recursively mines the FP-tree to find all frequent item-sets.\n4. **fp_growth Function**: The main function that orchestrates the building of the FP-tree and the mining of frequent item-sets, ensuring the results are sorted as required.\n\n### Usage:\nYou can call the `fp_growth` function with a list of transactions and a minimum support threshold to get the frequent item-sets. For example:\n\n```python\ntransactions = [['milk', 'bread'], ['bread', 'diaper', 'beer'], ['milk', 'diaper', 'bread', 'cola']]\nmin_support = 2\nresult = fp_growth(transactions, min_support)\nprint(result)\n```\n\nThis will output the frequent item-sets that meet the specified support threshold."}
{"task_id": 75, "completion_id": 0, "completion": "Here's a complete implementation of the k-Nearest Neighbour (k-NN) classifier based on the specifications provided:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    \n    # Validate k\n    if not isinstance(k, int) or k <= 0 or k > len(X_train):\n        return -1\n    \n    predictions = []\n    \n    # Iterate over each test sample\n    for test_sample in X_test:\n        # Calculate the squared Euclidean distances to all training samples\n        distances = []\n        for i, train_sample in enumerate(X_train):\n            distance = sum((a - b) ** 2 for a, b in zip(train_sample, test_sample))\n            distances.append((distance, y_train[i]))  # Store distance and corresponding label\n        \n        # Sort distances and get the labels of the k nearest neighbors\n        distances.sort(key=lambda x: x[0])  # Sort by distance\n        nearest_labels = [label for _, label in distances[:k]]  # Get the labels of the k nearest\n        \n        # Count the occurrences of each label\n        label_count = Counter(nearest_labels)\n        \n        # Find the label with the highest count (majority vote)\n        max_count = max(label_count.values())\n        candidates = [label for label, count in label_count.items() if count == max_count]\n        \n        # If there's a tie, return the smallest label\n        predicted_label = min(candidates)\n        \n        predictions.append(predicted_label)\n    \n    return predictions\n```\n\n### Explanation of the Code:\n1. **Input Validation**: The function first checks if `k` is a valid integer and within the acceptable range. If not, it returns `-1`.\n\n2. **Distance Calculation**: For each test sample, the function computes the squared Euclidean distance to each training sample. The square root is omitted as per the task requirements.\n\n3. **Sorting and Nearest Neighbors**: The distances are sorted, and the labels of the `k` nearest neighbors are collected.\n\n4. **Majority Voting**: A `Counter` is used to count the occurrences of each label among the nearest neighbors. The label with the highest count is selected. In case of a tie, the smallest label is chosen.\n\n5. **Return Predictions**: Finally, the function returns a list of predicted labels for all test samples.\n\n### Usage Example:\nYou can use the function as follows:\n\n```python\nX_train = [[1.0, 2.0], [2.0, 3.0], [3.0, 1.0]]\ny_train = [0, 0, 1]\nX_test = [[1.5, 2.5], [3.0, 2.0]]\nk = 2\n\npredictions = knn_predict(X_train, y_train, X_test, k)\nprint(predictions)  # Output will be the predicted class labels for the test samples\n```\n\nThis implementation is efficient for small to medium-sized datasets and adheres to the specified requirements."}
{"task_id": 76, "completion_id": 0, "completion": "Here's the implementation of the `cross_entropy_loss` function that calculates the unnormalized categorical cross-entropy loss for a batch of one-hot encoded targets. The function uses NumPy for efficient computation and handles numerical stability by adding a small epsilon value to the predicted probabilities before taking the logarithm.\n\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are lists\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    \n    # Define a small epsilon for numerical stability\n    epsilon = 1e-15\n    \n    # Clip y_pred to avoid log(0)\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    \n    # Calculate the categorical cross-entropy loss\n    loss = -np.sum(y * np.log(y_pred_clipped))\n    \n    # Return the loss rounded to 4 decimal places\n    return round(loss, 4)\n\n# Example usage:\n# y_true = [[0, 1, 0], [1, 0, 0], [0, 0, 1]]\n# y_pred = [[0.1, 0.7, 0.2], [0.8, 0.1, 0.1], [0.2, 0.3, 0.5]]\n# print(cross_entropy_loss(y_true, y_pred))  # Example output\n```\n\n### Explanation:\n1. **Input Conversion**: The function first converts the input lists to NumPy arrays for easier manipulation.\n2. **Numerical Stability**: A small epsilon value is defined to prevent taking the logarithm of zero. The predicted probabilities are clipped to the range `[epsilon, 1 - epsilon]`.\n3. **Loss Calculation**: The categorical cross-entropy loss is computed using the formula provided, where the sum is taken over all samples and classes.\n4. **Rounding**: Finally, the result is rounded to four decimal places before being returned.\n\nThis implementation assumes that the input shapes are valid and that the predicted probabilities are within the range [0, 1]."}
{"task_id": 77, "completion_id": 0, "completion": "To implement the forward propagation for an L-layer neural network as described, we will follow the steps outlined in the task. We'll iterate through each layer, perform the linear transformation followed by the activation function, and store the intermediate values in a cache for potential use in backpropagation.\n\nHere's the complete implementation of the `L_model_forward` function:\n\n```python\nimport numpy as np\n\ndef sigmoid(Z: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-Z))\n\ndef relu(Z: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the ReLU activation function.\"\"\"\n    return np.maximum(0, Z)\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimals and converted to a plain\n        Python list. The shape is (1, m).\n    \"\"\"\n    L = len(parameters) // 2  # Number of layers in the neural network\n    A = X  # Initialize A to the input data\n    caches = []  # List to store caches for each layer\n\n    # Loop through each layer\n    for l in range(1, L + 1):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        \n        # Linear step: Z = W\u00b7A_prev + b\n        Z = np.dot(W, A) + b\n        \n        # Non-linear step\n        if l < L:  # For hidden layers, use ReLU\n            A = relu(Z)\n        else:  # For output layer, use sigmoid\n            A = sigmoid(Z)\n        \n        # Store the cache (Z, A) for the current layer\n        caches.append((Z, A))\n\n    # Round the final activation AL to 4 decimal places and convert to a list\n    AL = A.round(4).tolist()\n    \n    return AL, caches\n\n# Example usage:\n# X = np.random.rand(n_x, m)  # Input data\n# parameters = {\n#     'W1': np.random.rand(n1, n_x),\n#     'b1': np.random.rand(n1, 1),\n#     'W2': np.random.rand(n2, n1),\n#     'b2': np.random.rand(n2, 1),\n#     # ... continue for all layers\n#     'WL': np.random.rand(nL, n_(L-1)),\n#     'bL': np.random.rand(nL, 1)\n# }\n# AL, caches = L_model_forward(X, parameters)\n```\n\n### Explanation:\n1. **Activation Functions**: We define two helper functions, `sigmoid` and `relu`, to compute the respective activation functions.\n2. **Initialization**: We start with the input `X` as the initial activation `A` and prepare an empty list `caches` to store the intermediate values.\n3. **Layer Iteration**: We loop through each layer:\n   - Compute the linear transformation \\( Z = W \\cdot A_{\\text{prev}} + b \\).\n   - Apply the appropriate activation function (ReLU for hidden layers and sigmoid for the output layer).\n   - Store the cache for each layer, which includes both \\( Z \\) and \\( A \\).\n4. **Final Output**: After processing all layers, we round the final activation \\( A_L \\) to 4 decimal places and convert it to a list before returning it along with the caches.\n\nThis implementation adheres to the requirements specified in the task and is ready for use in a neural network context."}
{"task_id": 81, "completion_id": 0, "completion": "Here's the implementation of the `compute_cost` function that calculates the binary cross-entropy cost based on the provided formula. The function clips the predicted probabilities to avoid taking the logarithm of zero and then computes the cost.\n\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    # Ensure A2 and Y are numpy arrays\n    A2 = np.array(A2)\n    Y = np.array(Y)\n    \n    # Clip A2 to avoid log(0)\n    epsilon = 1e-15\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    \n    # Number of samples\n    m = Y.shape[0]\n    \n    # Compute the binary cross-entropy cost\n    cost = -np.sum(Y * np.log(A2_clipped) + (1 - Y) * np.log(1 - A2_clipped)) / m\n    \n    # Return the cost rounded to 6 decimal places\n    return round(cost, 6)\n\n# Example usage:\n# A2 = np.array([[0.9, 0.1, 0.8]])\n# Y = np.array([[1, 0, 1]])\n# print(compute_cost(A2, Y))  # Output should be the computed cost\n```\n\n### Explanation:\n1. **Clipping**: The predicted probabilities `A2` are clipped to the range `[\u03b5, 1-\u03b5]` to prevent taking the logarithm of zero, which is undefined.\n2. **Cost Calculation**: The binary cross-entropy cost is computed using the formula provided, summing over all samples and dividing by the number of samples `m`.\n3. **Rounding**: The final cost is rounded to six decimal places before being returned.\n\nYou can test the function with different inputs to ensure it behaves as expected."}
{"task_id": 82, "completion_id": 0, "completion": "Here's the implementation of the \u03b5-soft exploration policy in Python. The function `epsilon_soft` takes a list or NumPy array of Q-values and a scalar epsilon (0 \u2264 \u03b5 \u2264 1) and returns the probabilities of selecting each action according to the \u03b5-soft policy.\n\n```python\nimport numpy as np\n\ndef epsilon_soft(Q: list[float] | np.ndarray, epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state. epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list must contain the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    n = len(Q)  # Number of actions\n    a_star_index = np.argmax(Q)  # Index of the first maximum Q-value\n    base_probability = epsilon / n  # Base probability for non-greedy actions\n    \n    # Initialize probabilities\n    probabilities = [base_probability] * n\n    probabilities[a_star_index] = 1 - epsilon + base_probability  # Update the greedy action's probability\n    \n    # Round probabilities to 4 decimal places\n    probabilities = [round(p, 4) for p in probabilities]\n    \n    # Ensure the probabilities sum to 1 (within tolerance)\n    total_probability = sum(probabilities)\n    if not np.isclose(total_probability, 1.0, atol=1e-12):\n        raise ValueError(\"Probabilities do not sum to 1.\")\n    \n    return probabilities\n\n# Example usage\nQ = [1.2, 0.3, 1.2, -0.1]\nepsilon = 0.1\nprint(epsilon_soft(Q, epsilon))  # Output: [0.925, 0.025, 0.025, 0.025]\n```\n\n### Explanation:\n1. **Input Parameters**:\n   - `Q`: A list or NumPy array of Q-values representing the value of each action.\n   - `epsilon`: A scalar value between 0 and 1 that determines the level of exploration.\n\n2. **Finding the Greedy Action**:\n   - We use `np.argmax(Q)` to find the index of the first maximum Q-value, which corresponds to the greedy action.\n\n3. **Calculating Probabilities**:\n   - The base probability for each action (except the greedy one) is calculated as `epsilon / n`.\n   - The probability for the greedy action is calculated as `1 - epsilon + base_probability`.\n\n4. **Rounding and Validation**:\n   - Each probability is rounded to four decimal places.\n   - We check if the sum of the probabilities is close to 1, raising an error if it is not.\n\n5. **Output**:\n   - The function returns a list of probabilities for each action under the \u03b5-soft policy."}
{"task_id": 86, "completion_id": 0, "completion": "To implement the `aggregate_random_forest_votes` function, we will follow the steps outlined in the task description. We'll use the `Counter` class from the `collections` module to count the votes for each class label for every sample. After counting, we'll determine the class label with the highest votes, and in case of a tie, we'll select the smallest label.\n\nHere's the complete implementation:\n\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # Number of samples is the length of the first inner list\n    num_samples = len(predictions[0])\n    \n    # Initialize a list to hold the final predictions\n    final_predictions = []\n    \n    # Iterate over each sample (column)\n    for i in range(num_samples):\n        # Collect votes for the current sample from all trees\n        votes = [predictions[j][i] for j in range(len(predictions))]\n        \n        # Count the votes using Counter\n        vote_count = Counter(votes)\n        \n        # Find the maximum vote count\n        max_votes = max(vote_count.values())\n        \n        # Collect all class labels that have the maximum vote count\n        candidates = [label for label, count in vote_count.items() if count == max_votes]\n        \n        # Select the smallest label in case of a tie\n        final_prediction = min(candidates)\n        \n        # Append the final prediction for the current sample\n        final_predictions.append(final_prediction)\n    \n    return final_predictions\n```\n\n### Explanation of the Code:\n1. **Input Handling**: The function takes a two-dimensional list `predictions`, where each inner list corresponds to the predictions from a single decision tree.\n2. **Vote Counting**: For each sample (column), we gather all votes from the trees and count them using `Counter`.\n3. **Finding the Winner**: We determine the maximum number of votes received by any class label. If there are ties (multiple labels with the same maximum count), we select the smallest label using the `min()` function.\n4. **Final Output**: The function returns a list of final predictions, one for each sample.\n\n### Example Usage:\n```python\npredictions = [\n    ['A', 'B', 'A', 'A'],\n    ['B', 'B', 'A', 'A'],\n    ['A', 'B', 'B', 'C']\n]\n\nfinal_predictions = aggregate_random_forest_votes(predictions)\nprint(final_predictions)  # Output: ['A', 'B', 'A', 'A']\n```\n\nThis implementation efficiently aggregates the predictions from a Random Forest classifier using majority voting, adhering to the specified requirements."}
{"task_id": 88, "completion_id": 0, "completion": "Here's the implementation of the softplus activation function as described in your task. This function handles scalar, list, and NumPy ndarray inputs, and it uses the `np.logaddexp` function for numerical stability. The results are rounded to four decimal places, and the output is formatted according to the input type.\n\n```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z). This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    # Convert input to a numpy array for consistent processing\n    z = np.asarray(z)\n    \n    # Compute the softplus using the numerically stable logaddexp\n    softplus_values = np.logaddexp(0.0, z)\n    \n    # Round the results to 4 decimal places\n    softplus_values = np.round(softplus_values, 4)\n    \n    # Check if the input was a scalar or array-like\n    if np.isscalar(z):\n        return float(softplus_values)  # Return as float for scalar input\n    else:\n        return softplus_values.tolist()  # Convert to list for array-like input\n\n# Example usage:\n# z = np.array([-1000, 0, 3])\n# print(softplus(z))  # Output: [0.0, 0.6931, 3.0486]\n```\n\n### Explanation:\n1. **Input Handling**: The function first converts the input `z` to a NumPy array using `np.asarray(z)`. This allows for consistent handling of both scalars and array-like structures.\n  \n2. **Softplus Calculation**: The softplus values are computed using `np.logaddexp(0.0, z)`, which is numerically stable and avoids overflow issues.\n\n3. **Rounding**: The results are rounded to four decimal places using `np.round`.\n\n4. **Output Formatting**: The function checks if the input was a scalar using `np.isscalar(z)`. If it is a scalar, it returns a single float; otherwise, it converts the result to a pure Python list using `tolist()`.\n\nThis implementation meets all the requirements specified in your task."}
{"task_id": 90, "completion_id": 0, "completion": "To implement the `mse` function that computes the mean-squared error between the true expected payouts of each arm and the policy's estimates, we can follow these steps:\n\n1. Check if the `policy` has any estimates. If not, return `numpy.nan`.\n2. Extract the true expected values from the `bandit` and the estimated values from the `policy`.\n3. Sort the indices of the estimates to ensure they match the order of the true expected values.\n4. Calculate the squared error for each arm and compute the mean of these squared errors.\n5. Round the result to four decimal places and return it.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    # Check if the policy has any estimates\n    if not hasattr(policy, 'ev_estimates') or not policy.ev_estimates:\n        return np.nan\n    \n    # Extract true expected values and policy estimates\n    true_values = bandit['arm_evs']\n    estimates = policy['ev_estimates']\n    \n    # Sort the indices of the estimates\n    sorted_indices = sorted(estimates.keys())\n    \n    # Calculate squared errors\n    squared_errors = []\n    for i in sorted_indices:\n        if i < len(true_values):  # Ensure we don't go out of bounds\n            true_value = true_values[i]\n            estimate = estimates[i]\n            squared_error = (estimate - true_value) ** 2\n            squared_errors.append(squared_error)\n    \n    # Calculate mean squared error\n    if squared_errors:\n        mse_value = np.mean(squared_errors)\n        return round(mse_value, 4)\n    else:\n        return np.nan\n```\n\n### Explanation of the Code:\n- We first check if the `policy` has the `ev_estimates` attribute and if it is not empty. If it is missing or empty, we return `numpy.nan`.\n- We then extract the true expected values from the `bandit` and the estimates from the `policy`.\n- We sort the indices of the estimates to ensure that we are comparing the correct pairs of true values and estimates.\n- We compute the squared error for each arm and store these in a list.\n- Finally, we calculate the mean of the squared errors and round it to four decimal places before returning it.\n\nThis implementation ensures that we handle the input correctly and compute the mean-squared error as required."}
{"task_id": 96, "completion_id": 0, "completion": "Here's a complete implementation of the `additive_ngram_log_prob` function that computes the additive-smoothed log-probability of a given sequence based on a training corpus. The function builds N-gram frequency tables, handles unseen words with an `<UNK>` token, and calculates the log-probability of the sequence.\n\n```python\nimport math\nfrom collections import Counter, defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    \n    # Step 1: Build N-gram frequency tables\n    ngram_counts = defaultdict(Counter)\n    total_counts = Counter()\n    \n    # Create a vocabulary set\n    vocab = set()\n    \n    for sentence in corpus:\n        words = sentence.split()\n        vocab.update(words)\n        \n        # Create N-grams\n        for i in range(len(words)):\n            for n in range(1, N + 1):\n                if i + n <= len(words):\n                    ngram = tuple(words[i:i + n])\n                    context = ngram[:-1]  # context is all but the last word\n                    ngram_counts[n][context][ngram[-1]] += 1\n                    total_counts[context] += 1\n    \n    # Step 2: Add <UNK> to the vocabulary\n    vocab.add(\"<UNK>\")\n    vocab_size = len(vocab)\n    \n    # Step 3: Calculate log-probability of the target sequence\n    words = sequence.split()\n    if len(words) < N:\n        return 0.0  # No N-grams can be formed\n    \n    log_prob = 0.0\n    \n    for i in range(len(words)):\n        for n in range(1, N + 1):\n            if i + n <= len(words):\n                ngram = tuple(words[i:i + n])\n                context = ngram[:-1]  # context is all but the last word\n                word = ngram[-1]\n                \n                # Handle unseen words\n                if word not in vocab:\n                    word = \"<UNK>\"\n                \n                # Get counts with smoothing\n                count_context = total_counts[context]\n                count_ngram = ngram_counts[n][context][word]\n                \n                # Calculate the smoothed probability\n                prob = (count_ngram + K) / (count_context + K * vocab_size)\n                \n                # Update log probability\n                log_prob += math.log(prob)\n    \n    return round(log_prob, 4)\n\n# Example usage:\ncorpus = [\"the cat sat\", \"the dog barked\", \"the cat meowed\"]\nsequence = \"the cat\"\nN = 2\nprint(additive_ngram_log_prob(corpus, sequence, N))  # Example output\n```\n\n### Explanation:\n1. **N-gram Frequency Tables**: The function builds frequency tables for N-grams using a nested loop. The outer loop iterates through each sentence, while the inner loop generates N-grams of varying lengths.\n\n2. **Vocabulary Handling**: The vocabulary is built from the corpus, and an `<UNK>` token is added to handle unseen words.\n\n3. **Log-Probability Calculation**: For each N-gram in the target sequence, the function calculates the smoothed probability using the formula provided. It sums the logarithms of these probabilities.\n\n4. **Return Value**: The final log-probability is rounded to four decimal places before being returned.\n\nThis implementation adheres to the requirements specified in the task description."}
{"task_id": 108, "completion_id": 0, "completion": "Here's a complete implementation of the `als_factorization` function that performs regularized Alternating Least Squares (ALS) matrix factorization. The function initializes the factor matrices, iteratively updates them, and checks for convergence based on the specified tolerance or maximum iterations.\n\n```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    np.random.seed(0)  # Set random seed for reproducibility\n    N, M = X.shape\n    \n    # Initialize W and H with random values\n    W = np.random.rand(N, K)\n    H = np.random.rand(K, M)\n\n    for iteration in range(max_iter):\n        # Update W while keeping H fixed\n        for i in range(N):\n            HHT = H @ H.T\n            regularization = alpha * np.eye(K)\n            W[i, :] = np.linalg.solve(HHT + regularization, X[i, :] @ H.T)\n\n        # Update H while keeping W fixed\n        for j in range(M):\n            WTW = W.T @ W\n            regularization = alpha * np.eye(K)\n            H[:, j] = np.linalg.solve(WTW + regularization, W.T @ X[:, j])\n\n        # Calculate the current loss\n        X_hat = W @ H\n        loss = np.linalg.norm(X - X_hat, 'fro')**2 + alpha * (np.linalg.norm(W, 'fro')**2 + np.linalg.norm(H, 'fro')**2)\n\n        # Check for convergence\n        if loss < tol:\n            break\n\n    # Return the reconstructed matrix rounded to 4 decimals\n    return X_hat.round(4).tolist()\n```\n\n### Explanation of the Code:\n1. **Initialization**: The function starts by setting a random seed for reproducibility and initializes the factor matrices `W` and `H` with random values.\n\n2. **ALS Iteration**:\n   - The outer loop runs for a maximum of `max_iter` iterations.\n   - Inside the loop, `W` is updated by solving a regularized least squares problem for each row of `X` while keeping `H` fixed.\n   - Similarly, `H` is updated by solving a regularized least squares problem for each column of `X` while keeping `W` fixed.\n\n3. **Loss Calculation**: After updating both matrices, the function calculates the current loss based on the Frobenius norm of the difference between `X` and the reconstructed matrix `X_hat`, along with the regularization terms.\n\n4. **Convergence Check**: If the loss drops below the specified tolerance `tol`, the loop breaks, indicating convergence.\n\n5. **Return Value**: Finally, the reconstructed matrix `X_hat` is rounded to four decimal places and converted to a list of lists before being returned.\n\nThis implementation should work effectively for matrix factorization using the ALS method."}
{"task_id": 109, "completion_id": 0, "completion": "Here's a complete implementation of the K-Means clustering algorithm with K-Means++ initialization, following the specifications you've provided:\n\n```python\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with shape (m, n) where *m* is the number of samples and *n* is the\n        number of features.\n    K : int\n        The number of clusters to form.\n    max_iter : int, default = 100\n        Maximum number of iterations for the Lloyd refinement loop.\n    random_state : int | None, default = None\n        Seed for both Python's `random` module and NumPy's RNG to make the result\n        reproducible. If *None*, no seed is set.\n\n    Returns\n    -------\n    list[list[float]]\n        A list of K centroids (each centroid is a list of floats) rounded to 4 decimal\n        places and sorted in ascending order by their first coordinate.\n    \"\"\"\n    # Set random seed for reproducibility\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n\n    m, n = X.shape\n\n    # Step 1: Initialize centroids using K-Means++\n    centroids = []\n    # Choose the first centroid randomly from the data points\n    first_centroid_idx = np.random.choice(m)\n    centroids.append(X[first_centroid_idx])\n\n    for _ in range(1, K):\n        # Calculate distances from each point to the nearest centroid\n        distances = np.array([min(np.linalg.norm(x - c) ** 2 for c in centroids) for x in X])\n        # Choose the next centroid with probability proportional to the squared distance\n        probabilities = distances / distances.sum()\n        next_centroid_idx = np.random.choice(m, p=probabilities)\n        centroids.append(X[next_centroid_idx])\n\n    centroids = np.array(centroids)\n\n    # Step 2: Lloyd's algorithm\n    for _ in range(max_iter):\n        # Assignment step: assign clusters based on closest centroid\n        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        # Update step: calculate new centroids\n        new_centroids = np.array([X[labels == k].mean(axis=0) if np.any(labels == k) else centroids[k] for k in range(K)])\n\n        # Check for convergence (if centroids do not change)\n        if np.all(centroids == new_centroids):\n            break\n\n        centroids = new_centroids\n\n    # Round centroids to 4 decimal places and sort\n    centroids = np.round(centroids, 4)\n    centroids = sorted(centroids.tolist(), key=lambda x: (x[0], x))\n\n    return centroids\n```\n\n### Explanation of the Code:\n1. **Random Seed Initialization**: The function sets the random seed for both Python's `random` module and NumPy's random generator to ensure reproducibility.\n\n2. **K-Means++ Initialization**:\n   - The first centroid is chosen randomly from the dataset.\n   - Subsequent centroids are chosen based on the squared distance from the nearest existing centroid, with probabilities proportional to these distances.\n\n3. **Lloyd's Algorithm**:\n   - The algorithm iteratively assigns each data point to the nearest centroid and then updates the centroids based on the mean of the assigned points.\n   - If a cluster ends up empty, its centroid remains unchanged.\n\n4. **Final Output**:\n   - The centroids are rounded to four decimal places and sorted primarily by the first coordinate and secondarily by the complete centroid tuple to handle ties.\n\nThis implementation should work correctly for the K-Means clustering task as specified."}
{"task_id": 111, "completion_id": 0, "completion": "Here's the implementation of the three helper functions as per your requirements:\n\n```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Encodes a string into a fixed-length list of integer token ids.\"\"\"\n    encoded = []\n    \n    for char in text:\n        # Append the token id if the character is in the vocabulary, otherwise append padding token (0)\n        encoded.append(vocabulary.get(char, 0))\n    \n    # Truncate or pad the sequence to the fixed length\n    if len(encoded) < time_steps:\n        encoded += [0] * (time_steps - len(encoded))  # Pad with 0s\n    else:\n        encoded = encoded[:time_steps]  # Truncate to time_steps\n    \n    return encoded\n\ndef int_to_string(indices: list[int], inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer ids back to a human-readable string.\"\"\"\n    decoded = []\n    \n    for index in indices:\n        if index != 0:  # Ignore padding tokens\n            decoded.append(inverse_vocab.get(index, ''))  # Get the character or empty string if not found\n    \n    return ''.join(decoded)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encodes the input text, predicts using the model, and decodes the output.\"\"\"\n    # Step 1: Encode the input text\n    encoded_input = string_to_int(text, TIME_STEPS, input_vocabulary)\n    \n    # Step 2: Prepare the input for the model (batch size of 1)\n    input_array = np.array(encoded_input).reshape(1, -1)  # Reshape to (1, TIME_STEPS)\n    \n    # Step 3: Get model predictions\n    predictions = model.predict(input_array)  # Assuming model.predict returns a 2D array\n    \n    # Step 4: Apply argmax to get the predicted ids\n    predicted_ids = np.argmax(predictions, axis=-1)[0]  # Get the first (and only) batch\n    \n    # Step 5: Decode the predicted ids back to string\n    decoded_output = int_to_string(predicted_ids, inv_output_vocabulary)\n    \n    return decoded_output\n```\n\n### Explanation:\n1. **`string_to_int`**:\n   - This function takes a string and encodes each character using the provided vocabulary. If a character is not found in the vocabulary, it uses the padding token `0`.\n   - The resulting list is either truncated or padded to ensure it has a fixed length of `TIME_STEPS`.\n\n2. **`int_to_string`**:\n   - This function converts a list of integer ids back to a string, ignoring any padding tokens (`0`). It uses the inverse vocabulary to map integer ids back to characters.\n\n3. **`run_example`**:\n   - This function orchestrates the encoding of the input text, feeding it to the model for prediction, and then decoding the output back to a string.\n   - It reshapes the input to match the expected input shape of the model and processes the predictions to obtain the final output string.\n\n### Note:\n- The `model.predict` function is assumed to return a 2D NumPy array where the last dimension corresponds to the predicted probabilities for each character in the vocabulary. Adjustments may be needed based on the actual model's output shape."}
{"task_id": 113, "completion_id": 0, "completion": "Here's the implementation of the `run_examples` function as per your requirements:\n\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str]\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    # Use the global EXAMPLES if examples is None\n    if examples is None:\n        global EXAMPLES\n        examples = EXAMPLES\n\n    predictions = []\n\n    for example in examples:\n        # Call the run_example function to get the predicted characters\n        predicted_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        \n        # Join the characters to form the predicted string\n        predicted_string = ''.join(predicted_chars)\n        \n        # Print the input and output\n        print(f\"input:  {example}\")\n        print(f\"output: {predicted_string}\")\n        \n        # Collect the predicted string\n        predictions.append(predicted_string)\n\n    return predictions\n```\n\n### Explanation:\n1. **Function Signature**: The function `run_examples` takes four parameters as specified.\n2. **Global Constant Handling**: If `examples` is `None`, it falls back to a global constant `EXAMPLES`.\n3. **Prediction Loop**: For each example in `examples`:\n   - It calls the `run_example` function to get the predicted characters.\n   - The characters are concatenated into a single string.\n   - The input and output are printed in the specified format.\n   - The predicted string is collected in a list.\n4. **Return Value**: Finally, the function returns a list of all predicted strings in the same order as the inputs.\n\nThis implementation assumes that the `run_example` function is defined elsewhere in the codebase and works as expected."}
{"task_id": 115, "completion_id": 0, "completion": "Here's the implementation of the `logistic_loss_and_gradient` function that computes the average binary cross-entropy loss and its gradient for logistic regression:\n\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    # Number of samples\n    m = X.shape[0]\n    \n    # Compute the linear combination z = X \u00b7 w\n    z = np.dot(X, w)\n    \n    # Compute the predicted probabilities using the sigmoid function\n    p = 1 / (1 + np.exp(-z))\n    \n    # To avoid log(0), we clip the probabilities\n    p = np.clip(p, 1e-20, 1 - 1e-20)\n    \n    # Compute the binary cross-entropy loss\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    \n    # Compute the gradient of the loss with respect to the weights\n    gradient = (1 / m) * np.dot(X.T, (p - y))\n    \n    # Round the loss and gradient to 4 decimals\n    loss_rounded = round(loss, 4)\n    gradient_rounded = gradient.round(4).tolist()\n    \n    return loss_rounded, gradient_rounded\n```\n\n### Explanation:\n1. **Input Parameters**:\n   - `X`: The feature matrix of shape (m, n), where m is the number of samples and n is the number of features.\n   - `y`: The binary target vector of shape (m,) or (m, 1).\n   - `w`: The weight vector of shape (n,) or (n, 1).\n\n2. **Calculating Predictions**:\n   - We compute the linear combination `z = X \u00b7 w`.\n   - The predicted probabilities `p` are calculated using the sigmoid function.\n\n3. **Clipping Probabilities**:\n   - To avoid numerical issues with logarithms, we clip the predicted probabilities to be within the range [1e-20, 1 - 1e-20].\n\n4. **Loss Calculation**:\n   - The average binary cross-entropy loss is computed using the formula provided.\n\n5. **Gradient Calculation**:\n   - The gradient of the loss with respect to the weights is computed.\n\n6. **Rounding and Formatting**:\n   - The loss is rounded to 4 decimal places.\n   - The gradient is rounded to 4 decimal places and converted to a nested Python list.\n\n### Usage:\nYou can call this function with appropriate `X`, `y`, and `w` to get the loss and gradient for logistic regression."}
{"task_id": 118, "completion_id": 0, "completion": "Here's a complete implementation of the AdaBoost algorithm using decision stumps as weak learners. The code follows the specifications provided in your task description.\n\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    \n    # Ensure n_clf is at least 1\n    n_clf = max(1, n_clf)\n    \n    m, n = X_train.shape\n    # Initialize weights\n    weights = np.ones(m) / m\n    stumps = []  # To store the weak classifiers\n    alphas = []  # To store the weights of the classifiers\n\n    for _ in range(n_clf):\n        # Initialize the best stump parameters\n        best_stump = None\n        best_error = float('inf')\n        best_pred = None\n        \n        # Iterate over each feature\n        for feature_index in range(n):\n            # Get unique values of the feature\n            unique_values = np.unique(X_train[:, feature_index])\n            \n            # Test each unique value as a potential threshold\n            for threshold in unique_values:\n                # Create predictions for the stump\n                predictions = np.ones(m)\n                predictions[X_train[:, feature_index] < threshold] = -1\n                \n                # Calculate the weighted error\n                error = np.sum(weights * (predictions != y_train))\n                \n                # If the error is better than the best found so far, update\n                if error < best_error:\n                    best_error = error\n                    best_stump = (feature_index, threshold)\n                    best_pred = predictions\n        \n        # Calculate alpha (weight of the stump)\n        alpha = 0.5 * np.log((1 - best_error) / (best_error + 1e-10))  # Add small value to avoid division by zero\n        alphas.append(alpha)\n        stumps.append(best_stump)\n        \n        # Update weights\n        weights *= np.exp(-alpha * y_train * best_pred)\n        weights /= np.sum(weights)  # Normalize weights\n\n    # Make predictions on the test set\n    k = X_test.shape[0]\n    final_predictions = np.zeros(k)\n\n    for alpha, stump in zip(alphas, stumps):\n        feature_index, threshold = stump\n        predictions = np.ones(k)\n        predictions[X_test[:, feature_index] < threshold] = -1\n        final_predictions += alpha * predictions\n\n    # Return the sign of the final predictions\n    return list(np.sign(final_predictions).astype(int))\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 1], [4, 2]])\n# y_train = np.array([1, 1, -1, -1])\n# X_test = np.array([[1, 1], [2, 2], [3, 3]])\n# predictions = adaboost_predict(X_train, y_train, X_test, n_clf=5)\n# print(predictions)\n```\n\n### Explanation:\n1. **Initialization**: The weights for each training sample are initialized uniformly.\n2. **Stump Selection**: For each feature, we evaluate all unique values as potential thresholds to create decision stumps. We calculate the weighted error for each stump and select the one with the lowest error.\n3. **Weight Update**: After selecting a stump, we compute its weight (alpha) and update the weights of the training samples based on whether they were classified correctly or not.\n4. **Prediction**: For the test set, we aggregate the predictions from all stumps using their respective weights and return the final predictions based on the sign of the aggregated result.\n\nThis implementation is efficient and adheres to the principles of the AdaBoost algorithm using decision stumps as weak learners."}
{"task_id": 128, "completion_id": 0, "completion": "Here's a complete implementation of the multi-class Linear Discriminant Analysis (LDA) as per your requirements. The code computes the within-class and between-class scatter matrices, performs eigen-decomposition, and projects the data onto the specified number of discriminant components.\n\n```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    # Number of samples and features\n    m, d = X.shape\n    \n    # Calculate the global mean\n    mu = np.mean(X, axis=0)\n    \n    # Unique classes\n    classes = np.unique(y)\n    \n    # Initialize within-class scatter matrix and between-class scatter matrix\n    S_W = np.zeros((d, d))\n    S_B = np.zeros((d, d))\n    \n    # Calculate within-class scatter matrix S_W and between-class scatter matrix S_B\n    for c in classes:\n        X_c = X[y == c]\n        mu_c = np.mean(X_c, axis=0)\n        N_c = X_c.shape[0]\n        \n        # Within-class scatter\n        S_W += np.dot((X_c - mu_c).T, (X_c - mu_c))\n        \n        # Between-class scatter\n        S_B += N_c * np.dot((mu_c - mu).reshape(-1, 1), (mu_c - mu).reshape(1, -1))\n    \n    # Compute the pseudo-inverse of S_W\n    S_W_inv = np.linalg.pinv(S_W)\n    \n    # Compute the matrix A\n    A = np.dot(S_W_inv, S_B)\n    \n    # Eigen-decomposition\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    \n    # Sort the eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Select the top n_components eigenvectors\n    W = eigenvectors[:, :n_components]\n    \n    # Ensure deterministic sign\n    for i in range(W.shape[1]):\n        if W[0, i] < 0:\n            W[:, i] = -W[:, i]\n    \n    # Project the data\n    X_lda = np.dot(X, W)\n    \n    # Round the results to four decimal places and convert to list of lists\n    return X_lda.round(4).tolist()\n\n# Example usage\nX = np.array([[1,1],[1,2],[2,1],[2,2],[8,8],[9,8],[8,9],[9,9]])\ny = np.array([0,0,0,0,1,1,1,1])\nn_components = 1\n\noutput = multi_class_lda(X, y, n_components)\nprint(output)\n```\n\n### Explanation of the Code:\n1. **Input Parameters**: The function takes a data matrix `X`, a label vector `y`, and the number of components `n_components`.\n2. **Mean Calculation**: It computes the global mean of the data.\n3. **Scatter Matrices**: It calculates the within-class scatter matrix `S_W` and the between-class scatter matrix `S_B`.\n4. **Pseudo-Inverse**: The pseudo-inverse of `S_W` is computed to handle cases where `S_W` might be singular.\n5. **Eigen-Decomposition**: The eigenvalues and eigenvectors of the matrix `A = S_W^{-1} S_B` are computed, and the eigenvalues are sorted in descending order.\n6. **Deterministic Sign**: The signs of the eigenvectors are adjusted to ensure consistency.\n7. **Projection**: The original data is projected onto the selected eigenvectors, and the results are rounded to four decimal places before being returned as a list of lists.\n\nThis implementation should work correctly for valid inputs as specified in the task description."}
{"task_id": 140, "completion_id": 0, "completion": "To solve the problem of escaping from a fire maze, we can use a breadth-first search (BFS) approach. The key steps are to simulate the spread of fire and the movement of the agent while ensuring that the agent does not enter a cell that is on fire when it arrives.\n\nHere's how we can implement the solution:\n\n1. **Fire Spread Simulation**: We will first simulate the spread of fire using BFS. We will create a grid that keeps track of the time at which each cell catches fire.\n\n2. **Agent Movement Simulation**: After determining when each cell catches fire, we will perform another BFS for the agent, ensuring that it only moves to cells that are not on fire when it arrives.\n\n3. **Edge Cases**: We will handle edge cases such as the starting or goal cell being on fire, and the case where the maze is 1x1.\n\nHere is the complete implementation:\n\n```python\nfrom collections import deque\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    n = len(grid)\n    \n    # Edge case: if the start or goal is on fire\n    if grid[0][0] == 'F' or grid[n-1][n-1] == 'F':\n        return -1\n    \n    # Directions for moving in the maze\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    # Step 1: Spread fire using BFS\n    fire_time = [[float('inf')] * n for _ in range(n)]\n    fire_queue = deque()\n    \n    # Initialize fire positions\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_queue.append((i, j))\n                fire_time[i][j] = 0\n    \n    while fire_queue:\n        x, y = fire_queue.popleft()\n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            if 0 <= nx < n and 0 <= ny < n and grid[nx][ny] == '.' and fire_time[nx][ny] == float('inf'):\n                fire_time[nx][ny] = fire_time[x][y] + 1\n                fire_queue.append((nx, ny))\n    \n    # Step 2: Move the agent using BFS\n    agent_queue = deque([(0, 0, 0)])  # (x, y, time)\n    visited = [[False] * n for _ in range(n)]\n    visited[0][0] = True\n    \n    while agent_queue:\n        x, y, time = agent_queue.popleft()\n        \n        # If the agent reaches the goal\n        if x == n - 1 and y == n - 1:\n            return time\n        \n        for dx, dy in directions:\n            nx, ny = x + dx, y + dy\n            if 0 <= nx < n and 0 <= ny < n and not visited[nx][ny]:\n                # Check if the cell is free and not on fire when the agent arrives\n                if grid[nx][ny] == '.' and time + 1 < fire_time[nx][ny]:\n                    visited[nx][ny] = True\n                    agent_queue.append((nx, ny, time + 1))\n    \n    # If we exhaust the queue without reaching the goal\n    return -1\n```\n\n### Explanation of the Code:\n- **Fire Spread Simulation**: We initialize a queue with all fire positions and use BFS to fill the `fire_time` grid, which records the time at which each cell catches fire.\n- **Agent Movement Simulation**: We initialize the agent's queue starting from the top-left corner and use BFS to explore possible moves. We check if the next cell is free and not on fire at the time the agent would arrive.\n- **Return Value**: If the agent reaches the bottom-right corner, we return the time taken; otherwise, we return -1 if it's impossible to reach the goal.\n\nThis approach ensures that we efficiently simulate both the fire spread and the agent's movement while adhering to the constraints of the problem."}
{"task_id": 141, "completion_id": 0, "completion": "Here's a complete implementation of the k-Nearest Neighbors (k-NN) classifier as specified in your task. The function computes distances using the specified metrics, finds the k nearest neighbors, and predicts the class based on majority voting.\n\n```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"Predict the class labels for the test set using k-NN classifier.\"\"\"\n    \n    def euclidean_distance(a, b):\n        return np.sqrt(np.sum((a - b) ** 2))\n\n    def manhattan_distance(a, b):\n        return np.sum(np.abs(a - b))\n\n    def cosine_distance(a, b):\n        return 1 - np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n    # Select the distance function based on the metric\n    if metric == 'euclidean':\n        distance_function = euclidean_distance\n    elif metric == 'manhattan':\n        distance_function = manhattan_distance\n    elif metric == 'cosine':\n        distance_function = cosine_distance\n    else:\n        distance_function = euclidean_distance  # fallback to Euclidean\n\n    predictions = []\n    \n    for test_sample in X_test:\n        # Compute distances from the test sample to all training samples\n        distances = np.array([distance_function(test_sample, train_sample) for train_sample in X_train])\n        \n        # Get the indices of the k nearest neighbors\n        k_indices = np.argsort(distances)[:k]\n        \n        # Get the labels of the k nearest neighbors\n        k_nearest_labels = y_train[k_indices]\n        \n        # Perform majority voting\n        unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)\n        max_count_index = np.argmax(counts)\n        \n        # In case of a tie, return the smallest label value\n        if np.sum(counts == counts[max_count_index]) > 1:\n            predicted_label = unique_labels[np.argmin(unique_labels[counts == counts[max_count_index]])]\n        else:\n            predicted_label = unique_labels[max_count_index]\n        \n        predictions.append(predicted_label)\n\n    return np.array(predictions)\n```\n\n### Explanation:\n1. **Distance Functions**: Three distance functions are defined: `euclidean_distance`, `manhattan_distance`, and `cosine_distance`.\n2. **Distance Calculation**: For each test sample, the function calculates the distance to all training samples using the selected distance metric.\n3. **Finding Neighbors**: It sorts the distances and retrieves the indices of the k smallest distances.\n4. **Majority Voting**: It counts the occurrences of each label among the k nearest neighbors and determines the predicted label based on majority voting. In case of a tie, it selects the smallest label.\n5. **Return Value**: The function returns a 1-D NumPy array of predicted labels for the test samples.\n\n### Usage:\nYou can use this function by passing your training data, test data, labels, and the desired value of k. For example:\n\n```python\nX_train = np.array([[1, 2], [2, 3], [3, 4]])\ny_train = np.array([0, 0, 1])\nX_test = np.array([[1, 1], [2, 2]])\nk = 2\n\npredictions = knn_predict(X_train, y_train, X_test, k)\nprint(predictions)  # Output will be the predicted labels for the test samples\n```"}
{"task_id": 146, "completion_id": 0, "completion": "Here's a complete implementation of the k-Nearest Neighbours (k-NN) classifier as per your specifications. The function computes distances using the specified metric, selects the k closest neighbors, and performs a majority vote to predict the labels for the test samples.\n\n```python\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    \n    def euclidean_distance(a, b):\n        return np.sqrt(np.sum((a - b) ** 2))\n\n    def manhattan_distance(a, b):\n        return np.sum(np.abs(a - b))\n\n    def cosine_distance(a, b):\n        dot_product = np.dot(a, b)\n        norm_a = np.linalg.norm(a)\n        norm_b = np.linalg.norm(b)\n        if norm_a == 0 or norm_b == 0:\n            return 1.0  # If either vector is zero, return max distance\n        return 1 - (dot_product / (norm_a * norm_b + 1e-12))\n\n    # Select the appropriate distance function\n    if metric == 'euclidean':\n        distance_function = euclidean_distance\n    elif metric == 'manhattan':\n        distance_function = manhattan_distance\n    elif metric == 'cosine':\n        distance_function = cosine_distance\n    else:\n        raise ValueError(\"Unknown metric: choose 'euclidean', 'manhattan', or 'cosine'.\")\n\n    predictions = []\n    n_train = X.shape[0]\n    k = min(k, n_train)  # Use all samples if k exceeds the number of training samples\n\n    for test_sample in X_test:\n        # Compute distances from the test sample to all training samples\n        distances = np.array([distance_function(test_sample, train_sample) for train_sample in X])\n        \n        # Get the indices of the k closest neighbors\n        k_indices = np.argsort(distances)[:k]\n        \n        # Get the labels of the k closest neighbors\n        k_nearest_labels = y[k_indices]\n        \n        # Perform a majority vote\n        unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)\n        max_count_index = np.argmax(counts)\n        max_count = counts[max_count_index]\n        \n        # Handle tie by selecting the smallest label\n        if np.sum(counts == max_count) > 1:\n            predicted_label = unique_labels[counts == max_count].min()\n        else:\n            predicted_label = unique_labels[max_count_index]\n        \n        predictions.append(predicted_label)\n\n    return predictions\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 4]])\n# y_train = np.array([0, 1, 0])\n# X_test = np.array([[1, 2], [2, 2]])\n# print(knn_predict(X_train, y_train, X_test, k=2, metric='euclidean'))\n```\n\n### Explanation:\n1. **Distance Functions**: Three distance functions are defined: Euclidean, Manhattan, and Cosine. Each function computes the distance between two points.\n2. **Distance Calculation**: For each test sample, distances to all training samples are calculated using the selected distance metric.\n3. **Finding Neighbors**: The indices of the k closest training samples are determined using `np.argsort`.\n4. **Majority Voting**: The labels of the k nearest neighbors are counted, and the most common label is selected. In case of a tie, the smallest label is chosen.\n5. **Return Value**: The function returns a list of predicted labels for the test samples.\n\nThis implementation adheres to the constraints and requirements specified in your task description."}
{"task_id": 155, "completion_id": 0, "completion": "Here's the implementation of the `actor_forward` function that performs the forward pass of the actor network as described in your task. The function uses NumPy to handle the matrix operations and applies the ReLU and tanh activation functions as specified.\n\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # Convert state to a numpy array if it's a list\n    state = np.array(state)\n\n    # First layer: Linear(W1, b1)\n    z1 = np.dot(state, weights['W1']) + weights['b1']\n    # First activation: ReLU\n    a1 = np.maximum(0, z1)\n\n    # Second layer: Linear(W2, b2)\n    z2 = np.dot(a1, weights['W2']) + weights['b2']\n    # Second activation: ReLU\n    a2 = np.maximum(0, z2)\n\n    # Third layer: Linear(W3, b3)\n    z3 = np.dot(a2, weights['W3']) + weights['b3']\n    # Final activation: tanh\n    action = np.tanh(z3)\n\n    # Round the action vector to 4 decimal places and convert to list\n    return list(np.round(action, 4))\n```\n\n### Explanation of the Code:\n1. **Input Handling**: The function accepts a state vector and a dictionary of weights. If the state is provided as a list, it is converted to a NumPy array for easier mathematical operations.\n\n2. **First Layer**:\n   - The first affine transformation is computed using the weight matrix `W1` and bias vector `b1`.\n   - The ReLU activation function is applied to the result.\n\n3. **Second Layer**:\n   - The second affine transformation is computed using the weight matrix `W2` and bias vector `b2`.\n   - The ReLU activation function is applied again.\n\n4. **Output Layer**:\n   - The final affine transformation is computed using the weight matrix `W3` and bias vector `b3`.\n   - The tanh activation function is applied to ensure the output is in the range [-1, 1].\n\n5. **Output Formatting**: The resulting action vector is rounded to four decimal places and converted to a list before being returned.\n\nThis implementation adheres to the specifications provided and should work correctly for the forward pass of the actor network."}
{"task_id": 160, "completion_id": 0, "completion": "Here's a complete implementation of Lasso regression using the coordinate descent optimization strategy as described in your task. The function `fit_lasso` takes a design matrix `X`, a target vector `y`, a regularization parameter `lambda_param`, the maximum number of iterations, and a flag to fit an intercept. It returns the learned weights and the intercept.\n\n```python\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\n\n    Args:\n        X:   2-D array of shape (m, n) \u2013 feature matrix.\n        y:   1-D array of length m \u2013 target values.\n        lambda_param: Regularisation strength \u03bb (non-negative).\n        max_iters: Number of full passes over the coordinates.\n        fit_intercept: Whether to fit an intercept term.\n\n    Returns:\n        Tuple (weights, bias) where `weights` is a list of length n and `bias`\n        is a float.  Round all returned numbers to 4 decimal places.\n    \"\"\"\n    m, n = X.shape\n    \n    # Step 1: Add intercept column if needed\n    if fit_intercept:\n        X = np.hstack((np.ones((m, 1)), X))  # Add a column of ones for the intercept\n        n += 1  # Increase the number of features to account for the intercept\n    \n    # Initialize weights (including intercept)\n    w = np.zeros(n)\n    \n    for _ in range(max_iters):\n        # Compute the residuals\n        if fit_intercept:\n            r = y - X @ w  # Residuals without the intercept\n        else:\n            r = y - X @ w\n        \n        # Step 3: Coordinate descent\n        for j in range(n):\n            # Temporarily set the current weight to 0\n            w_j_temp = w[j]\n            w[j] = 0\n            \n            # Compute the partial residual\n            r_j = r + X[:, j] * w_j_temp\n            \n            # Compute the soft-thresholding operator\n            a_j = X[:, j].T @ r_j\n            tau = lambda_param * m\n            \n            # Update weight w_j using the soft-thresholding operator\n            w[j] = np.sign(a_j) * max(abs(a_j) - tau, 0) / np.sum(X[:, j] ** 2) if np.sum(X[:, j] ** 2) != 0 else 0\n            \n            # Restore the residuals\n            r = r_j - X[:, j] * w[j]\n        \n        # Step 2: Update intercept if fitting\n        if fit_intercept:\n            w[0] = np.mean(y - X[:, 1:] @ w[1:])  # Update intercept based on the mean residual\n    \n    # Return the weights and bias, rounded to 4 decimal places\n    weights = w[1:] if fit_intercept else w\n    bias = w[0] if fit_intercept else 0.0\n    return [round(weight, 4) for weight in weights], round(bias, 4)\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4]])\n# y = np.array([1, 2, 3])\n# weights, bias = fit_lasso(X, y, lambda_param=0.1, max_iters=100, fit_intercept=True)\n# print(weights, bias)\n```\n\n### Explanation of the Code:\n1. **Input Handling**: The function accepts a feature matrix `X`, a target vector `y`, a regularization parameter `lambda_param`, the maximum number of iterations, and a flag to fit an intercept.\n2. **Intercept Handling**: If `fit_intercept` is `True`, a column of ones is added to `X` to account for the intercept term.\n3. **Weight Initialization**: All weights (including the intercept) are initialized to zero.\n4. **Coordinate Descent Loop**: For each iteration, the algorithm updates each weight using the soft-thresholding operator after calculating the partial residuals.\n5. **Intercept Update**: If fitting an intercept, it is updated based on the mean of the residuals.\n6. **Output**: The function returns the weights and bias rounded to four decimal places.\n\nThis implementation should work effectively for Lasso regression using coordinate descent."}
{"task_id": 165, "completion_id": 0, "completion": "Here's a Python function that computes the normalized Hamming distance between two 1-D integer vectors, adhering to the requirements you've specified:\n\n```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | np.ndarray, y: list[int] | np.ndarray) -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are lists\n    if isinstance(x, list):\n        x = np.array(x)\n    if isinstance(y, list):\n        y = np.array(y)\n    \n    # Check if the lengths are the same\n    if x.shape[0] != y.shape[0]:\n        return -1\n    \n    # Calculate the Hamming distance\n    differences = np.sum(x != y)\n    N = x.shape[0]\n    distance = differences / N\n    \n    # Return the distance rounded to 4 decimal places\n    return round(distance, 4)\n\n# Example usage:\n# print(hamming_distance([1, 0, 1], [1, 1, 0]))  # Output: 0.3333\n# print(hamming_distance([1, 0, 1], [1, 0, 1]))  # Output: 0.0\n# print(hamming_distance([1, 0], [1, 0, 1]))     # Output: -1\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if the inputs are lists and converts them to NumPy arrays for easier manipulation.\n2. **Length Check**: It checks if the lengths of the two vectors are the same. If not, it returns -1.\n3. **Hamming Distance Calculation**: It computes the number of differing positions using `np.sum(x != y)`, which counts how many times the elements differ. The normalized distance is then calculated by dividing the number of differences by the total number of elements (N).\n4. **Rounding**: Finally, the result is rounded to four decimal places before being returned.\n\nThis implementation is efficient and leverages NumPy's capabilities for handling array operations."}
{"task_id": 169, "completion_id": 0, "completion": "Here's the implementation of the `generate_window` function that creates a list of window coefficients for the specified window types. The function handles the special cases as described in the task and ensures that the coefficients are rounded to four decimal places before returning.\n\n```python\nimport numpy as np\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Name of the desired window. Supported values are\n        \"hamming\", \"hann\", \"blackman_harris\" and \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate. Must be a positive integer.\n    coefficients : list[float] | None, optional\n        List of cosine\u2013series coefficients used **only** when\n        window == \"generalized_cosine\". The default is ``None``.\n\n    Returns\n    -------\n    list[float]\n        List with *N* floats rounded to 4 decimal places.\n    \"\"\"\n    \n    # Check if N is a positive integer\n    if not isinstance(N, int) or N <= 0:\n        raise ValueError(\"N must be a positive integer.\")\n    \n    # Handle the case where N = 1\n    if N == 1:\n        return [1.0]\n    \n    # Initialize the list to hold the window coefficients\n    window_coefficients = []\n    \n    # Generate the window coefficients based on the specified window type\n    if window == \"hamming\":\n        for n in range(N):\n            w_n = 0.54 - 0.46 * np.cos(2 * np.pi * n / (N - 1))\n            window_coefficients.append(round(w_n, 4))\n    \n    elif window == \"hann\":\n        for n in range(N):\n            w_n = 0.5 - 0.5 * np.cos(2 * np.pi * n / (N - 1))\n            window_coefficients.append(round(w_n, 4))\n    \n    elif window == \"blackman_harris\":\n        a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n        for n in range(N):\n            w_n = (a0 - a1 * np.cos(2 * np.pi * n / (N - 1)) +\n                    a2 * np.cos(4 * np.pi * n / (N - 1)) -\n                    a3 * np.cos(6 * np.pi * n / (N - 1)))\n            window_coefficients.append(round(w_n, 4))\n    \n    elif window == \"generalized_cosine\":\n        if coefficients is None:\n            raise ValueError(\"Coefficients must be provided for generalized_cosine window.\")\n        for n in range(N):\n            w_n = sum(a_k * np.cos(2 * np.pi * k * n / (N - 1)) for k, a_k in enumerate(coefficients))\n            window_coefficients.append(round(w_n, 4))\n    \n    else:\n        raise ValueError(\"Unknown window type. Supported values are 'hamming', 'hann', 'blackman_harris', and 'generalized_cosine'.\")\n    \n    return window_coefficients\n```\n\n### Explanation:\n1. **Input Validation**: The function checks if `N` is a positive integer and raises a `ValueError` if not. It also handles the special case where `N = 1` by returning a list with a single value of `1.0`.\n\n2. **Window Coefficient Calculation**: Depending on the specified window type, the function calculates the coefficients using the provided mathematical definitions. The coefficients are computed in a loop and rounded to four decimal places.\n\n3. **Generalized Cosine Window**: For the \"generalized_cosine\" window, the function checks if the `coefficients` list is provided and raises an error if it is not.\n\n4. **Return Value**: The function returns a list of the computed window coefficients.\n\nYou can use this function to generate the desired window coefficients for your digital signal processing tasks."}
{"task_id": 171, "completion_id": 0, "completion": "Here's a complete implementation of the binary Logistic Regression classifier using mini-batch Gradient Descent as per your requirements:\n\n```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    \n    # Convert input lists to numpy arrays for easier manipulation\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n\n    # Check for unique labels in y_train\n    unique_labels = np.unique(y_train)\n    if len(unique_labels) != 2:\n        raise ValueError(\"y_train must contain exactly two distinct labels.\")\n    \n    # Map labels to {0, 1}\n    label_mapping = {unique_labels[0]: 0, unique_labels[1]: 1}\n    y_train_mapped = np.vectorize(label_mapping.get)(y_train)\n\n    # Add intercept term (bias) to the training data\n    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n\n    # Initialize weights\n    weights = np.zeros(X_train.shape[1])\n\n    # Check if all training labels are identical\n    if np.all(y_train_mapped == y_train_mapped[0]):\n        # If all labels are the same, predict that label for all test samples\n        return [unique_labels[0]] * len(X_test)\n\n    # Mini-batch Gradient Descent\n    for epoch in range(epochs):\n        # Shuffle the training data\n        indices = np.arange(X_train.shape[0])\n        np.random.shuffle(indices)\n        X_train_shuffled = X_train[indices]\n        y_train_shuffled = y_train_mapped[indices]\n\n        for i in range(0, X_train.shape[0], batch_size):\n            # Create mini-batch\n            X_batch = X_train_shuffled[i:i + batch_size]\n            y_batch = y_train_shuffled[i:i + batch_size]\n\n            # Compute predictions\n            predictions = 1 / (1 + np.exp(-X_batch.dot(weights)))\n\n            # Compute gradients\n            errors = predictions - y_batch\n            gradient = X_batch.T.dot(errors) / len(y_batch)\n\n            # Update weights\n            weights -= learning_rate * gradient\n\n    # Make predictions on the test set\n    test_predictions = 1 / (1 + np.exp(-X_test.dot(weights)))\n    test_labels_mapped = (test_predictions >= 0.5).astype(int)\n\n    # Map predictions back to original labels\n    reverse_mapping = {0: unique_labels[0], 1: unique_labels[1]}\n    predictions = np.vectorize(reverse_mapping.get)(test_labels_mapped)\n\n    return predictions.tolist()\n```\n\n### Explanation of the Code:\n1. **Input Handling**: The function accepts training and test datasets, along with hyperparameters for training.\n2. **Label Mapping**: It checks for two distinct labels in `y_train` and maps them to `{0, 1}` for internal processing.\n3. **Intercept Term**: An intercept term (bias) is added to both training and test datasets.\n4. **Training**: The weights are initialized to zero, and mini-batch gradient descent is performed over the specified number of epochs. The weights are updated based on the gradient of the loss function.\n5. **Prediction**: After training, predictions are made on the test set, and the results are mapped back to the original labels.\n6. **Special Case Handling**: If all training labels are identical, the function directly predicts that label for all test samples.\n\nThis implementation should work well for linearly separable data and provide reasonable predictions on simple toy problems."}
{"task_id": 176, "completion_id": 0, "completion": "Here's a complete implementation of the AdaBoost algorithm using decision stumps as weak learners. The code follows the specifications provided in the task description.\n\n```python\nimport numpy as np\n\nclass DecisionStump:\n    def __init__(self):\n        self.feature_index = None\n        self.threshold = None\n        self.polarity = None\n        self.alpha = None\n\n    def fit(self, X, y, sample_weights):\n        n_samples, n_features = X.shape\n        min_error = float('inf')\n\n        for feature_index in range(n_features):\n            thresholds = np.unique(X[:, feature_index])\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    predictions = np.ones(n_samples)\n                    predictions[polarity * X[:, feature_index] < polarity * threshold] = 0\n\n                    error = np.sum(sample_weights * (predictions != y))\n\n                    if error < min_error:\n                        min_error = error\n                        self.alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n                        self.feature_index = feature_index\n                        self.threshold = threshold\n                        self.polarity = polarity\n\n    def predict(self, X):\n        n_samples = X.shape[0]\n        predictions = np.ones(n_samples)\n        if self.polarity == 1:\n            predictions[X[:, self.feature_index] < self.threshold] = 0\n        else:\n            predictions[X[:, self.feature_index] >= self.threshold] = 0\n        return predictions\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n\n    n_samples = X_train.shape[0]\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    classifier_weights = []\n\n    # Convert y_train from {0, 1} to {-1, 1}\n    y_train = np.where(y_train == 0, -1, 1)\n\n    for _ in range(n_estimators):\n        stump = DecisionStump()\n        stump.fit(X_train, y_train, sample_weights)\n        predictions = stump.predict(X_train)\n\n        # Calculate the weighted error\n        error = np.sum(sample_weights * (predictions != y_train))\n\n        # If error is 0, we can stop early\n        if error == 0:\n            break\n\n        # Update the sample weights\n        sample_weights *= np.exp(-stump.alpha * y_train * predictions)\n        sample_weights /= np.sum(sample_weights)  # Normalize\n\n        classifiers.append(stump)\n        classifier_weights.append(stump.alpha)\n\n    # Make predictions on the test set\n    final_predictions = np.zeros(X_test.shape[0])\n    for stump, alpha in zip(classifiers, classifier_weights):\n        final_predictions += alpha * stump.predict(X_test)\n\n    # Convert to class labels {0, 1}\n    return [1 if pred > 0 else 0 for pred in final_predictions]\n\n# Example usage:\n# X_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\n# y_train = [0, 0, 1, 1]\n# X_test = [[1, 2], [3, 3]]\n# predictions = adaboost_predict(X_train, y_train, X_test, n_estimators=10)\n# print(predictions)  # Output: [0, 1]\n```\n\n### Explanation:\n1. **DecisionStump Class**: This class implements a decision stump. It has methods to fit the stump to the data and make predictions.\n2. **adaboost_predict Function**: This function implements the AdaBoost algorithm. It initializes sample weights, trains decision stumps, updates weights based on the errors, and aggregates the predictions from all stumps to make final predictions.\n3. **Weight Update**: The sample weights are updated according to the AdaBoost rules, and they are normalized after each iteration.\n4. **Final Predictions**: The predictions from all stumps are combined using their respective weights, and the final class labels are determined based on the sign of the aggregated predictions.\n\nThis implementation adheres to the requirements specified in the task description and should work correctly for the given input format."}
{"task_id": 178, "completion_id": 0, "completion": "Here's a complete implementation of the Particle Swarm Optimization (PSO) algorithm to minimize the Sphere function as described in your task. The code follows the specifications provided, including input validation and the use of a random number generator with a specified seed.\n\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Input validation\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n\n    # Initialize random number generator\n    rng = np.random.default_rng(seed)\n\n    # PSO parameters\n    w = 0.5  # inertia weight\n    c1 = 1.5  # cognitive weight\n    c2 = 1.5  # social weight\n    lower_bound = -1\n    upper_bound = 1\n\n    # Initialize particles\n    positions = rng.uniform(lower_bound, upper_bound, (num_particles, n_dims))\n    velocities = rng.uniform(lower_bound, upper_bound, (num_particles, n_dims))\n    personal_best_positions = np.copy(positions)\n    personal_best_values = np.array([sphere_function(pos) for pos in positions])\n    \n    # Global best initialization\n    global_best_index = np.argmin(personal_best_values)\n    global_best_position = personal_best_positions[global_best_index]\n    global_best_value = personal_best_values[global_best_index]\n\n    # Main PSO loop\n    for _ in range(num_iterations):\n        r1 = rng.random(num_particles)\n        r2 = rng.random(num_particles)\n\n        # Update velocities\n        velocities = (w * velocities +\n                      c1 * r1[:, np.newaxis] * (personal_best_positions - positions) +\n                      c2 * r2[:, np.newaxis] * (global_best_position - positions))\n\n        # Update positions\n        positions += velocities\n        positions = np.clip(positions, lower_bound, upper_bound)\n\n        # Update personal bests\n        current_values = np.array([sphere_function(pos) for pos in positions])\n        better_mask = current_values < personal_best_values\n        personal_best_positions[better_mask] = positions[better_mask]\n        personal_best_values[better_mask] = current_values[better_mask]\n\n        # Update global best\n        current_global_best_index = np.argmin(personal_best_values)\n        current_global_best_value = personal_best_values[current_global_best_index]\n        if current_global_best_value < global_best_value:\n            global_best_value = current_global_best_value\n            global_best_position = personal_best_positions[current_global_best_index]\n\n    return round(global_best_value, 4)\n\ndef sphere_function(x):\n    \"\"\"Calculates the Sphere function value for a given input vector.\"\"\"\n    return np.sum(x ** 2)\n\n# Example usage\nif __name__ == \"__main__\":\n    best_value = particle_swarm_optimisation(5, 30, 100, seed=42)\n    print(best_value)\n```\n\n### Explanation of the Code:\n1. **Input Validation**: The function checks if the input parameters are positive integers. If not, it returns -1.\n2. **Initialization**: The positions and velocities of the particles are initialized randomly within the bounds of -1 and 1. The personal best positions and values are also initialized.\n3. **Main PSO Loop**: For each iteration, the velocities and positions of the particles are updated based on the PSO equations. The personal bests and global best are updated accordingly.\n4. **Sphere Function**: A helper function `sphere_function` computes the value of the Sphere function for a given position.\n5. **Return Value**: The best value found is rounded to four decimal places before being returned.\n\nYou can run the example usage at the bottom to see how the function works. Adjust the parameters as needed to test different scenarios."}
{"task_id": 180, "completion_id": 0, "completion": "Here's a complete implementation of the k-Nearest Neighbours (k-NN) algorithm as specified in your task. The function handles both classification and regression tasks, calculates Euclidean distances, and returns predictions based on the specified parameters.\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    \n    # Ensure k is within the valid range\n    if k <= 0 or k > len(X_train):\n        k = len(X_train)\n\n    predictions = []\n\n    for test_point in X_test:\n        # Calculate Euclidean distances from the test point to all training points\n        distances = np.linalg.norm(np.array(X_train) - np.array(test_point), axis=1)\n        \n        # Get the indices of the k nearest neighbors\n        k_indices = np.argsort(distances)[:k]\n        \n        # Get the labels of the k nearest neighbors\n        k_nearest_labels = [y_train[i] for i in k_indices]\n\n        if task == \"classification\":\n            # Majority vote\n            most_common = Counter(k_nearest_labels).most_common()\n            max_count = most_common[0][1]\n            # In case of a tie, return the smallest label\n            candidates = [label for label, count in most_common if count == max_count]\n            prediction = min(candidates)\n        \n        elif task == \"regression\":\n            # Average of the neighbors' target values\n            prediction = round(np.mean(k_nearest_labels), 4)\n        \n        predictions.append(prediction)\n\n    return predictions\n```\n\n### Explanation:\n1. **Input Parameters**:\n   - `X_train`: A list of training feature vectors.\n   - `y_train`: A list of target values corresponding to the training data.\n   - `X_test`: A list of feature vectors for which predictions are to be made.\n   - `k`: The number of nearest neighbors to consider.\n   - `task`: Specifies whether to perform classification or regression.\n\n2. **Distance Calculation**:\n   - The Euclidean distance is calculated using NumPy's `linalg.norm` function, which computes the distance between each test point and all training points.\n\n3. **Finding Neighbors**:\n   - The indices of the `k` nearest neighbors are found using `np.argsort`, which sorts the distances and retrieves the indices of the smallest distances.\n\n4. **Prediction Logic**:\n   - For classification, the most common label among the neighbors is determined using `Counter`. In case of a tie, the smallest label is returned.\n   - For regression, the mean of the neighbors' target values is calculated and rounded to four decimal places.\n\n5. **Output**:\n   - The function returns a list of predictions corresponding to each test point, preserving the order of the test samples. \n\nThis implementation is efficient and adheres to the requirements specified in your task."}
{"task_id": 184, "completion_id": 0, "completion": "To implement a binary decision-tree classifier using the CART algorithm from scratch, we will follow the steps outlined in the task description. The implementation will include functions for calculating Gini impurity, finding the best split, recursively building the tree, and making predictions.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\nclass Node:\n    def __init__(self, gini=None, num_samples=None, predicted_class=None, \n                 feature_index=None, threshold=None, left=None, right=None):\n        self.gini = gini\n        self.num_samples = num_samples\n        self.predicted_class = predicted_class\n        self.feature_index = feature_index\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n\ndef gini_impurity(y):\n    \"\"\"Calculate the Gini impurity for a list of classes.\"\"\"\n    m = len(y)\n    if m == 0:\n        return 0\n    class_counts = np.bincount(y)\n    probabilities = class_counts / m\n    return 1 - np.sum(probabilities ** 2)\n\ndef best_split(X, y):\n    \"\"\"Find the best split for the dataset.\"\"\"\n    m, n = X.shape\n    best_gini = float('inf')\n    best_idx, best_thr = None, None\n    parent_gini = gini_impurity(y)\n\n    for feature_index in range(n):\n        thresholds, classes = zip(*sorted(zip(X[:, feature_index], y)))\n        num_left = [0] * len(np.unique(y))\n        num_right = class_counts = np.bincount(classes)\n        total_samples = len(y)\n\n        for i in range(1, total_samples):  # Avoid splitting at the last index\n            c = classes[i - 1]\n            num_left[c] += 1\n            num_right[c] -= 1\n\n            if thresholds[i] == thresholds[i - 1]:\n                continue  # Skip if the threshold is the same\n\n            gini_left = gini_impurity(num_left)\n            gini_right = gini_impurity(num_right)\n            gini = (i * gini_left + (total_samples - i) * gini_right) / total_samples\n\n            if gini < best_gini:\n                best_gini = gini\n                best_idx = feature_index\n                best_thr = (thresholds[i] + thresholds[i - 1]) / 2  # Midpoint\n\n    return best_idx, best_thr, parent_gini - best_gini\n\ndef build_tree(X, y, depth=0, max_depth=None):\n    \"\"\"Recursively build the decision tree.\"\"\"\n    num_samples_per_class = [np.sum(y == i) for i in range(np.max(y) + 1)]\n    predicted_class = np.argmax(num_samples_per_class)\n    node = Node(predicted_class=predicted_class, num_samples=len(y), gini=gini_impurity(y))\n\n    # Stopping criteria\n    if len(set(y)) == 1 or (max_depth is not None and depth >= max_depth):\n        return node\n\n    feature_index, threshold, gain = best_split(X, y)\n\n    if gain == 0:\n        return node\n\n    indices_left = X[:, feature_index] < threshold\n    X_left, y_left = X[indices_left], y[indices_left]\n    X_right, y_right = X[~indices_left], y[~indices_left]\n\n    node.feature_index = feature_index\n    node.threshold = threshold\n    node.left = build_tree(X_left, y_left, depth + 1, max_depth)\n    node.right = build_tree(X_right, y_right, depth + 1, max_depth)\n\n    return node\n\ndef predict_sample(node, sample):\n    \"\"\"Predict the class for a single sample.\"\"\"\n    while node.left:  # Traverse the tree\n        if sample[node.feature_index] < node.threshold:\n            node = node.left\n        else:\n            node = node.right\n    return node.predicted_class\n\ndef decision_tree_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_depth: int | None = None) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\"\"\"\n    tree = build_tree(X_train, y_train, max_depth=max_depth)\n    predictions = [predict_sample(tree, sample) for sample in X_test]\n    return predictions\n```\n\n### Explanation of the Code:\n1. **Node Class**: Represents a node in the decision tree, storing information about the Gini impurity, number of samples, predicted class, feature index, threshold, and child nodes.\n\n2. **Gini Impurity Function**: Computes the Gini impurity for a given set of class labels.\n\n3. **Best Split Function**: Iterates through each feature and finds the best threshold that maximizes the Gini gain.\n\n4. **Build Tree Function**: Recursively builds the decision tree based on the stopping criteria and the best splits found.\n\n5. **Predict Sample Function**: Traverses the tree to predict the class for a single sample.\n\n6. **Decision Tree Predict Function**: Combines everything to build the tree from the training data and predict the classes for the test data.\n\nThis implementation adheres to the requirements specified in the task and should work correctly for binary classification tasks."}
{"task_id": 190, "completion_id": 0, "completion": "To implement the function `best_gini_split`, we will follow these steps:\n\n1. **Calculate the Gini impurity for the entire dataset**.\n2. **Iterate through each feature** and for each feature, find all unique values to consider as potential split thresholds.\n3. **For each unique value**, split the dataset into left and right subsets based on the threshold and calculate the Gini impurity for both subsets.\n4. **Compute the weighted Gini impurity** for the split and keep track of the best split found.\n5. **Return the best feature index, threshold, and Gini impurity** rounded to four decimal places, or return the specified tuple if no valid split exists.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef gini_impurity(y):\n    \"\"\"Calculate the Gini impurity for a given array of class labels.\"\"\"\n    if len(y) == 0:\n        return 0.0\n    class_labels, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    return 1 - np.sum(probabilities ** 2)\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\n\n    Parameters\n    ----------\n    X : list[list[float]] or numpy.ndarray\n        A 2-D structure where each inner list/row contains the numerical\n        feature values of one sample.\n    y : list[int] or numpy.ndarray\n        A 1-D structure containing the class labels corresponding to the rows\n        of X.\n\n    Returns\n    -------\n    tuple\n        A tuple `(best_feature_index, best_threshold_value, best_gini)` where\n        `best_gini` is rounded to 4 decimal places.  If no valid split exists\n        the function must return `(-1, None, round(G_whole_dataset, 4))`.\n    \"\"\"\n    n_samples, n_features = X.shape\n    G_whole_dataset = gini_impurity(y)\n    \n    best_feature_index = -1\n    best_threshold_value = None\n    best_gini = float('inf')\n    \n    for j in range(n_features):\n        # Get unique values for the feature\n        unique_values = np.unique(X[:, j])\n        \n        for v in unique_values:\n            # Create left and right splits\n            left_indices = X[:, j] <= v\n            right_indices = X[:, j] > v\n            \n            if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n                continue  # Skip if either side is empty\n            \n            left_y = y[left_indices]\n            right_y = y[right_indices]\n            \n            # Calculate Gini impurity for left and right\n            G_left = gini_impurity(left_y)\n            G_right = gini_impurity(right_y)\n            \n            # Calculate weighted Gini impurity for the split\n            n_left = len(left_y)\n            n_right = len(right_y)\n            G_split = (n_left / n_samples) * G_left + (n_right / n_samples) * G_right\n            \n            # Check if this is the best split found so far\n            if (G_split < best_gini or\n                (abs(G_split - best_gini) < 1e-12 and j < best_feature_index) or\n                (abs(G_split - best_gini) < 1e-12 and j == best_feature_index and v < best_threshold_value)):\n                best_gini = G_split\n                best_feature_index = j\n                best_threshold_value = v\n    \n    if best_feature_index == -1:\n        return (-1, None, round(G_whole_dataset, 4))\n    \n    return (best_feature_index, best_threshold_value, round(best_gini, 4))\n```\n\n### Explanation of the Code:\n- **Gini Impurity Calculation**: The `gini_impurity` function computes the Gini impurity for a given set of class labels.\n- **Main Function**: The `best_gini_split` function iterates through each feature and its unique values to find the best split that minimizes the weighted Gini impurity.\n- **Conditions for Best Split**: The function checks for the best split based on the defined tie-breaking rules.\n- **Return Value**: If no valid split is found, it returns the specified tuple with the Gini impurity of the whole dataset. Otherwise, it returns the best feature index, threshold, and Gini impurity rounded to four decimal places."}
{"task_id": 191, "completion_id": 0, "completion": "To implement the distance metric factory as described, we will create a function `metric` that returns a callable distance function based on the specified metric name. This callable will validate the input vectors and compute the appropriate distance if the inputs are valid. If any validation fails or if the metric name is unsupported, it will return -1.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n    \n    def validate_vectors(x, y):\n        \"\"\"Validate that x and y are one-dimensional, non-empty, equal-length numeric iterables.\"\"\"\n        if not (isinstance(x, (list, tuple, np.ndarray)) and isinstance(y, (list, tuple, np.ndarray))):\n            return False\n        if len(x) != len(y) or len(x) == 0:\n            return False\n        try:\n            x = np.asarray(x, dtype=float)\n            y = np.asarray(y, dtype=float)\n        except ValueError:\n            return False\n        if x.ndim != 1 or y.ndim != 1:\n            return False\n        return x, y\n\n    def euclidean(x, y):\n        return np.sqrt(np.sum((x - y) ** 2))\n\n    def manhattan(x, y):\n        return np.sum(np.abs(x - y))\n\n    def chebyshev(x, y):\n        return np.max(np.abs(x - y))\n\n    def cosine(x, y):\n        norm_x = np.linalg.norm(x)\n        norm_y = np.linalg.norm(y)\n        if norm_x == 0 or norm_y == 0:\n            return -1\n        return 1 - np.dot(x, y) / (norm_x * norm_y)\n\n    def distance_function(x, y):\n        # Validate the input vectors\n        validated = validate_vectors(x, y)\n        if validated is False:\n            return -1\n        x, y = validated\n        \n        # Compute the distance based on the metric name\n        if name == 'euclidean':\n            return round(euclidean(x, y), 4)\n        elif name == 'manhattan':\n            return round(manhattan(x, y), 4)\n        elif name == 'chebyshev':\n            return round(chebyshev(x, y), 4)\n        elif name == 'cosine':\n            return round(cosine(x, y), 4)\n        else:\n            return -1\n\n    return distance_function\n```\n\n### Explanation:\n1. **Validation Function**: The `validate_vectors` function checks if the inputs are valid one-dimensional numeric iterables of the same length and non-empty. It also converts them to NumPy arrays for easier mathematical operations.\n\n2. **Distance Functions**: We define separate functions for each distance metric:\n   - **Euclidean**: Uses the formula for Euclidean distance.\n   - **Manhattan**: Computes the sum of absolute differences.\n   - **Chebyshev**: Finds the maximum absolute difference.\n   - **Cosine**: Calculates the cosine distance, ensuring that neither vector has a zero norm.\n\n3. **Distance Callable**: The `distance_function` checks the validity of the input vectors and computes the distance based on the specified metric name. It rounds the result to four decimal places.\n\n4. **Return**: The `metric` function returns the `distance_function`, which can be called with two vectors to compute the desired distance.\n\n### Usage Example:\n```python\neuclidean_distance = metric('euclidean')\nresult = euclidean_distance([1, 2, 3], [4, 5, 6])\nprint(result)  # Output: 5.1962\n``` \n\nThis implementation meets all the requirements specified in the task description."}
{"task_id": 197, "completion_id": 0, "completion": "Here's a complete implementation of the `action_stats` function that meets the specified requirements:\n\n```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if the action space is continuous (infinite number of actions).\n\n    Returns:\n        A tuple ``(n_actions_per_dim, action_ids, action_dim)`` where\n            \u2022 n_actions_per_dim: list with the number of actions in every dimension\n                                  (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product) or ``None`` if\n                           any dimension is continuous,\n            \u2022 action_dim: number of action dimensions.\n    \"\"\"\n    if md_action:\n        # Multi-dimensional action space\n        if cont_action:\n            # If it's multi-dimensional and continuous\n            n_actions_per_dim = [np.inf] * len(env.action_space.spaces)\n            action_ids = None\n        else:\n            # If it's multi-dimensional and discrete\n            n_actions_per_dim = []\n            action_ranges = []\n            for space in env.action_space.spaces:\n                if hasattr(space, 'n'):\n                    n_actions_per_dim.append(space.n)\n                    action_ranges.append(range(space.n))\n                elif hasattr(space, 'shape'):\n                    n_actions_per_dim.append(np.inf)\n                    action_ranges.append(None)  # No range for continuous\n                    action_ids = None\n                    break\n            else:\n                # If we didn't break, it means all dimensions are discrete\n                action_ids = list(product(*action_ranges))\n    else:\n        # Single-dimensional action space\n        if cont_action:\n            n_actions_per_dim = [np.inf]\n            action_ids = None\n        else:\n            n_actions_per_dim = [env.action_space.n]\n            action_ids = list(range(env.action_space.n))\n\n    action_dim = len(n_actions_per_dim)\n    \n    return n_actions_per_dim, action_ids, action_dim\n```\n\n### Explanation of the Code:\n1. **Imports**: We import necessary modules such as `numpy` for handling infinite values and `product` from `itertools` for generating Cartesian products.\n\n2. **Function Definition**: The function `action_stats` takes an environment object `env`, and two boolean flags `md_action` and `cont_action`.\n\n3. **Multi-dimensional Action Space**:\n   - If `md_action` is `True`, we check if the action space is continuous.\n   - If it is continuous, we set `n_actions_per_dim` to a list of `np.inf` and `action_ids` to `None`.\n   - If it is discrete, we iterate through each sub-space in `env.action_space.spaces`, checking if it has `n` (discrete) or `shape` (continuous). We build `n_actions_per_dim` and `action_ranges` accordingly.\n   - If all dimensions are discrete, we compute the Cartesian product of the action ranges to get `action_ids`.\n\n4. **Single-dimensional Action Space**:\n   - If `md_action` is `False`, we check if the action space is continuous.\n   - If it is continuous, we set `n_actions_per_dim` to `[np.inf]` and `action_ids` to `None`.\n   - If it is discrete, we set `n_actions_per_dim` to `[env.action_space.n]` and generate `action_ids` as a list of valid actions.\n\n5. **Return Values**: Finally, we return the three required values: `n_actions_per_dim`, `action_ids`, and `action_dim`.\n\nThis implementation should work correctly for environments that follow the OpenAI Gym structure for action spaces."}
{"task_id": 198, "completion_id": 0, "completion": "To implement the \u03b2-maximization step in Latent Dirichlet Allocation (LDA), we need to follow the mathematical formulation provided. The goal is to compute the word-topic matrix \u03b2 based on the variational parameters \u03c6 and the corpus of documents.\n\nHere's how we can achieve this step-by-step:\n\n1. **Initialize the \u03b2 matrix**: Create a matrix of zeros with dimensions (V, T) where V is the vocabulary size and T is the number of topics.\n\n2. **Compute the counts**: For each document, iterate through its tokens and update the counts in the \u03b2 matrix based on the variational parameters \u03c6.\n\n3. **Normalize the columns**: After accumulating the counts, normalize each column of the \u03b2 matrix so that the sum of each column equals 1.\n\n4. **Handle zero rows**: If a word index never occurs in the corpus, ensure that its corresponding row in \u03b2 contains only zeros.\n\n5. **Round the values**: Finally, round the values in the \u03b2 matrix to four decimal places.\n\nHere is the implementation of the above steps:\n\n```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    # Number of topics\n    T = phi[0].shape[1]\n    \n    # Initialize the beta matrix with zeros\n    beta = np.zeros((V, T))\n    \n    # Accumulate counts for each word and topic\n    for d, document in enumerate(corpus):\n        for n, word in enumerate(document):\n            beta[word] += phi[d][n]\n    \n    # Normalize the columns of beta\n    for t in range(T):\n        column_sum = np.sum(beta[:, t])\n        if column_sum > 0:\n            beta[:, t] /= column_sum\n    \n    # Convert to a list of lists and round to 4 decimal places\n    beta_rounded = beta.round(4).tolist()\n    \n    return beta_rounded\n```\n\n### Explanation of the Code:\n- We first determine the number of topics `T` from the shape of the first element of `phi`.\n- We initialize a zero matrix `beta` of shape (V, T).\n- We iterate through each document and each word in the document, updating the `beta` matrix using the corresponding values from `phi`.\n- After accumulating the counts, we normalize each column of `beta` by dividing by the sum of that column.\n- Finally, we round the values to four decimal places and convert the NumPy array to a list of lists before returning it.\n\nThis implementation ensures that the resulting \u03b2 matrix meets the specified requirements, including normalization and rounding."}
{"task_id": 202, "completion_id": 0, "completion": "Here's a complete implementation of the Partitioning Around Medoids (PAM) clustering algorithm in Python, following the specifications you provided:\n\n```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    The function must follow these rules:\n    1. Use the first *k* samples of *X* as the initial medoids.\n    2. Repeatedly try all possible single swaps between a medoid and a non-\n       medoid, accepting the swap that strictly reduces the total cost the\n       most.  Stop when no swap can further decrease the cost.\n    3. After convergence label every sample with the index (0 \u2026 k\u22121) of the\n       closest medoid (ties are broken by the medoid that appears first in the\n       current medoid list).\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features).\n        k: Number of clusters to form.\n\n    Returns:\n        A Python list of length *n_samples* containing integer cluster labels.\n    \"\"\"\n    \n    n_samples = X.shape[0]\n    \n    # Step 1: Initialize medoids as the first k samples\n    medoids = list(range(k))\n    \n    def compute_cost():\n        \"\"\"Compute the total cost of the current medoids.\"\"\"\n        total_cost = 0\n        for i in range(n_samples):\n            # Find the closest medoid\n            closest_medoid = np.argmin([np.linalg.norm(X[i] - X[m]) for m in medoids])\n            total_cost += np.linalg.norm(X[i] - X[medoids[closest_medoid]])\n        return total_cost\n    \n    current_cost = compute_cost()\n    \n    while True:\n        best_cost = current_cost\n        best_swap = None\n        \n        # Step 2: Try all possible swaps\n        for m in medoids:\n            for j in range(n_samples):\n                if j not in medoids:  # Only consider non-medoids\n                    # Swap medoid m with non-medoids j\n                    new_medoids = [medoid if medoid != m else j for medoid in medoids]\n                    # Compute new cost\n                    new_cost = 0\n                    for i in range(n_samples):\n                        closest_medoid = np.argmin([np.linalg.norm(X[i] - X[medoid]) for medoid in new_medoids])\n                        new_cost += np.linalg.norm(X[i] - X[new_medoids[closest_medoid]])\n                    \n                    # Check if this swap is better\n                    if new_cost < best_cost:\n                        best_cost = new_cost\n                        best_swap = (m, j)\n        \n        # If no swap improves the cost, we are done\n        if best_swap is None:\n            break\n        \n        # Perform the best swap\n        m, j = best_swap\n        medoids[medoids.index(m)] = j\n        current_cost = best_cost\n    \n    # Step 3: Assign labels based on closest medoid\n    labels = []\n    for i in range(n_samples):\n        closest_medoid = np.argmin([np.linalg.norm(X[i] - X[m]) for m in medoids])\n        labels.append(closest_medoid)\n    \n    return labels\n\n# Example usage\nX = np.array([[0, 0], [1, 0], [0, 1], [5, 5], [6, 5], [5, 6]])\nk = 2\nprint(pam_clustering(X, k))  # Output: [0, 0, 0, 1, 1, 1]\n```\n\n### Explanation:\n1. **Initialization**: The medoids are initialized as the first `k` samples from the dataset.\n2. **Cost Calculation**: The total cost is calculated as the sum of distances from each sample to its closest medoid.\n3. **Swapping Logic**: The algorithm iterates through all possible swaps between current medoids and non-medoids, calculating the new cost for each swap. If a swap results in a lower cost, it is accepted.\n4. **Label Assignment**: After convergence, each sample is assigned to the closest medoid, with ties broken by the order of medoids.\n\nThis implementation ensures that the algorithm is deterministic and follows the specified steps closely."}
{"task_id": 216, "completion_id": 0, "completion": "Here's a complete implementation of the `elastic_net_regression` function as per your requirements. This function includes polynomial feature generation, standard scoring, and the Elastic Net regression using batch gradient descent.\n\n```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic Net regression implemented with batch gradient descent.\n\n    Parameters\n    ----------\n    x_train : list[list[float]] | np.ndarray\n        Training feature matrix where each sub-list is a sample.\n    y_train : list[float] | np.ndarray\n        Target values for every row in `x_train`.\n    x_test : list[list[float]] | np.ndarray\n        Matrix of samples to predict after training.\n    degree : int, default 1\n        Degree of polynomial expansion applied to every original feature.\n    reg_factor : float, default 0.05\n        Overall regularisation strength (alpha).\n    l1_ratio : float, default 0.5\n        Portion of L1 penalty in Elastic Net (0 = pure ridge, 1 = pure lasso).\n    n_iterations : int, default 3000\n        Number of gradient descent iterations.\n    learning_rate : float, default 0.01\n        Step size used in each gradient update.\n\n    Returns\n    -------\n    list[float]\n        Predictions for `x_test` rounded to 4 decimals. Returns `-1` if the\n        inputs are invalid (different feature counts, or non-positive\n        `n_iterations`).\n    \"\"\"\n    \n    # Convert inputs to numpy arrays\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_test = np.array(x_test)\n\n    # Validate input shapes\n    if x_train.shape[0] != y_train.shape[0] or x_train.shape[1] == 0 or x_test.shape[1] != x_train.shape[1]:\n        return -1\n    if n_iterations < 1:\n        return -1\n\n    # Generate polynomial features\n    def polynomial_features(X, degree):\n        from sklearn.preprocessing import PolynomialFeatures\n        poly = PolynomialFeatures(degree=degree, include_bias=True)\n        return poly.fit_transform(X)\n\n    x_train_poly = polynomial_features(x_train, degree)\n    x_test_poly = polynomial_features(x_test, degree)\n\n    # Standard score the features (excluding the bias term)\n    mean = np.mean(x_train_poly[:, 1:], axis=0)\n    std = np.std(x_train_poly[:, 1:], axis=0)\n    x_train_poly[:, 1:] = (x_train_poly[:, 1:] - mean) / std\n    x_test_poly[:, 1:] = (x_test_poly[:, 1:] - mean) / std\n\n    # Initialize weights\n    weights = np.zeros(x_train_poly.shape[1])\n\n    # Gradient descent\n    for _ in range(n_iterations):\n        predictions = x_train_poly.dot(weights)\n        errors = predictions - y_train\n        \n        # Compute gradients\n        gradient = (2 / len(y_train)) * x_train_poly.T.dot(errors)\n        \n        # Regularization terms\n        l2_penalty = reg_factor * (1 - l1_ratio) * weights\n        l1_penalty = reg_factor * l1_ratio * np.sign(weights)\n        \n        # Update weights\n        weights[1:] -= learning_rate * (gradient[1:] + l2_penalty[1:] + l1_penalty[1:])\n        weights[0] -= learning_rate * gradient[0]  # Bias term is not regularized\n\n    # Make predictions on the test set\n    test_predictions = x_test_poly.dot(weights)\n    \n    return np.round(test_predictions.tolist(), 4)\n\n# Example usage:\n# x_train = [[1, 2], [2, 3], [3, 4]]\n# y_train = [1, 2, 3]\n# x_test = [[1, 2], [2, 3]]\n# print(elastic_net_regression(x_train, y_train, x_test, degree=2))\n```\n\n### Explanation:\n1. **Input Validation**: The function checks if the shapes of `x_train`, `y_train`, and `x_test` are compatible and if `n_iterations` is positive.\n2. **Polynomial Features**: It generates polynomial features using `PolynomialFeatures` from `sklearn`.\n3. **Standard Scoring**: The features (excluding the bias term) are standardized to have zero mean and unit variance.\n4. **Gradient Descent**: The weights are updated using the gradients calculated from the mean squared error and the Elastic Net regularization terms.\n5. **Predictions**: Finally, predictions are made on the test set and rounded to four decimal places before returning.\n\nThis implementation should meet all the specified requirements for Elastic Net regression."}
{"task_id": 217, "completion_id": 0, "completion": "Here's the implementation of the `logistic_components` function that computes the gradient, Hessian, and sigmoid probability for logistic loss based on the provided definitions. The function handles input as lists, tuples, or NumPy arrays, and ensures that the output is rounded to six decimal places.\n\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized computation\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n\n    # Calculate the sigmoid function\n    sigmoid = 1 / (1 + np.exp(-predicted))\n    \n    # Calculate the gradient\n    gradient = actual * sigmoid * (1 - sigmoid)\n    \n    # Calculate the Hessian\n    hessian = sigmoid * (1 - sigmoid)\n    \n    # Round results to 6 decimal places\n    gradient_rounded = np.round(gradient, 6).tolist()\n    hessian_rounded = np.round(hessian, 6).tolist()\n    probability_rounded = np.round(sigmoid, 6).tolist()\n\n    return (gradient_rounded, hessian_rounded, probability_rounded)\n\n# Example usage:\n# actual = [1, -1, 1]\n# predicted = [0.5, -0.5, 1.0]\n# result = logistic_components(actual, predicted)\n# print(result)\n```\n\n### Explanation:\n1. **Input Conversion**: The function first converts the input lists or tuples into NumPy arrays for efficient vectorized operations.\n2. **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} \\).\n3. **Gradient Calculation**: The gradient is calculated using the formula \\( \\text{grad}_i = \\text{actual}_i \\cdot \\text{sigmoid}(-\\text{actual}_i \\cdot \\text{predicted}_i) \\).\n4. **Hessian Calculation**: The Hessian is computed using the formula \\( \\text{hess}_i = \\text{sigmoid}(\\text{predicted}_i) \\cdot (1 - \\text{sigmoid}(\\text{predicted}_i)) \\).\n5. **Rounding**: All results are rounded to six decimal places and converted back to Python lists before returning.\n\nThis implementation is efficient and adheres to the requirements specified in the task."}
{"task_id": 218, "completion_id": 0, "completion": "To implement the `blackjack_outcome` function, we need to follow the rules outlined in the task description. The function will evaluate the player's and dealer's hands, calculate their scores, and determine the outcome based on the specified rules.\n\nHere's how we can implement this:\n\n1. **Calculate the hand value**: We need a helper function to calculate the score of a hand, taking into account the special rules for Aces.\n2. **Determine if a hand is a bust**: A hand is considered a bust if its score exceeds 21.\n3. **Evaluate the outcome**: Based on the scores of the player and dealer, we will determine the reward according to the rules provided.\n\nHere's the complete implementation:\n\n```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n    \n    def calculate_hand_value(hand: list[int]) -> int:\n        \"\"\"Calculate the value of a Blackjack hand.\"\"\"\n        total = sum(hand)\n        aces_count = hand.count(1)\n        \n        # Adjust for usable Aces\n        while aces_count > 0 and total + 10 <= 21:\n            total += 10\n            aces_count -= 1\n        \n        return total\n\n    player_score = calculate_hand_value(player)\n    dealer_score = calculate_hand_value(dealer)\n\n    player_bust = player_score > 21\n    dealer_bust = dealer_score > 21\n\n    if player_bust:\n        return -1  # Player loses\n    if dealer_bust:\n        return 1  # Player wins\n\n    # Check for natural blackjack\n    player_natural_blackjack = (len(player) == 2 and player_score == 21)\n    dealer_natural_blackjack = (len(dealer) == 2 and dealer_score == 21)\n\n    if player_score > dealer_score:\n        if player_natural_blackjack:\n            return 1.5  # Player wins with natural blackjack\n        return 1  # Player wins\n    elif player_score < dealer_score:\n        return -1  # Dealer wins\n    else:\n        return 0  # Draw\n```\n\n### Explanation of the Code:\n- **calculate_hand_value**: This helper function computes the total value of a hand. It sums the values of the cards and adjusts for Aces if possible (counting one Ace as 11 instead of 1 if it keeps the total \u2264 21).\n- **Player and Dealer Scores**: We calculate the scores for both the player and dealer using the helper function.\n- **Bust Check**: We check if either the player or dealer has busted.\n- **Natural Blackjack Check**: We check if either player has a natural blackjack (two cards totaling 21).\n- **Outcome Evaluation**: Based on the scores and the bust conditions, we return the appropriate reward.\n\nThis implementation adheres to the rules provided and should work correctly for any valid input."}
{"task_id": 221, "completion_id": 0, "completion": "Here's the implementation of the Mean Squared Logarithmic Error (MSLE) as per your requirements. The code includes the helper function `squared_log_error` and the main function `mean_squared_log_error`, along with input validation.\n\n```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"Calculate element-wise squared logarithmic errors.\"\"\"\n    # Calculate the squared log error\n    log_actual = np.log1p(actual)  # log(1 + actual)\n    log_predicted = np.log1p(predicted)  # log(1 + predicted)\n    return (log_actual - log_predicted) ** 2\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence of true values. All elements must be non-negative.\n    predicted : list | tuple | np.ndarray\n        Sequence of predicted values. Must be the same length as\n        `actual` and contain only non-negative numbers.\n\n    Returns\n    -------\n    float\n        The MSLE rounded to 4 decimals. If the inputs are invalid the\n        function returns -1.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    actual = np.asarray(actual)\n    predicted = np.asarray(predicted)\n\n    # Input validation\n    if actual.shape != predicted.shape or np.any(actual < 0) or np.any(predicted < 0):\n        return -1\n\n    # Calculate squared log errors\n    squared_errors = squared_log_error(actual, predicted)\n\n    # Calculate mean of squared errors and round to 4 decimal places\n    msle = np.mean(squared_errors)\n    return round(msle, 4)\n\n# Example usage:\n# actual = [3, 5, 2.5, 7]\n# predicted = [2.5, 5, 4, 8]\n# print(mean_squared_log_error(actual, predicted))  # Example output\n```\n\n### Explanation:\n1. **Input Validation**: The function checks if the shapes of `actual` and `predicted` are the same and ensures that all values are non-negative. If any of these checks fail, it returns `-1`.\n2. **Logarithmic Calculation**: The `squared_log_error` function computes the squared logarithmic error using `np.log1p`, which computes `log(1 + x)` efficiently and handles the case when `x` is zero.\n3. **Mean Calculation**: The main function calculates the mean of the squared errors and rounds the result to four decimal places before returning it.\n\nYou can test the implementation with various inputs to ensure it behaves as expected."}
{"task_id": 222, "completion_id": 0, "completion": "Here's a complete implementation of the sigmoid activation function that meets all the specified requirements:\n\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    # Convert input to a NumPy array for vectorized operations\n    z = np.asarray(z)\n\n    # Compute the sigmoid in a numerically stable way\n    # Using the fact that sigmoid(z) = 1 / (1 + exp(-z))\n    # We handle large positive and negative values separately\n    with np.errstate(over='ignore'):\n        result = np.where(z >= 0,\n                          1 / (1 + np.exp(-z)),  # For z >= 0\n                          np.exp(z) / (1 + np.exp(z)))  # For z < 0\n\n    # Round the result to 4 decimal places\n    result = np.round(result, 4)\n\n    # If the input was a scalar, return a scalar; otherwise return a list\n    if np.ndim(result) == 0:\n        return float(result)\n    else:\n        return result.tolist()\n\n# Example usage:\nprint(sigmoid(0))  # Should return 0.5\nprint(sigmoid([-1, 0, 1]))  # Should return [0.2689, 0.5, 0.7311]\nprint(sigmoid(np.array([[1, 2], [-1, -2]])))  # Should return [[0.7311, 0.8808], [0.2689, 0.1192]]\n```\n\n### Explanation:\n1. **Input Handling**: The function first converts the input `z` into a NumPy array using `np.asarray(z)`, which allows for easy element-wise operations.\n  \n2. **Numerical Stability**: The sigmoid function is computed using a numerically stable approach:\n   - For values of `z` that are greater than or equal to 0, we use the standard formula.\n   - For values of `z` that are less than 0, we rearrange the formula to avoid overflow issues with `exp(-z)`.\n\n3. **Rounding**: The results are rounded to 4 decimal places using `np.round(result, 4)`.\n\n4. **Return Type**: The function checks if the result is a scalar (0-dimensional array) and returns it as a float. If the result is an array, it converts it back to a Python list using `.tolist()`.\n\nThis implementation ensures that the function works correctly for scalars, lists, and NumPy arrays while maintaining numerical stability and returning the expected types."}
{"task_id": 224, "completion_id": 0, "completion": "Here's a Python function that implements the Leaky ReLU activation function according to the specifications you've provided. The function handles input as a scalar, a nested list, or a `numpy.ndarray`, and it returns the activated values in the same structure as the input.\n\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    \n    # Define the Leaky ReLU function\n    def apply_leaky_relu(x):\n        return x if x >= 0 else a * x\n    \n    # Check if the input is a numpy array\n    if isinstance(z, np.ndarray):\n        # Apply the function element-wise and convert to list\n        return z.tolist() if a == 0.01 else np.where(z >= 0, z, a * z).tolist()\n    \n    # If the input is a scalar, apply the function directly\n    elif isinstance(z, (int, float)):\n        return apply_leaky_relu(z)\n    \n    # If the input is a list, we need to apply the function recursively\n    elif isinstance(z, list):\n        return [leaky_relu(item, a) for item in z]\n    \n    # Raise an error if the input type is not supported\n    else:\n        raise TypeError(\"Input must be a scalar, list, or numpy.ndarray.\")\n\n# Example usage:\n# print(leaky_relu(np.array([[1, -2], [3, -4]])))  # Should return [[1, -0.02], [3, -0.04]]\n# print(leaky_relu([-1, 0, 1]))  # Should return [-0.01, 0, 1]\n```\n\n### Explanation:\n1. **Function Definition**: The function `leaky_relu` takes an input `z` and an optional parameter `a`.\n2. **Inner Function**: `apply_leaky_relu` is defined to apply the Leaky ReLU transformation to a single value.\n3. **Input Handling**:\n   - If `z` is a `numpy.ndarray`, it uses `np.where` to apply the transformation element-wise and converts the result to a list.\n   - If `z` is a scalar (int or float), it applies the transformation directly.\n   - If `z` is a list, it recursively applies the `leaky_relu` function to each element.\n4. **Error Handling**: If the input type is not supported, a `TypeError` is raised.\n\nThis implementation ensures that the output maintains the same structure as the input while applying the Leaky ReLU activation function."}
{"task_id": 226, "completion_id": 0, "completion": "Here's a complete implementation of the AdaBoost algorithm using decision stumps as weak learners. The code includes the training of the ensemble and the prediction of labels for unseen data.\n\n```python\nimport numpy as np\nimport math\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D numpy array of shape (n_samples, n_features) with training data.\n        y_train: 1-D numpy array of shape (n_samples,) containing class labels (-1 or 1).\n        X_test:  2-D numpy array of unseen samples to classify.\n        n_clf:   Number of weak learners (decision stumps) to use in the ensemble.\n\n    Returns:\n        A Python list with one element per row in *X_test*. Each element must be\n        either -1 or 1 indicating the predicted class label.\n    \"\"\"\n    \n    n_samples, n_features = X_train.shape\n    # Initialize weights for each sample\n    weights = np.ones(n_samples) / n_samples\n    stumps = []  # To store the weak learners\n    alphas = []  # To store the weights of the weak learners\n\n    for _ in range(n_clf):\n        best_stump = None\n        best_error = float('inf')\n        best_predictions = None\n        \n        # Iterate over all features\n        for feature_index in range(n_features):\n            thresholds = np.unique(X_train[:, feature_index])\n            \n            # Test each threshold\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    # Make predictions based on the stump\n                    predictions = np.ones(n_samples)\n                    if polarity == 1:\n                        predictions[X_train[:, feature_index] < threshold] = -1\n                    else:\n                        predictions[X_train[:, feature_index] >= threshold] = -1\n                    \n                    # Calculate the weighted error\n                    misclassified = predictions != y_train\n                    error = np.dot(weights, misclassified)  # Weighted error\n                    \n                    # If this stump is better, save it\n                    if error < best_error:\n                        best_error = error\n                        best_stump = (feature_index, threshold, polarity)\n                        best_predictions = predictions\n        \n        # Calculate the alpha value\n        alpha = 0.5 * math.log((1 - best_error) / (best_error + 1e-10))\n        \n        # Update weights\n        weights *= np.exp(-alpha * y_train * best_predictions)\n        weights /= np.sum(weights)  # Normalize weights\n        \n        # Store the stump and its weight\n        stumps.append(best_stump)\n        alphas.append(alpha)\n\n    # Make predictions on the test set\n    final_predictions = np.zeros(X_test.shape[0])\n    \n    for alpha, stump in zip(alphas, stumps):\n        feature_index, threshold, polarity = stump\n        predictions = np.ones(X_test.shape[0])\n        \n        if polarity == 1:\n            predictions[X_test[:, feature_index] < threshold] = -1\n        else:\n            predictions[X_test[:, feature_index] >= threshold] = -1\n        \n        final_predictions += alpha * predictions\n    \n    # Return the sign of the final predictions\n    return list(np.sign(final_predictions).astype(int))\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 1], [4, 2]])\n# y_train = np.array([1, 1, -1, -1])\n# X_test = np.array([[1, 1], [3, 3]])\n# predictions = adaboost_predict(X_train, y_train, X_test, n_clf=5)\n# print(predictions)  # Output will be a list of -1 and 1\n```\n\n### Explanation:\n1. **Initialization**: The weights for each sample are initialized uniformly.\n2. **Training Loop**: For each weak learner (decision stump):\n   - Iterate over each feature and each unique threshold.\n   - For each threshold, evaluate both polarities (1 and -1).\n   - Calculate the weighted classification error.\n   - Select the stump with the lowest error and compute its weight (`alpha`).\n   - Update the sample weights to emphasize misclassified samples.\n3. **Prediction**: For the test set, the final prediction is computed as the weighted sum of the predictions from all stumps, and the sign of this sum is returned as the final prediction.\n\nThis implementation is efficient and follows the AdaBoost algorithm closely, ensuring that it can be used for binary classification tasks with decision stumps."}
{"task_id": 241, "completion_id": 0, "completion": "Here's a Python implementation of the orthogonal weight initializer based on the specifications provided. The function generates an orthogonal matrix according to the algorithm described and returns it as a Python list.\n\n```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers. Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix. Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor. If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n\n    # Step 2: Create a 2-D matrix A\n    d0 = shape[0]\n    d1 = np.prod(shape[1:])  # d1 * d2 * ... * dn\n    A = np.random.normal(size=(d0, d1))\n\n    # Step 3: Compute the SVD of A\n    U, _, Vt = np.linalg.svd(A, full_matrices=False)\n\n    # Step 4: Choose the appropriate SVD factor\n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = Vt\n\n    # Step 5: Reshape Q back to the requested tensor shape and scale\n    result = Q.reshape(shape) * scale\n\n    return result.tolist()\n\n# Example usage\nnp.random.seed(0)\nshape = (2, 2)\nscale = 0.5\noutput = orthogonal(shape, scale)\nprint(output)\n```\n\n### Explanation of the Code:\n1. **Input Validation**: The function first checks if the length of the `shape` is less than 2. If so, it returns `-1`.\n2. **Matrix Creation**: A 2-D matrix `A` is created with dimensions `(d0, d1)` filled with samples from a standard normal distribution.\n3. **SVD Calculation**: The Singular Value Decomposition (SVD) of `A` is computed, yielding matrices `U`, `\u03a3`, and `V\u1d40`.\n4. **Choosing the Orthogonal Matrix**: Depending on the shape of `U`, either `U` or `V\u1d40` is selected as the orthogonal matrix `Q`.\n5. **Reshaping and Scaling**: The orthogonal matrix `Q` is reshaped to the desired output shape and scaled by the specified factor.\n6. **Return Value**: The resulting matrix is converted to a Python list and returned.\n\n### Testing the Output:\nYou can run the provided example to see the output. The orthogonality condition can be verified by checking the dot product of the reshaped result against the identity matrix, scaled by `scale**2`."}
{"task_id": 243, "completion_id": 0, "completion": "Here's the implementation of the `actor_critic_forward` function based on the specifications provided. The function performs the forward pass through the Actor-Critic network using NumPy operations, and it returns the action probabilities and the state value rounded to four decimal places.\n\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    # Convert the input state to a NumPy array\n    s = np.array(state)\n\n    # Layer 1: Dense-1\n    z1 = np.maximum(0, np.sum(s) * np.ones(4))  # ReLU(s \u00b7 W1 + b1)\n    \n    # Layer 2: Dense-2\n    z2 = np.maximum(0, 4 * z1)  # ReLU(z1 \u00b7 W2 + b2)\n    \n    # Layer 3: Dense-3\n    z3 = np.maximum(0, 4 * z2)  # ReLU(z2 \u00b7 W3 + b3)\n    \n    # Actor head: logits\n    logits = 4 * z3  # z3 \u00b7 Wa + ba\n    \n    # Softmax activation for action probabilities\n    exp_logits = np.exp(logits - np.max(logits))  # for numerical stability\n    action_probs = exp_logits / np.sum(exp_logits)  # Softmax(logits)\n    \n    # Critic head: state value\n    state_value = 4 * z3[0]  # (z3 \u00b7 Wc + bc)[0]\n    \n    # Round results to 4 decimal places\n    action_probs_rounded = np.round(action_probs, 4).tolist()\n    state_value_rounded = round(state_value, 4)\n    \n    return action_probs_rounded, state_value_rounded\n\n# Example usage:\nstate = [1.0, 2.0, 3.0]\naction_probs, state_value = actor_critic_forward(state)\nprint(\"Action Probabilities:\", action_probs)\nprint(\"State Value:\", state_value)\n```\n\n### Explanation:\n1. **Input Handling**: The input state is converted to a NumPy array for easier mathematical operations.\n2. **Layer Calculations**:\n   - Each layer's output is computed using the specified transformations. The ReLU activation is applied where necessary.\n   - The logits for the actor head are calculated, and the softmax function is applied to obtain action probabilities.\n3. **State Value Calculation**: The state value is computed directly from the output of the last hidden layer.\n4. **Rounding**: The results are rounded to four decimal places before returning.\n\nThis implementation adheres to the architecture and behavior described in the task."}
{"task_id": 249, "completion_id": 0, "completion": "Here's the implementation of the `actor_critic_forward` function that performs the forward pass through the Actor-Critic network as described in your task. The function uses NumPy for matrix operations and applies the ReLU activation function, softmax function, and rounding as specified.\n\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head\n        value         : float          \u2013 scalar output of the critic head\n    \"\"\"\n    # Convert state to a NumPy array\n    state = np.array(state)\n\n    # Shared layers\n    # First layer\n    z1 = np.dot(state, params['W1']) + params['b1']\n    a1 = np.maximum(0, z1)  # ReLU\n\n    # Second layer\n    z2 = np.dot(a1, params['W2']) + params['b2']\n    a2 = np.maximum(0, z2)  # ReLU\n\n    # Third layer\n    z3 = np.dot(a2, params['W3']) + params['b3']\n    a3 = np.maximum(0, z3)  # ReLU\n\n    # Actor head\n    raw_action_scores = np.dot(a3, params['Wa']) + params['ba']\n    exp_scores = np.exp(raw_action_scores - np.max(raw_action_scores))  # for numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Critic head\n    state_value = np.dot(a3, params['Wc']) + params['bc']\n\n    # Rounding the results\n    probabilities = np.round(probabilities, 4)\n    state_value = np.round(state_value.item(), 4)  # Convert to scalar and round\n\n    # Adjusting the output format for probabilities\n    probabilities = [round(p, 1) if p in [0.0, 1.0] else round(p, 4) for p in probabilities]\n\n    return probabilities, state_value\n```\n\n### Explanation:\n1. **Input Conversion**: The input state is converted to a NumPy array for easier matrix operations.\n2. **Shared Layers**: The function computes the forward pass through three shared dense layers, applying the ReLU activation function after each layer.\n3. **Actor Head**: The output from the last shared layer is fed into the actor head, where it computes raw action scores. The softmax function is applied to convert these scores into probabilities.\n4. **Critic Head**: The same output from the shared layers is used to compute the state value in the critic head.\n5. **Rounding**: The probabilities and state value are rounded to four decimal places, with special handling for values that are exactly 0 or 1.\n6. **Return**: The function returns a tuple containing the action probabilities and the state value.\n\nThis implementation adheres to the requirements specified in your task description."}
{"task_id": 253, "completion_id": 0, "completion": "Here's the implementation of the `elastic_net_regularization` function that computes the Elastic-Net penalty and its gradient based on the provided specifications:\n\n```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    # Convert w to a NumPy array for easier calculations\n    w = np.asarray(w)\n    \n    # Calculate the L2 norm and the penalty\n    l2_norm = np.linalg.norm(w, 2)\n    penalty = alpha * (l1_ratio * l2_norm + (1 - l1_ratio) * 0.5 * np.dot(w, w))\n    \n    if gradient:\n        # Calculate the gradient\n        gradient_value = alpha * (l1_ratio * np.sign(w) + (1 - l1_ratio) * w)\n        return [round(g, 4) for g in gradient_value]\n    \n    return round(penalty, 4)\n\n# Example usage:\n# penalty_value = elastic_net_regularization([1.0, -2.0, 3.0], 0.5, 0.5)\n# gradient_value = elastic_net_regularization([1.0, -2.0, 3.0], 0.5, 0.5, gradient=True)\n```\n\n### Explanation:\n1. **Input Handling**: The function accepts a weight vector `w`, regularization strength `alpha`, L1 ratio `l1_ratio`, and a boolean `gradient` to determine whether to return the penalty or the gradient.\n2. **Penalty Calculation**: The penalty is computed using the formula provided, which combines both L1 and L2 regularization terms.\n3. **Gradient Calculation**: If `gradient` is set to `True`, the function computes the gradient using the specified formula and returns it as a list of rounded values.\n4. **Output**: The penalty is rounded to 4 decimal places, and if the gradient is requested, each element of the gradient is also rounded to 4 decimal places.\n\nYou can test the function with different weight vectors and parameters to see how it behaves."}
{"task_id": 256, "completion_id": 0, "completion": "Here's a complete implementation of the `grad_check_RNN` function that performs numerical gradient checking for a vanilla RNN parameter. The function follows the specifications provided in your task description.\n\n```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\n\n    Args:\n        model:        A model exposing a ``parameters`` dict, a ``forward``\n                       method (single time-step) and a ``flush_gradients``\n                       method.\n        loss_func:    Callable that maps the list of predictions to a scalar\n                       loss value.\n        param_name:   Name of the parameter to be checked.  \"Ba\" and \"Bx\" must\n                       be redirected to the lower-case keys.  If the name is\n                       \"X\" or \"y\" the function should immediately return None.\n        n_t:          Number of time-steps to unroll the network for.\n        X:            Input tensor of shape (batch, input_dim, n_t).\n        epsilon:      Small perturbation added/subtracted to the parameter.\n\n    Returns:\n        NumPy array containing the *transposed* numerical gradient of the\n        chosen parameter or None for the non-trainable names.\n    \"\"\"\n    \n    # Handle special cases for parameter names\n    if param_name in [\"X\", \"y\"]:\n        return None\n    elif param_name == \"Ba\":\n        param_name = \"ba\"\n    elif param_name == \"Bx\":\n        param_name = \"bx\"\n\n    # Get the original parameter and its shape\n    original_param = model.parameters[param_name].astype(np.float64)\n    param_shape = original_param.shape\n    \n    # Initialize the gradient array\n    grads = np.zeros(param_shape, dtype=np.float64)\n\n    # Iterate over each element in the parameter\n    for index in np.ndindex(param_shape):\n        # Perturb the parameter by +epsilon\n        original_value = original_param[index]\n        \n        # Compute L(\u03b8\u1d62 + \u03b5)\n        original_param[index] = original_value + epsilon\n        model.parameters[param_name] = original_param\n        model.flush_gradients()  # Reset gradients before forward pass\n        \n        predictions_plus = []\n        for t in range(n_t):\n            predictions_plus.append(model.forward(X[:, :, t]))\n        loss_plus = loss_func(predictions_plus)\n        \n        # Compute L(\u03b8\u1d62 - \u03b5)\n        original_param[index] = original_value - epsilon\n        model.parameters[param_name] = original_param\n        model.flush_gradients()  # Reset gradients before forward pass\n        \n        predictions_minus = []\n        for t in range(n_t):\n            predictions_minus.append(model.forward(X[:, :, t]))\n        loss_minus = loss_func(predictions_minus)\n        \n        # Compute the numerical gradient\n        grads[index] = (loss_plus - loss_minus) / (2 * epsilon)\n        \n        # Restore the original parameter value\n        original_param[index] = original_value\n\n    # Return the transposed gradient\n    return grads.T\n```\n\n### Explanation of the Code:\n1. **Parameter Handling**: The function first checks if the `param_name` is one of the special cases (\"X\", \"y\", \"Ba\", \"Bx\") and adjusts accordingly.\n2. **Gradient Initialization**: It initializes a gradient array `grads` with zeros, having the same shape as the parameter being checked.\n3. **Perturbation Loop**: For each element in the parameter:\n   - It perturbs the parameter by `+epsilon` and computes the loss.\n   - It then perturbs the parameter by `-epsilon` and computes the loss again.\n   - The numerical gradient is calculated using the centered finite difference formula.\n4. **Restoration**: After computing the gradient for each element, it restores the original parameter value.\n5. **Return Value**: Finally, it returns the transposed gradient array.\n\nThis implementation should work correctly for any trainable parameter in the RNN model, allowing you to perform gradient checking effectively."}
{"task_id": 257, "completion_id": 0, "completion": "Here's a complete implementation of the AdaBoost algorithm using decision stumps as weak learners. The code follows the specifications provided in your task description.\n\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    \n    # Ensure n_clf is at least 1\n    n_clf = max(1, n_clf)\n    \n    m, n = X_train.shape\n    # Initialize weights for each sample\n    weights = np.ones(m) / m\n    \n    # Store the weak classifiers and their weights\n    classifiers = []\n    alphas = []\n    \n    for _ in range(n_clf):\n        # Initialize the best stump parameters\n        best_stump = None\n        best_error = float('inf')\n        best_pred = None\n        \n        # Iterate over each feature\n        for feature_index in range(n):\n            # Get unique values for the feature\n            unique_values = np.unique(X_train[:, feature_index])\n            \n            # Test each unique value as a potential threshold\n            for threshold in unique_values:\n                # Create predictions based on the threshold\n                predictions = np.ones(m)\n                predictions[X_train[:, feature_index] < threshold] = -1\n                \n                # Calculate the weighted error\n                error = weights[(predictions != y_train)].sum()\n                \n                # If the error is better than the best found so far, update\n                if error < best_error:\n                    best_error = error\n                    best_stump = (feature_index, threshold)\n                    best_pred = predictions\n        \n        # Calculate alpha (weight of the classifier)\n        alpha = 0.5 * np.log((1 - best_error) / (best_error + 1e-10))  # Add small value to avoid division by zero\n        \n        # Update weights\n        weights *= np.exp(-alpha * y_train * best_pred)\n        weights /= weights.sum()  # Normalize weights\n        \n        # Store the classifier and its weight\n        classifiers.append(best_stump)\n        alphas.append(alpha)\n    \n    # Make predictions on the test set\n    k = X_test.shape[0]\n    final_predictions = np.zeros(k)\n    \n    for i in range(k):\n        for alpha, (feature_index, threshold) in zip(alphas, classifiers):\n            if X_test[i, feature_index] < threshold:\n                final_predictions[i] += -alpha\n            else:\n                final_predictions[i] += alpha\n    \n    # Return the sign of the final predictions\n    return [1 if pred > 0 else -1 for pred in final_predictions]\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 1], [4, 2]])\n# y_train = np.array([1, 1, -1, -1])\n# X_test = np.array([[1, 1], [3, 3]])\n# predictions = adaboost_predict(X_train, y_train, X_test, n_clf=5)\n# print(predictions)\n```\n\n### Explanation:\n1. **Initialization**: The weights for each training sample are initialized uniformly.\n2. **Training**: For each weak classifier (decision stump):\n   - Iterate through each feature and each unique value to find the best threshold that minimizes the weighted classification error.\n   - Calculate the weight of the classifier using the formula provided.\n   - Update the weights of the training samples based on the predictions of the current stump.\n3. **Prediction**: For each sample in the test set, aggregate the predictions from all stumps weighted by their respective alpha values and return the sign of the result.\n\nThis implementation should work efficiently for binary classification tasks using the AdaBoost algorithm with decision stumps."}
{"task_id": 261, "completion_id": 0, "completion": "Here's the implementation of the Glorot (Xavier) normal weight initialization function as per your requirements:\n\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape. Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError(\"Shape must have at least two dimensions.\")\n\n    # Determine fan_in and fan_out\n    if len(shape) == 2:\n        fan_in, fan_out = shape\n    else:\n        fan_in = shape[1] * np.prod(shape[2:])  # in_channels * receptive_field_size\n        fan_out = shape[0] * np.prod(shape[2:])  # out_channels * receptive_field_size\n\n    # Calculate standard deviation\n    s = np.sqrt(2 / (fan_in + fan_out))\n\n    # Generate the weights from a normal distribution\n    return np.random.normal(loc=0.0, scale=s, size=shape).astype(np.float32)\n\n# Example usage:\n# weights = glorot_normal((64, 32, 3, 3))  # Example for a convolutional layer\n# print(weights)\n```\n\n### Explanation:\n1. **Input Validation**: The function checks if the shape has at least two dimensions. If not, it raises a `ValueError`.\n2. **Fan-in and Fan-out Calculation**:\n   - For a 2D shape, it directly assigns the values from the shape.\n   - For shapes with more than two dimensions, it calculates the `fan_in` and `fan_out` based on the number of input and output channels and the receptive field size.\n3. **Standard Deviation Calculation**: It computes the standard deviation `s` using the formula provided.\n4. **Weight Initialization**: It generates a NumPy array of the specified shape, filled with values drawn from a normal distribution with mean 0 and standard deviation `s`. The result is cast to `float32` to ensure the correct data type.\n\n### Testing the Function:\nYou can test the function by calling it with different shapes and checking the mean and standard deviation of the generated weights to ensure they are close to the expected values."}
{"task_id": 266, "completion_id": 0, "completion": "To build an adjacency list for an undirected graph from a given vertex list `V` and edge list `E`, we can follow these steps:\n\n1. Initialize a dictionary to hold the adjacency list, where each vertex in `V` will map to a set of its neighbors. Using a set helps to automatically handle duplicate edges.\n2. Iterate through the edge list `E` and for each edge `(u, v)`, add `v` to the set of neighbors for `u` and `u` to the set of neighbors for `v`.\n3. After processing all edges, convert the sets of neighbors back to lists, ensuring that the order of neighbors follows the order in `V`.\n4. Return the final adjacency list.\n\nHere's the implementation of the above logic:\n\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples `(u, v)` describing undirected edges that connect\n        vertices `u` and `v`.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list `G` where `G[i]` contains all vertices adjacent to `V[i]`.\n        \u2022 The outer list has the same length and order as `V`.\n        \u2022 Inner lists contain no duplicates and are ordered by their\n          appearance order in `V`.\n    \"\"\"\n    # Initialize a dictionary to hold the adjacency list\n    adj_dict = {v: set() for v in V}\n    \n    # Populate the adjacency list with edges\n    for u, v in E:\n        if u in adj_dict and v in adj_dict:  # Ensure both vertices are in V\n            adj_dict[u].add(v)\n            adj_dict[v].add(u)\n    \n    # Build the final adjacency list in the order of V\n    adj_list = []\n    for v in V:\n        # Convert the set to a sorted list based on the order in V\n        neighbors = sorted(adj_dict[v], key=lambda x: V.index(x))\n        adj_list.append(neighbors)\n    \n    return adj_list\n```\n\n### Explanation of the Code:\n- We first create a dictionary `adj_dict` where each vertex in `V` is initialized with an empty set to store its neighbors.\n- We then iterate through each edge `(u, v)` in `E`. For each edge, we add `v` to the set of neighbors for `u` and vice versa, ensuring that we only add neighbors if both vertices are present in `V`.\n- Finally, we construct the adjacency list by iterating through `V` and converting each set of neighbors back to a list, sorted according to their order in `V` using `V.index(x)`.\n\n### Example Usage:\n```python\nV = ['A', 'B', 'C', 'D']\nE = [('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'D')]\nadj_list = build_adj_list(V, E)\nprint(adj_list)  # Output: [['B', 'C'], ['A', 'C'], ['A', 'B', 'D'], ['C']]\n```\n\nThis implementation ensures that the adjacency list is built correctly according to the specified requirements."}
{"task_id": 267, "completion_id": 0, "completion": "Here's a Python implementation of the function `train_decision_stump` that finds the optimal weighted decision stump for a binary classification task. The function examines all features and unique feature values as candidate thresholds, and it calculates the weighted classification error for each candidate stump.\n\n```python\nimport numpy as np\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump (see task description).\n    \"\"\"\n    n_samples, n_features = X.shape\n    best_stump = {\n        \"feature_index\": None,\n        \"threshold\": None,\n        \"polarity\": None,\n        \"weighted_error\": float('inf')\n    }\n\n    for feature_index in range(n_features):\n        # Get unique values of the feature\n        unique_values = np.unique(X[:, feature_index])\n        \n        for threshold in unique_values:\n            # Calculate predictions for polarity = 1\n            predictions_pos = np.where(X[:, feature_index] < threshold, 1, -1)\n            error_pos = np.sum(sample_weights[predictions_pos != y])\n            \n            # Calculate predictions for polarity = -1\n            predictions_neg = np.where(X[:, feature_index] < threshold, -1, 1)\n            error_neg = np.sum(sample_weights[predictions_neg != y])\n            \n            # Check if we found a better stump for polarity = 1\n            if error_pos < best_stump[\"weighted_error\"]:\n                best_stump[\"feature_index\"] = feature_index\n                best_stump[\"threshold\"] = threshold\n                best_stump[\"polarity\"] = 1\n                best_stump[\"weighted_error\"] = error_pos\n            \n            # Check if we found a better stump for polarity = -1\n            if error_neg < best_stump[\"weighted_error\"]:\n                best_stump[\"feature_index\"] = feature_index\n                best_stump[\"threshold\"] = threshold\n                best_stump[\"polarity\"] = -1\n                best_stump[\"weighted_error\"] = error_neg\n\n    # Round the threshold and weighted error to 4 decimal places\n    best_stump[\"threshold\"] = round(best_stump[\"threshold\"], 4)\n    best_stump[\"weighted_error\"] = round(best_stump[\"weighted_error\"], 4)\n\n    return best_stump\n```\n\n### Explanation:\n1. **Input Parameters**:\n   - `X`: The feature matrix with shape `(n_samples, n_features)`.\n   - `y`: The label vector containing binary values `-1` and `1`.\n   - `sample_weights`: A vector of non-negative weights for each sample.\n\n2. **Initialization**:\n   - A dictionary `best_stump` is initialized to keep track of the best feature index, threshold, polarity, and the minimal weighted error.\n\n3. **Looping through Features and Thresholds**:\n   - For each feature, the unique values are extracted to serve as potential thresholds.\n   - For each threshold, predictions are made for both polarities (1 and -1).\n   - The weighted error is calculated for each polarity.\n\n4. **Updating the Best Stump**:\n   - If a stump with a lower weighted error is found, the `best_stump` dictionary is updated.\n\n5. **Rounding**:\n   - The threshold and weighted error are rounded to four decimal places before returning.\n\nThis implementation efficiently finds the optimal decision stump by evaluating all possible thresholds for each feature and considering the weights of the samples."}
{"task_id": 273, "completion_id": 0, "completion": "To implement the `best_split` function, we need to follow these steps:\n\n1. **Calculate the Shannon Entropy**: This will help us measure the uncertainty in the class labels.\n2. **Identify Candidate Thresholds**: These are the midpoints between consecutive distinct sorted values of the feature.\n3. **Calculate Information Gain for Each Threshold**: For each candidate threshold, we will split the dataset and compute the information gain.\n4. **Select the Best Threshold**: We will keep track of the maximum information gain and the corresponding threshold.\n\nHere's the complete implementation of the `best_split` function:\n\n```python\nimport math\nfrom collections import Counter\n\ndef entropy(labels):\n    \"\"\"Calculate the Shannon entropy of a list of labels.\"\"\"\n    label_counts = Counter(labels)\n    total = len(labels)\n    if total == 0:\n        return 0\n    return -sum((count / total) * math.log2(count / total) for count in label_counts.values() if count > 0)\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimals or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    # Convert to list if input is a numpy array\n    if hasattr(feature, 'tolist'):\n        feature = feature.tolist()\n    if hasattr(target, 'tolist'):\n        target = target.tolist()\n\n    # Get unique sorted feature values\n    unique_features = sorted(set(feature))\n    \n    # If all feature values are the same, return (None, 0.0)\n    if len(unique_features) < 2:\n        return (None, 0.0)\n\n    # Calculate the entropy of the parent node\n    parent_entropy = entropy(target)\n    n = len(target)\n\n    best_gain = 0.0\n    best_threshold = None\n\n    # Evaluate each candidate threshold\n    for i in range(len(unique_features) - 1):\n        t = (unique_features[i] + unique_features[i + 1]) / 2  # Midpoint\n        left_indices = [j for j in range(n) if feature[j] < t]\n        right_indices = [j for j in range(n) if feature[j] >= t]\n\n        if not left_indices or not right_indices:\n            continue  # Skip if any side is empty\n\n        left_labels = [target[j] for j in left_indices]\n        right_labels = [target[j] for j in right_indices]\n\n        # Calculate the weighted entropy of the split\n        left_entropy = entropy(left_labels)\n        right_entropy = entropy(right_labels)\n\n        # Information Gain\n        gain = parent_entropy - (len(left_labels) / n) * left_entropy - (len(right_labels) / n) * right_entropy\n\n        # Update best gain and threshold\n        if gain > best_gain or (gain == best_gain and (best_threshold is None or t < best_threshold)):\n            best_gain = gain\n            best_threshold = t\n\n    # Return the best threshold and gain rounded to 4 decimal places\n    return (round(best_threshold, 4), round(best_gain, 4)) if best_gain > 0 else (None, 0.0)\n```\n\n### Explanation of the Code:\n- **Entropy Calculation**: The `entropy` function computes the Shannon entropy for a list of class labels.\n- **Candidate Thresholds**: We find unique sorted values of the feature and compute midpoints between them.\n- **Information Gain Calculation**: For each threshold, we split the dataset and calculate the information gain.\n- **Best Threshold Selection**: We keep track of the maximum information gain and the corresponding threshold, ensuring to return the smallest threshold in case of ties.\n\n### Edge Cases:\n- If all feature values are identical, we return `(None, 0.0)`.\n- If no threshold provides a positive information gain, we also return `(None, 0.0)`. \n\nThis implementation should work efficiently for the task described."}
{"task_id": 286, "completion_id": 0, "completion": "Here's the complete implementation of the required functions, including the `get_initializer` function that retrieves the appropriate weight initializer based on the provided name. Each initializer function is defined to return a matrix of the specified shape filled with the appropriate values.\n\n```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    return [[0.0 for _ in range(shape[1])] for _ in range(shape[0])]\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    return [[1.0 for _ in range(shape[1])] for _ in range(shape[0])]\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    random.seed(seed)  # Set the seed for reproducibility\n    return [[random.uniform(low, high) for _ in range(shape[1])] for _ in range(shape[0])]\n\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    # Retrieve the function from the global namespace\n    initializer = globals().get(name)\n    \n    # Check if the initializer is callable\n    if callable(initializer):\n        return initializer\n    else:\n        raise ValueError(\"Invalid initialization function.\")\n\n# Example usage:\nif __name__ == \"__main__\":\n    name = \"ones_init\"\n    shape = (2, 2)\n\n    init_fn = get_initializer(name)  # returns the function ones_init\n    output = init_fn(shape)           # [[1.0, 1.0], [1.0, 1.0]]\n    print(output)                     # Output: [[1.0, 1.0], [1.0, 1.0]]\n```\n\n### Explanation:\n1. **Initializer Functions**:\n   - `zeros_init`: Creates a matrix filled with zeros.\n   - `ones_init`: Creates a matrix filled with ones.\n   - `random_uniform_init`: Creates a matrix filled with random numbers uniformly distributed between `low` and `high`. It uses a seed for reproducibility.\n\n2. **`get_initializer` Function**:\n   - It retrieves the function from the global namespace using `globals().get(name)`.\n   - It checks if the retrieved object is callable.\n   - If it is callable, it returns the function; otherwise, it raises a `ValueError` with the specified message.\n\n3. **Example Usage**:\n   - The example at the bottom demonstrates how to use the `get_initializer` function to retrieve the `ones_init` function and call it with a shape of `(2, 2)`, resulting in a 2x2 matrix of ones."}
{"task_id": 287, "completion_id": 0, "completion": "Here's the implementation of the 2-D average pooling forward and backward pass as described in your task:\n\n```python\nimport numpy as np\n\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    s_h, s_w = stride\n\n    # Calculate output dimensions\n    out_h = (H - p_h) // s_h + 1\n    out_w = (W - p_w) // s_w + 1\n\n    # Initialize pooled output\n    pooled_output = np.zeros((N, C, out_h, out_w))\n\n    # Forward pass: Average pooling\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                for j in range(out_w):\n                    h_start = i * s_h\n                    h_end = h_start + p_h\n                    w_start = j * s_w\n                    w_end = w_start + p_w\n                    pooled_output[n, c, i, j] = np.mean(X[n, c, h_start:h_end, w_start:w_end])\n\n    # Backward pass: Distributing gradients\n    grad_input = np.zeros_like(X)\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                for j in range(out_w):\n                    h_start = i * s_h\n                    h_end = h_start + p_h\n                    w_start = j * s_w\n                    w_end = w_start + p_w\n                    # Distribute the accumulated gradient equally\n                    grad_input[n, c, h_start:h_end, w_start:w_end] += accum_grad[n, c, i, j] / (p_h * p_w)\n\n    # Round the outputs to the nearest 4-th decimal and convert to lists\n    pooled_output = np.round(pooled_output, 4).tolist()\n    grad_input = np.round(grad_input, 4).tolist()\n\n    return pooled_output, grad_input\n```\n\n### Explanation:\n1. **Forward Pass**:\n   - We calculate the output dimensions based on the input dimensions, pooling window size, and stride.\n   - We iterate over each batch, channel, and the output dimensions to compute the average of the elements in the pooling window and store it in the `pooled_output`.\n\n2. **Backward Pass**:\n   - We initialize a gradient input array with the same shape as the input `X`.\n   - For each pooled output, we distribute the incoming gradient (`accum_grad`) back to the corresponding elements in the input `X`. The gradient is divided by the number of elements in the pooling window (`p_h * p_w`) to ensure equal distribution.\n\n3. **Rounding and Conversion**:\n   - Finally, we round both the pooled output and the gradient input to four decimal places and convert them to Python lists before returning.\n\nThis implementation should work correctly for the average pooling operation as specified."}
{"task_id": 290, "completion_id": 0, "completion": "To implement the `compare_trees` function that checks if two binary decision trees are equivalent, we will follow the criteria outlined in the task description. The function will recursively compare the structure and values of the two trees.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    # If both are leaves, compare their values\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-8)\n    \n    # If both are nodes, compare their features and thresholds\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        if (tree_a.feature != tree_b.feature or\n            not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8)):\n            return False\n        \n        # Recursively compare left and right subtrees\n        return (compare_trees(tree_a.left, tree_b.left) and\n                compare_trees(tree_a.right, tree_b.right))\n    \n    # If one is a leaf and the other is a node, they are not equivalent\n    return False\n```\n\n### Explanation of the Code:\n\n1. **Base Case for Leaves**: \n   - If both `tree_a` and `tree_b` are instances of `Leaf`, we use `np.allclose` to check if their values are numerically equal within a tolerance of \\(1 \\times 10^{-8}\\).\n\n2. **Base Case for Nodes**: \n   - If both are instances of `Node`, we first check if their `feature` indices are the same and if their `thresholds` are close enough using `np.isclose`.\n   - If they pass these checks, we recursively call `compare_trees` on their left and right children.\n\n3. **Different Types**: \n   - If one is a `Leaf` and the other is a `Node`, we return `False` since they cannot be equivalent.\n\n4. **Recursion**: \n   - The function continues to call itself until it reaches the leaves or finds a discrepancy in the structure or values.\n\nThis implementation ensures that the trees are compared thoroughly and correctly according to the specified criteria."}
{"task_id": 292, "completion_id": 0, "completion": "Here's a Python function that implements the single-point column crossover for neural network weights as described in your task. The function uses NumPy for efficient matrix operations and ensures that the input matrices are validated before performing the crossover.\n\n```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    # Convert input lists to NumPy arrays for easier manipulation\n    np_parent1 = np.array(parent1)\n    np_parent2 = np.array(parent2)\n\n    # Validate that both parents have the same shape\n    if np_parent1.shape != np_parent2.shape:\n        return -1\n\n    # Get the number of rows and columns\n    num_rows, num_cols = np_parent1.shape\n\n    # Create children matrices\n    child1 = np.zeros_like(np_parent1)\n    child2 = np.zeros_like(np_parent2)\n\n    # Perform crossover\n    child1[:, :cutoff] = np_parent1[:, :cutoff]\n    child1[:, cutoff:] = np_parent2[:, cutoff:]\n\n    child2[:, :cutoff] = np_parent2[:, :cutoff]\n    child2[:, cutoff:] = np_parent1[:, cutoff:]\n\n    # Convert the children back to lists\n    return (child1.tolist(), child2.tolist())\n\n# Example usage:\nparent1 = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\nparent2 = [[1.1, 1.2, 1.3], [1.4, 1.5, 1.6]]\ncutoff = 1\n\nchildren = single_point_crossover(parent1, parent2, cutoff)\nprint(children)\n```\n\n### Explanation:\n1. **Input Conversion**: The function first converts the input lists into NumPy arrays for easier manipulation.\n2. **Shape Validation**: It checks if both parent matrices have the same shape. If not, it returns `-1`.\n3. **Crossover Logic**: It creates two new child matrices:\n   - For `child1`, it copies columns from `parent1` before the cutoff and from `parent2` from the cutoff onwards.\n   - For `child2`, it does the opposite.\n4. **Output Conversion**: Finally, it converts the resulting NumPy arrays back to nested Python lists and returns them as a tuple.\n\n### Example Usage:\nThe example at the end demonstrates how to use the function with two parent matrices and a specified cutoff. The output will be the two new child matrices after performing the crossover."}
{"task_id": 294, "completion_id": 0, "completion": "To convert the custom graph representation into an adjacency dictionary, we need to follow the specifications provided. The function will iterate through the edges in the graph, populate the adjacency dictionary, and handle both directed and undirected graphs appropriately. Here's how we can implement this:\n\n```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph represented as a dictionary with the keys:\n          * 'is_directed' (bool)\n          * '_V2I'        (dict mapping vertex label -> index)\n          * '_G'          (list of adjacency lists, each edge is a\n                           tuple (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a vertex label and the value is a list\n        of `(neighbour, weight)` tuples.  For undirected graphs every edge\n        appears once in each direction and the neighbour lists are sorted\n        alphabetically.\n    \"\"\"\n    # Initialize the adjacency dictionary\n    adj = {}\n    \n    # Extract the necessary components from the graph\n    is_directed = G['is_directed']\n    vertex_map = G['_V2I']\n    edges = G['_G']\n    \n    # Initialize the adjacency list for each vertex\n    for vertex in vertex_map.keys():\n        adj[vertex] = []\n    \n    # Populate the adjacency list\n    for source_label, target_label, weight in edges:\n        adj[source_label].append((target_label, weight))\n        \n        # If the graph is undirected, add the reverse edge\n        if not is_directed:\n            adj[target_label].append((source_label, weight))\n    \n    # Sort the adjacency lists alphabetically\n    for vertex in adj:\n        adj[vertex].sort(key=lambda x: x[0])  # Sort by neighbour label\n    \n    return adj\n```\n\n### Explanation of the Code:\n1. **Initialization**: We start by creating an empty adjacency dictionary `adj`.\n2. **Extracting Graph Properties**: We retrieve whether the graph is directed, the vertex mapping, and the list of edges from the input dictionary `G`.\n3. **Setting Up Adjacency Lists**: We initialize an empty list for each vertex in the adjacency dictionary.\n4. **Populating Edges**: We iterate through the list of edges. For each edge `(source_label, target_label, weight)`, we append the target vertex and weight to the source vertex's list. If the graph is undirected, we also append the source vertex to the target vertex's list.\n5. **Sorting**: Finally, we sort the adjacency lists for each vertex to ensure the output is deterministic and meets the requirement of being sorted alphabetically.\n6. **Return**: The function returns the populated adjacency dictionary.\n\nThis implementation adheres to the requirements specified in the task and should work correctly for both directed and undirected graphs."}
{"task_id": 296, "completion_id": 0, "completion": "Here's a Python function that checks if all elements in a NumPy array are binary (i.e., only `0` or `1`). The function handles integer, float, and boolean arrays of any shape. \n\n```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    # Convert the input to a NumPy array if it isn't one already\n    x = np.asarray(x)\n    \n    # Check if all elements are either 0 or 1\n    return np.all((x == 0) | (x == 1))\n\n# Example usage:\n# print(is_binary(np.array([0, 1, 1, 0])))  # True\n# print(is_binary(np.array([0, 1, 2])))      # False\n# print(is_binary(np.array([True, False])))   # True\n# print(is_binary(np.array([1.0, 0.0, 1.0]))) # True\n# print(is_binary(np.array([1.0, 0.5])))      # False\n```\n\n### Explanation:\n1. **Input Handling**: The function first converts the input to a NumPy array using `np.asarray()`. This ensures that the function can handle various input types, including lists or tuples.\n  \n2. **Binary Check**: The expression `(x == 0) | (x == 1)` creates a boolean array where each element is `True` if it is either `0` or `1`. The `np.all()` function checks if all values in this boolean array are `True`.\n\n3. **Return Value**: The function returns `True` if all elements are binary and `False` otherwise.\n\nThis implementation is efficient and works for arrays of any shape, as required."}
{"task_id": 298, "completion_id": 0, "completion": "To implement the Maximum Likelihood N-gram Log-Probability Calculator, we will follow the steps outlined in the task description. Below is the complete implementation of the `ngram_log_prob` function:\n\n```python\nimport math\nfrom collections import Counter, defaultdict\nfrom typing import List\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        A list of training sentences. Each sentence is a plain string; tokens\n        are assumed to be separated by white-spaces.\n    query : str\n        The sentence whose probability should be calculated.\n    N : int\n        The order of the N-gram model (N \u2265 1).\n\n    Returns\n    -------\n    float\n        The sum of natural logarithms of the probabilities of every\n        consecutive N-gram occurring in *query*, rounded to 4 decimal places.\n        If any necessary N-gram is unseen in the corpus the function returns\n        float('-inf').\n    \"\"\"\n    # Tokenize the corpus\n    tokenized_corpus = [sentence.split() for sentence in corpus]\n    \n    # Initialize counters for N-grams and (N-1)-grams\n    ngram_counts = Counter()\n    n_minus_1_gram_counts = Counter()\n    \n    # Process each sentence in the corpus\n    for tokens in tokenized_corpus:\n        if N > 1:\n            # Pad the tokens for N-grams\n            tokens = ['<bol>'] * (N - 1) + tokens + ['<eol>']\n        \n        # Count N-grams and (N-1)-grams\n        for i in range(len(tokens) - N + 1):\n            ngram = tuple(tokens[i:i + N])\n            n_minus_1_gram = tuple(tokens[i:i + N - 1])\n            ngram_counts[ngram] += 1\n            n_minus_1_gram_counts[n_minus_1_gram] += 1\n    \n    # Tokenize the query\n    query_tokens = query.split()\n    \n    # Pad the query for N-grams\n    if N > 1:\n        query_tokens = ['<bol>'] * (N - 1) + query_tokens + ['<eol>']\n    \n    # Calculate the log-probability\n    log_prob_sum = 0.0\n    \n    for i in range(len(query_tokens) - N + 1):\n        ngram = tuple(query_tokens[i:i + N])\n        n_minus_1_gram = tuple(query_tokens[i:i + N - 1])\n        \n        # Get counts\n        ngram_count = ngram_counts[ngram]\n        n_minus_1_gram_count = n_minus_1_gram_counts[n_minus_1_gram]\n        \n        # Calculate probabilities\n        if N == 1:\n            total_count = sum(ngram_counts.values())\n            prob = ngram_count / total_count if total_count > 0 else 0\n        else:\n            prob = ngram_count / n_minus_1_gram_count if n_minus_1_gram_count > 0 else 0\n        \n        # If probability is zero, return negative infinity\n        if prob == 0:\n            return float('-inf')\n        \n        # Accumulate the log probability\n        log_prob_sum += math.log(prob)\n    \n    # Return the rounded log probability\n    return round(log_prob_sum, 4)\n\n# Example usage\ncorpus = [\"a b a\", \"b a b\"]\nquery = \"a b\"\nN = 1\nprint(ngram_log_prob(corpus, query, N))  # Output: -1.3863\n```\n\n### Explanation of the Code:\n1. **Tokenization**: The corpus is tokenized into lists of words. Each sentence is split by whitespace.\n2. **Padding**: For N-grams where N > 1, we pad the token list with `<bol>` tokens at the beginning and `<eol>` at the end.\n3. **Counting N-grams**: We use `Counter` to count occurrences of N-grams and their (N-1)-gram prefixes.\n4. **Query Processing**: The query is also tokenized and padded similarly to the corpus.\n5. **Log-Probability Calculation**: For each N-gram in the query, we calculate the conditional probability and accumulate the log probabilities. If any N-gram has a zero probability, we return `float('-inf')`.\n6. **Return Value**: Finally, we return the total log-probability rounded to four decimal places."}
{"task_id": 302, "completion_id": 0, "completion": "Here's a complete implementation of the Spectral Clustering algorithm from scratch, following the specifications you provided:\n\n```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n_samples = X.shape[0]\n    \n    # Step 1: Build the weighted adjacency matrix W\n    W = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            if i != j:\n                W[i, j] = 1 / (1 + np.linalg.norm(X[i] - X[j]))\n    \n    # Step 2: Compute the degree matrix D\n    D = np.diag(np.sum(W, axis=1))\n    \n    # Step 3: Compute the unnormalized graph Laplacian L\n    L = D - W\n    \n    # Step 4: Compute the eigenvalues and eigenvectors of L\n    eigenvalues, eigenvectors = np.linalg.eig(L)\n    \n    # Step 5: Sort the eigenvalues and corresponding eigenvectors\n    idx = np.argsort(eigenvalues)\n    E = eigenvectors[:, idx[:k]]  # Take the first k eigenvectors\n    \n    # Step 6: Run k-means on the rows of E\n    centroids = E[:k]  # Initial centroids\n    labels = np.zeros(n_samples, dtype=int)\n    \n    for iteration in range(100):\n        # Step 6a: Assign clusters based on closest centroid\n        for i in range(n_samples):\n            distances = np.linalg.norm(E[i] - centroids, axis=1)\n            labels[i] = np.argmin(distances)\n        \n        # Step 6b: Update centroids\n        new_centroids = np.array([E[labels == j].mean(axis=0) for j in range(k)])\n        \n        # Handle empty clusters\n        for j in range(k):\n            if np.isnan(new_centroids[j]).any() or np.all(labels != j):\n                new_centroids[j] = E[np.random.choice(n_samples)]\n        \n        # Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n        \n        centroids = new_centroids\n    \n    return labels.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [1, 4], [1, 0],\n#               [4, 2], [4, 4], [4, 0]])\n# k = 2\n# print(spectral_clustering(X, k))\n```\n\n### Explanation of the Code:\n1. **Weighted Adjacency Matrix (W)**: We compute the weight between each pair of points using the formula provided. The diagonal is set to zero to ensure no self-connections.\n  \n2. **Degree Matrix (D)**: This is a diagonal matrix where each entry \\( D_{ii} \\) is the sum of the weights of the connections for point \\( i \\).\n\n3. **Graph Laplacian (L)**: The Laplacian is computed as \\( L = D - W \\).\n\n4. **Eigenvalues and Eigenvectors**: We compute the eigenvalues and eigenvectors of the Laplacian matrix. We then sort them and select the first \\( k \\) eigenvectors.\n\n5. **K-means Clustering**: We initialize the centroids with the first \\( k \\) rows of the eigenvector matrix. We then iteratively assign points to the nearest centroid and update the centroids until convergence or a maximum of 100 iterations is reached. If a cluster becomes empty, we reinitialize its centroid with a random point from \\( E \\).\n\n6. **Return Labels**: Finally, we return the cluster labels as a list.\n\nThis implementation adheres to the constraints of using only NumPy for numerical operations and does not rely on any external libraries for clustering or linear algebra."}
{"task_id": 303, "completion_id": 0, "completion": "Here's the implementation of the `err_fmt` function that formats the error message according to the specified requirements:\n\n```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    prev_index = max(ix - 1, 0)\n    prev_mine, prev_label = params[prev_index]\n    curr_mine, curr_label = params[ix]\n\n    debug_message = (\n        \"------------------------- DEBUG -------------------------\\n\"\n        f\"Mine (prev) [{prev_label}]:\\n\"\n        f\"{prev_mine}\\n\\n\"\n        f\"Theirs (prev) [{prev_label}]:\\n\"\n        f\"{golds[prev_label]}\\n\\n\"\n        f\"Mine [{curr_label}]:\\n\"\n        f\"{curr_mine}\\n\\n\"\n        f\"Theirs [{curr_label}]:\\n\"\n        f\"{golds[curr_label]}{warn_str}\\n\"\n        \"----------------------- END DEBUG -----------------------\"\n    )\n    \n    return debug_message\n```\n\n### Explanation:\n1. **Previous Index Calculation**: The previous index is calculated using `max(ix - 1, 0)` to ensure it does not go out of bounds.\n2. **Extracting Values**: The function extracts the `prev_mine` and `prev_label` from the `params` list using the calculated previous index, and the current values using the provided index `ix`.\n3. **Formatting the Output**: The output string is constructed using formatted string literals (f-strings) to ensure the correct values are inserted in the right places.\n4. **Warning String**: The `warn_str` is appended directly after the expected output for the current label, ensuring it appears exactly as provided.\n5. **Return Value**: The final formatted string is returned.\n\nThis implementation adheres strictly to the specified format, including the exact number of dashes and line breaks."}
{"task_id": 304, "completion_id": 0, "completion": "Here's the implementation of the `bayesian_posterior_mean` function that computes the posterior mean of the regression coefficients in Bayesian linear regression with an unknown noise variance. The function handles the specified prior configurations and includes an option to fit an intercept.\n\n```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu = 0.0,\n    V = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility).\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    # Add intercept if required\n    if fit_intercept:\n        X = np.hstack((np.ones((X.shape[0], 1)), X))\n    \n    N, M = X.shape\n    \n    # Handle prior mean (mu)\n    if np.isscalar(mu):\n        mu = np.full(M, mu)\n    \n    # Handle prior scale (V)\n    if V is None:\n        V = np.eye(M)\n    elif np.isscalar(V):\n        V = V * np.eye(M)\n    elif isinstance(V, (list, tuple)):\n        V = np.diag(V)\n    \n    # Compute the inverse of V\n    V_inv = np.linalg.inv(V)\n    \n    # Compute posterior covariance \u03a3_b\n    Sigma_b = np.linalg.inv(V_inv + X.T @ X)\n    \n    # Compute posterior mean \u03bc_b\n    mu_b = Sigma_b @ (V_inv @ mu + X.T @ y)\n    \n    # Round the coefficients to 4 decimal places\n    return np.round(mu_b, 4).tolist()\n```\n\n### Explanation of the Code:\n1. **Input Handling**: The function accepts a design matrix `X`, a target vector `y`, and hyperparameters for the prior distribution. It also allows for an intercept term to be added to the design matrix.\n\n2. **Intercept Handling**: If `fit_intercept` is `True`, a column of ones is prepended to `X` to account for the intercept in the regression model.\n\n3. **Prior Mean and Scale**: The prior mean `mu` is broadcasted to match the number of features. The prior scale `V` is set to the identity matrix if not provided, or adjusted based on the input type (scalar, list, or full matrix).\n\n4. **Posterior Calculations**:\n   - The inverse of the prior scale `V` is computed.\n   - The posterior covariance `\u03a3_b` is calculated using the formula provided.\n   - The posterior mean `\u03bc_b` is computed using the closed-form expression.\n\n5. **Output**: The coefficients are rounded to four decimal places and returned as a list.\n\nThis implementation should work correctly for the specified requirements of Bayesian linear regression with an unknown noise variance."}
{"task_id": 308, "completion_id": 0, "completion": "Here's a complete implementation of the `mel_filterbank` function that constructs a Mel filterbank matrix according to the specifications provided:\n\n```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    The returned matrix has *n_filters* rows and *N//2 + 1* columns. Each row\n    is a triangular filter defined on the Mel scale. See the detailed task\n    description for the exact algorithm that has to be implemented.\n    \"\"\"\n    if max_freq is None:\n        max_freq = fs / 2\n\n    # Convert frequency limits to Mel scale\n    def hz_to_mel(f):\n        return 2595 * np.log10(1 + f / 700)\n\n    def mel_to_hz(m):\n        return 700 * (10**(m / 2595) - 1)\n\n    # Generate Mel scale points\n    mel_min = hz_to_mel(min_freq)\n    mel_max = hz_to_mel(max_freq)\n    mel_points = np.linspace(mel_min, mel_max, n_filters + 2)\n\n    # Convert Mel points back to Hz\n    hz_points = mel_to_hz(mel_points)\n\n    # Create the filterbank\n    filterbank = []\n    bin_frequencies = np.linspace(0, fs / 2, N // 2 + 1)\n\n    for i in range(1, n_filters + 1):\n        # Initialize the filter\n        filter_row = np.zeros(N // 2 + 1)\n        \n        # Left and right slopes\n        left_slope = (bin_frequencies - hz_points[i - 1]) / (hz_points[i] - hz_points[i - 1])\n        right_slope = (hz_points[i + 1] - bin_frequencies) / (hz_points[i + 1] - hz_points[i])\n        \n        # Calculate the filter weights\n        filter_row = np.minimum(left_slope, right_slope)\n        filter_row[filter_row < 0] = 0  # Only keep positive values\n        \n        # Normalize if required\n        if normalize:\n            filter_row *= 2 / (hz_points[i + 1] - hz_points[i - 1])\n        \n        filterbank.append(filter_row)\n\n    # Round the filterbank values to 4 decimal places and convert to list\n    filterbank = [[round(value, 4) for value in row] for row in filterbank]\n    \n    return filterbank\n```\n\n### Explanation of the Code:\n1. **Frequency to Mel Conversion**: The functions `hz_to_mel` and `mel_to_hz` convert frequencies between Hertz and the Mel scale.\n2. **Mel Points Generation**: We generate `n_filters + 2` equally spaced points on the Mel scale between the minimum and maximum frequencies.\n3. **Filter Construction**: For each filter, we calculate the left and right slopes based on the triangular filter definition. The minimum of these slopes gives the filter weights.\n4. **Normalization**: If normalization is enabled, we scale the filter weights so that the area under each filter equals 1.\n5. **Rounding and Returning**: Finally, we round the filter values to four decimal places and return the filterbank as a list of lists.\n\nThis implementation should work as expected and produce a Mel filterbank similar to those generated by audio processing libraries like LibROSA."}
{"task_id": 312, "completion_id": 0, "completion": "Here's the implementation of the Blackman\u2013Harris window generator in Python, following the specifications provided:\n\n```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    # Handle special cases\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n\n    # Coefficients for the Blackman-Harris window\n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n\n    # Calculate N\n    N = window_len - 1\n\n    # Generate the window\n    n = np.arange(window_len)\n    if symmetric:\n        # Symmetric form\n        window = (a0 \n                  - a1 * np.cos(2 * np.pi * n / N) \n                  + a2 * np.cos(4 * np.pi * n / N) \n                  - a3 * np.cos(6 * np.pi * n / N))\n    else:\n        # Periodic form\n        n = np.arange(window_len + 1)  # Generate one extra point\n        window = (a0 \n                  - a1 * np.cos(2 * np.pi * n / N) \n                  + a2 * np.cos(4 * np.pi * n / N) \n                  - a3 * np.cos(6 * np.pi * n / N))\n        window = window[:-1]  # Discard the last value\n\n    # Round the window coefficients to four decimal places\n    return list(np.round(window, 4))\n\n# Example usage\nprint(blackman_harris(10, symmetric=False))\nprint(blackman_harris(10, symmetric=True))\n```\n\n### Explanation:\n1. **Special Cases**: The function first checks for special cases where the window length is less than or equal to zero, returning an empty list, or equal to one, returning a list with a single value of 1.0.\n\n2. **Coefficients**: The fixed coefficients for the Blackman-Harris window are defined as per the formula.\n\n3. **Window Calculation**:\n   - For the **symmetric** form, the window is calculated directly using the formula.\n   - For the **periodic** form, an extra point is generated (length `L + 1`), and the last value is discarded to ensure the window is periodic.\n\n4. **Rounding**: Finally, the window coefficients are rounded to four decimal places and returned as a list.\n\nYou can test the function with different window lengths and the symmetric flag to see the results."}
{"task_id": 313, "completion_id": 0, "completion": "To implement the function `expected_log_dirichlet`, we need to compute the expectation of the logarithm of a Dirichlet-distributed random variable using the provided formula. This involves implementing the digamma function as specified and then using it to calculate the expected value.\n\nHere's the complete implementation:\n\n```python\nimport math\nimport numpy as np\n\ndef digamma(x):\n    \"\"\"Compute the digamma function using recursion and asymptotic expansion.\"\"\"\n    if x <= 0:\n        raise ValueError(\"Input must be positive.\")\n    \n    # Use recursion to shift x to a moderate value\n    while x < 6:\n        x += 1\n        # \u03c8(x) = \u03c8(x + 1) - 1/x\n    # Asymptotic expansion for large x\n    return math.log(x) - 1/(2*x) - 1/(12*x**2) + 1/(120*x**4) - 1/(252*x**6)\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    # Convert gamma to a NumPy array if it's a list for easier manipulation\n    gamma = np.array(gamma)\n    \n    # Get the specific gamma values\n    gamma_dt = gamma[d, t]\n    sum_gamma_d = np.sum(gamma[d, :])\n    \n    # Calculate the digamma values\n    digamma_gamma_dt = digamma(gamma_dt)\n    digamma_sum_gamma_d = digamma(sum_gamma_d)\n    \n    # Calculate the expected log\n    expected_log = digamma_gamma_dt - digamma_sum_gamma_d\n    \n    # Round the result to 4 decimal places and return\n    return round(expected_log, 4)\n\n# Example usage:\ngamma = [[1.0, 2.0, 3.0], [2.0, 3.0, 4.0]]\nd = 0\nt = 1\nresult = expected_log_dirichlet(gamma, d, t)\nprint(result)  # Output the result\n```\n\n### Explanation:\n1. **Digamma Function**: The `digamma` function is implemented using recursion to handle small values and an asymptotic expansion for larger values. This ensures that we can compute the digamma function accurately for a wide range of inputs.\n\n2. **Expected Logarithm Calculation**: The `expected_log_dirichlet` function retrieves the necessary parameters from the `gamma` matrix, computes the digamma values for the specified component and the sum of the parameters, and then calculates the expected logarithm.\n\n3. **Rounding**: The final result is rounded to four decimal places before being returned.\n\nThis implementation adheres to the constraints of not using external scientific libraries and provides a clear and efficient solution to the problem."}
{"task_id": 317, "completion_id": 0, "completion": "Here's the implementation of the `err_fmt` function that generates a formatted debug-error report based on the provided specifications:\n\n```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\n\n    See the task description for the exact required format.\n\n    Args:\n        params: A list where every element is a tuple (prediction, label).\n        golds:  A dictionary mapping each label to its gold/expected string.\n        ix:     The index inside `params` for which the detailed comparison\n                 is built.\n        warn_str: Optional extra text appended before the footer.\n\n    Returns:\n        A string that follows the specified layout exactly.\n    \"\"\"\n    # Determine the previous index\n    prev_ix = max(ix - 1, 0)\n    \n    # Get previous and current predictions and labels\n    prev_prediction, prev_label = params[prev_ix]\n    curr_prediction, curr_label = params[ix]\n    \n    # Get the gold text for the previous and current labels\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    \n    # Format the output string\n    report = (\n        \"------------------------- DEBUG -------------------------\\n\"\n        f\"Mine (prev) [{prev_label}]:\\n\"\n        f\"{prev_prediction}\\n\\n\"\n        f\"Theirs (prev) [{prev_label}]:\\n\"\n        f\"{prev_gold}\\n\\n\"\n        f\"Mine [{curr_label}]:\\n\"\n        f\"{curr_prediction}\\n\\n\"\n        f\"Theirs [{curr_label}]:\\n\"\n        f\"{curr_gold}{warn_str}\\n\"\n        \"----------------------- END DEBUG -----------------------\"\n    )\n    \n    return report\n```\n\n### Explanation of the Code:\n1. **Input Parameters**: The function takes four parameters: `params`, `golds`, `ix`, and an optional `warn_str`.\n2. **Previous Index Calculation**: It calculates the previous index using `max(ix - 1, 0)` to ensure it doesn't go below zero.\n3. **Data Extraction**: It extracts the previous and current predictions and labels from `params`, and retrieves the corresponding gold values from `golds`.\n4. **String Formatting**: It constructs the output string using formatted string literals (f-strings) to ensure the correct values are inserted into the specified format.\n5. **Return Value**: Finally, it returns the formatted string.\n\nThis implementation adheres strictly to the format specified in the task description, ensuring that all dashes and newlines are preserved exactly as required."}
{"task_id": 318, "completion_id": 0, "completion": "Here's a complete implementation of the AdaBoost.M1 algorithm using decision stumps as weak learners. The code follows the specifications provided in your task description.\n\n```python\nimport math\nimport numpy as np\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples containing the training labels. Each label\n        is either -1 or 1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds (weak learners) to use.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 or 1) for each sample in X_test.\n    \"\"\"\n    \n    n_samples, n_features = X_train.shape\n    # Initialize weights\n    weights = np.ones(n_samples) / n_samples\n    classifiers = []  # To store the weak classifiers\n    alphas = []       # To store the classifier weights\n\n    for _ in range(n_estimators):\n        best_stump = None\n        best_error = float('inf')\n        best_polarity = None\n        best_threshold = None\n\n        # Iterate over all features\n        for j in range(n_features):\n            feature_values = X_train[:, j]\n            thresholds = np.unique(feature_values)\n\n            # Test each threshold\n            for t in thresholds:\n                for p in [-1, 1]:  # polarity\n                    # Make predictions\n                    predictions = p * np.sign(feature_values - t)\n                    \n                    # Calculate the weighted error\n                    error = np.sum(weights * (predictions != y_train))\n\n                    # If the error is better than the best found so far, update\n                    if error < best_error:\n                        best_error = error\n                        best_stump = (j, t)\n                        best_polarity = p\n\n        # Calculate alpha\n        if best_error == 0:\n            alpha = float('inf')  # Perfect classifier\n        else:\n            alpha = 0.5 * math.log((1 - best_error) / best_error)\n\n        # Update weights\n        predictions = best_polarity * np.sign(X_train[:, best_stump[0]] - best_stump[1])\n        weights *= np.exp(-alpha * y_train * predictions)\n        weights /= np.sum(weights)  # Normalize weights\n\n        # Store the classifier and its weight\n        classifiers.append((best_stump, best_polarity))\n        alphas.append(alpha)\n\n    # Make predictions on the test set\n    final_predictions = np.zeros(X_test.shape[0])\n    \n    for m in range(n_estimators):\n        stump, polarity = classifiers[m]\n        j, t = stump\n        predictions = polarity * np.sign(X_test[:, j] - t)\n        final_predictions += alphas[m] * predictions\n\n    return list(np.sign(final_predictions).astype(int))\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 1], [4, 5]])\n# y_train = np.array([1, 1, -1, -1])\n# X_test = np.array([[1, 1], [3, 3]])\n# predictions = adaboost_predict(X_train, y_train, X_test, n_estimators=10)\n# print(predictions)  # Output will be a list of -1 and 1\n```\n\n### Explanation:\n1. **Initialization**: The weights for each sample are initialized uniformly.\n2. **Training Loop**: For each boosting round, we find the best decision stump (feature and threshold) that minimizes the weighted classification error.\n3. **Weight Update**: After finding the best stump, we update the weights of the samples based on the predictions of the stump.\n4. **Final Prediction**: After training, we combine the predictions of all the stumps using their respective weights to make predictions on the test set.\n\nThis implementation adheres to the constraints and requirements specified in your task."}
{"task_id": 329, "completion_id": 0, "completion": "Here's the implementation of the `dft_bins` function according to the specifications provided:\n\n```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When *True* return only the non-negative frequency bins; when\n        *False* return all *N* bins.  Default is *True*.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.  An empty\n        list is returned when *N* or *fs* is non-positive.\n    \"\"\"\n    if N <= 0 or fs <= 0:\n        return []\n\n    # Calculate the frequency resolution\n    delta_f = fs / N\n    \n    if positive_only:\n        # Generate positive frequencies\n        frequencies = np.arange(0, (N // 2) + 1) * delta_f\n    else:\n        # Generate full range of frequencies\n        frequencies = np.concatenate((np.arange(0, (N + 1) // 2) * delta_f,\n                                       np.arange(-(N // 2), 0) * delta_f))\n    \n    # Round to 4 decimal places and convert to list\n    return frequencies.round(4).tolist()\n\n# Example usage:\n# print(dft_bins(8, 44000, True))  # Should return positive frequencies\n# print(dft_bins(8, 44000, False)) # Should return all frequencies\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if `N` or `fs` is non-positive. If so, it returns an empty list.\n2. **Frequency Resolution Calculation**: It calculates the frequency resolution `\u0394f` as `fs / N`.\n3. **Frequency Bins Calculation**:\n   - If `positive_only` is `True`, it generates frequencies from `0` to `fs/2` (the non-negative part).\n   - If `positive_only` is `False`, it generates the full range of frequencies, including negative frequencies.\n4. **Rounding and Conversion**: The frequencies are rounded to 4 decimal places and converted to a Python list before being returned.\n\nThis implementation adheres to the requirements and should work correctly for the specified inputs."}
{"task_id": 331, "completion_id": 0, "completion": "Here's a Python function that computes the out-of-bag mean squared error (OOB-MSE) for a Random Forest regressor, following the specifications you've provided:\n\n```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    y_true = np.asarray(y_true)\n    predictions = np.asarray(predictions)\n    oob_mask = np.asarray(oob_mask)\n\n    # Initialize a list to store squared errors\n    squared_errors = []\n    \n    # Iterate over each sample\n    for i in range(predictions.shape[0]):\n        # Get the OOB predictions for the current sample\n        oob_preds = predictions[i, oob_mask[i]]\n        \n        # Check if there are any OOB predictions\n        if oob_preds.size > 0:\n            # Calculate the average of OOB predictions\n            avg_oob_pred = np.mean(oob_preds)\n            # Calculate the squared error\n            squared_error = (avg_oob_pred - y_true[i]) ** 2\n            squared_errors.append(squared_error)\n\n    # If no samples have OOB predictions, return -1\n    if not squared_errors:\n        return -1\n\n    # Calculate the OOB-MSE\n    oob_mse_value = np.mean(squared_errors)\n    \n    # Return the OOB-MSE rounded to 4 decimal places\n    return round(oob_mse_value, 4)\n```\n\n### Explanation:\n1. **Input Conversion**: The function converts the input lists to NumPy arrays for easier manipulation.\n2. **Squared Errors Calculation**: It iterates through each sample, checks for OOB predictions, computes the average of those predictions, and calculates the squared error against the true value.\n3. **Handling No OOB Predictions**: If no sample has OOB predictions, the function returns `-1`.\n4. **Final Calculation**: The mean of the squared errors is computed and returned, rounded to four decimal places.\n\n### Usage:\nYou can call this function by passing the true target values, the predictions from each tree, and the OOB mask as arguments. The function will return the OOB-MSE or `-1` if applicable."}
{"task_id": 332, "completion_id": 0, "completion": "Here's a complete implementation of the FP-Tree construction without using classes, following the specifications provided:\n\n```python\nfrom collections import Counter\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of transactions an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree. Each node contains two keys:\n        \"support\" and \"children\" (the latter mapping item \u2192 child-node).\n    \"\"\"\n    # Step 1: Calculate support for each item\n    item_support = Counter()\n    for transaction in transactions:\n        unique_items = set(transaction)  # Remove duplicates\n        item_support.update(unique_items)\n\n    # Step 2: Filter items by min_support\n    frequent_items = {item: support for item, support in item_support.items() if support >= min_support}\n\n    # Step 3: Order items by decreasing support and then alphabetically\n    ordered_items = sorted(frequent_items.items(), key=lambda x: (-x[1], x[0]))\n    item_order = [item for item, _ in ordered_items]\n\n    # Step 4: Initialize the root of the FP-Tree\n    fp_tree = {\"support\": 0, \"children\": {}}\n\n    # Step 5: Build the FP-Tree\n    for transaction in transactions:\n        # Remove duplicates and keep only frequent items\n        filtered_transaction = [item for item in transaction if item in frequent_items]\n        # Reorder according to the global ordering\n        ordered_transaction = sorted(filtered_transaction, key=lambda x: item_order.index(x))\n\n        # Update the FP-Tree\n        current_node = fp_tree\n        for item in ordered_transaction:\n            if item not in current_node[\"children\"]:\n                current_node[\"children\"][item] = {\"support\": 0, \"children\": {}}\n            current_node[\"children\"][item][\"support\"] += 1\n            current_node = current_node[\"children\"][item]\n\n    return fp_tree\n\n# Example usage\ntransactions = [\n    ['a', 'b', 'c'],\n    ['a', 'b'],\n    ['a', 'c'],\n    ['b', 'c'],\n    ['b'],\n    ['c']\n]\nmin_support = 2\nfp_tree = build_fp_tree(transactions, min_support)\nprint(fp_tree)\n```\n\n### Explanation of the Code:\n1. **Support Calculation**: We use `Counter` to count the occurrences of each item across all transactions.\n2. **Filtering**: We create a dictionary of items that meet or exceed the `min_support`.\n3. **Ordering**: We sort the items first by their support in descending order and then alphabetically to resolve ties.\n4. **FP-Tree Initialization**: We start with a root node that has a support of 0 and an empty dictionary for children.\n5. **Tree Construction**: For each transaction, we filter out non-frequent items, reorder the remaining items according to the global order, and then traverse the tree to update the support counts.\n\nThis implementation adheres to the requirements and constructs the FP-Tree as specified."}
{"task_id": 336, "completion_id": 0, "completion": "Here's a complete implementation of the `linear_kernel` function that meets the specified requirements:\n\n```python\nimport numpy as np\n\ndef linear_kernel(X: list[list[int | float]] | \"np.ndarray\", \n                  Y: list[list[int | float]] | \"np.ndarray\" | None = None, \n                  c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function must set\n        ``Y = X``.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y have\n        incompatible shapes (different number of columns).\n    \"\"\"\n    \n    # Convert inputs to numpy arrays for easier manipulation\n    X = np.array(X)\n    if Y is None:\n        Y = X\n    else:\n        Y = np.array(Y)\n    \n    # Validate shapes\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    \n    # Get the number of rows in X and Y\n    N = X.shape[0]\n    M = Y.shape[0]\n    \n    # Initialize the kernel matrix\n    K = np.zeros((N, M), dtype=float)\n    \n    # Compute the kernel matrix\n    for i in range(N):\n        for j in range(M):\n            K[i, j] = np.dot(X[i], Y[j]) + c0\n    \n    # Round the result to 4 decimal places and convert to list of lists\n    return K.round(4).tolist()\n\n# Example usage:\n# X = [[1, 2], [3, 4]]\n# Y = [[5, 6], [7, 8]]\n# print(linear_kernel(X, Y, c0=1))\n```\n\n### Explanation:\n1. **Input Handling**: The function accepts both lists and NumPy arrays. It converts the inputs to NumPy arrays for easier manipulation.\n2. **Shape Validation**: It checks if the number of columns in `X` and `Y` are the same. If not, it returns `-1`.\n3. **Kernel Matrix Calculation**: It initializes a zero matrix `K` of shape `(N, M)` and fills it by calculating the dot product of each pair of vectors from `X` and `Y`, adding the constant `c0`.\n4. **Rounding**: The resulting kernel matrix is rounded to four decimal places and converted back to a list of lists before returning.\n\n### Edge Cases:\n- If `X` is empty, the function will return an empty list since `N` will be `0`, and the kernel matrix will also have `0` rows.\n- If `Y` is not provided, it defaults to `X`, resulting in a symmetric kernel matrix."}
{"task_id": 340, "completion_id": 0, "completion": "Here's the implementation of the single-step Adam optimizer as per your requirements. The function performs the Adam update step and returns the updated parameter along with the new moment estimates, all rounded to 8 decimal places.\n\n```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_parameter, new_moment, new_variance) **as\n        python lists**, each rounded to 8 decimal places.\n    \"\"\"\n    \n    # Update the first moment estimate\n    m_t = beta1 * m + (1 - beta1) * grad\n    \n    # Update the second moment estimate\n    v_t = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m_t / (1 - beta1 ** t)\n    \n    # Compute bias-corrected second moment estimate\n    v_hat = v_t / (1 - beta2 ** t)\n    \n    # Update the parameter \u03b8\n    if np.any(grad != 0):  # Only update if gradient is not zero\n        param = param - lr * m_hat / (np.sqrt(v_hat) + eps)\n    \n    # Round results to 8 decimal places and convert to lists\n    updated_param = np.round(param, 8).tolist()\n    new_m = np.round(m_t, 8).tolist()\n    new_v = np.round(v_t, 8).tolist()\n    \n    return updated_param, new_m, new_v\n```\n\n### Explanation:\n1. **Moment Estimates**: The first moment (`m_t`) and second moment (`v_t`) are updated using the provided gradient and the decay rates.\n2. **Bias Correction**: The bias-corrected estimates (`m_hat` and `v_hat`) are computed to account for the initialization bias in the first few iterations.\n3. **Parameter Update**: The parameter is updated only if the gradient is not zero. If the gradient is zero, the parameter remains unchanged, but the moment estimates are still updated.\n4. **Rounding and Conversion**: Finally, the updated parameter and moment estimates are rounded to 8 decimal places and converted to lists before being returned.\n\nThis implementation is fully vectorized and works with multi-dimensional parameters."}
{"task_id": 343, "completion_id": 0, "completion": "Here's the implementation of the `cross_entropy_update` function that follows the requirements specified in your task. This function retains the top samples based on their rewards, computes the mean and variance of those samples, and returns them as lists rounded to four decimal places.\n\n```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    # Convert rewards to a NumPy array for easier manipulation\n    rewards = np.array(rewards)\n    \n    # Determine the number of samples to retain\n    N = theta_samples.shape[0]\n    num_to_retain = int(retain_prcnt * N)\n    \n    # Get the indices of the top rewards\n    elite_indices = np.argsort(rewards)[-num_to_retain:]\n    \n    # Select the elite samples\n    elite_samples = theta_samples[elite_indices]\n    \n    # Calculate the mean and variance of the elite samples\n    mean = np.mean(elite_samples, axis=0)\n    variance = np.var(elite_samples, axis=0)\n    \n    # Round the results to four decimal places and convert to lists\n    mean_rounded = np.round(mean, 4).tolist()\n    variance_rounded = np.round(variance, 4).tolist()\n    \n    return mean_rounded, variance_rounded\n```\n\n### Explanation:\n1. **Input Conversion**: The `rewards` are converted to a NumPy array for easier manipulation.\n2. **Sample Retention**: The number of samples to retain is calculated using the `retain_prcnt` parameter. The indices of the top rewards are obtained using `np.argsort`.\n3. **Elite Samples Selection**: The elite samples are selected based on the indices of the top rewards.\n4. **Mean and Variance Calculation**: The mean and variance of the elite samples are computed using `np.mean` and `np.var`, respectively.\n5. **Rounding and Returning**: The results are rounded to four decimal places and converted to lists before being returned.\n\nThis function can be used in the context of the Cross-Entropy Method for updating policy parameters in reinforcement learning."}
{"task_id": 353, "completion_id": 0, "completion": "Here's the implementation of the `entropy` function that calculates the Shannon entropy of a sequence of class labels. The function handles the special cases as described and uses the formula for entropy to compute the result.\n\n```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    # Convert labels to a list if it's not already one\n    labels = list(labels)\n    \n    # Special case: if the input is empty, return 0.0\n    if not labels:\n        return 0.0\n    \n    # Count the occurrences of each label\n    label_counts = Counter(labels)\n    total_count = len(labels)\n    \n    # Special case: if all labels are identical, the entropy is 0.0\n    if len(label_counts) == 1:\n        return 0.0\n    \n    # Calculate the entropy\n    entropy_value = 0.0\n    for count in label_counts.values():\n        probability = count / total_count\n        entropy_value -= probability * math.log2(probability)\n    \n    # Return the entropy rounded to 5 decimal places\n    return round(entropy_value, 5)\n\n# Example usage:\n# print(entropy(['A', 'A', 'B', 'B', 'B']))  # Should return a value based on the distribution\n```\n\n### Explanation:\n1. **Input Handling**: The function first converts the input to a list to ensure it can handle any iterable type.\n2. **Empty Input**: If the input is empty, it returns `0.0`.\n3. **Counting Labels**: It uses `Counter` from the `collections` module to count occurrences of each label.\n4. **Identical Labels**: If all labels are the same (i.e., the length of `label_counts` is 1), it returns `0.0`.\n5. **Entropy Calculation**: It calculates the entropy using the formula provided, iterating over the counts of each label to compute the probabilities and their contributions to the entropy.\n6. **Rounding**: Finally, it rounds the result to five decimal places before returning it.\n\nThis implementation is efficient and handles the specified edge cases correctly."}
{"task_id": 354, "completion_id": 0, "completion": "Here's a complete implementation of the `conv2D` function that performs 2-D convolution using the im2col + GEMM strategy. The function handles various padding specifications and dilation as described in your requirements.\n\n```python\nimport numpy as np\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"Performs a 2-D convolution (cross-correlation).\n\n    Args:\n        X: NumPy array of shape ``(n_ex, in_rows, in_cols, in_ch)`` representing the input batch.\n        W: NumPy array of shape ``(kernel_rows, kernel_cols, in_ch, out_ch)`` containing the kernels.\n        stride: Stride of the convolution.\n        pad: Padding specification \u2013 integer, tuple or the string ``'same'``.\n        dilation: Optional dilation factor. ``0`` corresponds to a normal convolution.\n\n    Returns:\n        The convolution result **as a Python list** obtained through ``numpy.ndarray.tolist()``.\n    \"\"\"\n    \n    n_ex, in_rows, in_cols, in_ch = X.shape\n    kernel_rows, kernel_cols, _, out_ch = W.shape\n\n    # Determine padding\n    if pad == 'same':\n        out_rows = int(np.ceil(in_rows / stride))\n        out_cols = int(np.ceil(in_cols / stride))\n        pad_h = max((out_rows - 1) * stride + kernel_rows - in_rows, 0)\n        pad_w = max((out_cols - 1) * stride + kernel_cols - in_cols, 0)\n        pr = pad_h // 2\n        pc = pad_w // 2\n        padding = (pr, pr, pc, pc)\n    elif isinstance(pad, int):\n        padding = (pad, pad, pad, pad)\n    elif isinstance(pad, tuple):\n        if len(pad) == 2:\n            pr, pc = pad\n            padding = (pr, pr, pc, pc)\n        elif len(pad) == 4:\n            padding = pad\n        else:\n            raise ValueError(\"Invalid padding tuple length.\")\n    else:\n        raise ValueError(\"Invalid padding specification.\")\n\n    # Apply padding\n    X_padded = np.pad(X, ((0, 0), (padding[0], padding[1]), (padding[2], padding[3]), (0, 0)), mode='constant')\n\n    # Calculate output dimensions\n    out_rows = (in_rows + padding[0] + padding[1] - kernel_rows) // stride + 1\n    out_cols = (in_cols + padding[2] + padding[3] - kernel_cols) // stride + 1\n\n    # Prepare im2col\n    cols = []\n    for ex in range(n_ex):\n        for h in range(out_rows):\n            for w in range(out_cols):\n                h_start = h * stride\n                w_start = w * stride\n                h_end = h_start + kernel_rows + dilation * (kernel_rows - 1)\n                w_end = w_start + kernel_cols + dilation * (kernel_cols - 1)\n\n                # Extract the patch\n                patch = X_padded[ex, h_start:h_end:dilation+1, w_start:w_end:dilation+1, :]\n                cols.append(patch.flatten())\n    \n    cols = np.array(cols)\n\n    # Reshape W for matrix multiplication\n    W_reshaped = W.reshape(-1, out_ch)\n\n    # Perform the matrix multiplication\n    output = cols @ W_reshaped\n\n    # Reshape output to the desired shape\n    output = output.reshape(n_ex, out_rows, out_cols, out_ch)\n\n    return output.tolist()\n```\n\n### Explanation:\n1. **Padding Handling**: The function calculates the necessary padding based on the input specifications. It supports integer padding, tuple padding, and the 'same' padding option.\n2. **Padding Application**: The input array `X` is padded using `np.pad`.\n3. **Output Dimensions Calculation**: The output dimensions are calculated based on the input dimensions, kernel size, stride, and padding.\n4. **im2col Implementation**: The function extracts patches from the padded input using the specified stride and dilation, flattening them into columns for matrix multiplication.\n5. **Matrix Multiplication**: The reshaped kernels are multiplied with the extracted columns to produce the output.\n6. **Output Reshaping**: The output is reshaped to the desired dimensions and converted to a Python list.\n\nThis implementation should work efficiently for the specified task."}
{"task_id": 355, "completion_id": 0, "completion": "Here's a complete implementation of the PCA with Deterministic Sign Fix as per your requirements:\n\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array in which rows correspond to samples and columns to features.\n    n_components : int\n        Number of principal components to retain (must be \u22651).\n    solver : str, optional (default=\"svd\")\n        ``\"svd\"`` to use singular value decomposition or ``\"eigen\"`` to use\n        eigen-decomposition of the covariance matrix.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 The projected data (scores) as a list of lists \u2013 each inner list is a\n          sample represented in the new sub-space.\n        \u2022 The list of explained variance ratios corresponding to the kept\n          components.\n    \"\"\"\n    # Step 1: Standardize the data (mean centering)\n    data_meaned = data - np.mean(data, axis=0)\n    \n    # Step 2: Obtain principal directions\n    if solver == \"svd\":\n        U, S, Vt = np.linalg.svd(data_meaned, full_matrices=False)\n        components = Vt.T  # Transpose to get components as columns\n        explained_variance = (S ** 2) / (data.shape[0] - 1)\n    elif solver == \"eigen\":\n        covariance_matrix = np.cov(data_meaned, rowvar=False, ddof=1)\n        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n        components = eigenvectors[:, ::-1]  # Sort eigenvectors in descending order\n        explained_variance = eigenvalues[::-1]  # Sort eigenvalues in descending order\n    else:\n        raise ValueError(\"Solver must be either 'svd' or 'eigen'.\")\n\n    # Step 3: Sort components and explained variance\n    n_components = min(n_components, components.shape[1])  # Ensure we do not exceed available components\n    components = components[:, :n_components]\n    explained_variance = explained_variance[:n_components]\n    \n    # Step 4: Deterministic sign fix\n    for i in range(n_components):\n        if np.any(components[:, i] < 0):\n            if components[components[:, i] != 0, i][0] < 0:\n                components[:, i] *= -1\n\n    # Step 5: Project the data onto the retained directions\n    scores = np.dot(data_meaned, components)\n    \n    # Step 6: Prepare the output\n    explained_variance_ratio = explained_variance / np.sum(explained_variance)\n    \n    # Round results to 4 decimals and convert to lists\n    scores_rounded = np.round(scores, 4).tolist()\n    explained_variance_ratio_rounded = np.round(explained_variance_ratio, 4).tolist()\n    \n    return scores_rounded, explained_variance_ratio_rounded\n```\n\n### Explanation of the Code:\n1. **Mean Centering**: The data is centered by subtracting the mean of each feature.\n2. **Principal Directions Calculation**:\n   - For `svd`, we use `numpy.linalg.svd` to decompose the centered data.\n   - For `eigen`, we compute the covariance matrix and use `numpy.linalg.eigh` to get eigenvalues and eigenvectors.\n3. **Sorting**: The components and their corresponding explained variance are sorted in descending order.\n4. **Deterministic Sign Fix**: Each component's sign is adjusted to ensure consistency across runs.\n5. **Projection**: The centered data is projected onto the selected principal components.\n6. **Output Preparation**: The scores and explained variance ratios are rounded to four decimal places and converted to lists.\n\nThis implementation meets all the specified requirements and constraints."}
{"task_id": 356, "completion_id": 0, "completion": "To complete the `leaf_predict` function, we need to handle two scenarios based on the `classifier` argument:\n\n1. If `classifier` is `True`, we will assume that the `value` attribute of the `Leaf` instance contains a list or tuple of class probabilities. We will return the index of the class with the highest probability. In case of ties, we will return the smallest index, which is the default behavior of `numpy.argmax`.\n\n2. If `classifier` is `False`, we will simply return the scalar value stored in the `value` attribute of the `Leaf` instance.\n\nHere's the completed code:\n\n```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        # Return the index of the maximum probability\n        return int(np.argmax(leaf.value))\n    else:\n        # Return the raw scalar value\n        return leaf.value\n```\n\n### Explanation:\n- We check if `classifier` is `True`. If it is, we use `numpy.argmax` to find the index of the maximum value in `leaf.value`, which is expected to be a list or tuple of probabilities. We convert the result to an `int` to match the expected return type.\n- If `classifier` is `False`, we simply return the `value` attribute, which is expected to be a scalar (either `float` or `int`).\n\n### Note:\nMake sure to import `numpy` at the beginning of your script if you haven't done so already:\n\n```python\nimport numpy as np\n```\n\nThis implementation will correctly handle both classification and regression predictions based on the structure of the `Leaf` instance."}
{"task_id": 357, "completion_id": 0, "completion": "Here's a complete implementation of the K-Means clustering algorithm as described in your task. The function initializes the centroids, assigns samples to the nearest centroid, updates the centroids, and checks for convergence. Finally, it sorts and rounds the centroids before returning them.\n\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    n_samples, n_features = data.shape\n    \n    # Step 1: Initialize centroids with the first K samples\n    centroids = data[:K].copy()\n    \n    for _ in range(max_iters):\n        # Step 2: Assign samples to the closest centroid\n        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        # Step 3: Recompute centroids\n        new_centroids = np.array([\n            data[labels == k].mean(axis=0) if np.any(labels == k) else centroids[k]\n            for k in range(K)\n        ])\n        \n        # Check for convergence (if centroids do not change)\n        if np.all(np.abs(new_centroids - centroids) < 1e-6):\n            break\n        \n        centroids = new_centroids\n    \n    # Sort centroids lexicographically and round to 4 decimals\n    centroids = np.round(centroids, 4)\n    sorted_centroids = sorted(centroids.tolist())\n    \n    return sorted_centroids\n\n# Example usage\ndata = np.array([[1, 1],\n                 [1, 2],\n                 [2, 1],\n                 [8, 8],\n                 [9, 8],\n                 [8, 9]])\nK = 2\nresult = kmeans(data, K)\nprint(result)  # Output: [[1.3333, 1.3333], [8.3333, 8.3333]]\n```\n\n### Explanation of the Code:\n1. **Initialization**: The centroids are initialized using the first `K` samples from the data.\n2. **Distance Calculation**: The Euclidean distance from each sample to each centroid is calculated using broadcasting.\n3. **Label Assignment**: Each sample is assigned to the nearest centroid based on the calculated distances.\n4. **Centroid Update**: The centroids are updated by calculating the mean of the samples assigned to each centroid. If a centroid has no samples assigned, it remains unchanged.\n5. **Convergence Check**: The algorithm checks if the centroids have changed significantly. If not, it breaks out of the loop early.\n6. **Sorting and Rounding**: Finally, the centroids are sorted lexicographically and rounded to four decimal places before being returned as a list of lists.\n\nThis implementation is efficient and adheres to the specifications provided in your task description."}
{"task_id": 362, "completion_id": 0, "completion": "Here's a Python function that meets the requirements for converting a 2-D list or NumPy array into a row-stochastic matrix. The function handles the normalization process, checks for rows that sum to zero, and rounds the results to four decimal places before returning them as a native Python list.\n\n```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | np.ndarray) -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation. If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    # Convert input data to a NumPy array of floats\n    matrix = np.array(data, dtype=float)\n    \n    # Compute the sum of each row\n    row_sums = matrix.sum(axis=1)\n    \n    # Check for any row that sums to 0\n    if np.any(row_sums == 0):\n        return -1\n    \n    # Normalize each row by its sum\n    normalized_matrix = matrix / row_sums[:, np.newaxis]\n    \n    # Round the result to 4 decimal places\n    rounded_matrix = np.round(normalized_matrix, 4)\n    \n    # Convert the result back to a native Python list\n    return rounded_matrix.tolist()\n```\n\n### Explanation of the Code:\n1. **Input Conversion**: The input data is converted to a NumPy array of floats to facilitate numerical operations.\n2. **Row Sum Calculation**: The sum of each row is calculated using `matrix.sum(axis=1)`.\n3. **Zero Row Check**: The function checks if any row sum is zero using `np.any(row_sums == 0)`. If so, it returns `-1`.\n4. **Normalization**: Each element in a row is divided by the sum of that row. The `[:, np.newaxis]` is used to ensure proper broadcasting.\n5. **Rounding**: The resulting matrix is rounded to four decimal places using `np.round()`.\n6. **Return Format**: Finally, the normalized and rounded matrix is converted back to a native Python list using `tolist()` and returned.\n\n### Example Usage:\n```python\ndata = [[1, 2, 3], [4, 5, 6]]\nresult = row_stochastic_matrix(data)\nprint(result)  # Output: [[0.1667, 0.3333, 0.5], [0.2667, 0.3333, 0.4]]\n```\n\nThis function will work for any 2-D list or NumPy array of non-negative numbers, ensuring that the output is a valid row-stochastic matrix or `-1` if normalization is not possible."}
{"task_id": 363, "completion_id": 0, "completion": "To implement the `is_continuous` function, we need to check the types of the action and observation spaces in the provided environment. The function will determine if these spaces are continuous based on the rules specified in the task description.\n\nHere's how we can implement the function:\n\n1. If `tuple_action` is `True`, we need to check if the action space is a `Tuple` or `Dict`. If it is, we will iterate through its sub-spaces and check if all of them are instances of `Box`. If any sub-space is not a `Box`, the action space is not continuous.\n\n2. If `tuple_action` is `False`, we simply check if the action space is an instance of `Box`.\n\n3. We perform a similar check for the observation space using the `tuple_obs` flag.\n\nHere is the complete implementation of the `is_continuous` function:\n\n```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool) -> PyTuple[bool, bool]:\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    \n    def is_space_continuous(space: Space) -> bool:\n        if isinstance(space, Box):\n            return True\n        elif isinstance(space, Tuple):\n            return all(is_space_continuous(sub_space) for sub_space in space.spaces)\n        elif isinstance(space, Dict):\n            return all(is_space_continuous(sub_space) for sub_space in space.spaces.values())\n        else:\n            return False\n\n    # Check action space\n    if tuple_action:\n        cont_action = is_space_continuous(env.action_space)\n    else:\n        cont_action = isinstance(env.action_space, Box)\n\n    # Check observation space\n    if tuple_obs:\n        cont_obs = is_space_continuous(env.observation_space)\n    else:\n        cont_obs = isinstance(env.observation_space, Box)\n\n    return cont_action, cont_obs\n```\n\n### Explanation:\n- The `is_space_continuous` helper function checks if a given space is continuous. It handles `Box`, `Tuple`, and `Dict` types accordingly.\n- The main function `is_continuous` uses this helper function to determine the continuity of the action and observation spaces based on the provided flags.\n- Finally, it returns a tuple of booleans indicating whether each space is continuous."}
{"task_id": 369, "completion_id": 0, "completion": "Here's the implementation of the `gradient_boosting_predict` function that meets the specified requirements for both regression and multi-class classification:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    # Initialize the running prediction with zeros\n    running_pred = np.zeros_like(updates[0])\n\n    # Aggregate the updates\n    for u in updates:\n        running_pred -= learning_rate * u\n\n    if regression:\n        # For regression, return the rounded predictions\n        return np.round(running_pred, 4)\n    else:\n        # For classification, apply softmax and return the predicted class labels\n        exp_pred = np.exp(running_pred - np.max(running_pred, axis=1, keepdims=True))  # Stability improvement\n        softmax_pred = exp_pred / np.sum(exp_pred, axis=1, keepdims=True)\n        return np.argmax(softmax_pred, axis=1)\n\n# Example usage:\n# updates = [np.array([0.1, 0.2]), np.array([0.3, 0.4])]\n# learning_rate = 0.1\n# print(gradient_boosting_predict(updates, learning_rate, regression=True))\n# print(gradient_boosting_predict(updates, learning_rate, regression=False))\n```\n\n### Explanation:\n1. **Initialization**: The `running_pred` array is initialized to zeros with the same shape as the first update array.\n2. **Aggregation**: For each update in the `updates` list, the scaled output (using the `learning_rate`) is subtracted from `running_pred`.\n3. **Regression Handling**: If `regression` is `True`, the function rounds the final predictions to 4 decimal places and returns them.\n4. **Classification Handling**: If `regression` is `False`, the function applies the softmax function to convert logits into probabilities and then uses `argmax` to get the predicted class labels.\n\n### Note:\n- The softmax implementation includes a stability improvement by subtracting the maximum value from the logits before exponentiation, which helps prevent overflow issues when dealing with large values."}
{"task_id": 371, "completion_id": 0, "completion": "Here's a complete implementation of the `conv1D` function that performs 1-D cross-correlation with support for batches, multiple input/output channels, strides, zero padding, and dilation. The function returns the result as a nested Python list.\n\n```python\nimport numpy as np\n\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\n\n    The function must support batches, multiple input/output channels, padding\n    (integer, tuple or \"same\"), arbitrary stride and dilation.  It should return\n    the output volume as *list* obtained via ``ndarray.tolist()``.\n\n    Args:\n        X: ndarray of shape (n_examples, signal_length, in_channels)\n        W: ndarray of shape (kernel_width, in_channels, out_channels)\n        stride: positive integer, the step size of the sliding window\n        pad:  int, 2-tuple, or the string \"same\" specifying the amount of zero\n              padding to add to the left and right of the signal\n        dilation: non-negative integer, number of points inserted between\n                   neighbouring kernel elements (0 \u21d2 standard convolution)\n\n    Returns:\n        A Python list representing the convolved volume with shape\n        (n_examples, output_length, out_channels).\n    \"\"\"\n    \n    n_examples, signal_length, in_channels = X.shape\n    kernel_width, _, out_channels = W.shape\n\n    # Calculate effective kernel width considering dilation\n    effective_kernel_width = (kernel_width - 1) * dilation + 1\n\n    # Determine padding\n    if isinstance(pad, int):\n        p_left = p_right = pad\n    elif isinstance(pad, tuple):\n        p_left, p_right = pad\n    elif pad == \"same\":\n        out_len = (signal_length + stride - 1) // stride\n        total_pad = max(0, (out_len - 1) * stride + effective_kernel_width - signal_length)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n    else:\n        raise ValueError(\"Padding must be an integer, a tuple, or 'same'.\")\n\n    # Pad the input signal\n    X_padded = np.pad(X, ((0, 0), (p_left, p_right), (0, 0)), mode='constant')\n\n    # Calculate output dimensions\n    output_length = (signal_length + p_left + p_right - effective_kernel_width) // stride + 1\n\n    # Initialize output\n    output = np.zeros((n_examples, output_length, out_channels))\n\n    # Perform cross-correlation\n    for i in range(output_length):\n        for j in range(out_channels):\n            for k in range(in_channels):\n                # Calculate the start index for the current slice\n                start_index = i * stride\n                # Extract the relevant slice from the padded input\n                x_slice = X_padded[:, start_index:start_index + effective_kernel_width, k]\n                # Perform the cross-correlation\n                output[:, i, j] += np.sum(x_slice * W[:, k, j], axis=1)\n\n    return output.tolist()\n```\n\n### Explanation:\n1. **Input Parameters**:\n   - `X`: Input data of shape `(n_examples, signal_length, in_channels)`.\n   - `W`: Kernel weights of shape `(kernel_width, in_channels, out_channels)`.\n   - `stride`: Step size for the sliding window.\n   - `pad`: Padding specification (integer, tuple, or \"same\").\n   - `dilation`: Number of zeros inserted between kernel elements.\n\n2. **Padding Calculation**:\n   - The function calculates the required padding based on the input parameters. If \"same\" padding is specified, it computes the necessary left and right padding to maintain the output size.\n\n3. **Padded Input**:\n   - The input signal is padded using `np.pad`.\n\n4. **Output Dimensions**:\n   - The output dimensions are calculated based on the input size, padding, kernel size, and stride.\n\n5. **Cross-Correlation**:\n   - The function iterates over the output dimensions and computes the cross-correlation for each output channel by summing the product of the input slice and the kernel weights.\n\n6. **Return Value**:\n   - Finally, the output is converted to a nested Python list using `tolist()` and returned.\n\nThis implementation should work efficiently for the specified requirements."}
{"task_id": 373, "completion_id": 0, "completion": "Here's a Python function that calculates the Gini impurity of a discrete label sequence as per your requirements:\n\n```python\nimport numpy as np\n\ndef gini(y: list[int] | np.ndarray) -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    if len(y) == 0:\n        return 0.0\n    \n    # Convert to a numpy array if it's a list\n    if isinstance(y, list):\n        y = np.array(y)\n    \n    # Count the occurrences of each class\n    classes, counts = np.unique(y, return_counts=True)\n    \n    # Calculate the total number of samples\n    N = len(y)\n    \n    # Calculate the relative frequencies\n    probabilities = counts / N\n    \n    # Calculate Gini impurity\n    gini_impurity = 1 - np.sum(probabilities ** 2)\n    \n    return round(gini_impurity, 4)\n\n# Example usage:\n# print(gini([0, 1, 1, 2, 2, 2]))  # Output: 0.6111\n```\n\n### Explanation:\n1. **Input Handling**: The function accepts either a list or a NumPy array. If the input is a list, it converts it to a NumPy array for easier manipulation.\n2. **Empty Check**: If the input sequence is empty, it returns `0.0` as specified.\n3. **Class Counting**: It uses `np.unique` to find the unique classes and their counts.\n4. **Probability Calculation**: It calculates the relative frequencies of each class.\n5. **Gini Calculation**: It computes the Gini impurity using the formula provided.\n6. **Rounding**: Finally, it rounds the result to four decimal places before returning it.\n\nYou can test the function with different inputs to verify its correctness."}
{"task_id": 374, "completion_id": 0, "completion": "Here's a Python function that implements multiple linear regression using batch gradient descent according to the specifications provided:\n\n```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Target vector of shape (m,).\n        learning_rate (float): The learning rate for gradient descent.\n        n_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n        tuple: A tuple containing the learned weights and bias, both rounded to 4 decimal places.\n               Returns -1 if the input data is inconsistent.\n    \"\"\"\n    \n    # Check if the number of samples in X and y are consistent\n    if X.shape[0] != y.shape[0]:\n        return -1\n    \n    m, n = X.shape  # m is the number of samples, n is the number of features\n    theta = np.zeros(n)  # Initialize weights (n zeros)\n    b = 0.0  # Initialize bias (0)\n\n    # Gradient descent\n    for _ in range(n_iterations):\n        y_hat = X.dot(theta) + b  # Predicted values\n        error = y_hat - y  # Error term\n        \n        # Calculate gradients\n        dw = (2/m) * X.T.dot(error)  # Gradient for weights\n        db = (2/m) * np.sum(error)    # Gradient for bias\n        \n        # Update weights and bias\n        theta -= learning_rate * dw\n        b -= learning_rate * db\n\n    # Round the results to 4 decimal places\n    theta_rounded = np.round(theta, 4).tolist()\n    b_rounded = round(b, 4)\n\n    return (theta_rounded, b_rounded)\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the number of samples in `X` and `y` are consistent. If not, it returns `-1`.\n2. **Initialization**: It initializes the weight vector `theta` with zeros and the bias `b` with zero.\n3. **Gradient Descent Loop**: For a specified number of iterations, it computes the predicted values, calculates the error, computes the gradients for `theta` and `b`, and updates them accordingly.\n4. **Rounding**: Finally, it rounds the learned parameters to four decimal places before returning them as a tuple.\n\n### Usage:\nYou can call this function by passing a feature matrix `X` and a target vector `y`, along with optional parameters for the learning rate and number of iterations."}
{"task_id": 376, "completion_id": 0, "completion": "Here's a Python function that checks if a given 2-D numeric array is symmetric with respect to its main diagonal. The function uses NumPy to handle the array and to check for symmetry while considering floating-point precision issues.\n\n```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Convert the list of lists to a NumPy array\n    matrix = np.array(X)\n    \n    # Check if the matrix is square\n    if matrix.shape[0] != matrix.shape[1]:\n        return False\n    \n    # Check for symmetry using NumPy's allclose for floating-point comparison\n    return np.allclose(matrix, matrix.T, atol=1e-8)\n\n# Example usage:\n# print(is_symmetric([[1, 2, 3], [2, 4, 5], [3, 5, 6]]))  # Should return True\n# print(is_symmetric([[1, 2], [3, 4]]))  # Should return False\n```\n\n### Explanation:\n1. **Input Conversion**: The input list of lists is converted to a NumPy array for easier manipulation and comparison.\n2. **Square Check**: The function checks if the number of rows is equal to the number of columns. If not, it returns `False`.\n3. **Symmetry Check**: The function uses `np.allclose` to compare the matrix with its transpose (`matrix.T`). The `atol` parameter is set to `1e-8` to account for floating-point precision issues.\n4. **Return Value**: The function returns `True` if the matrix is symmetric and `False` otherwise. \n\nYou can test the function with various matrices to see if it behaves as expected."}
{"task_id": 377, "completion_id": 0, "completion": "Here's the implementation of the function that calculates the negative gradient for the logistic loss as described in your task. The function checks for valid binary labels and computes the negative gradient accordingly.\n\n```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Calculate the negative gradient of the logistic loss for binary classification.\n\n    Parameters:\n    y (list): A list of binary class labels (0 or 1).\n    f (list): A list of current prediction scores (floats).\n\n    Returns:\n    list: A list of negative gradients rounded to four decimal places.\n          Returns -1 if the label array contains values other than 0 or 1.\n    \"\"\"\n    # Convert input lists to numpy arrays for easier manipulation\n    y = np.array(y)\n    f = np.array(f)\n    \n    # Check if y contains only 0s and 1s\n    if not np.all(np.isin(y, [0, 1])):\n        return -1\n    \n    # Convert y from {0, 1} to {-1, 1}\n    y_prime = 2 * y - 1\n    \n    # Calculate the negative gradient\n    g = y_prime / (1 + np.exp(-y_prime * f))\n    \n    # Round the results to four decimal places and convert to list\n    return np.round(g, 4).tolist()\n\n# Example usage:\n# y = [0, 1, 1, 0]\n# f = [0.5, -0.2, 0.3, 0.1]\n# print(logistic_negative_gradient(y, f))\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the input labels `y` contain only binary values (0 or 1). If not, it returns -1.\n2. **Label Transformation**: The labels are transformed from {0, 1} to {-1, 1} using the formula `y' = 2y - 1`.\n3. **Negative Gradient Calculation**: The negative gradient is computed using the formula provided: \n   \\[\n   g_i = \\frac{y'_i}{1 + \\exp(-y'_i f_i)}\n   \\]\n4. **Rounding**: The results are rounded to four decimal places and returned as a list.\n\nYou can test the function with different inputs to ensure it behaves as expected."}
{"task_id": 380, "completion_id": 0, "completion": "Here's the implementation of the Scaled Exponential Linear Unit (SELU) activation function along with its first and second derivatives. The function handles scalar inputs, 1-D arrays, and multi-D arrays while preserving the input shape. The results are rounded to four decimal places and returned as Python lists.\n\n```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    # Constants for SELU\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n    \n    # Convert input to a NumPy array for vectorized operations\n    x = np.asarray(x)\n    \n    if order == 0:\n        # SELU activation\n        selu_result = np.where(x > 0, scale * x, scale * (alpha * np.exp(x) - alpha))\n        return np.round(selu_result, 4).tolist()\n    \n    elif order == 1:\n        # First derivative of SELU\n        first_derivative = np.where(x > 0, scale, scale * alpha * np.exp(x))\n        return np.round(first_derivative, 4).tolist()\n    \n    elif order == 2:\n        # Second derivative of SELU\n        second_derivative = np.where(x > 0, 0, scale * alpha * np.exp(x))\n        return np.round(second_derivative, 4).tolist()\n    \n    else:\n        raise ValueError(\"Order must be 0, 1, or 2.\")\n\n# Example usage:\n# print(selu([1, -1, 0], order=0))  # SELU activation\n# print(selu([1, -1, 0], order=1))  # First derivative\n# print(selu([1, -1, 0], order=2))  # Second derivative\n```\n\n### Explanation:\n1. **Constants**: The constants `alpha` and `scale` are defined as per the SELU activation function.\n2. **Input Handling**: The input `x` is converted to a NumPy array to facilitate element-wise operations.\n3. **Order Handling**:\n   - For `order = 0`, the SELU activation is computed using the defined piecewise function.\n   - For `order = 1`, the first derivative is computed based on the conditions for `x`.\n   - For `order = 2`, the second derivative is computed similarly.\n4. **Output**: The results are rounded to four decimal places and converted to a list using `tolist()`.\n\nThis implementation is efficient and works seamlessly with scalars, lists, and NumPy arrays."}
{"task_id": 387, "completion_id": 0, "completion": "Here's a complete implementation of a one-dimensional Gradient Boosting Regressor using decision stumps, as per your specifications:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\n\n    Args:\n        x: A list with a single numeric feature for every sample.\n        y: Target values.\n        n_estimators: Number of boosting rounds.\n        learning_rate: Shrinkage applied to every stump (\u03b7).\n\n    Returns:\n        In-sample predictions rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n    # Convert input lists to numpy arrays for easier manipulation\n    x = np.array(x)\n    y = np.array(y)\n    \n    # Initialize predictions to the mean of y\n    y_hat = np.full_like(y, np.mean(y), dtype=np.float64)\n    \n    for _ in range(n_estimators):\n        # Compute residuals\n        residuals = y - y_hat\n        \n        # Sort the data by x\n        sorted_indices = np.argsort(x)\n        x_sorted = x[sorted_indices]\n        residuals_sorted = residuals[sorted_indices]\n        \n        # Initialize variables to find the best split\n        best_split = None\n        best_sse = float('inf')\n        best_left_mean = None\n        best_right_mean = None\n        \n        # Evaluate possible splits\n        for i in range(1, len(x_sorted)):\n            if x_sorted[i] != x_sorted[i - 1]:  # Ensure we have a valid split\n                # Calculate means for left and right\n                left_residuals = residuals_sorted[:i]\n                right_residuals = residuals_sorted[i:]\n                \n                left_mean = np.mean(left_residuals) if len(left_residuals) > 0 else 0\n                right_mean = np.mean(right_residuals) if len(right_residuals) > 0 else 0\n                \n                # Calculate sum of squared errors\n                sse = np.sum((left_residuals - left_mean) ** 2) + np.sum((right_residuals - right_mean) ** 2)\n                \n                # Update best split if this one is better\n                if sse < best_sse:\n                    best_sse = sse\n                    best_split = (left_mean, right_mean)\n        \n        # If no valid split was found, use the global mean of residuals\n        if best_split is None:\n            best_split = (np.mean(residuals), np.mean(residuals))\n        \n        # Update predictions\n        left_mean, right_mean = best_split\n        y_hat += learning_rate * np.where(x <= x_sorted[i - 1], left_mean, right_mean)\n    \n    # Return the final predictions rounded to 4 decimals\n    return np.round(y_hat, 4).tolist()\n```\n\n### Explanation of the Code:\n1. **Initialization**: The predictions are initialized to the mean of the target values.\n2. **Residual Calculation**: In each boosting round, the residuals (the difference between actual and predicted values) are computed.\n3. **Sorting**: The data is sorted based on the feature values to evaluate potential split points.\n4. **Finding the Best Split**: For each unique split point, the algorithm calculates the mean residuals on both sides and the corresponding sum of squared errors (SSE). The split that minimizes the SSE is selected.\n5. **Updating Predictions**: The predictions are updated based on the best split found, scaled by the learning rate.\n6. **Final Output**: The final predictions are rounded to four decimal places and returned as a list.\n\nThis implementation adheres to the constraints and requirements specified in your task."}
{"task_id": 394, "completion_id": 0, "completion": "Here's the implementation of the RMSprop optimizer update step as per your specifications:\n\n```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    # Initialize Eg if it is None\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n\n    # Update the running average of squared gradients\n    Eg_next = rho * Eg + (1 - rho) * (grad ** 2)\n\n    # Update the parameters\n    epsilon = 1e-8  # Small constant for numerical stability\n    w_next = w - (learning_rate * grad) / (np.sqrt(Eg_next) + epsilon)\n\n    # Round and convert to lists\n    w_next_list = w_next.round(4).tolist()\n    Eg_next_list = Eg_next.round(4).tolist()\n\n    return w_next_list, Eg_next_list\n```\n\n### Explanation:\n1. **Initialization**: If `Eg` is `None`, we initialize it to a zero array of the same shape as `grad`.\n2. **Update Step**:\n   - We compute the new running average of the squared gradients using the formula provided.\n   - We then update the parameter vector `w` using the RMSprop update rule.\n3. **Rounding and Conversion**: Both the updated parameters and the running average are rounded to four decimal places and converted to regular Python lists before being returned.\n\nThis implementation adheres to the mathematical formulation you provided and ensures numerical stability with the small constant `epsilon`."}
{"task_id": 398, "completion_id": 0, "completion": "Here's the implementation of the Exponential Linear Unit (ELU) activation function and its gradient in Python. The function accepts a one-dimensional list or NumPy array, a float for the alpha parameter, and a boolean flag to determine whether to return the activation or the gradient.\n\n```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    # Convert input to a NumPy array for element-wise operations\n    x = np.asarray(x)\n    \n    if derivative:\n        # Calculate the gradient of ELU\n        gradient = np.where(x >= 0, 1, elu(x, alpha) + alpha)\n        return list(np.round(gradient, 4))\n    else:\n        # Calculate the ELU activation\n        elu_values = np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n        return list(np.round(elu_values, 4))\n\n# Example usage:\n# print(elu([-1, 0, 1, 2], alpha=0.1, derivative=False))  # ELU activation\n# print(elu([-1, 0, 1, 2], alpha=0.1, derivative=True))   # ELU gradient\n```\n\n### Explanation:\n1. **Input Handling**: The function first converts the input `x` into a NumPy array to facilitate element-wise operations.\n2. **Activation Calculation**: If `derivative` is `False`, it computes the ELU activation using the formula provided.\n3. **Gradient Calculation**: If `derivative` is `True`, it computes the gradient using the ELU values and the specified formula.\n4. **Rounding**: The results are rounded to four decimal places before being returned as a list.\n\nYou can test the function with different inputs to see how it behaves for both the activation and the gradient."}
{"task_id": 411, "completion_id": 0, "completion": "Here's a complete implementation of the `env_stats` function that computes the required statistics for the given actions and observations:\n\n```python\nimport numpy as np\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n\n    Parameters\n    ----------\n    actions : list\n        A list with the actions that were taken. Each element is either a\n        scalar or an iterable of scalars (for multi-dimensional spaces).\n    observations : list\n        A list with the corresponding observations. Same structural\n        requirements as *actions*.\n\n    Returns\n    -------\n    dict\n        A dictionary with the keys described in the task description.\n    \"\"\"\n    \n    def is_iterable(obj):\n        return isinstance(obj, (list, tuple, np.ndarray)) and not isinstance(obj, (str, bytes))\n\n    def flatten(lst):\n        return [item for sublist in lst for item in sublist]\n\n    # Initialize statistics\n    stats = {\n        'tuple_actions': any(is_iterable(action) for action in actions),\n        'tuple_observations': any(is_iterable(obs) for obs in observations),\n        'multidim_actions': False,\n        'multidim_observations': False,\n        'continuous_actions': False,\n        'continuous_observations': False,\n        'n_actions_per_dim': [],\n        'action_dim': 0,\n        'n_obs_per_dim': [],\n        'obs_dim': 0,\n        'action_ids': [],\n        'obs_ids': []\n    }\n\n    # Process actions\n    if actions:\n        first_action = actions[0]\n        if is_iterable(first_action):\n            stats['action_dim'] = len(first_action)\n            stats['multidim_actions'] = any(len(action) > 1 for action in actions)\n            unique_values_per_dim = [set() for _ in range(stats['action_dim'])]\n            for action in actions:\n                for i, value in enumerate(action):\n                    unique_values_per_dim[i].add(value)\n            stats['n_actions_per_dim'] = [len(unique_values) for unique_values in unique_values_per_dim]\n            stats['action_ids'] = [sorted(unique_values) for unique_values in unique_values_per_dim]\n            # Check for continuous values\n            flattened_actions = flatten(actions)\n            stats['continuous_actions'] = any(isinstance(value, float) and not value.is_integer() for value in flattened_actions)\n        else:\n            stats['action_dim'] = 1\n            stats['n_actions_per_dim'] = [len(set(actions))]\n            stats['action_ids'] = [sorted(set(actions))]\n            stats['continuous_actions'] = any(isinstance(value, float) and not value.is_integer() for value in actions)\n\n    # Process observations\n    if observations:\n        first_observation = observations[0]\n        if is_iterable(first_observation):\n            stats['obs_dim'] = len(first_observation)\n            stats['multidim_observations'] = any(len(obs) > 1 for obs in observations)\n            unique_values_per_dim = [set() for _ in range(stats['obs_dim'])]\n            for obs in observations:\n                for i, value in enumerate(obs):\n                    unique_values_per_dim[i].add(value)\n            stats['n_obs_per_dim'] = [len(unique_values) for unique_values in unique_values_per_dim]\n            stats['obs_ids'] = [sorted(unique_values) for unique_values in unique_values_per_dim]\n            # Check for continuous values\n            flattened_observations = flatten(observations)\n            stats['continuous_observations'] = any(isinstance(value, float) and not value.is_integer() for value in flattened_observations)\n        else:\n            stats['obs_dim'] = 1\n            stats['n_obs_per_dim'] = [len(set(observations))]\n            stats['obs_ids'] = [sorted(set(observations))]\n            stats['continuous_observations'] = any(isinstance(value, float) and not value.is_integer() for value in observations)\n\n    return stats\n```\n\n### Explanation:\n1. **Helper Functions**:\n   - `is_iterable`: Checks if an object is an iterable (excluding strings and bytes).\n   - `flatten`: Flattens a list of lists into a single list.\n\n2. **Statistics Initialization**: A dictionary `stats` is initialized to store the required statistics.\n\n3. **Processing Actions**:\n   - Checks if the actions are iterable and determines their dimensionality.\n   - Collects unique values for each dimension and counts them.\n   - Checks if any action values are continuous (non-integer floats).\n\n4. **Processing Observations**: Similar to actions, it processes the observations to gather statistics.\n\n5. **Return**: Finally, the function returns the `stats` dictionary containing all the computed statistics. \n\nThis implementation should work correctly for the specified requirements and handle both scalar and multi-dimensional inputs."}
{"task_id": 413, "completion_id": 0, "completion": "Here's a complete implementation of a simplified Gradient Boosting Decision Tree (GBDT) regressor from scratch, following the specifications you provided. The implementation includes a basic binary regression tree that uses mean-squared-error as the split criterion.\n\n```python\nimport numpy as np\n\nclass TreeNode:\n    def __init__(self, value=None, feature_index=None, threshold=None, left=None, right=None):\n        self.value = value  # For leaf nodes, this will hold the prediction value\n        self.feature_index = feature_index  # Index of the feature to split on\n        self.threshold = threshold  # Value to split on\n        self.left = left  # Left child\n        self.right = right  # Right child\n\nclass RegressionTree:\n    def __init__(self, max_depth):\n        self.max_depth = max_depth\n        self.root = None\n\n    def fit(self, X, y):\n        self.root = self._grow_tree(X, y, depth=0)\n\n    def _grow_tree(self, X, y, depth):\n        n_samples, n_features = X.shape\n        if depth < self.max_depth and n_samples > 1:\n            best_split = self._best_split(X, y)\n            if best_split is not None:\n                left_indices, right_indices = best_split['indices']\n                left_node = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n                right_node = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n                return TreeNode(feature_index=best_split['feature_index'],\n                                threshold=best_split['threshold'],\n                                left=left_node,\n                                right=right_node)\n        # If no split is found or max depth is reached, return a leaf node\n        return TreeNode(value=np.mean(y))\n\n    def _best_split(self, X, y):\n        best_mse = float('inf')\n        best_split = None\n        n_samples, n_features = X.shape\n        \n        for feature_index in range(n_features):\n            thresholds = np.unique(X[:, feature_index])\n            for threshold in thresholds:\n                left_indices = np.where(X[:, feature_index] < threshold)[0]\n                right_indices = np.where(X[:, feature_index] >= threshold)[0]\n                \n                if len(left_indices) == 0 or len(right_indices) == 0:\n                    continue\n                \n                left_target = y[left_indices]\n                right_target = y[right_indices]\n                \n                mse = (np.mean((left_target - np.mean(left_target)) ** 2) * len(left_target) +\n                        np.mean((right_target - np.mean(right_target)) ** 2) * len(right_target)) / n_samples\n                \n                if mse < best_mse:\n                    best_mse = mse\n                    best_split = {\n                        'feature_index': feature_index,\n                        'threshold': threshold,\n                        'indices': (left_indices, right_indices)\n                    }\n        \n        return best_split\n\n    def predict(self, X):\n        return np.array([self._traverse_tree(x, self.root) for x in X])\n\n    def _traverse_tree(self, x, node):\n        if node.value is not None:  # Leaf node\n            return node.value\n        if x[node.feature_index] < node.threshold:\n            return self._traverse_tree(x, node.left)\n        else:\n            return self._traverse_tree(x, node.right)\n\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 10, learning_rate: float = 0.1, max_depth: int = 3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the training\n        features.\n    y_train : np.ndarray\n        1-D array of length n_samples containing the training targets.\n    X_test : np.ndarray\n        2-D array with shape (m_samples, n_features) containing the test\n        features to predict.\n    n_estimators : int, default=10\n        Number of boosting iterations.\n    learning_rate : float, default=0.1\n        Shrinkage factor applied to each tree\u2019s prediction.\n    max_depth : int, default=3\n        Maximum depth of every individual regression tree.\n\n    Returns\n    -------\n    list[float]\n        Predictions for every sample in *X_test*, rounded to 4 decimal places.\n    \"\"\"\n    # Initialize predictions with the mean of the training targets\n    y_pred = np.full(y_train.shape, np.mean(y_train))\n    \n    for _ in range(n_estimators):\n        # Compute residuals (negative gradients)\n        residuals = y_train - y_pred\n        \n        # Fit a regression tree to the residuals\n        tree = RegressionTree(max_depth=max_depth)\n        tree.fit(X_train, residuals)\n        \n        # Predict the tree's output on the training data\n        tree_predictions = tree.predict(X_train)\n        \n        # Update predictions\n        y_pred += learning_rate * tree_predictions\n    \n    # Predict on the test set\n    final_predictions = tree.predict(X_test)\n    \n    # Round to 4 decimal places and convert to list\n    return list(np.round(final_predictions, 4))\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 4]])\n# y_train = np.array([1, 2, 3])\n# X_test = np.array([[1, 2], [2, 3]])\n# predictions = gbdt_regression(X_train, y_train, X_test)\n# print(predictions)\n```\n\n### Explanation:\n1. **TreeNode and RegressionTree Classes**: These classes implement a simple binary regression tree. The `fit` method builds the tree by recursively finding the best splits based on mean-squared-error.\n2. **GBDT Function**: The `gbdt_regression` function initializes predictions with the mean of the training targets, computes residuals, fits a regression tree to those residuals, and updates the predictions iteratively for a specified number of estimators.\n3. **Final Predictions**: After all iterations, predictions for the test set are made, rounded to four decimal places, and returned as a list.\n\nThis implementation is a simplified version of GBDT and is intended for educational purposes. In practice, you would use optimized libraries like `scikit-learn` for production-level tasks."}
{"task_id": 416, "completion_id": 0, "completion": "Here's a complete implementation of the multivariate Gaussian probability density function (PDF) as per your requirements. The function computes the PDF values for the given data points, mean vector, and covariance matrix without using any third-party libraries like SciPy.\n\n```python\nimport numpy as np\nimport math\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Calculate the multivariate Gaussian probability density function (PDF) for given data points.\n\n    Parameters:\n    X (np.ndarray): A NumPy array of shape (n_samples, n_features) containing the data points.\n    mean (np.ndarray): A mean vector of shape (n_features,).\n    cov (np.ndarray): A covariance matrix of shape (n_features, n_features) which must be positive-definite.\n\n    Returns:\n    List[float]: A list of PDF values for each data point in X, rounded to four decimal places.\n    \"\"\"\n    # Ensure X is a 2D array\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    \n    n_samples, n_features = X.shape\n    mean = np.array(mean).reshape(-1, 1)  # Ensure mean is a column vector\n    cov = np.array(cov)\n\n    # Calculate the determinant of the covariance matrix\n    det_cov = np.linalg.det(cov)\n    if det_cov <= 0:\n        raise ValueError(\"Covariance matrix must be positive-definite.\")\n\n    # Calculate the inverse of the covariance matrix\n    inv_cov = np.linalg.inv(cov)\n\n    # Calculate the normalization constant\n    norm_const = 1 / (math.pow((2 * math.pi), n_features / 2) * math.sqrt(det_cov))\n\n    # Calculate the PDF for each sample\n    pdf_values = []\n    for i in range(n_samples):\n        x_i = X[i].reshape(-1, 1)  # Reshape to column vector\n        diff = x_i - mean\n        exponent = -0.5 * (diff.T @ inv_cov @ diff)\n        pdf_value = norm_const * math.exp(exponent)\n        pdf_values.append(round(pdf_value.item(), 4))  # Round to 4 decimal places\n\n    return pdf_values\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if `X` is a one-dimensional array and reshapes it to ensure it is treated as a two-dimensional array with shape `(n_samples, n_features)`.\n\n2. **Mean and Covariance**: The mean vector is reshaped to ensure it is a column vector. The covariance matrix is converted to a NumPy array.\n\n3. **Determinant and Inverse**: The determinant of the covariance matrix is calculated, and an error is raised if it is not positive-definite. The inverse of the covariance matrix is also computed.\n\n4. **Normalization Constant**: The normalization constant is calculated using the formula provided.\n\n5. **PDF Calculation**: For each sample in `X`, the function computes the PDF value using the formula provided, rounding each result to four decimal places before appending it to the result list.\n\n### Usage:\nYou can call this function with your data points, mean vector, and covariance matrix to get the PDF values. Here's an example:\n\n```python\nX = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\nmean = np.array([2.0, 2.0])\ncov = np.array([[1.0, 0.5], [0.5, 1.0]])\n\npdf_values = multivariate_gaussian_pdf(X, mean, cov)\nprint(pdf_values)\n```\n\nThis will output the PDF values for the given data points."}
{"task_id": 419, "completion_id": 0, "completion": "Here's a complete implementation of the Bayesian linear regression with MAP prediction as described in your task. The function computes the posterior distribution parameters and uses them to predict the mean for new data points.\n\n```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    The function must return the MAP predictive means for every row in\n    `X_new`.  See the task description for full details.\n    \"\"\"\n    # Add intercept if required\n    if fit_intercept:\n        X = np.hstack((np.ones((X.shape[0], 1)), X))\n        X_new = np.hstack((np.ones((X_new.shape[0], 1)), X_new))\n    \n    N, M = X.shape\n    \n    # Handle the prior mean\n    if np.isscalar(mu):\n        mu = np.full(M, mu)\n    \n    # Handle the prior covariance V\n    if V is None:\n        V = np.eye(M)\n    elif np.isscalar(V):\n        V = V * np.eye(M)\n    elif V.ndim == 1:\n        V = np.diag(V)\n    \n    # Compute the posterior covariance \u03a3\u209a\n    V_inv = np.linalg.inv(V)\n    XTX = X.T @ X\n    Sigma_p = np.linalg.inv(V_inv + XTX)\n    \n    # Compute the posterior mean \u03bc\u209a\n    mu_p = Sigma_p @ (V_inv @ mu + X.T @ y)\n    \n    # Predictive mean for new data\n    y_pred = X_new @ mu_p\n    \n    # Round to 4 decimal places and return as a list\n    return list(np.round(y_pred, 4))\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4]])\n# y = np.array([1, 2, 3])\n# X_new = np.array([[1, 1], [2, 2]])\n# predictions = bayesian_linear_regression_pred(X, y, X_new)\n# print(predictions)\n```\n\n### Explanation of the Code:\n1. **Intercept Handling**: If `fit_intercept` is `True`, an intercept column of ones is added to both `X` and `X_new`.\n2. **Prior Mean and Covariance**: The prior mean `mu` is handled to ensure it is a vector of the correct shape. The prior covariance `V` is set to the identity matrix if `None`, or converted to a diagonal or full matrix as needed.\n3. **Posterior Calculation**:\n   - The posterior covariance `\u03a3\u209a` is computed using the formula provided.\n   - The posterior mean `\u03bc\u209a` is calculated using the posterior covariance and the transformed prior mean and data.\n4. **Prediction**: The MAP predictive mean is computed for the new data `X_new`.\n5. **Output**: The predictions are rounded to four decimal places and returned as a list.\n\nThis implementation should work correctly for the specified task and handle various forms of the prior covariance matrix."}
{"task_id": 423, "completion_id": 0, "completion": "Here's a Python function that calculates the Root Mean Squared Logarithmic Error (RMSLE) according to the specifications you've provided. The function includes input validation to ensure that both lists have the same length and contain only non-negative numbers. If the validation fails, it returns -1. Otherwise, it computes the RMSLE and returns the result rounded to four decimal places.\n\n```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Calculate the Root Mean Squared Logarithmic Error (RMSLE) between actual and predicted values.\"\"\"\n    \n    # Validate input\n    if len(actual) != len(predicted) or any(x < 0 for x in actual) or any(x < 0 for x in predicted):\n        return -1\n    \n    # Calculate RMSLE\n    n = len(actual)\n    log_diff = np.log1p(predicted) - np.log1p(actual)  # log1p is used for log(1 + x)\n    rmsle = np.sqrt(np.mean(log_diff ** 2))\n    \n    return round(rmsle, 4)\n\n# Example usage:\n# actual = [3, 5, 2.5, 7]\n# predicted = [2.5, 5, 4, 8]\n# print(root_mean_squared_log_error(actual, predicted))  # Output: RMSLE value\n```\n\n### Explanation:\n1. **Input Validation**:\n   - The function checks if the lengths of `actual` and `predicted` are the same.\n   - It also checks if all elements in both lists are non-negative. If any of these conditions fail, it returns -1.\n\n2. **RMSLE Calculation**:\n   - The function uses `np.log1p()` which computes `log(1 + x)` efficiently and handles the case when `x` is zero.\n   - It calculates the difference between the logarithms of the predicted and actual values, squares these differences, and computes the mean.\n   - Finally, it takes the square root of the mean to get the RMSLE.\n\n3. **Return Value**:\n   - The result is rounded to four decimal places before being returned. \n\nYou can test the function with different inputs to ensure it behaves as expected."}
{"task_id": 428, "completion_id": 0, "completion": "Here's the implementation of the Hann window generator function according to the specifications provided:\n\n```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    elif window_len == 1:\n        return [1.0]\n\n    # Calculate the number of samples\n    N = window_len + 1 if not symmetric else window_len\n    n = np.arange(N)\n\n    # Calculate the Hann window coefficients\n    window = 0.5 - 0.5 * np.cos(2 * np.pi * n / (N - 1))\n\n    # If symmetric is False, drop the last sample\n    if not symmetric:\n        window = window[:-1]\n\n    # Round to 4 decimal places and convert to a list\n    return list(np.round(window, 4))\n\n# Example usage:\n# print(hann(10, symmetric=True))\n# print(hann(10, symmetric=False))\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if `window_len` is less than or equal to 0, returning an empty list in that case. If `window_len` is 1, it returns a list containing `[1.0]`.\n\n2. **Window Length Calculation**: Depending on the `symmetric` flag, it calculates the effective length of the window. If `symmetric` is `False`, it adds 1 to the length.\n\n3. **Hann Window Calculation**: It uses NumPy to create an array of indices and computes the Hann window coefficients using the provided formula.\n\n4. **Dropping Last Sample**: If `symmetric` is `False`, the last sample is dropped to ensure periodicity.\n\n5. **Rounding and Conversion**: Finally, the coefficients are rounded to four decimal places and converted to a regular Python list before being returned.\n\nThis implementation is efficient and leverages NumPy for numerical operations, ensuring that it can handle larger window sizes effectively."}
{"task_id": 433, "completion_id": 0, "completion": "Here's the implementation of the `glorot_fan` function according to the specifications provided:\n\n```python\nimport numpy as np\n\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Your task is to implement this function following the rules described in\n    the problem statement.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    if len(shape) == 4:\n        # For 4-D convolutional kernels\n        out_channels = shape[0]\n        in_channels = shape[1]\n        kernel_height = shape[2]\n        kernel_width = shape[3]\n        \n        receptive_field_size = kernel_height * kernel_width\n        fan_in = in_channels * receptive_field_size\n        fan_out = out_channels * receptive_field_size\n    else:\n        # For all other shapes, take the first two dimensions\n        fan_in = shape[0]\n        fan_out = shape[1]\n    \n    return float(fan_in), float(fan_out)\n\n# Example usage:\n# print(glorot_fan((64, 32, 3, 3)))  # For a convolutional layer\n# print(glorot_fan((128, 256)))       # For a dense layer\n```\n\n### Explanation:\n1. **Input Handling**: The function accepts a shape that can be either a tuple or a list of integers.\n2. **4-D Tensor Case**: If the shape has exactly 4 dimensions, it calculates the `fan_in` and `fan_out` based on the convolutional kernel layout:\n   - `receptive_field_size` is computed as the product of the last two dimensions (kernel height and width).\n   - `fan_in` is calculated as the product of `in_channels` and `receptive_field_size`.\n   - `fan_out` is calculated as the product of `out_channels` and `receptive_field_size`.\n3. **Other Cases**: For any other shape, it directly takes the first two dimensions as `fan_in` and `fan_out`.\n4. **Return Values**: The function returns the values as floats in a tuple.\n\nThis implementation adheres to the rules specified in the task description."}
{"task_id": 435, "completion_id": 0, "completion": "Here's a simple implementation of a tiny Gradient Boosting Regressor using ordinary least squares (OLS) linear regression as the weak learner. The implementation follows the steps outlined in your task description.\n\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                                 y: list[float],\n                                 n_estimators: int = 10,\n                                 learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    # Convert X and y to NumPy arrays of type float\n    X = np.array(X, dtype=float)\n    y = np.array(y, dtype=float)\n\n    # Special cases\n    if n_estimators <= 0 or learning_rate == 0:\n        return [round(np.mean(y), 4)] * len(y)\n\n    # Initialize the current prediction with the mean of y\n    y_pred = np.full(y.shape, np.mean(y))\n\n    for t in range(n_estimators):\n        # Compute the residuals\n        residuals = y - y_pred\n        \n        # Fit an OLS linear model to predict the residuals from X\n        # Add a column of ones to X for the intercept\n        X_with_intercept = np.column_stack((np.ones(X.shape[0]), X))\n        # Calculate coefficients using the normal equation\n        coefficients = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ (X_with_intercept.T @ residuals)\n        \n        # Obtain the weak-learner prediction h(t)(X)\n        h_t = X_with_intercept @ coefficients\n        \n        # Update the overall prediction\n        y_pred += learning_rate * h_t\n\n    # Return the final prediction vector rounded to 4 decimal places\n    return [round(pred, 4) for pred in y_pred.tolist()]\n\n# Example usage:\n# X = [[1, 2], [2, 3], [3, 4]]\n# y = [1, 2, 3]\n# print(gradient_boosting_regressor(X, y, n_estimators=10, learning_rate=0.1))\n```\n\n### Explanation:\n1. **Input Conversion**: The input lists `X` and `y` are converted to NumPy arrays of type float.\n2. **Special Cases**: If `n_estimators` is less than or equal to 0 or `learning_rate` is 0, the function returns a list filled with the mean of `y`.\n3. **Initialization**: The initial prediction `y_pred` is set to the mean of `y`.\n4. **Main Loop**: For each estimator:\n   - Compute the residuals.\n   - Fit an OLS model to the residuals using the normal equation.\n   - Update the predictions using the weak learner's predictions scaled by the learning rate.\n5. **Final Output**: The predictions are rounded to four decimal places and returned as a list.\n\nThis implementation adheres to the constraints provided and should work for small datasets."}
{"task_id": 437, "completion_id": 0, "completion": "Here's the implementation of the logistic sigmoid function along with its derivatives as specified in your task:\n\n```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    # Convert input to a NumPy array for easier handling\n    x = np.asarray(x)\n\n    # Calculate the sigmoid function\n    sigmoid_value = 1 / (1 + np.exp(-x))\n\n    if order == 0:\n        result = sigmoid_value\n    elif order == 1:\n        result = sigmoid_value * (1 - sigmoid_value)\n    elif order == 2:\n        result = sigmoid_value * (1 - sigmoid_value) * (1 - 2 * sigmoid_value)\n    else:\n        return -1  # Invalid order\n\n    # Round the result to 4 decimal places\n    result = np.round(result, 4)\n\n    # Return as float if input was scalar, otherwise return as list\n    if np.isscalar(x):\n        return float(result)\n    else:\n        return result.tolist()\n```\n\n### Explanation:\n1. **Input Handling**: The function first converts the input `x` into a NumPy array using `np.asarray()`. This allows the function to handle scalars, lists, and NumPy arrays uniformly.\n\n2. **Sigmoid Calculation**: The logistic sigmoid function is computed using the formula \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\).\n\n3. **Derivatives**:\n   - For `order = 0`, it returns the sigmoid value.\n   - For `order = 1`, it computes the first derivative \\(\\sigma(x)(1 - \\sigma(x))\\).\n   - For `order = 2`, it computes the second derivative \\(\\sigma(x)(1 - \\sigma(x))(1 - 2\\sigma(x))\\).\n   - If the `order` is not 0, 1, or 2, it returns -1.\n\n4. **Rounding**: The results are rounded to four decimal places using `np.round()`.\n\n5. **Output Format**: The function checks if the input was a scalar or not. If it was a scalar, it returns a float; otherwise, it returns a list using `tolist()`.\n\n### Example Usage:\n```python\nprint(sigmoid(0))  # Scalar input\nprint(sigmoid([0, 1, 2]))  # List input\nprint(sigmoid(np.array([0, 1, 2])))  # NumPy array input\nprint(sigmoid(0, order=1))  # First derivative for scalar\nprint(sigmoid([0, 1, 2], order=2))  # Second derivative for list\nprint(sigmoid(0, order=3))  # Invalid order\n``` \n\nThis implementation meets all the requirements specified in your task."}
{"task_id": 438, "completion_id": 0, "completion": "Here's a Python implementation of the `to_categorical` function that meets the specified requirements:\n\n```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    # Convert y to a numpy array if it's a list\n    if isinstance(y, list):\n        y = np.array(y)\n\n    # Determine the maximum class index\n    max_index = np.max(y)\n\n    # Determine the number of classes\n    if num_classes is None:\n        num_classes = max_index + 1\n    elif num_classes <= max_index:\n        return -1  # Not enough classes\n\n    # Create a 2-D array of zeros\n    one_hot = np.zeros((len(y), num_classes), dtype=int)\n\n    # Set the appropriate indices to 1\n    one_hot[np.arange(len(y)), y] = 1\n\n    # Convert to a list of lists and return\n    return one_hot.tolist()\n\n# Example usage:\n# print(to_categorical([0, 1, 2, 1], num_classes=3))\n# Output: [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]\n```\n\n### Explanation of the Code:\n1. **Input Handling**: The function first checks if `y` is a list and converts it to a NumPy array for easier manipulation.\n2. **Determine Maximum Index**: It calculates the maximum index in `y` to determine how many classes are needed.\n3. **Class Count Logic**:\n   - If `num_classes` is `None`, it sets `num_classes` to `max_index + 1`.\n   - If `num_classes` is provided but is less than or equal to `max_index`, it returns `-1` to indicate that one-hot encoding is impossible.\n4. **One-Hot Encoding**: It creates a 2-D NumPy array filled with zeros and then sets the appropriate indices to 1 based on the values in `y`.\n5. **Return Format**: Finally, it converts the NumPy array to a list of lists and returns it.\n\n### Example Usage:\nYou can test the function with various inputs to see how it behaves. The provided example demonstrates how to use the function and what output to expect."}
{"task_id": 439, "completion_id": 0, "completion": "Here's a complete implementation of the Apriori algorithm for frequent itemset mining in Python. The function `apriori_frequent_itemsets` takes a list of transactions and a minimum support threshold, and returns all frequent itemsets as specified.\n\n```python\nimport itertools\nfrom collections import defaultdict\n\ndef apriori_frequent_itemsets(transactions: list[list[int]], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        List of transactions; each transaction is itself a list of items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction (>0 and \u22641).\n\n    Returns\n    -------\n    list[tuple]\n        Frequent itemsets ordered by length and then lexicographically.\n    \"\"\"\n    # Step 1: Calculate the minimum support count\n    num_transactions = len(transactions)\n    min_sup_count = min_sup * num_transactions\n\n    # Step 2: Generate frequent itemsets\n    def get_frequent_itemsets(transactions, min_sup_count):\n        # Count occurrences of each item\n        item_count = defaultdict(int)\n        for transaction in transactions:\n            for item in transaction:\n                item_count[frozenset([item])] += 1\n        \n        # Filter out infrequent itemsets\n        frequent_itemsets = {itemset for itemset, count in item_count.items() if count >= min_sup_count}\n        return frequent_itemsets\n\n    # Step 3: Generate candidates and prune\n    def generate_candidates(frequent_itemsets, length):\n        return {frozenset(i) for i in itertools.combinations(set().union(*frequent_itemsets), length)}\n\n    # Step 4: Main loop to find all frequent itemsets\n    all_frequent_itemsets = set()\n    current_frequent_itemsets = get_frequent_itemsets(transactions, min_sup_count)\n    all_frequent_itemsets.update(current_frequent_itemsets)\n\n    length = 2\n    while current_frequent_itemsets:\n        candidates = generate_candidates(current_frequent_itemsets, length)\n        candidate_count = defaultdict(int)\n\n        # Count support for candidates\n        for transaction in transactions:\n            transaction_set = set(transaction)\n            for candidate in candidates:\n                if candidate.issubset(transaction_set):\n                    candidate_count[candidate] += 1\n\n        # Filter candidates to find frequent itemsets\n        current_frequent_itemsets = {itemset for itemset, count in candidate_count.items() if count >= min_sup_count}\n        all_frequent_itemsets.update(current_frequent_itemsets)\n        length += 1\n\n    # Convert frozensets to sorted tuples and sort the result\n    result = sorted(tuple(sorted(itemset)) for itemset in all_frequent_itemsets)\n    return result\n\n# Example usage:\ntransactions = [['milk', 'bread'], ['bread', 'diaper', 'beer'], ['milk', 'diaper', 'beer', 'cola'], ['milk', 'bread', 'diaper', 'beer'], ['bread', 'milk']]\nmin_sup = 0.4\nfrequent_itemsets = apriori_frequent_itemsets(transactions, min_sup)\nprint(frequent_itemsets)\n```\n\n### Explanation of the Code:\n1. **Counting Single Items**: The function starts by counting the occurrences of each individual item in the transactions.\n2. **Filtering Frequent Itemsets**: It filters out the items that do not meet the minimum support count.\n3. **Generating Candidates**: For each subsequent length, it generates candidate itemsets by combining the frequent itemsets from the previous length.\n4. **Counting Candidates**: It counts how many transactions contain each candidate itemset.\n5. **Pruning**: It filters the candidates to keep only those that are frequent.\n6. **Sorting and Returning**: Finally, it converts the frequent itemsets from `frozenset` to sorted tuples and returns them sorted by length and lexicographically.\n\nThis implementation adheres to the Apriori algorithm's principles and efficiently finds all frequent itemsets based on the provided transactions and support threshold."}
{"task_id": 440, "completion_id": 0, "completion": "Here's a Python function that implements the required functionality to aggregate the predictions from an ensemble of classifiers. The function takes a three-level nested list of probabilities, averages them, rounds the results, and determines the predicted class for each sample.\n\n```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    # Convert the nested list to a NumPy array for easier manipulation\n    predictions_array = np.array(predictions)\n    \n    # Average the probabilities across the first dimension (estimators)\n    averaged_probabilities = np.mean(predictions_array, axis=0)\n    \n    # Round the averaged probabilities to 4 decimal places\n    rounded_probabilities = np.round(averaged_probabilities, 4)\n    \n    # Convert back to a regular Python list\n    averaged_probabilities_list = rounded_probabilities.tolist()\n    \n    # Get the predicted class index for each sample\n    predicted_classes = np.argmax(rounded_probabilities, axis=1).tolist()\n    \n    return averaged_probabilities_list, predicted_classes\n```\n\n### Explanation:\n1. **Input Handling**: The function accepts a three-level nested list representing the predictions from multiple estimators.\n2. **Conversion to NumPy Array**: The nested list is converted to a NumPy array to facilitate mathematical operations like averaging.\n3. **Averaging**: The function computes the mean across the first dimension (which corresponds to the estimators) using `np.mean`.\n4. **Rounding**: The averaged probabilities are rounded to four decimal places using `np.round`.\n5. **Conversion Back to List**: The rounded probabilities are converted back to a regular Python list.\n6. **Predicted Classes**: The function uses `np.argmax` to find the index of the maximum probability for each sample, which corresponds to the predicted class.\n7. **Return Value**: Finally, the function returns a tuple containing the averaged probabilities and the predicted class indices.\n\nThis implementation ensures that the output meets the specified requirements, including rounding and handling ties by returning the smallest index."}
{"task_id": 444, "completion_id": 0, "completion": "Here's a complete implementation of the Radial Basis Function (RBF) kernel matrix as specified in your task. The function validates the inputs, computes the kernel matrix, and returns the result rounded to four decimal places.\n\n```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function should follow the specifications given in the task\n    description. It must return -1 on invalid input, otherwise a nested list\n    containing the kernel matrix rounded to four decimal places.\n    \"\"\"\n    # Convert input lists to numpy arrays for easier manipulation\n    X = np.array(X)\n    if Y is not None:\n        Y = np.array(Y)\n\n    # Validate input dimensions\n    n_samples_X, n_features = X.shape\n    if Y is not None:\n        n_samples_Y, n_features_Y = Y.shape\n        if n_features != n_features_Y:\n            return -1  # Feature dimensions do not match\n\n    # Validate sigma\n    if sigma is None:\n        sigma = np.sqrt(n_features / 2)\n    elif isinstance(sigma, (float, int)):\n        if sigma <= 0:\n            return -1  # Sigma is non-positive\n        sigma = np.full(n_features, sigma)  # Convert to isotropic\n    elif isinstance(sigma, list) or isinstance(sigma, np.ndarray):\n        sigma = np.array(sigma)\n        if sigma.shape[0] != n_features or np.any(sigma <= 0):\n            return -1  # Sigma is invalid\n    else:\n        return -1  # Sigma is of an invalid type\n\n    # Compute the squared Euclidean distance\n    if Y is None:\n        Y = X  # If Y is not provided, use X for self-kernel\n\n    # Calculate the RBF kernel matrix\n    K = np.zeros((n_samples_X, Y.shape[0]))\n    for i in range(n_samples_X):\n        for j in range(Y.shape[0]):\n            diff = (X[i] - Y[j]) / sigma\n            K[i, j] = np.exp(-0.5 * np.sum(diff ** 2))\n\n    # Round the result to 4 decimal places and convert to a nested list\n    return K.round(4).tolist()\n\n# Example usage\nX = [[1, 0], [0, 1]]\nY = [[1, 0], [0, 1]]\nsigma = 1.0\nresult = rbf_kernel(X, Y, sigma)\nprint(result)\n```\n\n### Explanation:\n1. **Input Conversion**: The input lists `X` and `Y` are converted to NumPy arrays for easier mathematical operations.\n2. **Validation**:\n   - The function checks if the feature dimensions of `X` and `Y` match.\n   - It validates the `sigma` parameter to ensure it is positive and correctly sized.\n3. **Kernel Calculation**:\n   - The squared Euclidean distance is computed, and the RBF kernel values are calculated using the formula provided.\n4. **Output**: The resulting kernel matrix is rounded to four decimal places and returned as a nested list.\n\n### Example Output:\nFor the provided example, the output will be:\n```\n[[1.0, 0.3679], [0.3679, 1.0]]\n``` \n\nThis implementation should meet all the requirements specified in your task description."}
{"task_id": 446, "completion_id": 0, "completion": "To implement the `decision_stump` function that finds the best single-level split based on information gain, we need to follow these steps:\n\n1. **Calculate the Shannon Entropy**: This will help us measure the uncertainty in the class labels.\n2. **Identify Possible Splits**: For each feature, we will consider midpoints between successive distinct values as potential thresholds.\n3. **Calculate Information Gain**: For each split, compute the information gain and keep track of the best one.\n4. **Handle Ties**: Ensure that we follow the specified rules for resolving ties in information gain, feature index, and threshold.\n5. **Return the Result**: If no valid split is found, return the specified default values.\n\nHere's the implementation of the `decision_stump` function:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef entropy(y):\n    \"\"\"Calculate the Shannon entropy of the label vector y.\"\"\"\n    if len(y) == 0:\n        return 0\n    counts = Counter(y)\n    probabilities = [count / len(y) for count in counts.values()]\n    return -sum(p * np.log2(p) for p in probabilities if p > 0)\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\"\"\"\n    \n    # Convert input to numpy arrays for easier manipulation\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Calculate the entropy of the parent node\n    H_parent = entropy(y)\n    \n    # If all labels are the same, return the default case\n    if len(set(y)) == 1:\n        majority_label = min(Counter(y).keys())\n        return (-1, None, majority_label, majority_label)\n    \n    n_samples, n_features = X.shape\n    best_gain = -1\n    best_feature_index = -1\n    best_threshold = None\n    left_majority_label = None\n    right_majority_label = None\n    \n    # Iterate over each feature\n    for j in range(n_features):\n        # Get the unique values of the feature and sort them\n        unique_values = np.unique(X[:, j])\n        \n        # Generate possible thresholds (midpoints between unique values)\n        thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n        \n        for t in thresholds:\n            # Split the data based on the threshold\n            left_indices = X[:, j] <= t\n            right_indices = X[:, j] > t\n            \n            y_left = y[left_indices]\n            y_right = y[right_indices]\n            \n            # Calculate proportions\n            p_left = len(y_left) / n_samples\n            p_right = len(y_right) / n_samples\n            \n            # Calculate the entropies of the left and right nodes\n            H_left = entropy(y_left)\n            H_right = entropy(y_right)\n            \n            # Calculate information gain\n            gain = H_parent - (p_left * H_left + p_right * H_right)\n            \n            # Check if this gain is the best we've seen\n            if gain > best_gain:\n                best_gain = gain\n                best_feature_index = j\n                best_threshold = round(t, 4)\n                left_majority_label = Counter(y_left).most_common(1)[0][0]\n                right_majority_label = Counter(y_right).most_common(1)[0][0]\n            elif gain == best_gain:\n                # Resolve ties by feature index\n                if j < best_feature_index:\n                    best_feature_index = j\n                    best_threshold = round(t, 4)\n                    left_majority_label = Counter(y_left).most_common(1)[0][0]\n                    right_majority_label = Counter(y_right).most_common(1)[0][0]\n                elif j == best_feature_index and round(t, 4) < best_threshold:\n                    best_threshold = round(t, 4)\n                    left_majority_label = Counter(y_left).most_common(1)[0][0]\n                    right_majority_label = Counter(y_right).most_common(1)[0][0]\n    \n    # If no gain was found, return the default case\n    if best_gain <= 0:\n        majority_label = min(Counter(y).keys())\n        return (-1, None, majority_label, majority_label)\n    \n    return (best_feature_index, best_threshold, left_majority_label, right_majority_label)\n```\n\n### Explanation of the Code:\n- **Entropy Calculation**: The `entropy` function computes the Shannon entropy for a given label vector.\n- **Main Logic**: The `decision_stump` function iterates through each feature and calculates potential thresholds. For each threshold, it splits the data and computes the information gain.\n- **Tie Resolution**: The function checks for ties in information gain, feature index, and threshold, ensuring the smallest values are chosen as specified.\n- **Return Values**: The function returns the best feature index, threshold, and majority labels for the left and right splits, or the default values if no valid split is found."}
{"task_id": 452, "completion_id": 0, "completion": "Here's a complete implementation of the `divide_on_feature` function that meets the requirements specified in your task. This function will split the dataset based on the given feature index and threshold, handling both numeric and categorical thresholds appropriately.\n\n```python\nimport numpy as np\nfrom typing import List, Union\n\ndef divide_on_feature(X: Union[np.ndarray, List[list]], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\n\n    The function must create two disjoint subsets:\n    1. For a numeric threshold (int or float) the first subset contains every\n       sample whose value in column *feature_i* is **greater than or equal** to\n       the threshold.\n    2. For any other type of threshold the first subset contains every sample\n       whose value in column *feature_i* is **exactly equal** to the threshold.\n\n    The second subset always contains the remaining samples.  Both subsets must\n    keep the original order of appearance in *X*.\n\n    Args:\n        X: 2-D iterable (list or ndarray) where each element is a sample.\n        feature_i: Index of the feature column used for the split.\n        threshold: Value that determines how the split is performed.\n\n    Returns:\n        A list of length two.  *result[0]* is the first subset, *result[1]* is\n        the second subset.  Each subset must be converted to a regular Python\n        list via ``ndarray.tolist()`` before returning.\n    \"\"\"\n    # Ensure X is a numpy array\n    if isinstance(X, list):\n        X = np.array(X)\n\n    # Initialize subsets\n    subset1 = []\n    subset2 = []\n\n    # Determine if the threshold is numeric\n    is_numeric_threshold = isinstance(threshold, (int, float))\n\n    # Iterate through each sample in X\n    for sample in X:\n        feature_value = sample[feature_i]\n        if (is_numeric_threshold and feature_value >= threshold) or (not is_numeric_threshold and feature_value == threshold):\n            subset1.append(sample)\n        else:\n            subset2.append(sample)\n\n    # Convert subsets to lists\n    return [subset1, subset2]\n\n# Example usage:\nX_numeric = np.array([[1, 5], [3, 2], [4, 6], [2, 1]])\nresult_numeric = divide_on_feature(X_numeric, feature_i=0, threshold=3)\nprint(result_numeric)  # Output: [[3, 2], [4, 6]], [[1, 5], [2, 1]]\n\nX_categorical = np.array([[1, \"A\"], [2, \"B\"], [3, \"A\"], [4, \"C\"]])\nresult_categorical = divide_on_feature(X_categorical, feature_i=1, threshold=\"A\")\nprint(result_categorical)  # Output: [[1, \"A\"], [3, \"A\"]], [[2, \"B\"], [4, \"C\"]]\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if the input `X` is a list and converts it to a NumPy array if necessary.\n2. **Subset Initialization**: Two empty lists, `subset1` and `subset2`, are initialized to hold the samples based on the threshold condition.\n3. **Threshold Check**: The function determines if the threshold is numeric (int or float) to decide how to compare the feature values.\n4. **Iteration**: It iterates through each sample in `X`, checking the value at the specified feature index against the threshold. Depending on the result, it appends the sample to the appropriate subset.\n5. **Return Value**: Finally, it returns a list containing the two subsets.\n\nThis implementation preserves the order of the samples and handles both numeric and categorical thresholds as specified."}
{"task_id": 453, "completion_id": 0, "completion": "Here's the implementation of the `update_q_values` function that computes the training targets for a Deep Q-Network (DQN) based on the provided specifications:\n\n```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    \n    # Validate inputs\n    if not (0 <= gamma <= 1):\n        return -1\n    if not (Q.shape == Q_next.shape):\n        return -1\n    if not (len(actions) == len(rewards) == len(dones) == Q.shape[0]):\n        return -1\n\n    # Initialize the target Q-values\n    y = Q.copy()\n\n    # Calculate the targets\n    for i in range(Q.shape[0]):\n        if dones[i]:\n            target = rewards[i]\n        else:\n            target = rewards[i] + gamma * np.max(Q_next[i])\n        \n        # Update the target Q-value for the action taken\n        y[i, actions[i]] = target\n\n    # Round the result to 4 decimal places and convert to a nested list\n    return y.round(4).tolist()\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if `gamma` is within the range [0, 1] and if the shapes of `Q` and `Q_next` are the same. It also checks if the lengths of `actions`, `rewards`, and `dones` match the number of samples in `Q`. If any of these checks fail, it returns `-1`.\n\n2. **Target Calculation**: It initializes a copy of `Q` to `y`. For each sample in the batch, it calculates the target based on whether the next state is terminal (`dones[i]`). If it is terminal, the target is simply the reward. If not, it adds the discounted maximum Q-value of the next state to the reward.\n\n3. **Updating the Q-values**: The target value is then placed in the corresponding action index of `y`.\n\n4. **Rounding and Returning**: Finally, the function rounds the values in `y` to four decimal places and converts it to a nested list before returning it. \n\nThis implementation adheres to the requirements and constraints specified in the task."}
{"task_id": 458, "completion_id": 0, "completion": "Here's a Python function that implements the one-hot encoding of integer labels according to the specified requirements:\n\n```python\nimport numpy as np\n\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    # Convert labels to a list if it's a numpy array or tuple\n    if isinstance(labels, np.ndarray):\n        labels = labels.tolist()\n    elif isinstance(labels, tuple):\n        labels = list(labels)\n\n    # Check for negative labels\n    if any(label < 0 for label in labels):\n        return -1\n\n    # Determine the number of classes\n    max_label = max(labels)\n    if num_classes is None:\n        num_classes = max_label + 1\n    elif num_classes <= max_label:\n        return -1\n\n    # Create the one-hot encoded matrix\n    one_hot = [[0] * num_classes for _ in range(len(labels))]\n    for i, label in enumerate(labels):\n        one_hot[i][label] = 1\n\n    return one_hot\n```\n\n### Explanation of the Code:\n1. **Input Handling**: The function accepts a list, tuple, or NumPy array of labels. If the input is a NumPy array or tuple, it converts it to a list for easier processing.\n\n2. **Negative Label Check**: It checks if any label is negative. If so, it returns `-1`.\n\n3. **Class Count Determination**: \n   - If `num_classes` is not provided, it infers it as `max(labels) + 1`.\n   - If `num_classes` is provided but is less than or equal to `max(labels)`, it returns `-1`.\n\n4. **One-Hot Encoding**: It initializes a list of lists (the one-hot encoded matrix) with zeros. Then, it sets the appropriate index to `1` for each label.\n\n5. **Return Value**: Finally, it returns the one-hot encoded matrix.\n\n### Example Usage:\n```python\nlabels = [0, 1, 2, 1]\nprint(to_categorical(labels))  # Output: [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]\n\nlabels = [0, 1, 2, 1]\nprint(to_categorical(labels, num_classes=3))  # Output: [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]\n\nlabels = [0, 1, 2, 1]\nprint(to_categorical(labels, num_classes=2))  # Output: -1 (num_classes too small)\n\nlabels = [0, 1, -1, 1]\nprint(to_categorical(labels))  # Output: -1 (negative label)\n``` \n\nThis function meets all the specified requirements and handles various edge cases effectively."}
{"task_id": 461, "completion_id": 0, "completion": "Here's a complete implementation of the `gradient_boosting_regressor` function that follows the specified algorithm for gradient boosting with decision stumps for one-dimensional regression:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\n\n    Parameters\n    ----------\n    x : list[float]\n        Feature values (one-dimensional).\n    y : list[float]\n        Target values.\n    n_estimators : int, default 200\n        Number of boosting iterations.\n    learning_rate : float, default 0.5\n        Shrinkage applied to each weak learner.\n\n    Returns\n    -------\n    list[float]\n        Final predictions for the training data, rounded to four decimals.\n    \"\"\"    \n    # Convert input lists to numpy arrays for easier manipulation\n    x = np.array(x)\n    y = np.array(y)\n    \n    # Special case: if n_estimators is 0 or negative, return the mean target\n    if n_estimators <= 0:\n        return np.round(np.full_like(y, np.mean(y), dtype=float), 4).tolist()\n    \n    # Initialize predictions with the mean of the targets\n    y_hat = np.full_like(y, np.mean(y), dtype=float)\n    \n    for _ in range(n_estimators):\n        # Compute residuals\n        residuals = y - y_hat\n        \n        # Get unique split points (excluding the maximum value)\n        unique_x = np.unique(x)\n        split_points = unique_x[:-1]  # Exclude the last unique value\n        \n        best_sse = float('inf')\n        best_split = None\n        best_vL = None\n        best_vR = None\n        \n        # Iterate over each possible split point\n        for t in split_points:\n            left_mask = x <= t\n            right_mask = x > t\n            \n            if np.any(left_mask) and np.any(right_mask):\n                # Calculate mean residuals for left and right\n                v_L = np.mean(residuals[left_mask])\n                v_R = np.mean(residuals[right_mask])\n                \n                # Calculate SSE for this split\n                sse = np.sum((residuals[left_mask] - v_L) ** 2) + np.sum((residuals[right_mask] - v_R) ** 2)\n                \n                # Check if this is the best split\n                if sse < best_sse:\n                    best_sse = sse\n                    best_split = t\n                    best_vL = v_L\n                    best_vR = v_R\n        \n        # If no valid split was found, use the mean of the residuals\n        if best_split is None:\n            best_vL = best_vR = np.mean(residuals)\n        \n        # Update predictions based on the best split found\n        y_hat += learning_rate * np.where(x <= best_split, best_vL, best_vR)\n        \n        # Check for early stopping if residuals are all zeros\n        if np.all(residuals == 0):\n            break\n    \n    return np.round(y_hat, 4).tolist()\n```\n\n### Explanation of the Code:\n1. **Input Handling**: The function takes in lists of `x` and `y`, along with parameters for the number of estimators and the learning rate. It converts these lists to NumPy arrays for efficient numerical operations.\n\n2. **Initial Prediction**: The initial prediction (`y_hat`) is set to the mean of the target values.\n\n3. **Main Loop**: The loop runs for `n_estimators` iterations or until the residuals become all zeros:\n   - **Residual Calculation**: The residuals are computed as the difference between the actual target values and the current predictions.\n   - **Finding Splits**: For each unique value in `x` (except the last one), the function checks for potential splits and calculates the sum of squared errors (SSE) for each split.\n   - **Best Split Selection**: The split that results in the lowest SSE is selected. If no valid split is found, the mean of the residuals is used for predictions.\n   - **Updating Predictions**: The predictions are updated based on the best split found.\n\n4. **Return Value**: Finally, the function returns the predictions rounded to four decimal places as a list.\n\nThis implementation adheres to the requirements and efficiently performs gradient boosting using decision stumps for one-dimensional regression."}
{"task_id": 471, "completion_id": 0, "completion": "Here's the implementation of the Expected-SARSA TD(0) update function as described in your task:\n\n```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One step Expected\u2013SARSA TD(0) update for a tabular Q function.\n\n    Parameters\n    ----------\n    q_table : list[list[float]]\n        Current Q-table; q_table[s][a] is Q(s,a).\n    state : int\n        Index of the state *s* where the action was taken.\n    action : int\n        Index of the action *a* taken in state *s*.\n    reward : float\n        Immediate reward received after executing the action.\n    next_state : int | None\n        The successor state *s'*.  Use ``None`` if the transition ended the\n        episode.\n    epsilon : float\n        \u03b5 in the \u03b5-soft policy used to compute the expectation.\n    lr : float\n        Learning-rate \u03b7.\n    gamma : float\n        Discount factor \u03b3.\n\n    Returns\n    -------\n    list[list[float]]\n        The updated Q-table (all entries rounded to four decimals).\n    \"\"\"\n    num_actions = len(q_table[0])  # Number of actions available\n    if next_state is None:\n        # If the next state is None, the expected future value is 0\n        expected_future_value = 0\n    else:\n        # Calculate the expected value for the next state\n        greedy_action = np.argmax(q_table[next_state])  # Get the greedy action\n        expected_future_value = (1 - epsilon) * q_table[next_state][greedy_action]  # Contribution from the greedy action\n        # Add contributions from all other actions\n        for a in range(num_actions):\n            if a != greedy_action:\n                expected_future_value += (epsilon / num_actions) * q_table[next_state][a]\n\n    # Calculate the TD target\n    target = reward + gamma * expected_future_value\n\n    # Update the Q-value for the current state-action pair\n    q_table[state][action] += lr * (target - q_table[state][action])\n\n    # Round all entries to four decimal places\n    q_table = [[round(value, 4) for value in row] for row in q_table]\n\n    return q_table\n```\n\n### Explanation:\n1. **Input Parameters**: The function takes the Q-table, the current state and action indices, the received reward, the next state, and the hyperparameters (epsilon, learning rate, and discount factor).\n\n2. **Expected Future Value Calculation**:\n   - If `next_state` is `None`, the expected future value is set to 0.\n   - If there is a valid `next_state`, the function calculates the expected value based on the \u03b5-soft policy derived from the Q-table. It identifies the greedy action (the action with the highest Q-value) and computes the expected value considering both the greedy action and the other actions.\n\n3. **TD Target Calculation**: The target is computed using the immediate reward and the expected future value.\n\n4. **Q-Table Update**: The Q-value for the current state-action pair is updated using the learning rate and the difference between the target and the current Q-value.\n\n5. **Rounding**: Finally, all entries in the Q-table are rounded to four decimal places before returning.\n\nThis implementation adheres to the Expected-SARSA algorithm and should work correctly for the specified task."}
{"task_id": 474, "completion_id": 0, "completion": "Here's the implementation of the `generate_data` function that meets the specified requirements. This function generates a 2-D toy dataset for a binary (or multi-class) classifier, ensuring that the parameters are reusable and that it generates labels for both the training and validation sets.\n\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    # Initialize arrays for training and validation data\n    X_train = np.zeros((m * n_train, 2))\n    X_val = np.zeros((m * n_val, 2))\n    Y_train = np.zeros(m * n_train)\n    Y_val = np.zeros(m * n_val)\n\n    for i in range(m):\n        # Generate training data\n        X_train[i * n_train:(i + 1) * n_train, 0] = np.random.uniform(\n            (i // 2 + 0.1) * 10, (i // 2 + 0.9) * 10, n_train)\n        X_train[i * n_train:(i + 1) * n_train, 1] = np.random.uniform(\n            (i % 2 * interval + 0.1) * 10, (i % 2 * interval + 0.9) * 10, n_train)\n        Y_train[i * n_train:(i + 1) * n_train] = (i - 0.5) * 2  # -1 or +1\n\n        # Generate validation data\n        X_val[i * n_val:(i + 1) * n_val, 0] = np.random.uniform(\n            (i // 2 + 0.1) * 10, (i // 2 + 0.9) * 10, n_val)\n        X_val[i * n_val:(i + 1) * n_val, 1] = np.random.uniform(\n            (i % 2 * interval + 0.1) * 10, (i % 2 * interval + 0.9) * 10, n_val)\n        Y_val[i * n_val:(i + 1) * n_val] = (i - 0.5) * 2  # -1 or +1\n\n    # Round the coordinates to 4 decimal places\n    X_train = np.round(X_train, 4)\n    X_val = np.round(X_val, 4)\n    \n    return X_train, X_val, Y_train, Y_val\n```\n\n### Explanation:\n1. **Parameters**: The function accepts the number of classes (`m`), the number of training samples per class (`n_train`), the number of validation samples per class (`n_val`), the vertical distance between clusters (`interval`), and an optional random seed for reproducibility.\n\n2. **Data Generation**:\n   - The function initializes arrays for training and validation data (`X_train`, `X_val`) and their corresponding labels (`Y_train`, `Y_val`).\n   - It uses a loop to generate data for each class. The x-coordinates are generated uniformly within the specified range based on the class index, and the y-coordinates are adjusted by the `interval`.\n   - Labels are assigned based on the class index.\n\n3. **Rounding**: The coordinates are rounded to four decimal places before returning.\n\n4. **Return Value**: The function returns the training and validation datasets along with their labels as NumPy arrays."}
{"task_id": 475, "completion_id": 0, "completion": "Here's the implementation of the Adam optimizer's single-step update function in Python. This function takes the current parameters, gradient, previous moment estimates, and other hyperparameters to compute the updated parameters and new moment estimates.\n\n```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    # Initialize m_prev and v_prev as zeros if they are None\n    if m_prev is None:\n        m_prev = np.zeros_like(grad)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad)\n\n    # Update biased first moment estimate\n    m_new = b1 * m_prev + (1 - b1) * grad\n    \n    # Update biased second moment estimate\n    v_new = b2 * v_prev + (1 - b2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m_new / (1 - b1 ** t)\n    \n    # Compute bias-corrected second moment estimate\n    v_hat = v_new / (1 - b2 ** t)\n    \n    # Update parameters\n    w_new = w - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n    \n    return w_new, m_new, v_new\n```\n\n### Explanation of the Code:\n1. **Initialization**: If `m_prev` or `v_prev` is `None`, they are initialized to arrays of zeros with the same shape as `grad`.\n2. **First Moment Update**: The first moment `m_new` is updated using the formula provided.\n3. **Second Moment Update**: The second moment `v_new` is updated similarly.\n4. **Bias Correction**: The bias-corrected estimates `m_hat` and `v_hat` are computed.\n5. **Parameter Update**: The parameters `w` are updated using the corrected moments and the learning rate.\n6. **Return Values**: The function returns the updated parameters and the new moment estimates.\n\n### Usage Example:\nYou can call this function with the current parameters, gradient, previous moments, and time step to perform an Adam update. Here's an example:\n\n```python\nw_new, m_new, v_new = adam_update(1.0, 0.1, 0.0, 0.0, 1)\nprint(round(w_new, 9))  # Output should be approximately 0.999000001\n```\n\nThis implementation is designed to work with both scalars and NumPy arrays, making it versatile for various optimization tasks."}
{"task_id": 479, "completion_id": 0, "completion": "Here's a complete implementation of the \u03b5-greedy algorithm for the stationary multi-armed bandit problem based on the specifications you provided:\n\n```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where ``rewards[t][i]`` is the reward for arm *i* at time-step\n        *t*.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Number of arms\n    N = len(rewards[0])\n    \n    # Initialize estimated values and counters\n    estimated_values = [ev_prior] * N\n    counts = [0] * N\n\n    # Iterate over each time-step\n    for t in range(len(rewards)):\n        if np.random.rand() < epsilon:\n            # Explore: choose an arm uniformly at random\n            chosen_arm = np.random.randint(N)\n        else:\n            # Exploit: choose the arm with the highest estimated value\n            chosen_arm = np.argmax(estimated_values)\n\n        # Get the reward for the chosen arm at time-step t\n        reward = rewards[t][chosen_arm]\n\n        # Update the counts for the chosen arm\n        counts[chosen_arm] += 1\n\n        # Update the estimated value for the chosen arm using incremental sample mean\n        estimated_values[chosen_arm] += (reward - estimated_values[chosen_arm]) / counts[chosen_arm]\n\n    # Round the estimated values to 4 decimal places\n    return [round(value, 4) for value in estimated_values]\n\n# Example usage:\n# rewards = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]\n# print(epsilon_greedy_bandit(rewards, epsilon=0.1, ev_prior=0.5, seed=42))\n```\n\n### Explanation of the Code:\n1. **Initialization**: The function initializes the estimated values for each arm to `ev_prior` and sets the counts of pulls for each arm to zero.\n2. **Time-step Loop**: For each time-step, it decides whether to explore or exploit based on the value of `epsilon`.\n   - If exploring, it randomly selects an arm.\n   - If exploiting, it selects the arm with the highest estimated value.\n3. **Reward and Update**: It retrieves the reward for the selected arm and updates the estimated value using the incremental sample mean formula.\n4. **Final Output**: After processing all time-steps, it returns the estimated values rounded to four decimal places.\n\nYou can test the function with different reward matrices and parameters to see how it performs."}
{"task_id": 481, "completion_id": 0, "completion": "Here's a Python implementation of the DBSCAN algorithm from scratch, following the specifications provided:\n\n```python\nimport numpy as np\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if data.size == 0:\n        return []\n\n    # Initialize variables\n    n_samples = data.shape[0]\n    labels = -1 * np.ones(n_samples, dtype=int)  # Initialize all labels to -1 (noise)\n    cluster_id = 0\n\n    # Function to calculate the Euclidean distance\n    def euclidean_distance(point1, point2):\n        return np.sqrt(np.sum((point1 - point2) ** 2))\n\n    # Function to find all neighbors within eps\n    def region_query(point_idx):\n        neighbors = []\n        for idx in range(n_samples):\n            if euclidean_distance(data[point_idx], data[idx]) <= eps:\n                neighbors.append(idx)\n        return neighbors\n\n    # Main DBSCAN algorithm\n    for point_idx in range(n_samples):\n        if labels[point_idx] != -1:  # Already labeled\n            continue\n        \n        neighbors = region_query(point_idx)\n\n        if len(neighbors) < min_samples:  # Mark as noise\n            labels[point_idx] = -1\n        else:\n            # Found a core point, start a new cluster\n            labels[point_idx] = cluster_id\n            # Expand the cluster\n            i = 0\n            while i < len(neighbors):\n                neighbor_idx = neighbors[i]\n                if labels[neighbor_idx] == -1:  # Change noise to border point\n                    labels[neighbor_idx] = cluster_id\n                if labels[neighbor_idx] == 0:  # Not yet labeled\n                    labels[neighbor_idx] = cluster_id\n                    # Get new neighbors\n                    new_neighbors = region_query(neighbor_idx)\n                    if len(new_neighbors) >= min_samples:\n                        neighbors.extend(new_neighbors)  # Add new neighbors to the list\n                i += 1\n            \n            cluster_id += 1  # Move to the next cluster id\n\n    return labels.tolist()\n\n# Example usage:\ndata = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])\neps = 1.5\nmin_samples = 2\nprint(dbscan(data, eps, min_samples))\n```\n\n### Explanation:\n1. **Initialization**: The function initializes the labels for all points to -1 (indicating noise) and sets up a cluster ID counter.\n2. **Distance Calculation**: A helper function computes the Euclidean distance between two points.\n3. **Neighbor Query**: Another helper function finds all points within the `eps` distance from a given point.\n4. **Main Loop**: The algorithm iterates through each point:\n   - If the point is already labeled, it skips to the next.\n   - It retrieves the neighbors of the current point.\n   - If the number of neighbors is less than `min_samples`, it marks the point as noise.\n   - If it is a core point, it starts a new cluster and expands it by checking the neighbors recursively.\n5. **Cluster Labeling**: Points that are reachable from core points are labeled with the current cluster ID, and the cluster ID is incremented for the next cluster.\n\nThis implementation adheres to the rules specified for labeling core points, border points, and noise points."}
{"task_id": 482, "completion_id": 0, "completion": "Here's the implementation of the `to_categorical` function that performs one-hot encoding of integer class labels. The function handles the optional `num_classes` parameter and raises appropriate errors for invalid inputs.\n\n```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    # Convert input to a NumPy array\n    y = np.asarray(y)\n    \n    # Validate input\n    if np.any(y < 0):\n        raise ValueError(\"Labels must be non-negative integers.\")\n    \n    if num_classes is None:\n        num_classes = np.max(y) + 1\n    else:\n        if np.any(y >= num_classes):\n            raise ValueError(f\"Labels must be less than {num_classes}.\")\n    \n    # Create the one-hot encoded array\n    one_hot = np.zeros((len(y), num_classes), dtype=int)\n    one_hot[np.arange(len(y)), y] = 1\n    \n    return one_hot\n```\n\n### Explanation:\n1. **Input Conversion**: The input `y` is converted to a NumPy array to facilitate operations.\n2. **Validation**:\n   - It checks if any label is negative and raises a `ValueError` if so.\n   - If `num_classes` is not provided, it calculates it as `max(y) + 1`.\n   - If `num_classes` is provided, it checks that all labels are less than `num_classes`, raising a `ValueError` if any label is out of bounds.\n3. **One-Hot Encoding**:\n   - A zero matrix of shape `(len(y), num_classes)` is created.\n   - The appropriate indices are set to 1 using advanced indexing.\n4. **Return**: The function returns the one-hot encoded matrix.\n\n### Example Usage:\n```python\nlabels = [0, 1, 2, 1]\none_hot_encoded = to_categorical(labels)\nprint(one_hot_encoded)\n```\n\nThis will output:\n```\n[[1 0 0]\n [0 1 0]\n [0 0 1]\n [0 1 0]]\n```"}
{"task_id": 485, "completion_id": 0, "completion": "Here's a complete implementation of a univariate regression tree using the CART algorithm as described in your task. The function builds a binary tree by recursively splitting the data based on the feature values, minimizing the sum of squared errors (SSE) at each split. It also predicts the target values for the given test points.\n\n```python\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\n\n    Parameters\n    ----------\n    X_train : list[float | int]\n        1-D feature values for the training set.\n    y_train : list[float | int]\n        Continuous target values for the training set.\n    X_test : list[float | int]\n        1-D feature values for which predictions are required.\n    max_depth : int, default 3\n        Maximum depth of the binary tree (root has depth 0).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every element in *X_test*.\n    \"\"\"\n    \n    if not X_train or not y_train:\n        return []\n\n    class Node:\n        def __init__(self, value=None, left=None, right=None):\n            self.value = value  # Mean value for leaf nodes\n            self.left = left    # Left child\n            self.right = right  # Right child\n\n    def sse(y):\n        \"\"\"Calculate the sum of squared errors.\"\"\"\n        mean_y = np.mean(y)\n        return np.sum((y - mean_y) ** 2)\n\n    def best_split(X, y):\n        \"\"\"Find the best split point for the data.\"\"\"\n        best_sse = float('inf')\n        best_value = None\n        best_left_y = None\n        best_right_y = None\n        \n        # Sort the data by feature values\n        sorted_indices = np.argsort(X)\n        X_sorted = np.array(X)[sorted_indices]\n        y_sorted = np.array(y)[sorted_indices]\n        \n        for i in range(1, len(y_sorted)):\n            if X_sorted[i] != X_sorted[i - 1]:  # Only consider unique split points\n                left_y = y_sorted[:i]\n                right_y = y_sorted[i:]\n                current_sse = sse(left_y) + sse(right_y)\n                \n                if current_sse < best_sse:\n                    best_sse = current_sse\n                    best_value = (X_sorted[i] + X_sorted[i - 1]) / 2  # Midpoint\n                    best_left_y = left_y\n                    best_right_y = right_y\n        \n        return best_value, best_left_y, best_right_y\n\n    def build_tree(X, y, depth):\n        \"\"\"Recursively build the regression tree.\"\"\"\n        if depth >= max_depth or len(y) < 2:\n            return Node(value=np.mean(y))\n        \n        split_value, left_y, right_y = best_split(X, y)\n        \n        if left_y is None or right_y is None:\n            return Node(value=np.mean(y))\n        \n        left_indices = np.where(X < split_value)[0]\n        right_indices = np.where(X >= split_value)[0]\n        \n        left_node = build_tree(X[left_indices], y[left_indices], depth + 1)\n        right_node = build_tree(X[right_indices], y[right_indices], depth + 1)\n        \n        return Node(value=None, left=left_node, right=right_node)\n\n    def predict_tree(node, x):\n        \"\"\"Predict using the regression tree.\"\"\"\n        if node.value is not None:  # Leaf node\n            return node.value\n        if x < node.left.value:  # Go left\n            return predict_tree(node.left, x)\n        else:  # Go right\n            return predict_tree(node.right, x)\n\n    # Build the tree\n    root = build_tree(X_train, y_train, 0)\n    \n    # Make predictions\n    predictions = [predict_tree(root, x) for x in X_test]\n    \n    return predictions\n```\n\n### Explanation:\n1. **Node Class**: Represents each node in the tree. It can either be a leaf (with a mean value) or an internal node (with left and right children).\n2. **SSE Calculation**: The `sse` function computes the sum of squared errors for a given set of target values.\n3. **Best Split**: The `best_split` function finds the optimal split point that minimizes the SSE for the left and right child nodes.\n4. **Tree Building**: The `build_tree` function recursively constructs the tree until the maximum depth is reached or there are fewer than two samples.\n5. **Prediction**: The `predict_tree` function traverses the tree to make predictions for the test points.\n\nThis implementation adheres to the constraints and requirements specified in your task."}
{"task_id": 490, "completion_id": 0, "completion": "Here's a Python function that converts a one-dimensional vector into a square diagonal matrix, as per your requirements:\n\n```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    # Check if the input is empty\n    if not x:\n        return []\n\n    # Convert the input to a NumPy array for easier manipulation\n    vector = np.asarray(x)\n\n    # Create a diagonal matrix using NumPy\n    diagonal_matrix = np.diag(vector)\n\n    # Convert the NumPy array to a list of lists and return\n    return diagonal_matrix.tolist()\n\n# Example usage:\n# print(make_diagonal([1, 2, 3]))  # Output: [[1, 0, 0], [0, 2, 0], [0, 0, 3]]\n# print(make_diagonal((4, 5)))      # Output: [[4, 0], [0, 5]]\n# print(make_diagonal(np.array([7]))) # Output: [[7]]\n# print(make_diagonal([]))           # Output: []\n```\n\n### Explanation:\n1. **Input Check**: The function first checks if the input vector `x` is empty. If it is, it returns an empty list.\n2. **Conversion to NumPy Array**: The input is converted to a NumPy array using `np.asarray()`, which allows for easy manipulation.\n3. **Creating the Diagonal Matrix**: The `np.diag()` function is used to create a diagonal matrix from the vector.\n4. **Conversion to List of Lists**: Finally, the resulting NumPy array is converted to a list of lists using the `tolist()` method before returning it.\n\nThis function handles various input types (list, tuple, or NumPy array) and returns the desired output format."}
{"task_id": 491, "completion_id": 0, "completion": "To implement the FP-Growth algorithm for frequent item-set mining, we will follow the two main stages outlined in the task description: constructing the FP-tree and recursively mining it for frequent item-sets. Below is the complete implementation of the `fp_growth` function.\n\n```python\nfrom collections import Counter, defaultdict\nimport itertools\n\nclass FPNode:\n    def __init__(self, item, count, parent=None):\n        self.item = item\n        self.count = count\n        self.parent = parent\n        self.children = {}\n        self.link = None  # Link to the next node with the same item\n\ndef build_fp_tree(transactions, min_sup):\n    # Count item frequencies\n    item_count = Counter()\n    for transaction in transactions:\n        item_count.update(set(transaction))\n    \n    # Filter items by min_sup and sort by frequency\n    frequent_items = {item: count for item, count in item_count.items() if count >= min_sup}\n    if not frequent_items:\n        return None, None\n    \n    # Sort items by frequency (descending) and lexicographically\n    sorted_items = sorted(frequent_items.items(), key=lambda x: (-x[1], x[0]))\n    item_order = [item for item, _ in sorted_items]\n    \n    # Build the FP-tree\n    root = FPNode('root', 0)\n    header_table = {}\n    \n    for transaction in transactions:\n        # Filter and sort transaction items\n        filtered_items = [item for item in item_order if item in transaction]\n        if not filtered_items:\n            continue\n        \n        # Increment the count in the FP-tree\n        current_node = root\n        for item in filtered_items:\n            if item in current_node.children:\n                current_node.children[item].count += 1\n            else:\n                new_node = FPNode(item, 1, current_node)\n                current_node.children[item] = new_node\n                \n                # Link to header table\n                if item not in header_table:\n                    header_table[item] = new_node\n                else:\n                    # Link the new node to the previous node in the header table\n                    current_link = header_table[item]\n                    while current_link.link is not None:\n                        current_link = current_link.link\n                    current_link.link = new_node\n            \n            current_node = current_node.children[item]\n            current_node.count += 1\n    \n    return root, header_table\n\ndef mine_fp_tree(node, prefix, min_sup, header_table, result):\n    # If the node is None, return\n    if node is None:\n        return\n    \n    # Collect the frequent item-set\n    if node.item != 'root' and node.count >= min_sup:\n        new_prefix = prefix + [node.item]\n        result.append(tuple(sorted(new_prefix)))  # Add the new item-set\n        \n        # Mine the conditional pattern base\n        conditional_base = []\n        current = header_table[node.item]\n        while current is not None:\n            path = []\n            parent = current.parent\n            while parent and parent.item != 'root':\n                path.append(parent.item)\n                parent = parent.parent\n            if path:\n                conditional_base.extend([path] * current.count)\n            current = current.link\n        \n        # Build the conditional FP-tree\n        conditional_tree, conditional_header = build_fp_tree(conditional_base, min_sup)\n        mine_fp_tree(conditional_tree, new_prefix, min_sup, conditional_header, result)\n    \n    # Continue mining the next nodes in the header table\n    next_node = header_table.get(node.item)\n    while next_node is not None:\n        mine_fp_tree(next_node, prefix, min_sup, header_table, result)\n        next_node = next_node.link\n\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\n\n    Parameters\n    ----------\n    transactions : Iterable[Iterable[Hashable]]\n        A collection of transactions.  Each transaction is an iterable containing hashable items.\n    min_sup : int\n        Minimum number of occurrences an item-set must have to be considered frequent.\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets sorted 1) by length, 2) lexicographically.  Every\n        tuple itself is sorted lexicographically.\n    \"\"\"\n    # Build the FP-tree\n    root, header_table = build_fp_tree(transactions, min_sup)\n    \n    # Result list to hold frequent item-sets\n    result = []\n    \n    # Mine the FP-tree\n    mine_fp_tree(root, [], min_sup, header_table, result)\n    \n    # Sort the result by length and lexicographically\n    result = sorted(result, key=lambda x: (len(x), x))\n    \n    return result\n\n# Example usage\ntransactions = [\n    [\"A\", \"B\", \"D\", \"E\"],\n    [\"B\", \"C\", \"E\"],\n    [\"A\", \"B\", \"D\", \"E\"],\n    [\"A\", \"B\", \"C\", \"E\"],\n    [\"A\", \"B\", \"C\", \"D\", \"E\"],\n    [\"B\", \"C\", \"D\"]\n]\nmin_sup = 3\nprint(fp_growth(transactions, min_sup))\n```\n\n### Explanation:\n1. **FPNode Class**: Represents a node in the FP-tree, containing the item, its count, a reference to its parent, and a link to the next node with the same item.\n2. **build_fp_tree Function**: Constructs the FP-tree from the transactions, filtering out infrequent items and linking nodes in the header table.\n3. **mine_fp_tree Function**: Recursively mines the FP-tree to find all frequent item-sets, building conditional pattern bases and conditional FP-trees as needed.\n4. **fp_growth Function**: The main function that orchestrates the building of the FP-tree and the mining process, returning the sorted list of frequent item-sets.\n\nThis implementation adheres to the requirements and constraints specified in the task description."}
{"task_id": 492, "completion_id": 0, "completion": "To implement the polynomial feature expansion as described, we can follow these steps:\n\n1. **Check for Negative Degree**: If the degree is negative, return -1.\n2. **Initialize the Result**: Start with a list that contains the constant term (1) for each sample.\n3. **Generate Monomials**: Use `itertools.combinations_with_replacement` to generate all combinations of feature indices that sum up to degrees from 0 to the specified degree.\n4. **Construct the Polynomial Features**: For each sample, compute the polynomial features based on the generated combinations.\n5. **Return the Result**: Convert the result to a list of lists and return it.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to a given degree.\n\n    The function should take a 2-D list *X* whose rows correspond to\n    observations and columns correspond to original features.  It must\n    return a new list of lists containing, for every observation, all\n    monomials of the original features whose total degree does not exceed\n    the supplied *degree*.  The very first element in every row should be\n    1 (the degree-0 term).\n\n    If *degree* is negative the function must return -1.\n\n    Args:\n        X: Input data of shape (n_samples, n_features).\n        degree: Maximum total degree of the generated polynomial terms.\n\n    Returns:\n        A 2-D Python list with the enriched feature matrix, or \u20111 when\n        *degree* is negative.\n    \"\"\"\n    if degree < 0:\n        return -1\n    \n    n_samples = len(X)\n    n_features = len(X[0]) if n_samples > 0 else 0\n    \n    # Initialize the result with the degree-0 term (1)\n    result = []\n    \n    # Generate polynomial features\n    for sample in X:\n        features = [1]  # Start with the constant term\n        for d in range(1, degree + 1):\n            for indices in combinations_with_replacement(range(n_features), d):\n                term = 1\n                for index in indices:\n                    term *= sample[index]\n                features.append(term)\n        result.append(features)\n    \n    return result\n```\n\n### Explanation of the Code:\n- **Input Validation**: The function first checks if the degree is negative and returns -1 if it is.\n- **Feature Generation**: For each sample in the input matrix `X`, we start with a list containing the constant term (1). We then iterate through degrees from 1 to the specified degree. For each degree, we generate combinations of feature indices using `combinations_with_replacement`.\n- **Term Calculation**: For each combination of indices, we calculate the product of the corresponding features from the sample and append the result to the features list.\n- **Final Output**: The result is a list of lists, where each inner list corresponds to the polynomial features of a sample.\n\nThis implementation efficiently constructs the polynomial feature matrix as required."}
{"task_id": 493, "completion_id": 0, "completion": "Here's a Python function that calculates the mean-squared error (MSE) of a given sample, either as a Python list or a 1-D NumPy array. The function handles the case of an empty input by returning -1, and it rounds the result to four decimal places.\n\n```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean. If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    if len(y) == 0:\n        return -1\n    \n    # Convert to numpy array if it's a list\n    if isinstance(y, list):\n        y = np.array(y)\n    \n    # Calculate the mean\n    mean_y = np.mean(y)\n    \n    # Calculate the MSE\n    mse_value = np.mean((y - mean_y) ** 2)\n    \n    # Return the MSE rounded to 4 decimal places\n    return round(mse_value, 4)\n\n# Example usage:\n# print(mse([1, 2, 3, 4, 5]))  # Output: 2.0\n# print(mse(np.array([1, 2, 3, 4, 5])))  # Output: 2.0\n# print(mse([]))  # Output: -1\n```\n\n### Explanation:\n1. **Input Check**: The function first checks if the input `y` is empty. If it is, it returns -1.\n2. **Type Handling**: If `y` is a list, it converts it to a NumPy array for easier calculations.\n3. **Mean Calculation**: It calculates the mean of the array using `np.mean()`.\n4. **MSE Calculation**: It computes the MSE by finding the average of the squared differences between each element and the mean.\n5. **Rounding**: Finally, it rounds the MSE to four decimal places before returning it. \n\nThis function is efficient and leverages NumPy's capabilities for numerical operations."}
{"task_id": 496, "completion_id": 0, "completion": "Here's the implementation of the `apply_affine` function that meets the specified requirements:\n\n```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Ensure x is a NumPy array for element-wise operations\n    x = np.asarray(x)\n    \n    # Calculate the affine activation\n    y = slope * x + intercept\n    # The first derivative is constant and equal to the slope\n    grad = np.full_like(x, slope)\n    # The second derivative is constant and equal to 0\n    grad2 = np.zeros_like(x)\n    \n    # Round the results to 4 decimal places\n    y = np.round(y, 4).tolist()\n    grad = np.round(grad, 4).tolist()\n    grad2 = np.round(grad2, 4).tolist()\n    \n    return (y, grad, grad2)\n\n# Example usage:\n# result = apply_affine([1, 2, 3], slope=2, intercept=1)\n# print(result)  # Output: ([3.0, 5.0, 7.0], [2.0, 2.0, 2.0], [0.0, 0.0, 0.0])\n```\n\n### Explanation:\n1. **Input Handling**: The function accepts a 1-D list or NumPy array. It converts the input to a NumPy array for easier element-wise operations.\n2. **Affine Calculation**: The affine activation is computed using the formula \\(y = \\text{slope} \\cdot x + \\text{intercept}\\).\n3. **Derivatives**:\n   - The first derivative (`grad`) is a constant array filled with the value of `slope`.\n   - The second derivative (`grad2`) is a constant array filled with zeros.\n4. **Rounding**: Each of the resulting lists is rounded to four decimal places and converted back to a list before returning.\n5. **Return Value**: The function returns a tuple containing the three lists: the activation output, the first derivative, and the second derivative. \n\nYou can test the function with different inputs to see how it behaves."}
{"task_id": 499, "completion_id": 0, "completion": "Here's a complete implementation of the PCA dimensionality reduction routine as specified in your task. The function `pca_transform` performs all the required steps, including standardization, covariance matrix computation, eigen-decomposition, sorting, and projection onto the principal components.\n\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places. If *k* is outside the\n        valid range, return \u22121.\n    \"\"\"\n    n_samples, n_features = data.shape\n    \n    # Step 1: Check if k is valid\n    if k < 1 or k > n_features:\n        return -1\n    \n    # Step 2: Standardize the data\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0, ddof=1)  # Bessel's correction\n    std_dev[std_dev == 0] = 1  # Avoid division by zero for zero variance features\n    standardized_data = (data - mean) / std_dev\n    \n    # Step 3: Compute the sample covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)  # rowvar=False for features in columns\n    \n    # Step 4: Eigen-decomposition\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n    \n    # Step 5: Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Step 6: Fix the sign of eigenvectors\n    for i in range(sorted_eigenvectors.shape[1]):\n        if sorted_eigenvectors[:, i].max() < 0:\n            sorted_eigenvectors[:, i] *= -1\n    \n    # Step 7: Project the standardized data onto the first k eigenvectors\n    projected_data = standardized_data @ sorted_eigenvectors[:, :k]\n    \n    # Round the projected data to 4 decimal places and convert to a list\n    return np.round(projected_data, 4).tolist()\n\n# Example usage:\n# data = np.array([[1, 2], [3, 4], [5, 6]])\n# k = 1\n# print(pca_transform(data, k))\n```\n\n### Explanation of the Code:\n1. **Input Validation**: The function first checks if `k` is within the valid range. If not, it returns `-1`.\n2. **Standardization**: Each feature is standardized to have zero mean and unit variance. If a feature has zero variance, it is left unchanged by setting its standard deviation to 1.\n3. **Covariance Matrix**: The sample covariance matrix is computed using Bessel's correction (dividing by `n - 1`).\n4. **Eigen-decomposition**: The eigenvalues and eigenvectors of the covariance matrix are computed using NumPy's `linalg.eigh`, which is efficient for symmetric matrices.\n5. **Sorting**: The eigenvalues and corresponding eigenvectors are sorted in descending order.\n6. **Sign Fixing**: The sign of each eigenvector is adjusted so that the entry with the largest absolute value is positive.\n7. **Projection**: The standardized data is projected onto the first `k` eigenvectors.\n8. **Output**: The projected data is rounded to four decimal places and converted to a nested list before being returned.\n\nThis implementation should work efficiently for typical datasets used in PCA."}
{"task_id": 500, "completion_id": 0, "completion": "Here's a complete implementation of a tiny Gradient Boosting binary classifier using decision stumps as weak learners. The code follows the specifications provided in your task description.\n\n```python\nimport numpy as np\n\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int = 20, learning_rate: float = 0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\n\n    The ensemble minimises the squared-error on the *binary* targets and turns the\n    final regression score into a class label by thresholding at 0.5.\n\n    Args:\n        X_train: Training samples, each sample being a list of feature values.\n        y_train: Binary labels (0 or 1) for the training samples.\n        X_test: Samples to predict \u2013 identical structure to X_train.\n        n_estimators: Number of boosting stages to perform.\n        learning_rate: Shrinkage factor (\u03b7) applied to each weak learner.\n\n    Returns:\n        A list containing the predicted class (0 or 1) for every sample in\n        X_test.\n    \"\"\"\n    \n    # Convert inputs to numpy arrays for easier manipulation\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    # Initialize the model with the average of the training labels\n    c = np.mean(y_train)\n    F = np.full(y_train.shape, c)  # Initial prediction for all samples\n    \n    # Store the weak learners\n    stumps = []\n    \n    for _ in range(n_estimators):\n        # Calculate residuals\n        residuals = y_train - F\n        \n        # Find the best stump\n        best_stump = None\n        best_loss = float('inf')\n        \n        for feature_index in range(X_train.shape[1]):\n            thresholds = np.unique(X_train[:, feature_index])\n            for threshold in thresholds:\n                # Predictions for the stump\n                left_prediction = np.mean(y_train[X_train[:, feature_index] <= threshold])\n                right_prediction = np.mean(y_train[X_train[:, feature_index] > threshold])\n                \n                # Calculate predictions\n                stump_predictions = np.where(X_train[:, feature_index] <= threshold, left_prediction, right_prediction)\n                \n                # Calculate loss (squared error)\n                loss = np.sum((residuals - stump_predictions) ** 2)\n                \n                # Update best stump if current loss is lower\n                if loss < best_loss:\n                    best_loss = loss\n                    best_stump = (feature_index, threshold, left_prediction, right_prediction)\n        \n        # Save the best stump\n        stumps.append(best_stump)\n        \n        # Update the model with the new stump\n        feature_index, threshold, left_prediction, right_prediction = best_stump\n        F += learning_rate * np.where(X_train[:, feature_index] <= threshold, left_prediction, right_prediction)\n    \n    # Make predictions on the test set\n    predictions = np.full(X_test.shape[0], c)\n    \n    for feature_index, threshold, left_prediction, right_prediction in stumps:\n        predictions += learning_rate * np.where(X_test[:, feature_index] <= threshold, left_prediction, right_prediction)\n    \n    # Convert predictions to binary class labels\n    return (predictions > 0.5).astype(int).tolist()\n\n# Example usage:\n# X_train = [[1], [2], [3], [4], [5]]\n# y_train = [0, 0, 1, 1, 1]\n# X_test = [[1.5], [3.5], [5.5]]\n# print(gradient_boosting_classifier(X_train, y_train, X_test))\n```\n\n### Explanation:\n1. **Initialization**: The model starts with an initial prediction equal to the average of the training labels.\n2. **Residual Calculation**: For each iteration, the residuals are computed as the difference between the true labels and the current predictions.\n3. **Finding the Best Stump**: For each feature, the code evaluates all possible thresholds to find the best decision stump that minimizes the squared error of the residuals.\n4. **Updating Predictions**: After finding the best stump, the predictions are updated using the learning rate.\n5. **Final Predictions**: After all stumps are added, predictions for the test set are computed, and the final class labels are determined by thresholding at 0.5.\n\nThis implementation is efficient for small datasets and demonstrates the principles of gradient boosting with decision stumps."}
{"task_id": 505, "completion_id": 0, "completion": "Here's the implementation of the `contrastive_divergence` function for a binary Restricted Boltzmann Machine (RBM) using the Contrastive Divergence (CD-k) algorithm. This function performs a single weight update step based on the provided mini-batch of visible vectors.\n\n```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\n\n    Args:\n        X: A 2-D NumPy array of shape (batch_size, n_visible) containing the\n           mini-batch of visible units. Values are expected to be in [0,1].\n        W: Weight matrix of shape (n_visible, n_hidden).\n        hbias: 1-D NumPy array (length n_hidden) with hidden unit biases.\n        vbias: 1-D NumPy array (length n_visible) with visible unit biases.\n        learning_rate: Scalar learning-rate used for the update.\n        k: Number of Gibbs sampling steps to run (CD-k).\n\n    Returns:\n        The updated weight matrix **rounded to 4 decimal places** and converted\n        to a regular Python list of lists.\n    \"\"\"\n    if X.size == 0:\n        return []\n\n    m, n_v = X.shape\n    n_h = W.shape[1]\n\n    # Step 1: Compute positive phase hidden probabilities\n    h0_prob = 1 / (1 + np.exp(-X @ W - hbias))  # Shape: (m, n_h)\n\n    # Step 2: Run k Gibbs sampling steps\n    v_k_prob = X.copy()  # Start with the visible layer as the input\n    for _ in range(k):\n        # Sample hidden probabilities\n        h_k_prob = 1 / (1 + np.exp(-v_k_prob @ W.T - hbias))  # Shape: (m, n_h)\n        \n        # Sample visible probabilities\n        v_k_prob = 1 / (1 + np.exp(-h_k_prob @ W + vbias))  # Shape: (m, n_v)\n\n    # Step 3: Compute positive and negative gradients\n    pos_grad = X.T @ h0_prob  # Shape: (n_v, n_h)\n    neg_grad = v_k_prob.T @ h_k_prob  # Shape: (n_v, n_h)\n\n    # Step 4: Update the weight matrix\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n\n    # Return the updated weight matrix rounded to 4 decimal places\n    return W_new.round(4).tolist()\n```\n\n### Explanation of the Code:\n1. **Input Check**: The function first checks if the input mini-batch `X` is empty. If it is, it returns an empty list.\n2. **Positive Phase**: It computes the hidden probabilities (`h0_prob`) using the visible data and the weights.\n3. **Gibbs Sampling**: It performs `k` Gibbs sampling steps, alternating between calculating hidden probabilities and visible probabilities without stochastic sampling.\n4. **Gradient Calculation**: It computes the positive and negative gradients based on the visible and hidden probabilities.\n5. **Weight Update**: The weights are updated using the calculated gradients and the learning rate.\n6. **Return Value**: Finally, the updated weight matrix is rounded to four decimal places and converted to a list of lists before being returned. \n\nThis implementation adheres to the requirements specified in the task description."}
{"task_id": 509, "completion_id": 0, "completion": "Here's the implementation of the Chebyshev distance calculator as per your specifications:\n\n```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | np.ndarray, y: list[float] | np.ndarray) -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length. If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    # Check if the lengths of the vectors are the same\n    if len(x) != len(y):\n        return -1\n    \n    # Calculate the Chebyshev distance\n    distance = max(abs(xi - yi) for xi, yi in zip(x, y))\n    \n    # Return the distance rounded to four decimal places\n    return round(distance, 4)\n\n# Example usage\nx = [1, 2, 3]\ny = [2, 4, 6]\nprint(chebyshev(x, y))  # Output: 3.0\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the lengths of the two input vectors `x` and `y` are the same. If they are not, it returns `-1`.\n2. **Distance Calculation**: It computes the Chebyshev distance by calculating the maximum of the absolute differences between corresponding elements of the two vectors using a generator expression.\n3. **Rounding**: Finally, it rounds the computed distance to four decimal places before returning it.\n\n### Example Usage:\nYou can test the function with different vectors to see how it behaves. The provided example demonstrates the expected output for the given input vectors."}
{"task_id": 510, "completion_id": 0, "completion": "To implement the Variational Auto-Encoder (VAE) loss function as described, we need to compute two components: the reconstruction loss and the KL divergence. Below is the complete implementation of the `vae_loss` function:\n\n```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    # Numerical stability\n    epsilon = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n\n    # Compute the reconstruction loss (binary cross-entropy)\n    reconstruction_loss = -np.mean(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n\n    # Compute the KL divergence\n    kl_divergence = -0.5 * np.sum(1 + t_log_var - np.square(t_mean) - np.exp(t_log_var), axis=1)\n    kl_divergence_mean = np.mean(kl_divergence)\n\n    # Total loss\n    total_loss = reconstruction_loss + kl_divergence_mean\n\n    return round(total_loss, 6)\n```\n\n### Explanation of the Code:\n1. **Clipping**: We clip the predicted probabilities `y_pred` to avoid taking the logarithm of zero, which would result in numerical instability. The clipping is done using `np.clip` to ensure values are within the range `(\u03b5, 1-\u03b5)`.\n\n2. **Reconstruction Loss**: The reconstruction loss is calculated using the binary cross-entropy formula:\n   \\[\n   \\text{Reconstruction Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)\n   \\]\n   where \\(N\\) is the number of samples in the mini-batch.\n\n3. **KL Divergence**: The KL divergence is computed using the formula provided:\n   \\[\n   \\text{KL} = -\\frac{1}{2} \\sum \\left( 1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2 \\right)\n   \\]\n   This is averaged over the mini-batch.\n\n4. **Total Loss**: The total loss is the sum of the reconstruction loss and the average KL divergence.\n\n5. **Return Value**: Finally, the function returns the total loss rounded to six decimal places.\n\nThis implementation should work correctly for computing the VAE loss for Bernoulli visible units."}
{"task_id": 513, "completion_id": 0, "completion": "Here's a Python function that implements the Factorization Machine regression prediction as described in your task. The function uses NumPy for efficient numerical computations and handles an arbitrary number of samples, features, and latent factors.\n\n```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for efficient computation\n    X = np.array(X)\n    w = np.array(w)\n    V = np.array(V)\n\n    # Calculate the linear part: w0 + \u03a3 w_j * x_j\n    linear_part = w0 + np.dot(X, w)\n\n    # Calculate the interaction part\n    # Step 1: Calculate the sum of V_j * x_j for each sample\n    interactions = np.dot(X, V)  # Shape: (n_samples, k)\n\n    # Step 2: Calculate the squared sum of interactions\n    squared_interactions_sum = np.square(interactions).sum(axis=1)  # Shape: (n_samples,)\n\n    # Step 3: Calculate the sum of squared V_j * x_j\n    squared_XV = np.dot(X, V**2)  # Shape: (n_samples, k)\n    squared_XV_sum = squared_XV.sum(axis=1)  # Shape: (n_samples,)\n\n    # Step 4: Calculate the interaction part\n    interaction_part = 0.5 * (squared_interactions_sum - squared_XV_sum)\n\n    # Final predictions\n    predictions = linear_part + interaction_part\n\n    # Round the predictions to 4 decimal places\n    return np.round(predictions, 4).tolist()\n```\n\n### Explanation:\n1. **Input Conversion**: The function converts the input lists into NumPy arrays for efficient matrix operations.\n2. **Linear Part Calculation**: It computes the linear part of the prediction using the dot product of the feature matrix `X` and the weight vector `w`, adding the bias term `w0`.\n3. **Interaction Part Calculation**:\n   - It first computes the interaction terms by calculating the dot product of `X` and the latent factors `V`.\n   - It then calculates the squared sum of these interactions and the sum of squared terms.\n   - Finally, it computes the interaction part using the formula provided.\n4. **Final Prediction**: The linear and interaction parts are summed to get the final predictions, which are rounded to four decimal places before being returned as a list.\n\nThis function should work correctly for any well-formed input as specified in your requirements."}
{"task_id": 517, "completion_id": 0, "completion": "Here's the implementation of the `sigmoid` function that meets all the specified requirements:\n\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    # Convert input to a NumPy array for element-wise operations\n    z = np.asarray(z)\n\n    # Compute the sigmoid in a numerically stable way\n    # Using the fact that sigmoid(z) = 1 / (1 + exp(-z))\n    # We can avoid overflow by using np.clip\n    z_clipped = np.clip(z, -700, 700)  # Clip values to avoid overflow\n    sigmoid_values = 1 / (1 + np.exp(-z_clipped))\n\n    # Round the results to four decimal places\n    sigmoid_values = np.round(sigmoid_values, 4)\n\n    # Check if the input was a scalar\n    if np.isscalar(z):\n        return float(sigmoid_values)  # Return as float if input was scalar\n    else:\n        return sigmoid_values.tolist()  # Return as list if input was not scalar\n\n# Example usage\nresult = sigmoid([-1, 0, 1])\nprint(result)  # Output: [0.2689, 0.5, 0.7311]\n```\n\n### Explanation:\n1. **Input Handling**: The function first converts the input `z` into a NumPy array using `np.asarray(z)`, which allows for easy element-wise operations.\n2. **Numerical Stability**: The values of `z` are clipped to the range of -700 to 700 using `np.clip(z, -700, 700)`. This prevents overflow when calculating the exponential function for very large or very small values.\n3. **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( g(z) = \\frac{1}{1 + e^{-z}} \\).\n4. **Rounding**: The results are rounded to four decimal places using `np.round()`.\n5. **Return Type**: The function checks if the input was a scalar using `np.isscalar(z)`. If it was, it returns a float; otherwise, it returns a list of the computed sigmoid values.\n\nThis implementation ensures that the function behaves correctly for all specified input types and maintains numerical stability."}
{"task_id": 518, "completion_id": 0, "completion": "Here's the implementation of the `unhot` decorator that meets the specified requirements. This decorator will convert one-hot encoded NumPy arrays into their corresponding integer label representations before passing them to the metric function.\n\n```python\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n    def wrapper(actual: np.ndarray, predicted: np.ndarray) -> float:\n        # Check if actual is one-hot encoded\n        if actual.ndim == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        \n        # Check if predicted is one-hot encoded\n        if predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        \n        # Call the original metric function with the possibly modified arrays\n        return function(actual, predicted)\n    \n    return wrapper\n\n# Example usage\n@unhot\ndef accuracy(actual: np.ndarray, predicted: np.ndarray) -> float:\n    \"\"\"Simple accuracy rounded to 4 decimals.\"\"\"\n    return float(np.round(np.mean(actual == predicted), 4))\n\n# Test the decorator\nactual = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])  # one-hot\npredicted = np.array([[0, 1, 0], [0, 1, 0], [0, 0, 1]])  # one-hot\nprint(accuracy(actual, predicted))  # Output: 0.6667\n```\n\n### Explanation:\n1. **Decorator Definition**: The `unhot` function is defined as a decorator that takes a metric function as an argument.\n2. **Wrapper Function**: Inside the decorator, a `wrapper` function is defined that performs the necessary checks and conversions.\n3. **One-Hot Check**: The wrapper checks if `actual` and `predicted` are two-dimensional arrays with more than one column. If so, it uses `argmax` to convert them to one-dimensional arrays of class labels.\n4. **Function Call**: After potentially modifying the inputs, the original metric function is called with the processed arrays, and its return value is returned unchanged.\n5. **Example Usage**: The decorator is applied to an `accuracy` function, and the example demonstrates how it works with one-hot encoded inputs.\n\nThis implementation ensures that the metric function receives the correct format of labels while maintaining the integrity of the output."}
{"task_id": 520, "completion_id": 0, "completion": "Here's the implementation of the `sigmoid_grad` function that meets the specified requirements:\n\n```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  \n    The derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    # Handle empty input\n    if isinstance(z, list) and not z:\n        return []\n\n    # Convert input to a NumPy array for easier manipulation\n    z_array = np.array(z)\n\n    # Compute the gradient\n    grad = z_array * (1 - z_array)\n\n    # Round the result to 4 decimal places\n    grad_rounded = np.round(grad, 4)\n\n    # Return the result in the same structure as the input\n    if isinstance(z, list):\n        return grad_rounded.tolist()\n    else:\n        return grad_rounded.item() if np.ndim(grad_rounded) == 0 else grad_rounded\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if the input is an empty list and returns an empty list if true.\n2. **Conversion to NumPy Array**: The input is converted to a NumPy array to facilitate element-wise operations.\n3. **Gradient Calculation**: The gradient is calculated using the formula \\( g'(z) = z \\cdot (1 - z) \\).\n4. **Rounding**: The results are rounded to four decimal places using `np.round`.\n5. **Output Structure**: The function checks the type of the input and returns the output in the same structure (float for scalars, list for nested lists).\n\n### Example Usage:\n```python\nprint(sigmoid_grad(0.5))  # Output: 0.25\nprint(sigmoid_grad([[0.2, 0.5], [0.8]]))  # Output: [[0.16, 0.25], [0.16]]\nprint(sigmoid_grad([]))  # Output: []\n``` \n\nThis implementation should work correctly for scalars, lists, and NumPy arrays, while maintaining the required output structure."}
{"task_id": 528, "completion_id": 0, "completion": "Here's the implementation of the `decision_boundary_grid` function as per your requirements. This function generates a grid of predictions for a 2-D linear classifier based on the provided weights, bias, and input data.\n\n```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise. The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    # Convert X to a NumPy array for easier manipulation\n    X = np.array(X)\n    \n    # Determine the min and max for each dimension\n    x1_min, x1_max = X[:, 0].min(), X[:, 0].max()\n    x2_min, x2_max = X[:, 1].min(), X[:, 1].max()\n    \n    # Create equally spaced 1-D arrays for the grid\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    \n    # Create a meshgrid from the 1-D arrays\n    x1_grid, x2_grid = np.meshgrid(x1_plot, x2_plot)\n    \n    # Flatten the grid points for classification\n    grid_points = np.c_[x1_grid.ravel(), x2_grid.ravel()]\n    \n    # Calculate the linear combination for classification\n    z = W[0] * grid_points[:, 0] + W[1] * grid_points[:, 1] + b\n    \n    # Apply the sign function to classify the points\n    labels = np.sign(z)\n    \n    # Convert labels to -1 where the sign is 0 (if any)\n    labels[labels == 0] = -1\n    \n    # Reshape the labels back to a 2D grid\n    labels_reshaped = labels.reshape((grid_n, grid_n))\n    \n    # Convert the NumPy array back to a nested list\n    return labels_reshaped.tolist()\n\n# Example usage:\n# X = [[1, 2], [2, 3], [3, 1], [5, 4]]\n# W = [1, -1]\n# b = 0\n# grid = decision_boundary_grid(X, W, b)\n# print(grid)\n```\n\n### Explanation:\n1. **Input Conversion**: The input dataset `X` is converted to a NumPy array for easier manipulation.\n2. **Grid Creation**: The minimum and maximum values for each dimension are calculated to create a grid that spans the bounding box of the data. The `np.linspace` function generates equally spaced values.\n3. **Meshgrid**: The `np.meshgrid` function creates a grid of points from the 1-D arrays.\n4. **Classification**: Each point in the grid is classified using the linear combination defined by the weights and bias. The `np.sign` function is used to determine the labels.\n5. **Reshape and Convert**: The labels are reshaped back into a 2D array and converted to a nested list before returning.\n\nThis function will return a grid of predictions that can be used for visualizing decision boundaries in a contour plot."}
{"task_id": 537, "completion_id": 0, "completion": "Here's a complete implementation of the Gaussian Naive Bayes classifier from scratch, following the specifications you provided:\n\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\n\n    Training phase: estimate mean, standard deviation and class prior for\n    every feature of both classes.\n    Prediction phase: compute the posterior probability for each class\n    using the Gaussian likelihood (with feature independence assumption)\n    and return the most probable class (0 or 1) for every test sample.\n\n    Args:\n        X_train: 2-D NumPy array of shape (n_samples, n_features) containing\n                  the training features.\n        y_train: 1-D NumPy array of length n_samples with binary labels\n                  (each entry is 0 or 1).\n        X_test:  2-D NumPy array of shape (m_samples, n_features) containing\n                  the data to classify.\n\n    Returns:\n        A Python list of length m_samples where each element is the\n        predicted class label (0 or 1).\n    \"\"\"\n    # Constants\n    epsilon = 1e-9\n    \n    # Calculate means, stds, and priors for each class\n    classes = np.unique(y_train)\n    n_features = X_train.shape[1]\n    \n    means = {}\n    stds = {}\n    priors = {}\n    \n    for c in classes:\n        X_c = X_train[y_train == c]\n        means[c] = np.mean(X_c, axis=0)\n        stds[c] = np.std(X_c, axis=0) + epsilon  # Add epsilon to avoid division by zero\n        priors[c] = X_c.shape[0] / X_train.shape[0]  # Class prior\n\n    # Prediction phase\n    predictions = []\n    \n    for x in X_test:\n        posteriors = {}\n        \n        for c in classes:\n            # Calculate the likelihood for each feature\n            likelihood = np.prod(\n                (1 / (np.sqrt(2 * np.pi) * stds[c])) * \n                np.exp(-0.5 * ((x - means[c]) ** 2) / (stds[c] ** 2))\n            )\n            # Calculate the posterior probability\n            posteriors[c] = likelihood * priors[c]\n        \n        # Choose the class with the highest posterior probability\n        predicted_class = max(posteriors, key=posteriors.get)\n        predictions.append(predicted_class)\n    \n    return predictions\n```\n\n### Explanation of the Code:\n1. **Parameter Estimation**:\n   - For each class (0 and 1), we calculate the mean and standard deviation of each feature using the training data.\n   - We also compute the class prior, which is the proportion of samples belonging to each class.\n\n2. **Prediction**:\n   - For each test sample, we compute the likelihood of the sample belonging to each class using the Gaussian probability density function.\n   - We then calculate the posterior probability for each class by multiplying the likelihood by the class prior.\n   - The predicted class for each test sample is the one with the highest posterior probability.\n\n3. **Numerical Stability**:\n   - A small constant `epsilon` is added to the standard deviation to prevent division by zero.\n\n### Usage:\nYou can use this function by passing your training data (`X_train`, `y_train`) and your test data (`X_test`). The function will return a list of predicted class labels for the test samples."}
{"task_id": 539, "completion_id": 0, "completion": "Here's the implementation of the binary cross-entropy cost function as per your requirements:\n\n```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\"\"\"\n    # Ensure AL and Y are numpy arrays\n    AL = np.array(AL)\n    Y = np.array(Y)\n    \n    # Clip the probabilities to avoid log(0)\n    epsilon = 1e-15\n    AL_clipped = np.clip(AL, epsilon, 1 - epsilon)\n    \n    # Calculate the binary cross-entropy cost\n    m = Y.shape[0]  # number of training examples\n    cost = -np.sum(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped)) / m\n    \n    return float(np.round(cost, 4))\n\n# Example usage:\n# AL = np.array([[0.9, 0.1, 0.8]])\n# Y = np.array([[1, 0, 1]])\n# print(compute_cost(AL, Y))  # Output: cost value\n```\n\n### Explanation:\n1. **Input Handling**: The function accepts two NumPy arrays, `AL` (predicted probabilities) and `Y` (true labels).\n2. **Clipping**: The probabilities in `AL` are clipped to the range \\([1 \\times 10^{-15}, 1 - 1 \\times 10^{-15}]\\) to prevent taking the logarithm of zero, which would result in numerical instability.\n3. **Cost Calculation**: The binary cross-entropy cost is computed using the provided formula, summing over all training examples and dividing by the number of examples `m`.\n4. **Return Value**: The cost is returned as a Python float, rounded to four decimal places.\n\nYou can test the function with different inputs to ensure it behaves as expected."}
{"task_id": 552, "completion_id": 0, "completion": "To implement the function that computes the posterior state distribution \\( \\gamma_t(i) \\) for a Hidden Markov Model (HMM), we will follow the steps outlined in the task description. The function will compute the forward probabilities \\( \\alpha \\), the backward probabilities \\( \\beta \\), and then combine them to get \\( \\gamma_t(i) \\).\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation must use the forward-backward algorithm described in\n    the task description and return the resulting vector rounded to four\n    decimal places.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'A' and 'B'.\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t.\n    \"\"\"\n    # Check if t is within the valid range\n    if t < 0 or t >= len(observations):\n        return []\n\n    # Extract HMM parameters\n    pi = hmm['pi']  # Initial state distribution\n    A = hmm['A']    # Transition matrix\n    B = hmm['B']    # Emission matrix\n\n    n_states = len(pi)  # Number of hidden states\n    n_observations = len(observations)\n\n    # Step 1: Compute forward probabilities (\u03b1)\n    alpha = np.zeros((n_observations, n_states))\n    \n    # Initialization\n    alpha[0] = pi * B[:, observations[0]]\n    \n    # Induction\n    for t in range(1, n_observations):\n        for j in range(n_states):\n            alpha[t, j] = np.sum(alpha[t-1] * A[:, j]) * B[j, observations[t]]\n\n    # Step 2: Compute backward probabilities (\u03b2)\n    beta = np.zeros((n_observations, n_states))\n    \n    # Initialization\n    beta[n_observations - 1] = 1  # Base case for the last observation\n    \n    # Induction\n    for t in range(n_observations - 2, -1, -1):\n        for i in range(n_states):\n            beta[t, i] = np.sum(A[i, :] * B[:, observations[t + 1]] * beta[t + 1])\n\n    # Step 3: Compute \u03b3\u209c(i)\n    gamma_t = np.zeros(n_states)\n    for i in range(n_states):\n        gamma_t[i] = alpha[t, i] * beta[t, i]\n\n    # Normalize \u03b3\u209c\n    gamma_t /= np.sum(gamma_t)\n\n    # Round to four decimal places and return as a list\n    return np.round(gamma_t, 4).tolist()\n```\n\n### Explanation of the Code:\n1. **Input Validation**: The function first checks if the time index \\( t \\) is within the valid range of the observation sequence. If not, it returns an empty list.\n\n2. **Parameter Extraction**: The initial state distribution \\( \\pi \\), transition matrix \\( A \\), and emission matrix \\( B \\) are extracted from the input dictionary.\n\n3. **Forward Probabilities Calculation**:\n   - The forward probabilities \\( \\alpha \\) are computed using dynamic programming. The first observation is initialized using the initial state distribution and the emission probabilities. Subsequent probabilities are calculated based on the previous time step's probabilities and the transition and emission probabilities.\n\n4. **Backward Probabilities Calculation**:\n   - The backward probabilities \\( \\beta \\) are computed similarly, but in reverse order. The last observation is initialized to 1, and the probabilities are calculated backwards.\n\n5. **Posterior Probability Calculation**:\n   - The posterior probabilities \\( \\gamma_t(i) \\) are computed by multiplying the forward and backward probabilities for each state at time \\( t \\) and then normalizing them.\n\n6. **Rounding and Return**: Finally, the resulting probabilities are rounded to four decimal places and returned as a list.\n\nThis implementation efficiently computes the desired posterior state distribution for a given HMM and observation sequence."}
{"task_id": 555, "completion_id": 0, "completion": "Here's a complete implementation of the DBSCAN algorithm in Python, following the specifications you've provided. The code computes pairwise Euclidean distances, builds a neighborhood list, identifies core points, and expands clusters using a breadth-first search (BFS) approach.\n\n```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample. Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    \n    n_samples = data.shape[0]\n    labels = [-1] * n_samples  # Initialize all points as noise\n    cluster_id = 0  # Start cluster id from 0\n\n    # Function to compute the Euclidean distance\n    def euclidean_distance(point1, point2):\n        return np.sqrt(np.sum((point1 - point2) ** 2))\n\n    # Build the neighborhood list\n    def region_query(point_idx):\n        neighbors = []\n        for idx in range(n_samples):\n            if euclidean_distance(data[point_idx], data[idx]) <= eps:\n                neighbors.append(idx)\n        return neighbors\n\n    # Identify core points\n    core_points = []\n    for i in range(n_samples):\n        neighbors = region_query(i)\n        if len(neighbors) >= min_samples:\n            core_points.append(i)\n\n    # Expand clusters\n    for core_point in core_points:\n        if labels[core_point] != -1:  # Already labeled\n            continue\n        \n        cluster_id += 1  # New cluster\n        labels[core_point] = cluster_id  # Label the core point\n        queue = deque([core_point])  # Initialize the queue with the core point\n\n        while queue:\n            current_point = queue.popleft()\n            neighbors = region_query(current_point)\n\n            for neighbor in neighbors:\n                if labels[neighbor] == -1:  # If it's noise, change it to the current cluster\n                    labels[neighbor] = cluster_id\n                if labels[neighbor] == 0:  # If it's unvisited\n                    labels[neighbor] = cluster_id\n                    queue.append(neighbor)  # Add to the queue\n                    # Check if the neighbor is a core point\n                    if len(region_query(neighbor)) >= min_samples:\n                        queue.append(neighbor)  # Expand the cluster\n\n    return labels\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    data = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])\n    eps = 1.5\n    min_samples = 2\n    cluster_labels = dbscan(data, eps, min_samples)\n    print(cluster_labels)  # Output cluster labels\n```\n\n### Explanation of the Code:\n1. **Initialization**: The `labels` list is initialized to -1 (indicating noise). The `cluster_id` starts at 0.\n2. **Distance Calculation**: A helper function `euclidean_distance` computes the distance between two points.\n3. **Neighborhood Query**: The `region_query` function finds all points within the `eps` radius of a given point.\n4. **Core Point Identification**: We iterate through all points to find core points based on the `min_samples` criterion.\n5. **Cluster Expansion**: For each core point, if it hasn't been labeled, we start a new cluster and use a queue to perform a breadth-first search to label all reachable points.\n6. **Return**: Finally, the function returns the list of cluster labels.\n\n### Note:\n- The cluster labels start from 1 for the first cluster, and noise points are labeled as -1.\n- You can adjust the `eps` and `min_samples` parameters to see how the clustering changes with different datasets."}
{"task_id": 561, "completion_id": 0, "completion": "Here's a complete implementation of the Gaussian Mixture Model (GMM) using the Expectation-Maximization (EM) algorithm in Python. The function `gmm_em` initializes the parameters, performs the E and M steps, and assigns cluster labels as specified.\n\n```python\nimport numpy as np\nimport math\n\ndef gmm_em(data: np.ndarray, k: int, max_iter: int = 200, epsilon: float = 1e-4) -> list[int]:\n    \"\"\"Performs Gaussian Mixture Model clustering using the Expectation-Maximization algorithm.\n\n    Args:\n        data (np.ndarray): A 2-D array of shape (N, D) where N is the number of samples and D is the number of features.\n        k (int): The number of Gaussian components.\n        max_iter (int): The maximum number of iterations for the EM algorithm.\n        epsilon (float): The convergence threshold for the change in mixture weights.\n\n    Returns:\n        list[int]: A list of length N containing the cluster labels (0 to K-1) for each sample.\n    \"\"\"\n    n_samples, n_features = data.shape\n    \n    # Step 1: Randomly initialize parameters\n    weights = np.ones(k) / k  # Mixture weights\n    means = data[np.random.choice(n_samples, k, replace=False)]  # Randomly chosen data points as means\n    covariances = np.array([np.eye(n_features)] * k)  # Identity matrices as initial covariances\n\n    # Step 2: EM algorithm\n    for iteration in range(max_iter):\n        # E-step: Calculate responsibilities\n        responsibilities = np.zeros((n_samples, k))\n        \n        for j in range(k):\n            # Multivariate Gaussian probability density function\n            diff = data - means[j]\n            cov_inv = np.linalg.inv(covariances[j])\n            norm_const = 1 / np.sqrt((2 * np.pi) ** n_features * np.linalg.det(covariances[j]))\n            exponent = -0.5 * np.einsum('ij,jk->i', diff, np.dot(cov_inv, diff.T))\n            responsibilities[:, j] = weights[j] * norm_const * np.exp(exponent)\n        \n        # Normalize responsibilities\n        responsibilities_sum = responsibilities.sum(axis=1, keepdims=True)\n        responsibilities /= responsibilities_sum\n        \n        # M-step: Update parameters\n        weights = responsibilities.mean(axis=0)\n        for j in range(k):\n            # Update means\n            means[j] = np.dot(responsibilities[:, j], data) / responsibilities[:, j].sum()\n            # Update covariances\n            diff = data - means[j]\n            covariances[j] = np.dot(responsibilities[:, j] * diff.T, diff) / responsibilities[:, j].sum()\n        \n        # Check for convergence\n        if np.max(np.abs(weights - responsibilities.mean(axis=0))) < epsilon:\n            break\n\n    # Step 3: Assign cluster labels\n    cluster_labels = np.argmax(responsibilities, axis=1)\n\n    # Step 4: Sort components by the first coordinate of their mean and relabel\n    sorted_indices = np.argsort(means[:, 0])\n    relabeled_clusters = np.zeros_like(cluster_labels)\n    \n    for new_label, old_label in enumerate(sorted_indices):\n        relabeled_clusters[cluster_labels == old_label] = new_label\n\n    return relabeled_clusters.tolist()\n\n# Example usage:\n# data = np.random.rand(100, 2)  # 100 samples, 2 features\n# labels = gmm_em(data, k=3)\n# print(labels)\n```\n\n### Explanation:\n1. **Initialization**: The mixture weights are initialized uniformly, means are chosen randomly from the data, and covariances are initialized as identity matrices.\n2. **E-step**: The responsibilities (posterior probabilities) are calculated for each sample and each Gaussian component using the multivariate Gaussian probability density function.\n3. **M-step**: The parameters (weights, means, and covariances) are updated based on the responsibilities.\n4. **Convergence Check**: The algorithm checks if the change in weights is below the specified tolerance.\n5. **Label Assignment**: After convergence, each sample is assigned to the component with the highest responsibility. The components are then sorted based on the first coordinate of their means, and the labels are relabeled accordingly to ensure determinism.\n\nThis implementation should meet all the requirements specified in the task."}
{"task_id": 562, "completion_id": 0, "completion": "Here's a complete implementation of the Spectral Clustering algorithm as per your specifications:\n\n```python\nimport numpy as np\n\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A list of length N where the i-th element is an integer label identifying the\n        cluster assignment of the i-th sample (labels are 0-based).\n    \"\"\"\n    N = data.shape[0]\n    \n    # Step 1: Build the similarity graph using the Gaussian kernel\n    W = np.zeros((N, N))\n    for i in range(N):\n        for j in range(N):\n            if i != j:\n                W[i, j] = np.exp(-gamma * np.sum((data[i] - data[j]) ** 2))\n    \n    # Step 2: Construct the un-normalized Laplacian L = D - W\n    D = np.diag(np.sum(W, axis=1))\n    L = D - W\n    \n    # Step 3: Compute the eigenvectors corresponding to the n_cluster smallest eigenvalues\n    eigenvalues, eigenvectors = np.linalg.eig(L)\n    idx = np.argsort(eigenvalues)[:n_cluster]\n    selected_eigenvectors = eigenvectors[:, idx]\n    \n    # If normalized method is chosen, normalize the eigenvector matrix\n    if method == 'normalized':\n        row_sums = np.linalg.norm(selected_eigenvectors, axis=1, keepdims=True)\n        selected_eigenvectors = selected_eigenvectors / row_sums\n    \n    # Step 4: Run k-means in the eigenvector space\n    centroids = selected_eigenvectors[:n_cluster]\n    labels = np.zeros(N, dtype=int)\n    prev_labels = np.ones(N, dtype=int) * -1\n    \n    while not np.array_equal(labels, prev_labels):\n        prev_labels = labels.copy()\n        \n        # Assign labels based on closest centroid\n        distances = np.linalg.norm(selected_eigenvectors[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        # Update centroids\n        for k in range(n_cluster):\n            if np.any(labels == k):\n                centroids[k] = np.mean(selected_eigenvectors[labels == k], axis=0)\n    \n    # Step 5: Relabel clusters\n    relabeled = np.zeros(n_cluster, dtype=int)\n    for k in range(n_cluster):\n        relabeled[k] = np.argmin(np.where(labels == k)[0])  # Find the smallest index in each cluster\n    \n    final_labels = np.zeros(N, dtype=int)\n    for k in range(n_cluster):\n        final_labels[labels == k] = relabeled[k]\n    \n    return final_labels.tolist()\n\n# Example usage:\n# data = np.random.rand(10, 2)  # 10 samples, 2 features\n# print(spectral_clustering(data, n_cluster=3))\n```\n\n### Explanation of the Code:\n1. **Similarity Graph Construction**: We compute the pairwise similarities using the Gaussian kernel.\n2. **Laplacian Matrix**: We construct the un-normalized Laplacian matrix \\( L = D - W \\).\n3. **Eigenvalue Decomposition**: We compute the eigenvalues and eigenvectors of the Laplacian matrix and select the smallest `n_cluster` eigenvectors.\n4. **K-Means Clustering**: We perform a deterministic k-means clustering on the selected eigenvectors.\n5. **Relabeling**: Finally, we relabel the clusters based on the smallest original index in each cluster.\n\nThis implementation adheres to the constraints and requirements specified in your task."}
{"task_id": 563, "completion_id": 0, "completion": "Here's the implementation of the dual-form perceptron learning algorithm as per your specifications:\n\n```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    N, d = X_data.shape\n    alpha = np.zeros(N)\n    b = 0.0\n    \n    # Build the Gram matrix G\n    G = np.dot(X_data, X_data.T)\n\n    for _ in range(max_iter):\n        updated = False\n        \n        for i in range(N):\n            g = np.sum(alpha * y_data * G[:, i])\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n        \n        # If no updates were made, we can stop\n        if not updated:\n            break\n\n    # Recover the weight vector\n    w = np.sum((alpha * y_data).reshape(-1, 1) * X_data, axis=0)\n\n    # Round the results to 4 decimal places\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = round(b, 4)\n\n    return w_rounded, b_rounded\n```\n\n### Explanation of the Code:\n1. **Initialization**: We initialize the `alpha` coefficients to zero and the bias `b` to zero.\n2. **Gram Matrix**: We compute the Gram matrix `G`, where each entry `G[i, j]` is the dot product of the feature vectors `X_data[i]` and `X_data[j]`.\n3. **Training Loop**: We iterate up to `max_iter` times. In each iteration, we check each sample:\n   - Compute the inner product `g` using the current `alpha` values and the Gram matrix.\n   - If the condition `y_i * (g + b) <= 0` is met, we update `alpha[i]` and `b`.\n   - We track if any updates were made; if not, we break out of the loop early.\n4. **Weight Recovery**: After training, we recover the weight vector `w` using the formula provided.\n5. **Rounding**: Finally, we round the components of `w` and `b` to four decimal places before returning them.\n\nThis implementation adheres to the algorithm you specified and should work correctly for training a dual-form perceptron."}
{"task_id": 564, "completion_id": 0, "completion": "Here's the implementation of the `predict_sample` function that traverses a decision tree based on the provided rules for both classification and regression tasks:\n\n```python\nclass Node:\n    def __init__(self, label=None, x=None, s=None, y=None, child=None):\n        self.label = label  # Index of the feature to inspect (for internal nodes)\n        self.x = x          # Value associated with the edge (for classification)\n        self.s = s          # Split point (for regression)\n        self.y = y          # Value kept in the leaf (prediction)\n        self.child = child if child is not None else []  # List of children\n\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    # Start at the root node\n    current_node = root\n    \n    while True:\n        # If the current node is a leaf, return its value\n        if current_node.y is not None:\n            return current_node.y\n        \n        # Otherwise, we are at an internal node\n        feature_value = features[current_node.label]\n        \n        if task_type == 'classification':\n            # For classification, find the appropriate child based on feature value\n            found_child = False\n            for child in current_node.child:\n                if child.x == feature_value:\n                    current_node = child\n                    found_child = True\n                    break\n            \n            # If no child matched, default to the second child\n            if not found_child:\n                current_node = current_node.child[1]\n        \n        elif task_type == 'regression':\n            # For regression, compare the feature value with the split point\n            if feature_value <= current_node.s:\n                current_node = current_node.child[0]\n            else:\n                current_node = current_node.child[1]\n```\n\n### Explanation:\n1. **Node Class**: The `Node` class is defined to represent each node in the decision tree. It can store information about the feature index, edge value, split point, leaf value, and children.\n\n2. **predict_sample Function**:\n   - The function starts at the root node and enters a loop to traverse the tree.\n   - If the current node is a leaf (indicated by `y` being not `None`), it returns the value stored in `y`.\n   - For classification tasks, it checks the feature value against the `x` values of the children. If a match is found, it moves to that child; otherwise, it defaults to the second child.\n   - For regression tasks, it compares the feature value with the split point `s` to decide which child to traverse next.\n\nThis implementation adheres to the specified traversal rules for both classification and regression decision trees."}
{"task_id": 565, "completion_id": 0, "completion": "To implement the utility function for selecting the best feature based on maximum information gain in the ID3 decision-tree learning algorithm, we need to follow these steps:\n\n1. **Calculate the entropy of the label distribution** \\( H(y) \\).\n2. **For each feature** in the dataset, calculate the conditional entropy \\( H(y|A) \\) after splitting the dataset based on the values of that feature.\n3. **Compute the information gain** for each feature using the formula \\( IG(A) = H(y) - H(y|A) \\).\n4. **Select the feature** with the maximum information gain, and in case of ties, choose the feature with the smallest index.\n\nHere's the implementation of the `choose_best_feature` function:\n\n```python\nimport numpy as np\nimport math\n\ndef entropy(labels):\n    \"\"\"Calculate the entropy of the label distribution.\"\"\"\n    value, counts = np.unique(labels, return_counts=True)\n    probabilities = counts / counts.sum()\n    return -np.sum(probabilities * np.log2(probabilities + 1e-10))  # Adding a small value to avoid log(0)\n\ndef conditional_entropy(X, y, feature_index):\n    \"\"\"Calculate the conditional entropy H(y|A) for a given feature.\"\"\"\n    feature_values, counts = np.unique(X[:, feature_index], return_counts=True)\n    total_count = y.shape[0]\n    \n    cond_entropy = 0.0\n    for value, count in zip(feature_values, counts):\n        subset_labels = y[X[:, feature_index] == value]\n        subset_entropy = entropy(subset_labels)\n        cond_entropy += (count / total_count) * subset_entropy\n    \n    return cond_entropy\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n    # Calculate the entropy of the labels\n    H_y = entropy(y)\n    \n    max_information_gain = -1\n    best_feature_index = -1\n    \n    # Iterate over each feature to calculate information gain\n    n_features = X.shape[1]\n    for feature_index in range(n_features):\n        H_y_given_A = conditional_entropy(X, y, feature_index)\n        information_gain = H_y - H_y_given_A\n        \n        # Check if this is the best feature so far\n        if information_gain > max_information_gain:\n            max_information_gain = information_gain\n            best_feature_index = feature_index\n        elif information_gain == max_information_gain:\n            # If there's a tie, choose the smallest index\n            best_feature_index = min(best_feature_index, feature_index)\n    \n    return (best_feature_index, round(max_information_gain, 6))\n```\n\n### Explanation of the Code:\n- **Entropy Calculation**: The `entropy` function computes the entropy of the label distribution using the formula \\( H(y) = -\\sum p(y) \\log_2(p(y)) \\).\n- **Conditional Entropy Calculation**: The `conditional_entropy` function computes the conditional entropy \\( H(y|A) \\) for a given feature by iterating over the unique values of that feature and calculating the entropy of the corresponding subsets of labels.\n- **Main Function**: The `choose_best_feature` function calculates the information gain for each feature and keeps track of the maximum gain and the corresponding feature index. It returns the index of the feature with the highest information gain and the gain itself rounded to six decimal places.\n\nThis implementation should work efficiently for datasets with discrete non-negative integer values as specified."}
{"task_id": 566, "completion_id": 0, "completion": "Here's a complete implementation of the Mini Isolation Forest algorithm for outlier detection as per your specifications:\n\n```python\nimport numpy as np\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation.\n\n    Args:\n        data: 2-D NumPy array where each row is an observation and each column a feature.\n        n_trees: Number of random isolation trees that will be grown.\n        sample_size: Number of samples drawn (without replacement) to grow every tree.\n        epsilon: Fraction (0 \u2264 \u03b5 < 1) of observations that are considered outliers.\n\n    Returns:\n        A list with the zero-based indices of the detected outliers, sorted increasingly.\n    \"\"\"\n    np.random.seed(42)  # Set the random seed for reproducibility\n    n_samples, n_features = data.shape\n    \n    if epsilon == 0:\n        return []  # No outliers if epsilon is 0\n\n    # Calculate \u03c6\n    phi = 2 * np.log(n_samples - 1) - 2 * (n_samples - 1) / n_samples\n\n    # Function to compute the expected path length correction\n    def c(n):\n        if n <= 1:\n            return 0\n        else:\n            return 2 * np.log(n - 1) + 0.5772156649 - 2 * (n - 1) / n\n\n    # Function to build a single isolation tree\n    def build_tree(subsample):\n        if len(subsample) <= 1 or depth >= height_limit:\n            return 0  # Leaf node, path length is 0\n        \n        # Randomly select a feature and a threshold\n        feature_index = np.random.randint(n_features)\n        feature_values = subsample[:, feature_index]\n        threshold = np.random.uniform(np.min(feature_values), np.max(feature_values))\n        \n        # Split the subsample\n        left_subsample = subsample[subsample[:, feature_index] < threshold]\n        right_subsample = subsample[subsample[:, feature_index] >= threshold]\n        \n        # Recursively build the left and right trees\n        left_length = build_tree(left_subsample)\n        right_length = build_tree(right_subsample)\n        \n        # Return the path length (1 for the edge to the next node + lengths of left and right)\n        return 1 + left_length + right_length\n\n    # Initialize path lengths for each observation\n    path_lengths = np.zeros(n_samples)\n\n    # Build n_trees isolation trees\n    height_limit = math.ceil(np.log2(sample_size))\n    for _ in range(n_trees):\n        # Draw a random subsample\n        if sample_size >= n_samples:\n            subsample = data\n        else:\n            indices = np.random.choice(n_samples, sample_size, replace=False)\n            subsample = data[indices]\n        \n        # Calculate path lengths for each observation in the subsample\n        for i in range(len(subsample)):\n            depth = 0\n            path_length = build_tree(subsample)\n            path_lengths[indices[i]] += path_length  # Update the path length for the original index\n\n    # Average path lengths\n    path_lengths /= n_trees\n\n    # Calculate anomaly scores\n    scores = 2 ** (-path_lengths / phi)\n\n    # Determine the threshold for outliers\n    threshold_index = int(n_samples * (1 - epsilon))\n    threshold_score = np.partition(scores, threshold_index)[threshold_index]\n\n    # Get the indices of the outliers\n    outlier_indices = np.where(scores >= threshold_score)[0]\n\n    return sorted(outlier_indices.tolist())\n\n# Example usage:\n# data = np.random.rand(100, 2)  # Example data\n# outliers = detect_outliers(data, n_trees=100, sample_size=50, epsilon=0.1)\n# print(outliers)\n```\n\n### Explanation:\n1. **Random Seed**: The random seed is set to ensure reproducibility.\n2. **Parameters**: The function takes in the data, number of trees, sample size, and epsilon.\n3. **Path Length Calculation**: The function builds isolation trees recursively, calculating the path length for each observation.\n4. **Anomaly Score Calculation**: The average path lengths are used to compute the anomaly scores.\n5. **Outlier Detection**: The function identifies outliers based on the specified epsilon and returns their indices sorted in ascending order.\n\nYou can test the function with your own data to see how it performs!"}
{"task_id": 568, "completion_id": 0, "completion": "To implement a Maximum Entropy (MaxEnt) classifier using the Generalized Iterative Scaling (GIS) algorithm, we need to follow a structured approach. Below is the complete implementation of the `maxent_predict` function, which includes building empirical distributions, updating weights iteratively, and predicting labels for the test set.\n\nHere's the code:\n\n```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    # Step 1: Build empirical distributions\n    feature_counts = defaultdict(lambda: defaultdict(int))\n    label_counts = defaultdict(int)\n    \n    # Count occurrences of each feature-value pair for each label\n    for x, y in zip(X_train, y_train):\n        label_counts[y] += 1\n        for feature_index, feature_value in enumerate(x):\n            feature_counts[(feature_index, feature_value)][y] += 1\n    \n    # Calculate the total number of samples\n    total_samples = len(y_train)\n    \n    # Step 2: Initialize weights\n    weights = defaultdict(float)\n    for (feature_index, feature_value), labels in feature_counts.items():\n        for label in labels:\n            weights[(feature_index, feature_value, label)] = 1.0  # Initialize weights to 1.0\n\n    # Step 3: GIS algorithm\n    for iteration in range(n_iter):\n        # Calculate the expected counts based on current weights\n        expected_counts = defaultdict(float)\n        for x, y in zip(X_train, y_train):\n            # Calculate the unnormalized probabilities\n            unnormalized_probs = defaultdict(float)\n            for feature_index, feature_value in enumerate(x):\n                for label in label_counts.keys():\n                    unnormalized_probs[label] += weights[(feature_index, feature_value, label)]\n            \n            # Calculate the partition function Z(x)\n            Z_x = sum(math.exp(unnormalized_prob) for unnormalized_prob in unnormalized_probs.values())\n            \n            # Calculate probabilities\n            probabilities = {label: math.exp(unnormalized_probs[label]) / Z_x for label in label_counts.keys()}\n            \n            # Update expected counts\n            for feature_index, feature_value in enumerate(x):\n                for label in label_counts.keys():\n                    expected_counts[(feature_index, feature_value, label)] += probabilities[label]\n        \n        # Update weights\n        max_update = 0.0\n        for (feature_index, feature_value), labels in feature_counts.items():\n            for label in labels:\n                empirical_count = feature_counts[(feature_index, feature_value)][label] / total_samples\n                expected_count = expected_counts[(feature_index, feature_value, label)]\n                update = empirical_count - expected_count\n                weights[(feature_index, feature_value, label)] += update\n                max_update = max(max_update, abs(update))\n        \n        # Check for convergence\n        if max_update < epsilon:\n            break\n\n    # Step 4: Predict labels for X_test\n    predictions = []\n    for x in X_test:\n        unnormalized_probs = defaultdict(float)\n        for feature_index, feature_value in enumerate(x):\n            for label in label_counts.keys():\n                unnormalized_probs[label] += weights[(feature_index, feature_value, label)]\n        \n        # Calculate the partition function Z(x)\n        Z_x = sum(math.exp(unnormalized_prob) for unnormalized_prob in unnormalized_probs.values())\n        \n        # Calculate probabilities\n        probabilities = {label: math.exp(unnormalized_probs[label]) / Z_x for label in label_counts.keys()}\n        \n        # Predict the label with the highest probability\n        predicted_label = max(probabilities, key=probabilities.get)\n        predictions.append(predicted_label)\n\n    return predictions\n```\n\n### Explanation of the Code:\n1. **Empirical Distribution Building**: We count how many times each feature-value pair occurs for each label in the training data.\n2. **Weight Initialization**: We initialize the weights for each feature-value-label combination to 1.0.\n3. **GIS Algorithm**: We iteratively update the weights based on the difference between empirical counts and expected counts until convergence (when the maximum update is less than `epsilon`) or until we reach the maximum number of iterations (`n_iter`).\n4. **Prediction**: For each test sample, we compute the probabilities for each label and select the one with the highest probability.\n\nThis implementation should work effectively for categorical data and provide predictions based on the learned MaxEnt model."}
{"task_id": 569, "completion_id": 0, "completion": "Here's a complete implementation of the `statistic_key_word` function that meets the requirements outlined in your task description. The function calculates the document-frequency ratio for each distinct word and returns a sorted list of these ratios along with a set of the most common words based on the `cut_off` parameter.\n\n```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio must be rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                  ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of (word, ratio) pairs and (2) a set of the most\n        frequent words as described above.\n    \"\"\"\n    # Count the total number of documents\n    total_documents = len(data)\n    \n    # Use a set to track unique words across documents\n    word_document_count = Counter()\n    \n    # Iterate through each document\n    for _, words in data:\n        # Use a set to avoid counting the same word multiple times in the same document\n        unique_words = set(words)\n        word_document_count.update(unique_words)\n    \n    # Calculate the document-frequency ratio for each word\n    word_ratios = [\n        (word, round(count / total_documents, 4)) \n        for word, count in word_document_count.items()\n    ]\n    \n    # Sort the list by the document-frequency ratio in descending order\n    word_ratios.sort(key=lambda x: x[1], reverse=True)\n    \n    # Prepare the set of most common words based on cut_off\n    if cut_off is None:\n        most_common_words = set(word_document_count.keys())\n    elif cut_off > 0:\n        most_common_words = set(word for word, _ in word_ratios[:cut_off])\n    else:\n        most_common_words = set()\n    \n    return word_ratios, most_common_words\n```\n\n### Explanation:\n1. **Counting Documents**: The function first counts the total number of documents.\n2. **Unique Word Tracking**: It uses a `Counter` to keep track of how many documents each word appears in. A set is used to ensure that each word is only counted once per document.\n3. **Calculating Ratios**: After counting, it calculates the document-frequency ratio for each word and rounds it to four decimal places.\n4. **Sorting**: The list of words and their ratios is sorted in descending order based on the ratio.\n5. **Handling `cut_off`**: Depending on the value of `cut_off`, it prepares a set of the most common words or all words if `cut_off` is `None`.\n\n### Usage Example:\nYou can use the function like this:\n\n```python\ndocuments = [\n    (\"doc1\", [\"apple\", \"banana\", \"apple\"]),\n    (\"doc2\", [\"banana\", \"kiwi\"]),\n    (\"doc3\", [\"apple\", \"kiwi\", \"kiwi\"]),\n]\n\nratios, common_words = statistic_key_word(documents, cut_off=2)\nprint(ratios)        # List of (word, ratio) pairs\nprint(common_words)  # Set of most common words\n```\n\nThis will give you the document-frequency ratios and the most common words based on the specified `cut_off`."}
{"task_id": 571, "completion_id": 0, "completion": "Here's the implementation of the `compute_svm_parameters` function that computes the weight vector and bias term for a linear Support Vector Machine using the provided formulas. The function takes in the training matrix `X`, the label vector `y`, and the Lagrange multipliers `alpha`, and returns the weight vector and bias term rounded to four decimal places.\n\n```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    # Compute the weight vector w\n    w = np.sum(alpha[:, np.newaxis] * y[:, np.newaxis] * X, axis=0)\n    \n    # Compute the bias term b\n    b = np.mean(y - np.dot(X, w))\n    \n    # Round the results to 4 decimal places\n    w_rounded = np.round(w, 4)\n    b_rounded = round(b, 4)\n    \n    # Convert w to a list, ensuring it's a 1D list if X has only one feature\n    w_list = w_rounded.tolist() if w_rounded.ndim > 1 else w_rounded.flatten().tolist()\n    \n    return w_list, b_rounded\n```\n\n### Explanation:\n1. **Weight Vector Calculation**: The weight vector `w` is computed using the formula \\( \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_i y_i \\mathbf{x}_i \\). This is done using NumPy's broadcasting to multiply `alpha`, `y`, and `X`, and then summing along the appropriate axis.\n\n2. **Bias Calculation**: The bias `b` is calculated using the mean of the differences between the actual labels `y` and the predicted values obtained from the dot product of `X` and `w`.\n\n3. **Rounding**: Both the weight vector and bias are rounded to four decimal places.\n\n4. **Output Format**: The weight vector is converted to a list. If `X` has only one feature, it ensures that `w` is returned as a one-dimensional list.\n\nThis function can now be used to compute the parameters of a linear SVM after training with the SMO algorithm."}
{"task_id": 572, "completion_id": 0, "completion": "Here's a complete implementation of the Local Outlier Factor (LOF) algorithm in Python, following the steps outlined in your task description. The code computes the LOF scores for a given dataset and identifies the indices of samples that are considered outliers based on a specified threshold (`epsilon`).\n\n```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"Compute the Local Outlier Factor (LOF) for a given dataset.\"\"\"\n    \n    # Step 1: Compute the pair-wise Euclidean distance matrix\n    n = data.shape[0]\n    dist_matrix = np.linalg.norm(data[:, np.newaxis] - data, axis=2)\n    \n    # Step 2: Compute k-distance and k-nearest neighbors for each sample\n    k_distances = np.zeros(n)\n    k_nearest_neighbors = np.zeros((n, k), dtype=int)\n    \n    for i in range(n):\n        # Get the indices of the k nearest neighbors (excluding the point itself)\n        neighbors_indices = np.argsort(dist_matrix[i])[:k + 1][1:]  # Exclude the point itself\n        k_nearest_neighbors[i] = neighbors_indices\n        k_distances[i] = dist_matrix[i, neighbors_indices[-1]]  # k-distance is the distance to the k-th nearest neighbor\n    \n    # Step 3: Compute the reachability distance\n    reachability_distances = np.zeros((n, n))\n    \n    for i in range(n):\n        for j in range(n):\n            if j in k_nearest_neighbors[i]:\n                reachability_distances[i, j] = max(k_distances[j], dist_matrix[i, j])\n            else:\n                reachability_distances[i, j] = np.inf  # Not a neighbor, set to infinity\n    \n    # Step 4: Compute Local Reachability Density (LRD)\n    LRD = np.zeros(n)\n    \n    for i in range(n):\n        sum_reach_dist = np.sum(reachability_distances[i, k_nearest_neighbors[i]])\n        LRD[i] = k / sum_reach_dist if sum_reach_dist > 0 else 0  # Avoid division by zero\n    \n    # Step 5: Compute Local Outlier Factor (LOF)\n    LOF = np.zeros(n)\n    \n    for i in range(n):\n        sum_LRD = np.sum(LRD[k_nearest_neighbors[i]])\n        LOF[i] = (sum_LRD / k) / LRD[i] if LRD[i] > 0 else 0  # Avoid division by zero\n    \n    # Round LOF scores to 4 decimal places\n    LOF_rounded = np.round(LOF, 4)\n    \n    # Get indices of outliers\n    outlier_indices = np.where(LOF_rounded > epsilon)[0].tolist()\n    \n    return (LOF_rounded.tolist(), outlier_indices)\n\n# Example usage:\n# data = np.array([[...], [...], ...])  # Your dataset here\n# k = 5  # Number of neighbors\n# epsilon = 1.5  # LOF threshold\n# scores, outliers = local_outlier_factor(data, k, epsilon)\n```\n\n### Explanation of the Code:\n1. **Distance Matrix Calculation**: We compute the pairwise Euclidean distance matrix using NumPy's broadcasting capabilities.\n2. **k-Nearest Neighbors**: For each sample, we find the indices of its k nearest neighbors and compute the k-distance.\n3. **Reachability Distance**: We calculate the reachability distance for each pair of points based on the k-distance and the actual distance.\n4. **Local Reachability Density (LRD)**: We compute the LRD for each sample based on the reachability distances of its k nearest neighbors.\n5. **Local Outlier Factor (LOF)**: Finally, we compute the LOF score for each sample and round it to four decimal places.\n6. **Outlier Detection**: We identify the indices of samples whose LOF score exceeds the specified `epsilon`.\n\nThis implementation is efficient and adheres to the constraints provided in the task description."}
{"task_id": 574, "completion_id": 0, "completion": "Here's a complete implementation of the `generate_data` function that meets the specified requirements:\n\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility. If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n\n    # Initialize lists to hold the data\n    X_train = []\n    X_val = []\n    y_train = []\n    y_val = []\n\n    for c in range(m):\n        # Calculate the base for the current class\n        base = 10 * (c // 2)\n        \n        # Generate training points\n        x_train = np.random.uniform(base + 1, base + 9, n_train)\n        y_train_points = np.random.uniform(base + 1, base + 9, n_train)\n        \n        # Append to the lists\n        X_train.extend(np.column_stack((x_train, y_train_points)))\n        y_train.extend([c] * n_train)\n\n        # Generate validation points\n        x_val = np.random.uniform(base + 1, base + 9, n_val)\n        y_val_points = np.random.uniform(base + 1, base + 9, n_val)\n        \n        # Append to the lists\n        X_val.extend(np.column_stack((x_val, y_val_points)))\n        y_val.extend([c] * n_val)\n\n    # Convert lists to NumPy arrays and round to 4 decimal places\n    X_train = np.round(np.array(X_train), 4)\n    X_val = np.round(np.array(X_val), 4)\n    y_train = np.array(y_train)\n    y_val = np.array(y_val)\n\n    return X_train, X_val, y_train, y_val\n```\n\n### Explanation:\n1. **Parameters**: The function takes the number of classes `m`, the number of training points per class `n_train`, the number of validation points per class `n_val`, and an optional `seed` for reproducibility.\n  \n2. **Random Seed**: If a seed is provided, it sets the seed for both the `random` and `numpy` random number generators to ensure reproducibility.\n\n3. **Data Generation**:\n   - For each class `c`, it calculates the `base` value.\n   - It generates `n_train` training points and `n_val` validation points for both x and y coordinates using a uniform distribution within the specified range.\n   - The generated points are stored in lists, which are later converted to NumPy arrays.\n\n4. **Rounding**: The coordinates are rounded to 4 decimal places to meet the output constraints.\n\n5. **Return Value**: The function returns a tuple containing the training and validation datasets along with their corresponding labels. \n\nYou can call this function with the desired parameters to generate synthetic 2-D data for your experiments or visualizations."}
{"task_id": 579, "completion_id": 0, "completion": "Here's a complete implementation of the K-Means clustering algorithm as per your specifications. The code follows the steps outlined in your task description, ensuring that the algorithm is deterministic and adheres to the constraints provided.\n\n```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres. Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached. The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    m, d = X.shape\n    \n    # Step 1: Initialization\n    centres = X[:k].copy()  # Take the first k points as initial centres\n    labels = np.zeros(m, dtype=int)  # Initialize labels for each point\n\n    for iteration in range(max_iters):\n        # Step 2: Assignment step\n        new_labels = np.zeros(m, dtype=int)\n        for i in range(m):\n            distances = np.linalg.norm(X[i] - centres, axis=1)  # Compute distances to each centre\n            new_labels[i] = np.argmin(distances)  # Assign to the nearest centre\n\n        # Check for convergence (if labels do not change)\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # Step 3: Update step\n        for j in range(k):\n            assigned_points = X[labels == j]  # Get points assigned to cluster j\n            if len(assigned_points) > 0:\n                centres[j] = np.mean(assigned_points, axis=0)  # Update centre to mean of assigned points\n            # If no points are assigned, the centre remains unchanged\n\n    # Round the centres to 4 decimal places\n    centres = np.round(centres, 4).tolist()\n\n    return labels.tolist(), centres\n\n# Example usage:\n# X = np.array([[1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0]])\n# k = 3\n# labels, centres = kmeans(X, k)\n# print(\"Labels:\", labels)\n# print(\"Centres:\", centres)\n```\n\n### Explanation of the Code:\n1. **Initialization**: The first `k` points from `X` are used as the initial cluster centers. The labels for each point are initialized to zero.\n\n2. **Assignment Step**: For each point, the Euclidean distance to each cluster center is calculated, and the point is assigned to the nearest center. In case of a tie, the center with the smaller index is chosen.\n\n3. **Update Step**: For each cluster, the new center is computed as the mean of all points assigned to that cluster. If a cluster has no points assigned, its center remains unchanged.\n\n4. **Convergence Check**: The algorithm checks if the labels have changed. If they haven't, the algorithm stops early.\n\n5. **Rounding**: Finally, the cluster centers are rounded to four decimal places before returning.\n\nThis implementation is efficient and adheres to the constraints provided in your task description."}
{"task_id": 581, "completion_id": 0, "completion": "Here's a complete implementation of the Elastic-Net regression using batch gradient descent as per your specifications:\n\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation.\n\n    Args:\n        X: A 2-D NumPy array of shape (m, n) containing the input features.\n        y: A 1-D NumPy array of length m containing the target values.\n        learning_rate: Step size for gradient descent (\u03b1).\n        iterations: Number of optimisation steps to perform.\n        l1_penalty: L1 regularisation strength (\u03bb\u2081).\n        l2_penalty: L2 regularisation strength (\u03bb\u2082).\n\n    Returns:\n        Tuple (weights, bias) where weights is a list of length n and bias is a\n        float.  Every number must be rounded to four decimal places.\n    \"\"\"\n    # Initialize weights and bias\n    m, n = X.shape\n    w = np.zeros(n)\n    b = 0.0\n\n    # If learning rate or iterations are zero, return initial parameters\n    if learning_rate == 0 or iterations == 0:\n        return [0.0] * n, 0.0\n\n    # Gradient descent\n    for _ in range(iterations):\n        # Compute predictions\n        y_hat = X.dot(w) + b\n        \n        # Compute residuals\n        r = y - y_hat\n        \n        # Compute gradients\n        gradient_w = -2 * X.T.dot(r) + l1_penalty * np.sign(w) + 2 * l2_penalty * w\n        gradient_b = -2 * np.sum(r)\n        \n        # Update weights and bias\n        w -= learning_rate * gradient_w / m\n        b -= learning_rate * gradient_b / m\n\n    # Round the results to four decimal places\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = round(b, 4)\n\n    return w_rounded, b_rounded\n```\n\n### Explanation of the Code:\n1. **Initialization**: The weights `w` and bias `b` are initialized to zero. The shape of `X` is used to determine the number of samples `m` and features `n`.\n\n2. **Early Exit**: If the learning rate or the number of iterations is zero, the function returns the initial parameters.\n\n3. **Gradient Descent Loop**: For each iteration:\n   - Predictions `y_hat` are computed using the current weights and bias.\n   - Residuals `r` are calculated as the difference between the actual values `y` and the predicted values `y_hat`.\n   - Gradients for weights and bias are computed using the specified formulas.\n   - Weights and bias are updated using the gradients and the learning rate.\n\n4. **Rounding**: After all iterations, the weights and bias are rounded to four decimal places before returning.\n\n### Usage:\nYou can call this function with your data `X`, target values `y`, and the specified parameters to get the learned weights and bias."}
{"task_id": 591, "completion_id": 0, "completion": "To implement the feed-forward neural network prediction function, we will follow the steps outlined in the task description. We'll use NumPy for efficient numerical operations, particularly for the dot product calculations. Here's how we can implement the `neural_net_predict` function:\n\n```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Perform a forward pass through a fully-connected ReLU neural network.\n\n    The network topology and parameters are encoded in *weights* where:\n      \u2022 *weights[L]* \u2013 weight matrix of layer L,\n      \u2022 each *weights[L][i]* \u2013 list of weights of neuron i of layer L,\n      \u2022 the first weight of every neuron is its bias weight.\n\n    All hidden layers use ReLU, the last layer is linear.\n\n    Args:\n        weights: A three-level nested list containing the network\u2019s weights.\n        x:       Input feature vector (without the bias term).\n\n    Returns:\n        The prediction rounded to four decimals (float if the output layer has\n        one neuron, otherwise list of floats).\n    \"\"\"\n    # Start with the input vector\n    current_input = np.array(x)\n    \n    # Iterate through each layer\n    for layer_weights in weights:\n        # Add bias term (1) to the input\n        input_with_bias = np.concatenate(([1], current_input))\n        \n        # Calculate the output for the current layer\n        layer_output = np.dot(np.array(layer_weights), input_with_bias)\n        \n        # Apply ReLU activation function for all layers except the last one\n        current_input = np.maximum(0, layer_output)\n    \n    # Round the final output to four decimals\n    final_output = np.round(current_input, 4)\n    \n    # If there's only one output neuron, return a float, otherwise return a list\n    if final_output.size == 1:\n        return float(final_output[0])\n    else:\n        return final_output.tolist()\n\n# Example usage:\n# weights = [\n#     [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]],  # Layer 1 weights (2 neurons, 3 inputs)\n#     [[0.7, 0.8], [0.9, 1.0]]              # Layer 2 weights (2 neurons, 2 inputs)\n# ]\n# x = [1.0, 2.0, 3.0]\n# print(neural_net_predict(weights, x))  # Example prediction\n```\n\n### Explanation of the Code:\n1. **Input Handling**: The input vector `x` is converted to a NumPy array for efficient numerical operations.\n2. **Layer Iteration**: We loop through each layer's weights:\n   - We prepend a bias term (1) to the current input using `np.concatenate`.\n   - We compute the output of the layer using the dot product of the weights and the input with bias.\n   - We apply the ReLU activation function using `np.maximum(0, layer_output)` for all layers except the last one.\n3. **Final Output**: After processing all layers, we round the output to four decimal places. If there is only one output neuron, we return it as a float; otherwise, we return a list of floats.\n\nThis implementation adheres to the constraints and requirements specified in the task description."}
{"task_id": 595, "completion_id": 0, "completion": "Here's a complete implementation of the K-Means clustering algorithm that computes centroids only, following the specifications you've provided:\n\n```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Compute centroids using K-Means clustering algorithm.\"\"\"\n    \n    # Step 1: Initialize centroids with the first k samples in X\n    centroids = X[:k].copy()\n    \n    for _ in range(max_iters):\n        # Step 2: Assign each sample to the nearest centroid\n        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2) ** 2\n        labels = np.argmin(distances, axis=1)\n        \n        # Step 3: Update centroids\n        new_centroids = np.zeros_like(centroids)\n        for i in range(k):\n            assigned_samples = X[labels == i]\n            if len(assigned_samples) > 0:\n                new_centroids[i] = assigned_samples.mean(axis=0)\n            else:\n                new_centroids[i] = centroids[i]  # Keep the old centroid if no samples are assigned\n        \n        # Step 4: Check for convergence\n        if np.linalg.norm(new_centroids - centroids) < 1e-4:\n            break\n        \n        centroids = new_centroids\n    \n    # Step 5: Sort centroids and round to 4 decimal places\n    sorted_centroids = sorted(map(lambda c: [round(coord, 4) for coord in c], centroids))\n    \n    return sorted_centroids\n```\n\n### Explanation of the Code:\n1. **Initialization**: The centroids are initialized with the first `k` samples from the dataset `X`.\n2. **Distance Calculation**: For each sample in `X`, the squared Euclidean distance to each centroid is calculated.\n3. **Label Assignment**: Each sample is assigned to the nearest centroid based on the computed distances.\n4. **Centroid Update**: For each cluster, the new centroid is computed as the mean of all samples assigned to that cluster. If a cluster has no samples assigned, the centroid remains unchanged.\n5. **Convergence Check**: The algorithm checks if the centroids have moved less than `1e-4` from their previous positions. If so, it stops early.\n6. **Sorting and Rounding**: Finally, the centroids are sorted in ascending lexicographical order and rounded to four decimal places before being returned as a list of lists.\n\n### Usage:\nYou can use this function by passing a 2-D NumPy array and the desired number of clusters. For example:\n\n```python\nX = np.array([[1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0]])\nk = 3\ncentroids = kmeans_centroids(X, k)\nprint(centroids)\n```\n\nThis will compute the centroids for the given dataset `X` with `k` clusters."}
