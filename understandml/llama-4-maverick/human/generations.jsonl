{"task_id": 1, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Check the input types and dimensions**: The function takes a 2D list `a` representing a matrix and a 1D list `b` representing a vector. We need to check if the number of columns in `a` is equal to the length of `b`.\n2. **Validate the matrix and vector dimensions**: To perform a dot product, the number of columns in the matrix `a` must be equal to the length of the vector `b`. If not, the function should return -1.\n3. **Perform the dot product operation**: For each row in the matrix `a`, we need to calculate the dot product with the vector `b`. This involves multiplying corresponding elements and summing them up.\n4. **Return the resulting vector**: The result of the dot product operation should be a list where each element is the dot product of a row of `a` with `b`.\n\n# Fixed solution:\n```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float] | int:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n\n    Args:\n    a (list[list[int|float]]): A 2D list representing a matrix.\n    b (list[int|float]): A 1D list representing a vector.\n\n    Returns:\n    list[int|float] | int: A list representing the resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n\n    # Check if the input matrix is empty\n    if not a:\n        return -1\n\n    # Get the number of columns in the matrix 'a'\n    num_cols_a = len(a[0])\n\n    # Check if the number of columns in 'a' matches the length of 'b'\n    if num_cols_a != len(b):\n        return -1\n\n    # Check if all rows in 'a' have the same number of columns\n    if not all(len(row) == num_cols_a for row in a):\n        raise ValueError(\"All rows in the matrix 'a' must have the same number of columns\")\n\n    # Initialize an empty list to store the result\n    result = []\n\n    # Iterate over each row in the matrix 'a'\n    for row in a:\n        # Calculate the dot product of the current row with 'b'\n        dot_product = sum(x * y for x, y in zip(row, b))\n        \n        # Append the dot product to the result list\n        result.append(dot_product)\n\n    # Return the resulting vector\n    return result\n```\n\n# Explanation of changes:\n*   **Added input validation**: We check if the input matrix `a` is empty and return -1 in this case.\n*   **Checked the number of columns in the matrix**: We verify that the number of columns in `a` matches the length of `b` before performing the dot product.\n*   **Validated the matrix structure**: We ensure that all rows in `a` have the same number of columns to avoid potential errors.\n*   **Used list comprehension and built-in functions**: We utilized the `zip` function to iterate over corresponding elements in the rows of `a` and `b`, and the `sum` function to calculate the dot product.\n*   **Improved code readability**: We added comments and docstrings to explain the purpose of the function, its arguments, and its return value.\n\n# Tests and example uses:\n```python\n# Test case 1: Compatible matrix and vector dimensions\nmatrix = [[1, 2, 3], [4, 5, 6]]\nvector = [7, 8, 9]\nprint(matrix_dot_vector(matrix, vector))  # Output: [50, 122]\n\n# Test case 2: Incompatible matrix and vector dimensions\nmatrix = [[1, 2, 3], [4, 5, 6]]\nvector = [7, 8]\nprint(matrix_dot_vector(matrix, vector))  # Output: -1\n\n# Test case 3: Empty matrix\nmatrix = []\nvector = [7, 8, 9]\nprint(matrix_dot_vector(matrix, vector))  # Output: -1\n\n# Test case 4: Matrix with rows of different lengths\nmatrix = [[1, 2, 3], [4, 5]]\nvector = [7, 8, 9]\ntry:\n    print(matrix_dot_vector(matrix, vector))\nexcept ValueError as e:\n    print(e)  # Output: All rows in the matrix 'a' must have the same number of columns\n```"}
{"task_id": 2, "completion_id": 0, "completion": "## Transpose of a Matrix\n### Task Description\nThe task is to write a Python function that computes the transpose of a given matrix.\n\n### Code Solution\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    This function computes the transpose of a given matrix.\n\n    Args:\n    a (list[list[int|float]]): A 2D list representing the input matrix.\n\n    Returns:\n    list[list[int|float]]: The transpose of the input matrix.\n    \"\"\"\n    # Check if the input is a list of lists\n    if not isinstance(a, list) or not all(isinstance(row, list) for row in a):\n        raise ValueError(\"Input must be a list of lists\")\n\n    # Check if the input matrix is empty\n    if not a:\n        return []\n\n    # Check if all rows have the same number of columns\n    num_cols = len(a[0])\n    if not all(len(row) == num_cols for row in a):\n        raise ValueError(\"All rows must have the same number of columns\")\n\n    # Use the built-in zip function to transpose the matrix\n    # The * operator unpacks the list of lists into separate arguments for zip\n    # The result is a zip object which is an iterator of tuples\n    # We convert each tuple to a list using a list comprehension\n    return [list(row) for row in zip(*a)]\n\n# Example usage:\ndef main():\n    # Define a sample matrix\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n\n    # Compute the transpose of the matrix\n    transposed_matrix = transpose_matrix(matrix)\n    print(\"\\nTransposed Matrix:\")\n    for row in transposed_matrix:\n        print(row)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Explanation\n\n*   The `transpose_matrix` function takes a 2D list `a` as input and returns its transpose.\n*   It first checks if the input is a list of lists and if all rows have the same number of columns.\n*   The function uses the built-in `zip` function with the `*` operator to transpose the matrix. The `zip` function returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The `*` operator unpacks the list of lists into separate arguments for `zip`.\n*   The result is converted to a list of lists using a list comprehension.\n*   The `main` function demonstrates the usage of `transpose_matrix` with a sample matrix.\n\n### Output\n\n```\nOriginal Matrix:\n[1, 2, 3]\n[4, 5, 6]\n[7, 8, 9]\n\nTransposed Matrix:\n[1, 4, 7]\n[2, 5, 8]\n[3, 6, 9]\n```"}
{"task_id": 3, "completion_id": 0, "completion": "## Reshaping a Matrix using NumPy\n### Problem Statement\nThe task is to write a Python function that reshapes a given matrix into a specified shape. If the matrix cannot be reshaped, the function should return an empty list.\n\n### Code Solution\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshapes a given matrix into a specified shape.\n\n    Args:\n    a (list[list[int|float]]): The input matrix.\n    new_shape (tuple[int, int]): The desired shape of the output matrix.\n\n    Returns:\n    list[list[int|float]]: The reshaped matrix if possible, otherwise an empty list.\n    \"\"\"\n    \n    # Convert the input list to a NumPy array\n    array = np.array(a)\n    \n    # Check if the array can be reshaped into the new shape\n    if array.size != new_shape[0] * new_shape[1]:\n        return []\n    \n    # Reshape the array and convert it back to a list\n    reshaped_array = array.reshape(new_shape)\n    reshaped_list = reshaped_array.tolist()\n    \n    return reshaped_list\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [[1, 2, 3], [4, 5, 6]]\n    new_shape = (3, 2)\n    print(reshape_matrix(matrix, new_shape))  # Output: [[1, 2], [3, 4], [5, 6]]\n    \n    invalid_shape = (2, 4)\n    print(reshape_matrix(matrix, invalid_shape))  # Output: []\n```\n\n### Explanation\n\n1. The function `reshape_matrix` takes a 2D list `a` and a tuple `new_shape` as input.\n2. It first converts the input list to a NumPy array using `np.array(a)`.\n3. It checks if the total number of elements in the array is equal to the product of the dimensions in `new_shape`. If not, it returns an empty list.\n4. If the array can be reshaped, it uses the `reshape` method to change the shape of the array to `new_shape`.\n5. Finally, it converts the reshaped NumPy array back to a list using the `tolist` method and returns the result.\n\n### Advice\n\n* This function relies on NumPy for efficient array operations. Make sure to have NumPy installed in your Python environment.\n* The function is designed to work with both integer and floating-point numbers in the input matrix.\n* The example usage demonstrates how to call the function with a sample matrix and different shapes."}
{"task_id": 4, "completion_id": 0, "completion": "## Task: Calculate Mean by Row or Column\n### Description\nThe task is to write a Python function that calculates the mean of a matrix either by row or by column based on a given mode.\n\n### Code\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Args:\n    - matrix (list[list[float]]): A 2D list of numbers.\n    - mode (str): The mode of calculation, either 'row' or 'column'.\n\n    Returns:\n    - list[float]: A list of means according to the specified mode.\n\n    Raises:\n    - ValueError: If the mode is neither 'row' nor 'column'.\n    - ValueError: If the matrix is empty or not a list of lists.\n    - ZeroDivisionError: If a row or column is empty.\n    \"\"\"\n\n    # Check if the matrix is valid\n    if not matrix or not all(isinstance(row, list) for row in matrix):\n        raise ValueError(\"Matrix must be a non-empty list of lists\")\n\n    # Check if the mode is valid\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n\n    # Calculate mean by row\n    if mode == 'row':\n        # Use list comprehension to calculate the mean of each row\n        return [sum(row) / len(row) for row in matrix]\n\n    # Calculate mean by column\n    elif mode == 'column':\n        # Use zip to transpose the matrix, then calculate the mean of each column\n        # Check if all rows have the same length\n        row_lengths = set(map(len, matrix))\n        if len(row_lengths) > 1:\n            raise ValueError(\"All rows must have the same length for column mode\")\n        \n        # Calculate the mean of each column\n        return [sum(col) / len(col) for col in zip(*matrix)]\n\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    print(calculate_matrix_mean(matrix, 'row'))  # Output: [2.0, 5.0, 8.0]\n    print(calculate_matrix_mean(matrix, 'column'))  # Output: [4.0, 5.0, 6.0]\n```\n\n### Explanation\n\n*   The function `calculate_matrix_mean` takes a matrix and a mode as input and returns a list of means.\n*   It first checks if the matrix is valid (not empty and a list of lists) and if the mode is either 'row' or 'column'. If not, it raises a `ValueError`.\n*   For 'row' mode, it calculates the mean of each row using list comprehension.\n*   For 'column' mode, it first checks if all rows have the same length. If not, it raises a `ValueError`. Then, it transposes the matrix using `zip(*matrix)` and calculates the mean of each column.\n*   The example usage demonstrates how to use the function with a sample matrix."}
{"task_id": 5, "completion_id": 0, "completion": "### Scalar Multiplication of a Matrix\n#### Task Description\nThe task is to write a Python function that performs scalar multiplication on a given matrix.\n\n#### Code Solution\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    This function performs scalar multiplication on a given matrix.\n\n    Args:\n    matrix (list[list[int|float]]): A 2D list representing the matrix.\n    scalar (int|float): The scalar value to multiply the matrix with.\n\n    Returns:\n    list[list[int|float]]: The result of the scalar multiplication.\n    \"\"\"\n    # Use list comprehension to multiply each element in the matrix by the scalar\n    return [[element * scalar for element in row] for row in matrix]\n\n# Example usage:\ndef main():\n    # Define a sample matrix\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    scalar = 2\n    \n    # Perform scalar multiplication\n    result = scalar_multiply(matrix, scalar)\n    \n    # Print the original matrix and the result\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n    print(f\"\\nScalar: {scalar}\")\n    print(\"\\nResult:\")\n    for row in result:\n        print(row)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n#### Explanation\n\n*   The `scalar_multiply` function takes a 2D list (`matrix`) and a scalar value (`scalar`) as input.\n*   It uses list comprehension to iterate over each element in the matrix and multiplies it by the scalar.\n*   The result is a new 2D list where each element is the product of the corresponding element in the original matrix and the scalar.\n*   The `main` function demonstrates the usage of `scalar_multiply` with a sample matrix and scalar.\n\n#### Output\n\n```\nOriginal Matrix:\n[1, 2, 3]\n[4, 5, 6]\n[7, 8, 9]\n\nScalar: 2\n\nResult:\n[2, 4, 6]\n[8, 10, 12]\n[14, 16, 18]\n```"}
{"task_id": 6, "completion_id": 0, "completion": "## Calculating Eigenvalues of a 2x2 Matrix\n### Task Description\nThe task is to write a Python function that calculates the eigenvalues of a given 2x2 matrix. The function should return a list containing the eigenvalues sorted in descending order.\n\n### Code Solution\n```python\nimport math\nfrom typing import Union, List\n\ndef calculate_eigenvalues(matrix: List[List[Union[float, int]]]) -> List[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Args:\n    matrix (List[List[Union[float, int]]]): A 2x2 matrix.\n\n    Returns:\n    List[float]: A list containing the eigenvalues in descending order.\n\n    Raises:\n    ValueError: If the input matrix is not a 2x2 matrix.\n    \"\"\"\n\n    # Check if the input matrix is a 2x2 matrix\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError(\"Input matrix must be a 2x2 matrix\")\n\n    # Extract the elements of the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n\n    # Calculate the trace and determinant of the matrix\n    trace = a + d\n    determinant = a * d - b * c\n\n    # Calculate the eigenvalues using the quadratic formula\n    eigenvalue1 = (trace + math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue2 = (trace - math.sqrt(trace ** 2 - 4 * determinant)) / 2\n\n    # Return the eigenvalues in descending order\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [[1, 2], [3, 4]]\n    eigenvalues = calculate_eigenvalues(matrix)\n    print(\"Eigenvalues:\", eigenvalues)\n```\n\n### Explanation\n\n1.  The function `calculate_eigenvalues` takes a 2x2 matrix as input and returns a list of its eigenvalues.\n2.  It first checks if the input is a valid 2x2 matrix and raises a `ValueError` if not.\n3.  The eigenvalues of a 2x2 matrix `[[a, b], [c, d]]` are calculated using the formula derived from the characteristic equation `|A - \u03bbI| = 0`, where `A` is the matrix, `\u03bb` is the eigenvalue, and `I` is the identity matrix.\n4.  The characteristic equation for a 2x2 matrix simplifies to a quadratic equation: `\u03bb^2 - (a + d)\u03bb + (ad - bc) = 0`, where `a + d` is the trace of the matrix and `ad - bc` is the determinant.\n5.  The roots of this quadratic equation (eigenvalues) are calculated using the quadratic formula.\n6.  Finally, the function returns the eigenvalues in descending order.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `calculate_eigenvalues` function with a sample 2x2 matrix `[[1, 2], [3, 4]]`. The calculated eigenvalues are then printed to the console."}
{"task_id": 7, "completion_id": 0, "completion": "### Matrix Transformation Function\n#### Overview\n\nThe following Python function performs a matrix transformation using the operation $T^{-1}AS$, where $T$ and $S$ are invertible matrices. It first checks if $T$ and $S$ are invertible, then applies the transformation.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    This function transforms a given matrix A using the operation T^{-1}AS, \n    where T and S are invertible matrices.\n\n    Args:\n    A (list[list[int|float]]): The input matrix to be transformed.\n    T (list[list[int|float]]): The first invertible matrix.\n    S (list[list[int|float]]): The second invertible matrix.\n\n    Returns:\n    list[list[int|float]]: The transformed matrix, or -1 if the transformation is not possible.\n    \"\"\"\n\n    # Convert input lists to NumPy arrays\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n\n    # Check if T and S are square matrices\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n\n    # Check if T and S are invertible by checking their determinants\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n\n    # Check if the dimensions of A, T, and S are compatible for the transformation\n    if A.shape[0] != T.shape[0] or A.shape[1] != S.shape[0]:\n        return -1\n\n    try:\n        # Calculate the inverse of T\n        T_inv = np.linalg.inv(T)\n        \n        # Perform the transformation T^{-1}AS\n        transformed_A = np.dot(np.dot(T_inv, A), S)\n        \n        # Round the result to the nearest 4th decimal\n        transformed_A = np.round(transformed_A, 4)\n        \n        # Return the transformed matrix as a list\n        return transformed_A.tolist()\n    \n    except np.linalg.LinAlgError:\n        # If any linear algebra operation fails, return -1\n        return -1\n\n# Example usage\nif __name__ == \"__main__\":\n    A = [[1, 2], [3, 4]]\n    T = [[5, 6], [7, 8]]\n    S = [[9, 10], [11, 12]]\n    \n    result = transform_matrix(A, T, S)\n    print(result)\n```\n\n#### Explanation\n\n1. The function `transform_matrix` takes three 2D lists `A`, `T`, and `S` as input, representing the matrices $A$, $T$, and $S$ respectively.\n2. It first converts these input lists to NumPy arrays for easier matrix operations.\n3. It checks if `T` and `S` are square matrices and if they are invertible by verifying that their determinants are non-zero. If either condition is not met, it returns `-1`.\n4. It then checks if the dimensions of `A`, `T`, and `S` are compatible for the transformation $T^{-1}AS$. If not, it returns `-1`.\n5. The function calculates $T^{-1}$ using `np.linalg.inv(T)` and performs the matrix multiplication $T^{-1}AS` using `np.dot()`.\n6. The result is rounded to the nearest 4th decimal using `np.round()`.\n7. Finally, the transformed matrix is returned as a list using the `tolist()` method.\n8. The example usage demonstrates how to call the `transform_matrix` function with sample matrices `A`, `T`, and `S`."}
{"task_id": 8, "completion_id": 0, "completion": "## Calculating the Inverse of a 2x2 Matrix\n### Task Description\nThe task is to write a Python function that calculates the inverse of a 2x2 matrix. If the matrix is not invertible, the function should return `None`.\n\n### Code Solution\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    list[list[float]]: The inverse of the input matrix, or None if the matrix is not invertible.\n    \"\"\"\n\n    # Check if the input matrix is a valid 2x2 matrix\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError(\"Input must be a 2x2 matrix\")\n\n    # Extract the elements of the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n\n    # Calculate the determinant of the matrix\n    determinant = a * d - b * c\n\n    # Check if the matrix is invertible (i.e., determinant is non-zero)\n    if determinant == 0:\n        return None\n\n    # Calculate the inverse of the matrix\n    inverse = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n\n    return inverse\n\n\n# Example usage:\ndef main():\n    # Define a 2x2 matrix\n    matrix = [[1, 2], [3, 4]]\n\n    # Calculate the inverse of the matrix\n    inverse = inverse_2x2(matrix)\n\n    if inverse is not None:\n        print(\"Inverse of the matrix:\")\n        for row in inverse:\n            print(row)\n    else:\n        print(\"The matrix is not invertible.\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Explanation\n\n1.  The function `inverse_2x2` takes a 2x2 matrix as input and returns its inverse.\n2.  It first checks if the input is a valid 2x2 matrix and raises a `ValueError` if not.\n3.  The determinant of the matrix is calculated using the formula `ad - bc`, where `a`, `b`, `c`, and `d` are the elements of the matrix.\n4.  If the determinant is zero, the matrix is not invertible, and the function returns `None`.\n5.  Otherwise, the function calculates the inverse of the matrix using the formula for the inverse of a 2x2 matrix.\n6.  The `main` function demonstrates how to use `inverse_2x2` with an example matrix.\n\n### Advice\n\n*   This code follows best practices for Python coding, including type hints, docstrings, and clear variable names.\n*   The function is designed to be reusable and can be easily integrated into larger projects.\n*   Error handling is implemented to ensure the function behaves correctly with invalid inputs."}
{"task_id": 9, "completion_id": 0, "completion": "## Matrix Multiplication Function\n### Overview\n\nThe following Python function performs matrix multiplication on two input matrices `a` and `b`. If the matrices are incompatible for multiplication (i.e., the number of columns in `a` is not equal to the number of rows in `b`), the function returns `-1`.\n\n### Code\n\n```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices.\n\n    Args:\n    a (list[list[int | float]]): The first matrix.\n    b (list[list[int | float]]): The second matrix.\n\n    Returns:\n    list[list[int | float]]: The product of the two matrices. Returns -1 if the matrices are incompatible for multiplication.\n    \"\"\"\n\n    # Check if matrices are empty\n    if not a or not b:\n        return -1\n\n    # Get the dimensions of the matrices\n    num_rows_a = len(a)\n    num_cols_a = len(a[0])\n    num_rows_b = len(b)\n    num_cols_b = len(b[0])\n\n    # Check if the matrices can be multiplied\n    if num_cols_a != num_rows_b:\n        return -1\n\n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(num_cols_b)] for _ in range(num_rows_a)]\n\n    # Perform matrix multiplication\n    for i in range(num_rows_a):\n        for j in range(num_cols_b):\n            for k in range(num_cols_a):  # or 'num_rows_b'\n                result[i][j] += a[i][k] * b[k][j]\n\n    return result\n\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix_a = [[1, 2, 3], [4, 5, 6]]\n    matrix_b = [[7, 8], [9, 10], [11, 12]]\n    print(matrixmul(matrix_a, matrix_b))\n\n    # Test case for incompatible matrices\n    matrix_c = [[1, 2], [3, 4]]\n    matrix_d = [[5, 6, 7], [8, 9, 10], [11, 12, 13]]\n    print(matrixmul(matrix_c, matrix_d))  # Should return -1\n```\n\n### Explanation\n\n1.  The function first checks if either of the input matrices is empty. If so, it returns `-1`.\n2.  It then retrieves the dimensions of both matrices.\n3.  The function checks if the number of columns in the first matrix (`num_cols_a`) is equal to the number of rows in the second matrix (`num_rows_b`). If not, it returns `-1` because the matrices are incompatible for multiplication.\n4.  It initializes a result matrix filled with zeros, with dimensions based on the number of rows in `a` and the number of columns in `b`.\n5.  The function performs the actual matrix multiplication using nested loops, iterating over the rows of `a`, the columns of `b`, and the elements within the row of `a` (or column of `b`).\n6.  Finally, it returns the resulting matrix.\n\n### Example Usage\n\nThe provided example demonstrates the function's usage with two compatible matrices (`matrix_a` and `matrix_b`) and two incompatible matrices (`matrix_c` and `matrix_d`)."}
{"task_id": 10, "completion_id": 0, "completion": "## Covariance Matrix Calculation in Python\n### Overview\n\nThe following Python function calculates the covariance matrix for a given set of vectors. It takes a list of lists as input, where each inner list represents a feature with its observations, and returns the covariance matrix as a list of lists.\n\n### Implementation\n\n```python\nimport numpy as np\nfrom typing import List\n\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n    vectors (List[List[float]]): A list of lists, where each inner list represents a feature with its observations.\n\n    Returns:\n    List[List[float]]: The covariance matrix as a list of lists.\n    \"\"\"\n    \n    # Convert the input list of lists to a NumPy array for efficient computation\n    vector_array = np.array(vectors)\n    \n    # Calculate the covariance matrix using NumPy's cov function\n    # The rowvar parameter is set to False to indicate that each column represents a variable\n    covariance_matrix = np.cov(vector_array, rowvar=False)\n    \n    # Convert the covariance matrix back to a list of lists and return it\n    return covariance_matrix.tolist()\n\n# Test cases\nif __name__ == \"__main__\":\n    # Test case 1: Two features with 3 observations each\n    vectors1 = [[1, 2, 3], [4, 5, 6]]\n    print(\"Covariance Matrix for vectors1:\")\n    print(calculate_covariance_matrix(vectors1))\n\n    # Test case 2: Three features with 4 observations each\n    vectors2 = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n    print(\"\\nCovariance Matrix for vectors2:\")\n    print(calculate_covariance_matrix(vectors2))\n\n    # Test case 3: Single feature with 5 observations\n    vectors3 = [[1, 2, 3, 4, 5]]\n    print(\"\\nCovariance Matrix for vectors3:\")\n    print(calculate_covariance_matrix(vectors3))\n```\n\n### Explanation\n\n1. The function `calculate_covariance_matrix` takes a list of lists `vectors` as input, where each inner list represents a feature with its observations.\n2. It converts the input list of lists to a NumPy array `vector_array` for efficient computation.\n3. The covariance matrix is calculated using NumPy's `cov` function with `rowvar=False`, indicating that each column represents a variable.\n4. The resulting covariance matrix is converted back to a list of lists and returned.\n5. The test cases demonstrate the function's correctness with different input scenarios.\n\n### Advice\n\n*   Ensure that the input list of lists has a consistent number of observations across all features.\n*   For large datasets, consider using optimized libraries like NumPy or Pandas for efficient computation.\n*   You can modify the function to handle edge cases, such as empty input or features with a single observation."}
{"task_id": 11, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Understanding the Jacobi Method**: The Jacobi method is an iterative algorithm used to solve a system of linear equations. It works by iteratively improving an initial guess for the solution until it converges to the actual solution.\n2. **Given inputs and expected output**: The function `solve_jacobi` takes three inputs: a square matrix `A` representing the coefficients of the linear equations, a vector `b` representing the constants on the right-hand side of the equations, and an integer `n` representing the number of iterations to perform. The function is expected to return the approximate solution `x` as a list after `n` iterations, with each intermediate solution rounded to four decimal places.\n3. **Checking the inputs**: We need to ensure that `A` is a square matrix and `b` is a vector with the same number of rows as `A`. Also, `n` should be a positive integer.\n4. **Implementing the Jacobi Method**: To implement the Jacobi method, we first need to check if the matrix `A` is diagonally dominant. However, since the problem doesn't explicitly require checking for diagonal dominance, we will proceed with the assumption that `A` is suitable for the Jacobi method. We then initialize an initial guess for `x`, typically a vector of zeros. In each iteration, we update `x` using the formula derived from the Jacobi method.\n\n# Fixed solution:\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi method.\n\n    Args:\n    A (np.ndarray): A square matrix representing the coefficients of the linear equations.\n    b (np.ndarray): A vector representing the constants on the right-hand side of the equations.\n    n (int): The number of iterations to perform.\n\n    Returns:\n    list: The approximate solution x after n iterations.\n    \"\"\"\n\n    # Check if A is a square matrix\n    assert A.shape[0] == A.shape[1], \"Matrix A must be square\"\n    # Check if b is a vector with the same number of rows as A\n    assert b.shape[0] == A.shape[0], \"Vector b must have the same number of rows as matrix A\"\n    # Check if n is a positive integer\n    assert n > 0 and isinstance(n, int), \"n must be a positive integer\"\n\n    # Get the number of equations\n    num_equations = A.shape[0]\n\n    # Initialize the solution vector with zeros\n    x = np.zeros(num_equations)\n\n    # Perform n iterations of the Jacobi method\n    for _ in range(n):\n        # Initialize a new solution vector for this iteration\n        new_x = np.zeros(num_equations)\n        \n        # Update each element of the new solution vector\n        for i in range(num_equations):\n            # Calculate the sum of the terms involving other variables\n            sum_terms = np.sum([A[i, j] * x[j] for j in range(num_equations) if j != i])\n            # Update the new solution vector\n            new_x[i] = (b[i] - sum_terms) / A[i, i]\n        \n        # Update the solution vector and round to four decimal places\n        x = np.round(new_x, 4)\n\n    # Return the final solution as a list\n    return x.tolist()\n```\n\n# Explanation of changes:\n*   **Added input validation**: We added checks to ensure that `A` is a square matrix, `b` has the same number of rows as `A`, and `n` is a positive integer.\n*   **Implemented the Jacobi method**: We initialized a solution vector `x` with zeros and iteratively updated it using the Jacobi method formula.\n*   **Rounded intermediate solutions**: We rounded the solution vector `x` to four decimal places after each iteration.\n*   **Returned the final solution as a list**: We returned the final solution vector `x` as a list after `n` iterations.\n\n# Tests and example uses:\n```python\n# Example usage:\nA = np.array([[5, 1, 2], [1, 4, 1], [2, 1, 5]])\nb = np.array([8, 6, 9])\nn = 3\n\nresult = solve_jacobi(A, b, n)\nprint(result)\n```"}
{"task_id": 12, "completion_id": 0, "completion": "## Singular Value Decomposition (SVD) using Jacobian Method\n### Task Description\nThe task is to implement a Python function that approximates the Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method without utilizing the NumPy SVD function.\n\n### Code Implementation\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the left singular vectors (U), singular values, and right singular vectors (V).\n    \"\"\"\n\n    # Ensure the input matrix is a 2x2 NumPy array\n    assert isinstance(A, np.ndarray) and A.shape == (2, 2), \"Input must be a 2x2 NumPy array\"\n\n    # Compute A^T A\n    A_T_A = np.dot(A.T, A)\n\n    # Initialize V as an identity matrix\n    V = np.eye(2)\n\n    # Perform Jacobi iterations to diagonalize A^T A\n    for _ in range(10):  # Perform a fixed number of iterations for simplicity\n        # Find the off-diagonal element with the largest absolute value\n        if abs(A_T_A[0, 1]) > abs(A_T_A[1, 0]):\n            p, q = 0, 1\n        else:\n            p, q = 1, 0\n\n        # Compute the Jacobi rotation angle\n        if A_T_A[p, p] == A_T_A[q, q]:\n            theta = np.pi / 4\n        else:\n            theta = 0.5 * np.arctan2(2 * A_T_A[p, q], A_T_A[p, p] - A_T_A[q, q])\n\n        # Compute the Jacobi rotation matrix\n        c = np.cos(theta)\n        s = np.sin(theta)\n        J = np.array([[c, -s], [s, c]])\n\n        # Update A^T A and V\n        A_T_A = np.dot(J.T, np.dot(A_T_A, J))\n        V = np.dot(V, J)\n\n    # Extract the singular values from the diagonal of A^T A\n    singular_values = np.sqrt(np.diag(A_T_A))\n\n    # Compute U using the formula U = A V \u03a3^(-1)\n    U = np.dot(A, np.dot(V, np.diag(1 / singular_values)))\n\n    # Ensure the determinants of U and V are positive\n    if np.linalg.det(U) < 0:\n        U[:, 0] *= -1\n        singular_values[0] *= -1\n    if np.linalg.det(V) < 0:\n        V[:, 0] *= -1\n\n    # Round the results to the nearest 4th decimal\n    U = np.round(U, 4)\n    singular_values = np.round(singular_values, 4)\n    V = np.round(V, 4)\n\n    return U, singular_values, V\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([[1, 2], [3, 4]])\n    U, singular_values, V = svd_2x2_singular_values(A)\n    print(\"Left Singular Vectors (U):\")\n    print(U)\n    print(\"Singular Values:\")\n    print(singular_values)\n    print(\"Right Singular Vectors (V):\")\n    print(V)\n```\n\n### Explanation\n\n1.  The function `svd_2x2_singular_values` takes a 2x2 NumPy array `A` as input and returns a tuple containing the left singular vectors (`U`), singular values, and right singular vectors (`V`).\n2.  It first computes `A^T A` and initializes `V` as an identity matrix.\n3.  The function then performs a fixed number of Jacobi iterations to diagonalize `A^T A`, updating `V` accordingly.\n4.  After diagonalization, it extracts the singular values from the diagonal of `A^T A` and computes `U` using the formula `U = A V \u03a3^(-1)`.\n5.  The determinants of `U` and `V` are checked to ensure they are positive. If not, the corresponding columns are negated.\n6.  Finally, the results are rounded to the nearest 4th decimal and returned.\n\n### Example Usage\n\nThe example usage demonstrates how to call the `svd_2x2_singular_values` function with a sample 2x2 matrix `A`. The resulting `U`, singular values, and `V` are printed to the console."}
{"task_id": 13, "completion_id": 0, "completion": "## Determinant of a 4x4 Matrix using Laplace's Expansion\n\n### Code\n\n```python\ndef determinant_3x3(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 3x3 matrix.\n\n    Args:\n    matrix: A 3x3 matrix represented as a list of lists.\n\n    Returns:\n    The determinant of the matrix.\n    \"\"\"\n    a, b, c = matrix[0][0], matrix[0][1], matrix[0][2]\n    d, e, f = matrix[1][0], matrix[1][1], matrix[1][2]\n    g, h, i = matrix[2][0], matrix[2][1], matrix[2][2]\n\n    return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n\n\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n\n    Args:\n    matrix: A 4x4 matrix represented as a list of lists.\n\n    Returns:\n    The determinant of the matrix.\n    \"\"\"\n    # Check if the input matrix is a 4x4 matrix\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"Input matrix must be a 4x4 matrix\")\n\n    det = 0.0\n    for i in range(4):\n        # Create a 3x3 minor matrix by removing the current row and column\n        minor = [row[:i] + row[i+1:] for row in matrix[1:]]\n        # Recursively calculate the determinant of the minor matrix\n        det += ((-1) ** i) * matrix[0][i] * determinant_3x3(minor)\n\n    return det\n\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12],\n        [13, 14, 15, 16]\n    ]\n    print(determinant_4x4(matrix))\n```\n\n### Explanation\n\nThe provided Python code defines two functions: `determinant_3x3` and `determinant_4x4`. The `determinant_3x3` function calculates the determinant of a 3x3 matrix using the standard formula. The `determinant_4x4` function calculates the determinant of a 4x4 matrix using Laplace's Expansion method.\n\n1.  The `determinant_4x4` function first checks if the input matrix is a valid 4x4 matrix. If not, it raises a `ValueError`.\n2.  It then initializes a variable `det` to store the determinant.\n3.  The function iterates over the elements of the first row of the matrix. For each element, it creates a 3x3 minor matrix by removing the current column and the first row from the original matrix.\n4.  The determinant of the minor matrix is calculated using the `determinant_3x3` function.\n5.  The `det` is updated by adding the product of the current element, the determinant of the minor matrix, and a sign factor (`(-1) ** i`) to it.\n6.  Finally, the function returns the calculated determinant.\n\nThe example usage demonstrates how to use the `determinant_4x4` function to calculate the determinant of a sample 4x4 matrix.\n\n### Advice\n\n*   This implementation assumes that the input matrix is a list of lists where each inner list represents a row of the matrix.\n*   The code uses type hints to indicate that the matrix elements can be either integers or floating-point numbers.\n*   The `determinant_3x3` function is used to calculate the determinant of the 3x3 minor matrices that arise during Laplace's Expansion.\n*   You can modify the example usage to test the `determinant_4x4` function with different input matrices."}
{"task_id": 14, "completion_id": 0, "completion": "## Linear Regression Using Normal Equation\n### Task Description\nThe task is to implement a Python function that performs linear regression using the normal equation. The function takes a matrix `X` (features) and a vector `y` (target) as input and returns the coefficients of the linear regression model.\n\n### Code Implementation\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    This function performs linear regression using the normal equation.\n\n    Args:\n    X (list[list[float]]): A 2D list representing the feature matrix.\n    y (list[float]): A list representing the target vector.\n\n    Returns:\n    list[float]: A list of coefficients of the linear regression model.\n    \"\"\"\n\n    # Convert input lists to NumPy arrays for efficient numerical computations\n    X = np.array(X)\n    y = np.array(y)\n\n    # Add a column of ones to the feature matrix X to account for the bias term\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n\n    # Calculate the coefficients using the normal equation: (X^T * X)^-1 * X^T * y\n    # np.linalg.inv is used to compute the inverse of a matrix\n    # np.dot is used for matrix multiplication\n    coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n\n    # Round the coefficients to four decimal places\n    coefficients = np.round(coefficients, 4)\n\n    # Return the coefficients as a list\n    return coefficients.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    X = [[1], [2], [3], [4]]\n    y = [2, 3, 5, 7]\n    print(linear_regression_normal_equation(X, y))\n```\n\n### Explanation\n\n1. The function `linear_regression_normal_equation` takes two inputs: `X` (a 2D list of features) and `y` (a list of target values).\n2. It first converts these inputs into NumPy arrays for efficient numerical computations.\n3. A column of ones is added to `X` to account for the bias term in the linear regression model.\n4. The coefficients are calculated using the normal equation, which is `(X^T * X)^-1 * X^T * y`. This involves matrix multiplication and inversion, which are efficiently performed using NumPy's `dot` and `linalg.inv` functions, respectively.\n5. The calculated coefficients are then rounded to four decimal places using `np.round`.\n6. Finally, the coefficients are returned as a list.\n\n### Advice\n\n* Ensure that the input matrix `X` has a sufficient number of rows (observations) and that the matrix `X^T * X` is invertible. If `X^T * X` is singular (not invertible), consider using regularization techniques or alternative methods for linear regression.\n* The normal equation is suitable for small to moderate-sized datasets. For very large datasets, consider using iterative methods or stochastic gradient descent for linear regression."}
{"task_id": 15, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Understand the task**: The task is to implement a Python function that performs linear regression using gradient descent. The function should take NumPy arrays `X` (features with a column of ones for the intercept) and `y` (target) as input, along with learning rate `alpha` and the number of iterations.\n2. **Identify the inputs and outputs**: The inputs are `X`, `y`, `alpha`, and `iterations`. The output is the coefficients of the linear regression model.\n3. **Determine the steps for gradient descent**: \n    - Initialize the coefficients.\n    - For a specified number of iterations, update the coefficients using the gradient descent update rule.\n    - The update rule involves calculating the gradient of the cost function with respect to the coefficients.\n4. **Recall the gradient descent update rule for linear regression**: The update rule is given by `coefficients = coefficients - alpha * (1/m) * X^T * (X * coefficients - y)`, where `m` is the number of samples.\n\n# Fixed solution:\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    This function performs linear regression using gradient descent.\n\n    Parameters:\n    X (np.ndarray): Features with a column of ones for the intercept.\n    y (np.ndarray): Target variable.\n    alpha (float): Learning rate.\n    iterations (int): Number of iterations.\n\n    Returns:\n    list: Coefficients of the linear regression model.\n    \"\"\"\n    \n    # Get the number of samples\n    m = X.shape[0]\n    \n    # Initialize the coefficients with zeros\n    coefficients = np.zeros(X.shape[1])\n    \n    # Perform gradient descent for the specified number of iterations\n    for _ in range(iterations):\n        # Calculate the gradient\n        gradient = (1/m) * np.dot(X.T, np.dot(X, coefficients) - y)\n        \n        # Update the coefficients\n        coefficients = coefficients - alpha * gradient\n    \n    # Round the coefficients to four decimal places and return as a list\n    return np.round(coefficients, 4).tolist()\n```\n\n# Explanation of changes:\n*   **Initialized coefficients**: We started by initializing the coefficients with zeros using `np.zeros(X.shape[1])`.\n*   **Implemented gradient descent**: We performed gradient descent for the specified number of iterations, updating the coefficients in each iteration using the gradient descent update rule.\n*   **Calculated the gradient**: Inside the loop, we calculated the gradient using `(1/m) * np.dot(X.T, np.dot(X, coefficients) - y)`.\n*   **Updated the coefficients**: We updated the coefficients using `coefficients = coefficients - alpha * gradient`.\n*   **Rounded the coefficients**: Finally, we rounded the coefficients to four decimal places using `np.round(coefficients, 4)` and returned them as a list.\n\n# Tests and example uses:\n```python\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    X = np.random.rand(100, 2)  # 100 samples with 1 feature and a column of ones for the intercept\n    X[:, 0] = 1  # First column is ones for the intercept\n    y = 3 + 2 * X[:, 1] + np.random.randn(100)  # True relationship is y = 3 + 2*x + noise\n    \n    # Perform linear regression using gradient descent\n    alpha = 0.01\n    iterations = 1000\n    coefficients = linear_regression_gradient_descent(X, y, alpha, iterations)\n    print(\"Coefficients:\", coefficients)\n```\nThis example demonstrates how to use the `linear_regression_gradient_descent` function to perform linear regression on some generated data. The output will be the estimated coefficients of the linear regression model."}
{"task_id": 16, "completion_id": 0, "completion": "# Problem Understanding\nThe task is to implement a Python function that performs feature scaling on a given dataset using both standardization and min-max normalization. The input dataset is a 2D NumPy array where each row represents a data sample and each column represents a feature. The function should return two 2D lists: one scaled by standardization and one by min-max normalization.\n\nKey constraints and edge cases:\n- The input is a 2D NumPy array.\n- Each row represents a data sample, and each column represents a feature.\n- The output should be two 2D lists.\n- The results should be rounded to the nearest 4th decimal.\n- The function should handle the case where the standard deviation of a feature is zero.\n- The function should handle the case where the maximum and minimum values of a feature are the same.\n\nInput: A 2D NumPy array.\nOutput: Two 2D lists representing the scaled dataset using standardization and min-max normalization.\n\nSubtle requirements or implicit constraints:\n- The function should not modify the original dataset.\n- The output should be in the form of Python lists, not NumPy arrays.\n\n# Key Insights\n- Standardization involves subtracting the mean and then dividing by the standard deviation for each feature.\n- Min-max normalization involves subtracting the minimum value and then dividing by the range (max - min) for each feature.\n- The formula for standardization is \\(x' = \\frac{x - \\mu}{\\sigma}\\), where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of the feature.\n- The formula for min-max normalization is \\(x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\\), where \\(x_{min}\\) and \\(x_{max}\\) are the minimum and maximum values of the feature.\n- To avoid division by zero in standardization, if the standard deviation is zero, the feature should be set to zero.\n- To avoid division by zero in min-max normalization, if the range is zero, the feature should be set to a constant value (e.g., 0.5).\n- The results should be rounded to the nearest 4th decimal to meet the output constraints.\n- Using NumPy's vectorized operations can efficiently compute the mean, standard deviation, minimum, and maximum for each feature.\n- The `tolist()` method can be used to convert the scaled NumPy arrays to Python lists.\n\n# Solution Strategy\n1. Compute the mean and standard deviation for each feature (column) in the input dataset for standardization.\n2. Apply standardization by subtracting the mean and dividing by the standard deviation for each feature. Handle the case where the standard deviation is zero.\n3. Compute the minimum and maximum values for each feature in the input dataset for min-max normalization.\n4. Apply min-max normalization by subtracting the minimum value and dividing by the range for each feature. Handle the case where the range is zero.\n5. Round the results of both scaling methods to the nearest 4th decimal.\n6. Convert the scaled datasets from NumPy arrays to Python lists using the `tolist()` method.\n7. Return the two scaled datasets as 2D Python lists.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    This function performs feature scaling on a given dataset using both standardization and min-max normalization.\n\n    Args:\n    data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n\n    Returns:\n    tuple[list[list[float]], list[list[float]]]: Two 2D lists, the first one scaled by standardization and the second by min-max normalization.\n    \"\"\"\n\n    # Calculate mean and standard deviation for standardization\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n\n    # Standardization: (data - mean) / std_dev\n    # Handle division by zero by setting the result to zero where std_dev is zero\n    standardized_data = (data - mean) / np.where(std_dev == 0, 1, std_dev)\n    standardized_data = np.where(std_dev == 0, 0, standardized_data)\n    standardized_data = np.round(standardized_data, 4).tolist()\n\n    # Calculate min and max for min-max normalization\n    data_min = np.min(data, axis=0)\n    data_max = np.max(data, axis=0)\n\n    # Min-max normalization: (data - min) / (max - min)\n    # Handle division by zero by setting the result to 0.5 where (max - min) is zero\n    min_max_normalized_data = (data - data_min) / np.where((data_max - data_min) == 0, 1, (data_max - data_min))\n    min_max_normalized_data = np.where((data_max - data_min) == 0, 0.5, min_max_normalized_data)\n    min_max_normalized_data = np.round(min_max_normalized_data, 4).tolist()\n\n    return standardized_data, min_max_normalized_data\n```"}
{"task_id": 17, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Understanding the task**: The task is to implement the k-Means clustering algorithm in Python. This algorithm groups similar points together into clusters based on their coordinates.\n2. **Identifying the inputs and outputs**: The function takes in a list of points, the number of clusters (k), initial centroids, and the maximum number of iterations. It outputs a list of the final centroids of the clusters.\n3. **Breaking down the k-Means algorithm**: The k-Means algorithm involves the following steps:\n   - Initialize centroids.\n   - Assign each point to the nearest centroid.\n   - Update centroids based on the mean of points assigned to them.\n   - Repeat the assignment and update steps until convergence or a maximum number of iterations is reached.\n4. **Translating the algorithm into Python**: We will use NumPy for efficient numerical computations.\n\n# Fixed solution:\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    This function performs k-Means clustering on a given set of points.\n\n    Args:\n    - points: A list of points, where each point is a tuple of coordinates.\n    - k: An integer representing the number of clusters to form.\n    - initial_centroids: A list of initial centroid points, each a tuple of coordinates.\n    - max_iterations: An integer representing the maximum number of iterations to perform.\n\n    Returns:\n    - A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n    \"\"\"\n\n    # Convert the list of points and initial centroids to NumPy arrays for efficient computation\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n\n    # Perform k-Means clustering\n    for _ in range(max_iterations):\n        # Calculate the distance between each point and each centroid\n        distances = np.linalg.norm(points_array[:, np.newaxis] - centroids, axis=2)\n\n        # Assign each point to the nearest centroid\n        labels = np.argmin(distances, axis=1)\n\n        # Update centroids based on the mean of points assigned to them\n        new_centroids = np.array([points_array[labels == i].mean(axis=0) if np.any(labels == i) else centroids[i] for i in range(k)])\n\n        # Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n\n        centroids = new_centroids\n\n    # Round the final centroids to the nearest fourth decimal and return as a list of tuples\n    return [tuple(np.round(centroid, 4)) for centroid in centroids]\n\n# Example usage\nif __name__ == \"__main__\":\n    points = [(1.0, 1.0), (1.0, 2.0), (2.0, 1.0), (2.0, 2.0), (10.0, 10.0), (10.0, 11.0), (11.0, 10.0), (11.0, 11.0)]\n    k = 2\n    initial_centroids = [(0.0, 0.0), (5.0, 5.0)]\n    max_iterations = 100\n\n    final_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\n    print(\"Final Centroids:\", final_centroids)\n```\n\n# Explanation of changes:\n*   **Converted input lists to NumPy arrays**: For efficient numerical computations, we converted the list of points and initial centroids to NumPy arrays.\n*   **Implemented the k-Means algorithm**: We performed the k-Means clustering algorithm by iteratively assigning points to the nearest centroid and updating the centroids based on the mean of assigned points.\n*   **Added convergence check**: The algorithm checks for convergence by comparing the current centroids with the new centroids. If they are the same, the algorithm stops.\n*   **Rounded final centroids**: The final centroids are rounded to the nearest fourth decimal before being returned as a list of tuples.\n\n# Tests and example uses:\n*   The example usage demonstrates how to call the `k_means_clustering` function with a set of points, k, initial centroids, and maximum iterations. It prints the final centroids after performing the k-Means clustering."}
{"task_id": 18, "completion_id": 0, "completion": "## K-Fold Cross-Validation Implementation\n### Overview\n\nK-Fold Cross-Validation is a technique used to evaluate the performance of a machine learning model by dividing the dataset into k subsets or folds. The model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with each fold serving as the test set once.\n\n### Implementation\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generates train and test splits for K-Fold Cross-Validation.\n\n    Args:\n    - X (np.ndarray): Feature dataset.\n    - y (np.ndarray): Target variable.\n    - k (int, optional): Number of folds. Defaults to 5.\n    - shuffle (bool, optional): Whether to shuffle the data before splitting. Defaults to True.\n    - random_seed (int, optional): Seed for the random number generator. Defaults to None.\n\n    Returns:\n    - list: A list of tuples containing train-test indices for each fold.\n    \"\"\"\n\n    # Check if the number of samples in X and y are the same\n    assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples\"\n\n    # Get the total number of samples\n    n_samples = X.shape[0]\n\n    # Create an array of indices\n    indices = np.arange(n_samples)\n\n    # Shuffle the indices if required\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n\n    # Calculate the size of each fold\n    fold_size = n_samples // k\n\n    # Initialize a list to store the train-test splits\n    splits = []\n\n    # Generate train-test splits for each fold\n    for i in range(k):\n        # Calculate the start and end indices for the test fold\n        test_start = i * fold_size\n        test_end = (i + 1) * fold_size\n\n        # If it's the last fold, include any remaining samples\n        if i == k - 1:\n            test_end = n_samples\n\n        # Get the test indices\n        test_indices = indices[test_start:test_end]\n\n        # Get the train indices\n        train_indices = np.concatenate((indices[:test_start], indices[test_end:]))\n\n        # Append the train-test split to the list\n        splits.append((train_indices, test_indices))\n\n    return splits\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some sample data\n    X = np.random.rand(100, 10)\n    y = np.random.rand(100)\n\n    # Perform K-Fold Cross-Validation\n    k = 5\n    splits = k_fold_cross_validation(X, y, k=k, shuffle=True, random_seed=42)\n\n    # Print the train-test splits\n    for i, (train_indices, test_indices) in enumerate(splits):\n        print(f\"Fold {i+1}:\")\n        print(f\"Train indices: {train_indices.shape[0]}\")\n        print(f\"Test indices: {test_indices.shape[0]}\")\n        print()\n```\n\n### Explanation\n\n1.  The function `k_fold_cross_validation` takes in the feature dataset `X`, target variable `y`, number of folds `k`, a flag to shuffle the data, and an optional random seed.\n2.  It first checks if the number of samples in `X` and `y` are the same.\n3.  The function then creates an array of indices and shuffles them if required.\n4.  It calculates the size of each fold and initializes a list to store the train-test splits.\n5.  The function generates train-test splits for each fold by calculating the start and end indices for the test fold and getting the corresponding test indices.\n6.  The train indices are obtained by concatenating the indices before and after the test fold.\n7.  The train-test splits are appended to the list, which is returned at the end.\n\n### Example Usage\n\nThe example usage demonstrates how to perform K-Fold Cross-Validation on some sample data. It generates random feature and target datasets, performs K-Fold Cross-Validation with `k=5`, and prints the number of train and test indices for each fold."}
{"task_id": 19, "completion_id": 0, "completion": "## Step 1: Standardize the dataset\nTo perform PCA, the first step is to standardize the dataset. This involves subtracting the mean of each feature from the data points and then dividing by the standard deviation for each feature. This step is necessary to ensure that all features are on the same scale, which is crucial for PCA.\n\n## Step 2: Compute the covariance matrix\nAfter standardizing the dataset, the next step is to compute the covariance matrix. The covariance matrix represents the covariance between different features in the dataset. It is calculated as the dot product of the standardized data matrix and its transpose, divided by the number of data points minus one.\n\n## Step 3: Find the eigenvalues and eigenvectors of the covariance matrix\nThe eigenvalues and eigenvectors of the covariance matrix are then computed. The eigenvectors represent the directions of the new features (principal components), and the eigenvalues represent the amount of variance explained by each principal component.\n\n## Step 4: Sort the eigenvalues and eigenvectors\nThe eigenvalues and their corresponding eigenvectors are sorted in descending order based on the eigenvalues. This is because the principal components are the eigenvectors corresponding to the largest eigenvalues.\n\n## Step 5: Select the top k eigenvectors\nThe top k eigenvectors are selected based on the sorted eigenvalues. These eigenvectors represent the principal components that capture the most variance in the dataset.\n\n## Step 6: Implement the PCA function in Python\nNow, let's implement these steps in Python:\n\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the given dataset.\n\n    Args:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - k (int): The number of principal components to return.\n\n    Returns:\n    - list[list[float]]: The principal components (eigenvectors corresponding to the largest eigenvalues).\n    \"\"\"\n\n    # Step 1: Standardize the dataset\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n\n    # Step 2: Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data.T)\n\n    # Step 3: Find the eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n\n    # Step 4: Sort the eigenvalues and eigenvectors\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Step 5: Select the top k eigenvectors\n    principal_components = sorted_eigenvectors[:, :k].T\n\n    # Round the results to the nearest 4th decimal\n    principal_components = np.round(principal_components, 4).tolist()\n\n    return principal_components\n```\n\n## Example Usage\nHere's an example of how to use the `pca` function:\n\n```python\n# Generate a random dataset\nnp.random.seed(0)\ndata = np.random.rand(100, 5)\n\n# Perform PCA\nk = 2\nprincipal_components = pca(data, k)\nprint(principal_components)\n```\n\nThis code defines a function `pca` that takes a 2D NumPy array `data` and an integer `k` as input and returns the top `k` principal components. The function standardizes the dataset, computes the covariance matrix, finds the eigenvalues and eigenvectors, sorts them, and returns the top `k` eigenvectors. The results are rounded to the nearest 4th decimal."}
{"task_id": 20, "completion_id": 0, "completion": "### Decision Tree Learning Implementation\n\nHere's a Python implementation of the decision tree learning algorithm using recursive binary splitting based on entropy and information gain.\n\n```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree using recursive binary splitting based on entropy and information gain.\n\n    Args:\n    - examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    - attributes (list[str]): A list of attribute names.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - dict: A nested dictionary representing the decision tree.\n    \"\"\"\n\n    # Base case: If all examples have the same target attribute value, return that value\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return target_values[0]\n\n    # Base case: If there are no more attributes to split on, return the most common target attribute value\n    if not attributes:\n        return Counter(target_values).most_common(1)[0][0]\n\n    # Select the best attribute to split on based on information gain\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n\n    # Create a new decision tree node with the best attribute\n    tree = {best_attr: {}}\n\n    # Split the examples based on the best attribute and recursively build subtrees\n    attr_values = set(example[best_attr] for example in examples)\n    for value in attr_values:\n        subtree_examples = [example for example in examples if example[best_attr] == value]\n        subtree_attributes = [attr for attr in attributes if attr != best_attr]\n        tree[best_attr][value] = learn_decision_tree(subtree_examples, subtree_attributes, target_attr)\n\n    return tree\n\n\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Calculates the entropy of a list of examples with respect to a target attribute.\n\n    Args:\n    - examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - float: The entropy of the examples with respect to the target attribute.\n    \"\"\"\n\n    target_values = [example[target_attr] for example in examples]\n    value_counts = Counter(target_values)\n    total = len(target_values)\n\n    return -sum((count / total) * math.log2(count / total) for count in value_counts.values())\n\n\ndef information_gain(examples: list[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    Calculates the information gain of splitting a list of examples on an attribute.\n\n    Args:\n    - examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    - attr (str): The name of the attribute to split on.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - float: The information gain of splitting the examples on the attribute.\n    \"\"\"\n\n    total_entropy = entropy(examples, target_attr)\n    attr_values = set(example[attr] for example in examples)\n    weighted_entropy = sum(\n        (len([example for example in examples if example[attr] == value]) / len(examples))\n        * entropy([example for example in examples if example[attr] == value], target_attr)\n        for value in attr_values\n    )\n\n    return total_entropy - weighted_entropy\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    examples = [\n        {\"Outlook\": \"Sunny\", \"Temperature\": \"Hot\", \"Humidity\": \"High\", \"Wind\": \"Weak\", \"Play\": \"No\"},\n        {\"Outlook\": \"Sunny\", \"Temperature\": \"Hot\", \"Humidity\": \"High\", \"Wind\": \"Strong\", \"Play\": \"No\"},\n        {\"Outlook\": \"Overcast\", \"Temperature\": \"Hot\", \"Humidity\": \"High\", \"Wind\": \"Weak\", \"Play\": \"Yes\"},\n        {\"Outlook\": \"Rain\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Wind\": \"Weak\", \"Play\": \"Yes\"},\n        {\"Outlook\": \"Rain\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Wind\": \"Weak\", \"Play\": \"Yes\"},\n        {\"Outlook\": \"Rain\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Wind\": \"Strong\", \"Play\": \"No\"},\n        {\"Outlook\": \"Overcast\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Wind\": \"Strong\", \"Play\": \"Yes\"},\n        {\"Outlook\": \"Sunny\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Wind\": \"Weak\", \"Play\": \"No\"},\n        {\"Outlook\": \"Sunny\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Wind\": \"Weak\", \"Play\": \"Yes\"},\n        {\"Outlook\": \"Rain\", \"Temperature\": \"Mild\", \"Humidity\": \"Normal\", \"Wind\": \"Weak\", \"Play\": \"Yes\"},\n        {\"Outlook\": \"Sunny\", \"Temperature\": \"Mild\", \"Humidity\": \"Normal\", \"Wind\": \"Strong\", \"Play\": \"Yes\"},\n        {\"Outlook\": \"Overcast\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Wind\": \"Strong\", \"Play\": \"Yes\"},\n        {\"Outlook\": \"Overcast\", \"Temperature\": \"Hot\", \"Humidity\": \"Normal\", \"Wind\": \"Weak\", \"Play\": \"Yes\"},\n        {\"Outlook\": \"Rain\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Wind\": \"Strong\", \"Play\": \"No\"},\n    ]\n\n    attributes = [\"Outlook\", \"Temperature\", \"Humidity\", \"Wind\"]\n    target_attr = \"Play\"\n\n    decision_tree = learn_decision_tree(examples, attributes, target_attr)\n    print(\"Decision Tree:\", decision_tree)\n```\n\n### Code Explanation\n\nThe provided code implements a decision tree learning algorithm using recursive binary splitting based on entropy and information gain.\n\n1.  The `learn_decision_tree` function takes a list of examples, a list of attribute names, and a target attribute name as input. It returns a nested dictionary representing the decision tree.\n2.  The function first checks for two base cases:\n\n    *   If all examples have the same target attribute value, it returns that value.\n    *   If there are no more attributes to split on, it returns the most common target attribute value.\n3.  It then selects the best attribute to split on based on information gain using the `information_gain` function.\n4.  The function creates a new decision tree node with the best attribute and recursively builds subtrees for each possible value of the best attribute.\n5.  The `entropy` function calculates the entropy of a list of examples with respect to a target attribute.\n6.  The `information_gain` function calculates the information gain of splitting a list of examples on an attribute.\n\nThe example usage demonstrates how to use the `learn_decision_tree` function to build a decision tree from a given dataset.\n\n### Time Complexity\n\nThe time complexity of the decision tree learning algorithm is O(n \\* m \\* d), where:\n\n*   n is the number of examples.\n*   m is the number of attributes.\n*   d is the maximum depth of the decision tree.\n\nIn the worst-case scenario, the decision tree can be skewed, resulting in a time complexity of O(n \\* m \\* n). However, with proper attribute selection and pruning, the average-case time complexity is typically much better.\n\n### Space Complexity\n\nThe space complexity of the decision tree learning algorithm is O(m \\* d), as it needs to store the decision tree nodes. In the worst-case scenario, the space complexity can be O(n), but with proper pruning, it is typically much less."}
{"task_id": 21, "completion_id": 0, "completion": "Here's a Python function that implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier from scratch.\n\n```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    This function implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier.\n\n    Args:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - labels (np.ndarray): A 1D NumPy array where each entry corresponds to the label of the sample.\n    - kernel (str, optional): The choice of kernel. Defaults to 'linear'. Options: 'linear', 'rbf'.\n    - lambda_val (float, optional): The regularization parameter. Defaults to 0.01.\n    - iterations (int, optional): The number of iterations. Defaults to 100.\n    - sigma (float, optional): The sigma value for the RBF kernel. Defaults to 1.0.\n\n    Returns:\n    - alpha (list): The model's alpha coefficients.\n    - bias (float): The model's bias.\n    \"\"\"\n\n    # Check if the input data and labels have compatible shapes\n    assert data.shape[0] == labels.shape[0], \"Data and labels must have the same number of samples\"\n\n    # Map labels to {-1, 1} if they are not already\n    if np.any(labels != -1) and np.any(labels != 1):\n        labels = np.where(labels > 0, 1, -1)\n\n    num_samples = data.shape[0]\n    alpha = np.zeros(num_samples)\n\n    for t in range(1, iterations + 1):\n        # Compute the learning rate\n        learning_rate = 1 / (t * lambda_val)\n\n        # Compute the predictions using the current alpha values\n        if kernel == 'linear':\n            # For linear kernel, compute the dot product directly\n            predictions = np.dot(data, data.T).dot(alpha)\n        elif kernel == 'rbf':\n            # For RBF kernel, compute the kernel matrix\n            pairwise_distances = np.linalg.norm(data[:, np.newaxis] - data, axis=2)\n            kernel_matrix = np.exp(-pairwise_distances ** 2 / (2 * sigma ** 2))\n            predictions = kernel_matrix.dot(alpha)\n        else:\n            raise ValueError(\"Invalid kernel. Options: 'linear', 'rbf'.\")\n\n        # Compute the hinge loss\n        hinge_loss = np.maximum(0, 1 - labels * (predictions + alpha.dot(np.zeros(num_samples)) - alpha.dot(alpha * labels)))\n\n        # Update alpha values\n        alpha = alpha - learning_rate * lambda_val * alpha\n        alpha += learning_rate * (labels * (hinge_loss > 0))\n\n    # Compute the bias term\n    if kernel == 'linear':\n        # For linear kernel, compute the bias directly\n        bias = np.mean(labels - np.dot(data, data.T).dot(alpha * labels))\n    elif kernel == 'rbf':\n        # For RBF kernel, compute the bias using the kernel matrix\n        pairwise_distances = np.linalg.norm(data[:, np.newaxis] - data, axis=2)\n        kernel_matrix = np.exp(-pairwise_distances ** 2 / (2 * sigma ** 2))\n        bias = np.mean(labels - kernel_matrix.dot(alpha * labels))\n\n    # Round results to the nearest 4th decimal\n    alpha = np.round(alpha, 4)\n    bias = np.round(bias, 4)\n\n    return alpha.tolist(), bias\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    data = np.random.rand(100, 5)\n    labels = np.random.choice([-1, 1], size=100)\n\n    alpha, bias = pegasos_kernel_svm(data, labels, kernel='rbf', lambda_val=0.01, iterations=100, sigma=1.0)\n    print(\"Alpha:\", alpha)\n    print(\"Bias:\", bias)\n```\n\nThe provided Python code defines a function `pegasos_kernel_svm` that implements the Pegasos algorithm for training a kernel SVM classifier. The function takes in a dataset, labels, and various training parameters, and returns the model's alpha coefficients and bias.\n\nHere's a step-by-step breakdown:\n\n1.  **Input Validation and Label Mapping**: The function first checks if the input data and labels have compatible shapes. It then maps the labels to {-1, 1} if they are not already in this format.\n2.  **Initialization**: The function initializes the alpha values to zeros.\n3.  **Pegasos Algorithm Iterations**: The function iterates through the Pegasos algorithm for the specified number of iterations. In each iteration, it computes the learning rate, predictions using the current alpha values, and the hinge loss.\n4.  **Alpha Updates**: The alpha values are updated based on the hinge loss and the learning rate.\n5.  **Bias Computation**: After the iterations are complete, the function computes the bias term using the final alpha values.\n6.  **Rounding Results**: The alpha values and bias are rounded to the nearest 4th decimal place.\n7.  **Returning Results**: The function returns the alpha coefficients as a list and the bias as a float.\n\nThe example usage demonstrates how to call the `pegasos_kernel_svm` function with some random data and labels, and prints the resulting alpha coefficients and bias."}
{"task_id": 22, "completion_id": 0, "completion": "### Sigmoid Activation Function Implementation\n#### Overview\n\nThe sigmoid activation function is a mathematical function that maps any real-valued number to a value between 0 and 1. It's often used in machine learning models, particularly in the context of binary classification problems.\n\n#### Code Implementation\n\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function given an input value z.\n\n    Args:\n    z (float): The input to the sigmoid function.\n\n    Returns:\n    float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    # Calculate the sigmoid of z using the formula: 1 / (1 + e^(-z))\n    sigmoid_output = 1 / (1 + math.exp(-z))\n    \n    # Return the result rounded to four decimal places\n    return round(sigmoid_output, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    z = 1.0  # Example input\n    result = sigmoid(z)\n    print(f\"The sigmoid of {z} is {result}\")\n```\n\n#### Explanation\n\n*   The `sigmoid` function takes a float `z` as input and returns the sigmoid of `z` as a float.\n*   The sigmoid is calculated using the formula `1 / (1 + e^(-z))`, where `e` is the base of the natural logarithm.\n*   The `math.exp(-z)` expression computes `e` raised to the power of `-z`.\n*   The result is rounded to four decimal places using the `round` function before being returned.\n*   An example usage is provided in the `if __name__ == \"__main__\":` block, demonstrating how to call the `sigmoid` function with an input value and print the result."}
{"task_id": 23, "completion_id": 0, "completion": "## Softmax Activation Function Implementation\n### Overview\n\nThe softmax activation function is a common choice for the output layer in multi-class classification problems. It takes a list of scores as input and returns a list of probabilities, where each probability represents the likelihood of the input belonging to a particular class.\n\n### Code Implementation\n\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n\n    Args:\n    scores (list[float]): A list of scores.\n\n    Returns:\n    list[float]: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n    \n    # Calculate the maximum score to avoid potential overflow issues\n    max_score = max(scores)\n    \n    # Subtract the maximum score from all scores to improve numerical stability\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    \n    # Calculate the sum of the exponentials\n    sum_exp_scores = sum(exp_scores)\n    \n    # Compute the softmax values\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    \n    return softmax_values\n\n# Example usage:\nif __name__ == \"__main__\":\n    scores = [3.0, 1.0, 0.2]\n    print(softmax(scores))\n```\n\n### Explanation\n\n1. **Numerical Stability**: To avoid potential overflow issues when computing the exponentials, we subtract the maximum score from all scores. This is a common technique to improve numerical stability.\n2. **Softmax Computation**: We calculate the softmax values by dividing each exponential score by the sum of all exponential scores.\n3. **Rounding**: The softmax values are rounded to four decimal places as per the problem requirements.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `softmax` function with a list of scores `[3.0, 1.0, 0.2]`. The output will be the corresponding softmax values, each rounded to four decimal places."}
{"task_id": 24, "completion_id": 0, "completion": "Here's a Python function that simulates a single neuron with a sigmoid activation function for binary classification, handling multidimensional input features.\n\n```python\nimport math\nimport numpy as np\n\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Computes the sigmoid of x.\n    \n    Args:\n    x (float): Input to the sigmoid function.\n    \n    Returns:\n    float: Sigmoid of x.\n    \"\"\"\n    return 1 / (1 + math.exp(-x))\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Args:\n    features (list[list[float]]): A list of feature vectors, where each vector represents multiple features for an example.\n    labels (list[int]): Associated true binary labels.\n    weights (list[float]): The neuron's weights, one for each feature.\n    bias (float): The neuron's bias.\n\n    Returns:\n    tuple[list[float], float]: A tuple containing the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places.\n    \"\"\"\n\n    # Convert input lists to numpy arrays for efficient computation\n    features_array = np.array(features)\n    weights_array = np.array(weights)\n    labels_array = np.array(labels)\n\n    # Compute the dot product of features and weights, and add the bias\n    linear_combination = np.dot(features_array, weights_array) + bias\n\n    # Apply the sigmoid activation function\n    predicted_probabilities = np.vectorize(sigmoid)(linear_combination)\n\n    # Compute the mean squared error between predicted probabilities and true labels\n    mean_squared_error = np.mean((predicted_probabilities - labels_array) ** 2)\n\n    # Round the results to four decimal places and return\n    return np.round(predicted_probabilities, 4).tolist(), round(mean_squared_error, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    features = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n    labels = [0, 1, 1]\n    weights = [0.7, 0.8]\n    bias = 0.9\n    \n    predicted_probabilities, mean_squared_error = single_neuron_model(features, labels, weights, bias)\n    print(\"Predicted Probabilities:\", predicted_probabilities)\n    print(\"Mean Squared Error:\", mean_squared_error)\n```\n\nIn this code:\n\n1.  We define a `sigmoid` function to compute the sigmoid of a given input.\n2.  The `single_neuron_model` function takes in the feature vectors, true labels, weights, and bias as input.\n3.  It converts the input lists to numpy arrays for efficient computation.\n4.  It computes the dot product of the feature vectors and weights, adds the bias, and applies the sigmoid activation function to obtain the predicted probabilities.\n5.  The mean squared error between the predicted probabilities and true labels is calculated.\n6.  Finally, the predicted probabilities and mean squared error are rounded to four decimal places and returned.\n\nThe example usage demonstrates how to call the `single_neuron_model` function with sample inputs and print the predicted probabilities and mean squared error."}
{"task_id": 25, "completion_id": 0, "completion": "Here's a Python function that simulates a single neuron with sigmoid activation and implements backpropagation to update the neuron's weights and bias.\n\n```python\nimport numpy as np\n\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"The sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation and implements backpropagation to update the neuron's weights and bias.\n\n    Args:\n    - features (np.ndarray): A 2D array where each row is a feature vector.\n    - labels (np.ndarray): A 1D array of true binary labels associated with the feature vectors.\n    - initial_weights (np.ndarray): The initial weights of the neuron.\n    - initial_bias (float): The initial bias of the neuron.\n    - learning_rate (float): The learning rate for gradient descent.\n    - epochs (int): The number of epochs to train the neuron.\n\n    Returns:\n    - updated_weights (np.ndarray): The updated weights of the neuron.\n    - updated_bias (float): The updated bias of the neuron.\n    - mse_values (list[float]): A list of MSE values for each epoch.\n    \"\"\"\n\n    # Initialize weights and bias\n    weights = initial_weights.copy()\n    bias = initial_bias\n\n    # Initialize list to store MSE values\n    mse_values = []\n\n    # Train the neuron for the specified number of epochs\n    for _ in range(epochs):\n        # Forward pass: Compute the predicted probabilities\n        linear_combination = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_combination)\n\n        # Compute the Mean Squared Error (MSE)\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n\n        # Backward pass: Compute the gradients\n        d_predictions = 2 * (predictions - labels)\n        d_linear_combination = d_predictions * predictions * (1 - predictions)\n        d_weights = np.dot(features.T, d_linear_combination)\n        d_bias = np.sum(d_linear_combination)\n\n        # Update the weights and bias using gradient descent\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n\n    # Return the updated weights, bias, and MSE values\n    return np.round(weights, 4), round(bias, 4), mse_values\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    features = np.random.rand(100, 5)\n    labels = np.random.randint(0, 2, 100)\n    initial_weights = np.random.rand(5)\n    initial_bias = np.random.rand(1)[0]\n    learning_rate = 0.01\n    epochs = 100\n\n    updated_weights, updated_bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n    print(\"Updated Weights:\", updated_weights.tolist())\n    print(\"Updated Bias:\", updated_bias)\n    print(\"MSE Values:\", mse_values)\n```\n\nThe provided Python code defines a function `train_neuron` that simulates a single neuron with sigmoid activation and implements backpropagation to update the neuron's weights and bias. The function takes in the required parameters, trains the neuron using gradient descent based on the Mean Squared Error (MSE) loss, and returns the updated weights, bias, and a list of MSE values for each epoch. The results are rounded to the nearest 4th decimal place as required. The code also includes an example usage in the `if __name__ == \"__main__\":` block to demonstrate how to use the `train_neuron` function."}
{"task_id": 26, "completion_id": 0, "completion": "## Implementing Basic Autograd Operations\n### Overview\n\nThe task is to implement a Python class that supports basic autograd operations: addition, multiplication, and ReLU activation. The class should handle scalar values and compute gradients for these operations through automatic differentiation.\n\n### Implementation\n\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        \"\"\"\n        Initialize a Value object.\n\n        Args:\n        - data (float): The scalar value.\n        - _children (tuple, optional): The children nodes. Defaults to ().\n        - _op (str, optional): The operation that created this node. Defaults to ''.\n        \"\"\"\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the Value object.\n        \"\"\"\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        \"\"\"\n        Implement addition operation.\n\n        Args:\n        - other (Value or float): The other operand.\n\n        Returns:\n        - Value: The result of the addition.\n        \"\"\"\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        \"\"\"\n        Implement multiplication operation.\n\n        Args:\n        - other (Value or float): The other operand.\n\n        Returns:\n        - Value: The result of the multiplication.\n        \"\"\"\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        \"\"\"\n        Implement ReLU activation.\n\n        Returns:\n        - Value: The result of the ReLU activation.\n        \"\"\"\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            if self.data > 0:\n                self.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        \"\"\"\n        Compute gradients through automatic differentiation.\n        \"\"\"\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create Value objects\n    a = Value(2)\n    b = Value(3)\n\n    # Perform operations\n    c = a * b\n    d = c + a\n    e = d.relu()\n\n    # Compute gradients\n    e.backward()\n\n    # Print results\n    print(\"Value of e:\", e.data)\n    print(\"Gradient of e w.r.t. e:\", e.grad)\n    print(\"Gradient of e w.r.t. d:\", d.grad)\n    print(\"Gradient of e w.r.t. c:\", c.grad)\n    print(\"Gradient of e w.r.t. a:\", a.grad)\n    print(\"Gradient of e w.r.t. b:\", b.grad)\n```\n\n### Explanation\n\nThe provided `Value` class is designed to handle scalar values and support basic autograd operations. Here's a breakdown of the implementation:\n\n1.  The `__init__` method initializes a `Value` object with the given `data`, `_children`, and `_op`. The `grad` attribute is initialized to 0, and the `_backward` method is set to a lambda function that does nothing by default.\n2.  The `__repr__` method returns a string representation of the `Value` object, including its `data` and `grad` values.\n3.  The `__add__` and `__mul__` methods implement addition and multiplication operations, respectively. They create a new `Value` object with the result of the operation and define the `_backward` method to compute gradients.\n4.  The `relu` method implements the ReLU activation function. It creates a new `Value` object with the result of the ReLU operation and defines the `_backward` method to compute gradients.\n5.  The `backward` method computes gradients through automatic differentiation. It first builds a topological ordering of the nodes using a depth-first search (DFS) approach. Then, it sets the gradient of the output node to 1 and iterates through the nodes in reverse topological order, calling their `_backward` methods to compute gradients.\n\nThe example usage demonstrates how to create `Value` objects, perform operations, and compute gradients using the `backward` method. The output shows the value of the final result `e` and the gradients of `e` with respect to its predecessors."}
{"task_id": 27, "completion_id": 0, "completion": "## Transformation Matrix from Basis B to C\n### Problem Description\nGiven basis vectors in two different bases B and C for R^3, we need to compute the transformation matrix P from basis B to C.\n\n### Solution\n\nTo find the transformation matrix P from basis B to C, we need to follow these steps:\n1. Form the matrices B and C using the given basis vectors.\n2. Compute the transformation matrix P using the formula: P = C^(-1) * B.\n\nHere's a Python function that accomplishes this:\n\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    This function computes the transformation matrix P from basis B to C.\n\n    Args:\n    B (list[list[int]]): Basis vectors in basis B.\n    C (list[list[int]]): Basis vectors in basis C.\n\n    Returns:\n    list[list[float]]: The transformation matrix P from basis B to C.\n    \"\"\"\n\n    # Convert the input lists to NumPy arrays\n    B_array = np.array(B, dtype=float)\n    C_array = np.array(C, dtype=float)\n\n    # Compute the inverse of C\n    C_inv = np.linalg.inv(C_array)\n\n    # Compute the transformation matrix P = C^(-1) * B\n    P = np.dot(C_inv, B_array)\n\n    # Round the result to the nearest 4th decimal\n    P = np.round(P, 4)\n\n    # Return the transformation matrix as a Python list\n    return P.tolist()\n\n# Example usage:\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]  # Standard basis\nC = [[2, 0, 0], [0, 3, 0], [0, 0, 4]]  # Another basis\n\nP = transform_basis(B, C)\nprint(\"Transformation Matrix P:\")\nfor row in P:\n    print(row)\n```\n\n### Explanation\n\n1. We first convert the input lists `B` and `C` to NumPy arrays `B_array` and `C_array`.\n2. We compute the inverse of `C_array` using `np.linalg.inv`.\n3. We then compute the transformation matrix `P` by multiplying the inverse of `C_array` with `B_array` using `np.dot`.\n4. We round the result to the nearest 4th decimal using `np.round`.\n5. Finally, we return the transformation matrix `P` as a Python list using the `tolist` method.\n\nThis code is well-structured, readable, and follows the given constraints. It also includes an example usage to demonstrate how to use the `transform_basis` function."}
{"task_id": 28, "completion_id": 0, "completion": "## SVD of a 2x2 Matrix using Eigen Values & Vectors\n### Task Description\nThe task is to write a Python function to compute the Singular Value Decomposition (SVD) of a given 2x2 matrix. The function should return the matrices U, S, and V such that A = U * S * V^T.\n\n### Code\n\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the matrices U, S, and V such that A = U * S * V^T.\n    \"\"\"\n\n    # Ensure A is a numpy array\n    A = np.array(A)\n\n    # Check if A is a 2x2 matrix\n    assert A.shape == (2, 2), \"Input matrix must be 2x2\"\n\n    # Compute A^T * A\n    A_T_A = np.dot(A.T, A)\n\n    # Compute the eigenvalues and eigenvectors of A^T * A\n    eigen_values, V = np.linalg.eig(A_T_A)\n\n    # Sort the eigenvalues and eigenvectors in descending order\n    idx = np.argsort(eigen_values)[::-1]\n    eigen_values = eigen_values[idx]\n    V = V[:, idx]\n\n    # Ensure the determinant of V is 1 (i.e., it's a rotation matrix)\n    if np.linalg.det(V) < 0:\n        V[:, 1] *= -1\n\n    # Compute the singular values\n    singular_values = np.sqrt(eigen_values)\n\n    # Create the S matrix\n    S = np.diag(singular_values)\n\n    # Compute U = A * V * S^-1\n    U = np.dot(A, np.dot(V, np.linalg.inv(S)))\n\n    # Ensure the determinant of U is 1 (i.e., it's a rotation matrix)\n    if np.linalg.det(U) < 0:\n        U[:, 1] *= -1\n        S[1, 1] *= -1\n\n    # Round the results to the nearest 4th decimal\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n\n    # Return the results as Python lists\n    return U.tolist(), S.tolist(), V.tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([[1, 2], [3, 4]])\n    U, S, V = svd_2x2(A)\n    print(\"U:\", U)\n    print(\"S:\", S)\n    print(\"V:\", V)\n```\n\n### Explanation\n\n1.  The function `svd_2x2` takes a 2x2 matrix `A` as input and returns the matrices `U`, `S`, and `V` such that `A = U * S * V^T`.\n2.  It first computes `A^T * A` and then calculates the eigenvalues and eigenvectors of `A^T * A` using `np.linalg.eig`.\n3.  The eigenvalues and eigenvectors are sorted in descending order, and the eigenvectors are adjusted to ensure that `V` is a rotation matrix (i.e., its determinant is 1).\n4.  The singular values are computed as the square root of the eigenvalues, and the `S` matrix is created as a diagonal matrix with the singular values.\n5.  The `U` matrix is computed as `A * V * S^-1`, and its determinant is adjusted to ensure that it's a rotation matrix.\n6.  Finally, the results are rounded to the nearest 4th decimal and returned as Python lists.\n\n### Example Usage\n\nThe example usage demonstrates how to call the `svd_2x2` function with a sample 2x2 matrix `A` and print the resulting `U`, `S`, and `V` matrices."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n\n    Args:\n        X (numpy array): The feature dataset.\n        y (numpy array): The target dataset.\n        seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        list: A list containing the shuffled X and y datasets.\n    \"\"\"\n\n    # Check if a seed is provided for reproducibility\n    if seed is not None:\n        # Set the seed for the random number generator\n        np.random.seed(seed)\n    \n    # Check if the lengths of X and y are the same\n    assert len(X) == len(y), \"X and y must have the same length\"\n\n    # Generate a random permutation of the indices\n    indices = np.arange(len(X))\n    np.random.shuffle(indices)\n\n    # Shuffle X and y using the generated indices\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n\n    # Reshape the shuffled arrays and convert them to lists\n    X_shuffled_list = X_shuffled.tolist()\n    y_shuffled_list = y_shuffled.tolist()\n\n    # Return the shuffled lists\n    return [X_shuffled_list, y_shuffled_list]\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 2])\nprint(shuffle_data(X, y, seed=42))\n```"}
{"task_id": 30, "completion_id": 0, "completion": "## Batch Iterator for Dataset\n### Implementation\n\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Creates a batch iterator for the given numpy array X and optional numpy array y.\n\n    Args:\n    - X (numpy array): The input data.\n    - y (numpy array, optional): The target data. Defaults to None.\n    - batch_size (int, optional): The size of each batch. Defaults to 64.\n\n    Yields:\n    - batches of X if y is None, otherwise batches of (X, y) pairs.\n    \"\"\"\n    # Check if X is a numpy array\n    if not isinstance(X, np.ndarray):\n        raise TypeError(\"X must be a numpy array\")\n\n    # Check if y is a numpy array if it's not None\n    if y is not None and not isinstance(y, np.ndarray):\n        raise TypeError(\"y must be a numpy array or None\")\n\n    # Check if the length of X and y (if provided) are the same\n    if y is not None and len(X) != len(y):\n        raise ValueError(\"X and y must have the same length\")\n\n    # Calculate the number of batches\n    num_batches = int(np.ceil(len(X) / batch_size))\n\n    # Iterate over the batches\n    for i in range(num_batches):\n        # Calculate the start and end indices for the current batch\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, len(X))\n\n        # Slice the current batch from X and y (if provided)\n        X_batch = X[start_idx:end_idx]\n        y_batch = y[start_idx:end_idx] if y is not None else None\n\n        # Yield the batch(es)\n        if y_batch is not None:\n            # Yield a batch of (X, y) pairs\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            # Yield a batch of X\n            yield X_batch.tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create sample data\n    X = np.arange(100).reshape(-1, 1)\n    y = np.random.randint(0, 2, size=(100,))\n\n    # Use the batch iterator\n    batch_size = 10\n    for i, batch in enumerate(batch_iterator(X, y, batch_size)):\n        if isinstance(batch, tuple):\n            X_batch, y_batch = batch\n            print(f\"Batch {i+1}: X_batch.shape = {np.array(X_batch).shape}, y_batch.shape = {np.array(y_batch).shape}\")\n        else:\n            print(f\"Batch {i+1}: X_batch.shape = {np.array(batch).shape}\")\n\n    # Test with y=None\n    for i, batch in enumerate(batch_iterator(X, batch_size=batch_size)):\n        print(f\"Batch {i+1}: X_batch.shape = {np.array(batch).shape}\")\n```\n\n### Explanation\n\n*   The `batch_iterator` function takes in a numpy array `X`, an optional numpy array `y`, and a `batch_size`.\n*   It first checks if `X` and `y` (if provided) are numpy arrays and if they have the same length.\n*   It calculates the number of batches based on the length of `X` and the `batch_size`.\n*   The function then iterates over the batches, slicing the corresponding elements from `X` and `y` (if provided).\n*   It yields the batches, either as `(X, y)` pairs or just `X`, depending on whether `y` is provided.\n*   The yielded batches are converted to Python lists using numpy's `tolist()` method.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `batch_iterator` function with sample data `X` and `y`. It shows how to iterate over the batches and print their shapes. The example also tests the function with `y=None`."}
{"task_id": 31, "completion_id": 0, "completion": "### Divide Dataset Based on Feature Threshold\n\nHere's a Python function that divides a dataset into two subsets based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide a dataset into two subsets based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n    Args:\n    - X (numpy array): The input dataset.\n    - feature_i (int): The index of the feature to check.\n    - threshold (float): The threshold value.\n\n    Returns:\n    - subset1 (list): Samples where the feature value is greater than or equal to the threshold.\n    - subset2 (list): Samples where the feature value is less than the threshold.\n    \"\"\"\n\n    # Check if the input is a numpy array\n    if not isinstance(X, np.ndarray):\n        raise ValueError(\"Input dataset must be a numpy array\")\n\n    # Check if the feature index is within bounds\n    if feature_i < 0 or feature_i >= X.shape[1]:\n        raise IndexError(\"Feature index out of range\")\n\n    # Divide the dataset based on the feature threshold\n    subset1 = X[X[:, feature_i] >= threshold].tolist()\n    subset2 = X[X[:, feature_i] < threshold].tolist()\n\n    return subset1, subset2\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n    # Divide the dataset based on the feature threshold\n    feature_i = 1\n    threshold = 5\n    subset1, subset2 = divide_on_feature(X, feature_i, threshold)\n\n    print(\"Subset 1 (feature >= threshold):\", subset1)\n    print(\"Subset 2 (feature < threshold):\", subset2)\n```\n\n#### Explanation\n\n1.  The function `divide_on_feature` takes three parameters: `X` (the input dataset as a numpy array), `feature_i` (the index of the feature to check), and `threshold` (the threshold value).\n2.  It first checks if the input `X` is a numpy array and raises a `ValueError` if not.\n3.  Then, it checks if the `feature_i` is within the valid range of feature indices for `X` and raises an `IndexError` if it's out of range.\n4.  The function then divides the dataset `X` into two subsets based on the condition `X[:, feature_i] >= threshold` using numpy's boolean indexing.\n5.  The resulting subsets are converted to Python lists using the `tolist()` method and returned.\n6.  In the example usage, a sample dataset `X` is created, and the function is called with `feature_i = 1` and `threshold = 5`. The resulting subsets are printed to demonstrate the function's output."}
{"task_id": 32, "completion_id": 0, "completion": "Here's a Python function that generates polynomial features for a given dataset.\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n\n    Parameters:\n    X (2D numpy array): Input dataset.\n    degree (int): Degree of polynomial features.\n\n    Returns:\n    list: A list of polynomial features up to the specified degree.\n    \"\"\"\n    # Get the number of features in the input dataset\n    n_features = X.shape[1]\n    \n    # Initialize a list to store the polynomial features\n    poly_features = []\n    \n    # Iterate over each degree from 1 to the specified degree\n    for d in range(1, degree + 1):\n        # Generate all combinations of features with replacement for the current degree\n        for combo in combinations_with_replacement(range(n_features), d):\n            # Compute the product of the features in the current combination\n            feature = np.prod(X[:, combo], axis=1)\n            # Append the computed feature to the list of polynomial features\n            poly_features.append(feature)\n    \n    # Stack the polynomial features into a 2D numpy array\n    poly_features = np.array(poly_features).T\n    \n    # Return the polynomial features as a list after reshaping\n    return poly_features.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    X = np.array([[1, 2], [3, 4]])\n    degree = 2\n    \n    # Generate polynomial features\n    poly_features = polynomial_features(X, degree)\n    print(poly_features)\n```\n\nIn this code:\n\n1.  We define a function `polynomial_features` that takes a 2D numpy array `X` and an integer `degree` as input.\n2.  We iterate over each degree from 1 to the specified degree and generate all combinations of features with replacement using `combinations_with_replacement`.\n3.  For each combination, we compute the product of the corresponding features in `X` using `np.prod`.\n4.  We stack the computed features into a 2D numpy array `poly_features` and return it as a list after reshaping using the `tolist()` method.\n\nThe example usage demonstrates how to generate polynomial features for a sample dataset `X` with `degree = 2`. The output will be a list of polynomial features up to the specified degree."}
{"task_id": 33, "completion_id": 0, "completion": "## Task: Generate Random Subsets of a Dataset\n### Description\nThe task is to create a Python function that generates random subsets of a given dataset. The function takes in a 2D numpy array `X`, a 1D numpy array `y`, an integer `n_subsets`, and a boolean `replacements`. It returns a list of `n_subsets` random subsets of the dataset.\n\n### Code\n\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (2D numpy array): Feature dataset.\n    y (1D numpy array): Target variable.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to generate subsets with replacements. Default is True.\n    seed (int): Random seed for reproducibility. Default is 42.\n\n    Returns:\n    list: A list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Get the number of samples in the dataset\n    n_samples = X.shape[0]\n\n    # Initialize an empty list to store the subsets\n    subsets = []\n\n    # Generate n_subsets random subsets\n    for _ in range(n_subsets):\n        # If replacements is True, generate a random subset with replacements\n        if replacements:\n            # Generate a random index array with replacements\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        # If replacements is False, generate a random subset without replacements\n        else:\n            # Generate a random index array without replacements\n            indices = np.random.choice(n_samples, size=n_samples, replace=False)\n\n        # Create a subset using the generated indices\n        X_subset = X[indices]\n        y_subset = y[indices]\n\n        # Append the subset to the list of subsets\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n\n    # Return the list of subsets\n    return subsets\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate a random dataset\n    X = np.random.rand(100, 5)\n    y = np.random.randint(0, 2, 100)\n\n    # Generate 5 random subsets with replacements\n    subsets_with_replacements = get_random_subsets(X, y, 5, replacements=True)\n\n    # Generate 5 random subsets without replacements\n    subsets_without_replacements = get_random_subsets(X, y, 5, replacements=False)\n\n    print(\"Subsets with replacements:\")\n    print(subsets_with_replacements)\n\n    print(\"\\nSubsets without replacements:\")\n    print(subsets_without_replacements)\n```\n\n### Explanation\n\n1. The function `get_random_subsets` takes in the required parameters and sets the random seed for reproducibility.\n2. It generates `n_subsets` random subsets of the dataset based on the value of `replacements`.\n3. If `replacements` is `True`, it generates a random subset with replacements by creating a random index array with replacements using `np.random.choice`.\n4. If `replacements` is `False`, it generates a random subset without replacements by creating a random index array without replacements using `np.random.choice`.\n5. It creates a subset using the generated indices and appends it to the list of subsets.\n6. Finally, it returns the list of subsets after reshaping using numpy's `tolist()` method.\n\n### Example Usage\n\nThe example usage demonstrates how to generate 5 random subsets with and without replacements from a random dataset. The generated subsets are printed to the console."}
{"task_id": 34, "completion_id": 0, "completion": "# One-Hot Encoding of Nominal Values\n## Task Description\nThe task is to write a Python function to perform one-hot encoding of nominal values. The function should take a 1D numpy array `x` of integer values and an optional integer `n_col` representing the number of columns for the one-hot encoded array.\n\n## Code Solution\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    One-hot encoding of nominal values.\n\n    Parameters:\n    x (numpy.ndarray): A 1D numpy array of integer values.\n    n_col (int, optional): The number of columns for the one-hot encoded array. Defaults to None.\n\n    Returns:\n    list: A Python list representing the one-hot encoded array after reshaping.\n    \"\"\"\n    \n    # Check if input is a numpy array\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy array\")\n    \n    # Check if input array is 1D\n    if len(x.shape) != 1:\n        raise ValueError(\"Input array must be 1D\")\n    \n    # If n_col is not provided, determine it from the maximum value in x plus one\n    if n_col is None:\n        n_col = np.max(x) + 1\n    \n    # Check if n_col is a positive integer\n    if not isinstance(n_col, int) or n_col <= 0:\n        raise ValueError(\"n_col must be a positive integer\")\n    \n    # Perform one-hot encoding using numpy's eye function\n    one_hot = np.eye(n_col)[x]\n    \n    # Return the one-hot encoded array as a Python list after reshaping\n    return one_hot.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    x = np.array([0, 1, 2, 0, 1])\n    print(to_categorical(x))\n    # Output: [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]\n    \n    x = np.array([0, 1, 3, 0, 2])\n    print(to_categorical(x, n_col=4))\n    # Output: [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]]\n```\n\n## Explanation\n\n1.  The function `to_categorical` takes a 1D numpy array `x` and an optional integer `n_col` as input.\n2.  It first checks if the input `x` is a numpy array and if it's 1D. If not, it raises a `TypeError` or `ValueError` accordingly.\n3.  If `n_col` is not provided, it determines the number of columns for the one-hot encoded array by finding the maximum value in `x` and adding 1.\n4.  It checks if `n_col` is a positive integer. If not, it raises a `ValueError`.\n5.  The function uses numpy's `eye` function to perform one-hot encoding. `np.eye(n_col)` creates an identity matrix of size `n_col x n_col`, and indexing it with `x` gives the one-hot encoded array.\n6.  Finally, it returns the one-hot encoded array as a Python list after reshaping using the `tolist()` method.\n\n## Example Usage\n\nThe example usage demonstrates how to use the `to_categorical` function with and without specifying `n_col`. The output is a Python list representing the one-hot encoded array."}
{"task_id": 35, "completion_id": 0, "completion": "## Task: Convert Vector to Diagonal Matrix\n### Description:\nThe task is to write a Python function that takes a 1D numpy array and returns a 2D numpy array representing a diagonal matrix. The output should be converted to a Python list.\n\n### Code:\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n\n    Args:\n    x (numpy.ndarray): A 1D numpy array.\n\n    Returns:\n    list: A 2D Python list representing the diagonal matrix.\n    \"\"\"\n    # Check if input is a numpy array\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy array\")\n\n    # Check if input array is 1D\n    if len(x.shape) != 1:\n        raise ValueError(\"Input array must be 1D\")\n\n    # Create a diagonal matrix using numpy's diag function\n    diagonal_matrix = np.diag(x)\n\n    # Return the diagonal matrix as a Python list\n    return diagonal_matrix.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a 1D numpy array\n    x = np.array([1, 2, 3, 4, 5])\n\n    # Convert the array to a diagonal matrix and print the result\n    print(make_diagonal(x))\n```\n\n### Explanation:\n\n1. The function `make_diagonal` takes a 1D numpy array `x` as input.\n2. It checks if the input is a numpy array and if it's 1D. If not, it raises a `TypeError` or `ValueError` accordingly.\n3. It uses numpy's `diag` function to create a diagonal matrix from the input array.\n4. The resulting diagonal matrix is converted to a Python list using the `tolist` method.\n5. The function returns this list.\n\n### Example Output:\nFor the input `np.array([1, 2, 3, 4, 5])`, the output will be:\n```python\n[[1, 0, 0, 0, 0],\n [0, 2, 0, 0, 0],\n [0, 0, 3, 0, 0],\n [0, 0, 0, 4, 0],\n [0, 0, 0, 0, 5]]\n```"}
{"task_id": 36, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Understanding the task**: The task requires writing a Python function to calculate the accuracy score of a model's predictions. The accuracy score is the proportion of correctly predicted labels out of total labels.\n2. **Input parameters**: The function takes two 1D numpy arrays as input: `y_true` (true labels) and `y_pred` (predicted labels).\n3. **Output**: The function should return the accuracy score as a float, rounded to the nearest 4th decimal.\n4. **Calculating accuracy score**: To calculate the accuracy score, we need to compare the true labels (`y_true`) with the predicted labels (`y_pred`), count the number of correct predictions, and divide it by the total number of labels.\n\n# Fixed solution:\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (numpy array): 1D numpy array containing the true labels.\n    y_pred (numpy array): 1D numpy array containing the predicted labels.\n\n    Returns:\n    float: The accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    # Check if input arrays have the same shape\n    assert y_true.shape == y_pred.shape, \"Input arrays must have the same shape\"\n\n    # Compare y_true and y_pred element-wise to get a boolean array\n    correct_predictions = y_true == y_pred\n    \n    # Count the number of correct predictions\n    num_correct = np.sum(correct_predictions)\n    \n    # Calculate the total number of labels\n    total_labels = len(y_true)\n    \n    # Calculate the accuracy score\n    accuracy = num_correct / total_labels\n    \n    # Return the accuracy score rounded to the nearest 4th decimal\n    return round(accuracy, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([0, 1, 1, 0, 1, 0])\n    y_pred = np.array([0, 1, 0, 0, 1, 0])\n    print(accuracy_score(y_true, y_pred))\n```\n\n# Explanation of changes:\n*   **Added input validation**: We added an `assert` statement to check if the input arrays `y_true` and `y_pred` have the same shape.\n*   **Compared `y_true` and `y_pred` element-wise**: We used the `==` operator to compare `y_true` and `y_pred` element-wise, resulting in a boolean array `correct_predictions`.\n*   **Counted correct predictions**: We used `np.sum` to count the number of `True` values in `correct_predictions`, giving us the number of correct predictions `num_correct`.\n*   **Calculated accuracy score**: We divided `num_correct` by the total number of labels `total_labels` to get the accuracy score.\n*   **Rounded the result**: We used the `round` function to round the accuracy score to the nearest 4th decimal before returning it.\n\n# Tests and example uses:\nThe example usage in the code demonstrates how to use the `accuracy_score` function with sample `y_true` and `y_pred` arrays. You can modify these arrays to test different scenarios."}
{"task_id": 37, "completion_id": 0, "completion": "## Task: Calculate Correlation Matrix\n### Description\nThe task is to create a Python function that calculates the correlation matrix for a given dataset. The function will take in a 2D numpy array `X` and an optional 2D numpy array `Y`. If `Y` is not provided, the function will calculate the correlation matrix of `X` with itself.\n\n### Code\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Args:\n    X (numpy.ndarray): A 2D numpy array.\n    Y (numpy.ndarray, optional): An optional 2D numpy array. Defaults to None.\n\n    Returns:\n    list: The correlation matrix as a list, rounded to the nearest 4th decimal.\n    \"\"\"\n    \n    # If Y is not provided, set Y to X\n    if Y is None:\n        Y = X\n    \n    # Check if X and Y have the same number of observations (rows)\n    assert X.shape[0] == Y.shape[0], \"X and Y must have the same number of observations\"\n    \n    # Calculate the correlation matrix using numpy's corrcoef function\n    correlation_matrix = np.corrcoef(X.T, Y.T)[:X.shape[1], X.shape[1]:]\n    \n    # Round the results to the nearest 4th decimal\n    correlation_matrix = np.round(correlation_matrix, 4)\n    \n    # Return the correlation matrix as a list\n    return correlation_matrix.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate random data\n    np.random.seed(0)\n    X = np.random.rand(100, 3)\n    Y = np.random.rand(100, 2)\n\n    # Calculate the correlation matrix\n    correlation_matrix = calculate_correlation_matrix(X, Y)\n    print(\"Correlation Matrix:\")\n    print(correlation_matrix)\n\n    # Calculate the correlation matrix of X with itself\n    correlation_matrix_X = calculate_correlation_matrix(X)\n    print(\"\\nCorrelation Matrix of X with itself:\")\n    print(correlation_matrix_X)\n```\n\n### Explanation\n\n1. The function `calculate_correlation_matrix` takes in two parameters: `X` and `Y`. `Y` is optional and defaults to `None`.\n2. If `Y` is not provided, it is set to `X` to calculate the correlation matrix of `X` with itself.\n3. The function checks if `X` and `Y` have the same number of observations (rows) and raises an assertion error if they do not.\n4. It calculates the correlation matrix using `np.corrcoef`, which returns the correlation coefficient matrix of the variables.\n5. The correlation matrix is then rounded to the nearest 4th decimal using `np.round`.\n6. Finally, the function returns the correlation matrix as a list using the `tolist()` method.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `calculate_correlation_matrix` function with random data. It calculates the correlation matrix between `X` and `Y`, as well as the correlation matrix of `X` with itself. The results are printed to the console."}
{"task_id": 38, "completion_id": 0, "completion": "## Implementing AdaBoost Fit Method\n### Overview\n\nThe following Python function implements the fit method for an AdaBoost classifier. It takes in a dataset `X`, labels `y`, and the number of classifiers `n_clf` as input, and returns a list of classifiers with their parameters.\n\n### Code\n\n```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implement the fit method for an AdaBoost classifier.\n\n    Parameters:\n    X (numpy array): 2D array of shape (n_samples, n_features) representing the dataset.\n    y (numpy array): 1D array of shape (n_samples,) representing the labels.\n    n_clf (int): Number of classifiers.\n\n    Returns:\n    list: A list of classifiers with their parameters.\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Initialize sample weights\n    w = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n\n    for _ in range(n_clf):\n        # Initialize minimum error and best classifier parameters\n        min_error = float('inf')\n        best_clf = None\n\n        # Iterate over each feature\n        for feature in range(n_features):\n            # Get unique values for the feature\n            thresholds = np.unique(X[:, feature])\n            # Iterate over each threshold\n            for threshold in thresholds:\n                # Predict using the current threshold\n                predictions = np.where(X[:, feature] <= threshold, -1, 1)\n                # Calculate error\n                error = np.sum(w[y != predictions])\n                # Check if the error is less than the minimum error\n                if error < min_error:\n                    min_error = error\n                    best_clf = {'feature': feature, 'threshold': threshold, 'error': min_error}\n\n                # Check for the opposite prediction\n                predictions = np.where(X[:, feature] > threshold, -1, 1)\n                error = np.sum(w[y != predictions])\n                if error < min_error:\n                    min_error = error\n                    best_clf = {'feature': feature, 'threshold': threshold, 'error': min_error, 'invert': True}\n\n        # Calculate alpha\n        alpha = 0.5 * math.log((1 - best_clf['error']) / best_clf['error'])\n        # Update sample weights\n        predictions = np.where(X[:, best_clf['feature']] <= best_clf['threshold'], -1, 1)\n        if 'invert' in best_clf:\n            predictions = np.where(X[:, best_clf['feature']] > best_clf['threshold'], -1, 1)\n        w *= np.exp(-alpha * y * predictions)\n        w /= np.sum(w)\n\n        # Store the classifier\n        classifiers.append({\n            'feature': best_clf['feature'],\n            'threshold': best_clf['threshold'],\n            'alpha': round(alpha, 4),\n            'invert': 'invert' in best_clf\n        })\n\n    return classifiers\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([-1, -1, 1, 1])\n    n_clf = 2\n    classifiers = adaboost_fit(X, y, n_clf)\n    print(classifiers)\n```\n\n### Explanation\n\n1.  Initialize sample weights `w` with equal weights for all samples.\n2.  Iterate `n_clf` times to train multiple classifiers.\n3.  For each iteration, find the best feature and threshold that result in the minimum error.\n4.  Calculate the error for each feature and threshold by comparing the predictions with the actual labels.\n5.  Update the sample weights based on the error and the predictions made by the best classifier.\n6.  Store the parameters of the best classifier, including the feature, threshold, and alpha value.\n7.  Return a list of classifiers with their parameters.\n\nThe provided code implements the AdaBoost fit method using decision stumps as the base classifiers. The `adaboost_fit` function takes in the dataset `X`, labels `y`, and the number of classifiers `n_clf` as input and returns a list of classifiers with their parameters. The example usage demonstrates how to use the function with a sample dataset."}
{"task_id": 39, "completion_id": 0, "completion": "## Log Softmax Function Implementation\n### Overview\n\nThe log-softmax function is a crucial component in many machine learning algorithms, particularly in the context of classification problems. It is used to normalize a vector of scores into a probability distribution. Here, we will implement a Python function to compute the log-softmax of a given 1D numpy array of scores.\n\n### Code Implementation\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    This function computes the log-softmax of a given 1D array of scores.\n\n    Args:\n    scores (list): A list of scores.\n\n    Returns:\n    list: A list representing the log-softmax of the input scores, rounded to the nearest 4th decimal.\n    \"\"\"\n    \n    # Convert the input list to a numpy array for efficient numerical computations\n    scores_array = np.array(scores)\n    \n    # Compute the maximum score to improve numerical stability\n    max_score = np.max(scores_array)\n    \n    # Subtract the maximum score from all scores to prevent overflow\n    stable_scores = scores_array - max_score\n    \n    # Compute the log-softmax\n    log_softmax_values = stable_scores - np.log(np.sum(np.exp(stable_scores)))\n    \n    # Round the results to the nearest 4th decimal and return as a list\n    return np.round(log_softmax_values, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    scores = [1.0, 2.0, 3.0, 4.0, 5.0]\n    print(log_softmax(scores))\n```\n\n### Explanation\n\n1.  The function `log_softmax` takes a list of scores as input and converts it into a numpy array for efficient numerical computations.\n2.  To improve numerical stability, we subtract the maximum score from all scores. This step is crucial when dealing with large numbers to prevent overflow during the computation of the exponential function.\n3.  We then compute the log-softmax by subtracting the log of the sum of the exponentials of the stable scores from the stable scores themselves. This is based on the mathematical definition of the log-softmax function: `log_softmax(x) = x - log(sum(exp(x)))`.\n4.  Finally, we round the computed log-softmax values to the nearest 4th decimal using `np.round` and return the result as a list using the `tolist()` method.\n\n### Advice\n\n*   The implementation uses numpy for efficient numerical computations.\n*   The code includes a docstring that provides a description of the function, its arguments, and its return value.\n*   The example usage demonstrates how to call the `log_softmax` function with a sample list of scores."}
{"task_id": 40, "completion_id": 0, "completion": "Here's the implementation of the `Dense` class based on the given structure and requirements.\n\n```python\nimport numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optimizer = None\n        self.w0_optimizer = None\n\n    def initialize(self, optimizer):\n        # Initialize the weights\n        limit = 1 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        # Weight optimizers\n        self.W_optimizer = copy.copy(optimizer)\n        self.w0_optimizer = copy.copy(optimizer)\n\n    def parameters(self):\n        # Calculate the total number of trainable parameters\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        # Calculate the gradient w.r.t the input\n        grad_wrt_input = np.dot(accum_grad, self.W.T)\n\n        # Calculate the gradient w.r.t the weights and biases\n        grad_wrt_W = np.dot(self.layer_input.T, accum_grad)\n        grad_wrt_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n\n        # Update the weights and biases if the layer is trainable\n        if self.trainable:\n            self.W = self.W_optimizer.update(self.W, grad_wrt_W)\n            self.w0 = self.w0_optimizer.update(self.w0, grad_wrt_w0)\n\n        return grad_wrt_input\n\n    def output_shape(self):\n        return (self.n_units,)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the input shape and number of units\n    input_shape = (10,)\n    n_units = 5\n\n    # Create a Dense layer\n    dense_layer = Dense(n_units, input_shape)\n\n    # Initialize the layer with an optimizer (for demonstration, a simple SGD optimizer is used)\n    class SGD:\n        def __init__(self, learning_rate=0.01):\n            self.learning_rate = learning_rate\n\n        def update(self, w, grad_wrt_w):\n            return w - self.learning_rate * grad_wrt_w\n\n    optimizer = SGD(learning_rate=0.01)\n    dense_layer.initialize(optimizer)\n\n    # Print the number of parameters\n    print(\"Number of parameters:\", dense_layer.parameters())\n\n    # Perform a forward pass\n    X = np.random.rand(1, 10)\n    output = dense_layer.forward_pass(X)\n    print(\"Output shape:\", output.shape)\n    print(\"Output:\", np.round(output.tolist(), 4))\n\n    # Perform a backward pass\n    accum_grad = np.random.rand(1, 5)\n    grad_wrt_input = dense_layer.backward_pass(accum_grad)\n    print(\"Gradient w.r.t input shape:\", grad_wrt_input.shape)\n    print(\"Gradient w.r.t input:\", np.round(grad_wrt_input.tolist(), 4))\n```\n\nThe provided code defines a `Dense` class that inherits from the `Layer` class. The `Dense` class implements the required methods:\n\n1.  `__init__`: Initializes the layer with the specified number of units and input shape.\n2.  `initialize`: Initializes the weights and biases using a uniform distribution and sets up the optimizers.\n3.  `parameters`: Returns the total number of trainable parameters in the layer.\n4.  `forward_pass`: Computes the output of the layer by performing a dot product between the input and the weight matrix, and then adding the bias.\n5.  `backward_pass`: Calculates the gradient with respect to the input and updates the weights and biases using the optimizer's update rule if the layer is trainable.\n6.  `output_shape`: Returns the shape of the output produced by the forward pass.\n\nThe example usage demonstrates how to create a `Dense` layer, initialize it with an optimizer, and perform forward and backward passes. The output is rounded to the nearest 4th decimal place as required."}
{"task_id": 41, "completion_id": 0, "completion": "## Simple Convolutional 2D Layer Implementation\n### Overview\n\nThe following Python function implements a simple 2D convolutional layer. It takes an input matrix, a convolutional kernel, padding, and stride as inputs, and returns the result of the convolution operation.\n\n### Code\n\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int) -> list:\n    \"\"\"\n    Applies a 2D convolution operation on the input matrix using the given kernel, padding, and stride.\n\n    Args:\n    - input_matrix (np.ndarray): The input matrix to be convolved.\n    - kernel (np.ndarray): The convolutional kernel.\n    - padding (int): The number of pixels to pad the input matrix with.\n    - stride (int): The stride of the convolution operation.\n\n    Returns:\n    - list: The result of the convolution operation as a Python list, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    # Get the dimensions of the input matrix and the kernel\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n\n    # Calculate the output dimensions\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n\n    # Pad the input matrix\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n\n    # Initialize the output matrix\n    output_matrix = np.zeros((output_height, output_width))\n\n    # Perform the convolution operation\n    for i in range(output_height):\n        for j in range(output_width):\n            # Calculate the region of the padded input to be convolved with the kernel\n            region = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            # Convolve the region with the kernel and store the result in the output matrix\n            output_matrix[i, j] = np.sum(region * kernel)\n\n    # Round the output to the nearest 4th decimal and return as a Python list\n    return np.round(output_matrix, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    input_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n    padding = 1\n    stride = 1\n\n    result = simple_conv2d(input_matrix, kernel, padding, stride)\n    print(result)\n```\n\n### Explanation\n\n1. The function `simple_conv2d` takes four parameters: `input_matrix`, `kernel`, `padding`, and `stride`.\n2. It calculates the output dimensions based on the input dimensions, kernel size, padding, and stride.\n3. The input matrix is padded with zeros according to the specified padding.\n4. The function then performs the convolution operation by sliding the kernel over the padded input matrix, computing the dot product at each position, and storing the results in the output matrix.\n5. Finally, the output matrix is rounded to the nearest 4th decimal and returned as a Python list.\n\n### Notes\n\n* This implementation assumes that the input matrix and kernel are 2D NumPy arrays.\n* The padding is applied uniformly to all sides of the input matrix.\n* The stride is applied uniformly in both the horizontal and vertical directions."}
{"task_id": 42, "completion_id": 0, "completion": "### Implementing the ReLU Activation Function in Python\n#### Overview\n\nThe Rectified Linear Unit (ReLU) is a widely used activation function in deep learning models. It maps all negative values to 0 and all non-negative values to the same value.\n\n#### Code Implementation\n\n```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function.\n\n    Args:\n    z (float): The input value.\n\n    Returns:\n    float: The value after applying the ReLU function.\n    \"\"\"\n    # If z is greater than 0, return z; otherwise, return 0\n    return max(0, z)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test the ReLU function with different inputs\n    print(relu(5.0))   # Expected output: 5.0\n    print(relu(-3.0))  # Expected output: 0.0\n    print(relu(0.0))   # Expected output: 0.0\n```\n\n#### Explanation\n\n*   The `relu` function takes a single float `z` as input and returns the result after applying the ReLU function.\n*   The `max` function is used to implement the ReLU logic. It returns `z` if it's greater than 0; otherwise, it returns 0.\n*   The example usage demonstrates how to use the `relu` function with different inputs.\n\nThis implementation is concise and readable while effectively applying the ReLU activation function."}
{"task_id": 43, "completion_id": 0, "completion": "# Problem Understanding\nThe task is to implement the Ridge Regression loss function in Python. The function should take a 2D numpy array `X` representing the feature matrix, a 1D numpy array `w` representing the coefficients, a 1D numpy array `y_true` representing the true labels, and a float `alpha` representing the regularization parameter. The function should return the Ridge loss, which combines the Mean Squared Error (MSE) and a regularization term.\n\nKey constraints and edge cases:\n- `X` is a 2D numpy array.\n- `w` is a 1D numpy array.\n- `y_true` is a 1D numpy array.\n- `alpha` is a float.\n- The number of samples in `X` and `y_true` should be the same.\n- The number of features in `X` and the number of coefficients in `w` should be the same.\n\nInput: `X`, `w`, `y_true`, `alpha`.\nOutput: Ridge loss (a float value rounded to the nearest 4th decimal).\n\nSubtle requirements or implicit constraints:\n- The Ridge loss is calculated as the sum of the Mean Squared Error (MSE) and a regularization term.\n- The regularization term is proportional to the square of the magnitude of the coefficients.\n\n# Key Insights\n- The Ridge Regression loss function combines the Mean Squared Error (MSE) and a regularization term.\n- The MSE is calculated as the average of the squared differences between the predicted values and the true labels.\n- The predicted values are obtained by taking the dot product of the feature matrix `X` and the coefficient vector `w`.\n- The regularization term is `alpha` times the sum of the squares of the coefficients `w`.\n- The Ridge loss formula is: `Ridge Loss = (1/n) * ||Xw - y_true||^2 + alpha * ||w||^2`, where `n` is the number of samples.\n- The term `||Xw - y_true||^2` represents the sum of the squared differences between the predicted values and the true labels.\n- The term `||w||^2` represents the sum of the squares of the coefficients.\n- The `alpha` parameter controls the strength of the regularization.\n\n# Solution Strategy\n1. Calculate the predicted values by taking the dot product of `X` and `w`.\n2. Calculate the MSE by finding the average of the squared differences between the predicted values and `y_true`.\n3. Calculate the regularization term by multiplying `alpha` with the sum of the squares of `w`.\n4. Combine the MSE and the regularization term to obtain the Ridge loss.\n5. Round the Ridge loss to the nearest 4th decimal before returning it.\n\nImportant details:\n- The number of samples `n` is obtained from the shape of `X` or `y_true`.\n- The dot product of `X` and `w` is used to calculate the predicted values.\n- The sum of the squared differences is calculated using the `np.sum` function.\n- The sum of the squares of `w` is calculated using the `np.sum` function with `w**2`.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    This function calculates the Ridge Regression loss.\n\n    Parameters:\n    X (np.ndarray): A 2D numpy array representing the feature matrix.\n    w (np.ndarray): A 1D numpy array representing the coefficients.\n    y_true (np.ndarray): A 1D numpy array representing the true labels.\n    alpha (float): A float representing the regularization parameter.\n\n    Returns:\n    float: The Ridge loss, rounded to the nearest 4th decimal.\n    \"\"\"\n    # Calculate the predicted values\n    y_pred = np.dot(X, w)\n    \n    # Calculate the Mean Squared Error (MSE)\n    n = len(y_true)  # Get the number of samples\n    mse = np.sum((y_pred - y_true) ** 2) / n\n    \n    # Calculate the regularization term\n    reg_term = alpha * np.sum(w ** 2)\n    \n    # Calculate the Ridge loss\n    ridge_loss = mse + reg_term\n    \n    # Return the Ridge loss rounded to the nearest 4th decimal\n    return round(ridge_loss, 4)\n```"}
{"task_id": 44, "completion_id": 0, "completion": "### Leaky ReLU Activation Function Implementation\n\nHere's a Python function that implements the Leaky ReLU activation function. The Leaky ReLU function is a variation of the ReLU activation function that allows a small fraction of the input to pass through, even if it's negative.\n\n#### Code\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Args:\n    z (float): The input to the Leaky ReLU function.\n    alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n    float: The value after applying the Leaky ReLU function.\n    \"\"\"\n    # If z is positive, return z; otherwise, return alpha * z\n    return z if z >= 0 else alpha * z\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test the Leaky ReLU function with positive and negative inputs\n    print(leaky_relu(5.0))  # Output: 5.0\n    print(leaky_relu(-3.0))  # Output: -0.03 (with default alpha = 0.01)\n    print(leaky_relu(-3.0, alpha=0.1))  # Output: -0.3\n```\n\n#### Explanation\n\n*   The `leaky_relu` function takes two parameters: `z` (the input value) and `alpha` (the slope for negative inputs, defaulting to 0.01).\n*   It applies the Leaky ReLU activation function by checking if `z` is non-negative. If `z` is non-negative, it returns `z`; otherwise, it returns `alpha * z`.\n*   The example usage demonstrates how to use the `leaky_relu` function with different inputs and alpha values.\n\nThe return type hint has been corrected to `float` as the function will always return a float due to the input `z` being a float and the multiplication with `alpha` (also a float)."}
{"task_id": 45, "completion_id": 0, "completion": "## Linear Kernel Function Implementation\n### Overview\n\nThe task is to implement a Python function `kernel_function` that calculates the linear kernel between two input vectors `x1` and `x2`. The linear kernel is defined as the dot product of two vectors.\n\n### Code\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n\n    The linear kernel is defined as the dot product (inner product) of two vectors.\n\n    Args:\n        x1 (numpy.ndarray): The first input vector.\n        x2 (numpy.ndarray): The second input vector.\n\n    Returns:\n        float: The linear kernel value between x1 and x2.\n\n    Raises:\n        ValueError: If the input vectors are not of the same shape.\n    \"\"\"\n    # Check if input vectors are numpy arrays\n    if not isinstance(x1, np.ndarray) or not isinstance(x2, np.ndarray):\n        raise ValueError(\"Input vectors must be numpy arrays\")\n\n    # Check if input vectors have the same shape\n    if x1.shape != x2.shape:\n        raise ValueError(\"Input vectors must have the same shape\")\n\n    # Compute the linear kernel using numpy's dot product function\n    kernel_value = np.dot(x1, x2)\n\n    return kernel_value\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define two example vectors\n    x1 = np.array([1, 2, 3])\n    x2 = np.array([4, 5, 6])\n\n    # Compute the linear kernel\n    kernel_value = kernel_function(x1, x2)\n    print(\"Linear kernel value:\", kernel_value)\n```\n\n### Explanation\n\n*   The `kernel_function` takes two input vectors `x1` and `x2` as numpy arrays.\n*   It checks if both inputs are numpy arrays and have the same shape, raising a `ValueError` if not.\n*   The linear kernel is computed using `np.dot(x1, x2)`, which calculates the dot product of the two input vectors.\n*   The result is returned as a float value.\n*   An example usage is provided in the `if __name__ == \"__main__\":` block, demonstrating how to use the `kernel_function` with two example vectors."}
{"task_id": 46, "completion_id": 0, "completion": "## Implementing Precision Metric in Python\n### Overview\n\nThe precision metric is a measure used in classification problems to evaluate the performance of a model. It is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP).\n\n### Code Implementation\n\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true binary labels and predicted binary labels.\n\n    Parameters:\n    y_true (numpy array): Array containing true binary labels.\n    y_pred (numpy array): Array containing predicted binary labels.\n\n    Returns:\n    float: Precision value.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Check if inputs have the same shape\n    assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n\n    # Calculate true positives (TP) and false positives (FP)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n\n    # Check for division by zero\n    if tp + fp == 0:\n        return 0.0  # or np.nan, depending on the desired behavior\n\n    # Calculate precision\n    precision_value = tp / (tp + fp)\n\n    return precision_value\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0])\n    y_pred = np.array([0, 1, 0, 0, 1, 1, 0, 1, 1, 0])\n    print(\"Precision:\", precision(y_true, y_pred))\n```\n\n### Explanation\n\n1. The function `precision` takes two numpy arrays `y_true` and `y_pred` as input.\n2. It first ensures that both inputs are numpy arrays using `np.asarray`.\n3. It checks if the shapes of `y_true` and `y_pred` are the same, raising an assertion error if they are not.\n4. It calculates the number of true positives (`tp`) by summing the instances where both `y_true` and `y_pred` are 1.\n5. It calculates the number of false positives (`fp`) by summing the instances where `y_true` is 0 but `y_pred` is 1.\n6. To avoid division by zero, it checks if the sum of `tp` and `fp` is zero. If so, it returns 0.0 (or `np.nan` as an alternative).\n7. Finally, it calculates the precision as the ratio of `tp` to the sum of `tp` and `fp`, and returns this value.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `precision` function with sample `y_true` and `y_pred` arrays. The output will be the precision value for the given predictions."}
{"task_id": 47, "completion_id": 0, "completion": "Here's the implementation of the gradient descent variants with MSE loss:\n\n```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    This function performs three variants of gradient descent - Stochastic Gradient Descent (SGD), \n    Batch Gradient Descent, and Mini-Batch Gradient Descent - using Mean Squared Error (MSE) as the loss function.\n\n    Parameters:\n    X (numpy array): Feature matrix\n    y (numpy array): Target variable\n    weights (numpy array): Initial weights\n    learning_rate (float): Learning rate for gradient descent\n    n_iterations (int): Number of iterations for gradient descent\n    batch_size (int): Size of the batch for Mini-Batch Gradient Descent (default is 1)\n    method (str): Type of gradient descent to perform - 'batch', 'stochastic', or 'mini-batch' (default is 'batch')\n\n    Returns:\n    list: The optimized weights after gradient descent\n    \"\"\"\n\n    # Check if the method is valid\n    if method not in ['batch', 'stochastic', 'mini-batch']:\n        raise ValueError(\"Invalid method. Please choose from 'batch', 'stochastic', or 'mini-batch'.\")\n\n    # Initialize the weights\n    weights = np.array(weights)\n\n    # Perform gradient descent for the specified number of iterations\n    for _ in range(n_iterations):\n        # Batch Gradient Descent\n        if method == 'batch':\n            # Calculate the predictions\n            predictions = np.dot(X, weights)\n            # Calculate the gradient\n            gradient = (2 / len(X)) * np.dot(X.T, (predictions - y))\n            # Update the weights\n            weights = weights - learning_rate * gradient\n\n        # Stochastic Gradient Descent\n        elif method == 'stochastic':\n            # Randomly shuffle the data\n            indices = np.arange(len(X))\n            np.random.shuffle(indices)\n            X = X[indices]\n            y = y[indices]\n            # Calculate the predictions and gradient for each sample\n            for i in range(len(X)):\n                prediction = np.dot(X[i], weights)\n                gradient = 2 * (prediction - y[i]) * X[i]\n                # Update the weights\n                weights = weights - learning_rate * gradient\n\n        # Mini-Batch Gradient Descent\n        elif method == 'mini-batch':\n            # Randomly shuffle the data\n            indices = np.arange(len(X))\n            np.random.shuffle(indices)\n            X = X[indices]\n            y = y[indices]\n            # Calculate the predictions and gradient for each mini-batch\n            for i in range(0, len(X), batch_size):\n                X_batch = X[i:i + batch_size]\n                y_batch = y[i:i + batch_size]\n                predictions = np.dot(X_batch, weights)\n                gradient = (2 / len(X_batch)) * np.dot(X_batch.T, (predictions - y_batch))\n                # Update the weights\n                weights = weights - learning_rate * gradient\n\n    # Return the optimized weights as a list rounded to the nearest 4th decimal\n    return np.round(weights, 4).tolist()\n```\n\nHere's an example usage of the function:\n\n```python\n# Generate some random data\nnp.random.seed(0)\nX = np.random.rand(100, 5)\ny = np.random.rand(100)\nweights = np.random.rand(5)\n\n# Perform Batch Gradient Descent\nweights_batch = gradient_descent(X, y, weights, learning_rate=0.01, n_iterations=1000, method='batch')\nprint(\"Optimized weights (Batch Gradient Descent):\", weights_batch)\n\n# Perform Stochastic Gradient Descent\nweights_stochastic = gradient_descent(X, y, weights, learning_rate=0.01, n_iterations=1000, method='stochastic')\nprint(\"Optimized weights (Stochastic Gradient Descent):\", weights_stochastic)\n\n# Perform Mini-Batch Gradient Descent\nweights_mini_batch = gradient_descent(X, y, weights, learning_rate=0.01, n_iterations=1000, batch_size=32, method='mini-batch')\nprint(\"Optimized weights (Mini-Batch Gradient Descent):\", weights_mini_batch)\n```\n\nThis code implements the three variants of gradient descent using the Mean Squared Error (MSE) loss function. The `gradient_descent` function takes in the feature matrix `X`, target variable `y`, initial weights, learning rate, number of iterations, batch size, and the type of gradient descent to perform. It returns the optimized weights as a list rounded to the nearest 4th decimal. The example usage demonstrates how to use the function to perform Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent."}
{"task_id": 48, "completion_id": 0, "completion": "## Reduced Row Echelon Form (RREF) Implementation\n### Overview\n\nThe following code implements a function to convert a given matrix into its Reduced Row Echelon Form (RREF) using Gaussian elimination.\n\n### Code\n\n```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n\n    Args:\n    matrix (list): A 2D list representing the input matrix.\n\n    Returns:\n    list: The RREF of the input matrix as a Python list.\n    \"\"\"\n    # Convert the input matrix to a NumPy array\n    matrix = np.array(matrix, dtype=float)\n\n    # Get the number of rows and columns in the matrix\n    num_rows, num_cols = matrix.shape\n\n    # Initialize the current row and column indices\n    current_row = 0\n    current_col = 0\n\n    # Perform Gaussian elimination to achieve RREF\n    while current_row < num_rows and current_col < num_cols:\n        # Find the row with the maximum absolute value in the current column\n        max_row = np.argmax(np.abs(matrix[current_row:, current_col])) + current_row\n\n        # If the maximum value is zero, move to the next column\n        if matrix[max_row, current_col] == 0:\n            current_col += 1\n            continue\n\n        # Swap the current row with the row containing the maximum value\n        matrix[[current_row, max_row]] = matrix[[max_row, current_row]]\n\n        # Normalize the current row by dividing it by the pivot element\n        pivot = matrix[current_row, current_col]\n        matrix[current_row] /= pivot\n\n        # Eliminate the pivot element from other rows\n        for row in range(num_rows):\n            if row != current_row:\n                factor = matrix[row, current_col]\n                matrix[row] -= factor * matrix[current_row]\n\n        # Move to the next row and column\n        current_row += 1\n        current_col += 1\n\n    # Return the RREF matrix as a Python list\n    return matrix.tolist()\n\n# Example usage:\nmatrix = [\n    [1, 2, -1, -4],\n    [2, 3, -1, -11],\n    [-2, 0, -3, 22]\n]\n\nrref_matrix = rref(matrix)\nprint(\"RREF Matrix:\")\nfor row in rref_matrix:\n    print(row)\n```\n\n### Explanation\n\n1.  The function `rref` takes a 2D list `matrix` as input and converts it to a NumPy array.\n2.  It initializes variables to track the current row and column indices.\n3.  The function performs Gaussian elimination to achieve RREF by iterating through the matrix.\n4.  In each iteration, it finds the row with the maximum absolute value in the current column and swaps it with the current row.\n5.  The pivot element is normalized to 1, and other rows are updated to eliminate the pivot element.\n6.  The process continues until the entire matrix is processed.\n7.  Finally, the function returns the RREF matrix as a Python list using NumPy's `tolist` method.\n\n### Example Usage\n\nThe provided example demonstrates how to use the `rref` function to convert a given matrix into its RREF. The input matrix is defined, and the resulting RREF matrix is printed to the console."}
{"task_id": 49, "completion_id": 0, "completion": "Here's the Python code that implements the Adam optimization algorithm as per the given task:\n\n```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    \"\"\"\n    Adam Optimization Algorithm.\n\n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): A function that computes the gradient of `f`.\n    x0 (numpy array): Initial parameter values.\n    learning_rate (float, optional): The step size. Defaults to 0.001.\n    beta1 (float, optional): Exponential decay rate for the first moment estimates. Defaults to 0.9.\n    beta2 (float, optional): Exponential decay rate for the second moment estimates. Defaults to 0.999.\n    epsilon (float, optional): A small constant for numerical stability. Defaults to 1e-8.\n    num_iterations (int, optional): Number of iterations to run the optimizer. Defaults to 1000.\n\n    Returns:\n    list: The optimized parameters.\n    \"\"\"\n\n    # Initialize the first and second moment estimates\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n\n    # Initialize the iteration counter\n    t = 0\n\n    # Initialize the current parameters\n    x = x0\n\n    # Run the Adam optimization algorithm for the specified number of iterations\n    for _ in range(num_iterations):\n        # Increment the iteration counter\n        t += 1\n\n        # Compute the gradient of the objective function at the current parameters\n        g = grad(x)\n\n        # Update the first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n\n        # Update the second moment estimate\n        v = beta2 * v + (1 - beta2) * np.square(g)\n\n        # Compute the bias-corrected first and second moment estimates\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n\n        # Update the parameters using the Adam update rule\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # Return the optimized parameters as a list rounded to the nearest 4th decimal\n    return np.round(x, 4).tolist()\n\n# Example usage:\ndef objective_function(x):\n    return np.sum(np.square(x))\n\ndef gradient(x):\n    return 2 * x\n\nx0 = np.array([10.0, 20.0])\noptimized_params = adam_optimizer(objective_function, gradient, x0)\nprint(optimized_params)\n```\n\nThe provided Python code defines a function `adam_optimizer` that implements the Adam optimization algorithm. The function takes an objective function `f`, its gradient `grad`, initial parameters `x0`, and several optional parameters that control the Adam algorithm. It returns the optimized parameters as a list.\n\nHere's a step-by-step breakdown:\n\n1.  Initialize the first and second moment estimates (`m` and `v`) with zeros, and set the iteration counter `t` to 0.\n2.  Run the Adam optimization algorithm for the specified number of iterations (`num_iterations`).\n3.  In each iteration, compute the gradient of the objective function at the current parameters (`g = grad(x)`).\n4.  Update the first and second moment estimates (`m` and `v`) using the gradient and the decay rates (`beta1` and `beta2`).\n5.  Compute the bias-corrected first and second moment estimates (`m_hat` and `v_hat`) to avoid bias towards zero during the initial iterations.\n6.  Update the parameters (`x`) using the Adam update rule, which involves the learning rate (`learning_rate`), the bias-corrected moment estimates, and a small constant (`epsilon`) for numerical stability.\n7.  After the specified number of iterations, return the optimized parameters as a list, rounded to the nearest 4th decimal.\n\nThe code includes an example usage of the `adam_optimizer` function with a simple objective function (`objective_function`) and its gradient (`gradient`). The optimized parameters are printed to demonstrate the output."}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    \"\"\"\n    This function implements Lasso Regression using Gradient Descent.\n\n    Parameters:\n    X (np.array): Feature matrix\n    y (np.array): Target variable\n    alpha (float): Regularization parameter (default = 0.1)\n    learning_rate (float): Learning rate for gradient descent (default = 0.01)\n    max_iter (int): Maximum number of iterations (default = 1000)\n    tol (float): Tolerance for convergence (default = 1e-4)\n\n    Returns:\n    tuple: A tuple containing the weights and bias.\n    \"\"\"\n\n    # Initialize weights and bias\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n\n    # Gradient Descent\n    for _ in range(max_iter):\n        # Predict\n        y_pred = np.dot(X, weights) + bias\n\n        # Compute gradients\n        weights_grad = (-2 / n_samples) * np.dot(X.T, (y - y_pred)) + alpha * np.sign(weights)\n        bias_grad = (-2 / n_samples) * np.sum(y - y_pred)\n\n        # Update weights and bias\n        new_weights = weights - learning_rate * weights_grad\n        new_bias = bias - learning_rate * bias_grad\n\n        # Check convergence\n        if np.linalg.norm(weights - new_weights) < tol and np.abs(bias - new_bias) < tol:\n            break\n\n        # Update weights and bias\n        weights = new_weights\n        bias = new_bias\n\n    # Round results to 4 decimal places\n    weights = np.round(weights, 4)\n    bias = np.round(bias, 4)\n\n    # Return weights and bias as python lists\n    return weights.tolist(), bias\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(100)\n\n    # Run Lasso Regression\n    weights, bias = l1_regularization_gradient_descent(X, y)\n\n    print(\"Weights:\", weights)\n    print(\"Bias:\", bias)\n```\n\n### Explanation\n\n*   The function `l1_regularization_gradient_descent` takes in the feature matrix `X`, target variable `y`, regularization parameter `alpha`, learning rate, maximum number of iterations, and tolerance for convergence.\n*   It initializes the weights and bias to zero and then iteratively updates them using gradient descent. The gradient of the loss function with respect to the weights includes the L1 penalty term `alpha * np.sign(weights)`, which helps in shrinking some of the feature coefficients to zero.\n*   The function checks for convergence by comparing the norm of the difference between the current and new weights, as well as the absolute difference between the current and new bias, with the specified tolerance.\n*   Finally, it returns the optimized weights and bias as Python lists, rounded to the nearest 4th decimal place."}
{"task_id": 51, "completion_id": 0, "completion": "To solve this problem, we will implement a function `OSA` that calculates the Optimal String Alignment distance between two given strings `source` and `target`. The OSA distance is a measure of the minimum number of operations (insertions, deletions, substitutions, and transpositions of adjacent characters) required to transform one string into another.\n\nHere's a step-by-step breakdown of how we can implement this:\n\n1. **Initialize a Matrix**: We will create a 2D matrix `d` of size `(len(source) + 1) x (len(target) + 1)` to store the OSA distances between substrings of `source` and `target`. The extra row and column are for handling edge cases where one of the strings is empty.\n\n2. **Base Case Initialization**: We initialize the first row and column of the matrix `d` to represent the OSA distance when one of the strings is empty. The distance is simply the length of the non-empty string, as we need to insert or delete all characters of the non-empty string.\n\n3. **Dynamic Programming**: We then fill in the rest of the matrix using dynamic programming. For each cell `d[i, j]`, we consider the minimum cost of transforming the first `i` characters of `source` into the first `j` characters of `target`. We consider four operations:\n   - Insertion: `d[i, j-1] + 1`\n   - Deletion: `d[i-1, j] + 1`\n   - Substitution or no operation (if the characters match): `d[i-1, j-1] + (1 if source[i-1] != target[j-1] else 0)`\n   - Transposition: If the current character in `source` matches the previous character in `target` and vice versa, we consider transposing the previous character in `source` with the current one: `d[i-2, j-2] + 1` (this is valid only if `i > 1` and `j > 1`).\n\n4. **Return the OSA Distance**: The OSA distance between `source` and `target` is stored in the bottom-right cell of the matrix, `d[len(source), len(target)]`.\n\nHere's the Python code that implements the above steps:\n\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n\n    The OSA distance is the minimum number of operations (insertions, deletions, \n    substitutions, and transpositions of adjacent characters) required to transform \n    one string into another.\n\n    Parameters:\n    source (str): The source string.\n    target (str): The target string.\n\n    Returns:\n    int: The OSA distance between source and target.\n    \"\"\"\n\n    # Initialize a matrix to store the OSA distances between substrings\n    m, n = len(source), len(target)\n    d = np.zeros((m + 1, n + 1), dtype=int)\n\n    # Initialize the base cases\n    d[:, 0] = np.arange(m + 1)  # Deletions to match an empty string\n    d[0, :] = np.arange(n + 1)  # Insertions to match an empty string\n\n    # Fill in the rest of the matrix using dynamic programming\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            # Cost of substitution or no operation if characters match\n            substitution_cost = 0 if source[i - 1] == target[j - 1] else 1\n            \n            # Calculate the minimum cost\n            d[i, j] = min(\n                d[i - 1, j] + 1,  # Deletion\n                d[i, j - 1] + 1,  # Insertion\n                d[i - 1, j - 1] + substitution_cost  # Substitution or no operation\n            )\n            \n            # Consider transposition if applicable\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                d[i, j] = min(d[i, j], d[i - 2, j - 2] + 1)\n\n    # The OSA distance is stored in the bottom-right cell of the matrix\n    return d[m, n]\n```\n\nThis code correctly implements the Optimal String Alignment distance calculation between two input strings `source` and `target`, considering insertions, deletions, substitutions, and transpositions of adjacent characters."}
{"task_id": 52, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Understanding Recall Metric**: Recall is a metric used in binary classification problems to measure the proportion of actual positive instances that are correctly identified by the model. It is calculated as the ratio of True Positives (TP) to the sum of True Positives and False Negatives (TP + FN), where TP represents the number of positive instances correctly predicted, and FN represents the number of positive instances incorrectly predicted as negative.\n\n2. **Function Requirements**: The function `recall(y_true, y_pred)` needs to take two lists as input: `y_true` containing the actual binary labels and `y_pred` containing the predicted binary labels. It should return the recall value rounded to three decimal places.\n\n3. **Handling Division by Zero**: The function must handle the case where the denominator (TP + FN) is zero to avoid division by zero errors. In such cases, the recall should be considered as 0.0.\n\n4. **Implementation**: To implement the recall function, we need to:\n   - Calculate True Positives (TP) and False Negatives (FN) from `y_true` and `y_pred`.\n   - Compute the recall using the formula: Recall = TP / (TP + FN).\n   - Handle the division by zero case.\n   - Round the result to three decimal places.\n\n# Fixed solution:\n```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric in a binary classification setting.\n\n    Parameters:\n    y_true (list): A list of true binary labels (0 or 1) for the dataset.\n    y_pred (list): A list of predicted binary labels (0 or 1) from the model.\n\n    Returns:\n    float: The recall value rounded to three decimal places.\n    \"\"\"\n    # Convert input lists to numpy arrays for easier computation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate True Positives (TP) and False Negatives (FN)\n    # TP is where y_true is 1 and y_pred is 1\n    # FN is where y_true is 1 but y_pred is 0\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n\n    # Calculate the denominator (TP + FN)\n    denominator = tp + fn\n\n    # Check for division by zero\n    if denominator == 0:\n        # If denominator is zero, recall is 0.0\n        recall_value = 0.0\n    else:\n        # Calculate recall\n        recall_value = tp / denominator\n\n    # Round the recall to three decimal places\n    recall_value = round(recall_value, 3)\n\n    return recall_value\n```\n\n# Explanation of changes:\n*   **Input Validation and Conversion**: The input lists `y_true` and `y_pred` are converted to numpy arrays to facilitate easier and more efficient computation.\n*   **Calculation of TP and FN**: True Positives (TP) and False Negatives (FN) are calculated using numpy's conditional sum operations.\n*   **Recall Calculation**: The recall is calculated using the formula TP / (TP + FN), with a check for division by zero.\n*   **Rounding the Result**: The final recall value is rounded to three decimal places before being returned.\n\n# Tests and example uses:\n```python\n# Example usage\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\nprint(recall(y_true, y_pred))  # Output: 0.714\n\n# Test case with all true positives and no false negatives\ny_true = [1, 1, 1, 1]\ny_pred = [1, 1, 1, 1]\nprint(recall(y_true, y_pred))  # Output: 1.0\n\n# Test case with no true positives\ny_true = [0, 0, 0, 0]\ny_pred = [0, 0, 1, 1]\nprint(recall(y_true, y_pred))  # Output: 0.0\n\n# Test case with division by zero (no actual positives)\ny_true = [0, 0, 0, 0]\ny_pred = [0, 0, 0, 0]\nprint(recall(y_true, y_pred))  # Output: 0.0\n```"}
{"task_id": 53, "completion_id": 0, "completion": "## Self-Attention Mechanism Implementation\n### Overview\n\nThe self-attention mechanism is a key component of transformer models. It enables the model to attend to different parts of the input sequence simultaneously and weigh their importance. Here, we implement the self-attention mechanism using Python and NumPy.\n\n### Code\n\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Computes the self-attention output for the given input X and weight matrices W_q, W_k, W_v.\n\n    Args:\n    X (numpy array): Input sequence.\n    W_q (numpy array): Weight matrix for query.\n    W_k (numpy array): Weight matrix for key.\n    W_v (numpy array): Weight matrix for value.\n\n    Returns:\n    list: Self-attention output as a list after reshaping.\n    \"\"\"\n\n    # Compute query, key, and value matrices\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n\n    # Compute attention scores\n    attention_scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n\n    # Compute attention weights\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n\n    # Compute self-attention output\n    self_attention_output = np.dot(attention_weights, V)\n\n    # Round results to the nearest 4th decimal\n    self_attention_output = np.round(self_attention_output, 4)\n\n    # Return self-attention output as a list after reshaping\n    return self_attention_output.reshape(-1).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define input sequence and weight matrices\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    W_q = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n    W_k = np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]])\n    W_v = np.array([[1.3, 1.4], [1.5, 1.6], [1.7, 1.8]])\n\n    # Compute self-attention output\n    output = self_attention(X, W_q, W_k, W_v)\n    print(output)\n```\n\n### Explanation\n\n1.  The function `self_attention` takes the input sequence `X` and weight matrices `W_q`, `W_k`, `W_v` as inputs.\n2.  It computes the query, key, and value matrices `Q`, `K`, `V` by multiplying `X` with the corresponding weight matrices.\n3.  The attention scores are calculated by taking the dot product of `Q` and `K.T` and scaling by the square root of the last dimension of `Q`.\n4.  The attention weights are computed by applying the softmax function to the attention scores.\n5.  The self-attention output is obtained by taking the dot product of the attention weights and `V`.\n6.  The output is rounded to the nearest 4th decimal using `np.round`.\n7.  Finally, the self-attention output is reshaped into a 1D array and returned as a list using the `tolist` method.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `self_attention` function with sample input sequence `X` and weight matrices `W_q`, `W_k`, `W_v`. The output is printed as a list."}
{"task_id": 54, "completion_id": 0, "completion": "### Implementing a Simple RNN Cell in Python\n#### Function Description\n\nThe `rnn_forward` function implements a simple Recurrent Neural Network (RNN) cell. It processes a sequence of input vectors and produces the final hidden state. The function uses the tanh activation function for the hidden state updates.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    This function implements a simple Recurrent Neural Network (RNN) cell.\n    \n    Args:\n    input_sequence (list[list[float]]): A sequence of input vectors.\n    initial_hidden_state (list[float]): The initial hidden state.\n    Wx (list[list[float]]): The weight matrix for input-to-hidden connections.\n    Wh (list[list[float]]): The weight matrix for hidden-to-hidden connections.\n    b (list[float]): The bias vector.\n    \n    Returns:\n    list[float]: The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n\n    # Convert input lists to numpy arrays for efficient matrix operations\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n\n    # Initialize the hidden state with the initial hidden state\n    hidden_state = initial_hidden_state\n\n    # Process each input vector in the sequence\n    for input_vector in input_sequence:\n        # Compute the new hidden state using the RNN update formula: tanh(Wx * x + Wh * h + b)\n        hidden_state = np.tanh(np.dot(Wx, input_vector) + np.dot(Wh, hidden_state) + b)\n\n    # Return the final hidden state, rounded to four decimal places\n    return np.round(hidden_state, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    input_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n    initial_hidden_state = [0.0, 0.0]\n    Wx = [[0.1, 0.2], [0.3, 0.4]]\n    Wh = [[0.5, 0.6], [0.7, 0.8]]\n    b = [0.1, 0.2]\n\n    final_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\n    print(\"Final Hidden State:\", final_hidden_state)\n```\n\n#### Explanation\n\n1. The function takes in the input sequence, initial hidden state, weight matrices `Wx` and `Wh`, and bias vector `b` as inputs.\n2. It converts the input lists to numpy arrays for efficient matrix operations.\n3. The function initializes the hidden state with the initial hidden state and processes each input vector in the sequence.\n4. For each input vector, it computes the new hidden state using the RNN update formula: `tanh(Wx * x + Wh * h + b)`.\n5. After processing the entire sequence, the function returns the final hidden state, rounded to four decimal places.\n\n#### Example Usage\n\nThe example usage demonstrates how to call the `rnn_forward` function with a sample input sequence, initial hidden state, weight matrices, and bias vector. The final hidden state is printed to the console."}
{"task_id": 55, "completion_id": 0, "completion": "## 2D Translation Matrix Implementation\n### Task Description\nThe task is to implement a function `translate_object` that applies a 2D translation matrix to a set of points. The function takes a list of 2D points and translation distances in the x and y directions as input and returns a new list of points after applying the translation.\n\n### Code Implementation\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Args:\n    points (list): A list of [x, y] coordinates.\n    tx (float): The translation distance in the x direction.\n    ty (float): The translation distance in the y direction.\n\n    Returns:\n    list: A new list of points after applying the translation matrix.\n    \"\"\"\n    # Convert the list of points to a numpy array for efficient computation\n    points_array = np.array(points)\n\n    # Create a translation matrix\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n\n    # Add a column of ones to the points array to enable matrix multiplication\n    homogeneous_points = np.hstack((points_array, np.ones((points_array.shape[0], 1))))\n\n    # Apply the translation matrix to the points\n    translated_points = np.dot(homogeneous_points, translation_matrix.T)\n\n    # Return the translated points as a list\n    return translated_points[:, :2].tolist()\n\n# Example usage\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)\n```\n\n### Explanation\n\n1. The function `translate_object` takes a list of 2D points `points`, and translation distances `tx` and `ty` as input.\n2. It converts the list of points to a numpy array `points_array` for efficient computation.\n3. A translation matrix `translation_matrix` is created using the given translation distances `tx` and `ty`.\n4. To enable matrix multiplication, a column of ones is added to `points_array` to form `homogeneous_points`.\n5. The translation matrix is applied to `homogeneous_points` using matrix multiplication, resulting in `translated_points`.\n6. Finally, the translated points are returned as a list after removing the homogeneous coordinate.\n\n### Output\nFor the example usage provided, the output will be:\n```python\n[[3, 5], [5, 7], [7, 9]]\n```"}
{"task_id": 56, "completion_id": 0, "completion": "## KL Divergence Between Two Normal Distributions\n\nThe Kullback-Leibler (KL) divergence is a measure of how one probability distribution differs from a second, reference probability distribution. Here, we will implement a function to calculate the KL divergence between two normal distributions.\n\n### KL Divergence Formula for Normal Distributions\n\nFor two normal distributions $P \\sim N(\\mu_P, \\sigma_P^2)$ and $Q \\sim N(\\mu_Q, \\sigma_Q^2)$, the KL divergence from $P$ to $Q$ is given by:\n\n\\[D_{KL}(P || Q) = \\log\\frac{\\sigma_Q}{\\sigma_P} + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2} - \\frac{1}{2}\\]\n\n### Implementation\n\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the KL divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first normal distribution.\n    sigma_p (float): Standard deviation of the first normal distribution.\n    mu_q (float): Mean of the second normal distribution.\n    sigma_q (float): Standard deviation of the second normal distribution.\n\n    Returns:\n    float: The KL divergence between the two normal distributions.\n\n    Raises:\n    ValueError: If sigma_p or sigma_q is non-positive.\n    \"\"\"\n\n    # Check if standard deviations are positive\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError(\"Standard deviations must be positive.\")\n\n    # Calculate the KL divergence using the formula\n    kl_div = np.log(sigma_q / sigma_p) + ((sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2)) - 0.5\n\n    return kl_div\n\n# Example usage\nif __name__ == \"__main__\":\n    mu_p = 0.0\n    sigma_p = 1.0\n    mu_q = 1.0\n    sigma_q = 2.0\n\n    kl_div = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\n    print(f\"KL Divergence: {kl_div}\")\n```\n\n### Explanation\n\n*   The function `kl_divergence_normal` takes the means and standard deviations of two normal distributions as input and returns their KL divergence.\n*   It first checks if the standard deviations are positive, raising a `ValueError` if not, since standard deviations must be positive.\n*   The KL divergence is then calculated using the formula for the KL divergence between two normal distributions.\n*   The result is returned as a floating-point number.\n*   An example usage is provided in the `if __name__ == \"__main__\":` block, demonstrating how to use the function with specific distributions."}
{"task_id": 57, "completion_id": 0, "completion": "### Gauss-Seidel Method Implementation\n#### Overview\n\nThe Gauss-Seidel method is an iterative technique used to solve a system of linear equations of the form `Ax = b`, where `A` is a square matrix, `x` is the solution vector, and `b` is the right-hand side vector. This implementation provides a function `gauss_seidel` that takes the matrix `A`, vector `b`, the number of iterations `n`, and an optional initial guess `x_ini` for the solution vector.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implements the Gauss-Seidel method to solve a system of linear equations Ax = b.\n\n    Args:\n    - A (numpy array): A square matrix of coefficients.\n    - b (numpy array): The right-hand side vector.\n    - n (int): The number of iterations.\n    - x_ini (numpy array, optional): Initial guess for the solution vector. Defaults to None.\n\n    Returns:\n    - list: The approximated solution vector after n iterations, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    # Check if A is a square matrix\n    assert A.shape[0] == A.shape[1], \"Matrix A must be square.\"\n    assert A.shape[0] == b.shape[0], \"Matrix A and vector b must have compatible dimensions.\"\n\n    # Initialize x with zeros or the provided initial guess\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = x_ini.astype(float)\n\n    # Perform Gauss-Seidel iterations\n    for _ in range(n):\n        x_new = x.copy()\n        for i in range(A.shape[0]):\n            s1 = sum(A[i, j] * x_new[j] for j in range(i))\n            s2 = sum(A[i, j] * x[j] for j in range(i + 1, A.shape[0]))\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n\n    # Round the result to the nearest 4th decimal and return as a list\n    return np.round(x, 4).reshape(-1, 1).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([[3, 2], [1, 4]])\n    b = np.array([5, 6])\n    n = 10\n\n    solution = gauss_seidel(A, b, n)\n    print(\"Solution:\", solution)\n```\n\n#### Explanation\n\n1.  The function `gauss_seidel` takes in the matrix `A`, vector `b`, number of iterations `n`, and an optional initial guess `x_ini`.\n2.  It first checks if `A` is a square matrix and if its dimensions are compatible with `b`.\n3.  If `x_ini` is not provided, it initializes the solution vector `x` with zeros; otherwise, it uses `x_ini` as the initial guess.\n4.  The function then performs `n` iterations of the Gauss-Seidel method. In each iteration, it updates the solution vector `x` using the most recent values available.\n5.  After `n` iterations, it rounds the solution vector to the nearest 4th decimal and returns it as a list.\n\n#### Example Usage\n\nThe example usage demonstrates how to use the `gauss_seidel` function to solve a system of linear equations. It defines a matrix `A` and a vector `b`, specifies the number of iterations `n`, and then calls `gauss_seidel` to obtain the approximated solution vector. The result is printed to the console."}
{"task_id": 58, "completion_id": 0, "completion": "## Gaussian Elimination for Solving Linear Systems\n### Task Description\nThe task is to implement the Gaussian Elimination method with partial pivoting to solve a system of linear equations `Ax = b`, where `A` is the coefficient matrix and `b` is the constant vector. The method transforms the system into an upper triangular matrix, which can then be solved using backward substitution.\n\n### Implementation\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Gaussian Elimination with partial pivoting.\n\n    Args:\n    A (numpy.ndarray): The coefficient matrix.\n    b (numpy.ndarray): The constant vector.\n\n    Returns:\n    list: The solution vector x.\n    \"\"\"\n\n    # Ensure A and b are numpy arrays\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n\n    # Check if A is a square matrix and b is a vector\n    assert A.shape[0] == A.shape[1], \"Matrix A must be square\"\n    assert A.shape[0] == b.shape[0], \"Matrix A and vector b must have compatible dimensions\"\n\n    # Get the number of rows (equations)\n    n = A.shape[0]\n\n    # Create an augmented matrix [A | b]\n    augmented_matrix = np.column_stack((A, b))\n\n    # Perform Gaussian Elimination with partial pivoting\n    for i in range(n - 1):\n        # Find the row with the maximum absolute value in the current column\n        max_row = np.argmax(np.abs(augmented_matrix[i:, i])) + i\n\n        # Swap the current row with the row having the maximum absolute value\n        augmented_matrix[[i, max_row]] = augmented_matrix[[max_row, i]]\n\n        # Make sure the pivot element is not zero\n        assert augmented_matrix[i, i] != 0, \"Matrix A is singular\"\n\n        # Eliminate the elements below the pivot\n        for j in range(i + 1, n):\n            factor = augmented_matrix[j, i] / augmented_matrix[i, i]\n            augmented_matrix[j] -= factor * augmented_matrix[i]\n\n    # Perform backward substitution to find the solution vector x\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (augmented_matrix[i, -1] - np.dot(augmented_matrix[i, i + 1:-1], x[i + 1:])) / augmented_matrix[i, i]\n\n    # Round the results to the nearest 4th decimal and return as a list\n    return np.round(x, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    A = [[3, 2], [1, 4]]\n    b = [5, 6]\n    print(gaussian_elimination(A, b))\n```\n\n### Explanation\n\n1.  The function `gaussian_elimination(A, b)` takes the coefficient matrix `A` and the constant vector `b` as input.\n2.  It first checks if `A` is a square matrix and if `A` and `b` have compatible dimensions.\n3.  It creates an augmented matrix `[A | b]` by column-stacking `A` and `b`.\n4.  The function then performs Gaussian Elimination with partial pivoting. In each iteration, it finds the row with the maximum absolute value in the current column, swaps it with the current row, and eliminates the elements below the pivot.\n5.  After transforming the augmented matrix into upper triangular form, it performs backward substitution to find the solution vector `x`.\n6.  Finally, it rounds the solution vector to the nearest 4th decimal and returns it as a list.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `gaussian_elimination` function to solve a system of linear equations. You can replace the matrix `A` and vector `b` with your own inputs to solve different systems."}
{"task_id": 59, "completion_id": 0, "completion": "Here's the implementation of the LSTM network as per the given task:\n\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        \"\"\"Sigmoid activation function\"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        \"\"\"Tanh activation function\"\"\"\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n\n        Args:\n        x (numpy array): Input sequence of shape (sequence_length, input_size, 1)\n        initial_hidden_state (numpy array): Initial hidden state of shape (hidden_size, 1)\n        initial_cell_state (numpy array): Initial cell state of shape (hidden_size, 1)\n\n        Returns:\n        hidden_states (list): List of hidden states at each time step\n        final_hidden_state (list): Final hidden state\n        final_cell_state (list): Final cell state\n        \"\"\"\n        sequence_length = x.shape[0]\n        hidden_states = []\n\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n\n        for t in range(sequence_length):\n            # Concatenate input and previous hidden state\n            x_t = x[t]\n            concat_input = np.concatenate((x_t, hidden_state), axis=0)\n\n            # Compute forget gate\n            forget_gate = self.sigmoid(np.dot(self.Wf, concat_input) + self.bf)\n\n            # Compute input gate\n            input_gate = self.sigmoid(np.dot(self.Wi, concat_input) + self.bi)\n\n            # Compute candidate cell state\n            candidate_cell_state = self.tanh(np.dot(self.Wc, concat_input) + self.bc)\n\n            # Update cell state\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell_state\n\n            # Compute output gate\n            output_gate = self.sigmoid(np.dot(self.Wo, concat_input) + self.bo)\n\n            # Update hidden state\n            hidden_state = output_gate * self.tanh(cell_state)\n\n            # Store hidden state\n            hidden_states.append(hidden_state)\n\n        # Get final hidden state and cell state\n        final_hidden_state = hidden_state\n        final_cell_state = cell_state\n\n        # Round results to 4 decimal places and convert to list\n        hidden_states = np.round(np.array(hidden_states), 4).reshape(-1, self.hidden_size).tolist()\n        final_hidden_state = np.round(final_hidden_state, 4).reshape(1, -1).tolist()[0]\n        final_cell_state = np.round(final_cell_state, 4).reshape(1, -1).tolist()[0]\n\n        return hidden_states, final_hidden_state, final_cell_state\n\n\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(0)  # For reproducibility\n\n    input_size = 3\n    hidden_size = 4\n    sequence_length = 5\n\n    lstm = LSTM(input_size, hidden_size)\n\n    # Generate random input sequence\n    x = np.random.randn(sequence_length, input_size, 1)\n\n    # Initialize hidden state and cell state\n    initial_hidden_state = np.zeros((hidden_size, 1))\n    initial_cell_state = np.zeros((hidden_size, 1))\n\n    hidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\n\n    print(\"Hidden States:\")\n    print(hidden_states)\n    print(\"Final Hidden State:\")\n    print(final_hidden_state)\n    print(\"Final Cell State:\")\n    print(final_cell_state)\n```\n\nThe provided code defines an LSTM class with the required methods. The `forward` method processes a sequence of inputs and returns the hidden states at each time step, as well as the final hidden state and cell state. The results are rounded to 4 decimal places and returned as Python lists.\n\n**Key aspects of the implementation:**\n\n1.  The LSTM class is initialized with random weights and zero biases.\n2.  The `forward` method processes a sequence of inputs using the LSTM equations.\n3.  The hidden states, final hidden state, and final cell state are computed and returned.\n4.  The results are rounded to 4 decimal places and converted to Python lists.\n\n**Example usage:**\n\nThe code includes an example usage section that demonstrates how to create an instance of the LSTM class, generate a random input sequence, and process it using the `forward` method. The hidden states, final hidden state, and final cell state are printed to the console."}
{"task_id": 60, "completion_id": 0, "completion": "## Implementing TF-IDF (Term Frequency-Inverse Document Frequency)\n### Task Description\nThe task is to implement a function `compute_tf_idf(corpus, query)` that computes the TF-IDF scores for a given query against a corpus of documents.\n\n### Code Implementation\n\n```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a given corpus of documents.\n\n    Args:\n    - corpus (list): A list of documents, where each document is a list of words.\n    - query (list): A list of words for which you want to compute the TF-IDF scores.\n\n    Returns:\n    - list: A list of lists containing the TF-IDF scores for the query words in each document, rounded to four decimal places.\n\n    Raises:\n    - ValueError: If the corpus is empty.\n    \"\"\"\n\n    # Check if the corpus is empty\n    if not corpus:\n        raise ValueError(\"Corpus cannot be empty\")\n\n    # Calculate the total number of documents in the corpus\n    num_docs = len(corpus)\n\n    # Initialize a dictionary to store the document frequency (DF) of each term\n    df = {}\n    for doc in corpus:\n        # Create a set of unique words in the document to avoid counting duplicates\n        unique_words = set(doc)\n        for word in unique_words:\n            df[word] = df.get(word, 0) + 1\n\n    # Initialize a list to store the TF-IDF scores for each document\n    tf_idf_scores = []\n\n    # Iterate over each document in the corpus\n    for doc in corpus:\n        # Calculate the term frequency (TF) of each word in the document\n        tf = {}\n        for word in doc:\n            tf[word] = tf.get(word, 0) + 1\n        max_tf = max(tf.values(), default=1)  # Avoid division by zero\n\n        # Initialize a list to store the TF-IDF scores for the query words in the current document\n        doc_tf_idf_scores = []\n\n        # Iterate over each word in the query\n        for word in query:\n            # Calculate the TF-IDF score for the word in the current document\n            word_tf = tf.get(word, 0) / max_tf if max_tf != 0 else 0\n            word_df = df.get(word, 0)\n            # Apply smoothing to avoid division by zero\n            word_idf = np.log((num_docs + 1) / (word_df + 1)) + 1\n            tf_idf_score = word_tf * word_idf\n            doc_tf_idf_scores.append(tf_idf_score)\n\n        # Append the TF-IDF scores for the query words in the current document to the result list\n        tf_idf_scores.append(doc_tf_idf_scores)\n\n    # Convert the TF-IDF scores to a numpy array, round to four decimal places, and then convert back to a list\n    tf_idf_scores = np.array(tf_idf_scores).round(4).tolist()\n\n    return tf_idf_scores\n\n# Example usage:\ncorpus = [\n    [\"this\", \"is\", \"a\", \"test\", \"document\"],\n    [\"this\", \"document\", \"is\", \"just\", \"a\", \"test\"],\n    [\"another\", \"test\", \"document\"]\n]\nquery = [\"test\", \"document\", \"example\"]\n\nprint(compute_tf_idf(corpus, query))\n```\n\n### Explanation\n\n1.  The function `compute_tf_idf(corpus, query)` takes a corpus of documents and a query as input and returns the TF-IDF scores for the query words in each document.\n2.  It first checks if the corpus is empty and raises a `ValueError` if it is.\n3.  It calculates the document frequency (DF) of each term in the corpus by iterating over each document and counting the number of documents containing each unique word.\n4.  Then, it iterates over each document in the corpus, calculates the term frequency (TF) of each word, and computes the TF-IDF score for each query word in the document.\n5.  The TF-IDF score is calculated as the product of the TF and IDF scores. The IDF score is computed using the formula `np.log((num_docs + 1) / (word_df + 1)) + 1`, which applies smoothing to avoid division by zero.\n6.  Finally, it rounds the TF-IDF scores to four decimal places, converts them to a list, and returns the result.\n\n### Advice\n\n*   The code handles edge cases such as an empty corpus, query terms not present in the corpus, and documents with no words.\n*   It uses smoothing in the IDF calculation to avoid division by zero errors.\n*   The TF-IDF scores are rounded to four decimal places as required.\n*   The code is well-structured, readable, and follows standard professional guidelines."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # First, we need to calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n    # TP is the count of correctly predicted positive labels\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    \n    # FP is the count of incorrectly predicted positive labels\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    \n    # FN is the count of incorrectly predicted negative labels (i.e., true positives that were missed)\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    \n    # Calculate Precision: TP / (TP + FP)\n    # Precision is the ratio of true positives to the sum of true positives and false positives\n    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n    \n    # Calculate Recall: TP / (TP + FN)\n    # Recall is the ratio of true positives to the sum of true positives and false negatives\n    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n    \n    # Calculate F-Score using the formula: (1 + beta^2) * (Precision * Recall) / (beta^2 * Precision + Recall)\n    # The F-Score is the weighted harmonic mean of Precision and Recall\n    f_score_value = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) if (precision + recall) != 0 else 0\n    \n    # Return the F-Score rounded to three decimal places\n    return round(f_score_value, 3)\n```\n\n### Explanation\n\n* The code starts by calculating `True Positives (TP)`, `False Positives (FP)`, and `False Negatives (FN)` using numpy's `logical_and` and `sum` functions. These values are essential for calculating Precision and Recall.\n* It then calculates `Precision` and `Recall` using the formulae `TP / (TP + FP)` and `TP / (TP + FN)` respectively. The code handles potential division by zero errors by checking if the denominators are zero before performing the division.\n* The `F-Score` is calculated using the formula `(1 + beta^2) * (Precision * Recall) / (beta^2 * Precision + Recall)`, which is the weighted harmonic mean of Precision and Recall. The result is rounded to three decimal places before being returned.\n* The function is well-documented with clear and concise comments explaining the purpose of each section of the code."}
{"task_id": 62, "completion_id": 0, "completion": "Here's a complete implementation of a Simple RNN with Backpropagation Through Time (BPTT):\n\n```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.W_xh = np.random.randn(hidden_size, input_size)*0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n        self.W_hy = np.random.randn(output_size, hidden_size)*0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Args:\n        input_sequence (numpy array): A 3D numpy array with shape (sequence_length, input_size, batch_size)\n        \n        Returns:\n        outputs (numpy array): A 3D numpy array with shape (sequence_length, output_size, batch_size)\n        last_inputs (numpy array): A 3D numpy array with shape (sequence_length, input_size + hidden_size, batch_size)\n        last_hiddens (numpy array): A 3D numpy array with shape (sequence_length + 1, hidden_size, batch_size)\n        \"\"\"\n        sequence_length = input_sequence.shape[0]\n        batch_size = input_sequence.shape[2]\n        outputs = np.zeros((sequence_length, self.output_size, batch_size))\n        last_inputs = np.zeros((sequence_length, self.input_size + self.hidden_size, batch_size))\n        last_hiddens = np.zeros((sequence_length + 1, self.hidden_size, batch_size))\n        \n        for t in range(sequence_length):\n            if t == 0:\n                hidden_state_prev = np.zeros((self.hidden_size, batch_size))\n            else:\n                hidden_state_prev = last_hiddens[t]\n            \n            x_t = input_sequence[t]\n            hidden_state = np.tanh(np.dot(self.W_xh, x_t) + np.dot(self.W_hh, hidden_state_prev) + self.b_h)\n            output = np.dot(self.W_hy, hidden_state) + self.b_y\n            \n            outputs[t] = output\n            last_inputs[t, :self.input_size, :] = x_t\n            last_inputs[t, self.input_size:, :] = hidden_state_prev\n            last_hiddens[t + 1] = hidden_state\n        \n        return outputs, last_inputs, last_hiddens\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs and expected outputs.\n        \n        Args:\n        input_sequence (numpy array): A 3D numpy array with shape (sequence_length, input_size, batch_size)\n        expected_output (numpy array): A 3D numpy array with shape (sequence_length, output_size, batch_size)\n        outputs (numpy array): A 3D numpy array with shape (sequence_length, output_size, batch_size)\n        last_inputs (numpy array): A 3D numpy array with shape (sequence_length, input_size + hidden_size, batch_size)\n        last_hiddens (numpy array): A 3D numpy array with shape (sequence_length + 1, hidden_size, batch_size)\n        learning_rate (float): The learning rate for weight updates\n        \n        Returns:\n        dW_xh, dW_hh, dW_hy, db_h, db_y (numpy arrays): The gradients of the loss with respect to the weights and biases\n        \"\"\"\n        sequence_length = input_sequence.shape[0]\n        batch_size = input_sequence.shape[2]\n        \n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        \n        dhidden_state_next = np.zeros((self.hidden_size, batch_size))\n        \n        for t in reversed(range(sequence_length)):\n            output_error = outputs[t] - expected_output[t]\n            doutput = output_error\n            \n            dW_hy += np.dot(doutput, last_hiddens[t + 1].T)\n            db_y += np.sum(doutput, axis=1, keepdims=True)\n            \n            dhidden_state = np.dot(self.W_hy.T, doutput) + dhidden_state_next\n            dhidden_state_raw = dhidden_state * (1 - last_hiddens[t + 1]**2)\n            \n            db_h += np.sum(dhidden_state_raw, axis=1, keepdims=True)\n            dW_xh += np.dot(dhidden_state_raw, last_inputs[t, :self.input_size, :].T)\n            dW_hh += np.dot(dhidden_state_raw, last_inputs[t, self.input_size:, :].T)\n            \n            dhidden_state_next = np.dot(self.W_hh.T, dhidden_state_raw)\n        \n        # Clip gradients to prevent exploding gradients\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        \n        # Update weights\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        \n        return dW_xh, dW_hh, dW_hy, db_h, db_y\n\n    def train(self, input_sequence, expected_output, learning_rate):\n        \"\"\"\n        Train the RNN on a given sequence of inputs and expected outputs.\n        \n        Args:\n        input_sequence (numpy array): A 3D numpy array with shape (sequence_length, input_size, batch_size)\n        expected_output (numpy array): A 3D numpy array with shape (sequence_length, output_size, batch_size)\n        learning_rate (float): The learning rate for weight updates\n        \"\"\"\n        outputs, last_inputs, last_hiddens = self.forward(input_sequence)\n        self.backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(0)\n    \n    # Define the RNN parameters\n    input_size = 3\n    hidden_size = 5\n    output_size = 2\n    sequence_length = 10\n    batch_size = 1\n    \n    # Initialize the RNN\n    rnn = SimpleRNN(input_size, hidden_size, output_size)\n    \n    # Generate a random input sequence\n    input_sequence = np.random.rand(sequence_length, input_size, batch_size)\n    \n    # Generate a random expected output sequence\n    expected_output = np.random.rand(sequence_length, output_size, batch_size)\n    \n    # Train the RNN\n    learning_rate = 0.01\n    rnn.train(input_sequence, expected_output, learning_rate)\n```\n\nThe provided code defines a SimpleRNN class that implements a basic Recurrent Neural Network (RNN) with backpropagation through time (BPTT). The RNN is trained on a sequence prediction task, where it learns to predict the next item in a sequence.\n\nHere's a step-by-step explanation of the code:\n\n1.  The `__init__` method initializes the RNN with random weights and zero biases.\n2.  The `forward` method performs a forward pass through the RNN for a given sequence of inputs. It returns the output, the last inputs, and the hidden states.\n3.  The `backward` method performs a backward pass through the RNN for a given sequence of inputs and expected outputs. It computes the gradients of the loss with respect to the weights and biases and updates the weights using the gradients and the learning rate.\n4.  The `train` method trains the RNN on a given sequence of inputs and expected outputs by calling the `forward` and `backward` methods.\n\nThe example usage demonstrates how to create an instance of the SimpleRNN class, generate random input and expected output sequences, and train the RNN using the `train` method.\n\n**Key aspects of the implementation:**\n\n*   The RNN is trained on a sequence prediction task.\n*   The loss function used is 1/2 \\* Mean Squared Error (MSE).\n*   The gradients are clipped to prevent exploding gradients.\n*   The weights are updated using the gradients and the learning rate.\n\n**Notes on the code:**\n\n*   The code uses NumPy for efficient numerical computations.\n*   The RNN is implemented using a simple tanh activation function for the hidden state.\n*   The output is computed using a linear layer.\n*   The gradients are computed using backpropagation through time (BPTT).\n\nThe code provides a basic implementation of a Simple RNN with BPTT and can be used as a starting point for more complex RNN architectures and tasks."}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    # Check if A is a numpy array and convert it if necessary\n    A = np.asarray(A)\n    \n    # Check if b is a numpy array and convert it if necessary\n    b = np.asarray(b)\n    \n    # Get the size of the matrix A\n    size = A.shape[0]\n    \n    # If x0 is not provided, initialize it as a zero vector\n    if x0 is None:\n        x = np.zeros(size)\n    else:\n        x = np.asarray(x0)\n    \n    # Calculate the initial residual r = b - Ax\n    r = b - np.dot(A, x)\n    \n    # Initialize the conjugate direction p = r\n    p = r.copy()\n    \n    # Initialize the squared norm of the residual\n    rsold = np.dot(r, r)\n    \n    # Perform the Conjugate Gradient iterations\n    for _ in range(n):\n        # Calculate Ap\n        Ap = np.dot(A, p)\n        \n        # Calculate the coefficient alpha = rsold / (p^T Ap)\n        alpha = rsold / np.dot(p, Ap)\n        \n        # Update the solution x = x + alpha * p\n        x = x + alpha * p\n        \n        # Update the residual r = r - alpha * Ap\n        r = r - alpha * Ap\n        \n        # Calculate the new squared norm of the residual\n        rsnew = np.dot(r, r)\n        \n        # Check for convergence\n        if np.sqrt(rsnew) < tol:\n            break\n        \n        # Update the conjugate direction p = r + (rsnew / rsold) * p\n        p = r + (rsnew / rsold) * p\n        \n        # Update rsold for the next iteration\n        rsold = rsnew\n    \n    # Return the solution vector rounded to 8 decimal places and converted to a list\n    return np.round(x, 8).tolist()\n```"}
{"task_id": 64, "completion_id": 0, "completion": "## Gini Impurity Calculation Implementation\n\n### Overview\n\nGini impurity is a measure used in decision tree algorithms to determine the purity of a node. It is calculated based on the probability of each class in the given dataset.\n\n### Implementation\n\n```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Convert the list to a numpy array for easier manipulation\n    y = np.array(y)\n    \n    # Get the unique classes and their respective counts\n    classes, counts = np.unique(y, return_counts=True)\n    \n    # Calculate the probability of each class\n    probabilities = counts / len(y)\n    \n    # Calculate Gini Impurity using the formula: 1 - \u03a3(p_i^2)\n    gini = 1 - np.sum(np.square(probabilities))\n    \n    # Return the Gini Impurity rounded to three decimal places\n    return round(gini, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    class_labels = [0, 0, 1, 1, 1, 0, 1, 0, 1, 1]\n    print(\"Gini Impurity:\", gini_impurity(class_labels))\n```\n\n### Explanation\n\n1. The function `gini_impurity` takes a list of class labels `y` as input.\n2. It converts the input list to a numpy array for easier manipulation.\n3. It calculates the unique classes and their respective counts using `np.unique`.\n4. The probability of each class is calculated by dividing the count of each class by the total number of elements.\n5. Gini impurity is calculated using the formula: `1 - \u03a3(p_i^2)`, where `p_i` is the probability of each class.\n6. The result is rounded to three decimal places before being returned.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `gini_impurity` function with a list of class labels. The output will be the Gini impurity of the given class labels, rounded to three decimal places."}
{"task_id": 65, "completion_id": 0, "completion": "# Problem Understanding\nThe problem requires converting a given dense matrix into the Compressed Row Sparse (CSR) format. The CSR format is an efficient storage representation for sparse matrices, storing only non-zero elements and their positions.\n\nKey constraints and edge cases:\n- The input is a 2D list representing a dense matrix.\n- The matrix can have any number of rows and columns.\n- The matrix elements can be any numbers (zero or non-zero).\n- The output should be a tuple containing three lists: values array, column indices array, and row pointer array.\n\nInput: A 2D list `dense_matrix`.\nOutput: A tuple containing three lists: \n- **Values array**: List of all non-zero elements in row-major order.\n- **Column indices array**: Column index for each non-zero element in the values array.\n- **Row pointer array**: Cumulative number of non-zero elements per row, indicating the start of each row in the values array.\n\nSubtle requirements or implicit constraints:\n- The CSR format should be implemented correctly, storing only non-zero elements.\n- The row pointer array should be correctly calculated, indicating the start of each row in the values array.\n- The output should be in the correct order (row-major order).\n\n# Key Insights\nCrucial observations that lead to the solution:\n- To convert a dense matrix to CSR format, we need to iterate over the dense matrix to identify non-zero elements and their positions.\n- We need to store the non-zero elements in a list (values array) in row-major order.\n- We need to store the column index for each non-zero element in another list (column indices array).\n- We need to keep track of the cumulative count of non-zero elements as we process each row to form the row pointer array.\n- The row pointer array should have a length equal to the number of rows in the dense matrix plus one, where the first element is 0, and the last element is the total number of non-zero elements.\n\nPatterns or properties being used:\n- The CSR format takes advantage of the sparsity of the matrix, storing only non-zero elements.\n\nKey algorithmic insights:\n- We need to iterate over the dense matrix row by row to maintain the correct order of elements in the CSR format.\n- For each non-zero element found, we append its value to the values array and its column index to the column indices array.\n- We update the row pointer array accordingly as we process each row.\n\nImportant realizations:\n- The row pointer array is not just a simple count of non-zero elements per row, but a cumulative count.\n- The row pointer array should be initialized with 0, and then updated as we process each row.\n\nCritical examples or test cases that led to insights:\n- A dense matrix with some zero and non-zero elements, such as [[1, 0, 2], [0, 3, 0], [4, 5, 6]].\n\n# Solution Strategy\nSteps to solve this problem:\n1. Initialize three empty lists: values array, column indices array, and row pointer array.\n2. Initialize the row pointer array with 0, which represents the start of the first row.\n3. Iterate over the dense matrix row by row.\n4. For each row, iterate over the elements in that row.\n5. For each non-zero element found, append its value to the values array and its column index to the column indices array.\n6. Update the row pointer array by incrementing the count of non-zero elements seen so far.\n7. After processing each row, update the row pointer array with the current count of non-zero elements.\n8. Return a tuple containing the values array, column indices array, and row pointer array.\n\nImportant optimizations or special cases to handle:\n- We should handle the case where the input dense matrix is empty.\n- We should handle the case where a row in the dense matrix is empty (i.e., all elements are zero).\n\nHere's the python solution:\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_ptr = [0]  # Initialize row_ptr with 0, representing the start of the first row\n    \n    for row in dense_matrix:\n        row_non_zero_count = 0\n        for col_idx, element in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_idx)\n                row_non_zero_count += 1\n        row_ptr.append(row_ptr[-1] + row_non_zero_count)\n    \n    return (values, column_indices, row_ptr)\n\n# Example usage\ndense_matrix = [[1, 0, 2], [0, 3, 0], [4, 5, 6]]\nvalues, column_indices, row_ptr = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values array:\", values)\nprint(\"Column indices array:\", column_indices)\nprint(\"Row pointer array:\", row_ptr)\n```"}
{"task_id": 66, "completion_id": 0, "completion": "## Step 1: Understand the Problem and the Formula for Orthogonal Projection\nThe orthogonal projection of a vector **v** onto a line defined by a vector **L** can be computed using the formula: $\\text{proj}_L(v) = \\frac{v \\cdot L}{L \\cdot L}L$, where $v \\cdot L$ represents the dot product of vectors **v** and **L**.\n\n## Step 2: Implement the Dot Product Calculation\nTo calculate the dot product of two vectors, we sum the products of corresponding entries. For vectors **v** = $(v_1, v_2, ..., v_n)$ and **L** = $(L_1, L_2, ..., L_n)$, the dot product $v \\cdot L = v_1L_1 + v_2L_2 + ... + v_nL_n$. This can be implemented in Python as `sum(a * b for a, b in zip(v, L))`.\n\n## Step 3: Calculate the Orthogonal Projection\nUsing the formula from Step 1, we need to calculate $\\frac{v \\cdot L}{L \\cdot L}$ and then multiply this scalar by **L** to get the projection vector.\n\n## Step 4: Implement the Orthogonal Projection Function\nWe will write the Python function `orthogonal_projection(v, L)` that takes two lists representing vectors **v** and **L**, computes the orthogonal projection of **v** onto **L**, and returns the result as a list rounded to three decimal places.\n\n## Step 5: Handle Potential Division by Zero\nWe need to check if **L** is a zero vector to avoid division by zero. If **L** is a zero vector, the function should return a zero vector or handle it according to the problem's requirements.\n\n## Step 6: Write the Python Code\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Calculate the dot product of v and L\n    dot_product_v_L = sum(a * b for a, b in zip(v, L))\n    \n    # Calculate the dot product of L with itself\n    dot_product_L_L = sum(i ** 2 for i in L)\n    \n    # Check for division by zero\n    if dot_product_L_L == 0:\n        return [0.0] * len(v)  # Return a zero vector if L is a zero vector\n    \n    # Calculate the scalar factor\n    scalar_factor = dot_product_v_L / dot_product_L_L\n    \n    # Calculate the projection vector\n    projection_vector = [scalar_factor * i for i in L]\n    \n    # Round the projection vector to three decimal places\n    rounded_projection_vector = [round(i, 3) for i in projection_vector]\n    \n    return rounded_projection_vector\n```\n\n## Step 7: Example Usage\nTo demonstrate the function's usage, we can project a vector **v** = $(1, 2, 3)$ onto **L** = $(4, 5, 6)$.\n```python\nv = [1, 2, 3]\nL = [4, 5, 6]\nprint(orthogonal_projection(v, L))\n```\n\nThe final answer is: \n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Calculate the dot product of v and L\n    dot_product_v_L = sum(a * b for a, b in zip(v, L))\n    \n    # Calculate the dot product of L with itself\n    dot_product_L_L = sum(i ** 2 for i in L)\n    \n    # Check for division by zero\n    if dot_product_L_L == 0:\n        return [0.0] * len(v)  # Return a zero vector if L is a zero vector\n    \n    # Calculate the scalar factor\n    scalar_factor = dot_product_v_L / dot_product_L_L\n    \n    # Calculate the projection vector\n    projection_vector = [scalar_factor * i for i in L]\n    \n    # Round the projection vector to three decimal places\n    rounded_projection_vector = [round(i, 3) for i in projection_vector]\n    \n    return rounded_projection_vector\n```"}
{"task_id": 67, "completion_id": 0, "completion": "# Problem Understanding\nThe task is to implement a function that converts a dense matrix into its Compressed Column Sparse (CSC) representation. The CSC format is efficient for storing matrices with a high number of zero elements.\n\nKey constraints and edge cases:\n- The input is a two-dimensional list representing the dense matrix.\n- The matrix can have any number of rows and columns.\n- The matrix elements can be any numeric values, with a mix of zero and non-zero elements.\n- The output should be a tuple of three lists: `values`, `row indices`, and `column pointer`.\n\nInput: A 2D list `dense_matrix`.\nOutput: A tuple of three lists: `values`, `row indices`, and `column pointer`.\n\nSubtle requirements or implicit constraints:\n- The `values` list should store non-zero elements in column-major order.\n- The `row indices` list should store the row indices corresponding to each value in the `values` list.\n- The `column pointer` list should indicate the starting index of each column in the `values` list.\n\n# Key Insights\nCrucial observations that lead to the solution:\n- To convert a dense matrix to CSC format, we need to iterate through the matrix in column-major order.\n- We need to store non-zero elements in the `values` list.\n- We need to store the row indices corresponding to each non-zero element in the `row indices` list.\n- We need to keep track of the starting index of each column in the `values` list using the `column pointer` list.\n- The `column pointer` list should have a length equal to the number of columns plus one, where the last element points to the index after the last element in `values`.\n\nPatterns or properties being used:\n- The CSC format takes advantage of the fact that sparse matrices have a majority of zero elements, allowing for efficient storage.\n\nKey algorithmic insights:\n- We iterate through the dense matrix column by column.\n- For each column, we iterate through the rows and identify non-zero elements.\n- We append non-zero elements to the `values` list and their corresponding row indices to the `row indices` list.\n- We update the `column pointer` list accordingly.\n\nImportant realizations:\n- The `column pointer` list is initialized with a length equal to the number of columns plus one, with the first element being 0.\n- The last element of the `column pointer` list is the length of the `values` list.\n\nCritical examples or test cases:\n- A sample dense matrix `[[1, 0, 2], [0, 3, 0], [4, 0, 5]]` is used to verify the implementation.\n\n# Solution Strategy\nSteps to solve this problem:\n1. Initialize the `values`, `row indices`, and `column pointer` lists.\n2. Iterate through the dense matrix column by column.\n3. For each column, iterate through the rows and identify non-zero elements.\n4. Append non-zero elements to the `values` list and their corresponding row indices to the `row indices` list.\n5. Update the `column pointer` list accordingly.\n6. Return the `values`, `row indices`, and `column pointer` lists as a tuple.\n\nImportant optimizations or special cases to handle:\n- We initialize the `column pointer` list with a length equal to the number of columns plus one to avoid resizing during iteration.\n- We update the `column pointer` list incrementally as we iterate through the columns.\n\nHere's the python solution:\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_ptr = [0] * (num_cols + 1)\n    \n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        col_ptr[col + 1] = len(values)\n    return (values, row_indices, col_ptr)\n\n# Example usage\ndense_matrix = [[1, 0, 2], [0, 3, 0], [4, 0, 5]]\nvalues, row_indices, col_ptr = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row Indices:\", row_indices)\nprint(\"Column Pointer:\", col_ptr)\n```"}
{"task_id": 68, "completion_id": 0, "completion": "## Step 1: Understand the Problem and the Task\nThe task requires implementing a function `matrix_image(A)` that calculates the column space (image) of a given matrix `A`. This involves finding the basis vectors that span the column space of `A`.\n\n## Step 2: Recall the Method to Find the Column Space\nTo find the column space of a matrix $A$, we need to identify the linearly independent columns of $A$. One way to do this is by transforming $A$ into its Reduced Row Echelon Form (RREF) or Row Echelon Form (REF) and then identifying the pivot columns.\n\n## Step 3: Choose the Appropriate Method for Finding Independent Columns\nWe will use the Row Echelon Form (REF) to identify the independent columns because it is sufficient for identifying pivot columns, and it is less computationally intensive than RREF.\n\n## Step 4: Implement the Function to Calculate the Column Space\nTo implement `matrix_image(A)`, we will:\n1. Convert the input matrix $A$ into a NumPy array for easier manipulation.\n2. Perform row operations to transform $A$ into its REF.\n3. Identify the pivot columns in the REF of $A$.\n4. Extract the corresponding columns from the original matrix $A$ as the basis vectors for the column space.\n\n## Step 5: Write the Code for the Function\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    # Convert A to a NumPy array\n    A = np.array(A, dtype=float)\n    \n    # Perform Gaussian elimination to get REF\n    num_rows, num_cols = A.shape\n    ref_A = A.copy()\n    pivot_cols = []\n    row = 0\n    for col in range(num_cols):\n        if row >= num_rows:\n            break\n        # Find the row with the largest absolute value in the current column\n        max_row = np.argmax(np.abs(ref_A[row:, col])) + row\n        if ref_A[max_row, col] == 0:\n            continue\n        # Swap the current row with the row having the maximum absolute value\n        ref_A[[row, max_row]] = ref_A[[max_row, row]]\n        # Normalize the pivot row\n        ref_A[row] /= ref_A[row, col]\n        # Eliminate the column below the pivot\n        for i in range(row + 1, num_rows):\n            ref_A[i] -= ref_A[i, col] * ref_A[row]\n        pivot_cols.append(col)\n        row += 1\n    \n    # Extract the basis vectors from the original matrix A\n    basis_vectors = A[:, pivot_cols]\n    \n    # Round the result to 8 decimal places and convert to list\n    return np.round(basis_vectors, 8).tolist()\n```\n\n## Step 6: Verify the Output Constraints\nThe function `matrix_image(A)` returns the basis vectors as a list, rounded to 8 decimal places, satisfying the output constraints.\n\nThe final answer is: \n```python\nimport numpy as np\n\ndef matrix_image(A):\n    # Convert A to a NumPy array\n    A = np.array(A, dtype=float)\n    \n    # Perform Gaussian elimination to get REF\n    num_rows, num_cols = A.shape\n    ref_A = A.copy()\n    pivot_cols = []\n    row = 0\n    for col in range(num_cols):\n        if row >= num_rows:\n            break\n        # Find the row with the largest absolute value in the current column\n        max_row = np.argmax(np.abs(ref_A[row:, col])) + row\n        if ref_A[max_row, col] == 0:\n            continue\n        # Swap the current row with the row having the maximum absolute value\n        ref_A[[row, max_row]] = ref_A[[max_row, row]]\n        # Normalize the pivot row\n        ref_A[row] /= ref_A[row, col]\n        # Eliminate the column below the pivot\n        for i in range(row + 1, num_rows):\n            ref_A[i] -= ref_A[i, col] * ref_A[row]\n        pivot_cols.append(col)\n        row += 1\n    \n    # Extract the basis vectors from the original matrix A\n    basis_vectors = A[:, pivot_cols]\n    \n    # Round the result to 8 decimal places and convert to list\n    return np.round(basis_vectors, 8).tolist()\n```"}
{"task_id": 69, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Understanding R-squared**: R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It's calculated using the formula: $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$, where $SS_{res}$ is the sum of the squared residuals and $SS_{tot}$ is the total sum of squares.\n\n2. **Calculating $SS_{res}$ and $SS_{tot}$**: To calculate $SS_{res}$, we need to find the sum of the squared differences between the actual values (`y_true`) and the predicted values (`y_pred`). For $SS_{tot}$, we calculate the sum of the squared differences between the actual values (`y_true`) and their mean.\n\n3. **Implementing the R-squared formula**: We will use NumPy to efficiently compute the necessary sums and means.\n\n# Fixed solution:\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for regression analysis.\n\n    Parameters:\n    y_true (array-like): Array of true values.\n    y_pred (array-like): Array of predicted values.\n\n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorized operations\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate the mean of y_true\n    mean_y_true = np.mean(y_true)\n\n    # Calculate the total sum of squares (SS_tot)\n    ss_tot = np.sum((y_true - mean_y_true) ** 2)\n\n    # Check if ss_tot is zero to avoid division by zero\n    if ss_tot == 0:\n        return np.nan  # or any other value that makes sense for your use case\n\n    # Calculate the sum of the squared residuals (SS_res)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n\n    # Calculate R-squared\n    r2 = 1 - (ss_res / ss_tot)\n\n    # Return R-squared rounded to three decimal places\n    return round(r2, 3)\n```\n\n# Explanation of changes:\n* We first convert the input arrays `y_true` and `y_pred` to NumPy arrays to leverage vectorized operations for efficient computation.\n* We calculate the mean of `y_true` using `np.mean(y_true)`.\n* We compute $SS_{tot}$ and $SS_{res}$ using NumPy's vectorized operations.\n* We check if $SS_{tot}$ is zero to avoid division by zero when calculating R-squared.\n* Finally, we return the R-squared value rounded to three decimal places.\n\n# Tests and example uses:\n```python\n# Example usage\ny_true = [1, 2, 3, 4, 5]\ny_pred = [1.1, 1.9, 3.2, 4.1, 5.0]\nprint(r_squared(y_true, y_pred))\n\n# Edge case: perfect prediction\ny_true = [1, 2, 3, 4, 5]\ny_pred = [1, 2, 3, 4, 5]\nprint(r_squared(y_true, y_pred))  # Expected output: 1.0\n\n# Edge case: constant y_true\ny_true = [5, 5, 5, 5, 5]\ny_pred = [1, 2, 3, 4, 5]\nprint(r_squared(y_true, y_pred))  # Expected output: nan\n```"}
{"task_id": 70, "completion_id": 0, "completion": "# Problem Understanding\nThe task is to implement a function `calculate_brightness(img)` that calculates the average brightness of a grayscale image represented as a 2D matrix. The image is given as a 2D matrix where each element is a pixel value between 0 (black) and 255 (white).\n\nKey constraints and edge cases:\n- The input image is a 2D matrix.\n- Each pixel value is between 0 and 255.\n- The image matrix could be empty.\n- The rows in the matrix might have inconsistent lengths.\n- Pixel values might be outside the valid range (0-255).\n\nInput: A 2D matrix representing the grayscale image.\nOutput: The average brightness of the image rounded to two decimal places, or -1 if any edge case is met.\n\nSubtle requirements or implicit constraints:\n- The function should handle edge cases and return -1 for any of them.\n- The average brightness should be rounded to two decimal places.\n\n# Key Insights\nCrucial observations that lead to the solution:\n- To calculate the average brightness, we need to sum all pixel values and divide by the total number of pixels.\n- We need to check if the image matrix is empty.\n- We must verify that all rows in the matrix have the same length.\n- We have to ensure that all pixel values are within the valid range (0-255).\n- If any edge case is met, the function should return -1.\n\nPatterns or properties being used:\n- The average brightness is calculated by summing all pixel values and dividing by the total count of pixels.\n\nKey algorithmic insights:\n- Iterate through the 2D matrix to sum pixel values and check for edge cases.\n- Check the length of the rows in the matrix for consistency.\n- Validate each pixel value to be within the range 0-255.\n\nImportant realizations:\n- We need to count the total number of pixels to calculate the average brightness.\n- We should return -1 for any edge case.\n\nCritical examples or test cases:\n- An example with a valid image: `img = [[0, 128, 255], [0, 128, 255], [0, 128, 255]]`.\n- Edge case: Empty image matrix `img = []`.\n- Edge case: Inconsistent row lengths `img = [[0, 128], [0, 128, 255]]`.\n- Edge case: Pixel value out of range `img = [[0, 128, 256], [0, 128, 255]]`.\n\n# Solution Strategy\nSteps to solve this problem:\n1. Check if the image matrix `img` is empty. If so, return -1.\n2. Verify that all rows in `img` have the same length. If not, return -1.\n3. Iterate through each pixel in `img` to sum all pixel values and count the total number of pixels. Also, check that each pixel value is within the valid range (0-255). If any pixel value is out of range, return -1.\n4. Calculate the average brightness by dividing the sum of pixel values by the total number of pixels.\n5. Round the average brightness to two decimal places.\n6. Return the rounded average brightness.\n\nHere's the python solution:\n```python\ndef calculate_brightness(img):\n    # Check if the image matrix is empty\n    if not img:\n        return -1\n    \n    # Check if rows have inconsistent lengths\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n    \n    total_brightness = 0\n    total_pixels = 0\n    \n    # Iterate over each pixel in the image\n    for row in img:\n        for pixel in row:\n            # Check if pixel value is within valid range\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    \n    # Calculate average brightness\n    if total_pixels == 0:\n        return -1\n    else:\n        average_brightness = total_brightness / total_pixels\n        return round(average_brightness, 2)\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Parameters:\n    y_true (array-like): Actual values.\n    y_pred (array-like): Predicted values.\n\n    Returns:\n    float: The RMSE value rounded to three decimal places.\n\n    Raises:\n    ValueError: If the input arrays have mismatched shapes or are empty.\n    TypeError: If the input arrays are not of a valid type.\n    \"\"\"\n\n    # Check if inputs are of valid types\n    if not isinstance(y_true, (list, np.ndarray)) or not isinstance(y_pred, (list, np.ndarray)):\n        raise TypeError(\"Inputs must be of type list or numpy.ndarray\")\n\n    # Convert inputs to numpy arrays for easier manipulation\n    try:\n        y_true = np.array(y_true, dtype=float)\n        y_pred = np.array(y_pred, dtype=float)\n    except ValueError:\n        raise TypeError(\"Inputs must be convertible to numpy arrays of dtype float\")\n\n    # Check for empty arrays\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"Input arrays cannot be empty\")\n\n    # Check for mismatched array shapes\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape\")\n\n    # Calculate the difference between actual and predicted values\n    residuals = y_true - y_pred\n\n    # Calculate the sum of squared residuals\n    sum_squared_residuals = np.sum(residuals ** 2)\n\n    # Calculate the mean of the squared residuals\n    mean_squared_residuals = sum_squared_residuals / len(y_true)\n\n    # Calculate the RMSE\n    rmse_value = np.sqrt(mean_squared_residuals)\n\n    # Return the RMSE rounded to three decimal places\n    return round(rmse_value, 3)\n```"}
{"task_id": 72, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Understanding the Jaccard Index**: The Jaccard Index is a measure used to compare the similarity between two sets by dividing the size of their intersection by the size of their union.\n2. **Binary Classification Context**: In the context of binary classification, the Jaccard Index is used to evaluate the similarity between the true labels (`y_true`) and the predicted labels (`y_pred`).\n3. **Handling Edge Cases**: The function needs to handle cases where there is no overlap between `y_true` and `y_pred`, as well as cases where both arrays contain only zeros.\n4. **Implementation Requirements**: The function `jaccard_index(y_true, y_pred)` should calculate the Jaccard Index and return it as a float value rounded to three decimal places.\n\n# Fixed solution:\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n\n    Returns:\n    float: The Jaccard Index rounded to three decimal places.\n    \"\"\"\n    \n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Check if inputs have the same shape\n    assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n    \n    # Calculate the intersection (element-wise AND operation)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    \n    # Calculate the union (element-wise OR operation)\n    union = np.logical_or(y_true, y_pred).sum()\n    \n    # Handle the case where the union is zero to avoid division by zero\n    if union == 0:\n        return 0.0\n    \n    # Calculate the Jaccard Index\n    jaccard = intersection / union\n    \n    # Return the Jaccard Index rounded to three decimal places\n    return round(jaccard, 3)\n```\n\n# Explanation of changes:\n*   **Input Validation**: Added a check to ensure `y_true` and `y_pred` have the same shape.\n*   **Intersection and Union Calculation**: Used NumPy's `logical_and` and `logical_or` functions to calculate the intersection and union of `y_true` and `y_pred`.\n*   **Handling Division by Zero**: Added a condition to return 0 when the union is zero, avoiding division by zero.\n*   **Rounding the Result**: Rounded the Jaccard Index to three decimal places before returning it.\n\n# Tests and example uses:\n```python\n# Example usage\ny_true = np.array([0, 1, 1, 0, 1, 0, 0, 1])\ny_pred = np.array([0, 1, 0, 0, 1, 1, 0, 1])\nprint(jaccard_index(y_true, y_pred))  # Output: 0.667\n\n# Test case with no overlap\ny_true = np.array([1, 1, 1, 1])\ny_pred = np.array([0, 0, 0, 0])\nprint(jaccard_index(y_true, y_pred))  # Output: 0.0\n\n# Test case with both arrays containing only zeros\ny_true = np.array([0, 0, 0, 0])\ny_pred = np.array([0, 0, 0, 0])\nprint(jaccard_index(y_true, y_pred))  # Output: 0.0\n\n# Test case with perfect overlap\ny_true = np.array([1, 1, 0, 0, 1])\ny_pred = np.array([1, 1, 0, 0, 1])\nprint(jaccard_index(y_true, y_pred))  # Output: 1.0\n```"}
{"task_id": 73, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Understanding the task**: The task is to implement a function `dice_score(y_true, y_pred)` that calculates the Dice Score between two binary arrays `y_true` and `y_pred`, representing true and predicted labels in a binary classification problem.\n2. **Defining the Dice Score**: The Dice Score is a measure of similarity between two sets, calculated as twice the number of elements in their intersection divided by the sum of the number of elements in each set.\n3. **Handling edge cases**: We need to handle cases where there are no true or predicted positives to avoid division by zero.\n4. **Implementing the calculation**: We will use NumPy to efficiently compute the required quantities.\n\n# Fixed solution:\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Ground truth binary labels.\n    y_pred (numpy.ndarray): Predicted binary labels.\n\n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    # Ensure inputs are NumPy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Check if inputs are binary\n    if not np.all(np.logical_or(y_true == 0, y_true == 1)) or not np.all(np.logical_or(y_pred == 0, y_pred == 1)):\n        raise ValueError(\"Inputs must be binary arrays\")\n\n    # Calculate the number of true positives (intersection)\n    true_positives = np.sum(y_true * y_pred)\n\n    # Calculate the sum of the number of elements in y_true and y_pred\n    sum_of_elements = np.sum(y_true) + np.sum(y_pred)\n\n    # Handle edge case where sum_of_elements is zero to avoid division by zero\n    if sum_of_elements == 0:\n        return 1.0  # Both arrays are empty or contain only zeros, so they are considered identical\n\n    # Calculate the Dice Score\n    dice = (2.0 * true_positives) / sum_of_elements\n\n    # Return the Dice Score rounded to 3 decimal places\n    return round(dice, 3)\n```\n\n# Explanation of changes:\n*   **Input validation**: We added checks to ensure that the inputs are binary arrays.\n*   **Handling edge cases**: We handle the case where `sum_of_elements` is zero to avoid division by zero, returning 1.0 as both arrays are considered identical in this case.\n*   **Efficient computation**: We used NumPy to efficiently compute the required quantities.\n*   **Documentation**: We added a docstring to explain the function's purpose, parameters, and return value.\n\n# Tests and example uses:\n```python\n# Example usage\ny_true = np.array([0, 1, 1, 0, 1])\ny_pred = np.array([0, 1, 0, 0, 1])\nprint(dice_score(y_true, y_pred))  # Output: 0.8\n\n# Test case with no true or predicted positives\ny_true = np.array([0, 0, 0, 0])\ny_pred = np.array([0, 0, 0, 0])\nprint(dice_score(y_true, y_pred))  # Output: 1.0\n\n# Test case with non-binary input\ntry:\n    y_true = np.array([0, 2, 1, 0])\n    y_pred = np.array([0, 1, 0, 0])\n    print(dice_score(y_true, y_pred))\nexcept ValueError as e:\n    print(e)  # Output: Inputs must be binary arrays\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    This function generates a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    dim (int): The dimensionality of the hypervectors.\n    random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    \n    Returns:\n    list: A composite hypervector representing the entire row.\n    \"\"\"\n\n    # Initialize the composite hypervector with zeros\n    composite_hv = np.zeros(dim)\n    \n    # Iterate over each feature in the row\n    for feature, value in row.items():\n        # Set the seed for the random number generator to ensure reproducibility\n        np.random.seed(random_seeds[feature])\n        \n        # Generate a random hypervector for the feature name\n        feature_hv = np.random.choice([-1, 1], size=dim)\n        \n        # Generate a random hypervector for the feature value using the same seed\n        # This ensures that the same value will always result in the same hypervector\n        value_hv = np.random.choice([-1, 1], size=dim)\n        \n        # Bind the feature name hypervector and the feature value hypervector\n        # In HDC, binding is typically done using element-wise multiplication\n        bound_hv = feature_hv * value_hv\n        \n        # Bundle the bound hypervector to the composite hypervector\n        # In HDC, bundling is typically done using element-wise addition\n        composite_hv += bound_hv\n    \n    # After bundling all feature hypervectors, threshold the composite hypervector to obtain a binary hypervector\n    # This is a common step in HDC to ensure the resulting hypervector is binary\n    composite_hv = np.where(composite_hv > 0, 1, -1)\n    \n    # Return the composite hypervector as a list\n    return composite_hv.tolist()\n```"}
{"task_id": 75, "completion_id": 0, "completion": "# Problem Understanding\nThe task is to generate a confusion matrix for a binary classification problem. The input is a list of lists, where each inner list contains a pair of actual label (`y_true`) and predicted label (`y_pred`) for one observation. The output is a 2x2 confusion matrix represented as a list of lists.\n\nKey constraints and edge cases:\n- The input data is a list of lists.\n- Each inner list contains a pair of actual and predicted labels.\n- The labels are binary (0 or 1).\n- The output is a 2x2 matrix.\n- The input data can be empty.\n\nInput: A list of lists, where each inner list represents a pair `[y_true, y_pred]`.\nOutput: A 2x2 confusion matrix represented as a list of lists.\n\nSubtle requirements or implicit constraints:\n- The actual and predicted labels are binary (0 or 1).\n- The confusion matrix should be able to handle cases where the input data is empty.\n\n# Key Insights\nCrucial observations that lead to the solution:\n- A confusion matrix for binary classification is a 2x2 matrix that summarizes the prediction results against the actual outcomes.\n- The matrix counts the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n- TP are cases where `y_true` = 1 and `y_pred` = 1.\n- TN are cases where `y_true` = 0 and `y_pred` = 0.\n- FP are cases where `y_true` = 0 and `y_pred` = 1.\n- FN are cases where `y_true` = 1 and `y_pred` = 0.\n- The counts of TP, TN, FP, and FN can be directly computed by iterating through the input data.\n\nPatterns or properties being used:\n- The definition of a confusion matrix for binary classification.\n\nKey algorithmic insights:\n- Initialize counters for TP, TN, FP, and FN to 0.\n- Iterate through the input data to count TP, TN, FP, and FN.\n- Construct the 2x2 confusion matrix using the counts of TP, TN, FP, and FN.\n\nImportant realizations:\n- The counts of TP, TN, FP, and FN can be computed in a single pass through the input data.\n- The confusion matrix can be constructed directly from these counts.\n\nCritical examples or test cases:\n- Example input data: `[[1, 1], [0, 0], [1, 0], [0, 1]]`.\n- Expected output for the example: `[[1, 1], [1, 1]]`.\n\n# Solution Strategy\nSteps to solve this problem:\n1. Initialize counters for TP, TN, FP, and FN to 0.\n2. Iterate through the input data. For each pair of `y_true` and `y_pred`, update the corresponding counter based on the conditions for TP, TN, FP, and FN.\n3. Construct the 2x2 confusion matrix using the counts of TP, TN, FP, and FN. The matrix should be in the format `[[TN, FP], [FN, TP]]`.\n\nImportant optimizations or special cases to handle:\n- Handle the case where the input data is empty. In this case, the counts of TP, TN, FP, and FN will remain 0, and the resulting confusion matrix will be `[[0, 0], [0, 0]]`.\n\nHere's the python solution:\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    tn, fp, fn, tp = 0, 0, 0, 0\n    for y_true, y_pred in data:\n        if y_true == 1 and y_pred == 1:\n            tp += 1\n        elif y_true == 0 and y_pred == 0:\n            tn += 1\n        elif y_true == 1 and y_pred == 0:\n            fn += 1\n        elif y_true == 0 and y_pred == 1:\n            fp += 1\n    return [[tn, fp], [fn, tp]]\n```"}
{"task_id": 76, "completion_id": 0, "completion": "# Problem Understanding\nThe task is to implement a function `cosine_similarity(v1, v2)` that calculates the cosine similarity between two input vectors `v1` and `v2`. The cosine similarity measures the cosine of the angle between two vectors, indicating their directional similarity.\n\nKey constraints and edge cases:\n- Both input vectors `v1` and `v2` must have the same shape.\n- Input vectors cannot be empty.\n- Input vectors cannot have zero magnitude.\n\nInput: Two numpy arrays `v1` and `v2` representing the input vectors.\nOutput: A float representing the cosine similarity between `v1` and `v2`, rounded to three decimal places.\n\nSubtle requirements or implicit constraints:\n- The input vectors are represented as numpy arrays.\n- The cosine similarity should be calculated using the dot product and magnitudes of the input vectors.\n\n# Key Insights\nCrucial observations that lead to the solution:\n- The cosine similarity between two vectors can be calculated using the formula: `cosine_similarity = (v1 \u00b7 v2) / (|v1| * |v2|)`, where `\u00b7` denotes the dot product and `|v|` denotes the magnitude (or Euclidean norm) of vector `v`.\n- The dot product `v1 \u00b7 v2` can be computed using `np.dot(v1, v2)`.\n- The magnitude of a vector `v` can be computed using `np.linalg.norm(v)`.\n- The input vectors must have the same shape for the cosine similarity to be defined.\n- The input vectors cannot be empty or have zero magnitude, as this would result in an undefined cosine similarity.\n\nImportant realizations:\n- The cosine similarity is a measure of directional similarity between two vectors.\n- The formula for cosine similarity involves the dot product and magnitudes of the input vectors.\n\nCritical examples or test cases:\n- `v1 = np.array([1, 2, 3])` and `v2 = np.array([4, 5, 6])` is used to illustrate the calculation of cosine similarity.\n\n# Solution Strategy\nSteps to solve this problem:\n1. Check if the input vectors `v1` and `v2` have the same shape. If not, raise a `ValueError`.\n2. Check if either of the input vectors is empty. If so, raise a `ValueError`.\n3. Calculate the dot product of `v1` and `v2` using `np.dot(v1, v2)`.\n4. Calculate the magnitudes of `v1` and `v2` using `np.linalg.norm(v1)` and `np.linalg.norm(v2)`, respectively.\n5. Check if either of the magnitudes is zero. If so, raise a `ValueError`.\n6. Calculate the cosine similarity using the formula: `cosine_similarity = (v1 \u00b7 v2) / (|v1| * |v2|)`.\n7. Round the calculated cosine similarity to three decimal places.\n8. Return the rounded cosine similarity as a float.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    This function calculates the cosine similarity between two vectors.\n    \n    Parameters:\n    v1 (numpy array): The first vector.\n    v2 (numpy array): The second vector.\n    \n    Returns:\n    float: The cosine similarity between v1 and v2, rounded to three decimal places.\n    \"\"\"\n    # Check if the input vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape\")\n    \n    # Check if the input vectors are not empty\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError(\"Input vectors cannot be empty\")\n    \n    # Calculate the dot product of v1 and v2\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate the magnitudes of v1 and v2\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    \n    # Check if either magnitude is zero to avoid division by zero\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude\")\n    \n    # Calculate the cosine similarity\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    \n    # Round the result to three decimal places\n    return round(cosine_sim, 3)\n```"}
{"task_id": 77, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Understanding the task**: The task requires implementing a function `performance_metrics` that calculates various performance metrics for a binary classification model. The function takes two lists, `actual` and `predicted`, representing the actual and predicted class labels, respectively.\n2. **Checking the input constraints**: The function needs to check if all elements in `actual` and `predicted` are either 0 or 1 and if both lists have the same length.\n3. **Calculating the confusion matrix**: The confusion matrix is a 2x2 matrix that summarizes the predictions against the actual outcomes. It can be calculated by counting the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n4. **Computing the performance metrics**: The required performance metrics are accuracy, F1 score, specificity, and negative predictive value. These can be calculated using the values from the confusion matrix.\n   - **Accuracy** = (TP + TN) / (TP + TN + FP + FN)\n   - **F1 Score** = 2*TP / (2*TP + FP + FN)\n   - **Specificity** = TN / (TN + FP)\n   - **Negative Predictive Value** = TN / (TN + FN)\n5. **Returning the results**: The function should return a tuple containing the confusion matrix and the calculated performance metrics, with the latter rounded to three decimal places.\n\n# Fixed solution:\n```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    This function calculates various performance metrics for a binary classification model.\n\n    Args:\n    actual (list[int]): The actual class labels (1 for positive, 0 for negative).\n    predicted (list[int]): The predicted class labels from the model.\n\n    Returns:\n    tuple: A tuple containing the confusion matrix, accuracy, F1 score, specificity, and negative predictive value.\n    \"\"\"\n\n    # Check if both lists have the same length\n    if len(actual) != len(predicted):\n        raise ValueError(\"Both 'actual' and 'predicted' lists must have the same length.\")\n\n    # Check if all elements in the lists are either 0 or 1\n    if not all(i in [0, 1] for i in actual) or not all(i in [0, 1] for i in predicted):\n        raise ValueError(\"All elements in 'actual' and 'predicted' lists must be either 0 or 1.\")\n\n    # Calculate the counts of true positives, true negatives, false positives, and false negatives\n    counts = Counter(zip(actual, predicted))\n    tn = counts[(0, 0)]\n    fp = counts[(0, 1)]\n    fn = counts[(1, 0)]\n    tp = counts[(1, 1)]\n\n    # Calculate the confusion matrix\n    confusion_matrix = [[tn, fp], [fn, tp]]\n\n    # Calculate the performance metrics\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3) if (tp + tn + fp + fn) != 0 else 0\n    f1_score = round(2*tp / (2*tp + fp + fn), 3) if (2*tp + fp + fn) != 0 else 0\n    specificity = round(tn / (tn + fp), 3) if (tn + fp) != 0 else 0\n    negative_predictive_value = round(tn / (tn + fn), 3) if (tn + fn) != 0 else 0\n\n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n```\n\n# Explanation of changes:\n*   **Added input validation**: The function now checks if the input lists `actual` and `predicted` have the same length and contain only 0s and 1s.\n*   **Used `Counter` to calculate TP, TN, FP, and FN**: The `Counter` class from the `collections` module is used to count the occurrences of each pair of actual and predicted labels, making it easier to calculate the confusion matrix.\n*   **Calculated the confusion matrix and performance metrics**: The function now correctly calculates the confusion matrix and the required performance metrics using the counts of TP, TN, FP, and FN.\n*   **Rounded the performance metrics to three decimal places**: The performance metrics are rounded to three decimal places as required.\n\n# Tests and example uses:\n```python\n# Example usage\nactual = [1, 0, 1, 1, 0, 0, 1, 0, 1, 1]\npredicted = [1, 1, 1, 0, 0, 0, 1, 0, 1, 1]\nconfusion_matrix, accuracy, f1_score, specificity, negative_predictive_value = performance_metrics(actual, predicted)\nprint(\"Confusion Matrix:\", confusion_matrix)\nprint(\"Accuracy:\", accuracy)\nprint(\"F1 Score:\", f1_score)\nprint(\"Specificity:\", specificity)\nprint(\"Negative Predictive Value:\", negative_predictive_value)\n\n# Test case with invalid input length\ntry:\n    actual = [1, 0, 1]\n    predicted = [1, 1, 1, 0]\n    performance_metrics(actual, predicted)\nexcept ValueError as e:\n    print(e)\n\n# Test case with invalid input values\ntry:\n    actual = [1, 0, 2]\n    predicted = [1, 1, 1]\n    performance_metrics(actual, predicted)\nexcept ValueError as e:\n    print(e)\n```"}
{"task_id": 78, "completion_id": 0, "completion": "Here's a Python function that fulfills your requirements. This function takes a list or NumPy array of numerical values, calculates various descriptive statistics, and returns them in a dictionary.\n\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n\n    Args:\n    - data (list or np.ndarray): A list or NumPy array of numerical values.\n\n    Returns:\n    - dict: A dictionary containing mean, median, mode, variance, standard deviation,\n            percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n\n    # Ensure data is a NumPy array for easier calculations\n    data = np.array(data)\n\n    # Check if data is empty\n    if len(data) == 0:\n        raise ValueError(\"Input data is empty\")\n\n    # Calculate mean\n    mean = np.mean(data)\n\n    # Calculate median\n    median = np.median(data)\n\n    # Calculate mode\n    mode = stats.mode(data)[0][0]\n\n    # Calculate variance\n    variance = np.var(data)\n\n    # Calculate standard deviation\n    standard_deviation = np.std(data)\n\n    # Calculate percentiles\n    percentiles = np.percentile(data, [25, 50, 75])\n\n    # Calculate interquartile range (IQR)\n    interquartile_range = percentiles[2] - percentiles[0]\n\n    # Create a dictionary with the calculated statistics\n    stats_dict = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,  # Mode is drawn from the data, so no rounding\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentiles[0], 4),\n        '50th_percentile': round(percentiles[1], 4),\n        '75th_percentile': round(percentiles[2], 4),\n        'interquartile_range': round(interquartile_range, 4)\n    }\n\n    return stats_dict\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [1, 2, 3, 4, 4, 5, 5, 5]\n    print(descriptive_statistics(data))\n```\n\nThis code defines a function `descriptive_statistics` that takes a list or NumPy array `data` as input and returns a dictionary containing various descriptive statistics. The function first checks if the input data is empty and raises a `ValueError` if it is. It then calculates the mean, median, mode, variance, standard deviation, percentiles, and interquartile range using NumPy and SciPy functions. The results are stored in a dictionary, with values rounded to four decimal places where applicable. The function includes a docstring that describes its purpose, arguments, and return value. The example usage demonstrates how to call the function with a sample dataset and print the resulting dictionary."}
{"task_id": 79, "completion_id": 0, "completion": "# Task: Binomial Distribution Probability\n## Description:\nThe task is to write a Python function to calculate the probability of achieving exactly k successes in n independent Bernoulli trials, each with probability p of success, using the Binomial distribution formula.\n\n## Code\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Check if the input values are valid\n    if not (isinstance(n, int) and isinstance(k, int) and isinstance(p, (int, float))):\n        raise TypeError(\"Input types are not valid. 'n' and 'k' should be integers, 'p' should be a number.\")\n    if n < 0 or k < 0:\n        raise ValueError(\"'n' and 'k' should be non-negative integers.\")\n    if k > n:\n        raise ValueError(\"'k' cannot be greater than 'n'.\")\n    if p < 0 or p > 1:\n        raise ValueError(\"'p' should be between 0 and 1.\")\n\n    # Calculate the binomial coefficient using math.comb\n    binomial_coefficient = math.comb(n, k)\n    \n    # Calculate the probability of k successes and n-k failures\n    probability = binomial_coefficient * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Return the probability rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    n = 10  # Total number of trials\n    k = 5   # Number of successes\n    p = 0.5  # Probability of success on each trial\n    \n    result = binomial_probability(n, k, p)\n    print(f\"The probability of {k} successes in {n} trials with a probability of success {p} is: {result}\")\n```\n\n## Explanation\n\n*   The function `binomial_probability(n, k, p)` takes three parameters: `n` (total number of trials), `k` (number of successes), and `p` (probability of success on each trial).\n*   It first checks if the input values are valid. It ensures that `n` and `k` are non-negative integers and `p` is a number between 0 and 1.\n*   The binomial coefficient is calculated using the `math.comb(n, k)` function, which is available in Python's standard library.\n*   The probability is then calculated using the Binomial distribution formula: `P(X = k) = (n choose k) * p^k * (1-p)^(n-k)`.\n*   Finally, the function returns the calculated probability rounded to five decimal places.\n\n## Example Usage\n\nThe example usage demonstrates how to use the `binomial_probability` function to calculate the probability of achieving exactly 5 successes in 10 independent Bernoulli trials, each with a probability of success of 0.5. The result is then printed to the console."}
{"task_id": 80, "completion_id": 0, "completion": "Here's the Python code that meets the task requirements:\n\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    # Check if the standard deviation is zero to avoid division by zero\n    if std_dev == 0:\n        raise ValueError(\"Standard deviation cannot be zero.\")\n\n    # Calculate the coefficient part of the normal distribution formula\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    \n    # Calculate the exponent part of the normal distribution formula\n    exponent = -((x - mean) ** 2) / (2 * std_dev ** 2)\n    \n    # Calculate the PDF using the formula for the normal distribution\n    pdf = coefficient * math.exp(exponent)\n    \n    # Return the PDF value rounded to 5 decimal places\n    return round(pdf, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    x = 0  # The value at which the PDF is evaluated\n    mean = 0  # The mean of the distribution\n    std_dev = 1  # The standard deviation of the distribution\n    \n    pdf_value = normal_pdf(x, mean, std_dev)\n    print(f\"The PDF at x = {x} is {pdf_value}\")\n```\n\n### Explanation:\n\n*   The `normal_pdf` function takes in three parameters: `x`, `mean`, and `std_dev`, representing the value at which the PDF is evaluated, the mean of the distribution, and the standard deviation of the distribution, respectively.\n*   It first checks if the `std_dev` is zero to avoid division by zero and raises a `ValueError` if true.\n*   The function then calculates the probability density function (PDF) using the formula for the normal distribution: `f(x | \u03bc, \u03c3) = (1 / (\u03c3 * sqrt(2 * \u03c0))) * exp(-((x - \u03bc)^2) / (2 * \u03c3^2))`, where `\u03bc` is the mean and `\u03c3` is the standard deviation.\n*   Finally, it returns the calculated PDF value rounded to 5 decimal places using the `round` function.\n\n### Example Use Case:\n\nThe example usage demonstrates how to use the `normal_pdf` function to calculate the PDF at `x = 0` for a standard normal distribution (mean = 0, standard deviation = 1). The result is then printed to the console."}
{"task_id": 81, "completion_id": 0, "completion": "Here's a Python function that implements the Poisson distribution probability calculator as per your requirements:\n\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    # Check if k is a non-negative integer\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"k must be a non-negative integer\")\n    # Check if lam is a non-negative number\n    if lam < 0:\n        raise ValueError(\"lam must be a non-negative number\")\n\n    # Calculate the probability using the Poisson distribution formula\n    probability = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    \n    # Return the probability rounded to 5 decimal places\n    return round(probability, 5)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    k = 5  # Number of events\n    lam = 4.2  # Mean rate of occurrences\n    print(f\"The probability of observing exactly {k} events with a mean rate of {lam} is: {poisson_probability(k, lam)}\")\n```\n\n### Explanation:\n\n1.  The function `poisson_probability(k, lam)` takes two parameters: `k` (the number of events) and `lam` (the mean rate of occurrences).\n2.  It first checks if `k` is a non-negative integer and `lam` is a non-negative number, raising a `ValueError` if either condition is not met.\n3.  The Poisson distribution probability is calculated using the formula: `P(k;\u03bb) = (e^(-\u03bb) * (\u03bb^k)) / k!`, where `e` is the base of the natural logarithm, `\u03bb` is the mean rate, and `k` is the number of events.\n4.  The `math.exp(-lam)` calculates `e^(-\u03bb)`, `lam ** k` calculates `\u03bb^k`, and `math.factorial(k)` calculates `k!`.\n5.  The calculated probability is then rounded to 5 decimal places using the `round()` function.\n6.  The example usage demonstrates how to use the `poisson_probability()` function with `k = 5` and `lam = 4.2`, printing the result."}
{"task_id": 82, "completion_id": 0, "completion": "## Grayscale Image Contrast Calculator\n### Task Description\nThe task is to write a Python function to calculate the contrast of a grayscale image using the difference between the maximum and minimum pixel values.\n\n### Code Solution\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n\n    Returns:\n        int: The contrast of the grayscale image, calculated as the difference between the maximum and minimum pixel values.\n\n    Raises:\n        ValueError: If the input image is not a 2D numpy array or if pixel values are not between 0 and 255.\n    \"\"\"\n    # Check if input is a 2D numpy array\n    if not isinstance(img, np.ndarray) or len(img.shape) != 2:\n        raise ValueError(\"Input image must be a 2D numpy array\")\n\n    # Check if pixel values are between 0 and 255\n    if np.any(img < 0) or np.any(img > 255):\n        raise ValueError(\"Pixel values must be between 0 and 255\")\n\n    # Calculate the minimum and maximum pixel values\n    min_pixel = np.min(img)\n    max_pixel = np.max(img)\n\n    # Calculate the contrast as the difference between the maximum and minimum pixel values\n    contrast = max_pixel - min_pixel\n\n    return contrast\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample grayscale image\n    img = np.array([[0, 128, 255], [0, 128, 255], [0, 128, 255]], dtype=np.uint8)\n\n    # Calculate the contrast of the image\n    contrast = calculate_contrast(img)\n\n    print(f\"The contrast of the image is: {contrast}\")\n```\n\n### Explanation\n\n*   The `calculate_contrast` function takes a 2D numpy array `img` representing a grayscale image as input.\n*   It first checks if the input is a valid 2D numpy array and if the pixel values are within the valid range of 0 to 255. If not, it raises a `ValueError` with an appropriate error message.\n*   The function then calculates the minimum and maximum pixel values in the image using `np.min` and `np.max`, respectively.\n*   The contrast is calculated as the difference between the maximum and minimum pixel values.\n*   The function returns the calculated contrast as an integer value.\n*   In the example usage, a sample grayscale image is created, and its contrast is calculated using the `calculate_contrast` function. The result is then printed to the console."}
{"task_id": 83, "completion_id": 0, "completion": "## Dot Product Calculator\n### Task Description\nThe task is to write a Python function to calculate the dot product of two vectors. The function should take two 1D NumPy arrays as input and return the dot product as a single number.\n\n### Code Solution\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n\n    Returns:\n        float: The dot product of vec1 and vec2.\n\n    Raises:\n        ValueError: If the input vectors are not of the same length.\n        TypeError: If the input vectors are not NumPy arrays.\n    \"\"\"\n    # Check if inputs are NumPy arrays\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise TypeError(\"Both inputs must be NumPy arrays\")\n\n    # Check if inputs are 1D arrays\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both inputs must be 1D arrays\")\n\n    # Check if input vectors have the same length\n    if len(vec1) != len(vec2):\n        raise ValueError(\"Input vectors must be of the same length\")\n\n    # Calculate the dot product using NumPy's dot function\n    dot_product = np.dot(vec1, vec2)\n\n    return dot_product\n\n# Example usage\nif __name__ == \"__main__\":\n    vec1 = np.array([1, 2, 3])\n    vec2 = np.array([4, 5, 6])\n    print(\"Vector 1:\", vec1)\n    print(\"Vector 2:\", vec2)\n    print(\"Dot Product:\", calculate_dot_product(vec1, vec2))\n```\n\n### Explanation\n\n*   The `calculate_dot_product` function takes two 1D NumPy arrays `vec1` and `vec2` as input.\n*   It first checks if both inputs are NumPy arrays and raises a `TypeError` if not.\n*   Then, it checks if both inputs are 1D arrays and raises a `ValueError` if not.\n*   It checks if the input vectors have the same length and raises a `ValueError` if they don't.\n*   The function uses NumPy's `dot` function to calculate the dot product of the two input vectors.\n*   Finally, it returns the calculated dot product.\n\n### Example Usage\n\nThe example usage is demonstrated in the `if __name__ == \"__main__\":` block. It creates two sample vectors `vec1` and `vec2`, calculates their dot product using the `calculate_dot_product` function, and prints the result."}
{"task_id": 84, "completion_id": 0, "completion": "## Phi Transformation for Polynomial Features\n### Task Description\nThe task is to implement a Python function `phi_transform` that performs a Phi Transformation. This transformation maps input features into a higher-dimensional space by generating polynomial features, allowing models like linear regression to fit nonlinear data.\n\n### Code Implementation\n```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list represents the transformed features of a data point.\n    \"\"\"\n    # Check if degree is less than 0 and return an empty list if true\n    if degree < 0:\n        return []\n\n    # Initialize an empty list to store the transformed data points\n    transformed_data = []\n\n    # Iterate over each data point in the input data\n    for point in data:\n        # Generate polynomial features using numpy's power function\n        # np.power(point, np.arange(degree + 1)) generates an array with elements [1, point, point^2, ..., point^degree]\n        # We then convert this array to a list, round the elements to 8 decimal places, and append it to transformed_data\n        transformed_data.append(np.round(np.power(point, np.arange(degree + 1)).tolist(), 8))\n\n    # Return the list of transformed data points\n    return transformed_data\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [1.0, 2.0, 3.0]\n    degree = 2\n    print(phi_transform(data, degree))\n    # Output: [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0], [1.0, 3.0, 9.0]]\n\n    # Test case with degree less than 0\n    degree = -1\n    print(phi_transform(data, degree))\n    # Output: []\n```\n\n### Explanation\n\n1.  The function `phi_transform` takes a list of numerical data and a degree as inputs.\n2.  It first checks if the degree is less than 0. If so, it returns an empty list as per the task requirements.\n3.  For each data point in the input list, it generates polynomial features up to the specified degree using NumPy's `power` function.\n4.  The `np.power(point, np.arange(degree + 1))` expression generates an array containing the polynomial features of the data point, i.e., `[1, point, point^2, ..., point^degree]`.\n5.  The generated polynomial features are then rounded to 8 decimal places using `np.round` and converted to a list.\n6.  The transformed features of each data point are appended to the `transformed_data` list.\n7.  Finally, the function returns the `transformed_data` list, which contains the polynomial features of all input data points.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `phi_transform` function with a list of data points and a specified degree. It also includes a test case with a degree less than 0 to verify that the function returns an empty list as expected."}
{"task_id": 85, "completion_id": 0, "completion": "## Positional Encoding Calculator\n### Task Description\nThe task is to implement a Python function to calculate positional encodings for a sequence length (`position`) and model dimensionality (`d_model`) using sine and cosine functions as specified in the Transformer architecture.\n\n### Code Implementation\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates positional encodings for a sequence length and model dimensionality.\n\n    Args:\n    position (int): The sequence length.\n    d_model (int): The model dimensionality.\n\n    Returns:\n    list: A list of positional encodings if position and d_model are valid, -1 otherwise.\n    \"\"\"\n    # Check for invalid inputs\n    if position == 0 or d_model <= 0:\n        return -1\n\n    # Initialize the positional encoding array\n    pe = np.zeros((position, d_model), dtype=np.float16)\n\n    # Calculate the positional encodings\n    for pos in range(position):\n        for i in range(d_model):\n            # Use sine for even indices and cosine for odd indices\n            if i % 2 == 0:\n                pe[pos, i] = np.sin(pos / np.power(10000, (2 * i) / d_model))\n            else:\n                pe[pos, i] = np.cos(pos / np.power(10000, (2 * (i - 1)) / d_model))\n\n    # Return the positional encoding array as a list\n    return pe.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    position = 10\n    d_model = 512\n    print(pos_encoding(position, d_model))\n```\n\n### Explanation\n1. The function `pos_encoding` takes two parameters: `position` and `d_model`, representing the sequence length and model dimensionality, respectively.\n2. It first checks if `position` is 0 or `d_model` is less than or equal to 0. If either condition is true, it returns -1 as per the task requirements.\n3. It initializes a numpy array `pe` of shape `(position, d_model)` with dtype `np.float16` to store the positional encodings.\n4. The function then calculates the positional encodings using nested loops. For each position `pos` and dimension `i`, it applies the sine function for even `i` and the cosine function for odd `i`, using the formula specified in the Transformer architecture.\n5. Finally, it returns the calculated positional encodings as a list by calling `tolist()` on the numpy array `pe`.\n\n### Example Usage\nThe example usage demonstrates how to call the `pos_encoding` function with a sequence length of 10 and a model dimensionality of 512, and prints the resulting positional encodings."}
{"task_id": 86, "completion_id": 0, "completion": "Here's a Python function that meets the task requirements:\n\n```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for overfitting\n    if training_accuracy - test_accuracy > 0.2:\n        return 1  # Overfitting\n    \n    # Check for underfitting\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1  # Underfitting\n    \n    # If none of the above conditions are met, it's a good fit\n    return 0  # Good fit\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test cases\n    test_cases = [\n        (0.9, 0.6),  # Overfitting\n        (0.6, 0.6),  # Underfitting\n        (0.8, 0.7),  # Good fit\n        (0.95, 0.95),  # Good fit\n        (0.4, 0.4)  # Underfitting\n    ]\n\n    for training_accuracy, test_accuracy in test_cases:\n        result = model_fit_quality(training_accuracy, test_accuracy)\n        if result == 1:\n            print(f\"Training Accuracy: {training_accuracy}, Test Accuracy: {test_accuracy} -> Overfitting\")\n        elif result == -1:\n            print(f\"Training Accuracy: {training_accuracy}, Test Accuracy: {test_accuracy} -> Underfitting\")\n        else:\n            print(f\"Training Accuracy: {training_accuracy}, Test Accuracy: {test_accuracy} -> Good fit\")\n```\n\nThe provided Python function `model_fit_quality` takes two parameters: `training_accuracy` and `test_accuracy`, representing the accuracy of a machine learning model on the training and test datasets, respectively. It returns an integer indicating whether the model is overfitting (1), underfitting (-1), or a good fit (0) based on predefined rules. The code includes example usage in the `if __name__ == \"__main__\":` block to demonstrate its functionality with various test cases."}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    \n    # Update the first moment estimate (m) using the current gradient and beta1\n    updated_m = beta1 * m + (1 - beta1) * grad\n    \n    # Update the second moment estimate (v) using the current gradient and beta2\n    updated_v = beta2 * v + (1 - beta2) * np.square(grad)\n    \n    # Calculate the bias-corrected first moment estimate\n    m_hat = updated_m / (1 - np.power(beta1, t + 1))  # Using t + 1 because t is the current timestep and we are calculating for the next step\n    \n    # Calculate the bias-corrected second moment estimate\n    v_hat = updated_v / (1 - np.power(beta2, t + 1))  # Using t + 1 because t is the current timestep and we are calculating for the next step\n    \n    # Update the parameter using the Adam update rule\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round the results to 5 decimal places and convert to list\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    updated_m = np.round(updated_m, 5).tolist()\n    updated_v = np.round(updated_v, 5).tolist()\n    \n    return updated_parameter, updated_m, updated_v\n```\n\n### Example Usage:\n\n```python\n# Scalar example\nparameter = 1.0\ngrad = 0.5\nm = 0.0\nv = 0.0\nt = 0\n\nupdated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\nprint(\"Updated Parameter:\", updated_parameter)\nprint(\"Updated First Moment:\", updated_m)\nprint(\"Updated Second Moment:\", updated_v)\n\n# Array example\nparameter = np.array([1.0, 2.0])\ngrad = np.array([0.5, 0.3])\nm = np.array([0.0, 0.0])\nv = np.array([0.0, 0.0])\nt = 0\n\nupdated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\nprint(\"Updated Parameter:\", updated_parameter)\nprint(\"Updated First Moment:\", updated_m)\nprint(\"Updated Second Moment:\", updated_v)\n```\n\nThe provided code implements the Adam optimizer, a popular stochastic gradient descent optimizer used in deep learning. The `adam_optimizer` function takes the current parameter value, gradient, and moving averages as inputs and returns the updated parameter value and new moving averages. It handles both scalar and array inputs and includes bias correction for the moving averages. The results are rounded to 5 decimal places and converted to lists before being returned. The example usage demonstrates how to use the `adam_optimizer` function with both scalar and array inputs."}
{"task_id": 88, "completion_id": 0, "completion": "Here's a Python implementation of a simplified GPT-2-like text generation function.\n\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12,\n        \"n_embd\": 10  # Added n_embd to hparams\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, hparams[\"n_embd\"]),  # Token embeddings\n        \"wpe\": np.random.rand(hparams[\"n_ctx\"], hparams[\"n_embd\"]),  # Positional embeddings\n        \"blocks\": [\n            {  # Simplified transformer block\n                \"ln_1\": {\n                    \"g\": np.ones(hparams[\"n_embd\"]),\n                    \"b\": np.zeros(hparams[\"n_embd\"]),\n                },\n                \"ln_2\": {\n                    \"g\": np.ones(hparams[\"n_embd\"]),\n                    \"b\": np.zeros(hparams[\"n_embd\"]),\n                },\n                \"attn\": {\n                    \"c_attn\": {\n                        \"w\": np.random.rand(hparams[\"n_embd\"], 3 * hparams[\"n_embd\"]),\n                        \"b\": np.zeros(3 * hparams[\"n_embd\"]),\n                    },\n                    \"c_proj\": {\n                        \"w\": np.random.rand(hparams[\"n_embd\"], hparams[\"n_embd\"]),\n                        \"b\": np.zeros(hparams[\"n_embd\"]),\n                    },\n                },\n                \"mlp\": {\n                    \"c_fc\": {\n                        \"w\": np.random.rand(hparams[\"n_embd\"], 4 * hparams[\"n_embd\"]),\n                        \"b\": np.zeros(4 * hparams[\"n_embd\"]),\n                    },\n                    \"c_proj\": {\n                        \"w\": np.random.rand(4 * hparams[\"n_embd\"], hparams[\"n_embd\"]),\n                        \"b\": np.zeros(hparams[\"n_embd\"]),\n                    },\n                },\n            }\n        ],\n        \"ln_f\": {  # Final layer normalization\n            \"g\": np.ones(hparams[\"n_embd\"]),\n            \"b\": np.zeros(hparams[\"n_embd\"]),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef layer_norm(x, g, b, eps=1e-5):\n    \"\"\"Layer normalization.\"\"\"\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x = (x - mean) / np.sqrt(variance + eps)\n    return g * x + b\n\ndef softmax(x, axis=-1):\n    \"\"\"Softmax function.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n\ndef multi_head_attention(q, k, v, n_head):\n    \"\"\"Multi-head attention.\"\"\"\n    d_k = q.shape[-1] // n_head\n    q = np.reshape(q, (*q.shape[:-1], n_head, d_k))\n    k = np.reshape(k, (*k.shape[:-1], n_head, d_k))\n    v = np.reshape(v, (*v.shape[:-1], n_head, d_k))\n    qk = np.matmul(np.transpose(q, (0, 1, 3, 2)), np.transpose(k, (0, 1, 2, 3))) / np.sqrt(d_k)\n    attention_weights = softmax(qk)\n    output = np.matmul(attention_weights, np.transpose(v, (0, 1, 3, 2)))\n    output = np.transpose(output, (0, 1, 3, 2))\n    return np.reshape(output, (*output.shape[:-2], -1))\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    np.random.seed(42)  # Set the random seed for reproducibility\n    \n    # Load the encoder, hyperparameters, and model parameters\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    \n    # Encode the prompt\n    input_ids = encoder.encode(prompt)\n    generated_ids = input_ids.copy()\n    \n    # Generate text\n    for _ in range(n_tokens_to_generate):\n        # Prepare input\n        input_ids = np.array(generated_ids[-hparams[\"n_ctx\"]:])  # Use the last n_ctx tokens\n        \n        # Token and positional embeddings\n        token_embeddings = params[\"wte\"][input_ids]\n        positional_embeddings = params[\"wpe\"][np.arange(len(input_ids))]\n        x = token_embeddings + positional_embeddings\n        \n        # Simplified transformer block\n        for block in params[\"blocks\"]:\n            # Layer normalization\n            x = layer_norm(x, **block[\"ln_1\"])\n            \n            # Multi-head attention\n            qkv = np.matmul(x, block[\"attn\"][\"c_attn\"][\"w\"]) + block[\"attn\"][\"c_attn\"][\"b\"]\n            q, k, v = np.split(qkv, 3, axis=-1)\n            attention_output = multi_head_attention(q, k, v, hparams[\"n_head\"])\n            attention_output = np.matmul(attention_output, block[\"attn\"][\"c_proj\"][\"w\"]) + block[\"attn\"][\"c_proj\"][\"b\"]\n            x = x + attention_output\n            \n            # Layer normalization\n            x = layer_norm(x, **block[\"ln_2\"])\n            \n            # Feed-forward network\n            mlp_output = np.maximum(np.matmul(x, block[\"mlp\"][\"c_fc\"][\"w\"]) + block[\"mlp\"][\"c_fc\"][\"b\"], 0)  # ReLU activation\n            mlp_output = np.matmul(mlp_output, block[\"mlp\"][\"c_proj\"][\"w\"]) + block[\"mlp\"][\"c_proj\"][\"b\"]\n            x = x + mlp_output\n        \n        # Final layer normalization\n        x = layer_norm(x, **params[\"ln_f\"])\n        \n        # Predict the next token\n        logits = np.matmul(x[-1], params[\"wte\"].T)  # Use the last token's representation\n        next_token_id = np.argmax(logits)\n        \n        # Append the predicted token to the generated IDs\n        generated_ids.append(next_token_id)\n    \n    # Decode the generated IDs\n    generated_text = encoder.decode(generated_ids)\n    return generated_text\n\n# Example usage\nprint(gen_text(\"hello world\", n_tokens_to_generate=10))\n```\n\nThe provided code implements a simplified GPT-2-like text generation function. The `gen_text` function takes a prompt and the number of tokens to generate as input and outputs the generated text. The `load_encoder_hparams_and_params` function is used to load a dummy encoder, model hyperparameters, and model parameters.\n\nThe text generation process involves the following steps:\n\n1.  Encoding the prompt using the provided encoder.\n2.  Initializing the generated IDs with the encoded prompt.\n3.  Generating text by iteratively predicting the next token based on the previous tokens.\n4.  Using a simplified transformer block to process the input sequence and predict the next token.\n5.  Decoding the generated IDs to obtain the final generated text.\n\nThe example usage demonstrates how to use the `gen_text` function to generate text based on a given prompt."}
{"task_id": 89, "completion_id": 0, "completion": "# Task: The Pattern Weaver's Code\n\n## Problem Description\nThe task is to implement a simplified self-attention mechanism for a given sequence of crystals and their values. The goal is to reveal their enhanced patterns by analyzing the relationships between the crystals using self-attention.\n\n## Implementation\n\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    This function implements a simplified self-attention mechanism.\n    \n    Parameters:\n    n (int): The number of crystals.\n    crystal_values (list): A list of crystal values.\n    dimension (int): The dimension of the crystal values.\n    \n    Returns:\n    list: A list of floats representing the final weighted pattern for each crystal, rounded to the 4th decimal place.\n    \"\"\"\n\n    def softmax(values):\n        \"\"\"\n        This function computes the softmax of a given set of values.\n        \n        Parameters:\n        values (numpy array): The input values.\n        \n        Returns:\n        numpy array: The softmax of the input values.\n        \"\"\"\n        e_values = np.exp(values - np.max(values))  # For numerical stability\n        return e_values / e_values.sum(axis=-1, keepdims=True)\n\n    # Convert crystal values to a numpy array for easier manipulation\n    crystal_values = np.array(crystal_values).reshape(n, dimension)\n\n    # Compute the query, key, and value matrices\n    # For simplicity, we assume the query, key, and value weights are the same\n    weights = np.random.rand(dimension, dimension)  # Random weights for demonstration\n    queries = np.dot(crystal_values, weights)\n    keys = np.dot(crystal_values, weights)\n    values = crystal_values  # Using crystal values as values for simplicity\n\n    # Compute attention scores\n    attention_scores = np.dot(queries, keys.T) / np.sqrt(dimension)\n\n    # Compute softmax of attention scores\n    attention_weights = softmax(attention_scores)\n\n    # Compute the final weighted pattern for each crystal\n    weighted_patterns = np.dot(attention_weights, values)\n\n    # Return the sum of the weighted patterns for each crystal, rounded to the 4th decimal place\n    return [round(np.sum(weighted_pattern), 4) for weighted_pattern in weighted_patterns]\n\n# Example usage\nn = 3\ncrystal_values = [1, 2, 3, 4, 5, 6, 7, 8, 9]\ndimension = 3\nprint(pattern_weaver(n, crystal_values, dimension))\n```\n\n## Explanation\n\n1.  The `pattern_weaver` function takes in the number of crystals `n`, their values `crystal_values`, and the dimension `dimension` of the crystal values.\n2.  It defines a nested `softmax` function to compute the softmax of a given set of values.\n3.  The crystal values are converted to a numpy array and reshaped to `(n, dimension)` for easier manipulation.\n4.  The query, key, and value matrices are computed using random weights for demonstration purposes.\n5.  The attention scores are calculated by taking the dot product of the queries and keys, scaled by the square root of the dimension.\n6.  The softmax of the attention scores is computed using the `softmax` function.\n7.  The final weighted pattern for each crystal is obtained by taking the dot product of the attention weights and values.\n8.  The sum of the weighted patterns for each crystal is returned, rounded to the 4th decimal place.\n\n## Advice\n\n*   This implementation assumes a simplified self-attention mechanism. In practice, you may need to consider more complex scenarios, such as multi-head attention or different weight matrices for queries, keys, and values.\n*   The `softmax` function is implemented with numerical stability in mind by subtracting the maximum value before exponentiation.\n*   The example usage demonstrates how to call the `pattern_weaver` function with sample inputs."}
{"task_id": 90, "completion_id": 0, "completion": "## BM25 Ranking Implementation\n### Overview\n\nThe BM25 ranking function is a widely used algorithm in information retrieval for calculating the relevance of documents to a given query. It improves upon the traditional TF-IDF by incorporating term frequency saturation and document length normalization.\n\n### Implementation\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a given corpus and query.\n\n    Parameters:\n    - corpus (list of list of str): A list of documents where each document is a list of terms.\n    - query (list of str): A list of query terms.\n    - k1 (float, optional): Term frequency saturation parameter. Defaults to 1.5.\n    - b (float, optional): Document length normalization parameter. Defaults to 0.75.\n\n    Returns:\n    - list of float: A list of BM25 scores for each document in the corpus, rounded to three decimal places.\n    \"\"\"\n\n    # Calculate document frequencies (DF) for terms in the corpus\n    df = Counter(term for doc in corpus for term in set(doc))\n    \n    # Calculate the total number of documents\n    N = len(corpus)\n    \n    # Calculate the average document length\n    avgdl = sum(len(doc) for doc in corpus) / N\n    \n    # Initialize a list to store BM25 scores for each document\n    bm25_scores = []\n    \n    # Iterate over each document in the corpus\n    for doc in corpus:\n        # Calculate the document length\n        dl = len(doc)\n        \n        # Count term frequencies in the document\n        tf = Counter(doc)\n        \n        # Initialize the BM25 score for the current document\n        score = 0\n        \n        # Iterate over each term in the query\n        for term in query:\n            # Check if the term is in the document\n            if term in tf:\n                # Calculate the term frequency\n                freq = tf[term]\n                \n                # Calculate the IDF (Inverse Document Frequency)\n                idf = np.log((N - df[term] + 0.5) / (df[term] + 0.5) + 1)\n                \n                # Calculate the BM25 score component for the term\n                score += idf * (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * dl / avgdl))\n        \n        # Append the BM25 score for the current document to the list\n        bm25_scores.append(round(score, 3))\n    \n    return bm25_scores\n\n# Example usage\nif __name__ == \"__main__\":\n    corpus = [\n        [\"this\", \"is\", \"a\", \"test\", \"document\"],\n        [\"another\", \"test\", \"document\", \"with\", \"different\", \"terms\"],\n        [\"a\", \"document\", \"with\", \"some\", \"similar\", \"terms\"]\n    ]\n    query = [\"test\", \"document\"]\n    scores = calculate_bm25_scores(corpus, query)\n    print(\"BM25 Scores:\", scores)\n```\n\n### Explanation\n\n1.  The function `calculate_bm25_scores` takes a corpus of documents, a query, and optional parameters `k1` and `b` as input.\n2.  It calculates the document frequency (DF) for each term in the corpus and the total number of documents (N).\n3.  The average document length (`avgdl`) is computed by summing the lengths of all documents and dividing by N.\n4.  For each document in the corpus, the function calculates its BM25 score by iterating over the query terms.\n5.  For each query term present in the document, it computes the term frequency (`freq`), inverse document frequency (`idf`), and the BM25 score component.\n6.  The BM25 score for the document is the sum of the score components for all query terms.\n7.  The function returns a list of BM25 scores for all documents in the corpus, rounded to three decimal places.\n\n### Advice\n\n*   The parameters `k1` and `b` control term frequency saturation and document length normalization, respectively. Adjusting these parameters can impact the ranking performance.\n*   The BM25 algorithm assumes that the input corpus and query are preprocessed (e.g., tokenized, stopwords removed, and stemmed or lemmatized).\n*   This implementation is suitable for a basic information retrieval context. For more complex scenarios or larger corpora, consider using optimized libraries or frameworks."}
{"task_id": 91, "completion_id": 0, "completion": "## F1 Score Calculation Implementation\n### Overview\n\nThe F1 score is the harmonic mean of precision and recall, where precision is the ratio of true positives to the sum of true positives and false positives, and recall is the ratio of true positives to the sum of true positives and false negatives.\n\n### Code Implementation\n\n```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Check if both lists have the same length\n    assert len(y_true) == len(y_pred), \"Both lists should have the same length\"\n\n    # Calculate true positives (TP), false positives (FP), and false negatives (FN)\n    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n    fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)\n    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)\n\n    # Calculate precision and recall\n    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n\n    # Calculate F1 score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n\n    # Return F1 score rounded to three decimal places\n    return round(f1_score, 3)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n    y_pred = [1, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n    print(\"F1 Score:\", calculate_f1_score(y_true, y_pred))\n```\n\n### Explanation\n\n1.  The function `calculate_f1_score` takes two lists, `y_true` and `y_pred`, representing true labels and predicted labels, respectively.\n2.  It first checks if both lists have the same length to ensure valid comparison.\n3.  It calculates true positives (`tp`), false positives (`fp`), and false negatives (`fn`) by comparing corresponding elements in `y_true` and `y_pred`.\n4.  Precision and recall are then calculated using `tp`, `fp`, and `fn`. The function handles potential division-by-zero errors.\n5.  The F1 score is computed as the harmonic mean of precision and recall, again handling potential division-by-zero.\n6.  Finally, the F1 score is rounded to three decimal places before being returned.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `calculate_f1_score` function with sample true labels (`y_true`) and predicted labels (`y_pred`). The calculated F1 score is then printed out."}
{"task_id": 92, "completion_id": 0, "completion": "## Step 1: Understand the given problem and the required steps\nThe problem requires us to forecast the power grid consumption for day 15, given 10 days of consumption data. The data is affected by a linear trend and a daily fluctuation. We need to remove the fluctuation, fit a linear regression model, predict day 15's consumption, add back the fluctuation, and include a 5% safety margin.\n\n## Step 2: Define the function to calculate the daily fluctuation\nThe daily fluctuation is given by $f(i) = 10 \\times \\sin(2\\pi i / 10)$, where $i$ is the day number. We will use this formula to detrend the data.\n\n## Step 3: Detrend the consumption data by subtracting the daily fluctuation\nTo detrend the data, we subtract the daily fluctuation from each data point. This step is necessary to apply linear regression.\n\n## Step 4: Implement the detrending and linear regression\nFirst, we detrend the data. Then, we perform linear regression on the detrended data to find the linear trend.\n\n## Step 5: Predict day 15's base consumption using the linear regression model\nAfter obtaining the linear regression model, we use it to predict day 15's base consumption.\n\n## Step 6: Add back the fluctuation for day 15 and apply a 5% safety margin\nWe calculate the fluctuation for day 15, add it to the predicted base consumption, and then apply a 5% safety margin.\n\n## Step 7: Write the Python code to implement these steps\n```python\nimport math\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # 1. Detrend the data by subtracting the daily fluctuation\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    \n    # 2. Perform linear regression on the detrended data\n    n = len(detrended_data)\n    x_sum = sum(range(1, n + 1))\n    y_sum = sum(detrended_data)\n    xy_sum = sum(i * detrended_data[i - 1] for i in range(1, n + 1))\n    x_squared_sum = sum(i ** 2 for i in range(1, n + 1))\n    \n    m = (n * xy_sum - x_sum * y_sum) / (n * x_squared_sum - x_sum ** 2)\n    b = (y_sum - m * x_sum) / n\n    \n    # 3. Predict day 15's base consumption\n    day_15_base = m * 15 + b\n    \n    # 4. Add the day 15 fluctuation back\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base + day_15_fluctuation\n    \n    # 5. Round, then add a 5% safety margin (rounded up)\n    day_15_consumption = round(day_15_consumption)\n    safety_margin = math.ceil(day_15_consumption * 0.05)\n    final_consumption = day_15_consumption + safety_margin\n    \n    # 6. Return the final integer\n    return final_consumption\n\n# Example usage:\nconsumption_data = [100, 105, 110, 115, 120, 125, 130, 135, 140, 145]  # Example data, actual data should be provided\nprint(power_grid_forecast(consumption_data))\n```\n\n## Step 8: Explanation of the linear regression implementation\nThe linear regression is implemented using the formulas for the slope ($m$) and intercept ($b$) of a linear regression line: $m = \\frac{n\\sum xy - \\sum x \\sum y}{n\\sum x^2 - (\\sum x)^2}$ and $b = \\frac{\\sum y - m\\sum x}{n}$, where $n$ is the number of data points, $x$ represents the day numbers, and $y$ represents the detrended consumption data.\n\nThe final answer is: $\\boxed{power_grid_forecast([100, 105, 110, 115, 120, 125, 130, 135, 140, 145])}$"}
{"task_id": 93, "completion_id": 0, "completion": "## Calculating Mean Absolute Error (MAE) in Python\n### Overview\n\nThe Mean Absolute Error (MAE) is a measure used to evaluate the average magnitude of the errors in a set of predictions. It is calculated as the average of the absolute differences between actual and predicted values.\n\n### Implementation\n\nHere's a Python function that calculates the MAE between two NumPy arrays of actual (`y_true`) and predicted (`y_pred`) values.\n\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Check if inputs are numpy arrays\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError(\"Both inputs must be numpy arrays\")\n\n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape\")\n\n    # Calculate absolute differences\n    absolute_errors = np.abs(y_true - y_pred)\n\n    # Calculate mean of absolute errors\n    mean_absolute_error = np.mean(absolute_errors)\n\n    # Return MAE rounded to 3 decimal places\n    return round(mean_absolute_error, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([1, 2, 3, 4, 5])\n    y_pred = np.array([1.1, 1.9, 3.2, 4.1, 5.0])\n    print(mae(y_true, y_pred))\n```\n\n### Explanation\n\n1. **Input Validation**: The function first checks if both `y_true` and `y_pred` are NumPy arrays and if they have the same shape. If not, it raises a `TypeError` or `ValueError` accordingly.\n2. **Calculate Absolute Errors**: It calculates the absolute differences between corresponding elements in `y_true` and `y_pred` using `np.abs(y_true - y_pred)`.\n3. **Calculate MAE**: The mean of these absolute errors is computed using `np.mean()`, giving the MAE.\n4. **Return Rounded MAE**: The MAE is rounded to 3 decimal places before being returned.\n\n### Example Usage\n\nThe example demonstrates how to use the `mae` function with two sample NumPy arrays `y_true` and `y_pred`, representing actual and predicted values, respectively. The calculated MAE is then printed out."}
{"task_id": 94, "completion_id": 0, "completion": "## Step 1: Implement the `compute_qkv` function to calculate the Query (Q), Key (K), and Value (V) matrices.\nTo compute Q, K, and V, we need to multiply the input X with the corresponding weight matrices W_q, W_k, and W_v. This can be achieved through matrix multiplication.\n\n## Step 2: Implement the `self_attention` function to perform self-attention on the given Q, K, and V matrices.\nThe self-attention mechanism involves calculating the attention scores by taking the dot product of Q and K, scaling it, and then applying a softmax function. The output is obtained by taking the dot product of the attention scores and V.\n\n## Step 3: Implement the `multi_head_attention` function to perform multi-head attention.\nFirst, we compute Q, K, and V using the `compute_qkv` function. Then, we split these matrices into multiple heads, perform self-attention on each head using the `self_attention` function, and finally concatenate the results.\n\n## Step 4: Write the code for the `compute_qkv`, `self_attention`, and `multi_head_attention` functions.\n```python\nimport numpy as np\nfrom scipy.special import softmax\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query (Q), Key (K), and Value (V) matrices.\n\n    Args:\n    X (np.ndarray): Input matrix.\n    W_q (np.ndarray): Weight matrix for Query.\n    W_k (np.ndarray): Weight matrix for Key.\n    W_v (np.ndarray): Weight matrix for Value.\n\n    Returns:\n    tuple: Q, K, V matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform self-attention on the given Q, K, and V matrices.\n\n    Args:\n    Q (np.ndarray): Query matrix.\n    K (np.ndarray): Key matrix.\n    V (np.ndarray): Value matrix.\n\n    Returns:\n    np.ndarray: Output of self-attention.\n    \"\"\"\n    # Calculate attention scores\n    attention_scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n    # Apply softmax\n    attention_weights = softmax(attention_scores, axis=-1)\n    # Compute output\n    output = np.dot(attention_weights, V)\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Perform multi-head attention.\n\n    Args:\n    X (np.ndarray): Input matrix.\n    W_q (np.ndarray): Weight matrix for Query.\n    W_k (np.ndarray): Weight matrix for Key.\n    W_v (np.ndarray): Weight matrix for Value.\n    n_heads (int): Number of heads.\n\n    Returns:\n    list: Output of multi-head attention as a list.\n    \"\"\"\n    # Compute the Query (Q), Key (K), and Value (V) matrices.\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n    # Get the dimensions\n    n_samples, seq_len, d_model = Q.shape\n\n    # Reshape Q, K, V for multi-head attention\n    head_size = d_model // n_heads\n    Q_heads = Q.reshape(n_samples, seq_len, n_heads, head_size).transpose(0, 2, 1, 3)\n    K_heads = K.reshape(n_samples, seq_len, n_heads, head_size).transpose(0, 2, 1, 3)\n    V_heads = V.reshape(n_samples, seq_len, n_heads, head_size).transpose(0, 2, 1, 3)\n\n    # Perform self-attention for each head\n    outputs = []\n    for i in range(n_heads):\n        output = self_attention(Q_heads[:, i, :, :], K_heads[:, i, :, :], V_heads[:, i, :, :])\n        outputs.append(output)\n\n    # Concatenate the outputs of all heads\n    outputs = np.array(outputs).transpose(1, 2, 0, 3).reshape(n_samples, seq_len, d_model)\n\n    # Round the results to the nearest 4th decimal\n    outputs = np.round(outputs, 4)\n\n    # Return the output as a list\n    return outputs.tolist()\n```\n\n## Step 5: Example usage of the `multi_head_attention` function.\n```python\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(0)\n    X = np.random.rand(1, 10, 512)  # batch_size, sequence_length, embedding_size\n    W_q = np.random.rand(512, 512)\n    W_k = np.random.rand(512, 512)\n    W_v = np.random.rand(512, 512)\n    n_heads = 8\n\n    output = multi_head_attention(X, W_q, W_k, W_v, n_heads)\n    print(output)\n```\n\nThe final answer is: \n```python\nimport numpy as np\nfrom scipy.special import softmax\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    attention_scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n    attention_weights = softmax(attention_scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    n_samples, seq_len, d_model = Q.shape\n    head_size = d_model // n_heads\n    Q_heads = Q.reshape(n_samples, seq_len, n_heads, head_size).transpose(0, 2, 1, 3)\n    K_heads = K.reshape(n_samples, seq_len, n_heads, head_size).transpose(0, 2, 1, 3)\n    V_heads = V.reshape(n_samples, seq_len, n_heads, head_size).transpose(0, 2, 1, 3)\n    outputs = []\n    for i in range(n_heads):\n        output = self_attention(Q_heads[:, i, :, :], K_heads[:, i, :, :], V_heads[:, i, :, :])\n        outputs.append(output)\n    outputs = np.array(outputs).transpose(1, 2, 0, 3).reshape(n_samples, seq_len, d_model)\n    outputs = np.round(outputs, 4)\n    return outputs.tolist()\n```"}
{"task_id": 95, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Understanding the Phi Coefficient**: The Phi coefficient is a measure of association between two binary variables. It is similar to the Pearson correlation coefficient but is used specifically for binary data.\n2. **Formula for Phi Coefficient**: The Phi coefficient can be calculated using the formula: $\\phi = \\frac{(n_{11}n_{00} - n_{10}n_{01})}{\\sqrt{(n_{11}+n_{10})(n_{11}+n_{01})(n_{00}+n_{10})(n_{00}+n_{01})}}$, where $n_{ij}$ represents the number of observations where $x=i$ and $y=j$.\n3. **Input Validation**: The input lists `x` and `y` should be of the same length and contain only binary values (0 or 1).\n4. **Calculating $n_{ij}$**: We need to count the occurrences of each combination of $x$ and $y$ to calculate $n_{11}$, $n_{00}$, $n_{10}$, and $n_{01}$.\n\n# Fixed solution:\n```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Check if input lists are of the same length\n    if len(x) != len(y):\n        raise ValueError(\"Input lists must be of the same length\")\n\n    # Check if input lists contain only binary values\n    if not all(i in [0, 1] for i in x) or not all(i in [0, 1] for i in y):\n        raise ValueError(\"Input lists must contain only binary values (0 or 1)\")\n\n    # Initialize counters for n_ij\n    n11, n00, n10, n01 = 0, 0, 0, 0\n    \n    # Count occurrences of each combination of x and y\n    for xi, yi in zip(x, y):\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 0 and yi == 0:\n            n00 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        else:\n            n01 += 1\n    \n    # Calculate the Phi coefficient\n    numerator = (n11 * n00) - (n10 * n01)\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    \n    # Check for division by zero\n    if denominator == 0:\n        return 0.0\n    \n    phi = numerator / denominator\n    \n    # Return the Phi coefficient rounded to 4 decimal places\n    return round(phi, 4)\n```\n\n# Explanation of changes:\n* **Added input validation**: Checks if the input lists are of the same length and contain only binary values.\n* **Initialized counters for $n_{ij}$**: Variables `n11`, `n00`, `n10`, and `n01` are used to count the occurrences of each combination of `x` and `y`.\n* **Counted occurrences of each combination of `x` and `y`**: A loop iterates over the input lists `x` and `y` simultaneously using `zip`, incrementing the corresponding counter for each pair of values.\n* **Calculated the Phi coefficient**: The formula for the Phi coefficient is implemented using the counted values.\n* **Handled division by zero**: If the denominator is zero, the function returns 0.0 to avoid a division by zero error.\n* **Rounded the result to 4 decimal places**: The final Phi coefficient is rounded before being returned.\n\n# Tests and example uses:\n```python\n# Example usage\nx = [1, 0, 1, 1, 0, 0, 1, 0]\ny = [1, 1, 0, 1, 0, 0, 1, 1]\nprint(phi_corr(x, y))\n\n# Test case with identical lists\nx = [1, 0, 1, 0, 1]\ny = [1, 0, 1, 0, 1]\nprint(phi_corr(x, y))  # Expected output: 1.0\n\n# Test case with opposite lists\nx = [1, 0, 1, 0, 1]\ny = [0, 1, 0, 1, 0]\nprint(phi_corr(x, y))  # Expected output: -1.0\n\n# Test case with invalid input (different lengths)\nx = [1, 0, 1]\ny = [1, 0]\ntry:\n    print(phi_corr(x, y))\nexcept ValueError as e:\n    print(e)  # Expected output: Input lists must be of the same length\n\n# Test case with invalid input (non-binary values)\nx = [1, 0, 2]\ny = [1, 0, 1]\ntry:\n    print(phi_corr(x, y))\nexcept ValueError as e:\n    print(e)  # Expected output: Input lists must contain only binary values (0 or 1)\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    # The Hard Sigmoid function is defined as:\n    #   - 0 if x < -2.5\n    #   - 1 if x > 2.5\n    #   - 0.2 * x + 0.5 if -2.5 <= x <= 2.5\n    # First, we check if x is less than -2.5 and return 0 if true\n    if x < -2.5:\n        return 0.0\n    \n    # Then, we check if x is greater than 2.5 and return 1 if true\n    if x > 2.5:\n        return 1.0\n    \n    # If x is within the range [-2.5, 2.5], we calculate the Hard Sigmoid using the formula 0.2 * x + 0.5\n    return 0.2 * x + 0.5\n```\n\n### Example Usage\n```python\ndef main():\n    # Test the Hard Sigmoid function with different inputs\n    print(hard_sigmoid(-3.0))  # Expected output: 0.0\n    print(hard_sigmoid(-2.5))  # Expected output: 0.0\n    print(hard_sigmoid(0.0))   # Expected output: 0.5\n    print(hard_sigmoid(2.5))   # Expected output: 1.0\n    print(hard_sigmoid(3.0))   # Expected output: 1.0\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThis code defines the `hard_sigmoid` function according to the given specification and includes example usage in the `main` function. The `hard_sigmoid` function takes a single float input `x` and returns the corresponding Hard Sigmoid output. The example usage demonstrates the function's behavior with different inputs."}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    # If x is greater than or equal to 0, the ELU activation function is equal to x\n    if x >= 0:\n        return round(x, 4)\n    # If x is less than 0, the ELU activation function is alpha * (e^x - 1)\n    else:\n        # Calculate e^x\n        exp_x = math.exp(x)\n        # Calculate alpha * (e^x - 1) and round to 4 decimal places\n        return round(alpha * (exp_x - 1), 4)\n\n# Example usage:\nprint(elu(2.5))  # Output: 2.5\nprint(elu(-1.2))  # Output: -0.6988\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    # If the input is positive, return the input as is (like ReLU)\n    if x >= 0:\n        return x\n    # If the input is negative, return the input scaled by alpha\n    else:\n        return alpha * x\n```"}
{"task_id": 99, "completion_id": 0, "completion": "# Problem Understanding\nThe task is to implement the Softplus activation function, a smooth approximation of the ReLU function. The input is a float value `x`, and the output is the Softplus value of `x`, which is `log(1 + e^x)`, rounded to the 4th decimal place.\n\nKey constraints and edge cases:\n\n- The input `x` is a float.\n- The output should be a float rounded to the 4th decimal place.\n- The implementation should handle edge cases to prevent numerical overflow or underflow.\n- For very large `x`, `e^x` could overflow.\n- For very small `x`, the formula might underflow or lose precision.\n\nSubtle requirements or implicit constraints:\n\n- The implementation should be numerically stable.\n- The output should be accurate for a wide range of input values.\n\n# Key Insights\nCrucial observations that lead to the solution:\n\n- The Softplus function is defined as `log(1 + e^x)`, which can be numerically unstable for large `x` due to the overflow of `e^x`.\n- For large `x`, `e^x` is very large, and `1 + e^x` is approximately `e^x`, so `log(1 + e^x)` is approximately `x`.\n- For very negative `x`, `e^x` is very small, and `log(1 + e^x)` is approximately `e^x` because `log(1 + u)` is approximately `u` when `u` is small.\n- The implementation can be optimized for different ranges of `x` to handle edge cases.\n- For `x` greater than a certain threshold, the result can be approximated by `x`.\n- For very negative `x`, the result can be approximated by `e^x`.\n\nPatterns or properties being used:\n\n- The property of the exponential function that `e^x` is very large for large `x` and very small for very negative `x`.\n- The property of the logarithmic function that `log(1 + u)` is approximately `u` when `u` is small.\n\nCritical examples or test cases that led to insights:\n\n- `x = 100`: `softplus(x)` should approximate `x` because `e^100` is very large.\n- `x = -100`: `softplus(x)` should be very small because `e^-100` is very small.\n- `x = 0`: `softplus(x)` should be `log(2)`.\n\n# Solution Strategy\nSteps to solve this problem:\n\n1. Check if `x` is greater than a certain threshold (e.g., 709.78 for Python's float type). If so, return `x` as the result, rounded to the 4th decimal place.\n2. If `x` is not greater than the threshold, compute `e^x` and then `log(1 + e^x)`.\n3. For very negative `x`, consider using the approximation `e^x` for `log(1 + e^x)` to avoid underflow or loss of precision.\n4. Return the computed Softplus value, rounded to the 4th decimal place.\n\nImportant optimizations or special cases to handle:\n\n- Handling large `x` by approximating the result as `x`.\n- Handling very negative `x` by approximating `log(1 + e^x)` as `e^x`.\n- Using a threshold to determine when to use the approximations.\n\nHere's the python solution:\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # For large x, e^x will overflow, so we return x directly\n    if x > 709.78:  # This is a safe threshold for Python's float type\n        return round(x, 4)\n    # For very negative x, log(1 + e^x) is approximately e^x\n    elif x < -37:  # This threshold ensures e^x is very small and log(1+e^x) ~= e^x\n        return round(math.exp(x), 4)\n    else:\n        # Compute softplus normally for other values of x\n        result = math.log(1 + math.exp(x))\n        return round(result, 4)\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    # The Softsign activation function is defined as x / (1 + |x|).\n    # We use the absolute value of x to ensure the denominator is always positive.\n    softsign_value = x / (1 + abs(x))\n    \n    # Return the Softsign value rounded to the 4th decimal place as required.\n    return round(softsign_value, 4)\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert input lists to numpy arrays for efficient computation\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n\n    # Compute the clipped likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n\n    # Compute the GRPO objective for the original and clipped likelihood ratios\n    obj = np.minimum(rhos * A, clipped_rhos * A)\n\n    # Compute the KL divergence penalty term\n    kl_divergence = pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref))\n    kl_divergence_penalty = beta * kl_divergence\n\n    # Compute the final GRPO objective value by taking the mean of the objective values and subtracting the KL divergence penalty\n    grpo_obj_value = np.mean(obj - kl_divergence_penalty)\n\n    # Return the result rounded to the nearest 6th decimal\n    return round(grpo_obj_value, 6)\n```"}
{"task_id": 102, "completion_id": 0, "completion": "Here's the implementation of the Swish activation function in Python:\n\n```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    # The Swish activation function is defined as f(x) = x * sigmoid(x)\n    # where sigmoid(x) = 1 / (1 + exp(-x))\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid_x\n    return round(swish_value, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test the Swish function with a single value\n    x = 2.5\n    print(f\"Swish({x}) = {swish(x)}\")\n\n    # Test the Swish function with a numpy array\n    x_array = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    swish_array = np.vectorize(swish)(x_array)\n    print(\"Swish values for the array:\", swish_array)\n```\n\nIn this code:\n\n1.  We define the `swish` function, which takes a float input `x` and returns the Swish activation value.\n2.  Inside the `swish` function, we compute the sigmoid of `x` using the formula `1 / (1 + exp(-x))`.\n3.  We then calculate the Swish value by multiplying `x` with its sigmoid.\n4.  The result is rounded to the nearest 4th decimal using the `round` function.\n5.  In the example usage section, we demonstrate the `swish` function with a single value and a numpy array.\n\nTo apply the Swish activation function to a numpy array, we use `np.vectorize` to apply the `swish` function element-wise to the array.\n\n**Swish Activation Function for Numpy Arrays**\n\nIf you need to apply the Swish activation function to large numpy arrays, you can implement a vectorized version of the Swish function for better performance:\n\n```python\ndef swish_vectorized(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Vectorized implementation of the Swish activation function for numpy arrays.\n\n    Args:\n        x: Input numpy array\n\n    Returns:\n        The Swish activation values for the input array\n    \"\"\"\n    sigmoid_x = 1 / (1 + np.exp(-x))\n    swish_values = x * sigmoid_x\n    return np.round(swish_values, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    x_array = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    swish_array = swish_vectorized(x_array)\n    print(\"Swish values for the array:\", swish_array)\n```\n\nThis vectorized implementation is more efficient for large numpy arrays."}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    # If x is greater than or equal to 0, return scale * x\n    if x >= 0:\n        return round(scale * x, 4)\n    # If x is less than 0, return scale * alpha * (e^x - 1)\n    else:\n        return round(scale * alpha * (math.exp(x) - 1), 4)\n\n\n# Example usage:\nprint(selu(1.0))   # Output: 1.0507\nprint(selu(-1.0))  # Output: -1.1113\nprint(selu(0.0))   # Output: 0.0000\n\n# Vectorized version for numpy arrays\ndef selu_vectorized(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Vectorized version of SELU activation function for numpy arrays.\n\n    Args:\n        x: Input numpy array\n\n    Returns:\n        SELU activation values\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    return scale * np.where(x >= 0, x, alpha * (np.exp(x) - 1)).round(4)\n\n\n# Example usage with numpy array:\narr = np.array([1.0, -1.0, 0.0])\nprint(selu_vectorized(arr))  # Output: [ 1.0507 -1.1113  0.    ]\n```"}
{"task_id": 104, "completion_id": 0, "completion": "## Binary Classification with Logistic Regression\n### Task Description\nThe task is to implement a prediction function for binary classification using Logistic Regression. The function should compute class probabilities using the sigmoid function and return binary predictions based on a threshold of 0.5.\n\n### Implementation\n\n```python\nimport numpy as np\n\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the sigmoid of z.\n\n    Args:\n        z: Input to the sigmoid function.\n\n    Returns:\n        Sigmoid of z.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of X, weights, and bias\n    z = np.dot(X, weights) + bias\n    \n    # Compute the probabilities using the sigmoid function\n    probabilities = sigmoid(z)\n    \n    # Make binary predictions based on a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n    \n    # Return the predictions as a list\n    return predictions.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    X = np.random.rand(5, 3)  # 5 samples, 3 features\n    weights = np.random.rand(3)  # 3 weights\n    bias = np.random.rand()  # 1 bias\n    \n    # Make predictions\n    predictions = predict_logistic(X, weights, bias)\n    print(\"Predictions:\", predictions)\n```\n\n### Explanation\n\n1. The `sigmoid` function computes the sigmoid of the input `z`.\n2. The `predict_logistic` function takes in the input feature matrix `X`, model weights `weights`, and model bias `bias`.\n3. It computes the linear combination of `X`, `weights`, and `bias` using `np.dot(X, weights) + bias`.\n4. The sigmoid function is applied to the linear combination to obtain the probabilities.\n5. Binary predictions are made by thresholding the probabilities at 0.5.\n6. The predictions are returned as a list using the `tolist()` method.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `predict_logistic` function with some random data. The output will be a list of binary predictions."}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Get the number of samples, features, and classes\n    N, M = X.shape\n    C = np.max(y) + 1\n    \n    # Initialize the parameter matrix B with zeros\n    B = np.zeros((C, M))\n    \n    # Initialize the list to store the Cross Entropy losses\n    losses = []\n    \n    # One-hot encode the target variable y\n    y_onehot = np.eye(C)[y]\n    \n    # Gradient descent iterations\n    for _ in range(iterations):\n        # Compute the scores\n        scores = np.dot(X, B.T)\n        \n        # Compute the softmax probabilities\n        probs = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs /= np.sum(probs, axis=1, keepdims=True)\n        \n        # Compute the Cross Entropy loss\n        loss = -np.mean(np.sum(y_onehot * np.log(probs + 1e-15), axis=1))\n        losses.append(round(loss, 4))\n        \n        # Compute the gradient of the loss with respect to B\n        dB = np.dot((probs - y_onehot).T, X) / N\n        \n        # Update the parameter matrix B\n        B -= learning_rate * dB\n    \n    # Return the optimized coefficients and collected losses\n    return B.round(4).tolist(), losses\n```\n\n### Example Usage:\n```python\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    y = np.random.randint(0, 3, 100)\n    \n    # Train the Softmax regression model\n    learning_rate = 0.1\n    iterations = 100\n    B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n    \n    # Print the results\n    print(\"Optimized Coefficients:\")\n    print(B)\n    print(\"Collected Losses:\")\n    print(losses)\n```\n\nThis code implements a gradient descent-based training algorithm for Softmax regression. It takes in the feature matrix `X`, target variable `y`, learning rate, and number of iterations as inputs. The function returns the optimized coefficients `B` and the collected Cross Entropy losses over the iterations. The results are rounded to the nearest 4th decimal place and returned as Python lists. The example usage demonstrates how to train a Softmax regression model on some random data and print the optimized coefficients and collected losses."}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the sigmoid of x.\n    \n    Args:\n    x (np.ndarray): Input array.\n    \n    Returns:\n    np.ndarray: Sigmoid of x.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Args:\n    X (np.ndarray): Feature matrix.\n    y (np.ndarray): Target vector.\n    learning_rate (float): Learning rate for gradient descent.\n    iterations (int): Number of iterations for gradient descent.\n    \n    Returns:\n    tuple[list[float], ...]: A tuple containing the optimized coefficients and the collected loss values over iterations.\n    \"\"\"\n    \n    # Add a column of ones to X for the bias term\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    \n    # Initialize coefficients with zeros\n    coefficients = np.zeros(X.shape[1])\n    \n    # Initialize lists to store the coefficients and loss values over iterations\n    coefficient_history = [coefficients.tolist()]\n    loss_history = []\n    \n    # Perform gradient descent for the specified number of iterations\n    for _ in range(iterations):\n        # Compute the predictions using the current coefficients\n        predictions = sigmoid(np.dot(X, coefficients))\n        \n        # Compute the Binary Cross Entropy loss\n        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n        loss = round(loss, 4)\n        loss_history.append(loss)\n        \n        # Compute the gradient of the loss with respect to the coefficients\n        gradient = np.dot(X.T, (predictions - y)) / X.shape[0]\n        \n        # Update the coefficients using the gradient and learning rate\n        coefficients -= learning_rate * gradient\n        \n        # Store the updated coefficients\n        coefficient_history.append(coefficients.tolist())\n    \n    # Return the optimized coefficients and the collected loss values\n    return (coefficient_history[-1], loss_history)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    X = np.random.rand(100, 5)\n    y = np.random.randint(0, 2, 100)\n    \n    # Train logistic regression using gradient descent\n    learning_rate = 0.1\n    iterations = 100\n    optimized_coefficients, loss_values = train_logreg(X, y, learning_rate, iterations)\n    \n    print(\"Optimized Coefficients:\", optimized_coefficients)\n    print(\"Loss Values:\", loss_values)\n```\nThe code implements a gradient descent-based training algorithm for logistic regression. It takes the feature matrix `X`, target vector `y`, learning rate, and number of iterations as input. The function `train_logreg` returns the optimized coefficients and the collected loss values over iterations. \n\nHere's a step-by-step breakdown:\n\n1.  The code starts by adding a column of ones to the feature matrix `X` to account for the bias term.\n2.  It initializes the coefficients with zeros and creates lists to store the coefficient history and loss values over iterations.\n3.  The code then performs gradient descent for the specified number of iterations. In each iteration, it computes the predictions using the current coefficients, calculates the Binary Cross Entropy loss, and updates the coefficients using the gradient and learning rate.\n4.  After each iteration, it stores the updated coefficients and loss value.\n5.  Finally, the function returns the optimized coefficients and the collected loss values.\n\nThe example usage demonstrates how to use the `train_logreg` function with some random data. It trains logistic regression using gradient descent and prints the optimized coefficients and loss values. \n\nThe results are rounded to the nearest 4th decimal as required. The coefficients and loss values are returned as Python lists after reshaping using NumPy's `tolist()` method."}
{"task_id": 107, "completion_id": 0, "completion": "## Masked Self-Attention Implementation\n### Overview\n\nThe task is to implement masked self-attention, a variation of the attention mechanism used in sequence modeling tasks such as text generation. We will compute masked self-attention using query (Q), key (K), value (V) matrices and an attention mask.\n\n### Code Implementation\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Compute attention scores\n    attention_scores = np.matmul(Q, K.T) / np.sqrt(Q.shape[-1])\n    \n    # Apply attention mask\n    attention_scores = np.where(mask == 0, -np.inf, attention_scores)\n    \n    # Compute attention weights\n    attention_weights = np.softmax(attention_scores, axis=-1)\n    \n    # Compute output\n    output = np.matmul(attention_weights, V)\n    \n    # Reshape output to a list\n    output_list = output.tolist()\n    \n    return output_list\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define input and weight matrices\n    X = np.random.rand(1, 10, 512)  # batch_size, sequence_length, embedding_dim\n    W_q = np.random.rand(512, 64)   # embedding_dim, attention_dim\n    W_k = np.random.rand(512, 64)   # embedding_dim, attention_dim\n    W_v = np.random.rand(512, 64)   # embedding_dim, attention_dim\n    \n    # Compute Q, K, V matrices\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    # Define attention mask\n    mask = np.random.randint(0, 2, size=(10, 10))  # sequence_length, sequence_length\n    \n    # Compute masked self-attention\n    output = masked_attention(Q.squeeze(0), K.squeeze(0), V.squeeze(0), mask)\n    \n    print(output)\n```\n\n### Explanation\n\n1. The `compute_qkv` function computes the Query (Q), Key (K), and Value (V) matrices by multiplying the input `X` with the corresponding weight matrices `W_q`, `W_k`, and `W_v`.\n2. The `masked_attention` function computes the masked self-attention.\n   - It first computes the attention scores by taking the dot product of `Q` and `K.T` and scaling by the square root of the attention dimension.\n   - It then applies the attention mask by replacing the attention scores corresponding to the masked positions with `-inf`.\n   - The attention weights are computed by taking the softmax of the attention scores along the last axis.\n   - The output is computed by taking the dot product of the attention weights and `V`.\n   - Finally, the output is reshaped to a list using the `tolist()` method.\n3. In the example usage, we demonstrate how to use the `compute_qkv` and `masked_attention` functions to compute masked self-attention. We define random input and weight matrices, compute the Q, K, V matrices, define an attention mask, and compute the masked self-attention output."}
{"task_id": 108, "completion_id": 0, "completion": "# Problem Understanding\nThe task is to implement a function that calculates the disorder in a basket of apples based on their colors, where each apple color is represented by an integer. The disorder should be 0 if all apples are the same color and should increase as the variety of colors increases.\n\nKey constraints and edge cases:\n- The disorder should be 0 if all apples are the same color.\n- The disorder should be higher for [1,1,0,0] than for [0,0,0,0].\n- The disorder should be higher for [0,1,2,3] than for [1,1,0,0].\n- The disorder should be higher for [0,0,1,1,2,2,3,3] than for [0,0,0,0,0,1,2,3].\n- The input is a list of integers representing apple colors.\n- The output is a float representing the disorder, rounded to the nearest 4th decimal.\n\nSubtle requirements or implicit constraints:\n- The disorder measure should be able to handle different numbers of unique colors and their distributions.\n\n# Key Insights\nCrucial observations that lead to the solution:\n- The disorder is related to the variety of colors in the list of apples.\n- The entropy of the distribution of apple colors can be used to measure the disorder.\n- Entropy is a measure of the randomness or diversity in a distribution.\n- If all apples are the same color, the entropy is 0.\n- As the variety of colors increases, the entropy should also increase.\n- Shannon's entropy formula can be used to calculate the entropy of the distribution of apple colors.\n- The formula for Shannon's entropy is \\(H = -\\sum_{i=1}^{n} p_i \\log_2(p_i)\\), where \\(p_i\\) is the probability of the i-th outcome (in this case, the i-th color).\n\nImportant realizations:\n- The probability of each color can be calculated by counting the occurrences of each unique color and dividing by the total number of apples.\n- The entropy can be calculated using the probabilities of the colors.\n\nCritical examples or test cases:\n- [0,0,0,0] should yield 0.\n- [1,1,0,0] should have a higher disorder than [0,0,0,0].\n- [0,1,2,3] should have a higher disorder than [1,1,0,0].\n- [0,0,1,1,2,2,3,3] should have a higher disorder than [0,0,0,0,0,1,2,3].\n\n# Solution Strategy\nSteps to solve this problem:\n1. Count the occurrences of each unique color in the list of apples.\n2. Calculate the probability of each color by dividing its count by the total number of apples.\n3. Use Shannon's entropy formula to calculate the entropy of the distribution of apple colors.\n4. If there are no apples, return 0.\n5. Round the calculated entropy to the nearest 4th decimal.\n\nImportant optimizations or special cases to handle:\n- Handle the case where there are no apples.\n- Use the natural logarithm (base e) instead of base 2 to allow for more flexibility in the entropy calculation.\n- Ensure that the output is rounded to the nearest 4th decimal.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    \n    # Count occurrences of each color\n    counts = {}\n    for color in apples:\n        if color in counts:\n            counts[color] += 1\n        else:\n            counts[color] = 1\n    \n    # Calculate probabilities\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in counts.values()]\n    \n    # Calculate entropy\n    entropy = -sum([p * np.log(p) for p in probabilities if p > 0])\n    \n    # Return the entropy rounded to 4 decimal places\n    return round(entropy, 4)\n```"}
{"task_id": 109, "completion_id": 0, "completion": "### Layer Normalization Implementation\n#### Overview\n\nThe following code implements a function to perform Layer Normalization on an input 3D tensor `X`, representing batch size, sequence length, and feature dimensions. The normalization is done across the feature dimension for each sequence, followed by the application of scaling (`gamma`) and shifting (`beta`) parameters.\n\n#### Code\n\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Applies Layer Normalization to the input data X.\n\n    Args:\n    - X (np.ndarray): Input 3D array of shape (batch_size, sequence_length, feature_dim).\n    - gamma (np.ndarray): Scaling parameter of shape (feature_dim,).\n    - beta (np.ndarray): Shifting parameter of shape (feature_dim,).\n    - epsilon (float, optional): Small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    - normalized_X (list): Normalized X, rounded to 5 decimal places, and converted to a list.\n    \"\"\"\n\n    # Calculate mean across the feature dimension for each sequence\n    mean = np.mean(X, axis=-1, keepdims=True)\n    \n    # Calculate variance across the feature dimension for each sequence\n    variance = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalize X\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n    \n    # Apply scaling and shifting parameters\n    normalized_X = gamma * normalized_X + beta\n    \n    # Round to 5 decimal places and convert to list\n    normalized_X = np.round(normalized_X, 5).tolist()\n    \n    return normalized_X\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample input data\n    X = np.random.rand(2, 3, 4)  # batch_size=2, sequence_length=3, feature_dim=4\n    \n    # Sample gamma and beta\n    gamma = np.ones(4)  # feature_dim=4\n    beta = np.zeros(4)  # feature_dim=4\n    \n    # Apply Layer Normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n    \n    print(normalized_X)\n```\n\n#### Explanation\n\n1. The function `layer_normalization` takes in the input data `X`, scaling parameter `gamma`, shifting parameter `beta`, and an optional `epsilon` for numerical stability.\n2. It calculates the mean and variance of `X` across the feature dimension (last axis) for each sequence.\n3. The input `X` is then normalized using the calculated mean and variance.\n4. The normalized `X` is scaled and shifted using `gamma` and `beta`, respectively.\n5. Finally, the result is rounded to 5 decimal places and converted to a list before being returned.\n\nThe example usage demonstrates how to apply the `layer_normalization` function to sample input data `X` with corresponding `gamma` and `beta` parameters."}
{"task_id": 110, "completion_id": 0, "completion": "## METEOR Score Calculation Function\n### Overview\n\nThe METEOR score is a metric used to evaluate the quality of machine translation. It is based on the harmonic mean of unigram precision and recall, with a penalty for word order fragmentation.\n\n### Implementation\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Args:\n    - reference (str): The reference translation.\n    - candidate (str): The candidate translation.\n    - alpha (float, optional): The parameter for calculating the F-mean. Defaults to 0.9.\n    - beta (float, optional): The parameter for calculating the penalty. Defaults to 3.\n    - gamma (float, optional): The parameter for calculating the penalty. Defaults to 0.5.\n\n    Returns:\n    - float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n\n    # Tokenize the reference and candidate translations\n    reference_tokens = reference.split()\n    candidate_tokens = candidate.split()\n\n    # Calculate unigram matches\n    reference_counter = Counter(reference_tokens)\n    candidate_counter = Counter(candidate_tokens)\n    matches = sum(min(reference_counter[token], candidate_counter[token]) for token in candidate_counter)\n\n    # Calculate precision and recall\n    precision = matches / len(candidate_tokens) if candidate_tokens else 0\n    recall = matches / len(reference_tokens) if reference_tokens else 0\n\n    # Calculate F-mean\n    f_mean = (precision * recall) / (alpha * precision + (1 - alpha) * recall) if precision + recall > 0 else 0\n\n    # Calculate the penalty for word order fragmentation\n    m = matches\n    ch = 0\n    prev_match = False\n    for token in candidate_tokens:\n        if token in reference_counter and reference_counter[token] > 0:\n            if not prev_match:\n                ch += 1\n            prev_match = True\n            reference_counter[token] -= 1\n        else:\n            prev_match = False\n    penalty = gamma * (ch / m) ** beta if m > 0 else 0\n\n    # Calculate the METEOR score\n    score = f_mean * (1 - penalty)\n\n    return round(score, 3)\n\n# Example usage:\nreference = \"The cat sat on the mat\"\ncandidate = \"The cat is sitting on the mat\"\nprint(meteor_score(reference, candidate))\n```\n\n### Explanation\n\n1.  The function `meteor_score` takes in the reference translation, candidate translation, and optional parameters `alpha`, `beta`, and `gamma`.\n2.  It tokenizes the reference and candidate translations into individual words.\n3.  The function calculates unigram matches by counting the common words between the reference and candidate translations.\n4.  It then computes precision and recall based on the unigram matches.\n5.  The F-mean is calculated using the precision and recall values, with `alpha` as a parameter.\n6.  A penalty is calculated based on the word order fragmentation, using `beta` and `gamma` as parameters.\n7.  Finally, the METEOR score is computed as the product of the F-mean and (1 - penalty), and returned rounded to 3 decimal places.\n\n### Advice\n\n*   You can adjust the values of `alpha`, `beta`, and `gamma` to fine-tune the METEOR score calculation based on your specific requirements.\n*   The METEOR score is sensitive to the tokenization of the reference and candidate translations. Ensure that the tokenization is consistent and accurate."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    This function computes the Pointwise Mutual Information (PMI) given the joint occurrence count of two events, \n    their individual counts, and the total number of samples.\n\n    Args:\n    joint_counts (int): The joint occurrence count of two events.\n    total_counts_x (int): The total count of event x.\n    total_counts_y (int): The total count of event y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The Pointwise Mutual Information rounded to 3 decimal places.\n    \"\"\"\n\n    # First, we need to compute the probability of the joint occurrence of the two events\n    # We do this by dividing the joint count by the total number of samples\n    prob_joint = joint_counts / total_samples\n    \n    # If the joint probability is zero, PMI is undefined, so we return 0\n    if prob_joint == 0:\n        return 0.0\n    \n    # Next, we compute the individual probabilities of the two events\n    # We do this by dividing their individual counts by the total number of samples\n    prob_x = total_counts_x / total_samples\n    prob_y = total_counts_y / total_samples\n    \n    # Now, we can compute the PMI using the formula: PMI = log2(P(x,y) / (P(x) * P(y)))\n    # We use numpy's log2 function to compute the logarithm to the base 2\n    pmi = np.log2(prob_joint / (prob_x * prob_y))\n    \n    # Finally, we return the PMI rounded to 3 decimal places\n    return round(pmi, 3)\n```"}
{"task_id": 112, "completion_id": 0, "completion": "# Step-by-step analysis of the problem:\n1. **Understanding Min-Max Normalization**: Min-Max normalization is a technique used to scale numeric data to a common range, usually between 0 and 1, to prevent features with large ranges from dominating the model.\n2. **Identifying the Minimum and Maximum Values**: To perform Min-Max normalization, we first need to identify the minimum and maximum values in the given list of integers.\n3. **Applying the Min-Max Normalization Formula**: The formula for Min-Max normalization is given by: `(x - min) / (max - min)`, where `x` is the value to be normalized, `min` is the minimum value in the list, and `max` is the maximum value in the list.\n4. **Handling Edge Cases**: We need to handle the case where `max` equals `min` to avoid division by zero. In such cases, we can return a list with a single value (e.g., 0.0) for all elements.\n5. **Rounding the Results**: Finally, we need to round the normalized values to 4 decimal places as per the output constraints.\n\n# Fixed solution:\n```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    This function performs Min-Max Normalization on a list of integers, \n    scaling all values to the range [0, 1] and returns a list of floats \n    rounded to 4 decimal places.\n\n    Args:\n        x (list[int]): A list of integers to be normalized.\n\n    Returns:\n        list[float]: A list of normalized floats rounded to 4 decimal places.\n    \"\"\"\n\n    # Check if the input list is empty\n    if not x:\n        return []\n\n    # Find the minimum and maximum values in the list\n    min_val = min(x)\n    max_val = max(x)\n\n    # Handle the edge case where max equals min\n    if max_val == min_val:\n        return [0.0] * len(x)\n\n    # Apply the Min-Max normalization formula\n    normalized = [(i - min_val) / (max_val - min_val) for i in x]\n\n    # Round the normalized values to 4 decimal places\n    normalized = [round(val, 4) for val in normalized]\n\n    return normalized\n```\n\n# Explanation of changes:\n*   We added a check to handle the case where the input list is empty.\n*   We identified the minimum and maximum values in the list using the built-in `min()` and `max()` functions.\n*   We applied the Min-Max normalization formula using a list comprehension.\n*   We handled the edge case where `max` equals `min` by returning a list with a single value (0.0) for all elements.\n*   We rounded the normalized values to 4 decimal places using a list comprehension and the `round()` function.\n\n# Tests and example uses:\n```python\n# Test case 1: Normalization of a list with varying values\nprint(min_max([1, 2, 3, 4, 5]))  # Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n\n# Test case 2: Normalization of a list with identical values\nprint(min_max([5, 5, 5, 5, 5]))  # Output: [0.0, 0.0, 0.0, 0.0, 0.0]\n\n# Test case 3: Normalization of an empty list\nprint(min_max([]))  # Output: []\n\n# Test case 4: Normalization of a list with negative values\nprint(min_max([-5, -2, 0, 2, 5]))  # Output: [0.0, 0.3, 0.5, 0.7, 1.0]\n```"}
{"task_id": 113, "completion_id": 0, "completion": "### Implementing a Simple Residual Block with Shortcut Connection\n\nHere's a Python function that implements a simple residual block using NumPy. The function takes a 1D input array `x` and two weight matrices `w1` and `w2` as inputs.\n\n```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    This function implements a simple residual block using NumPy.\n    \n    Parameters:\n    x (np.ndarray): 1D input array.\n    w1 (np.ndarray): Weight matrix for the first layer.\n    w2 (np.ndarray): Weight matrix for the second layer.\n    \n    Returns:\n    list: The output of the residual block after final ReLU activation, rounded to 4 decimal places, and converted to a list.\n    \"\"\"\n\n    # First weight layer (matrix multiplication) followed by ReLU activation\n    hidden_layer = np.maximum(np.dot(x, w1), 0)  # ReLU activation\n    \n    # Second weight layer (matrix multiplication) followed by ReLU activation\n    output_layer = np.dot(hidden_layer, w2)\n    \n    # Add the original input via a shortcut connection\n    output_layer += x\n    \n    # Final ReLU activation\n    output_layer = np.maximum(output_layer, 0)\n    \n    # Round the output to 4 decimal places and convert it to a list\n    output = np.round(output_layer, 4).tolist()\n    \n    return output\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define the input array and weight matrices\n    x = np.array([1.0, 2.0, 3.0])\n    w1 = np.random.rand(3, 3)\n    w2 = np.random.rand(3, 3)\n    \n    # Call the residual_block function\n    result = residual_block(x, w1, w2)\n    print(result)\n```\n\n### Code Explanation:\n\n1.  The function `residual_block` takes three parameters: `x`, `w1`, and `w2`, representing the input array and two weight matrices, respectively.\n2.  It applies the first weight layer using matrix multiplication (`np.dot(x, w1)`) followed by ReLU activation (`np.maximum(..., 0)`).\n3.  The output from the first layer is then processed through the second weight layer (`np.dot(hidden_layer, w2)`).\n4.  The original input `x` is added to the output of the second layer via a shortcut connection (`output_layer += x`).\n5.  The result is then passed through a final ReLU activation (`np.maximum(output_layer, 0)`).\n6.  Finally, the output is rounded to 4 decimal places (`np.round(..., 4)`) and converted to a list (`tolist()`) before being returned.\n\nThe example usage demonstrates how to call the `residual_block` function with a sample input array and randomly generated weight matrices."}
{"task_id": 114, "completion_id": 0, "completion": "## Implementing Global Average Pooling\n### Task Description\nThe task is to implement a function that performs Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer.\n\n### Code Implementation\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array representing feature maps.\n\n    Args:\n    x (np.ndarray): A 3D NumPy array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: A 1D NumPy array of shape (channels,), where each element is the average of all values in the corresponding feature map.\n    \"\"\"\n\n    # Check if the input is a 3D NumPy array\n    assert len(x.shape) == 3, \"Input must be a 3D NumPy array\"\n\n    # Calculate the average along the height and width axes\n    # np.mean is used with axis=(0, 1) to average along the first two dimensions (height and width)\n    pooled = np.mean(x, axis=(0, 1))\n\n    return pooled\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample 3D NumPy array\n    height, width, channels = 5, 5, 3\n    x = np.random.rand(height, width, channels)\n\n    # Perform Global Average Pooling\n    pooled = global_avg_pool(x)\n\n    print(\"Input Shape:\", x.shape)\n    print(\"Output Shape:\", pooled.shape)\n    print(\"Pooled Values:\", pooled)\n```\n\n### Explanation\n1. The function `global_avg_pool` takes a 3D NumPy array `x` as input and checks if it's indeed 3D.\n2. It uses `np.mean` with `axis=(0, 1)` to calculate the average along the height and width axes, effectively performing Global Average Pooling.\n3. The result is a 1D NumPy array where each element is the average of the corresponding feature map in the input.\n4. The example usage demonstrates how to use the `global_avg_pool` function with a sample 3D NumPy array and prints the input shape, output shape, and the pooled values."}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    This function performs Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format.\n\n    Args:\n    X (np.ndarray): A 4D NumPy array representing a batch of feature maps in the BCHW format.\n    gamma (np.ndarray): A 1D NumPy array representing the scale parameters for each channel.\n    beta (np.ndarray): A 1D NumPy array representing the shift parameters for each channel.\n    epsilon (float): A small value added to the denominator for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    list: The batch normalized output, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n\n    # Calculate the mean across the batch and spatial dimensions for each channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Calculate the variance across the batch and spatial dimensions for each channel\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize the input by subtracting the mean and dividing by the square root of variance plus epsilon\n    normalized_X = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Apply scale (gamma) and shift (beta) parameters to the normalized input\n    # We need to reshape gamma and beta to match the shape of X for broadcasting\n    gamma_reshaped = gamma.reshape(1, -1, 1, 1)\n    beta_reshaped = beta.reshape(1, -1, 1, 1)\n    output = gamma_reshaped * normalized_X + beta_reshaped\n    \n    # Round the output to 4 decimal places and convert it to a list\n    output = np.round(output, 4).tolist()\n    \n    return output\n```"}
{"task_id": 116, "completion_id": 0, "completion": "## Task: Derivative of a Polynomial\n### Description\nThe task is to implement a function that computes the derivative of a polynomial term of the form `c * x^n` at a given point `x`, where `c` is a coefficient and `n` is the exponent.\n\n### Solution\nTo solve this problem, we will apply the power rule of differentiation, which states that if `f(x) = x^n`, then `f'(x) = n * x^(n-1)`. For a polynomial term `c * x^n`, the derivative is `c * n * x^(n-1)`.\n\n#### Code\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n\n    Args:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    # Apply the power rule of differentiation\n    derivative = c * n * (x ** (n - 1))\n    \n    # Return the derivative rounded to 4 decimal places\n    return round(derivative, 4)\n```\n\n### Example Usage\n```python\n# Test the function\nprint(poly_term_derivative(2, 3, 4))  # Derivative of 2 * x^4 at x = 3\n```\n\n### Explanation\n1. The function `poly_term_derivative` takes three parameters: `c`, `x`, and `n`, representing the coefficient, the point of evaluation, and the exponent, respectively.\n2. It applies the power rule to compute the derivative of the polynomial term `c * x^n` at `x`.\n3. The result is then rounded to 4 decimal places using the `round` function to meet the output constraint.\n4. The function includes a docstring that provides a description of its purpose, parameters, and return value, making it easier for users to understand how to use it."}
{"task_id": 117, "completion_id": 0, "completion": "## Step 1: Understand the Problem and the Gram-Schmidt Process\nThe task is to implement a function that computes an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process. The Gram-Schmidt process is a method for orthonormalizing a set of vectors. It takes a finite, linearly independent set of vectors and generates an orthonormal set of vectors that spans the same subspace.\n\n## Step 2: Initialize the Orthonormal Basis List\nWe start by initializing an empty list to store the orthonormal basis vectors.\n\n## Step 3: Implement the Gram-Schmidt Process\nFor each input vector, we will apply the Gram-Schmidt process. This involves subtracting the projection of the current vector onto each of the previously orthonormalized vectors. If the resulting vector is not zero (within a certain tolerance), we normalize it and add it to our orthonormal basis.\n\n## Step 4: Check for Linear Independence\nWe need to check if a vector is linearly independent from the previous vectors by checking if its norm is greater than a certain tolerance after orthogonalization.\n\n## Step 5: Normalize the Vectors\nAfter orthogonalizing a vector against all previous orthonormal vectors, we normalize it by dividing it by its norm.\n\n## Step 6: Round the Result to 4 Decimal Places\nFinally, we round each component of the orthonormal basis vectors to 4 decimal places before returning them.\n\n## Step 7: Write the Python Code\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors.\n\n    Args:\n    vectors (list[list[float]]): A list of 2D vectors.\n    tol (float, optional): Tolerance value to determine linear independence. Defaults to 1e-10.\n\n    Returns:\n    list[list[float]]: A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n    \n    # Convert input vectors to NumPy array for easier manipulation\n    vectors = np.array(vectors, dtype=float)\n    \n    # Initialize an empty list to store the orthonormal basis vectors\n    orthonormal_basis = []\n    \n    # Iterate over each input vector\n    for v in vectors:\n        # Initialize a variable to store the orthogonalized vector\n        orthogonalized_v = v.copy()\n        \n        # Subtract the projection of v onto each orthonormal basis vector\n        for basis_v in orthonormal_basis:\n            basis_v = np.array(basis_v)  # Convert to NumPy array for operations\n            orthogonalized_v = orthogonalized_v - np.dot(orthogonalized_v, basis_v) * basis_v\n        \n        # Check if the orthogonalized vector is linearly independent\n        norm = np.linalg.norm(orthogonalized_v)\n        if norm > tol:\n            # Normalize the orthogonalized vector and add it to the orthonormal basis\n            orthonormal_basis.append((orthogonalized_v / norm).tolist())\n    \n    # Round the orthonormal basis vectors to 4 decimal places\n    orthonormal_basis = [[round(x, 4) for x in v] for v in orthonormal_basis]\n    \n    return orthonormal_basis\n```\n\n## Step 8: Example Usage\nTo demonstrate the function's usage, we can call it with a list of 2D vectors and print the result.\n\n```python\n# Example usage\nvectors = [[1, 0], [1, 1]]\nprint(orthonormal_basis(vectors))\n```\n\nThe final answer is: \n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors.\n\n    Args:\n    vectors (list[list[float]]): A list of 2D vectors.\n    tol (float, optional): Tolerance value to determine linear independence. Defaults to 1e-10.\n\n    Returns:\n    list[list[float]]: A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n    \n    # Convert input vectors to NumPy array for easier manipulation\n    vectors = np.array(vectors, dtype=float)\n    \n    # Initialize an empty list to store the orthonormal basis vectors\n    orthonormal_basis = []\n    \n    # Iterate over each input vector\n    for v in vectors:\n        # Initialize a variable to store the orthogonalized vector\n        orthogonalized_v = v.copy()\n        \n        # Subtract the projection of v onto each orthonormal basis vector\n        for basis_v in orthonormal_basis:\n            basis_v = np.array(basis_v)  # Convert to NumPy array for operations\n            orthogonalized_v = orthogonalized_v - np.dot(orthogonalized_v, basis_v) * basis_v\n        \n        # Check if the orthogonalized vector is linearly independent\n        norm = np.linalg.norm(orthogonalized_v)\n        if norm > tol:\n            # Normalize the orthogonalized vector and add it to the orthonormal basis\n            orthonormal_basis.append((orthogonalized_v / norm).tolist())\n    \n    # Round the orthonormal basis vectors to 4 decimal places\n    orthonormal_basis = [[round(x, 4) for x in v] for v in orthonormal_basis]\n    \n    return orthonormal_basis\n```"}
{"task_id": 118, "completion_id": 0, "completion": "## Computing the Cross Product of Two 3D Vectors\n### Task Overview\n\nThe task is to implement a function `cross_product(a, b)` that computes the cross product of two 3-dimensional vectors `a` and `b`. The result should be a vector perpendicular to both `a` and `b`, following the right-hand rule.\n\n### Implementation\n\nWe will utilize the NumPy library, which provides an efficient and straightforward way to compute the cross product.\n\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n\n    Args:\n    a (numpy.ndarray): The first 3D vector.\n    b (numpy.ndarray): The second 3D vector.\n\n    Returns:\n    list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    a = np.array(a)\n    b = np.array(b)\n\n    # Validate input dimensions\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError(\"Both inputs must be 3-dimensional vectors\")\n\n    # Compute the cross product using numpy's cross function\n    result = np.cross(a, b)\n\n    # Round the result to 4 decimal places and convert it to a list\n    result = np.round(result, 4).tolist()\n\n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    vector_a = [1, 2, 3]\n    vector_b = [4, 5, 6]\n    print(cross_product(vector_a, vector_b))\n```\n\n### Explanation\n\n1. **Input Validation**: The function first checks if the input vectors `a` and `b` are 3-dimensional by verifying their shapes after conversion to NumPy arrays.\n2. **Cross Product Computation**: It then uses `np.cross(a, b)` to compute the cross product. NumPy's `cross` function is optimized for performance and handles the vector operation efficiently.\n3. **Result Formatting**: The result is rounded to 4 decimal places using `np.round(result, 4)`. Finally, it is converted to a list using the `tolist()` method before being returned.\n4. **Example Usage**: The code includes an example where two vectors `[1, 2, 3]` and `[4, 5, 6]` are used to demonstrate how to call the `cross_product` function and print the result.\n\nThis implementation is concise, readable, and well-documented, making it easy to understand and use for computing the cross product of 3D vectors."}
{"task_id": 119, "completion_id": 0, "completion": "## Step 1: Understand Cramer's Rule\nCramer's Rule is a method for solving systems of linear equations using determinants. For a system $Ax = b$, where $A$ is a square matrix, $x$ is the variable vector, and $b$ is the constant vector, the rule states that $x_i = \\frac{\\det(A_i)}{\\det(A)}$, where $A_i$ is the matrix formed by replacing the $i^{th}$ column of $A$ with $b$.\n\n## Step 2: Implement Cramer's Rule in Python\nTo implement Cramer's Rule, we first need to check if the determinant of $A$ is zero. If it is, the system does not have a unique solution, and we return -1. Otherwise, we calculate $x_i$ for each $i$.\n\n## Step 3: Calculate Determinant of A\nWe will use NumPy's `linalg.det` function to calculate the determinant of $A$.\n\n## Step 4: Replace Columns of A with b and Calculate Determinants\nFor each column $i$ of $A$, we replace it with $b$ to form $A_i$ and calculate $\\det(A_i)$.\n\n## Step 5: Apply Cramer's Rule\nWe apply Cramer's Rule by dividing $\\det(A_i)$ by $\\det(A)$ for each $i$ to find $x_i$.\n\n## Step 6: Round Results and Return\nFinally, we round the results to the nearest 4th decimal and return them as a Python list.\n\n## Step 7: Write the Python Function\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    # Convert inputs to NumPy arrays\n    A = np.array(A)\n    b = np.array(b)\n    \n    # Check if A is square\n    assert A.shape[0] == A.shape[1], \"Matrix A must be square\"\n    assert A.shape[0] == b.shape[0], \"Matrix A and vector b must have compatible dimensions\"\n    \n    # Calculate determinant of A\n    det_A = np.linalg.det(A)\n    \n    # Check if determinant is zero\n    if det_A == 0:\n        return -1\n    \n    # Initialize solution vector\n    x = np.zeros_like(b, dtype=float)\n    \n    # Apply Cramer's Rule\n    for i in range(A.shape[1]):\n        A_i = A.copy()\n        A_i[:, i] = b\n        x[i] = np.linalg.det(A_i) / det_A\n    \n    # Round results to 4 decimal places and return as list\n    return np.round(x, 4).tolist()\n```\n\n## Step 8: Example Usage\n```python\nA = [[1, 2], [3, 4]]\nb = [5, 11]\nprint(cramers_rule(A, b))\n```\n\nThe final answer is: \n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    # Convert inputs to NumPy arrays\n    A = np.array(A)\n    b = np.array(b)\n    \n    # Check if A is square\n    assert A.shape[0] == A.shape[1], \"Matrix A must be square\"\n    assert A.shape[0] == b.shape[0], \"Matrix A and vector b must have compatible dimensions\"\n    assert len(b.shape) == 1, \"Vector b must be a 1D array\"\n    \n    # Calculate determinant of A\n    det_A = np.linalg.det(A)\n    \n    # Check if determinant is zero\n    if det_A == 0:\n        return -1\n    \n    # Initialize solution vector\n    x = np.zeros_like(b, dtype=float)\n    \n    # Apply Cramer's Rule\n    for i in range(A.shape[1]):\n        A_i = A.copy()\n        A_i[:, i] = b\n        x[i] = np.linalg.det(A_i) / det_A\n    \n    # Round results to 4 decimal places and return as list\n    return np.round(x, 4).reshape(-1).tolist()\n```"}
{"task_id": 120, "completion_id": 0, "completion": "# Problem Understanding\nThe task is to implement a function to calculate the Bhattacharyya distance between two discrete probability distributions represented as lists `p` and `q`. The function should return the Bhattacharyya distance rounded to 4 decimal places. Key constraints and edge cases include:\n- The input lists `p` and `q` should have the same length.\n- The input lists should not be empty.\n- If the inputs have different lengths or are empty, the function should return 0.0.\n- The inputs represent discrete probability distributions, implying that their elements should be non-negative and sum up to 1.\n\nThe function takes two lists of floats as input and returns a float value representing the Bhattacharyya distance.\n\nA subtle requirement is that the result should be rounded to the nearest 4th decimal place.\n\n# Key Insights\n- The Bhattacharyya distance is a measure used to compare two probability distributions.\n- For discrete probability distributions, it involves summing the square root of the product of the probabilities of the two distributions for each outcome.\n- The formula for the Bhattacharyya distance is \\(D_B = -\\ln\\left(\\sum_{i=1}^{N} \\sqrt{p_i q_i}\\right)\\), where \\(p_i\\) and \\(q_i\\) are the probabilities of the i-th outcome in distributions `p` and `q`, respectively.\n- To calculate the Bhattacharyya distance, we first need to check if the input lists have the same length and are not empty.\n- We then compute the sum of the square root of the product of corresponding elements from `p` and `q`.\n- The Bhattacharyya distance is the negative logarithm of this sum.\n- We should handle the case where the sum inside the logarithm is zero or very close to zero, as the logarithm of zero is undefined.\n- The result should be rounded to 4 decimal places as per the task's requirements.\n- An example with `p = [0.1, 0.9]` and `q = [0.2, 0.8]` helps illustrate the calculation: \n  - The sum is \\(\\sqrt{0.1*0.2} + \\sqrt{0.9*0.8}\\).\n  - The Bhattacharyya distance is \\(-\\ln(\\sqrt{0.1*0.2} + \\sqrt{0.9*0.8})\\), rounded to 4 decimal places.\n\n# Solution Strategy\nTo solve this problem, we will:\n1. Check if the input lists `p` and `q` have the same length and are not empty. If not, return 0.0.\n2. Convert the input lists to numpy arrays for easier mathematical operations.\n3. Compute the sum of the square root of the product of corresponding elements from `p` and `q`. This involves calculating \\(\\sum_{i=1}^{N} \\sqrt{p_i q_i}\\).\n4. Check if the computed sum is greater than 0 to avoid taking the logarithm of zero or a negative number.\n5. If the sum is greater than 0, calculate the Bhattacharyya distance using the formula \\(D_B = -\\ln\\left(\\sum_{i=1}^{N} \\sqrt{p_i q_i}\\right)\\).\n6. Round the result to 4 decimal places as required.\n7. Return the rounded Bhattacharyya distance.\n\nHere's the python solution:\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    This function calculates the Bhattacharyya distance between two discrete probability distributions p and q.\n    \n    Args:\n    p (list[float]): The first discrete probability distribution.\n    q (list[float]): The second discrete probability distribution.\n    \n    Returns:\n    float: The Bhattacharyya distance between p and q rounded to 4 decimal places. If p and q have different lengths or are empty, returns 0.0.\n    \"\"\"\n    \n    # Check if both lists have the same length and are not empty\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    \n    # Convert the lists to numpy arrays for easier mathematical operations\n    p_array = np.array(p)\n    q_array = np.array(q)\n    \n    # Calculate the sum of the square root of the product of corresponding elements from p and q\n    sum_sqrt_pq = np.sum(np.sqrt(p_array * q_array))\n    \n    # Check if sum_sqrt_pq is greater than 0 to avoid log(0) which is undefined\n    if sum_sqrt_pq > 0:\n        # Calculate the Bhattacharyya distance\n        distance = -np.log(sum_sqrt_pq)\n        # Round the result to 4 decimal places\n        return round(distance, 4)\n    else:\n        # If sum_sqrt_pq is not greater than 0, return 0.0 as per the problem's constraints and handling edge cases\n        return 0.0\n```"}
{"task_id": 121, "completion_id": 0, "completion": "## Vector Element-wise Sum Function\n### Description\nThis function computes the element-wise sum of two input vectors `a` and `b`. It returns a new vector representing the sum if the operation is valid; otherwise, it returns `-1` to indicate that the vectors have incompatible dimensions.\n\n### Code\n```python\nfrom typing import Union, List\n\ndef vector_sum(a: List[Union[int, float]], b: List[Union[int, float]]) -> Union[List[Union[int, float]], int]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Args:\n    a (list[int|float]): The first vector.\n    b (list[int|float]): The second vector.\n\n    Returns:\n    list[int|float] or int: A new vector representing the element-wise sum if the operation is valid; -1 otherwise.\n    \"\"\"\n    # Check if the vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Use list comprehension to compute the element-wise sum\n    return [x + y for x, y in zip(a, b)]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test case 1: Vectors with the same length\n    vector1 = [1, 2, 3]\n    vector2 = [4, 5, 6]\n    print(vector_sum(vector1, vector2))  # Expected output: [5, 7, 9]\n\n    # Test case 2: Vectors with different lengths\n    vector3 = [1, 2, 3]\n    vector4 = [4, 5]\n    print(vector_sum(vector3, vector4))  # Expected output: -1\n```\n\n### Explanation\n\n1. The function `vector_sum` takes two lists `a` and `b` as input, representing the vectors to be summed.\n2. It first checks if the lengths of `a` and `b` are equal. If not, it returns `-1` to indicate incompatible dimensions.\n3. If the lengths are equal, it uses a list comprehension with `zip(a, b)` to iterate over the elements of `a` and `b` in parallel, computing their sum element-wise.\n4. The resulting sums are collected in a new list, which is returned as the result.\n\n### Notes\n\n* The function uses type hints to indicate that it accepts lists of integers or floats and returns either a list of integers or floats or an integer (`-1`).\n* The example usage demonstrates the function's behavior with both compatible and incompatible input vectors."}
{"task_id": 122, "completion_id": 0, "completion": "Here's the implementation of the policy gradient estimator using the REINFORCE algorithm.\n\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> list[float]:\n    \"\"\"\n    This function computes the average gradient of the log-policy multiplied by the return at each time step.\n\n    Args:\n    theta (np.ndarray): A 2D NumPy array of shape `(num_states, num_actions)` parameterizing the policy.\n    episodes (list[list[tuple[int, int, float]]]): A list of episodes, where each episode is a list of (state, action, reward) tuples.\n\n    Returns:\n    list[float]: The average gradient of the log-policy multiplied by the return at each time step, reshaped into a list.\n    \"\"\"\n\n    # Initialize the total gradient to zero\n    total_gradient = np.zeros_like(theta)\n\n    # Iterate over each episode\n    for episode in episodes:\n        # Compute the return at each time step\n        returns = np.zeros(len(episode))\n        returns[-1] = episode[-1][2]\n        for i in range(len(episode) - 2, -1, -1):\n            returns[i] = episode[i][2] + returns[i + 1]\n\n        # Iterate over each time step in the episode\n        for t, (state, action, _) in enumerate(episode):\n            # Compute the policy for the current state\n            policy = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n\n            # Compute the gradient of the log-policy for the current state and action\n            log_policy_grad = np.zeros_like(theta)\n            log_policy_grad[state, :] = -policy\n            log_policy_grad[state, action] += 1\n\n            # Multiply the gradient by the return at the current time step and add to the total gradient\n            total_gradient += log_policy_grad * returns[t]\n\n    # Compute the average gradient\n    avg_gradient = total_gradient / len(episodes)\n\n    # Round the result to the nearest 4th decimal and return as a list\n    return np.round(avg_gradient, 4).flatten().tolist()\n```\n\nHere's an example usage of the function:\n\n```python\n# Define the policy parameters\ntheta = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n\n# Define some example episodes\nepisodes = [\n    [(0, 1, 10.0), (1, 2, 20.0)],\n    [(0, 0, 5.0), (1, 1, 15.0)],\n]\n\n# Compute the policy gradient\npolicy_gradient = compute_policy_gradient(theta, episodes)\nprint(policy_gradient)\n```"}
{"task_id": 123, "completion_id": 0, "completion": "## Calculating Computational Efficiency of MoE Layer\n### Task Description\nThe task is to calculate the computational cost savings of a Mixture of Experts (MoE) layer compared to a dense layer. We need to compute the floating-point operations (FLOPs) for both layers and determine the savings percentage.\n\n### Code Implementation\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Args:\n    - n_experts (int): The total number of experts in the MoE layer.\n    - k_active (int): The number of active experts in the MoE layer.\n    - d_in (int): The input dimension of the layer.\n    - d_out (int): The output dimension of the layer.\n\n    Returns:\n    - savings_percentage (float): The percentage of computational cost savings.\n    \"\"\"\n\n    # Compute FLOPs for the dense layer\n    # A dense layer with input dimension d_in and output dimension d_out requires d_in * d_out multiplications and (d_in - 1) * d_out additions\n    # We assume that multiplication and addition have the same cost, so the total FLOPs is d_in * d_out + (d_in - 1) * d_out = 2 * d_in * d_out - d_out\n    dense_flops = 2 * d_in * d_out - d_out\n    \n    # Compute FLOPs for the MoE layer\n    # The MoE layer consists of two parts: the gating network and the expert networks\n    # The gating network is a dense layer with input dimension d_in and output dimension n_experts, so its FLOPs is 2 * d_in * n_experts - n_experts\n    # The expert networks are k_active dense layers with input dimension d_in and output dimension d_out, so their total FLOPs is k_active * (2 * d_in * d_out - d_out)\n    # Therefore, the total FLOPs for the MoE layer is 2 * d_in * n_experts - n_experts + k_active * (2 * d_in * d_out - d_out)\n    moe_flops = 2 * d_in * n_experts - n_experts + k_active * (2 * d_in * d_out - d_out)\n    \n    # Compute the savings percentage\n    # If the dense layer has zero FLOPs, the savings percentage is 0\n    if dense_flops == 0:\n        savings_percentage = 0\n    else:\n        savings_percentage = round(((dense_flops - moe_flops) / dense_flops) * 100, 1)\n    \n    return savings_percentage\n\n# Example usage:\nn_experts = 10\nk_active = 2\nd_in = 512\nd_out = 512\n\nsavings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"The MoE layer saves {savings_percentage}% computational cost compared to the dense layer.\")\n```\n\n### Explanation\n1. We first compute the FLOPs for the dense layer using the formula `2 * d_in * d_out - d_out`.\n2. Then, we compute the FLOPs for the MoE layer by summing the FLOPs of the gating network and the expert networks.\n3. The savings percentage is calculated by comparing the FLOPs of the MoE layer with the dense layer.\n4. The result is rounded to the nearest 1st decimal place as required.\n\n### Advice\n* Make sure to input valid values for `n_experts`, `k_active`, `d_in`, and `d_out` to get accurate results.\n* The code assumes that the input dimensions and output dimensions are the same for all experts in the MoE layer. If this is not the case, you may need to modify the code accordingly."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    This function implements the Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models.\n\n    Parameters:\n    X (np.ndarray): Input matrix.\n    W_g (np.ndarray): Weight matrix for gating.\n    W_noise (np.ndarray): Weight matrix for noise.\n    N (np.ndarray): Pre-sampled noise.\n    k (int): Sparsity constraint.\n\n    Returns:\n    list: Final gating probabilities matrix reshaped as a list, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    # Compute the gating scores by multiplying the input matrix with the gating weight matrix\n    gating_scores = np.matmul(X, W_g)\n    \n    # Compute the noise by multiplying the pre-sampled noise with the noise weight matrix\n    noise = np.matmul(N, W_noise)\n    \n    # Add the noise to the gating scores\n    noisy_gating_scores = gating_scores + noise\n    \n    # Compute the top k scores for each row\n    top_k_scores = np.sort(noisy_gating_scores, axis=-1)[:, -k:]\n    \n    # Create a mask to select the top k scores\n    mask = noisy_gating_scores >= np.min(top_k_scores, axis=-1, keepdims=True)\n    \n    # Compute the gating probabilities using the softmax function\n    gating_probabilities = np.exp(noisy_gating_scores) * mask / np.sum(np.exp(noisy_gating_scores) * mask, axis=-1, keepdims=True)\n    \n    # Replace NaN values with 0 (in case of division by zero)\n    gating_probabilities = np.nan_to_num(gating_probabilities)\n    \n    # Round the results to the nearest 4th decimal\n    gating_probabilities = np.round(gating_probabilities, 4)\n    \n    # Reshape the gating probabilities matrix to a list\n    gating_probabilities_list = gating_probabilities.tolist()\n    \n    return gating_probabilities_list\n```"}
{"task_id": 125, "completion_id": 0, "completion": "## Implementing a Sparse Mixture of Experts Layer\n### Overview\n\nThe following code implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing. The MoE layer is a type of neural network component that allows for conditionally selecting a subset of experts (in this case, linear transformations) to process an input.\n\n### Code\n\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Computes the output of a Mixture-of-Experts (MoE) layer.\n\n    Args:\n    - x (np.ndarray): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n    - We (np.ndarray): Expert weight matrices of shape (n_experts, embedding_dim, embedding_dim).\n    - Wg (np.ndarray): Gating weight matrix of shape (embedding_dim, n_experts).\n    - n_experts (int): Number of experts.\n    - top_k (int): Number of top experts to select per token.\n\n    Returns:\n    - A python list representing the MoE output after reshaping.\n    \"\"\"\n\n    # Compute gating scores\n    gating_scores = np.dot(x, Wg)  # Shape: (batch_size, sequence_length, n_experts)\n    \n    # Apply softmax to gating scores to obtain probabilities\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=-1, keepdims=True)\n    gating_probs = np.round(gating_probs, 4)  # Round to 4 decimal places\n    \n    # Get top-k expert indices and their corresponding probabilities\n    top_k_indices = np.argsort(-gating_probs, axis=-1)[:, :, :top_k]  # Shape: (batch_size, sequence_length, top_k)\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=-1)  # Shape: (batch_size, sequence_length, top_k)\n    \n    # Normalize top-k probabilities\n    top_k_probs = top_k_probs / np.sum(top_k_probs, axis=-1, keepdims=True)\n    top_k_probs = np.round(top_k_probs, 4)  # Round to 4 decimal places\n    \n    # Initialize MoE output\n    moe_output = np.zeros_like(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n    \n    # Iterate over top-k experts\n    for k in range(top_k):\n        # Get the current expert indices\n        expert_indices = top_k_indices[:, :, k]  # Shape: (batch_size, sequence_length)\n        \n        # Get the corresponding expert weights\n        expert_weights = We[expert_indices]  # Shape: (batch_size, sequence_length, embedding_dim, embedding_dim)\n        \n        # Compute the expert outputs\n        expert_outputs = np.einsum('bse,bsed->bsd', x, expert_weights)  # Shape: (batch_size, sequence_length, embedding_dim)\n        \n        # Aggregate the expert outputs weighted by the gating probabilities\n        moe_output += np.expand_dims(top_k_probs[:, :, k], -1) * expert_outputs\n    \n    # Round the final MoE output to 4 decimal places\n    moe_output = np.round(moe_output, 4)\n    \n    # Reshape and return as a python list\n    return moe_output.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    batch_size = 2\n    sequence_length = 3\n    embedding_dim = 4\n    n_experts = 5\n    top_k = 2\n    \n    x = np.random.rand(batch_size, sequence_length, embedding_dim)\n    We = np.random.rand(n_experts, embedding_dim, embedding_dim)\n    Wg = np.random.rand(embedding_dim, n_experts)\n    \n    output = moe(x, We, Wg, n_experts, top_k)\n    print(output)\n```\n\n### Explanation\n\n1.  The `moe` function takes in the input tensor `x`, expert weight matrices `We`, gating weight matrix `Wg`, number of experts `n_experts`, and the value of `k` (`top_k`) as inputs.\n2.  It computes the gating scores by taking the dot product of `x` and `Wg`.\n3.  The gating scores are then passed through a softmax function to obtain the gating probabilities.\n4.  The top-k expert indices and their corresponding probabilities are obtained by sorting the gating probabilities in descending order and selecting the top-k values.\n5.  The top-k probabilities are normalized to ensure they sum up to 1.\n6.  The MoE output is computed by iterating over the top-k experts, applying their transformations to the input, and aggregating the results weighted by the normalized gating probabilities.\n7.  Finally, the MoE output is rounded to 4 decimal places, reshaped, and returned as a python list.\n\nThe example usage demonstrates how to call the `moe` function with random inputs and print the resulting MoE output."}
{"task_id": 126, "completion_id": 0, "completion": "## Group Normalization Implementation\n### Overview\n\nGroup Normalization is a normalization technique used in deep learning to normalize the activations of a layer. It is similar to Batch Normalization but normalizes over smaller groups of channels instead of the entire batch.\n\n### Code Implementation\n\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    \"\"\"\n    Performs Group Normalization on a 4D input tensor.\n\n    Args:\n    - X (np.ndarray): Input tensor with shape (B, C, H, W)\n    - gamma (np.ndarray): Learned scale with shape (C,)\n    - beta (np.ndarray): Learned shift with shape (C,)\n    - num_groups (int): Number of groups to divide the channels into\n    - epsilon (float): Small value added for numerical stability (default=1e-5)\n\n    Returns:\n    - normalized_X (list): Normalized tensor reshaped and returned as a list, rounded to 4 decimal places\n    \"\"\"\n\n    # Get the shape of the input tensor\n    B, C, H, W = X.shape\n\n    # Check if the number of channels is divisible by the number of groups\n    assert C % num_groups == 0, \"Number of channels must be divisible by the number of groups\"\n\n    # Reshape the input tensor to (B, num_groups, C // num_groups, H, W)\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n\n    # Calculate the mean and variance over the group\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n\n    # Normalize the input tensor\n    normalized_X_reshaped = (X_reshaped - mean) / np.sqrt(var + epsilon)\n\n    # Reshape back to the original shape\n    normalized_X = normalized_X_reshaped.reshape(B, C, H, W)\n\n    # Apply the learned scale and shift\n    normalized_X = gamma[None, :, None, None] * normalized_X + beta[None, :, None, None]\n\n    # Round the result to 4 decimal places and return as a list\n    return np.round(normalized_X, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random input tensor\n    X = np.random.rand(2, 8, 3, 3)\n\n    # Create learned scale and shift\n    gamma = np.ones(8)\n    beta = np.zeros(8)\n\n    # Perform Group Normalization\n    num_groups = 2\n    result = group_normalization(X, gamma, beta, num_groups)\n\n    print(result)\n```\n\n### Explanation\n\n1. The function `group_normalization` takes in the input tensor `X`, learned scale `gamma`, learned shift `beta`, number of groups `num_groups`, and a small value `epsilon` for numerical stability.\n2. It first checks if the number of channels `C` is divisible by `num_groups`.\n3. The input tensor is reshaped to `(B, num_groups, C // num_groups, H, W)` to facilitate group-wise normalization.\n4. The mean and variance are calculated over the group (i.e., over the axes `(2, 3, 4)`).\n5. The input tensor is normalized using the calculated mean and variance.\n6. The normalized tensor is reshaped back to its original shape.\n7. The learned scale `gamma` and shift `beta` are applied to the normalized tensor.\n8. Finally, the result is rounded to 4 decimal places and returned as a list.\n\n### Example Usage\n\nThe example usage demonstrates how to use the `group_normalization` function with a random input tensor, learned scale, and shift. The result is printed to the console."}
{"task_id": 127, "completion_id": 0, "completion": "## Task: Find Captain Redbeard's Hidden Treasure\n### Description:\nThe task is to implement a Python function that finds the value of $x$ where the given function $f(x) = x^4 - 3x^3 + 2$ reaches its minimum, starting from any random initial position.\n\n### Implementation:\nTo find the minimum of the given function, we will use the gradient descent algorithm, a popular optimization technique. The gradient descent algorithm iteratively adjusts the parameter ($x$ in this case) in the direction of the negative gradient of the function to minimize it.\n\nFirst, we need to compute the derivative of $f(x)$ with respect to $x$. The derivative $f'(x)$ is given by:\n\\[f'(x) = 4x^3 - 9x^2\\]\n\nHere's the Python implementation:\n\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    This function finds the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum using gradient descent.\n\n    Args:\n    - start_x (float): The initial value of x to start the search from.\n    - learning_rate (float, optional): The step size for each gradient descent iteration. Defaults to 0.1.\n    - tolerance (float, optional): The convergence criterion. Defaults to 1e-6.\n    - max_iters (int, optional): The maximum number of iterations. Defaults to 10000.\n\n    Returns:\n    - float: The value of x where f(x) is minimum, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    # Initialize x with the given start_x\n    x = start_x\n    \n    # Perform gradient descent\n    for _ in range(max_iters):\n        # Compute the derivative of f(x) at the current x\n        gradient = 4 * x**3 - 9 * x**2\n        \n        # Update x using the gradient descent update rule\n        new_x = x - learning_rate * gradient\n        \n        # Check for convergence\n        if np.abs(new_x - x) < tolerance:\n            break\n        \n        # Update x for the next iteration\n        x = new_x\n    \n    # Return the result rounded to the nearest 4th decimal\n    return round(x, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    start_x = np.random.uniform(-10, 10)  # Random initial position between -10 and 10\n    treasure_x = find_treasure(start_x)\n    print(f\"Starting from x = {start_x:.4f}, the treasure is found at x = {treasure_x}\")\n```\n\n### Explanation:\n- The `find_treasure` function takes an initial `start_x` and optional parameters `learning_rate`, `tolerance`, and `max_iters` to control the gradient descent algorithm.\n- It iteratively updates `x` based on the gradient of $f(x)$ until convergence or until it reaches `max_iters`.\n- The result is rounded to the nearest 4th decimal before being returned.\n- An example usage is provided in the `if __name__ == \"__main__\":` block, demonstrating how to use the function with a random `start_x`."}
{"task_id": 128, "completion_id": 0, "completion": "## Dynamic Tanh: Normalization-Free Transformer Activation\n\nThe Dynamic Tanh (DyT) function is a normalization-free transformation inspired by the Tanh function. It replaces layer normalization in Transformer architectures while preserving squashing behavior and enabling stable training.\n\n### Implementation\n\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function, a normalization-free transformation inspired by the Tanh function.\n\n    Args:\n    x (np.ndarray): Input array.\n    alpha (float): Scaling factor.\n    gamma (float): Hyperparameter for DyT.\n    beta (float): Shift parameter.\n\n    Returns:\n    list[float]: DyT transformed values as a Python list, rounded to the nearest 4th decimal.\n    \"\"\"\n    # Apply the Dynamic Tanh function\n    dyt = alpha * np.tanh(gamma * x + beta)\n    \n    # Flatten the result to a 1D array\n    dyt_flat = dyt.flatten()\n    \n    # Round the results to the nearest 4th decimal and convert to a Python list\n    result = np.round(dyt_flat, 4).tolist()\n    \n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate a random input array\n    np.random.seed(0)\n    x = np.random.rand(3, 3)\n    print(\"Input array:\")\n    print(x)\n\n    alpha = 1.0\n    gamma = 1.0\n    beta = 0.0\n\n    result = dynamic_tanh(x, alpha, gamma, beta)\n    print(\"\\nDynamic Tanh transformed values:\")\n    print(result)\n```\n\n### Explanation\n\n1.  The `dynamic_tanh` function takes an input array `x` and three parameters: `alpha`, `gamma`, and `beta`.\n2.  It applies the Dynamic Tanh transformation using the formula `alpha * np.tanh(gamma * x + beta)`, where `np.tanh` is the hyperbolic tangent function from NumPy.\n3.  The result is flattened to a 1D array using the `flatten` method.\n4.  The values are rounded to the nearest 4th decimal using `np.round`.\n5.  Finally, the result is converted to a Python list using the `tolist` method and returned.\n\nThe example usage demonstrates how to call the `dynamic_tanh` function with a random input array and print the transformed values."}
