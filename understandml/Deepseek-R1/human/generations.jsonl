{"task_id": 1, "completion_id": 0, "completion": "To solve this problem, we need to compute the dot product of a matrix and a vector. The function should return the resulting vector if the operation is valid, or -1 if the dimensions are incompatible. The matrix and vector are compatible if the number of columns in the matrix equals the length of the vector.\n\n### Approach\n1. **Check for Empty Matrix**: If the matrix is empty, check if the vector is also empty. If both are empty, return an empty list. If the vector is not empty, return -1.\n2. **Check Row Consistency**: Ensure all rows in the matrix have the same number of columns. If any row has a different length, return -1.\n3. **Check Dimension Compatibility**: Verify that the number of columns in the matrix (from the first row) matches the length of the vector. If not, return -1.\n4. **Compute Dot Product**: For each row in the matrix, compute the dot product with the vector and store the result in a list.\n\n### Solution Code\n```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float] | int:\n    if not a:\n        # Handle empty matrix case\n        return [] if not b else -1\n    # Determine the number of columns from the first row\n    num_columns = len(a[0])\n    # Check all rows have the same length as the first row\n    for row in a:\n        if len(row) != num_columns:\n            return -1\n    # Check if the number of columns matches the vector's length\n    if num_columns != len(b):\n        return -1\n    # Compute the dot product for each row\n    result = []\n    for row in a:\n        dot_product = sum(x * y for x, y in zip(row, b))\n        result.append(dot_product)\n    return result\n```\n\n### Explanation\n1. **Empty Matrix Handling**: The function first checks if the matrix is empty. If it is, it checks if the vector is also empty. If both are empty, the result is an empty list. If the vector is non-empty, the operation is invalid, returning -1.\n2. **Row Consistency Check**: The function ensures all rows in the matrix have the same number of columns. This is necessary for the matrix to be valid and to proceed with the dot product calculation.\n3. **Dimension Compatibility Check**: The function checks if the number of columns in the matrix matches the vector's length. If not, the operation is invalid, returning -1.\n4. **Dot Product Calculation**: For each row in the matrix, the function computes the dot product with the vector by summing the products of corresponding elements. The results are collected into a list which is returned as the final result.\n\nThis approach ensures that all edge cases and invalid inputs are handled correctly, providing a robust solution to the matrix-vector dot product problem."}
{"task_id": 2, "completion_id": 0, "completion": "To solve this problem, we need to compute the transpose of a given matrix. The transpose of a matrix is obtained by converting its rows into columns and vice versa. For example, the element at position (i, j) in the original matrix will be at position (j, i) in the transposed matrix.\n\n### Approach\nThe approach to solve this problem involves using Python's built-in `zip` function, which allows us to transpose the matrix efficiently. The `zip` function pairs elements from each row (sub-list) of the input matrix, effectively converting rows into columns. Each tuple generated by `zip` is then converted into a list to maintain the required output format of a list of lists.\n\n### Solution Code\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    return [list(row) for row in zip(*a)]\n```\n\n### Explanation\n1. **Using `zip(*a)`**: The `zip` function takes the elements of each row (sub-list) of the input matrix `a` and groups them by their positions. For example, if the input matrix is `[[1, 2, 3], [4, 5, 6]]`, `zip(*a)` will generate tuples `(1, 4)`, `(2, 5)`, and `(3, 6)`.\n2. **Converting Tuples to Lists**: The result of `zip(*a)` is an iterator of tuples. Each tuple is converted to a list using a list comprehension to ensure the output is a list of lists, as required.\n3. **Handling Valid Matrices**: The problem assumes the input is a valid matrix where all rows have the same length. This approach handles such cases efficiently and correctly.\n\nThis method is both concise and efficient, leveraging Python's built-in capabilities to achieve the desired result with minimal code."}
{"task_id": 3, "completion_id": 0, "completion": "To solve this problem, we need to reshape a given matrix into a specified shape using Python and NumPy. If reshaping is not possible, we should return an empty list. The solution involves validating the input matrix and leveraging NumPy's reshaping capabilities.\n\n### Approach\n1. **Validate Input Matrix**: Check if the input matrix is valid, i.e., all rows have the same length. If not, return an empty list.\n2. **Reshape with NumPy**: Convert the valid matrix into a NumPy array and attempt to reshape it into the specified shape. If reshaping is not possible (due to incompatible dimensions), catch the error and return an empty list.\n3. **Convert to List**: If reshaping is successful, convert the reshaped NumPy array back to a Python list using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Check if all rows have the same length\n    original_rows = len(a)\n    if original_rows == 0:\n        original_cols = 0\n    else:\n        original_cols = len(a[0]) if a[0] else 0\n        for row in a:\n            if len(row) != original_cols:\n                return []\n    # Attempt to reshape using numpy\n    try:\n        np_array = np.array(a)\n        reshaped = np_array.reshape(new_shape)\n    except ValueError:\n        return []\n    return reshaped.tolist()\n```\n\n### Explanation\n1. **Validation of Input Matrix**: The function first checks if all rows in the input matrix have the same length. If any row has a different length, the matrix is considered invalid, and an empty list is returned immediately.\n2. **Conversion to NumPy Array**: The input matrix is converted into a NumPy array. This allows us to use NumPy's efficient reshaping functions.\n3. **Reshaping Attempt**: Using NumPy's `reshape` method, we attempt to reshape the array into the specified shape. If the specified shape is incompatible with the number of elements in the original matrix, a `ValueError` is raised, and we return an empty list.\n4. **Conversion to List**: If reshaping is successful, the reshaped NumPy array is converted back to a Python list using the `tolist()` method, ensuring the result is in the required format.\n\nThis approach efficiently handles both valid and invalid input cases, leveraging NumPy's capabilities to manage complex reshaping scenarios, including cases where one dimension is specified as -1 (allowing NumPy to infer the correct dimension)."}
{"task_id": 4, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that calculates the mean of a matrix either by row or by column based on a specified mode. The function should return a list of means corresponding to each row or column as required.\n\n### Approach\nThe solution involves two primary steps depending on the mode specified:\n1. **Row-wise Mean Calculation**: When the mode is 'row', we iterate through each row of the matrix, compute the mean of the elements in each row, and collect these means into a list.\n2. **Column-wise Mean Calculation**: When the mode is 'column', we first transpose the matrix to convert columns into rows. This allows us to reuse the row-wise mean calculation approach. We then compute the mean for each transposed row (which represents the original column) and collect these means into a list.\n\n### Solution Code\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        transposed = zip(*matrix)\n        return [sum(col) / len(col) for col in transposed]\n    else:\n        raise ValueError(\"Invalid mode. Mode must be 'row' or 'column'.\")\n```\n\n### Explanation\n1. **Row-wise Calculation**:\n   - For each row in the matrix, the sum of the elements is divided by the number of elements in that row to compute the mean.\n   - This is done using a list comprehension that iterates over each row and calculates the mean directly.\n\n2. **Column-wise Calculation**:\n   - The matrix is transposed using `zip(*matrix)`, which converts columns into rows. For example, a matrix `[[1, 2], [3, 4]]` becomes `[(1, 3), (2, 4)]` when transposed.\n   - The same row-wise mean calculation is then applied to each transposed row (original column), and the results are collected into a list.\n\n3. **Error Handling**:\n   - If the mode provided is neither 'row' nor 'column', a `ValueError` is raised to indicate an invalid mode.\n\nThis approach efficiently handles both row and column mean calculations by leveraging matrix transposition for columns, ensuring clean and concise code. The use of list comprehensions ensures that the solution is both readable and efficient."}
{"task_id": 5, "completion_id": 0, "completion": "To solve this problem, we need to create a function that multiplies each element of a matrix by a given scalar value and returns the resulting matrix. The matrix is represented as a list of lists, where each inner list is a row of the matrix. Each element in these rows can be either an integer or a float, and the scalar can also be an integer or a float.\n\n### Approach\nThe approach to solve this problem involves iterating through each element of the matrix and multiplying it by the scalar value. This can be efficiently done using nested list comprehensions. The outer list comprehension iterates over each row of the matrix, and the inner list comprehension iterates over each element in the row, multiplying it by the scalar. This approach ensures that each element in the matrix is processed and the resulting matrix is constructed in a concise and efficient manner.\n\n### Solution Code\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    return [[element * scalar for element in row] for row in matrix]\n```\n\n### Explanation\n- **List Comprehensions**: The solution uses nested list comprehensions to iterate over each row and each element within the row. This allows for a concise and readable implementation.\n- **Element-wise Multiplication**: Each element in the matrix is multiplied by the scalar value. This handles both integers and floats seamlessly, as Python automatically converts the result to a float when necessary.\n- **Efficiency**: The use of list comprehensions ensures that the solution is both time and space efficient, as it processes each element exactly once and constructs the resulting matrix in linear time relative to the number of elements in the input matrix.\n\nThis approach efficiently handles all edge cases, including empty matrices, zero scalar values, and matrices with mixed integer and float elements. The solution is both elegant and effective for the given problem."}
{"task_id": 6, "completion_id": 0, "completion": "To solve this problem, we need to calculate the eigenvalues of a 2x2 matrix and return them sorted in descending order. Eigenvalues are determined by solving the characteristic equation derived from the matrix, which involves computing the trace and determinant of the matrix.\n\n### Approach\n1. **Extract Matrix Elements**: Extract the elements of the 2x2 matrix.\n2. **Compute Trace and Determinant**: The trace is the sum of the diagonal elements, and the determinant is calculated as (a*d - b*c) for a matrix [[a, b], [c, d]].\n3. **Solve the Characteristic Equation**: The eigenvalues are found using the quadratic formula applied to the characteristic equation \u03bb\u00b2 - trace * \u03bb + determinant = 0. This involves computing the discriminant and its square root.\n4. **Calculate Eigenvalues**: Using the trace and the square root of the discriminant, compute the two eigenvalues.\n5. **Return Sorted Eigenvalues**: The eigenvalues are returned in descending order.\n\n### Solution Code\n```python\nimport math\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    \n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    \n    sqrt_discriminant = math.sqrt(discriminant)\n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n    \n    return [eigenvalue1, eigenvalue2]\n```\n\n### Explanation\n1. **Matrix Elements Extraction**: The elements of the matrix are extracted for easier manipulation.\n2. **Trace and Determinant Calculation**: The trace is the sum of the diagonal elements (a + d), and the determinant is computed as (a*d - b*c).\n3. **Discriminant Calculation**: The discriminant of the characteristic equation is calculated as (trace\u00b2 - 4*determinant). This determines the nature of the eigenvalues (real or complex).\n4. **Eigenvalues Calculation**: Using the quadratic formula, the eigenvalues are derived from the trace and the square root of the discriminant. The eigenvalues are (trace \u00b1 sqrt(discriminant)) / 2.\n5. **Sorting**: The eigenvalues are inherently sorted in descending order due to the properties of the quadratic formula, ensuring the first eigenvalue is always greater than or equal to the second.\n\nThis approach efficiently computes the eigenvalues for a 2x2 matrix, ensuring they are real and correctly sorted."}
{"task_id": 7, "completion_id": 0, "completion": "To solve this problem, we need to transform a given matrix \\( A \\) using the operation \\( T^{-1}AS \\), where \\( T \\) and \\( S \\) are invertible matrices. The transformation involves validating the invertibility of \\( T \\) and \\( S \\), ensuring the matrices' dimensions are compatible, and then performing the matrix multiplication in the specified order. \n\n### Approach\n1. **Check Square Matrices**: Verify that matrices \\( T \\) and \\( S \\) are square (i.e., the number of rows equals the number of columns).\n2. **Dimension Compatibility**: Ensure that matrix \\( A \\) has the same number of rows as \\( T \\) and the same number of columns as \\( S \\).\n3. **Invertibility Check**: Calculate the determinants of \\( T \\) and \\( S \\) to check if they are non-zero, which is a necessary condition for invertibility.\n4. **Matrix Inversion and Multiplication**: Compute the inverse of \\( T \\), then perform the matrix multiplications \\( T^{-1} \\times A \\) followed by \\( (T^{-1} \\times A) \\times S \\).\n5. **Rounding and Conversion**: Round the resulting matrix to four decimal places and convert it back to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Convert input matrices to numpy arrays\n    T_np = np.array(T, dtype=np.float64)\n    S_np = np.array(S, dtype=np.float64)\n    A_np = np.array(A, dtype=np.float64)\n    \n    # Check if T and S are square matrices\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    \n    # Check dimensions compatibility\n    a_rows, a_cols = A_np.shape\n    t_size = T_np.shape[0]\n    s_size = S_np.shape[0]\n    \n    if a_rows != t_size or a_cols != s_size:\n        return -1\n    \n    # Check determinants to confirm invertibility\n    det_T = np.linalg.det(T_np)\n    det_S = np.linalg.det(S_np)\n    \n    if np.isclose(det_T, 0, atol=1e-8) or np.isclose(det_S, 0, atol=1e-8):\n        return -1\n    \n    # Compute T inverse\n    try:\n        T_inv = np.linalg.inv(T_np)\n    except np.linalg.LinAlgError:\n        return -1\n    \n    # Compute the product T^{-1} A S\n    try:\n        product = T_inv @ A_np @ S_np\n    except ValueError:\n        # This should not happen due to prior checks, but handle just in case\n        return -1\n    \n    # Round to 4 decimal places and convert to list of lists\n    rounded = np.round(product, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Square Matrix Check**: The function first checks if \\( T \\) and \\( S \\) are square matrices by comparing their rows and columns. If not, it returns -1.\n2. **Dimension Compatibility Check**: The function ensures that the number of rows in \\( A \\) matches the number of rows (and columns) in \\( T \\), and the number of columns in \\( A \\) matches the number of rows (and columns) in \\( S \\).\n3. **Determinant Check**: The determinants of \\( T \\) and \\( S \\) are computed. If either determinant is close to zero (considering floating-point precision), the matrices are not invertible, and the function returns -1.\n4. **Matrix Inversion and Multiplication**: If all checks pass, the inverse of \\( T \\) is computed. The product \\( T^{-1} \\times A \\times S \\) is then calculated using matrix multiplication.\n5. **Rounding and Conversion**: The resulting matrix is rounded to four decimal places and converted back to a list of lists to meet the output requirements.\n\nThis approach ensures that all necessary validations are performed efficiently, and the matrix operations are carried out correctly using numerical libraries for accuracy and performance."}
{"task_id": 8, "completion_id": 0, "completion": "To solve this problem, we need to calculate the inverse of a 2x2 matrix. The inverse of a matrix is a matrix that, when multiplied with the original matrix, results in the identity matrix. However, not all matrices are invertible. A matrix is invertible if and only if its determinant is non-zero. \n\n### Approach\n1. **Extract Elements**: Extract the elements of the 2x2 matrix. For a matrix [[a, b], [c, d]], the elements are a, b, c, and d.\n2. **Calculate Determinant**: Compute the determinant using the formula `ad - bc`. If the determinant is zero, the matrix is not invertible, and we return `None`.\n3. **Compute Inverse**: If the determinant is non-zero, compute the inverse using the formula:\n   \\[\n   \\text{Inverse} = \\frac{1}{\\text{det}} \\times \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n   \\]\n   This results in each element of the inverse matrix being the corresponding element from the adjugate matrix divided by the determinant.\n\n### Solution Code\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_det = 1.0 / det\n    return [\n        [d * inv_det, -b * inv_det],\n        [-c * inv_det, a * inv_det]\n    ]\n```\n\n### Explanation\n1. **Extract Elements**: The elements of the matrix are extracted from the input list of lists. For example, `a` is the element at the first row and first column, `b` at the first row and second column, and so on.\n2. **Determinant Calculation**: The determinant is calculated using the formula `ad - bc`. If this value is zero, the function returns `None` because the matrix is not invertible.\n3. **Inverse Calculation**: If the determinant is non-zero, the inverse is computed by rearranging the elements of the matrix, changing the signs of `b` and `c`, and dividing each element by the determinant. This results in the inverse matrix, which is returned as a list of lists.\n\nThis approach ensures that we correctly handle both invertible and non-invertible matrices efficiently with a time complexity of O(1), as all operations are performed in constant time."}
{"task_id": 9, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that multiplies two matrices. If the matrices cannot be multiplied due to incompatible dimensions, the function should return -1. \n\n### Approach\nMatrix multiplication requires that the number of columns in the first matrix (matrix A) is equal to the number of rows in the second matrix (matrix B). If this condition is not met, the matrices cannot be multiplied. The result of multiplying an m\u00d7n matrix A with an n\u00d7p matrix B is an m\u00d7p matrix where each element is computed as the dot product of the corresponding row from A and column from B.\n\n### Solution Code\n```python\ndef matrixmul(a: list[list[int | float]],\n              b: list[list[int | float]]) -> list[list[int | float]] | int:\n    # Check if either matrix is empty\n    if not a or not b:\n        return -1\n    # Get the number of columns in the first matrix and rows in the second\n    try:\n        cols_a = len(a[0])\n        cols_b = len(b[0])\n    except IndexError:\n        # Handle cases where a row might be empty\n        return -1\n    rows_b = len(b)\n    # Check if multiplication is possible\n    if cols_a != rows_b:\n        return -1\n    # Initialize the result matrix with zeros\n    rows_a = len(a)\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    # Perform matrix multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            total = 0\n            for k in range(cols_a):\n                # Check if indices are within bounds for both matrices\n                if k >= len(a[i]) or j >= len(b[k]):\n                    return -1\n                total += a[i][k] * b[k][j]\n            result[i][j] = total\n    return result\n```\n\n### Explanation\n1. **Check for Empty Matrices**: The function first checks if either matrix is empty. If so, it returns -1 immediately.\n2. **Check Dimensions**: The function checks if the number of columns in the first matrix (A) matches the number of rows in the second matrix (B). If not, it returns -1.\n3. **Initialize Result Matrix**: The result matrix is initialized with zeros, having the same number of rows as matrix A and the same number of columns as matrix B.\n4. **Matrix Multiplication**: Each element of the result matrix is computed as the dot product of the corresponding row from matrix A and column from matrix B. This involves iterating through each element and summing the products of corresponding elements from the rows and columns.\n5. **Bounds Checking**: During multiplication, the function ensures that all indices are within bounds to handle any potential invalid row lengths, returning -1 if an out-of-bounds access is detected.\n\nThis approach ensures that all edge cases, including empty matrices and dimension mismatches, are handled correctly while efficiently computing the matrix product when possible."}
{"task_id": 10, "completion_id": 0, "completion": "To solve this problem, we need to compute the covariance matrix for a given set of vectors. Each vector represents a feature with its observations, and the covariance matrix is a square matrix where each element (i, j) represents the covariance between the i-th and j-th features.\n\n### Approach\n1. **Input Validation**: Check if the input list is empty. If it is, return an empty list.\n2. **Compute Means**: Calculate the mean of each feature vector.\n3. **Covariance Calculation**: For each pair of features (i, j), compute the covariance using the formula:\n   \\[\n   \\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{k=1}^{n} (X_k - \\bar{X})(Y_k - \\bar{Y})\n   \\]\n   where \\( \\bar{X} \\) and \\( \\bar{Y} \\) are the means of features X and Y, respectively, and n is the number of observations.\n4. **Construct Matrix**: Build the covariance matrix by iterating over all pairs of features and storing their covariance values.\n\n### Solution Code\n```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    m = len(vectors)\n    if m == 0:\n        return []\n    n = len(vectors[0])\n    means = [sum(vec) / n for vec in vectors]\n    covariance_matrix = []\n    for i in range(m):\n        row = []\n        for j in range(m):\n            covariance = 0.0\n            for k in range(n):\n                diff_i = vectors[i][k] - means[i]\n                diff_j = vectors[j][k] - means[j]\n                covariance += diff_i * diff_j\n            covariance /= (n - 1)\n            row.append(covariance)\n        covariance_matrix.append(row)\n    return covariance_matrix\n\n# Test cases\nif __name__ == \"__main__\":\n    # Test case 1\n    vectors1 = [[1, 3], [2, 4]]\n    print(calculate_covariance_matrix(vectors1))  # [[2.0, 2.0], [2.0, 2.0]]\n\n    # Test case 2\n    vectors2 = [[1, 2, 3], [3, 1, 2]]\n    print(calculate_covariance_matrix(vectors2))  # [[1.0, -0.5], [-0.5, 1.0]]\n\n    # Test case 3: single feature\n    vectors3 = [[1, 2, 3]]\n    print(calculate_covariance_matrix(vectors3))  # [[1.0]]\n\n    # Test case 4: three features with all elements 2\n    vectors4 = [[2, 2, 2], [2, 2, 2], [2, 2, 2]]\n    print(calculate_covariance_matrix(vectors4))  # [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\n\n    # Test case 5: zero covariance between X and Y\n    vectors5 = [[1, 3], [2, 2]]\n    print(calculate_covariance_matrix(vectors5))  # [[2.0, 0.0], [0.0, 0.0]]\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the input list is empty and returns an empty list if true.\n- **Mean Calculation**: For each feature vector, the mean is calculated by summing all observations and dividing by the number of observations.\n- **Covariance Calculation**: For each pair of features, the covariance is computed by iterating over each observation, calculating the product of deviations from the mean for both features, summing these products, and then dividing by \\( n-1 \\) (where \\( n \\) is the number of observations).\n- **Matrix Construction**: The results are stored in a matrix where each element (i, j) represents the covariance between the i-th and j-th features.\n\nThis approach ensures that we correctly compute the covariance matrix using fundamental statistical formulas, providing an efficient and clear solution."}
{"task_id": 11, "completion_id": 0, "completion": "To solve this problem, we need to implement the Jacobi method to iteratively solve a system of linear equations given by Ax = b. The Jacobi method is an iterative algorithm that updates each component of the solution vector based on the previous iteration's values. The solution is updated iteratively, and each intermediate result is rounded to four decimal places to ensure precision.\n\n### Approach\n1. **Initialization**: Start with an initial guess for the solution vector, typically a zero vector.\n2. **Iteration**: For each iteration, compute a new solution vector using the values from the previous iteration. For each component of the solution vector, the update is based on the current values of all other components.\n3. **Rounding**: After each iteration, round the intermediate solution vector to four decimal places to maintain the required precision.\n4. **Termination**: After completing the specified number of iterations, return the solution vector as a list, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    x = np.zeros_like(b, dtype=np.float64)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            sum_ = np.dot(A[i], x) - A[i, i] * x[i]\n            x_new[i] = (b[i] - sum_) / A[i, i]\n        x_new = np.round(x_new, 4)\n        x = x_new\n    return x.tolist()\n```\n\n### Explanation\n1. **Initialization**: The solution vector `x` is initialized to a zero vector with the same shape as `b` using `np.zeros_like(b)`.\n2. **Iteration Process**: For each iteration, a new solution vector `x_new` is computed. For each element `i` in the solution vector:\n   - Compute the sum of the products of the corresponding row of matrix `A` (excluding the diagonal element) and the current solution vector `x`.\n   - Update the element using the formula derived from rearranging the linear equation to isolate the diagonal term.\n3. **Rounding**: After computing the new solution vector `x_new`, it is rounded to four decimal places using `np.round(x_new, 4)`.\n4. **Update and Termination**: The solution vector `x` is updated to `x_new` after each iteration. After completing all iterations, the final solution vector is converted to a list using `tolist()` and returned.\n\nThis approach ensures that each intermediate step is correctly computed and rounded, leading to the final solution after the specified number of iterations. The use of vectorized operations ensures efficiency, while the iterative nature of the Jacobi method guarantees convergence under suitable conditions (e.g., diagonal dominance)."}
{"task_id": 12, "completion_id": 0, "completion": "To solve this problem, we need to compute the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobi method. The SVD of a matrix A is given by U, \u03a3, and V such that A = U\u03a3V^T, where \u03a3 contains the singular values. The Jacobi method involves applying orthogonal transformations to diagonalize the matrix A^T A, whose eigenvalues are the squares of the singular values of A. \n\n### Approach\n1. **Compute Matrix B**: Calculate B = A^T A, which is a symmetric matrix.\n2. **Check for Diagonal Matrix**: If B is already diagonal, the singular values are the square roots of the diagonal elements.\n3. **Compute Rotation Angle**: Use the Jacobi rotation method to determine the angle \u03b8 that diagonalizes B. This angle is derived from the off-diagonal elements of B.\n4. **Apply Rotation**: Use the computed angle to form the rotation matrix and apply it to B to obtain the eigenvalues.\n5. **Calculate Singular Values**: The singular values are the square roots of the eigenvalues of B, sorted in descending order.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Compute B = A^T A\n    B = np.dot(A.T, A)\n    a = B[0, 0]\n    b = B[0, 1]\n    d = B[1, 1]\n    \n    if np.isclose(b, 0):\n        # Matrix is diagonal\n        s1 = np.sqrt(max(a, d))\n        s2 = np.sqrt(min(a, d))\n    else:\n        # Compute the rotation angle\n        theta = 0.5 * np.arctan2(2 * b, a - d)\n        c = np.cos(theta)\n        s = np.sin(theta)\n        \n        # Compute the eigenvalues of B\n        lambda1 = a * c**2 + 2 * b * c * s + d * s**2\n        lambda2 = a * s**2 - 2 * b * c * s + d * c**2\n        \n        # Ensure correct order of eigenvalues\n        if lambda1 < lambda2:\n            lambda1, lambda2 = lambda2, lambda1\n        \n        # Compute singular values\n        s1 = np.sqrt(lambda1)\n        s2 = np.sqrt(lambda2)\n    \n    # Ensure descending order and round\n    if s1 < s2:\n        s1, s2 = s2, s1\n    s1 = np.round(s1, 4)\n    s2 = np.round(s2, 4)\n    \n    return (s1, s2)\n```\n\n### Explanation\n1. **Matrix B Calculation**: The matrix B is computed as the product of the transpose of A and A itself. This matrix B is symmetric and positive semi-definite.\n2. **Diagonal Check**: If B is diagonal (off-diagonal elements are zero), the singular values are the square roots of the diagonal elements.\n3. **Rotation Angle**: The angle \u03b8 is calculated using the arctangent function to determine the rotation needed to diagonalize B.\n4. **Rotation Application**: Using trigonometric identities, the rotation matrix elements (cosine and sine of \u03b8) are computed and applied to B to find its eigenvalues.\n5. **Singular Values**: The singular values are derived from the square roots of these eigenvalues, ensuring they are sorted in descending order and rounded to four decimal places.\n\nThis approach efficiently leverages the properties of symmetric matrices and the Jacobi rotation method to compute the singular values without relying on advanced libraries beyond basic numerical operations."}
{"task_id": 13, "completion_id": 0, "completion": "To solve this problem, we need to calculate the determinant of a 4x4 matrix using Laplace's Expansion method. Laplace's Expansion involves expanding the determinant along a row or column, computing the minors for each element, and recursively determining the determinant of these smaller matrices until reaching a base case (2x2 matrices, which can be computed directly).\n\n### Approach\n1. **Base Cases**: \n   - For a 1x1 matrix, the determinant is the single element itself.\n   - For a 2x2 matrix, the determinant is calculated using the formula \\(ad - bc\\), where \\(a, b, c, d\\) are the elements of the matrix.\n   \n2. **Recursive Case**:\n   - For matrices larger than 2x2, expand along the first row (for simplicity). For each element in this row:\n     - Compute the minor matrix by removing the current row and column.\n     - Calculate the cofactor sign using \\((-1)^{i+j}\\), where \\(i\\) is the row index (0 for the first row) and \\(j\\) is the column index.\n     - Recursively compute the determinant of the minor matrix.\n     - Sum the contributions of each element multiplied by its cofactor and the determinant of its minor.\n\n### Solution Code\n```python\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    n = len(matrix)\n    if n == 1:\n        return matrix[0][0]\n    elif n == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    else:\n        det = 0.0\n        for j in range(n):\n            minor = [row[:j] + row[j+1:] for row in matrix[1:]]\n            sign = (-1) ** j\n            det += matrix[0][j] * sign * determinant_4x4(minor)\n        return det\n```\n\n### Explanation\n- **Base Cases**: Directly handle 1x1 and 2x2 matrices for quick computation without recursion.\n- **Recursive Expansion**: For larger matrices, the function expands along the first row. Each element's contribution is calculated by forming a minor matrix (excluding the current row and column), determining its determinant recursively, and applying the appropriate sign based on the element's position.\n- **Efficiency**: The recursion efficiently breaks down the problem into smaller sub-problems, ultimately reducing the computation to 2x2 determinants, which are solved directly. This approach ensures correctness while leveraging the structure of Laplace's Expansion."}
{"task_id": 14, "completion_id": 0, "completion": "To solve this problem, we need to perform linear regression using the normal equation. The normal equation is a mathematical approach that directly computes the optimal coefficients for a linear regression model without requiring iterative optimization. This method is efficient and provides an exact solution when the feature matrix is invertible.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert the input features matrix `X` and target vector `y` into NumPy arrays for efficient matrix operations.\n2. **Add Intercept Term**: Augment the feature matrix `X` by adding a column of ones to account for the intercept term in the linear regression model.\n3. **Compute Normal Equation**: Use the formula for the normal equation, which involves computing the transpose of the augmented matrix, matrix multiplication, matrix inversion, and finally solving for the coefficients.\n4. **Round Results**: Round the computed coefficients to four decimal places to meet the specified output constraints.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert input lists to numpy arrays\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Add a column of ones to X for the intercept term\n    ones = np.ones((X_np.shape[0], 1))\n    X_augmented = np.hstack((ones, X_np))\n    \n    # Compute the transpose of X_augmented\n    XT = X_augmented.T\n    \n    # Calculate X^T * X\n    XT_X = np.dot(XT, X_augmented)\n    \n    # Compute the inverse of X^T * X\n    XT_X_inv = np.linalg.inv(XT_X)\n    \n    # Calculate X^T * y\n    XT_y = np.dot(XT, y_np)\n    \n    # Compute the coefficients using theta = (X^T X)^-1 X^T y\n    theta = np.dot(XT_X_inv, XT_y)\n    \n    # Round the coefficients to four decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert the numpy array back to a list of floats\n    return theta_rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The input matrices are converted to NumPy arrays to leverage NumPy's efficient matrix operations.\n2. **Intercept Term**: A column of ones is added to the feature matrix to account for the bias term (intercept) in the linear regression model.\n3. **Matrix Operations**: The normal equation \\(\\theta = (X^T X)^{-1} X^T y\\) is computed step-by-step using matrix transpose, multiplication, and inversion. This directly solves for the coefficients that minimize the cost function.\n4. **Rounding**: The coefficients are rounded to four decimal places to meet the specified precision requirement, ensuring that even very small values are correctly formatted.\n\nThis approach efficiently computes the linear regression coefficients using the normal equation, providing an exact solution in a straightforward manner."}
{"task_id": 15, "completion_id": 0, "completion": "To solve this problem, we need to implement a linear regression model using gradient descent. The goal is to find the optimal coefficients (parameters) that minimize the cost function, which is typically the mean squared error in linear regression. The gradient descent algorithm iteratively updates these coefficients to approach the optimal values.\n\n### Approach\n1. **Initialization**: Start with initial coefficients (theta) set to zero. The shape of theta corresponds to the number of features in the input matrix X.\n2. **Iteration**: For each iteration, compute the predicted values using the current coefficients, calculate the error between the predicted and actual values, and update the coefficients using the gradient of the cost function.\n3. **Gradient Calculation**: The gradient is computed as the transpose of the feature matrix multiplied by the error vector, scaled by the inverse of the number of samples.\n4. **Update Coefficients**: Adjust the coefficients by subtracting the product of the learning rate (alpha) and the gradient.\n5. **Rounding**: After completing the specified number of iterations, round the coefficients to four decimal places and convert them to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = X.shape[0]  # Number of training examples\n    theta = np.zeros(X.shape[1])  # Initialize parameters\n    \n    for _ in range(iterations):\n        hypothesis = X.dot(theta)\n        error = hypothesis - y\n        gradient = (1 / m) * X.T.dot(error)\n        theta -= alpha * gradient\n    \n    # Round to four decimal places and convert to list\n    return np.round(theta, 4).tolist()\n```\n\n### Explanation\n- **Initialization**: The coefficients (theta) are initialized to zeros. This is a common starting point in gradient descent algorithms.\n- **Hypothesis Calculation**: For each iteration, the hypothesis (predicted values) is computed by taking the dot product of the feature matrix X and the current coefficients theta.\n- **Error Calculation**: The error is the difference between the predicted values (hypothesis) and the actual target values (y).\n- **Gradient Descent Update**: The gradient of the cost function is computed using the transpose of the feature matrix and the error vector. This gradient is then scaled by the learning rate (alpha) and subtracted from the current coefficients to update them towards values that minimize the cost function.\n- **Rounding and Conversion**: After the specified number of iterations, the coefficients are rounded to four decimal places to meet the problem's requirements and converted to a list for the final output.\n\nThis approach efficiently iterates towards the optimal coefficients using gradient descent, ensuring that the solution is both accurate and meets the specified precision requirements."}
{"task_id": 16, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs two types of feature scaling on a dataset: standardization and min-max normalization. Both techniques are essential in data preprocessing to bring features to a similar scale, which can improve the performance of many machine learning algorithms.\n\n### Approach\n\n1. **Standardization**: This technique transforms the data such that it has a mean of 0 and a standard deviation of 1. For each feature (column), we subtract the mean of the feature from each data point and then divide by the standard deviation of the feature. This is done using the formula:\n   \\[\n   z = \\frac{x - \\mu}{\\sigma}\n   \\]\n   where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of the feature.\n\n2. **Min-Max Normalization**: This technique scales the data to a fixed range, usually [0, 1]. For each feature, we subtract the minimum value of the feature from each data point and then divide by the range (max - min) of the feature. The formula is:\n   \\[\n   x' = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)}\n   \\]\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization\n    mean = data.mean(axis=0)\n    std = data.std(axis=0)\n    standardized = (data - mean) / std\n    standardized_rounded = np.round(standardized, 4)\n    \n    # Min-Max Normalization\n    min_vals = data.min(axis=0)\n    max_vals = data.max(axis=0)\n    ranges = max_vals - min_vals\n    minmax = (data - min_vals) / ranges\n    minmax_rounded = np.round(minmax, 4)\n    \n    return standardized_rounded.tolist(), minmax_rounded.tolist()\n```\n\n### Explanation\n\n1. **Standardization**:\n   - Compute the mean and standard deviation for each feature (column).\n   - Subtract the mean from each data point in the respective column.\n   - Divide the result by the standard deviation for that column.\n   - Round the result to 4 decimal places and convert the NumPy array to a Python list using `tolist()`.\n\n2. **Min-Max Normalization**:\n   - Compute the minimum and maximum values for each feature (column).\n   - Calculate the range (difference between max and min) for each feature.\n   - Subtract the minimum value from each data point in the respective column.\n   - Divide the result by the range for that column.\n   - Round the result to 4 decimal places and convert the NumPy array to a Python list using `tolist()`.\n\nBy following these steps, we ensure that the data is appropriately scaled using both techniques, which can then be used for further machine learning tasks. The rounding ensures that the output meets the specified precision requirement, and converting to lists ensures compatibility with non-NumPy environments."}
{"task_id": 17, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_np = np.array(points)\n    current_centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Calculate squared distances between each point and each centroid\n        distances = ((points_np[:, np.newaxis, :] - current_centroids) ** 2).sum(axis=2)\n        # Assign each point to the closest centroid\n        labels = np.argmin(distances, axis=1)\n        \n        new_centroids = np.zeros_like(current_centroids)\n        for i in range(k):\n            cluster_points = points_np[labels == i]\n            if len(cluster_points) > 0:\n                new_centroids[i] = cluster_points.mean(axis=0)\n            else:\n                new_centroids[i] = current_centroids[i]\n        \n        # Check for convergence\n        if np.allclose(current_centroids, new_centroids):\n            break\n        current_centroids = new_centroids\n    \n    # Round the final centroids to four decimal places\n    current_centroids = np.round(current_centroids, 4)\n    # Convert numpy array back to list of tuples\n    return [tuple(centroid.tolist()) for centroid in current_centroids]\n```"}
{"task_id": 18, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that generates train and test splits for K-Fold Cross-Validation. This involves dividing the dataset into k folds and returning a list of tuples containing the train and test indices for each fold. The function should handle both shuffled and non-shuffled data, and it should be reproducible using a random seed when shuffling is enabled.\n\n### Approach\n1. **Generate Indices**: Create an array of indices corresponding to the number of samples in the dataset.\n2. **Shuffle Indices**: If shuffling is enabled, shuffle the indices using a random number generator initialized with a provided seed for reproducibility.\n3. **Split into Folds**: Divide the indices into k approximately equal-sized folds using NumPy's `array_split` function, which handles cases where the number of samples is not perfectly divisible by k.\n4. **Generate Splits**: For each fold, use the current fold as the test set and the remaining folds as the training set. Collect these splits into a list of tuples.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    \n    rng = np.random.default_rng(seed=random_seed)\n    if shuffle:\n        indices = rng.permutation(indices)\n    \n    folds = np.array_split(indices, k)\n    \n    splits = []\n    for i in range(k):\n        test_indices = folds[i]\n        train_folds = [folds[j] for j in range(k) if j != i]\n        train_indices = np.concatenate(train_folds)\n        splits.append((train_indices, test_indices))\n    \n    return splits\n```\n\n### Explanation\n1. **Indices Generation**: The function starts by creating an array of indices from 0 to the number of samples minus one.\n2. **Shuffling**: If the `shuffle` parameter is set to True, the indices are shuffled using a permutation method from NumPy's random generator, which ensures reproducibility with the provided random seed.\n3. **Splitting into Folds**: The indices are split into k folds using `np.array_split`, which handles cases where the number of samples is not evenly divisible by k by distributing the remainder into the first few folds.\n4. **Generating Train-Test Splits**: For each fold, the current fold is designated as the test set, and the remaining folds are combined to form the training set. Each pair of training and test indices is collected into a list, which is returned as the result.\n\nThis approach ensures that each sample is used exactly once as a test sample across all k folds, making the cross-validation process robust and reliable. The use of a random generator with a seed ensures that the shuffling is reproducible, which is essential for consistent results across different runs."}
{"task_id": 19, "completion_id": 0, "completion": "To solve this problem, we need to implement Principal Component Analysis (PCA) from scratch. PCA is a dimensionality reduction technique that transforms data into a set of orthogonal components that capture the maximum variance. The steps involve standardizing the data, computing the covariance matrix, finding eigenvalues and eigenvectors, and selecting the top k eigenvectors (principal components).\n\n### Approach\n1. **Standardize the Data**: Subtract the mean and divide by the standard deviation for each feature to ensure each feature has zero mean and unit variance.\n2. **Compute Covariance Matrix**: Calculate the covariance matrix of the standardized data to understand the relationships between features.\n3. **Eigen Decomposition**: Find the eigenvalues and eigenvectors of the covariance matrix. Eigenvalues represent the magnitude of variance, and eigenvectors represent the direction of maximum variance.\n4. **Sort Eigenvectors**: Sort the eigenvectors based on their corresponding eigenvalues in descending order to prioritize components with higher variance.\n5. **Select Top k Components**: Choose the top k eigenvectors, which are the principal components that capture the most variance.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Standardize the data\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0, ddof=1)\n    standardized_data = (data - mean) / std\n    \n    # Compute covariance matrix\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    # Sort eigenvectors by descending eigenvalues\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Select top k eigenvectors and format the result\n    principal_components = sorted_eigenvectors[:, :k].T\n    return np.round(principal_components, 4).tolist()\n```\n\n### Explanation\n1. **Standardization**: Each feature is centered by subtracting its mean and scaled by dividing by its standard deviation. This ensures that all features contribute equally to the analysis.\n2. **Covariance Matrix**: The covariance matrix is computed to capture how each feature varies with respect to others. This matrix is symmetric and provides the basis for understanding the data's variance structure.\n3. **Eigen Decomposition**: Using numpy's `eigh` function, we efficiently compute eigenvalues and eigenvectors for the symmetric covariance matrix. The eigenvalues indicate the variance explained by each eigenvector.\n4. **Sorting Eigenvectors**: Eigenvectors are sorted in descending order of their eigenvalues to prioritize components that explain the most variance.\n5. **Selecting Components**: The top k eigenvectors (principal components) are selected and returned as a list of lists, each rounded to four decimal places. These components are the directions in the feature space that capture the most variance, allowing for dimensionality reduction while retaining critical information."}
{"task_id": 20, "completion_id": 0, "completion": "To solve this problem, we need to implement a decision tree learning algorithm for classification using recursive binary splitting based on entropy and information gain. The goal is to build a decision tree that can classify examples based on their attributes.\n\n### Approach\nThe decision tree learning algorithm follows these steps:\n\n1. **Base Case 1 - All Examples Same Class**: If all examples have the same target value, return a leaf node with that class.\n2. **Base Case 2 - No Attributes Left**: If there are no attributes left to split on, return a leaf node with the majority class of the current examples.\n3. **Select Best Attribute**: Calculate the information gain for each attribute and select the attribute with the highest information gain.\n4. **Split Examples**: Split the examples into subsets based on the selected attribute's values.\n5. **Recursive Tree Construction**: For each subset of examples, recursively build the decision tree using the remaining attributes and add branches to the current node.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    def entropy(targets):\n        counts = Counter(targets)\n        total = len(targets)\n        if total == 0:\n            return 0.0\n        return -sum((count / total) * math.log2(count / total) for count in counts.values())\n    \n    target_values = [ex[target_attr] for ex in examples]\n    \n    # Base case 1: All examples have the same target value\n    if len(set(target_values)) == 1:\n        return {'class': target_values[0]}\n    \n    # Base case 2: No attributes left to split on\n    if not attributes:\n        majority = Counter(target_values).most_common(1)[0][0]\n        return {'class': majority}\n    \n    # Calculate the original entropy\n    original_entropy = entropy(target_values)\n    best_attr = None\n    max_info_gain = -float('inf')\n    \n    # Find the attribute with the highest information gain\n    for attr in attributes:\n        # Group examples by the current attribute's value\n        groups = {}\n        for ex in examples:\n            val = ex[attr]\n            if val not in groups:\n                groups[val] = []\n            groups[val].append(ex)\n        # Calculate split entropy\n        split_entropy = 0.0\n        for group in groups.values():\n            group_targets = [ex[target_attr] for ex in group]\n            split_entropy += (len(group) / len(examples)) * entropy(group_targets)\n        info_gain = original_entropy - split_entropy\n        # Update best attribute if this is the highest gain so far\n        if info_gain > max_info_gain:\n            max_info_gain = info_gain\n            best_attr = attr\n    \n    # Create the decision node\n    decision_node = {'attribute': best_attr, 'branches': {}}\n    \n    # Get all unique values of the best attribute in current examples\n    values = {ex[best_attr] for ex in examples}\n    \n    # Recursively build subtrees for each value\n    for value in values:\n        subset = [ex for ex in examples if ex[best_attr] == value]\n        remaining_attrs = [a for a in attributes if a != best_attr]\n        subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n        decision_node['branches'][value] = subtree\n    \n    return decision_node\n```\n\n### Explanation\n1. **Entropy Calculation**: The entropy function calculates the entropy of a list of target values, which measures the impurity or disorder. Lower entropy means more homogeneity.\n2. **Information Gain**: For each attribute, the information gain is calculated by subtracting the weighted average entropy of the subsets (after splitting) from the original entropy. The attribute with the highest gain is chosen for splitting.\n3. **Recursive Tree Construction**: The algorithm recursively splits the dataset based on the selected attribute, creating branches for each possible value of the attribute. This process continues until a base case is met (all examples in a subset have the same target value or no attributes are left).\n\nThis approach ensures that the decision tree is built efficiently, making optimal splits at each node based on information gain, leading to a model that can classify new examples effectively."}
{"task_id": 21, "completion_id": 0, "completion": "To solve this problem, we need to implement a deterministic version of the Pegasos algorithm for training a kernel SVM classifier. The algorithm will be adapted to use all samples in each iteration and include a bias term. The solution involves computing the kernel matrix, iteratively updating the model's coefficients (alpha), and adjusting the bias based on the training data.\n\n### Approach\n1. **Kernel Matrix Computation**: Depending on the specified kernel type (linear or RBF), compute the kernel matrix that captures the similarity between each pair of data points.\n2. **Initialization**: Initialize the alpha coefficients and bias term to zero.\n3. **Iterative Updates**: For each iteration, compute the learning rate and update the alpha coefficients for samples that do not meet the margin condition. Scale the alpha coefficients to prevent overfitting and update the bias term based on the current model's performance.\n4. **Bias Adjustment**: Adjust the bias term using the sum of labels from samples that do not meet the margin condition, scaled by the learning rate.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples = data.shape[0]\n    \n    # Compute the kernel matrix\n    if kernel == 'linear':\n        K = data @ data.T\n    elif kernel == 'RBF':\n        data_sq = np.sum(data ** 2, axis=1)\n        pairwise_dist_sq = data_sq[:, np.newaxis] + data_sq[np.newaxis, :] - 2 * (data @ data.T)\n        K = np.exp(-pairwise_dist_sq / (2 * (sigma ** 2)))\n    else:\n        raise ValueError(\"Unsupported kernel type. Choose 'linear' or 'RBF'.\")\n    \n    alpha = np.zeros(n_samples)\n    b = 0.0\n    \n    for iteration in range(iterations):\n        t = iteration + 1\n        eta = 1.0 / (lambda_val * t)\n        \n        # Compute predictions and update alpha\n        predictions = np.dot(alpha * labels, K) + b\n        margin = labels * predictions\n        mask = margin <= 1\n        alpha[mask] += eta * labels[mask]\n        \n        # Scale alpha\n        alpha *= (1 - eta * lambda_val)\n        \n        # Compute new predictions for bias update\n        new_predictions = np.dot(alpha * labels, K) + b\n        new_margin = labels * new_predictions\n        mask_new = new_margin <= 1\n        sum_b = np.sum(labels[mask_new])\n        \n        # Update bias\n        b += eta * sum_b\n    \n    # Round the results to 4 decimal places\n    alpha_rounded = np.round(alpha, 4)\n    b_rounded = round(b, 4)\n    \n    return alpha_rounded.tolist(), b_rounded\n```\n\n### Explanation\n1. **Kernel Matrix**: The kernel matrix is computed once before training. For a linear kernel, it is the dot product of data points. For an RBF kernel, it uses the exponential of the negative squared Euclidean distances scaled by a bandwidth parameter.\n2. **Initialization**: The alpha coefficients and bias term are initialized to zero.\n3. **Iterative Updates**:\n   - **Learning Rate**: The learning rate decreases with each iteration, calculated as `1 / (lambda_val * t)` where `t` is the iteration number.\n   - **Margin Check**: For each sample, check if it meets the margin condition. If not, update the corresponding alpha coefficient.\n   - **Scaling**: Alpha coefficients are scaled to prevent overfitting by multiplying with `(1 - learning_rate * lambda_val)`.\n   - **Bias Adjustment**: The bias is updated based on the sum of labels of samples that do not meet the margin condition, scaled by the learning rate.\n4. **Rounding**: The final alpha coefficients and bias are rounded to four decimal places and returned as a list and a float, respectively.\n\nThis approach ensures that the model learns the optimal decision boundary using all samples in each iteration, incorporating both the kernel trick and a bias term for accurate classification."}
{"task_id": 22, "completion_id": 0, "completion": "To solve this problem, we need to compute the output of the sigmoid activation function for a given input value `z` and return the result rounded to four decimal places. The sigmoid function is a common activation function used in machine learning and neural networks, which maps any real-valued number into the range (0, 1).\n\n### Approach\nThe sigmoid function is mathematically defined as:\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\nWhere \\( e \\) is the base of the natural logarithm. \n\nTo compute this in Python, we can use the `math.exp` function to calculate the exponential part. The steps are as follows:\n1. Compute the exponential of the negative input value `z` using `math.exp(-z)`.\n2. Add 1 to the result of the exponential to form the denominator.\n3. Take the reciprocal of the denominator to get the sigmoid value.\n4. Round the result to four decimal places using the `round` function.\n\nThis approach ensures that we handle both very large positive and negative values of `z` correctly, as the exponential function will naturally handle these cases without causing overflow in typical Python environments.\n\n### Solution Code\n\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)\n```\n\n### Explanation\n1. **Exponential Calculation**: The term `math.exp(-z)` computes \\( e^{-z} \\), which is the exponential part of the sigmoid function.\n2. **Denominator Formation**: Adding 1 to the exponential result gives the denominator \\( 1 + e^{-z} \\).\n3. **Reciprocal for Sigmoid**: Taking the reciprocal of the denominator gives the sigmoid value, which scales the input `z` into the range (0, 1).\n4. **Rounding**: The `round` function is used to ensure the result is rounded to four decimal places, as required.\n\nThis method efficiently computes the sigmoid activation function and ensures the result is precise to four decimal places, making it suitable for applications requiring numerical stability and precision."}
{"task_id": 23, "completion_id": 0, "completion": "To solve this problem, we need to implement a Python function that computes the softmax activation for a given list of scores. The softmax function converts a vector of scores into a probability distribution, where each element's value is proportional to its exponentiated score relative to the sum of all exponentiated scores. This ensures the output values are in the range [0, 1] and sum to 1, making them suitable for probability interpretation.\n\n### Approach\n1. **Numerical Stability**: To prevent potential numerical overflow when exponentiating large scores, we first subtract the maximum score from each element in the input list. This adjustment ensures the largest value becomes zero, making the exponents more manageable.\n2. **Exponentiation**: Compute the exponential of each adjusted score.\n3. **Sum of Exponents**: Calculate the sum of these exponentiated values to use as the denominator in the softmax formula.\n4. **Normalization**: Divide each exponentiated value by the sum to get the softmax probabilities.\n5. **Rounding**: Round each resulting probability to four decimal places for the final output.\n\n### Solution Code\n\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp = sum(exp_scores)\n    softmax_values = [exp / sum_exp for exp in exp_scores]\n    return [round(val, 4) for val in softmax_values]\n```\n\n### Explanation\n1. **Numerical Stability Adjustment**: By subtracting the maximum score from each element, we ensure that the largest value in the adjusted list is zero. This step helps in avoiding very large numbers during exponentiation, which could lead to overflow issues.\n2. **Exponentiation**: Each adjusted score is exponentiated using the `math.exp` function. This transforms the scores into positive values, which are then used to compute probabilities.\n3. **Sum Calculation**: The sum of these exponentiated values is computed to normalize each value, ensuring they sum to 1.\n4. **Normalization**: Each exponentiated value is divided by the sum to convert them into probabilities.\n5. **Rounding**: The probabilities are rounded to four decimal places to meet the specified precision requirement.\n\nThis approach efficiently computes the softmax values while maintaining numerical stability and precision, making it suitable for both small and large input values."}
{"task_id": 24, "completion_id": 0, "completion": "To solve this problem, we need to simulate a single neuron with a sigmoid activation function for binary classification. The neuron will process multidimensional input features, compute predicted probabilities using the sigmoid function, and calculate the mean squared error (MSE) between the predicted probabilities and the true labels. Both the predicted probabilities and the MSE will be rounded to four decimal places for the final output.\n\n### Approach\n1. **Compute Weighted Sum and Sigmoid Activation**: For each feature vector, compute the weighted sum of the features using the given weights and bias. Apply the sigmoid function to this sum to get the predicted probability.\n2. **Store Exact Predictions**: Store the exact predicted probabilities (before rounding) to ensure the MSE is computed accurately.\n3. **Round Predictions**: Round each predicted probability to four decimal places for the output list.\n4. **Calculate MSE**: Compute the MSE using the exact predicted probabilities and the true labels, then round the result to four decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    exact_predictions = []\n    for x in features:\n        z = sum(w * xi for w, xi in zip(weights, x)) + bias\n        p = 1 / (1 + math.exp(-z))\n        exact_predictions.append(p)\n    \n    rounded_predictions = [round(p, 4) for p in exact_predictions]\n    \n    mse = sum((p - y) ** 2 for p, y in zip(exact_predictions, labels)) / len(labels)\n    mse = round(mse, 4)\n    \n    return (rounded_predictions, mse)\n```\n\n### Explanation\n1. **Weighted Sum and Sigmoid Activation**: For each feature vector, the weighted sum (z) is computed by taking the dot product of the feature vector with the weights and adding the bias. The sigmoid function is then applied to z to get the predicted probability, which transforms the output into a range between 0 and 1.\n2. **Exact Predictions Storage**: The exact values of the predicted probabilities are stored to ensure the MSE calculation is accurate and not affected by rounding errors.\n3. **Rounding Predictions**: The exact predicted probabilities are rounded to four decimal places to form the output list of probabilities.\n4. **MSE Calculation**: The MSE is computed using the exact predicted probabilities and the true labels. This ensures that the error metric is precise. The result is then rounded to four decimal places for the final output.\n\nThis approach ensures that the predicted probabilities are both accurate and properly formatted, while the MSE is calculated using the precise values before rounding, providing a reliable error metric."}
{"task_id": 25, "completion_id": 0, "completion": "To solve this problem, we need to simulate a single neuron with a sigmoid activation function and implement backpropagation using gradient descent based on Mean Squared Error (MSE) loss. The goal is to update the neuron's weights and bias over multiple epochs and track the MSE for each epoch.\n\n### Approach\n1. **Initialization**: Start with the given initial weights and bias.\n2. **Forward Pass**: For each epoch, compute the neuron's output using the sigmoid activation function.\n3. **Compute MSE**: Calculate the MSE loss for the current epoch using the predicted outputs and true labels.\n4. **Backward Pass**: Compute the gradients of the loss with respect to the weights and bias using the chain rule.\n5. **Update Parameters**: Adjust the weights and bias using the computed gradients and the learning rate.\n6. **Track MSE**: Store the MSE for each epoch after rounding it to four decimal places.\n7. **Return Results**: After all epochs, return the updated weights and bias (rounded to four decimal places) and the list of MSE values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_history = []\n    n_samples = features.shape[0]\n    \n    for _ in range(epochs):\n        # Forward pass: compute the activation\n        z = np.dot(features, weights) + bias\n        a = 1 / (1 + np.exp(-z))\n        \n        # Compute MSE for the current epoch\n        mse = np.mean((labels - a) ** 2)\n        mse_history.append(round(mse, 4))\n        \n        # Backward pass: compute gradients\n        error = a - labels\n        delta = error * a * (1 - a)\n        grad_weights = (2 / n_samples) * np.dot(features.T, delta)\n        grad_bias = (2 / n_samples) * np.sum(delta)\n        \n        # Update weights and bias\n        weights -= learning_rate * grad_weights\n        bias -= learning_rate * grad_bias\n    \n    # Round the final weights and bias to four decimal places\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    \n    return weights, bias, mse_history\n```\n\n### Explanation\n1. **Forward Pass**: The neuron's output is computed using the sigmoid activation function, which transforms the linear combination of inputs (features), weights, and bias into a probability-like value between 0 and 1.\n2. **MSE Calculation**: The MSE is calculated as the average of the squared differences between the predicted outputs and the true labels.\n3. **Gradient Calculation**: The gradients for the weights and bias are derived using the chain rule. The gradients are computed based on the error, the derivative of the sigmoid function, and the input features.\n4. **Parameter Update**: The weights and bias are updated by subtracting the product of the learning rate and their respective gradients. This step adjusts the parameters to minimize the loss.\n5. **Tracking MSE**: The MSE for each epoch is stored in a list after rounding to four decimal places to monitor the training progress.\n\nThis approach ensures that the neuron learns iteratively by adjusting its parameters to minimize the MSE loss, thereby improving its predictions over multiple epochs."}
{"task_id": 26, "completion_id": 0, "completion": "To solve this problem, we need to implement a Python class that supports basic autograd operations for scalar values, including addition, multiplication, and ReLU activation. The class should compute gradients automatically using reverse-mode automatic differentiation, similar to how modern deep learning frameworks like PyTorch handle gradients.\n\n### Approach\nThe `Value` class will handle scalar values and their gradients through the following key steps:\n\n1. **Initialization**: Each instance of `Value` stores the data value, gradient, backward function, previous children nodes, and the operation that created it.\n2. **Operations**: Implement the forward and backward passes for addition, multiplication, and ReLU activation:\n   - **Addition**: The gradient of the sum is distributed equally to both operands.\n   - **Multiplication**: The gradient is computed using the product rule, considering the values of the operands.\n   - **ReLU**: The gradient is passed through if the input is positive, otherwise it is zero.\n3. **Backward Pass**: Compute gradients using topological sorting to ensure the correct order of operations, starting from the output node and propagating gradients back through the computation graph.\n\n### Solution Code\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n```\n\n### Explanation\n1. **Initialization**: The `__init__` method initializes the data value, gradient, backward function, children nodes, and operation type.\n2. **Addition**: The `__add__` method creates a new `Value` instance for the sum of two values. The backward function propagates the gradient from the result to both operands.\n3. **Multiplication**: The `__mul__` method creates a new `Value` instance for the product of two values. The backward function uses the product rule to distribute gradients based on the values of the operands.\n4. **ReLU Activation**: The `relu` method applies the ReLU function (max(0, x)) and sets the gradient to 1 if the input was positive, otherwise 0.\n5. **Backward Pass**: The `backward` method performs a topological sort to process nodes in reverse order, applying the chain rule to compute gradients starting from the output node.\n6. **Reverse Operations**: `__radd__` and `__rmul__` handle cases where the `Value` instance is on the right side of the operation with non-`Value` types.\n\nThis approach ensures that gradients are computed correctly for each operation, enabling automatic differentiation for scalar values."}
{"task_id": 27, "completion_id": 0, "completion": "To solve this problem, we need to compute the transformation matrix from one basis B to another basis C in R\u00b3. The transformation matrix allows us to convert any vector represented in basis B to its corresponding representation in basis C.\n\n### Approach\n1. **Understand the Problem**: The goal is to find a matrix P such that when a vector represented in basis B is multiplied by P, it gives the vector's coordinates in basis C.\n2. **Matrix Representation of Basis**: Convert the basis vectors of C into a matrix where each column is a basis vector of C. This matrix is denoted as C_matrix.\n3. **Inverse of C Matrix**: Compute the inverse of C_matrix. This inverse matrix (C_inv) is used to convert vectors from the standard basis to the basis C.\n4. **Transform Basis Vectors**: For each basis vector in B, compute its coordinates in basis C by multiplying it with C_inv.\n5. **Construct Transformation Matrix**: Collect the resulting coordinates as columns to form the transformation matrix P.\n6. **Rounding and Formatting**: Round the elements of the transformation matrix to four decimal places and convert it to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert the basis C into a matrix with each vector as a column\n    C_matrix = np.array(C).T\n    # Compute the inverse of the C matrix\n    C_inv = np.linalg.inv(C_matrix)\n    # For each vector in B, compute its coordinates in basis C and collect as columns\n    P_columns = [C_inv @ np.array(b) for b in B]\n    # Stack the columns to form the transformation matrix\n    P = np.column_stack(P_columns)\n    # Round to 4 decimal places and convert to a list of lists\n    return np.round(P, 4).tolist()\n```\n\n### Explanation\n1. **Matrix Representation**: The basis vectors of C are arranged as columns in a matrix. This allows us to use matrix operations to transform vectors.\n2. **Inverse Calculation**: The inverse of the matrix formed by basis C's vectors is crucial. This inverse matrix converts vectors from the standard basis to the basis C.\n3. **Transformation of Vectors**: Each vector in basis B is transformed to the basis C by multiplying with the inverse matrix. This step ensures that each vector from B is expressed in terms of the basis C.\n4. **Matrix Construction**: The resulting vectors (coordinates in basis C) are stacked as columns to form the transformation matrix P.\n5. **Rounding and Conversion**: The final matrix is rounded to four decimal places for precision and converted to a list of lists to meet the output requirements.\n\nThis approach efficiently computes the transformation matrix using linear algebra principles, ensuring accuracy and correctness through matrix inversion and multiplication."}
{"task_id": 28, "completion_id": 0, "completion": "To solve this problem, we need to compute the Singular Value Decomposition (SVD) of a 2x2 matrix using eigenvalues and eigenvectors. The SVD decomposes a matrix into three matrices U, S, and V such that the original matrix can be reconstructed as U * S * V. Here, U and V are orthogonal matrices, and S is a diagonal matrix of singular values.\n\n### Approach\n1. **Compute the Covariance Matrix**: First, compute the matrix product of the transpose of the input matrix with the input matrix itself. This gives us a symmetric matrix whose eigenvalues and eigenvectors we will compute.\n2. **Eigenvalue Decomposition**: Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues will be used to determine the singular values, and the eigenvectors will form the matrix V.\n3. **Sort Eigenvalues and Eigenvectors**: Sort the eigenvalues in descending order and rearrange the corresponding eigenvectors accordingly.\n4. **Compute Singular Values**: The singular values are the square roots of the sorted eigenvalues.\n5. **Construct Orthogonal Matrices**: Using the eigenvectors, construct the matrix V. Then, compute the matrix U using the input matrix and the eigenvectors, scaled by the singular values. Handle cases where singular values are zero to ensure orthogonality.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Compute A^T A\n    B = A.T @ A\n    \n    # Compute eigenvalues and eigenvectors of B\n    eigen_values, eigen_vectors = np.linalg.eigh(B)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigen_values)[::-1]\n    sorted_eigen_vectors = eigen_vectors[:, sorted_indices]\n    \n    # Construct V by transposing the sorted eigenvectors matrix\n    V = sorted_eigen_vectors.T\n    \n    # Compute singular values\n    sigma1 = np.sqrt(eigen_values[sorted_indices[0]])\n    sigma2 = np.sqrt(eigen_values[sorted_indices[1]])\n    \n    # Compute right singular vectors (rows of V)\n    v1 = V[0]\n    v2 = V[1]\n    \n    # Compute left singular vectors u1 and u2\n    # Handle u1\n    if sigma1 > 1e-10:\n        u1 = (A @ v1) / sigma1\n    else:\n        u1 = np.array([1.0, 0.0])  # default unit vector if sigma1 is zero\n    \n    # Handle u2\n    if sigma2 > 1e-10:\n        u2 = (A @ v2) / sigma2\n    else:\n        # Compute orthogonal vector to u1\n        u2 = np.array([-u1[1], u1[0]])\n        u2_norm = np.linalg.norm(u2)\n        if u2_norm < 1e-10:\n            # Handle case where u1 is [0,0], unlikely if sigma2 is zero\n            u2 = np.array([0.0, 1.0])\n        else:\n            u2 /= u2_norm\n    \n    # Construct U matrix from u1 and u2\n    U = np.column_stack((u1, u2))\n    \n    # Construct S matrix\n    S = np.diag([sigma1, sigma2])\n    \n    # Round to 4 decimal places and convert to lists\n    U_rounded = np.round(U, 4)\n    S_rounded = np.round(S, 4)\n    V_rounded = np.round(V, 4)\n    \n    return (U_rounded.tolist(), S_rounded.tolist(), V_rounded.tolist())\n```\n\n### Explanation\n1. **Covariance Matrix**: The matrix product \\( B = A^T A \\) is computed to leverage the properties of symmetric matrices, ensuring real eigenvalues and orthogonal eigenvectors.\n2. **Eigenvalue Decomposition**: Using `numpy.linalg.eigh`, we efficiently compute the eigenvalues and eigenvectors of the symmetric matrix B. The eigenvalues are sorted in descending order to ensure the singular values are in the correct order.\n3. **Singular Values**: The singular values are derived from the square roots of the eigenvalues, sorted in descending order.\n4. **Orthogonal Matrices**: The eigenvectors form the matrix V, and the left singular vectors (columns of U) are computed using the input matrix and the right singular vectors. Special handling ensures orthogonality even when singular values are zero.\n5. **Rounding and Conversion**: The results are rounded to four decimal places and converted to lists to meet the output requirements.\n\nThis approach ensures that the decomposition is both accurate and efficient, adhering to the mathematical properties of SVD while handling edge cases such as zero singular values."}
{"task_id": 29, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that shuffles two numpy arrays, X and y, in the same random order while maintaining the correspondence between their elements. The function should also allow for a seed parameter to ensure reproducibility of the shuffle.\n\n### Approach\n1. **Set the Seed**: If a seed is provided, set the numpy random seed to ensure the shuffle can be reproduced.\n2. **Generate Permutation Indices**: Create a permutation of indices based on the length of the input arrays. This permutation will determine the new order of the elements in both arrays.\n3. **Apply Permutation**: Use the generated permutation indices to reorder both X and y arrays. This ensures that both arrays are shuffled in the same way.\n4. **Convert to Lists**: Convert the shuffled numpy arrays to Python lists using the `tolist()` method and return them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.random.permutation(len(X))\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return X_shuffled.tolist(), y_shuffled.tolist()\n```\n\n### Explanation\n- **Setting the Seed**: The seed parameter allows for reproducible results. By setting the numpy random seed, we ensure that the permutation of indices generated will be the same each time the function is called with the same seed.\n- **Generating Permutation Indices**: Using `np.random.permutation(len(X))`, we generate a shuffled list of indices from 0 to the length of X (or y, assuming they have the same length). This shuffled list of indices is used to reorder both arrays.\n- **Applying Permutation**: By indexing the arrays with the shuffled indices, both X and y are reordered in exactly the same way, maintaining their correspondence.\n- **Converting to Lists**: The shuffled arrays are converted to Python lists using the `tolist()` method, which is necessary to meet the output constraints specified.\n\nThis approach ensures that the arrays are shuffled consistently and efficiently, while also allowing for reproducibility through the optional seed parameter."}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    num_samples = X.shape[0]\n    for i in range(0, num_samples, batch_size):\n        X_batch = X[i:i + batch_size].tolist()\n        if y is not None:\n            y_batch = y[i:i + batch_size].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch\n```"}
{"task_id": 31, "completion_id": 0, "completion": "To solve this problem, we need to divide a dataset into two subsets based on whether the values of a specified feature meet or exceed a given threshold. This is a common task in data preprocessing and analysis, where splitting data based on feature values can help in tasks such as decision tree construction, data filtering, and more.\n\n### Approach\nThe approach involves using efficient array operations provided by NumPy to split the dataset. Here\u2019s a step-by-step breakdown of the approach:\n\n1. **Boolean Mask Creation**: Create a boolean mask where each element indicates whether the corresponding sample's specified feature meets or exceeds the threshold.\n2. **Subset Extraction**: Use the boolean mask to extract rows (samples) that meet the condition. The inverse of the mask is used to extract rows that do not meet the condition.\n3. **Conversion to Lists**: Convert the resulting NumPy arrays to Python lists using the `tolist()` method, ensuring the output meets the specified format requirements.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    mask = X[:, feature_i] >= threshold\n    return X[mask].tolist(), X[~mask].tolist()\n```\n\n### Explanation\n1. **Boolean Mask Creation**: The line `mask = X[:, feature_i] >= threshold` creates a boolean array where each element is `True` if the value of the specified feature (at index `feature_i`) in that row is greater than or equal to the threshold, and `False` otherwise.\n2. **Subset Extraction**: `X[mask]` uses boolean indexing to select all rows where the mask is `True` (i.e., the feature meets or exceeds the threshold). Conversely, `X[~mask]` selects all rows where the mask is `False` (i.e., the feature does not meet the threshold).\n3. **Conversion to Lists**: The `tolist()` method is called on the resulting arrays to convert them into Python lists, which is the required output format.\n\nThis approach efficiently leverages NumPy's optimized operations for quick and memory-efficient array manipulations, making it suitable for large datasets. The solution is both concise and effective, ensuring clarity and performance."}
{"task_id": 32, "completion_id": 0, "completion": "To solve this problem, we need to generate polynomial features for a given dataset up to a specified degree. This involves creating all possible combinations of the input features' products, where the sum of the exponents (degree) of each product does not exceed the specified degree. The result should be a new 2D numpy array converted to a Python list.\n\n### Approach\n1. **Generate Combinations**: For each degree from 1 to the specified degree, generate all possible combinations of feature indices with replacement. This allows us to create terms where each feature can be used multiple times (e.g., x1^2, x1*x2, etc.).\n2. **Compute Products**: For each sample in the input dataset, compute the product of the features corresponding to each combination of indices generated in the previous step. This gives us the polynomial features for that sample.\n3. **Construct Result**: Collect all the computed polynomial features for each sample into a new 2D array and convert it to a Python list using numpy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    m = X.shape[1]\n    all_combinations = []\n    for d in range(1, degree + 1):\n        combs = combinations_with_replacement(range(m), d)\n        all_combinations.extend(combs)\n    \n    X_poly = []\n    for row in X:\n        new_row = []\n        for comb in all_combinations:\n            product = np.prod(row[list(comb)])\n            new_row.append(product)\n        X_poly.append(new_row)\n    \n    return np.array(X_poly).tolist()\n```\n\n### Explanation\n1. **Generate Combinations**: We use `itertools.combinations_with_replacement` to generate all possible combinations of feature indices for each degree from 1 to the specified degree. This helps in creating terms like x1, x1^2, x1*x2, etc.\n2. **Compute Products**: For each row (sample) in the input dataset, we compute the product of the features for each combination of indices. This is done efficiently using `numpy.prod` to handle the multiplication.\n3. **Construct Result**: The resulting polynomial features for each sample are collected into a list of lists. This list is then converted to a numpy array and finally to a Python list using `tolist()` to meet the output constraints.\n\nThis approach ensures that we systematically generate all polynomial features up to the specified degree, providing a clear and efficient solution to the problem."}
{"task_id": 33, "completion_id": 0, "completion": "To solve this problem, we need to generate random subsets of a given dataset. The function should be able to create these subsets either with or without replacement, based on the input parameters. Each subset will be a tuple containing a subset of the input features and the corresponding labels.\n\n### Approach\n1. **Set the Seed**: To ensure reproducibility, we start by setting a seed for the random number generator.\n2. **Determine the Number of Samples**: Extract the number of samples from the input feature array.\n3. **Generate Subsets**:\n   - **With Replacement**: For each subset, generate indices by randomly sampling with replacement. This allows for duplicates in each subset.\n   - **Without Replacement**: For each subset, generate a permutation of the indices. This results in a shuffled version of the original data without any duplicates.\n4. **Convert to Lists**: Convert the selected subsets from numpy arrays to Python lists using the `tolist()` method to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\n```\n\n### Explanation\n- **Setting the Seed**: The `np.random.seed(seed)` ensures that the random number generator produces the same sequence of numbers each time the function is called with the same seed, making the results reproducible.\n- **Indices Generation**: \n  - **With Replacement**: Using `np.random.choice` with `replace=True` allows each index to be selected multiple times, leading to possible duplicates in the subset.\n  - **Without Replacement**: Using `np.random.permutation` generates a shuffled version of the indices, ensuring each index appears exactly once, thus creating a permutation of the original data.\n- **Subset Creation**: For each generated set of indices, the corresponding rows from the feature array `X` and label array `y` are selected. These are then converted to lists and stored as tuples in the result list.\n\nThis approach efficiently generates the required number of subsets, either with or without replacement, ensuring the output meets the specified format and constraints."}
{"task_id": 34, "completion_id": 0, "completion": "To solve this problem, we need to create a function that performs one-hot encoding on a 1D numpy array of integers. One-hot encoding converts each integer into a binary vector where the index corresponding to the integer is set to 1, and all other indices are set to 0. The function should allow an optional parameter to specify the number of columns in the resulting encoded array. If this parameter is not provided, it should be determined automatically based on the input array.\n\n### Approach\n1. **Determine the Number of Columns (n_col):** If the number of columns is not provided, it is calculated as the maximum value in the input array plus 1. This ensures that all possible values in the input array are covered.\n2. **Create a Zero Matrix:** Initialize a 2D numpy array of zeros with dimensions (number of elements in input array, n_col).\n3. **Set Ones at Appropriate Positions:** For each element in the input array, set the corresponding position in the zero matrix to 1 using the integer value as the column index.\n4. **Convert to List:** Convert the resulting numpy array to a Python list using the `tolist()` method and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    encoded = np.zeros((x.size, n_col), dtype=np.int64)\n    rows = np.arange(x.size)\n    cols = x\n    encoded[rows, cols] = 1\n    return encoded.tolist()\n```\n\n### Explanation\n1. **Determine n_col:** The function first checks if `n_col` is provided. If not, it calculates `n_col` as the maximum value in the input array plus 1. This ensures that all values in the input array are valid indices for the columns of the resulting matrix.\n2. **Zero Matrix Initialization:** A 2D numpy array of zeros is created with dimensions (number of elements in input array, n_col). This matrix will hold the one-hot encoded values.\n3. **Setting Values to One:** Using numpy indexing, the function efficiently sets the value at each row and corresponding column (determined by the input array values) to 1. This step ensures that each integer in the input array is converted to a binary vector with a 1 at the index corresponding to the integer.\n4. **Conversion to List:** Finally, the numpy array is converted to a Python list using the `tolist()` method to meet the output requirements.\n\nThis approach efficiently handles the conversion of integers to their one-hot encoded form, ensuring correctness and optimal performance using numpy operations."}
{"task_id": 35, "completion_id": 0, "completion": "To solve this problem, we need to convert a 1D numpy array into a diagonal matrix and return the result as a Python list. The diagonal matrix should have the elements of the input array on its main diagonal and zeros elsewhere. \n\n### Approach\n1. **Understanding Diagonal Matrices**: A diagonal matrix is a 2D matrix where all elements outside the main diagonal are zero. The main diagonal elements are the same as the elements of the input 1D array.\n2. **Using numpy.diag()**: The numpy library provides a function `np.diag()` that takes a 1D array and returns a 2D diagonal matrix. This function efficiently constructs the required matrix.\n3. **Conversion to Python List**: Once the diagonal matrix is created using numpy, we convert it to a Python list using the `tolist()` method provided by numpy arrays. This method ensures the numpy array is transformed into a nested list structure suitable for Python.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    return np.diag(x).tolist()\n```\n\n### Explanation\n- **Input Handling**: The function `make_diagonal` takes a 1D numpy array `x` as input.\n- **Diagonal Matrix Construction**: Using `np.diag(x)`, we create a 2D numpy array where the elements of `x` are placed on the main diagonal, and all other elements are zero.\n- **Conversion to List**: The resulting numpy array is converted to a Python list of lists using the `tolist()` method, ensuring the output meets the specified format.\n\nThis approach efficiently leverages numpy's built-in functions for both matrix creation and type conversion, ensuring clarity and performance."}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    correct = np.sum(y_true == y_pred)\n    total = y_true.size\n    accuracy = correct / total\n    return round(accuracy, 4)\n```"}
{"task_id": 37, "completion_id": 0, "completion": "To solve this problem, we need to calculate the correlation matrix for a given dataset. The function should handle two cases: when only one dataset (X) is provided, it computes the correlation matrix of X with itself. When a second dataset (Y) is provided, it computes the cross-correlation matrix between the columns of X and Y.\n\n### Approach\n1. **Input Handling**: If the second dataset Y is not provided, set Y to be the same as X.\n2. **Standardization**: Standardize both X and Y by subtracting their column means and dividing by their column standard deviations. This ensures that each column has a mean of 0 and a standard deviation of 1.\n3. **Matrix Multiplication**: Compute the matrix product of the transposed standardized X matrix and the standardized Y matrix. This product is then scaled by dividing by the number of samples minus one (n-1), which gives the Pearson correlation coefficients.\n4. **Rounding and Conversion**: Round the resulting correlation matrix to 4 decimal places and convert it to a Python list using NumPy's `tolist()` method for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError(\"X and Y must have the same number of rows.\")\n    n = X.shape[0]\n    X_mean = np.mean(X, axis=0)\n    Y_mean = np.mean(Y, axis=0)\n    X_std = np.std(X, axis=0, ddof=1)\n    Y_std = np.std(Y, axis=0, ddof=1)\n    X_normalized = (X - X_mean) / X_std\n    Y_normalized = (Y - Y_mean) / Y_std\n    corr_matrix = (X_normalized.T @ Y_normalized) / (n - 1)\n    corr_rounded = np.round(corr_matrix, 4)\n    return corr_rounded.tolist()\n```\n\n### Explanation\n1. **Standardization**: By standardizing the columns of X and Y, we convert each column to have a mean of 0 and a standard deviation of 1. This step is crucial because the Pearson correlation coefficient is the covariance of the standardized variables.\n2. **Matrix Multiplication**: The product of the transposed standardized X matrix and the standardized Y matrix gives the sum of the element-wise products of the columns of X and Y. Dividing this product by (n-1) scales it to the covariance of the standardized variables, which is the correlation coefficient.\n3. **Handling Edge Cases**: If a column in X or Y has zero variance, this would result in a division by zero during standardization. However, the problem assumes valid input data where such cases do not occur or are handled externally.\n\nThis approach efficiently computes the correlation matrix using matrix operations, ensuring both correctness and performance. The result is rounded and converted to a list format as specified, making it easy to use in further applications."}
{"task_id": 38, "completion_id": 0, "completion": "To solve this problem, we need to implement the AdaBoost algorithm to fit a model using a given number of weak classifiers (decision stumps). The AdaBoost algorithm works by iteratively adjusting the weights of training samples to focus on misclassified samples, thereby improving the model's accuracy over time.\n\n### Approach\n1. **Initialize Sample Weights**: Start with uniform weights for all samples.\n2. **Iterate for Each Classifier**: For each weak classifier, determine the best decision stump based on the current sample weights.\n3. **Find Best Decision Stump**:\n   - For each feature, sort the samples and compute potential thresholds.\n   - Calculate the weighted error for each possible threshold and determine the optimal split that minimizes this error.\n   - Track the best feature, threshold, and direction (majority class for each split region).\n4. **Compute Classifier Weight (Alpha)**: Using the error from the best decision stump, compute the weight of the classifier.\n5. **Update Sample Weights**: Adjust weights to focus more on misclassified samples and normalize them.\n6. **Store Classifier Parameters**: Save the parameters of each weak classifier, rounding numerical values to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    \n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_feature = None\n        best_threshold = None\n        best_a = None\n        best_b = None\n        \n        for j in range(n_features):\n            feature_vals = X[:, j]\n            sorted_idx = np.argsort(feature_vals)\n            sorted_feature = feature_vals[sorted_idx]\n            sorted_y = y[sorted_idx]\n            sorted_weights = weights[sorted_idx]\n            \n            # Compute cumulative sums for class 1 and -1\n            cum_sum_1 = np.cumsum((sorted_y == 1) * sorted_weights)\n            cum_sum_neg1 = np.cumsum((sorted_y == -1) * sorted_weights)\n            total_1 = cum_sum_1[-1] if len(cum_sum_1) > 0 else 0.0\n            total_neg1 = cum_sum_neg1[-1] if len(cum_sum_neg1) > 0 else 0.0\n            \n            min_error_j = float('inf')\n            best_threshold_j = None\n            best_a_j = None\n            best_b_j = None\n            \n            for k in range(len(sorted_feature) + 1):\n                if k == 0:\n                    sum_1_left = 0.0\n                    sum_neg1_left = 0.0\n                else:\n                    sum_1_left = cum_sum_1[k-1]\n                    sum_neg1_left = cum_sum_neg1[k-1]\n                sum_1_right = total_1 - sum_1_left\n                sum_neg1_right = total_neg1 - sum_neg1_left\n                \n                # Determine a and b\n                a = 1 if sum_1_right > sum_neg1_right else -1\n                error_right = sum_neg1_right if a == 1 else sum_1_right\n                b = 1 if sum_1_left > sum_neg1_left else -1\n                error_left = sum_neg1_left if b == 1 else sum_1_left\n                total_error = error_left + error_right\n                \n                # Compute threshold\n                if k == 0:\n                    current_threshold = sorted_feature[0] - 1e-5\n                elif k == len(sorted_feature):\n                    current_threshold = sorted_feature[-1] + 1e-5\n                else:\n                    current_threshold = (sorted_feature[k-1] + sorted_feature[k]) / 2\n                \n                if total_error < min_error_j:\n                    min_error_j = total_error\n                    best_threshold_j = current_threshold\n                    best_a_j = a\n                    best_b_j = b\n            \n            if min_error_j < best_error:\n                best_error = min_error_j\n                best_feature = j\n                best_threshold = best_threshold_j\n                best_a = best_a_j\n                best_b = best_b_j\n        \n        # Compute alpha\n        epsilon = 1e-10\n        numerator = (1 - best_error + epsilon)\n        denominator = (best_error + epsilon)\n        alpha = 0.5 * math.log(numerator / denominator)\n        \n        # Update weights\n        predictions = np.where(X[:, best_feature] >= best_threshold, best_a, best_b)\n        incorrect = (predictions != y)\n        weights *= np.exp(alpha * incorrect)\n        weights /= np.sum(weights)\n        \n        # Round to 4 decimal places\n        alpha_rounded = round(alpha, 4)\n        threshold_rounded = round(best_threshold, 4)\n        \n        classifiers.append({\n            'feature': best_feature,\n            'threshold': threshold_rounded,\n            'a': best_a,\n            'b': best_b,\n            'alpha': alpha_rounded\n        })\n    \n    return classifiers\n```\n\n### Explanation\n1. **Initialization**: Each sample's weight is initialized uniformly.\n2. **Finding the Best Decision Stump**: For each feature, potential thresholds are evaluated. The threshold that minimizes the weighted classification error is selected.\n3. **Alpha Calculation**: The classifier's weight (alpha) is computed using the error rate, ensuring numerical stability with a small epsilon.\n4. **Weight Update**: Sample weights are adjusted to focus on misclassified samples, then normalized.\n5. **Storing Classifiers**: Each classifier's parameters (feature, threshold, direction, and alpha) are stored with values rounded to four decimal places.\n\nThis approach ensures that each subsequent classifier focuses on the samples that previous classifiers misclassified, leading to a strong ensemble model."}
{"task_id": 39, "completion_id": 0, "completion": "To solve this problem, we need to compute the log-softmax of a 1D array of scores. The log-softmax function is used in machine learning for numerical stability when dealing with large numbers, and it converts a vector of scores into log probabilities.\n\n### Approach\nThe log-softmax function is defined as the logarithm of the softmax function. The softmax function converts a vector of scores into probabilities by exponentiating each element and normalizing by the sum of these exponentials. The log-softmax function is then the logarithm of these probabilities.\n\nTo ensure numerical stability, especially with large scores, we use a technique where we subtract the maximum score from each element before exponentiating. This approach avoids overflow issues by scaling down the exponent values. The steps involved are:\n\n1. **Subtract the Maximum Value**: Subtract the maximum value from each score to prevent large exponentials, which can cause numerical instability.\n2. **Exponentiate the Shifted Scores**: Compute the exponentials of these shifted scores.\n3. **Sum the Exponentiated Values**: Calculate the sum of these exponentials.\n4. **Compute the Logarithm of the Sum**: This step is necessary to compute the log-softmax values.\n5. **Adjust and Normalize**: Subtract the logarithm of the sum from the shifted scores to get the log-softmax values.\n6. **Rounding and Conversion**: Round the results to four decimal places and convert the numpy array back to a list.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    scores_np = np.array(scores)\n    x_max = scores_np.max()\n    shifted = scores_np - x_max\n    exp_shifted = np.exp(shifted)\n    sum_exp = exp_shifted.sum()\n    log_sum_exp = np.log(sum_exp)\n    result = shifted - log_sum_exp\n    rounded = np.round(result, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Subtracting the Maximum Value**: By subtracting the maximum value from each score, we ensure that the largest value in the shifted scores is zero. This prevents overflow when exponentiating large numbers.\n2. **Exponentiating Shifted Scores**: This step converts the shifted scores into positive values, which are then summed to form the denominator of the softmax function.\n3. **Summing Exponentiated Values**: The sum of the exponentiated values acts as a normalization factor to ensure the softmax values sum to 1.\n4. **Logarithm of the Sum**: Taking the logarithm of this sum allows us to compute the log-softmax values in a numerically stable manner.\n5. **Adjusting Shifted Scores**: Finally, we subtract the logarithm of the sum from each shifted score to obtain the log-softmax values, which are then rounded to four decimal places and converted back to a list.\n\nThis approach efficiently computes the log-softmax values while maintaining numerical stability and ensuring the results are in the required format."}
{"task_id": 40, "completion_id": 0, "completion": "To solve this problem, we need to implement a custom Dense layer in Python that extends a given base `Layer` class. This Dense layer will represent a fully connected neural network layer and must include methods for initialization, forward and backward propagation, parameter counting, and output shape determination.\n\n### Approach\n1. **Initialization (`__init__`)**: The Dense layer is initialized with a specified number of neurons and an optional input shape. It sets up placeholders for weights, biases, and optimizers.\n2. **Weight Initialization (`initialize`)**: Weights are initialized using a uniform distribution scaled by the inverse square root of the input dimension to prevent gradient issues. Biases are initialized to zeros.\n3. **Parameter Count (`parameters`)**: Returns the total number of trainable parameters, which is the sum of the elements in the weight matrix and bias vector.\n4. **Forward Pass (`forward_pass`)**: Computes the output by performing a matrix multiplication of the input with the weights and adding the bias.\n5. **Backward Pass (`backward_pass`)**: Computes gradients with respect to the input and updates the weights and biases using their respective optimizers if the layer is trainable.\n6. **Output Shape (`output_shape`)**: Returns the shape of the output produced by the forward pass.\n\n### Solution Code\n```python\nimport numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        super().__init__()\n        self.layer_input = None\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n        if input_shape is not None:\n            self.set_input_shape(input_shape)\n\n    def initialize(self):\n        if self.input_shape is None:\n            raise ValueError(\"Input shape must be set before initializing weights.\")\n        input_dim = self.input_shape[0]\n        limit = 1.0 / math.sqrt(input_dim)\n        self.W = np.random.uniform(-limit, limit, (input_dim, self.n_units))\n        self.w0 = np.zeros(self.n_units)\n        self.W_opt = copy.deepcopy(self.W_opt) if self.W_opt else None\n        self.w0_opt = copy.deepcopy(self.w0_opt) if self.w0_opt else None\n\n    def parameters(self):\n        if self.W is None or self.w0 is None:\n            return 0\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        grad_input = np.dot(accum_grad, self.W.T)\n        if self.trainable:\n            grad_W = np.dot(self.layer_input.T, accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=False)\n            if self.W_opt is not None:\n                self.W = self.W_opt.update(self.W, grad_W)\n            if self.w0_opt is not None:\n                self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        return grad_input\n\n    def output_shape(self):\n        return (self.n_units,)\n```\n\n### Explanation\n1. **Initialization (`__init__`)**: The constructor initializes the number of units, input shape, and placeholders for weights, biases, and optimizers. If an input shape is provided, it is set using the base class method.\n2. **Weight Initialization (`initialize`)**: Weights are initialized with a uniform distribution scaled by the inverse square root of the input dimension to ensure stable training. Biases are initialized to zeros. Optimizers are deep copied if provided.\n3. **Parameter Count (`parameters`)**: Returns the sum of the number of elements in the weight matrix and bias vector.\n4. **Forward Pass (`forward_pass`)**: Computes the output using matrix multiplication and adds the bias, storing the input for use in backpropagation.\n5. **Backward Pass (`backward_pass`)**: Computes gradients for the input, weights, and biases. If the layer is trainable, it updates the weights and biases using their respective optimizers.\n6. **Output Shape (`output_shape`)**: Returns the shape of the output as a tuple containing the number of units in the layer.\n\nThis implementation ensures that the Dense layer can be integrated into a neural network framework, supporting both forward and backward propagation, parameter management, and dynamic input/output shape handling."}
{"task_id": 41, "completion_id": 0, "completion": "To solve this problem, we need to implement a 2D convolutional layer in Python using numpy. This function will process an input matrix using a specified convolutional kernel, padding, and stride. The result should be a list of lists, rounded to four decimal places.\n\n### Approach\n1. **Padding**: First, apply padding to the input matrix to ensure the convolution operation can be applied to the borders of the input matrix. The padding amount is specified by the `padding` parameter.\n2. **Output Dimensions Calculation**: Compute the dimensions of the output matrix based on the input size, kernel size, padding, and stride. This is done using the formula:\n   - `output_height = (padded_input_height - kernel_height) // stride + 1`\n   - `output_width = (padded_input_width - kernel_width) // stride + 1`\n3. **Convolution Operation**: Slide the kernel over the padded input matrix using the specified stride. For each position, compute the element-wise multiplication of the kernel and the corresponding window of the input matrix, then sum these products to get the output value for that position.\n4. **Rounding and Conversion**: Round the resulting values to four decimal places and convert the numpy array to a Python list using `tolist()`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Apply padding to the input matrix\n    padded_input = np.pad(input_matrix, \n                          pad_width=((padding, padding), (padding, padding)), \n                          mode='constant', \n                          constant_values=0)\n    \n    # Get the dimensions of the padded input and kernel\n    H_pad, W_pad = padded_input.shape\n    K_h, K_w = kernel.shape\n    \n    # Calculate output dimensions\n    output_h = (H_pad - K_h) // stride + 1\n    output_w = (W_pad - K_w) // stride + 1\n    \n    # Initialize the output matrix\n    output = np.zeros((output_h, output_w))\n    \n    # Perform the convolution\n    for i in range(output_h):\n        for j in range(output_w):\n            h_start = i * stride\n            h_end = h_start + K_h\n            w_start = j * stride\n            w_end = w_start + K_w\n            \n            window = padded_input[h_start:h_end, w_start:w_end]\n            output[i, j] = np.sum(window * kernel)\n    \n    # Round to four decimal places and convert to list\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\n```\n\n### Explanation\n1. **Padding**: The input matrix is padded with zeros around its borders using `np.pad`. The padding amount is specified by the `padding` parameter, which adds the same number of zeros on all sides of the matrix.\n2. **Output Dimensions**: The output dimensions are calculated based on the padded input size, kernel size, and stride. This ensures that the kernel slides over the input matrix correctly without exceeding its bounds.\n3. **Convolution Operation**: For each position in the output matrix, a window of the same size as the kernel is extracted from the padded input. The element-wise product of this window and the kernel is computed, and the sum of these products gives the output value for that position.\n4. **Rounding and Conversion**: The resulting values are rounded to four decimal places to meet the problem's requirements and converted to a Python list for the final output.\n\nThis approach efficiently handles the convolution operation with specified padding and stride, ensuring accurate results even for edge cases."}
{"task_id": 42, "completion_id": 0, "completion": "To solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function. The ReLU function is a simple yet widely used activation function in neural networks. It outputs the input directly if it is positive; otherwise, it outputs zero. This function helps introduce non-linearity into the model, allowing it to learn more complex patterns.\n\n### Approach\nThe ReLU function can be implemented using a straightforward conditional check. The function takes a single float as input and returns the input value if it is greater than zero. If the input is zero or negative, the function returns zero. This can be efficiently achieved using a conditional expression in Python, which checks if the input value is greater than zero and returns the appropriate result based on this check.\n\n### Solution Code\n```python\ndef relu(z: float) -> float:\n    return z if z > 0 else 0\n```\n\n### Explanation\nThe provided code defines a function `relu` that takes a single float argument `z`. The function uses a conditional expression to check if `z` is greater than zero. If the condition is true, it returns `z`; otherwise, it returns 0. This implementation efficiently captures the essence of the ReLU activation function with minimal computational overhead, making it suitable for use in various machine learning models where activation functions are required. \n\nBy using a simple conditional check, the function ensures that it adheres to the mathematical definition of ReLU, which is:\n- ReLU(z) = z if z > 0\n- ReLU(z) = 0 otherwise\n\nThis approach is both time-efficient and easy to understand, making it ideal for applications requiring quick activation computations."}
{"task_id": 43, "completion_id": 0, "completion": "To solve this problem, we need to implement the Ridge Regression loss function, which combines the Mean Squared Error (MSE) and a regularization term. The Ridge loss is calculated by adding the MSE of the predictions to a penalty term that is proportional to the sum of the squares of the coefficients. This penalty term helps prevent overfitting by discouraging large coefficients.\n\n### Approach\n1. **Predictions**: Compute the predicted values using the feature matrix \\( X \\) and coefficient vector \\( w \\).\n2. **Mean Squared Error (MSE)**: Calculate the MSE between the predicted values and the true labels. The MSE is the average of the squared differences between the predicted and true values.\n3. **Regularization Term**: Compute the L2 regularization term using the sum of the squares of the coefficients multiplied by the regularization parameter \\( \\alpha \\).\n4. **Total Loss**: Sum the MSE and the regularization term to get the total Ridge loss.\n5. **Rounding**: Round the resulting loss to four decimal places for consistency.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X.dot(w)\n    mse = np.mean((y_pred - y_true) ** 2)\n    reg = alpha * np.sum(w ** 2)\n    total_loss = mse + reg\n    return round(total_loss, 4)\n```\n\n### Explanation\n- **Predictions**: The predicted values \\( \\hat{y} \\) are computed using the dot product of the feature matrix \\( X \\) and the coefficient vector \\( w \\).\n- **MSE Calculation**: The MSE is calculated by taking the mean of the squared differences between the predicted values \\( \\hat{y} \\) and the true labels \\( y_{\\text{true}} \\).\n- **Regularization Term**: The regularization term is computed as \\( \\alpha \\) multiplied by the sum of the squares of the coefficients \\( w \\). This term penalizes large coefficients to prevent overfitting.\n- **Total Loss**: The total Ridge loss is the sum of the MSE and the regularization term. This combined loss is then rounded to four decimal places to meet the specified output constraints.\n\nThis approach ensures that we correctly compute the Ridge Regression loss while efficiently utilizing vector operations provided by NumPy for performance and readability."}
{"task_id": 44, "completion_id": 0, "completion": "To solve this problem, we need to implement the Leaky Rectified Linear Unit (Leaky ReLU) activation function in Python. This function is commonly used in neural networks to introduce non-linearity while addressing the \"dying ReLU\" problem by allowing a small, non-zero gradient when the input is negative.\n\n### Approach\nThe Leaky ReLU function operates as follows:\n- For positive inputs, it returns the input value unchanged.\n- For non-positive inputs (zero or negative), it returns the input value multiplied by a small slope parameter (alpha), which is typically set to 0.01 by default.\n\nThe key steps to implement this function are:\n1. Check if the input value `z` is positive. If it is, return `z` as is.\n2. If `z` is zero or negative, return the product of `z` and the slope parameter `alpha`.\n\n### Solution Code\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float | int:\n    return z if z > 0 else alpha * z\n```\n\n### Explanation\n- **Positive Input Handling**: When the input `z` is greater than zero, the function directly returns `z`. This maintains the identity property of ReLU for positive values, ensuring no change is made to positive inputs.\n- **Non-Positive Input Handling**: For inputs that are zero or negative, the function returns `alpha * z`. This introduces a small gradient (controlled by `alpha`) for non-positive inputs, preventing the neuron from becoming completely inactive during training.\n- **Default Parameter**: The `alpha` parameter is optional and defaults to 0.01, a common choice in practice. This allows users to adjust the slope for negative inputs if needed, providing flexibility for different scenarios.\n\nThis implementation efficiently handles both positive and non-positive inputs with a simple conditional check, ensuring computational efficiency and correctness. The return type is either `float` or `int`, depending on the input type and the computation result, which Python handles dynamically."}
{"task_id": 45, "completion_id": 0, "completion": "To solve this problem, we need to compute the linear kernel between two input vectors using the dot product, which is the sum of the products of the corresponding elements of the vectors. The solution should efficiently handle vectors represented as 1D NumPy arrays.\n\n### Approach\nThe linear kernel between two vectors is defined as their dot product. The dot product of two vectors is calculated by multiplying corresponding elements and summing the results. Using NumPy's `np.dot` function, we can efficiently compute this dot product. The function `kernel_function` takes two vectors (1D arrays) as input and returns their dot product using `np.dot`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)\n```\n\n### Explanation\n1. **Import NumPy**: The NumPy library is imported to utilize its efficient array operations.\n2. **Define the Function**: The function `kernel_function` is defined to take two parameters, `x1` and `x2`, which are expected to be 1D NumPy arrays.\n3. **Compute Dot Product**: The function returns the result of `np.dot(x1, x2)`, which computes the dot product of the two input vectors. This operation is efficient and handles the element-wise multiplication and summation internally.\n\nThis approach ensures that the function works correctly for 1D arrays, which are standard representations of vectors in numerical computations. The use of NumPy guarantees both efficiency and simplicity in the implementation."}
{"task_id": 46, "completion_id": 0, "completion": "To solve this problem, we need to calculate the precision metric given two numpy arrays, `y_true` and `y_pred`, which contain true binary labels and predicted binary labels, respectively. Precision is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP). \n\n### Approach\n1. **Calculate True Positives (TP)**: These are the cases where both the true label and the predicted label are 1. This can be computed using a boolean mask to find elements where both arrays are 1 and summing the result.\n2. **Calculate False Positives (FP)**: These are the cases where the predicted label is 1 but the true label is 0. Similarly, this can be computed using a boolean mask to find elements where the predicted label is 1 and the true label is 0, then summing the result.\n3. **Handle Division by Zero**: If the sum of TP and FP (denominator) is zero, return 0.0 to avoid division by zero. This situation occurs when there are no predicted positive cases.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    denominator = tp + fp\n    if denominator == 0:\n        return 0.0\n    return tp / denominator\n```\n\n### Explanation\n1. **True Positives Calculation**: The expression `(y_true == 1) & (y_pred == 1)` creates a boolean array where each element is `True` if both the corresponding elements in `y_true` and `y_pred` are 1. Summing this array gives the count of true positives.\n2. **False Positives Calculation**: The expression `(y_pred == 1) & (y_true == 0)` creates a boolean array where each element is `True` if the predicted label is 1 but the true label is 0. Summing this array gives the count of false positives.\n3. **Division Handling**: The denominator is the sum of TP and FP. If this sum is zero (i.e., no predicted positives), we return 0.0 to handle the division by zero case. Otherwise, we return the ratio of TP to the sum of TP and FP.\n\nThis approach efficiently computes the precision metric using vectorized operations for performance and handles edge cases where no positive predictions are made."}
{"task_id": 47, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs three variants of gradient descent\u2014Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent\u2014using Mean Squared Error (MSE) as the loss function. The function will adjust the batch size based on the specified method and update the model weights iteratively.\n\n### Approach\n1. **Determine Batch Size**: Based on the specified method, adjust the batch size:\n   - **Batch Gradient Descent**: Use the entire dataset for each iteration.\n   - **Stochastic Gradient Descent (SGD)**: Use one sample per iteration.\n   - **Mini-Batch Gradient Descent**: Use a user-specified batch size.\n   \n2. **Iterative Weight Update**: For each iteration, select a batch of data, compute the predictions, calculate the error, and update the weights using the gradient of the MSE loss.\n\n3. **Gradient Calculation**: The gradient of the MSE loss is computed using the formula \\((X_{\\text{batch}}^T \\cdot (y_{\\text{pred}} - y_{\\text{batch}})) / \\text{batch\\_size}\\).\n\n4. **Weight Update**: Adjust the weights by subtracting the product of the learning rate and the computed gradient.\n\n5. **Result Formatting**: After completing all iterations, round the weights to four decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    X = np.array(X)\n    y = np.array(y).flatten()\n    weights = np.array(weights).flatten()\n    \n    n_samples = X.shape[0]\n    \n    if method == 'batch':\n        batch_size = n_samples\n    elif method == 'sgd':\n        batch_size = 1\n    \n    for _ in range(n_iterations):\n        if method == 'batch':\n            X_batch = X\n            y_batch = y\n        else:\n            indices = np.random.choice(n_samples, batch_size, replace=True)\n            X_batch = X[indices]\n            y_batch = y[indices]\n        \n        y_pred = np.dot(X_batch, weights)\n        error = y_pred - y_batch\n        gradient = np.dot(X_batch.T, error) / batch_size\n        weights -= learning_rate * gradient\n    \n    weights_rounded = np.round(weights, 4)\n    return weights_rounded.tolist()\n```\n\n### Explanation\n1. **Input Conversion**: Convert input data (X, y, weights) to numpy arrays to ensure compatibility with numpy operations.\n2. **Batch Size Adjustment**: Depending on the method specified, adjust the batch size to use the entire dataset for Batch GD, one sample for SGD, or the specified batch size for Mini-Batch GD.\n3. **Batch Selection**: For each iteration, select a random batch of data. For Batch GD, the entire dataset is used. For SGD and Mini-Batch, random samples are selected with replacement.\n4. **Gradient Calculation**: Compute the gradient of the MSE loss using the selected batch. The gradient is derived from the formula for the MSE loss derivative.\n5. **Weight Update**: Adjust the model weights iteratively using the computed gradient and learning rate.\n6. **Result Formatting**: After completing all iterations, the weights are rounded to four decimal places and converted to a list for the final output.\n\nThis approach efficiently handles different gradient descent variants by adjusting the batch size and selecting appropriate data samples for each iteration, ensuring optimal weight updates using the MSE loss function."}
{"task_id": 48, "completion_id": 0, "completion": "To solve this problem, we need to convert a given matrix into its Reduced Row Echelon Form (RREF). The RREF of a matrix is a special form where each leading entry (pivot) in a row is 1, and all other elements in the column containing the pivot are zeros. The pivots are positioned to the right of the pivots in the rows above, and rows with all zeros are at the bottom.\n\n### Approach\n1. **Initialization**: Convert the input matrix into a numpy array for efficient numerical operations.\n2. **Iterate through Columns**: For each column, determine the pivot position starting from the current row.\n3. **Find Pivot**: Identify the row with the maximum absolute value in the current column starting from the pivot row. This helps in numerical stability.\n4. **Row Swapping**: If the identified pivot element is non-zero, swap the current row with the pivot row.\n5. **Normalize Pivot Row**: Divide the pivot row by the pivot element to set the leading entry to 1.\n6. **Eliminate Other Entries**: Subtract appropriate multiples of the pivot row from all other rows to ensure all other entries in the pivot column are zero.\n7. **Adjust Pivot Row**: Move to the next row and repeat the process until all rows are processed.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rref(matrix):\n    mat = np.array(matrix, dtype=float).copy()\n    rows, cols = mat.shape\n    pivot_row = 0\n    \n    for col in range(cols):\n        if pivot_row >= rows:\n            break\n        \n        # Find the maximum element in the current column below and including the pivot_row\n        row_idx = np.argmax(np.abs(mat[pivot_row:, col])) + pivot_row\n        pivot_val = mat[row_idx, col]\n        \n        if pivot_val == 0:\n            continue  # Move to the next column if all elements are zero\n        \n        # Swap the current row with the pivot row\n        mat[[pivot_row, row_idx]] = mat[[row_idx, pivot_row]]\n        \n        # Normalize the pivot row\n        mat[pivot_row] = mat[pivot_row] / mat[pivot_row, col]\n        \n        # Eliminate all other entries in this column\n        for r in range(rows):\n            if r != pivot_row:\n                factor = mat[r, col]\n                mat[r] -= factor * mat[pivot_row]\n        \n        pivot_row += 1\n    \n    return mat.tolist()\n```\n\n### Explanation\n- **Initialization**: The input matrix is converted to a numpy array to leverage numpy's efficient array operations.\n- **Column Iteration**: Each column is processed to find the pivot element. If the column contains all zeros, it is skipped.\n- **Pivot Selection**: The pivot is chosen as the element with the maximum absolute value in the column to minimize numerical errors.\n- **Row Operations**: Swapping rows ensures the pivot is in the correct position. Normalizing the pivot row sets the leading entry to 1. Subsequent row operations eliminate other entries in the pivot column, ensuring each leading 1 is the only non-zero entry in its column.\n- **Result Conversion**: The final matrix is converted back to a Python list using numpy's `tolist()` method to meet the output requirements.\n\nThis approach systematically transforms the matrix into RREF by iterating through each column and performing necessary row operations, ensuring all leading entries are 1 and other entries in their columns are zero."}
{"task_id": 49, "completion_id": 0, "completion": "To solve this problem, we need to implement the Adam optimization algorithm, which adapts the learning rate for each parameter during the optimization process. Adam combines the advantages of two other optimization algorithms: Momentum and RMSprop, making it efficient and effective for a wide range of problems.\n\n### Approach\nThe Adam optimization algorithm maintains two moving averages for each parameter: the first moment (mean) and the second moment (uncentered variance) of the gradients. The steps involved in the algorithm are as follows:\n\n1. **Initialize Parameters**: Start with initial parameter values and initialize the first and second moment estimates to zero.\n2. **Iterative Update**: For each iteration:\n   - Compute the gradient of the objective function at the current parameters.\n   - Update the first and second moment estimates using exponential decay rates.\n   - Compute bias-corrected estimates of the first and second moments to account for initialization bias.\n   - Update the parameters using the learning rate, bias-corrected moments, and a small constant for numerical stability.\n3. **Finalize Result**: After completing all iterations, round the optimized parameters to four decimal places and return them as a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    x = np.array(x0, dtype=np.float64).copy()\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: The initial parameters `x0` are converted to a NumPy array for efficient computation. The first (`m`) and second (`v`) moment vectors are initialized to zero.\n2. **Gradient Computation**: In each iteration, the gradient of the objective function is computed at the current parameter values.\n3. **Moment Updates**: The first moment `m` is updated using a weighted average of the previous moments and the current gradient. Similarly, the second moment `v` is updated using a weighted average of the squared gradients.\n4. **Bias Correction**: Since the initial moments are zero, bias-corrected estimates `m_hat` and `v_hat` are computed to adjust for this initialization bias.\n5. **Parameter Update**: The parameters are updated using the learning rate, bias-corrected moments, and a small constant to avoid division by zero.\n6. **Result Finalization**: The optimized parameters are rounded to four decimal places and converted to a Python list for the final output.\n\nThis approach ensures that the learning rate is adaptively adjusted for each parameter, leading to efficient convergence towards the minimum of the objective function."}
{"task_id": 50, "completion_id": 0, "completion": "To solve this problem, we need to implement Lasso Regression using Gradient Descent. Lasso Regression (L1 Regularization) adds a penalty equal to the absolute value of the coefficients to the loss function, which helps in feature selection by shrinking some coefficients to zero.\n\n### Approach\n1. **Initialization**: Start with initial weights as zeros and bias as zero.\n2. **Gradient Calculation**: Compute the gradients for weights and bias. The gradient for weights includes the contribution from both the mean squared error (MSE) and the L1 regularization term. The gradient for bias only includes the contribution from the MSE.\n3. **Update Parameters**: Adjust the weights and bias using the computed gradients and the learning rate.\n4. **Convergence Check**: Stop the iterations if the change in parameters (weights and bias) falls below a specified tolerance.\n5. **Rounding**: Round the final weights and bias to four decimal places and return them as lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    \n    for _ in range(max_iter):\n        y_pred = X.dot(w) + b\n        error = y_pred - y\n        \n        # Compute gradients\n        gradient_w = (X.T.dot(error) / n_samples) + alpha * np.sign(w)\n        gradient_b = np.sum(error) / n_samples\n        \n        # Save previous parameters for convergence check\n        prev_w = w.copy()\n        prev_b = b\n        \n        # Update parameters\n        w -= learning_rate * gradient_w\n        b -= learning_rate * gradient_b\n        \n        # Check convergence based on parameter changes\n        current_params = np.append(w, b)\n        prev_params = np.append(prev_w, prev_b)\n        if np.linalg.norm(current_params - prev_params) < tol:\n            break\n    \n    # Round to four decimal places and convert to Python list and float\n    w_rounded = np.round(w, 4)\n    b_rounded = np.round(b, 4)\n    \n    return w_rounded.tolist(), b_rounded\n```\n\n### Explanation\n1. **Initialization**: We start with weights as a zero vector and bias as zero. This is a common starting point for gradient descent algorithms.\n2. **Prediction and Error Calculation**: For each iteration, compute the predicted values using the current weights and bias, then calculate the error between the predicted and actual values.\n3. **Gradient Calculation**:\n   - **Weights Gradient**: The gradient for weights includes the dot product of the transpose of the feature matrix with the error (scaled by the number of samples) and the L1 regularization term (alpha times the sign of the weights).\n   - **Bias Gradient**: The gradient for bias is the average of the errors.\n4. **Parameter Update**: Adjust the weights and bias by subtracting the product of the learning rate and their respective gradients.\n5. **Convergence Check**: The algorithm stops if the Euclidean norm of the change in parameters (weights and bias) is less than the specified tolerance.\n6. **Rounding**: The final weights and bias are rounded to four decimal places to meet the output requirements.\n\nThis approach efficiently combines the MSE loss with L1 regularization, allowing the model to perform feature selection by driving some coefficients to zero while maintaining computational efficiency through gradient descent."}
{"task_id": 51, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Optimal String Alignment (OSA) distance between two given strings. The OSA distance is the minimum number of edit operations (insertion, deletion, substitution, and transposition of adjacent characters) required to transform one string into another, with each operation costing 1 unit.\n\n### Approach\nThe approach to solve this problem involves using dynamic programming (DP) to build a table where each cell `dp[i][j]` represents the minimum number of edits needed to convert the first `i` characters of the source string into the first `j` characters of the target string. The key operations considered are:\n1. **Insertion**: Adding a character from the target string.\n2. **Deletion**: Removing a character from the source string.\n3. **Substitution**: Replacing a character in the source string with a character from the target string.\n4. **Transposition**: Swapping two adjacent characters in the source string to match the target string, if applicable.\n\nThe transposition operation is checked only when the current and previous characters in the source and target strings can be swapped to match each other.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    len_s = len(source)\n    len_t = len(target)\n    \n    # Initialize the DP table with dimensions (len_s + 1) x (len_t + 1)\n    dp = np.zeros((len_s + 1, len_t + 1), dtype=int)\n    \n    # Base cases: transforming to/from empty string requires all insertions or deletions\n    for i in range(len_s + 1):\n        dp[i][0] = i  # Deleting all characters from source[:i] to get empty string\n    for j in range(len_t + 1):\n        dp[0][j] = j  # Inserting all characters to empty string to get target[:j]\n    \n    # Fill the DP table\n    for i in range(1, len_s + 1):\n        for j in range(1, len_t + 1):\n            # Check if the current characters are the same; substitution cost is 0 if same, else 1\n            substitution_cost = 0 if source[i-1] == target[j-1] else 1\n            # Calculate the minimum cost for insertion, deletion, substitution\n            dp[i][j] = min(\n                dp[i-1][j] + 1,        # Deletion cost\n                dp[i][j-1] + 1,        # Insertion cost\n                dp[i-1][j-1] + substitution_cost  # Substitution cost\n            )\n            # Check for possible transposition of adjacent characters\n            if i >= 2 and j >= 2 and source[i-1] == target[j-2] and source[i-2] == target[j-1]:\n                # If transposition is possible, update the cost\n                dp[i][j] = min(dp[i][j], dp[i-2][j-2] + 1)\n    \n    return dp[len_s][len_t]\n```\n\n### Explanation\n1. **Initialization**: The DP table is initialized with dimensions `(len(source) + 1) x (len(target) + 1)`. The first row and column are filled to represent the base cases where converting an empty string to another string requires all insertions or deletions.\n2. **Filling the DP Table**: For each character in the source and target strings, we compute the minimum cost of converting the substring up to that point using insertion, deletion, and substitution.\n3. **Transposition Check**: If the current and previous characters in both strings can be transposed to match, we check if this operation reduces the edit cost and update the DP table accordingly.\n\nThis approach efficiently computes the OSA distance using dynamic programming with a time complexity of O(n*m), where n and m are the lengths of the source and target strings, respectively. This ensures that we consider all possible edit operations and their costs to find the minimum number of edits required."}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        return round(0.0, 3)\n    return round(tp / denominator, 3)\n```"}
{"task_id": 53, "completion_id": 0, "completion": "To solve this problem, we need to implement the self-attention mechanism, a core component of transformer models used in various machine learning tasks. The self-attention mechanism allows the model to focus on different parts of the input sequence dynamically, generating a contextualized representation.\n\n### Approach\nThe self-attention mechanism involves several steps:\n1. **Compute Queries, Keys, and Values**: Transform the input sequence `X` using learnable weight matrices `W_q`, `W_k`, and `W_v` to get the queries (Q), keys (K), and values (V).\n2. **Calculate Attention Scores**: Compute the dot product of queries and keys, then scale these scores by the square root of the dimension of the keys (d_k) to stabilize gradients.\n3. **Apply Softmax**: Normalize the scores using the softmax function to obtain attention weights, ensuring they sum to 1 for each query.\n4. **Compute Output**: Multiply the attention weights by the values to get the final output, which is a weighted sum of the values based on the attention weights.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    # Compute query, key, value matrices\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Calculate the dimension of the key vectors\n    d_k = K.shape[1]\n    \n    # Compute the attention scores\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    \n    # Apply softmax to get attention weights\n    max_scores = np.max(scores, axis=1, keepdims=True)\n    exp_scores = np.exp(scores - max_scores)\n    sum_exp = np.sum(exp_scores, axis=1, keepdims=True)\n    attention_weights = exp_scores / sum_exp\n    \n    # Compute the output by multiplying attention weights with values\n    output = np.dot(attention_weights, V)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(output, 4).tolist()\n```\n\n### Explanation\n1. **Queries, Keys, and Values**: The input `X` is transformed into Q, K, and V using matrix multiplication with their respective weight matrices. These transformations allow the model to project the input into different spaces to capture various aspects of the data.\n2. **Attention Scores**: The dot product of Q and K measures the similarity between each query and key. Scaling by `sqrt(d_k)` prevents the dot products from growing too large, which can lead to vanishing gradients during training.\n3. **Softmax Application**: Applying softmax ensures that the attention weights form a probability distribution, allowing the model to focus on relevant parts of the input sequence.\n4. **Output Calculation**: The final output is obtained by multiplying the attention weights with V, resulting in a context-aware representation of the input sequence. This step aggregates the values based on the computed attention weights, emphasizing more relevant parts of the input.\n\nThis implementation efficiently captures the relationships within the input sequence, enabling the model to dynamically adjust its focus and generate meaningful contextual representations."}
{"task_id": 54, "completion_id": 0, "completion": "To solve this problem, we need to implement a simple Recurrent Neural Network (RNN) cell that processes a sequence of input vectors and produces the final hidden state. The RNN cell uses the tanh activation function for updating the hidden state. \n\n### Approach\n1. **Understand the RNN Cell Operation**: The hidden state at each time step is updated using the formula:\n   \\[\n   h_t = \\tanh(Wx \\cdot x_t + Wh \\cdot h_{t-1} + b)\n   \\]\n   where \\(Wx\\) is the input-to-hidden weight matrix, \\(Wh\\) is the hidden-to-hidden weight matrix, \\(x_t\\) is the input vector at time step \\(t\\), \\(h_{t-1}\\) is the hidden state from the previous time step, and \\(b\\) is the bias vector.\n\n2. **Convert Inputs to Numpy Arrays**: Convert the initial hidden state, weight matrices, and bias vector to numpy arrays for efficient matrix operations.\n\n3. **Iterate Over Input Sequence**: For each input vector in the sequence, compute the new hidden state using matrix multiplications and the tanh activation function.\n\n4. **Return Final Hidden State**: After processing all input vectors, round the final hidden state to four decimal places and convert it back to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    h = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    for x in input_sequence:\n        x_arr = np.array(x)\n        a = np.dot(Wx, x_arr) + np.dot(Wh, h) + b\n        h = np.tanh(a)\n    \n    h_rounded = np.round(h, decimals=4)\n    return h_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: Convert the initial hidden state and parameters (weight matrices and bias vector) into numpy arrays to facilitate matrix operations.\n2. **Processing Each Input**: For each input vector in the sequence:\n   - Convert the input vector to a numpy array.\n   - Compute the linear combination of the input vector and the previous hidden state using the respective weight matrices and add the bias.\n   - Apply the tanh activation function to the result to get the new hidden state.\n3. **Final Output**: After processing all inputs, the final hidden state is rounded to four decimal places and converted back to a Python list for the output.\n\nThis approach efficiently processes the input sequence using numpy's optimized matrix operations, ensuring correctness and performance. The use of numpy arrays allows for concise and readable code while handling the necessary mathematical operations."}
{"task_id": 55, "completion_id": 0, "completion": "To solve this problem, we need to translate a set of 2D points by specified distances in the x and y directions using a translation matrix. This involves converting the points to homogeneous coordinates, applying the translation matrix, and then converting the result back to standard coordinates.\n\n### Approach\n1. **Convert Points to Homogeneous Coordinates**: Each point (x, y) is converted to (x, y, 1) to facilitate matrix multiplication with the translation matrix in homogeneous coordinates.\n2. **Construct the Translation Matrix**: The translation matrix for homogeneous coordinates is a 3x3 matrix that shifts points by tx in the x-direction and ty in the y-direction.\n3. **Matrix Multiplication**: Multiply each point in homogeneous coordinates by the translation matrix to get the translated points in homogeneous coordinates.\n4. **Convert Back to Standard Coordinates**: Extract the first two elements of each translated point to convert back from homogeneous coordinates to standard 2D coordinates.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    if not points:\n        return []\n    points_array = np.array(points)\n    # Add homogeneous coordinate (third element as 1)\n    homogeneous = np.hstack([points_array, np.ones((points_array.shape[0], 1))])\n    # Construct the translation matrix\n    translation_matrix = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [tx, ty, 1]\n    ])\n    # Apply the translation matrix\n    translated = homogeneous @ translation_matrix\n    # Extract the first two columns and convert to list\n    return translated[:, :2].tolist()\n```\n\n### Explanation\n1. **Conversion to Homogeneous Coordinates**: The input points are converted to a numpy array, and a column of ones is added to each point to form homogeneous coordinates. This allows us to use matrix multiplication for the translation.\n2. **Translation Matrix Construction**: The translation matrix is constructed as a 3x3 numpy array. This matrix is designed such that when multiplied by a point in homogeneous coordinates, it shifts the point by tx and ty in the x and y directions, respectively.\n3. **Matrix Multiplication**: Each point in homogeneous coordinates is multiplied by the translation matrix using numpy's matrix multiplication operator (@). This results in the translated points in homogeneous coordinates.\n4. **Conversion Back to Standard Coordinates**: The translated points are converted back to standard 2D coordinates by discarding the third element (which remains 1 after translation) and converting the result to a list of lists using numpy's `tolist()` method.\n\nThis approach efficiently applies the translation using matrix operations, ensuring clarity and leveraging numpy's optimized performance for numerical computations."}
{"task_id": 56, "completion_id": 0, "completion": "To solve this problem, we need to compute the Kullback-Leibler (KL) divergence between two univariate normal distributions. The KL divergence measures how one probability distribution diverges from a second, reference probability distribution. \n\n### Approach\nThe formula for the KL divergence between two normal distributions \\( P \\sim \\mathcal{N}(\\mu_P, \\sigma_P^2) \\) and \\( Q \\sim \\mathcal{N}(\\mu_Q, \\sigma_Q^2) \\) is derived using the properties of logarithms and expectations. The key steps in the derivation are:\n\n1. **Logarithm of the Ratio of Probabilities**: The logarithm of the ratio of the probability density functions of the two distributions.\n2. **Expectation Calculation**: Taking the expectation of this logarithm ratio under the distribution \\( P \\).\n\nThe final formula for the KL divergence \\( D_{KL}(P \\parallel Q) \\) is:\n\\[\nD_{KL}(P \\parallel Q) = \\ln\\left(\\frac{\\sigma_Q}{\\sigma_P}\\right) + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2} - 0.5\n\\]\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2)\n    return term1 + term2 - 0.5\n```\n\n### Explanation\n1. **Logarithm Term**: The term \\(\\ln\\left(\\frac{\\sigma_Q}{\\sigma_P}\\right)\\) accounts for the ratio of the standard deviations of the two distributions.\n2. **Variance and Mean Difference Term**: The term \\(\\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2}\\) combines the variance of \\( P \\) and the squared difference of the means of \\( P \\) and \\( Q \\), scaled by the variance of \\( Q \\).\n3. **Adjustment Term**: The subtraction of 0.5 adjusts the result to ensure correctness as derived from the integral calculations.\n\nThis approach efficiently computes the KL divergence using basic arithmetic operations and logarithms, ensuring both accuracy and computational efficiency. The function handles valid inputs where the standard deviations (\\(\\sigma_P\\) and \\(\\sigma_Q\\)) are positive, as required for normal distributions."}
{"task_id": 57, "completion_id": 0, "completion": "To solve this problem, we need to implement the Gauss-Seidel method, an iterative technique for solving a system of linear equations. The method updates each component of the solution vector using the most recently computed values, which often leads to faster convergence compared to other iterative methods.\n\n### Approach\n1. **Initialization**: Convert the input matrix `A` and vector `b` into NumPy arrays for efficient computation. If no initial guess `x_ini` is provided, initialize the solution vector `x` with zeros.\n2. **Iterative Update**: For each iteration, update each component of the solution vector `x` using the most recent values. Specifically, for each element `x[i]`, compute the sum of the products of the corresponding row of `A` and the current values of `x`, excluding the diagonal element. Adjust `x[i]` using the formula derived from the linear equation.\n3. **Convergence Check**: Perform the specified number of iterations to approximate the solution vector.\n4. **Result Formatting**: Round the resulting vector to four decimal places and convert it to a Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    len_x = len(b)\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    for _ in range(n):\n        for i in range(len_x):\n            sigma = np.dot(A[i, :i], x[:i]) + np.dot(A[i, i+1:], x[i+1:])\n            x[i] = (b[i] - sigma) / A[i, i]\n    \n    x_rounded = np.around(x, decimals=4)\n    return x_rounded.tolist()\n```\n\n### Explanation\n1. **Matrix and Vector Conversion**: The input matrix `A` and vector `b` are converted to NumPy arrays to leverage efficient array operations.\n2. **Initial Guess Handling**: If no initial guess is provided, the solution vector `x` is initialized to zeros. Otherwise, the provided initial guess is used.\n3. **Iterative Update**: For each iteration, the solution vector `x` is updated element-wise. Each element `x[i]` is computed using the most recent values of `x` for the preceding elements (already updated in the current iteration) and the previous values for the subsequent elements (from the last iteration).\n4. **Result Rounding and Conversion**: After completing the specified iterations, the solution vector is rounded to four decimal places and converted to a Python list for the final output, ensuring the result meets the specified format requirements.\n\nThis approach efficiently updates the solution vector using the Gauss-Seidel method, ensuring convergence towards the solution with each iteration. The use of NumPy arrays and vector operations ensures that the implementation is both concise and efficient."}
{"task_id": 58, "completion_id": 0, "completion": "To solve this problem, we need to implement the Gaussian Elimination method with partial pivoting to solve a system of linear equations. The method transforms the coefficient matrix into an upper triangular form, which is then used to solve for the variables using backward substitution.\n\n### Approach\n1. **Partial Pivoting**: At each step, select the row with the largest absolute value in the current column (from the current row downwards) to avoid division by zero and reduce numerical errors.\n2. **Row Swapping**: Swap the current row with the row identified by partial pivoting to ensure the largest element is the pivot.\n3. **Elimination**: For each row below the pivot row, eliminate the current column entry by subtracting an appropriate multiple of the pivot row from the current row.\n4. **Backward Substitution**: Once the matrix is in upper triangular form, solve for the variables starting from the last row and moving upwards.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    \n    for k in range(n - 1):\n        # Partial pivoting\n        max_row = np.argmax(np.abs(A[k:, k])) + k\n        if A[max_row, k] == 0:\n            continue  # Matrix is singular\n        \n        # Swap rows\n        A[[k, max_row]] = A[[max_row, k]]\n        b[[k, max_row]] = b[[max_row, k]]\n        \n        # Elimination\n        for i in range(k + 1, n):\n            factor = A[i, k] / A[k, k]\n            A[i, k:] -= factor * A[k, k:]\n            b[i] -= factor * b[k]\n    \n    # Back substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i+1:], x[i+1:])) / A[i, i]\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(x, 4).tolist()\n```\n\n### Explanation\n1. **Partial Pivoting**: This step ensures that the largest element in the current column is used as the pivot, which helps in reducing numerical errors. The row containing this pivot element is swapped with the current row.\n2. **Row Swapping**: Swapping rows in both the coefficient matrix (A) and the constant terms vector (b) maintains the integrity of the system of equations.\n3. **Elimination**: For each row below the pivot row, a factor is calculated to eliminate the current column entry. This factor is used to adjust the elements of the current row and the corresponding entry in the vector (b).\n4. **Backward Substitution**: Starting from the last row, each variable is solved using the values of the previously solved variables. This step works upwards through the matrix to find all solutions.\n\nThis approach efficiently transforms the system into an upper triangular matrix and solves it with numerical stability ensured by partial pivoting. The solution is then rounded to four decimal places and returned as a list."}
{"task_id": 59, "completion_id": 0, "completion": "To solve this problem, we need to implement a Long Short-Term Memory (LSTM) network that processes a sequence of inputs and produces the final hidden state and cell state after processing all inputs. The LSTM network uses gates to control the flow of information, which helps in maintaining long-term dependencies in sequences.\n\n### Approach\nThe LSTM network consists of several key components:\n1. **Forget Gate**: Determines which information from the previous cell state should be discarded.\n2. **Input Gate**: Updates the cell state with new information.\n3. **Candidate Cell State**: Computes potential new values to be added to the cell state.\n4. **Output Gate**: Determines the next hidden state based on the updated cell state.\n\nThe implementation involves initializing the weights and biases for these gates and processing each input in the sequence step-by-step. At each time step, the gates are computed using the current input and the previous hidden state, followed by updating the cell state and hidden state. The results are rounded to four decimal places and returned as lists.\n\n### Solution Code\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        h_prev = initial_hidden_state.copy()\n        c_prev = initial_cell_state.copy()\n        hidden_states = []\n\n        for x_t in x:\n            # Concatenate input and previous hidden state\n            concat = np.vstack((x_t, h_prev))\n\n            # Compute gates and candidate cell state\n            f = 1 / (1 + np.exp(-(self.Wf @ concat + self.bf))\n            i = 1 / (1 + np.exp(-(self.Wi @ concat + self.bi))\n            c_tilde = np.tanh(self.Wc @ concat + self.bc)\n            o = 1 / (1 + np.exp(-(self.Wo @ concat + self.bo))\n\n            # Update cell state and hidden state\n            c_prev = f * c_prev + i * c_tilde\n            h_prev = o * np.tanh(c_prev)\n\n            hidden_states.append(h_prev.copy())\n\n        # Rounding and converting to list\n        hidden_states_rounded = [np.round(h, 4).tolist() for h in hidden_states]\n        final_hidden = np.round(h_prev, 4).tolist()\n        final_cell = np.round(c_prev, 4).tolist()\n\n        return hidden_states_rounded, final_hidden, final_cell\n```\n\n### Explanation\n1. **Initialization**: The `__init__` method initializes the weights and biases for the forget, input, candidate cell, and output gates. The weights are randomly initialized, and biases are initialized to zeros.\n\n2. **Forward Pass**: The `forward` method processes each input in the sequence:\n   - **Concatenation**: Combines the current input and previous hidden state into a single vector.\n   - **Gate Computations**: Uses sigmoid activation for the forget, input, and output gates, and tanh activation for the candidate cell state.\n   - **State Updates**: Updates the cell state using the forget and input gates, then computes the new hidden state using the output gate and the updated cell state.\n\n3. **Rounding and Conversion**: After processing all inputs, the hidden states, final hidden state, and final cell state are rounded to four decimal places and converted to lists for the output.\n\nThis approach ensures that the LSTM processes sequences efficiently while maintaining the necessary information flow through the gates, allowing it to capture long-term dependencies in the data."}
{"task_id": 60, "completion_id": 0, "completion": "To solve this problem, we need to compute the TF-IDF (Term Frequency-Inverse Document Frequency) scores for a given query against a corpus of documents. TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection or corpus. The implementation must handle various edge cases such as empty corpus, division by zero, and terms not present in the corpus.\n\n### Approach\n1. **Term Frequency (TF)**: This measures how frequently a term appears in a document. It is calculated as the count of the term in the document divided by the total number of terms in the document. If a document is empty, the TF is zero.\n2. **Inverse Document Frequency (IDF)**: This measures how important a term is across the entire corpus. It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term, with smoothing to avoid division by zero. Smoothing is done by adding 1 to both the numerator and denominator.\n3. **TF-IDF Calculation**: For each term in the query, compute the TF-IDF score by multiplying the TF and IDF values. Handle edge cases such as empty corpus and documents gracefully.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    \n    N = len(corpus)\n    idf = {}\n    \n    # Compute IDF for each query term\n    for term in query:\n        df = 0\n        for doc in corpus:\n            if term in doc:\n                df += 1\n        # Apply smoothing to avoid division by zero\n        idf_value = np.log((N + 1) / (df + 1))\n        idf[term] = idf_value\n    \n    # Compute TF-IDF for each document\n    result = []\n    for doc in corpus:\n        doc_length = len(doc)\n        doc_scores = []\n        for term in query:\n            if doc_length == 0:\n                tf = 0.0\n            else:\n                term_count = doc.count(term)\n                tf = term_count / doc_length\n            # Calculate TF-IDF and round to 4 decimal places\n            tf_idf = tf * idf[term]\n            doc_scores.append(round(tf_idf, 4))\n        result.append(doc_scores)\n    \n    # Convert to numpy array and back to list as per the output requirement\n    return np.array(result).tolist()\n```\n\n### Explanation\n1. **Handling Empty Corpus**: If the corpus is empty, the function immediately returns an empty list.\n2. **IDF Calculation**: For each term in the query, the document frequency (df) is calculated by counting how many documents contain the term. Smoothing is applied to avoid division by zero by adding 1 to both the numerator (total documents + 1) and denominator (document frequency + 1).\n3. **TF Calculation**: For each document, the term frequency (TF) is calculated for each query term. If the document is empty, TF is zero.\n4. **TF-IDF Calculation**: The TF-IDF score for each term in each document is computed by multiplying the TF and IDF values. The result is rounded to four decimal places and converted to a list using numpy's `tolist()` method to ensure the output format is correct.\n\nThis approach ensures robustness and handles edge cases such as empty documents, terms not present in the corpus, and division by zero through appropriate smoothing techniques."}
{"task_id": 61, "completion_id": 0, "completion": "To solve this problem, we need to calculate the F-Score for a binary classification task. The F-Score is a metric that combines both Precision and Recall into a single value, adjusted by a parameter beta which determines the weight of Precision in the final score. When beta is 1, this score is known as the F1-Score, which is the harmonic mean of Precision and Recall.\n\n### Approach\n1. **Calculate True Positives (TP), False Positives (FP), and False Negatives (FN):** These values are derived from comparing the true labels (y_true) with the predicted labels (y_pred).\n2. **Compute Precision:** Precision is the ratio of TP to the sum of TP and FP. If the denominator is zero, Precision is set to 0 to handle division by zero.\n3. **Compute Recall:** Recall is the ratio of TP to the sum of TP and FN. If the denominator is zero, Recall is set to 0 to handle division by zero.\n4. **Calculate the F-Score:** Using the formula for the F-beta score, which is a weighted harmonic mean of Precision and Recall. If the denominator in this calculation is zero (which happens when both Precision and Recall are zero), the F-Score is set to 0.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate TP, FP, FN\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Compute Precision\n    denom_p = TP + FP\n    if denom_p == 0:\n        precision = 0.0\n    else:\n        precision = TP / denom_p\n    \n    # Compute Recall\n    denom_r = TP + FN\n    if denom_r == 0:\n        recall = 0.0\n    else:\n        recall = TP / denom_r\n    \n    # Compute F-Score\n    beta_sq = beta ** 2\n    numerator = (1 + beta_sq) * precision * recall\n    denominator = (beta_sq * precision) + recall\n    \n    if denominator == 0:\n        f_score_value = 0.0\n    else:\n        f_score_value = numerator / denominator\n    \n    return round(f_score_value, 3)\n```\n\n### Explanation\n1. **True Positives (TP):** Count of instances where both the true and predicted labels are 1.\n2. **False Positives (FP):** Count of instances where the true label is 0 but the predicted label is 1.\n3. **False Negatives (FN):** Count of instances where the true label is 1 but the predicted label is 0.\n4. **Precision:** Measures the proportion of correctly predicted positive instances out of all predicted positive instances. Handles division by zero by setting Precision to 0 if no positive predictions are made.\n5. **Recall:** Measures the proportion of correctly predicted positive instances out of all actual positive instances. Handles division by zero by setting Recall to 0 if there are no actual positive instances.\n6. **F-Score Calculation:** Combines Precision and Recall using the formula for the F-beta score. If both Precision and Recall are zero, the F-Score is set to 0 to handle the indeterminate form 0/0.\n\nThis approach ensures that the F-Score is a robust metric even in edge cases where there are no positive predictions or actual positives."}
{"task_id": 62, "completion_id": 0, "completion": "To solve this problem, we need to implement a simple Recurrent Neural Network (RNN) with Backpropagation Through Time (BPTT) to learn from sequential data. The RNN will process input sequences, update hidden states, and adjust weights based on error gradients computed during backpropagation.\n\n### Approach\n1. **Initialization**: The RNN is initialized with random weights scaled by 0.01 and zero biases. This helps in breaking symmetry and starting with small values to avoid gradient explosion.\n2. **Forward Pass**: The RNN processes each input in the sequence, updating the hidden state using a hyperbolic tangent (tanh) activation function and producing an output at each time step. The hidden states and outputs are stored for use in backpropagation.\n3. **Backward Pass (BPTT)**: The gradients of the loss with respect to the weights are computed by propagating the error backward through time. This involves iterating through the sequence in reverse order, computing gradients at each step, and accumulating them to update the weights.\n\n### Solution Code\n```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        Returns:\n            outputs (list): Output vectors at each time step.\n            inputs (list): The input sequence.\n            hidden_states (list): Hidden states at each time step (excluding initial state).\n        \"\"\"\n        T = len(x)\n        hidden_states = [np.zeros((self.hidden_size, 1))]  # Initial hidden state\n        outputs = []\n        for t in range(T):\n            x_t = x[t]\n            h_prev = hidden_states[-1]\n            h_t = np.tanh(self.W_xh @ x_t + self.W_hh @ h_prev + self.b_h)\n            y_t = self.W_hy @ h_t + self.b_y\n            hidden_states.append(h_t)\n            outputs.append(y_t)\n        # Exclude the initial hidden state\n        return outputs, x, hidden_states[1:]\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs Backpropagation Through Time (BPTT) to adjust the weights.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros((self.hidden_size, 1))\n\n        for t in reversed(range(T)):\n            dy = outputs[t] - expected_output[t]\n\n            # Gradients for W_hy and b_y\n            dW_hy += dy @ last_hiddens[t].T\n            db_y += dy\n\n            # Backpropagate into h\n            dh = self.W_hy.T @ dy + dh_next\n\n            # Backprop through tanh\n            da = dh * (1 - last_hiddens[t] ** 2)\n\n            # Gradients for W_xh, W_hh, b_h\n            x_t = last_inputs[t]\n            dW_xh += da @ x_t.T\n            if t > 0:\n                h_prev = last_hiddens[t-1]\n            else:\n                h_prev = np.zeros((self.hidden_size, 1))\n            dW_hh += da @ h_prev.T\n            db_h += da\n\n            # Update dh_next for next step\n            dh_next = self.W_hh.T @ da\n\n        # Update parameters\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n```\n\n### Explanation\n1. **Initialization**: The `__init__` method sets up the RNN with randomly initialized weights and zero biases. The weights are scaled by 0.01 to ensure they start small, which helps in stabilizing the training process.\n\n2. **Forward Pass**: The `forward` method processes each input in the sequence, updating the hidden state using the current input and the previous hidden state. The output at each time step is computed using the current hidden state. The method returns the outputs, inputs, and hidden states for each time step (excluding the initial zero state).\n\n3. **Backward Pass (BPTT)**: The `backward` method computes the gradients of the loss with respect to each weight and bias by iterating through the sequence in reverse order. The gradients are accumulated and used to update the weights, scaled by the learning rate. This process involves computing the error at each time step, propagating it back through the network, and adjusting the weights to minimize the loss.\n\nThis approach ensures that the RNN can learn from sequential data by adjusting its weights based on the error gradients computed during backpropagation through time."}
{"task_id": 63, "completion_id": 0, "completion": "To solve this problem, we need to implement the Conjugate Gradient (CG) method, an iterative algorithm designed to solve large, sparse, symmetric, positive-definite linear systems efficiently. The CG method is particularly useful for systems where direct methods (like matrix factorization) are computationally expensive.\n\n### Approach\n1. **Initialization**: Start with an initial guess for the solution vector. If no initial guess is provided, use a zero vector.\n2. **Residual Calculation**: Compute the initial residual vector, which represents the difference between the right-hand side vector and the product of the matrix with the initial guess.\n3. **Search Direction**: Set the initial search direction to the residual vector.\n4. **Iterative Process**: For each iteration, update the solution vector and residual vector using the current search direction. Adjust the search direction based on the new residual to ensure conjugacy with previous directions.\n5. **Stopping Criteria**: Check if the residual norm is below a specified tolerance. If so, terminate early. Otherwise, continue until the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array = None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    x = x0.copy() if x0 is not None else np.zeros_like(b)\n    r = b - A @ x\n    p = r.copy()\n    rsold = r.dot(r)\n    \n    if rsold < tol**2:\n        x_rounded = np.round(x, 8)\n        return x_rounded.tolist()\n    \n    for _ in range(n):\n        Ap = A @ p\n        alpha = rsold / p.dot(Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rsnew = r.dot(r)\n        \n        if rsnew < tol**2:\n            break\n        \n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n    \n    x_rounded = np.round(x, 8)\n    return x_rounded.tolist()\n```\n\n### Explanation\n1. **Initialization**: The solution vector `x` is initialized either with the provided initial guess `x0` or a zero vector. The residual vector `r` is computed as the difference between `b` and the product of `A` with `x`.\n2. **Residual Check**: If the initial residual is already below the tolerance, the solution is returned immediately.\n3. **Iterative Update**: For each iteration, the matrix-vector product `Ap` is computed. The step size `alpha` is determined to update the solution vector `x` and the residual vector `r`. The new residual's norm is checked against the tolerance to decide early termination.\n4. **Search Direction Update**: The search direction `p` is updated using the new residual and a scaling factor `beta`, ensuring conjugacy with previous directions for faster convergence.\n5. **Result Rounding**: The final solution vector is rounded to 8 decimal places and converted to a list for output.\n\nThis approach efficiently converges to the solution by leveraging the properties of conjugate directions, minimizing the residual in each iteration, and ensuring numerical stability through careful updates of the search directions."}
{"task_id": 64, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Gini impurity for a given set of class labels. Gini impurity is a measure used in decision tree algorithms to determine the impurity or disorder within a node. The lower the Gini impurity, the more homogeneous the node.\n\n### Approach\n1. **Handle Empty Input**: If the input list is empty, return a Gini impurity of 0.0 since there are no elements to measure impurity.\n2. **Count Class Frequencies**: Use NumPy's `unique` function to count the occurrences of each class label in the input list.\n3. **Calculate Probabilities**: Compute the probability of each class by dividing its count by the total number of elements.\n4. **Sum of Squares**: Calculate the sum of the squares of these probabilities.\n5. **Compute Gini Impurity**: Subtract the sum of squares from 1 to get the Gini impurity.\n6. **Round Result**: Round the resulting Gini impurity to three decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    _, counts = np.unique(y, return_counts=True)\n    total = len(y)\n    probs = counts / total\n    sum_squares = np.sum(probs ** 2)\n    gini = 1 - sum_squares\n    return round(gini, 3)\n```\n\n### Explanation\n- **Handling Empty Input**: The function first checks if the input list is empty. If it is, the function returns 0.0 immediately.\n- **Counting Frequencies**: Using `np.unique` with `return_counts=True`, we efficiently get the counts of each unique class label.\n- **Probabilities Calculation**: Each count is divided by the total number of elements to get the probability of each class.\n- **Sum of Squares**: The sum of the squares of these probabilities is computed, which represents the homogeneity of the node.\n- **Gini Impurity Calculation**: The Gini impurity is derived by subtracting the sum of squares from 1, reflecting the node's impurity.\n- **Rounding**: The result is rounded to three decimal places to meet the specified precision requirement.\n\nThis approach ensures that the Gini impurity is calculated efficiently and accurately, handling edge cases such as an empty input list gracefully."}
{"task_id": 65, "completion_id": 0, "completion": "To solve this problem, we need to convert a dense matrix into the Compressed Row Sparse (CSR) format. The CSR format efficiently stores sparse matrices by only keeping track of non-zero elements and their positions, thus reducing memory usage.\n\n### Approach\nThe CSR format consists of three arrays:\n1. **Values Array**: Contains all non-zero elements in row-major order.\n2. **Column Indices Array**: Stores the column index for each corresponding non-zero element in the values array.\n3. **Row Pointer Array**: Keeps track of the cumulative count of non-zero elements up to the end of each row, indicating where each row starts in the values array.\n\nThe approach involves iterating through each row of the dense matrix, collecting non-zero elements and their column indices, and maintaining a cumulative count of non-zero elements to build the row pointer array.\n\n### Solution Code\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_ptr = [0]  # Initialize row pointer array with the start index 0\n    \n    for row in dense_matrix:\n        count = 0  # Count of non-zero elements in the current row\n        for col_idx, val in enumerate(row):\n            if val != 0:\n                values.append(val)\n                column_indices.append(col_idx)\n                count += 1\n        row_ptr.append(row_ptr[-1] + count)  # Update row pointer with cumulative count\n    \n    return (values, column_indices, row_ptr)\n```\n\n### Explanation\n1. **Initialization**: We start by initializing three lists: `values` to store non-zero elements, `column_indices` to store their respective column indices, and `row_ptr` initialized with `[0]` to track the cumulative count of non-zero elements.\n2. **Iterating Through Rows**: For each row in the dense matrix, we count the non-zero elements.\n3. **Collecting Non-Zero Elements**: For each non-zero element encountered, its value and column index are added to `values` and `column_indices` respectively, and the count of non-zero elements for the current row is incremented.\n4. **Updating Row Pointer**: After processing each row, the cumulative count of non-zero elements is updated in `row_ptr` by adding the count from the current row to the last value in `row_ptr`.\n5. **Return Result**: The function returns a tuple containing the three arrays, which together form the CSR representation of the input dense matrix.\n\nThis approach efficiently constructs the CSR format by only iterating through the matrix once, ensuring a time complexity of O(n*m), where n is the number of rows and m is the number of columns in the dense matrix. This is optimal for converting a dense matrix to CSR format."}
{"task_id": 66, "completion_id": 0, "completion": "To solve this problem, we need to compute the orthogonal projection of a vector **v** onto a line defined by another vector **L**. The orthogonal projection of a vector onto a line results in the vector on the line that is closest to the original vector. \n\n### Approach\n1. **Dot Product Calculation**: The first step is to compute the dot product of the vector **v** and the line vector **L**, as well as the dot product of **L** with itself. These dot products are essential for determining the scalar factor used in the projection.\n2. **Scalar Factor**: Using the dot products, we calculate a scalar factor. This scalar is the ratio of the dot product of **v** and **L** to the dot product of **L** with itself.\n3. **Projection Vector**: Multiply each component of the line vector **L** by the scalar factor to obtain the projection vector.\n4. **Rounding**: Each component of the resulting projection vector is rounded to three decimal places for the final output.\n\n### Solution Code\n\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Calculate the dot product of v and L\n    dot_v_L = sum(vi * Li for vi, Li in zip(v, L))\n    # Calculate the dot product of L with itself\n    dot_L_L = sum(Li * Li for Li in L)\n    # Compute the scalar factor\n    scalar = dot_v_L / dot_L_L\n    # Calculate the projection vector components and round to three decimal places\n    projection = [round(scalar * Li, 3) for Li in L]\n    return projection\n```\n\n### Explanation\n1. **Dot Product Calculation**: The dot product of two vectors is computed by summing the products of their corresponding components. This gives us a measure of how much one vector goes in the direction of another.\n2. **Scalar Factor**: The scalar factor is derived from the ratio of the dot product of **v** and **L** to the dot product of **L** with itself. This scalar essentially scales the line vector **L** to the correct magnitude needed for the projection.\n3. **Projection Vector**: By multiplying each component of **L** by the scalar factor, we obtain the projection vector. This vector lies on the line defined by **L** and is the closest point to **v** on that line.\n4. **Rounding**: To ensure the result is user-friendly and meets precision requirements, each component of the projection vector is rounded to three decimal places.\n\nThis approach efficiently computes the orthogonal projection using fundamental vector operations, ensuring accuracy and clarity in the result."}
{"task_id": 67, "completion_id": 0, "completion": "To solve this problem, we need to convert a dense matrix into its Compressed Column Sparse (CSC) representation. The CSC format is efficient for storing sparse matrices (matrices with a high number of zero elements) by only storing non-zero elements along with their row indices and column pointers.\n\n### Approach\nThe CSC format consists of three arrays:\n1. **Values**: A list of non-zero elements stored in column-major order.\n2. **Row Indices**: A list of row indices corresponding to each non-zero element in the values array.\n3. **Column Pointers**: A list where each entry indicates the starting index of each column in the values array. The length of this list is one more than the number of columns, with the last entry indicating the total number of non-zero elements.\n\nThe approach to convert a dense matrix to CSC format involves the following steps:\n1. **Initialization**: Check if the input matrix is empty. If so, return empty lists and a column pointer initialized with zero.\n2. **Column-wise Processing**: Iterate over each column of the matrix. For each column, collect non-zero elements and their respective row indices.\n3. **Update Column Pointers**: After processing each column, update the column pointer to reflect the starting index of the next column in the values array.\n\n### Solution Code\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    rows = len(dense_matrix)\n    if rows == 0:\n        return ([], [], [0])\n    \n    cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]  # Initialize with the starting index of the first column\n    \n    for j in range(cols):\n        for i in range(rows):\n            val = dense_matrix[i][j]\n            if val != 0:\n                values.append(val)\n                row_indices.append(i)\n        # Append the current length of values as the start of the next column\n        column_pointer.append(len(values))\n    \n    return (values, row_indices, column_pointer)\n```\n\n### Explanation\n1. **Initialization**: The function starts by checking if the input matrix is empty. If it is, the function returns empty lists for values and row indices, and a column pointer initialized with zero.\n2. **Column-wise Processing**: For each column in the matrix, the function iterates through each row. If a non-zero element is found, it is added to the `values` list, and its row index is added to `row_indices`.\n3. **Column Pointers Update**: After processing each column, the current length of the `values` list (which represents the cumulative count of non-zero elements) is appended to `column_pointer`. This helps in tracking where each column's data starts in the values array.\n\nBy following this approach, we efficiently convert a dense matrix into its CSC format, which is particularly useful for sparse matrices where memory efficiency is crucial."}
{"task_id": 68, "completion_id": 0, "completion": "To solve this problem, we need to compute the column space (image) of a given matrix. The column space is spanned by the linearly independent columns of the matrix. The approach involves using QR decomposition with column pivoting to identify these independent columns efficiently.\n\n### Approach\n1. **QR Decomposition with Column Pivoting**: This decomposition helps in identifying the most significant columns of the matrix. The permutation array from this decomposition indicates the order of columns based on their significance (norms).\n2. **Rank Determination**: Using the diagonal entries of the upper triangular matrix \\( R \\) from the QR decomposition, we determine the rank of the matrix. The rank tells us how many columns are linearly independent.\n3. **Basis Extraction**: Using the permutation array and the rank, we extract the corresponding columns from the original matrix. These columns form the basis for the column space.\n4. **Rounding and Conversion**: The resulting basis vectors are rounded to 8 decimal places and converted to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    # Convert the input matrix to a numpy array of float type for numerical stability\n    A_np = np.array(A, dtype=np.float64)\n    # Perform QR decomposition with column pivoting\n    Q, R, p = np.linalg.qr(A_np, mode='reduced', pivoting=True)\n    # Calculate the absolute values of the diagonal elements of R\n    diag = np.abs(np.diag(R))\n    # Handle the case where the matrix is zero\n    if len(diag) == 0:\n        return []\n    # Compute the threshold for determining the rank\n    max_diag = np.max(diag)\n    tolerance = max(A_np.shape) * np.finfo(R.dtype).eps * max_diag\n    # Determine the rank by counting values above the tolerance\n    rank = np.sum(diag > tolerance)\n    # Extract the columns corresponding to the pivot positions from the original matrix\n    basis_matrix = A_np[:, p[:rank]]\n    # Transpose to make each basis vector a row, round, and convert to list\n    return np.round(basis_matrix.T, 8).tolist()\n```\n\n### Explanation\n1. **QR Decomposition**: The QR decomposition with column pivoting rearranges the columns of the matrix such that the most significant (largest norm) columns come first. This helps in identifying the linearly independent columns.\n2. **Rank Calculation**: The diagonal entries of matrix \\( R \\) from the QR decomposition are used to determine the rank. Entries above a certain tolerance (considering numerical precision) are considered non-zero, indicating independent columns.\n3. **Basis Vectors Extraction**: The permutation array from the QR decomposition gives the indices of the columns in the order of their significance. The first 'rank' columns from this permutation are the independent columns, which we extract from the original matrix.\n4. **Result Formatting**: The basis vectors (columns) are transposed to rows, rounded to 8 decimal places, and converted to a list to meet the output requirements.\n\nThis approach efficiently identifies the column space using numerical methods that are robust and stable, ensuring accuracy even with potential floating-point errors."}
{"task_id": 69, "completion_id": 0, "completion": "To solve this problem, we need to compute the R-squared value for a regression model. The R-squared value, also known as the coefficient of determination, measures how well the independent variables explain the variability of the dependent variable. It is calculated as 1 minus the ratio of the residual sum of squares (RSS) to the total sum of squares (TSS).\n\n### Approach\n1. **Convert Inputs to Arrays**: Convert the input lists `y_true` and `y_pred` to NumPy arrays to facilitate vectorized operations.\n2. **Compute the Mean**: Calculate the mean of the true values `y_true`.\n3. **Calculate TSS**: Compute the total sum of squares (TSS) as the sum of squared differences between each true value and the mean of the true values.\n4. **Calculate RSS**: Compute the residual sum of squares (RSS) as the sum of squared differences between each true value and the corresponding predicted value.\n5. **Handle Edge Cases**: If TSS is zero (indicating all true values are the same), check if RSS is also zero. If RSS is zero, return 1.0 (perfect prediction). Otherwise, return 0.0 (model predictions are worse than predicting the mean).\n6. **Compute R-squared**: If TSS is non-zero, compute R-squared as 1 minus the ratio of RSS to TSS and round the result to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    if tss == 0:\n        return round(1.0 if rss == 0 else 0.0, 3)\n    return round(1 - rss / tss, 3)\n```\n\n### Explanation\n- **Conversion to Arrays**: Using NumPy arrays allows efficient element-wise operations.\n- **Mean Calculation**: The mean of `y_true` is used as the baseline model's prediction.\n- **TSS and RSS Calculation**: TSS measures the total variance in the true values, while RSS measures the variance unexplained by the regression model.\n- **Edge Case Handling**: When TSS is zero (all true values are the same), the R-squared value is 1.0 if the model perfectly predicts the values (RSS is zero), otherwise 0.0, as any deviation from the mean prediction is considered worse than the baseline.\n- **R-squared Calculation**: The standard formula for R-squared is used when TSS is non-zero, providing a measure of how well the model's predictions explain the variance in the true values.\n\nThis approach ensures that the R-squared value is correctly computed and handles edge cases gracefully, providing a robust solution for evaluating regression models."}
{"task_id": 70, "completion_id": 0, "completion": "To solve this problem, we need to calculate the average brightness of a grayscale image represented as a 2D matrix. The function should handle edge cases such as an empty matrix, inconsistent row lengths, and invalid pixel values. If any of these edge cases are encountered, the function should return -1. Otherwise, it returns the average brightness rounded to two decimal places.\n\n### Approach\n1. **Check for Empty Matrix**: If the input matrix is empty, return -1 immediately.\n2. **Check Row Consistency**: Ensure all rows have the same length. If not, return -1.\n3. **Validate Pixel Values**: Each pixel must be between 0 and 255 (inclusive). If any pixel is invalid, return -1.\n4. **Check Total Pixels**: Calculate the total number of pixels. If there are no pixels (e.g., all rows are empty), return -1 to avoid division by zero.\n5. **Calculate Average Brightness**: Sum all valid pixel values, compute the average, and round the result to two decimal places.\n\n### Solution Code\n```python\ndef calculate_brightness(img):\n    # Check if the image matrix is empty\n    if not img:\n        return -1\n    \n    # Check if all rows have the same length\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n    \n    # Check if all pixel values are within 0-255\n    for row in img:\n        for pixel in row:\n            if not (0 <= pixel <= 255):\n                return -1\n    \n    # Calculate total number of pixels\n    total_pixels = len(img) * row_length\n    if total_pixels == 0:\n        return -1\n    \n    # Compute sum of all pixel values\n    sum_pixels = sum(pixel for row in img for pixel in row)\n    \n    # Calculate average and round to two decimal places\n    average = sum_pixels / total_pixels\n    return round(average, 2)\n```\n\n### Explanation\n1. **Empty Matrix Check**: The function first checks if the input image is an empty list. If so, it returns -1.\n2. **Row Consistency Check**: It verifies that all rows have the same length as the first row. If any row has a different length, it returns -1.\n3. **Pixel Validation**: Each pixel is checked to ensure it falls within the valid range (0-255). Any invalid pixel causes the function to return -1.\n4. **Total Pixels Check**: If the total number of pixels (rows * columns) is zero (e.g., all rows are empty), the function returns -1 to avoid division by zero.\n5. **Average Calculation**: The sum of all pixel values is computed, divided by the total number of pixels, and the result is rounded to two decimal places for the final output.\n\nThis approach ensures all edge cases are handled efficiently and the average brightness is computed accurately for valid inputs."}
{"task_id": 71, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Root Mean Square Error (RMSE) between actual values (y_true) and predicted values (y_pred). RMSE is a standard metric for evaluating the accuracy of regression models, providing insight into the standard deviation of residuals. The solution must handle various edge cases such as mismatched array shapes, empty arrays, and invalid input types.\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Convert the input lists or arrays into NumPy arrays to leverage NumPy's efficient array operations.\n2. **Check Dimensionality**: Ensure both arrays are 1-dimensional to handle vector operations correctly.\n3. **Validate Lengths**: Check that both arrays have the same length and are non-empty to avoid calculation errors.\n4. **Check Numeric Types**: Ensure both arrays contain numeric values to perform arithmetic operations.\n5. **Calculate RMSE**: Compute the squared differences between actual and predicted values, take their mean, and then the square root of the mean. Finally, round the result to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    if y_true.ndim != 1 or y_pred.ndim != 1:\n        raise ValueError(\"Inputs must be 1-dimensional arrays.\")\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"Input arrays must have the same length.\")\n    if len(y_true) == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n    if not (np.issubdtype(y_true.dtype, np.number) and np.issubdtype(y_pred.dtype, np.number)):\n        raise TypeError(\"Input arrays must contain numeric values.\")\n    \n    squared_errors = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    rmse_value = np.sqrt(mean_squared_error)\n    \n    return round(rmse_value, 3)\n```\n\n### Explanation\n1. **Input Conversion**: The inputs are converted to NumPy arrays using `np.asarray` to handle both lists and arrays uniformly.\n2. **Dimensionality Check**: The code checks if both arrays are 1-dimensional to ensure vector operations are valid.\n3. **Length Validation**: It ensures both arrays have the same length and are non-empty to avoid division by zero or mismatched element errors.\n4. **Numeric Check**: Verifies that both arrays contain numeric data to prevent type errors during arithmetic operations.\n5. **RMSE Calculation**: The squared differences between actual and predicted values are computed, averaged, and then the square root is taken. The result is rounded to three decimal places for readability.\n\nThis approach efficiently handles all edge cases and ensures the function operates correctly on valid inputs while providing meaningful error messages for invalid cases."}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    denominator = sum_true + sum_pred\n    if denominator == 0:\n        return 1.0  # Both are empty, perfect match\n    dice = (2.0 * intersection) / denominator\n    return round(dice, 3)\n```"}
{"task_id": 74, "completion_id": 0, "completion": "To solve this problem, we need to generate a composite hypervector for a dataset row using Hyperdimensional Computing (HDC). Each feature in the row is represented by binding hypervectors for the feature name and its value. The hypervectors for the values are created using seeds provided for each feature to ensure reproducibility. The composite hypervector is formed by bundling all feature hypervectors.\n\n### Approach\n1. **Feature Name Hypervectors**: Generate hypervectors for feature names using a seed derived from the hash of the feature name. This ensures that each feature name consistently generates the same hypervector.\n2. **Value Hypervectors**: Generate hypervectors for values using seeds provided in the `random_seeds` dictionary. Each feature's value hypervector is generated using its respective seed, ensuring reproducibility.\n3. **Binding**: Combine the feature name hypervector and the value hypervector using element-wise multiplication (binding) to represent the feature-value pair.\n4. **Bundling**: Sum all bound hypervectors to form the composite hypervector representing the entire row.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    composite = np.zeros(dim)\n    for feature, value in row.items():\n        # Generate feature name hypervector using a seed derived from the feature name\n        name_seed = abs(hash(feature))\n        rng_name = np.random.default_rng(name_seed)\n        name_hv = rng_name.choice([-1, 1], size=dim)\n        \n        # Generate value hypervector using the provided seed for the feature\n        value_seed = random_seeds[feature]\n        rng_value = np.random.default_rng(value_seed)\n        value_hv = rng_value.choice([-1, 1], size=dim)\n        \n        # Bind the two hypervectors using element-wise multiplication\n        bound_hv = name_hv * value_hv\n        \n        # Bundle into the composite hypervector\n        composite += bound_hv\n    \n    return composite.tolist()\n```\n\n### Explanation\n- **Feature Name Hypervectors**: Each feature name is hashed to generate a seed, ensuring the same name always produces the same hypervector. This is done using `numpy.random.default_rng` for reproducibility.\n- **Value Hypervectors**: Each value's hypervector is generated using the seed from `random_seeds` for the corresponding feature. This ensures that the same feature value (though the value itself isn't directly used in this approach) produces the same hypervector across different runs.\n- **Binding**: The element-wise multiplication of the feature name and value hypervectors combines their information into a single hypervector representing the feature-value pair.\n- **Bundling**: Summing all bound hypervectors aggregates the information from all features into a composite hypervector, which captures the entire row's data in a high-dimensional space.\n\nThis approach ensures that the composite hypervector is both reproducible and representative of the dataset row's features and their values."}
{"task_id": 75, "completion_id": 0, "completion": "To solve this problem, we need to generate a confusion matrix for a binary classification problem. The confusion matrix is a 2x2 matrix that summarizes the performance of a classification algorithm by comparing the actual labels to the predicted labels. The matrix is structured such that rows represent the actual classes and columns represent the predicted classes.\n\n### Approach\n1. **Initialize the Matrix**: Start with a 2x2 matrix filled with zeros. The matrix will be structured as follows:\n   - The first row corresponds to actual class 0.\n   - The second row corresponds to actual class 1.\n   - The first column corresponds to predicted class 0.\n   - The second column corresponds to predicted class 1.\n\n2. **Populate the Matrix**: Iterate over each pair of actual and predicted labels in the input data. For each pair, increment the corresponding cell in the matrix based on the actual and predicted labels.\n\n3. **Return the Matrix**: After processing all data points, return the populated matrix.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    matrix = [[0, 0], [0, 0]]\n    for y_true, y_pred in data:\n        matrix[y_true][y_pred] += 1\n    return matrix\n```\n\n### Explanation\n- **Matrix Initialization**: The matrix is initialized to a 2x2 structure with all elements set to zero. This matrix will hold counts of true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP).\n- **Matrix Population**: For each data point `(y_true, y_pred)`, we increment the cell at `matrix[y_true][y_pred]`. This effectively counts how many times each combination of actual and predicted labels occurs.\n  - `matrix[0][0]` (TN): Actual class 0, predicted class 0.\n  - `matrix[0][1]` (FP): Actual class 0, predicted class 1.\n  - `matrix[1][0]` (FN): Actual class 1, predicted class 0.\n  - `matrix[1][1]` (TP): Actual class 1, predicted class 1.\n- **Return**: The populated matrix is returned, providing a clear summary of classification performance.\n\nThis approach efficiently processes each data point in linear time, making it both time and space efficient with a complexity of O(n), where n is the number of data points."}
{"task_id": 76, "completion_id": 0, "completion": "To solve this problem, we need to calculate the cosine similarity between two vectors using numpy. Cosine similarity measures the cosine of the angle between two vectors, indicating their directional similarity. This is done by computing the dot product of the vectors and dividing it by the product of their magnitudes.\n\n### Approach\n1. **Flatten the Vectors**: Convert the input vectors into 1D arrays to handle any shape (row or column vectors) uniformly.\n2. **Compute Dot Product**: Calculate the dot product of the flattened vectors using numpy's dot function.\n3. **Calculate Magnitudes**: Determine the L2 norm (magnitude) of each flattened vector using numpy's norm function.\n4. **Compute Similarity**: Divide the dot product by the product of the magnitudes to get the cosine similarity.\n5. **Round the Result**: Round the resulting similarity to three decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    v1_flat = v1.flatten()\n    v2_flat = v2.flatten()\n    dot_product = np.dot(v1_flat, v2_flat)\n    norm_v1 = np.linalg.norm(v1_flat)\n    norm_v2 = np.linalg.norm(v2_flat)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)\n```\n\n### Explanation\n1. **Flattening Vectors**: The vectors are flattened to ensure they are treated as 1D arrays, which simplifies the calculation of the dot product and magnitudes regardless of their original shape.\n2. **Dot Product**: The dot product of two vectors is the sum of the products of their corresponding elements, which measures their alignment.\n3. **Magnitudes**: The L2 norm (or magnitude) of a vector is computed to understand its length in the multidimensional space.\n4. **Cosine Similarity**: By dividing the dot product by the product of magnitudes, we get a value between -1 and 1, where 1 means the vectors are identical in direction, 0 means they are orthogonal, and -1 means they are opposite in direction.\n5. **Rounding**: The result is rounded to three decimal places for readability and precision as required by the problem constraints."}
{"task_id": 77, "completion_id": 0, "completion": "To solve this problem, we need to compute various performance metrics for a binary classification model. These metrics include the confusion matrix, accuracy, F1 score, specificity, and negative predictive value (NPV). Each of these metrics provides insight into different aspects of the model's performance, such as how well it identifies positive and negative classes, its overall correctness, and the balance between precision and recall.\n\n### Approach\n1. **Confusion Matrix**: This matrix helps in understanding the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These counts are derived by comparing actual and predicted labels.\n2. **Accuracy**: This is the ratio of correctly predicted instances (both TP and TN) to the total number of instances.\n3. **F1 Score**: This is the harmonic mean of precision and recall. Precision is the ratio of TP to the sum of TP and FP, while recall is the ratio of TP to the sum of TP and FN.\n4. **Specificity**: This measures the proportion of actual negatives that are correctly identified as such, calculated as the ratio of TN to the sum of TN and FP.\n5. **Negative Predictive Value (NPV)**: This measures the proportion of predicted negatives that are actual negatives, calculated as the ratio of TN to the sum of TN and FN.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    TN = FP = FN = TP = 0\n    for a, p in zip(actual, predicted):\n        if a == 0:\n            if p == 0:\n                TN += 1\n            else:\n                FP += 1\n        else:\n            if p == 0:\n                FN += 1\n            else:\n                TP += 1\n    \n    confusion_matrix = [[TN, FP], [FN, TP]]\n    total = len(actual)\n    accuracy = (TN + TP) / total if total != 0 else 0.0\n    \n    # Calculate Precision and Recall for F1 Score\n    precision = TP / (TP + FP) if (TP + FP) != 0 else 0.0\n    recall = TP / (TP + FN) if (TP + FN) != 0 else 0.0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0.0\n    \n    # Calculate Specificity\n    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0.0\n    \n    # Calculate Negative Predictive Value\n    npv = TN / (TN + FN) if (TN + FN) != 0 else 0.0\n    \n    # Rounding to three decimal places\n    accuracy = round(accuracy, 3)\n    f1 = round(f1, 3)\n    specificity = round(specificity, 3)\n    npv = round(npv, 3)\n    \n    return (confusion_matrix, accuracy, f1, specificity, npv)\n```\n\n### Explanation\n1. **Confusion Matrix Calculation**: We iterate through each pair of actual and predicted labels to count TN, FP, FN, and TP. These counts are structured into a 2x2 matrix.\n2. **Accuracy Calculation**: The sum of TP and TN divided by the total number of instances gives the accuracy.\n3. **F1 Score Calculation**: Precision and recall are computed first. If their denominators are zero, they default to zero to avoid division errors. The F1 score is then derived from these values.\n4. **Specificity Calculation**: This is computed as the ratio of TN to the sum of TN and FP, handling division by zero by defaulting to zero.\n5. **NPV Calculation**: This is computed as the ratio of TN to the sum of TN and FN, again handling division by zero by defaulting to zero.\n\nEach metric is rounded to three decimal places to meet the problem's output requirements. This approach ensures that all edge cases, such as division by zero, are handled gracefully, providing robust and reliable performance metrics for binary classification models."}
{"task_id": 78, "completion_id": 0, "completion": "To solve this problem, we need to calculate various descriptive statistics metrics for a given dataset. The function should take a list or NumPy array of numerical values and return a dictionary containing mean, median, mode, variance, standard deviation, percentiles (25th, 50th, 75th), and interquartile range (IQR). \n\n### Approach\n1. **Mean**: Calculate the average of the dataset using numpy's mean function and round to four decimal places.\n2. **Median**: Calculate the middle value of the sorted dataset using numpy's median function and round to four decimal places.\n3. **Mode**: Determine the most frequently occurring value in the dataset. If there are multiple modes, return the smallest one. This is done using numpy's unique function to count occurrences and find the most frequent value.\n4. **Variance**: Compute the sample variance using numpy's variance function with `ddof=1` (to use `n-1` in the denominator) and round to four decimal places.\n5. **Standard Deviation**: Calculate the square root of the variance using numpy's standard deviation function with `ddof=1` and round to four decimal places.\n6. **Percentiles**: Calculate the 25th, 50th (which is the same as the median), and 75th percentiles using numpy's percentile function and round each to four decimal places.\n7. **Interquartile Range (IQR)**: Compute the difference between the 75th and 25th percentiles and round to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef descriptive_statistics(data):\n    # Calculate mean and round to 4 decimal places\n    mean_val = np.round(np.mean(data), 4).item()\n    \n    # Calculate median and round to 4 decimal places\n    median_val = np.round(np.median(data), 4).item()\n    \n    # Calculate mode\n    values, counts = np.unique(data, return_counts=True)\n    mode_index = np.argmax(counts)\n    mode_val = values[mode_index].item()  # Convert numpy type to Python type\n    \n    # Calculate sample variance (ddof=1) and round to 4 decimal places\n    variance_val = np.round(np.var(data, ddof=1), 4).item()\n    \n    # Calculate sample standard deviation (ddof=1) and round\n    std_dev_val = np.round(np.std(data, ddof=1), 4).item()\n    \n    # Calculate percentiles and round each to 4 decimal places\n    p25 = np.round(np.percentile(data, 25), 4).item()\n    p50 = np.round(np.percentile(data, 50), 4).item()\n    p75 = np.round(np.percentile(data, 75), 4).item()\n    \n    # Calculate interquartile range (IQR) and round\n    iqr_val = np.round(p75 - p25, 4).item()\n    \n    # Compile results into a dictionary\n    return {\n        'mean': mean_val,\n        'median': median_val,\n        'mode': mode_val,\n        'variance': variance_val,\n        'standard_deviation': std_dev_val,\n        '25th_percentile': p25,\n        '50th_percentile': p50,\n        '75th_percentile': p75,\n        'interquartile_range': iqr_val\n    }\n```\n\n### Explanation\n- **Mean**: The average value of the dataset, providing a central tendency measure.\n- **Median**: The middle value when the data is sorted, giving another measure of central tendency robust to outliers.\n- **Mode**: The most frequently occurring value, useful for understanding common values in the dataset.\n- **Variance**: Measures the spread of the data points from the mean, indicating data variability.\n- **Standard Deviation**: The square root of variance, providing a measure of spread in the same units as the data.\n- **Percentiles**: Values below which a certain percentage of the data falls, with the 25th, 50th (median), and 75th percentiles being commonly used.\n- **Interquartile Range (IQR)**: The range between the 25th and 75th percentiles, showing the spread of the middle 50% of the data, useful for identifying outliers.\n\nThis approach efficiently computes all required statistics using numpy functions for accuracy and performance, ensuring results are rounded and converted to Python types for compatibility."}
{"task_id": 79, "completion_id": 0, "completion": "To solve this problem, we need to calculate the probability of achieving exactly k successes in n independent Bernoulli trials using the Binomial distribution formula. The Binomial distribution formula involves combinations, powers of probabilities, and rounding the result to five decimal places.\n\n### Approach\n1. **Understanding the Binomial Formula**: The probability of achieving exactly k successes in n trials is given by the formula:\n   \\[\n   P(k) = C(n, k) \\times p^k \\times (1-p)^{n-k}\n   \\]\n   where \\( C(n, k) \\) is the number of combinations of n items taken k at a time, calculated as \\( C(n, k) = \\frac{n!}{k! \\times (n-k)!} \\).\n\n2. **Combination Calculation**: Using factorials to compute the combination \\( C(n, k) \\).\n\n3. **Probability Calculation**: Compute the probability using the combination and the given probabilities p and (1-p) raised to the respective powers of k and (n-k).\n\n4. **Rounding**: The result is rounded to five decimal places to meet the output constraints.\n\n### Solution Code\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n    probability = combination * (p ** k) * ((1 - p) ** (n - k))\n    return round(probability, 5)\n```\n\n### Explanation\n1. **Combination Calculation**: The combination \\( C(n, k) \\) is computed using factorials. This is done by dividing the factorial of n by the product of the factorials of k and (n-k).\n\n2. **Probability Terms**: The terms \\( p^k \\) and \\( (1-p)^{n-k} \\) are computed using exponentiation. These terms represent the probabilities of k successes and (n-k) failures, respectively.\n\n3. **Multiplication**: The combination is multiplied by the probability terms to get the final probability value.\n\n4. **Rounding**: The result is rounded to five decimal places using Python's `round` function to meet the specified precision requirement.\n\nThis approach ensures that the calculation is both efficient and accurate, adhering to the mathematical definition of the Binomial distribution while meeting the problem's constraints."}
{"task_id": 80, "completion_id": 0, "completion": "To solve this problem, we need to calculate the probability density function (PDF) of the normal distribution for given values of x, mean, and standard deviation. The normal distribution is a continuous probability distribution that is symmetric around the mean, showing that data near the mean are more frequent in occurrence than data far from the mean.\n\n### Approach\nThe PDF of a normal distribution is defined by the formula:\n\\[ f(x, \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\]\nwhere:\n- \\( \\mu \\) is the mean\n- \\( \\sigma \\) is the standard deviation\n- \\( x \\) is the value at which the PDF is evaluated\n\nThe steps to compute the PDF are as follows:\n1. Compute the coefficient part of the formula, which is \\( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\).\n2. Compute the exponent part of the formula, which is \\( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\).\n3. Use the coefficient and exponent to compute the final PDF value.\n4. Round the result to 5 decimal places for precision.\n\n### Solution Code\n\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -((x - mean) ** 2) / (2 * (std_dev ** 2))\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\n```\n\n### Explanation\n1. **Coefficient Calculation**: The coefficient \\( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\) is calculated first. This part normalizes the distribution such that the total area under the curve equals 1.\n2. **Exponent Calculation**: The exponent \\( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\) is computed next. This part determines how the value \\( x \\) deviates from the mean, scaled by the standard deviation.\n3. **Exponential Function**: The exponential function \\( e \\) raised to the computed exponent adjusts the value based on how far \\( x \\) is from the mean.\n4. **Final PDF Value**: The product of the coefficient and the exponential term gives the PDF value at \\( x \\).\n5. **Rounding**: The result is rounded to 5 decimal places to ensure precision and readability.\n\nThis approach efficiently computes the PDF using the mathematical formula of the normal distribution, ensuring accuracy and precision."}
{"task_id": 81, "completion_id": 0, "completion": "To solve this problem, we need to calculate the probability of observing exactly k events in a fixed interval using the Poisson distribution formula. The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space, given the average rate of occurrence.\n\n### Approach\nThe Poisson distribution probability formula is given by:\n\\[ P(k; \\lambda) = \\frac{e^{-\\lambda} \\cdot \\lambda^k}{k!} \\]\nwhere:\n- \\( \\lambda \\) (lambda) is the average rate of occurrences (mean) in the interval.\n- \\( k \\) is the number of occurrences of the event.\n- \\( e \\) is the base of the natural logarithm (approximately 2.71828).\n\nThe steps to compute the probability are as follows:\n1. **Handle Edge Cases**: If the mean rate (\\(\\lambda\\)) is zero, the probability of observing zero events (k=0) is 1, and the probability of observing any other number of events (k>0) is 0.\n2. **Compute the Exponential Term**: Calculate \\( e^{-\\lambda} \\).\n3. **Compute the Power Term**: Calculate \\( \\lambda^k \\).\n4. **Compute the Factorial**: Calculate \\( k! \\) (k factorial).\n5. **Combine the Terms**: Multiply the exponential term by the power term and divide by the factorial term.\n6. **Round the Result**: Return the result rounded to 5 decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    if lam == 0:\n        if k == 0:\n            prob = 1.0\n        else:\n            prob = 0.0\n    else:\n        prob = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    return round(prob, 5)\n```\n\n### Explanation\n- **Edge Cases Handling**: When the mean rate (\\(\\lambda\\)) is zero, the Poisson distribution simplifies. If no events are expected (\\(\\lambda = 0\\)), the probability of observing zero events (k=0) is 1, and the probability of observing any positive number of events (k>0) is 0.\n- **Exponential Term**: The term \\( e^{-\\lambda} \\) is computed using `math.exp(-lam)`.\n- **Power Term**: The term \\( \\lambda^k \\) is computed using Python's exponentiation operator `**`.\n- **Factorial Term**: The factorial of k is computed using `math.factorial(k)`, which handles non-negative integers.\n- **Combining Terms**: The product of the exponential and power terms divided by the factorial term gives the Poisson probability.\n- **Rounding**: The result is rounded to 5 decimal places using Python's `round` function to meet the problem's requirements.\n\nThis approach efficiently computes the Poisson probability using direct mathematical operations and handles edge cases to ensure correctness."}
{"task_id": 82, "completion_id": 0, "completion": "To solve this problem, we need to calculate the contrast of a grayscale image using the difference between the maximum and minimum pixel values. The contrast of an image is a measure of the difference in intensity between the lightest and darkest parts of the image. This can be determined by finding the highest and lowest pixel values in the image and computing their difference.\n\n### Approach\n1. **Understand the Problem Requirements**: The problem requires a function that takes a 2D numpy array representing a grayscale image and returns the contrast, defined as the difference between the maximum and minimum pixel values.\n2. **Use Numpy Functions**: Utilize numpy's `max()` and `min()` functions to efficiently find the maximum and minimum pixel values in the image array.\n3. **Compute the Difference**: Subtract the minimum pixel value from the maximum pixel value to get the contrast.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast value, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    max_val = np.max(img)\n    min_val = np.min(img)\n    return max_val - min_val\n```\n\n### Explanation\n1. **Import Numpy**: The numpy library is imported to handle array operations efficiently.\n2. **Function Definition**: The function `calculate_contrast` takes a 2D numpy array `img` as input.\n3. **Calculate Maximum and Minimum Values**: Using numpy's `max()` and `min()` functions, the highest and lowest pixel values in the image are determined.\n4. **Compute Contrast**: The contrast is computed as the difference between the maximum and minimum pixel values, which is then returned as the result.\n\nThis approach efficiently computes the contrast using built-in numpy functions, ensuring optimal performance even for larger images. The solution handles all valid grayscale images correctly, including edge cases where all pixel values are the same (resulting in a contrast of 0)."}
{"task_id": 83, "completion_id": 0, "completion": "To solve this problem, we need to calculate the dot product of two vectors represented as 1D NumPy arrays. The dot product of two vectors is computed by taking the sum of the products of their corresponding elements. This can be efficiently achieved using NumPy's built-in function for dot product calculation.\n\n### Approach\nThe approach involves utilizing the `numpy.dot` function, which directly computes the dot product of two arrays. This function is efficient and handles the underlying computations optimally. The steps are as follows:\n1. Import the NumPy library.\n2. Define a function `calculate_dot_product` that takes two 1D NumPy arrays as parameters.\n3. Use `numpy.dot` to compute the dot product of the two input vectors.\n4. Return the result of the dot product.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    return np.dot(vec1, vec2)\n```\n\n### Explanation\n1. **Import NumPy**: The NumPy library is imported to leverage its array operations and the `dot` function.\n2. **Function Definition**: The function `calculate_dot_product` is defined to take two parameters, `vec1` and `vec2`, both of which are 1D NumPy arrays.\n3. **Dot Product Calculation**: The `numpy.dot` function is called with the two vectors as arguments. This function computes the sum of the products of the corresponding elements of the vectors.\n4. **Return Result**: The result of the `numpy.dot` function call is returned, which is the dot product of the two vectors.\n\nThis approach ensures that the computation is both efficient and concise, leveraging NumPy's optimized operations for better performance."}
{"task_id": 84, "completion_id": 0, "completion": "To solve this problem, we need to perform a Phi Transformation on a list of numerical data to generate polynomial features up to a specified degree. This transformation allows models like linear regression to handle nonlinear data by creating polynomial combinations of the original features.\n\n### Approach\n1. **Check for Invalid Degree**: If the given degree is less than 0, return an empty list immediately.\n2. **Generate Polynomial Features**: For each data point in the input list, generate all polynomial terms from degree 0 up to the specified degree. This includes terms like \\(x^0, x^1, x^2, \\ldots, x^d\\), where \\(d\\) is the given degree.\n3. **Rounding**: Each generated polynomial term is rounded to 8 decimal places to meet the output constraints.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed = []\n    for x in data:\n        features = [round(x**i, 8) for i in range(degree + 1)]\n        transformed.append(features)\n    return transformed\n```\n\n### Explanation\n1. **Invalid Degree Handling**: The function first checks if the provided degree is negative. If it is, the function returns an empty list as specified.\n2. **Polynomial Feature Generation**: For each element in the input data list, the function computes all polynomial terms from \\(x^0\\) (which is always 1) up to \\(x^d\\), where \\(d\\) is the given degree. This is done using a list comprehension that iterates from 0 to the degree.\n3. **Rounding**: Each computed term is rounded to 8 decimal places using Python's built-in `round` function to ensure precision in the output.\n\nThis approach efficiently generates the required polynomial features and ensures the output meets the specified constraints. The use of list comprehensions ensures clarity and conciseness in the code."}
{"task_id": 85, "completion_id": 0, "completion": "To solve this problem, we need to implement a positional encoding layer for Transformers as described in the \"Attention is All You Need\" paper. The positional encoding helps the model to capture the position of each element in the sequence, which is crucial since Transformers do not have inherent sequential information.\n\n### Approach\n1. **Edge Cases Handling**: The function should first check if the `position` is 0 or if `d_model` (dimensionality of the model) is less than or equal to 0. If either condition is met, the function returns -1.\n2. **Indices Calculation**: Generate an array of indices from 0 to `d_model - 1`.\n3. **Exponent Calculation**: For each index, compute the corresponding exponent based on the index divided by 2 (integer division). This exponent is used to scale the divisor in the positional encoding formula.\n4. **Divisor and Angles Calculation**: Compute the divisor using the calculated exponents and then determine the angles by dividing the given position by these divisors.\n5. **Sine and Cosine Application**: Apply sine and cosine functions to these angles. Specifically, use sine for even indices and cosine for odd indices.\n6. **Data Type Conversion**: Convert the resulting array to `np.float16` and then to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    k = np.arange(d_model)\n    i = k // 2\n    exponents = (2 * i) / d_model\n    divisors = np.power(10000.0, exponents)\n    angles = position / divisors\n    pe = np.where(k % 2 == 0, np.sin(angles), np.cos(angles))\n    pe = pe.astype(np.float16)\n    return pe.tolist()\n```\n\n### Explanation\n- **Edge Cases Handling**: The function immediately checks if `position` is 0 or `d_model` is non-positive, returning -1 in such cases.\n- **Indices and Exponents**: By generating indices from 0 to `d_model - 1`, we can compute the appropriate exponents for each position in the encoding vector. The integer division by 2 ensures that pairs of indices (even and odd) share the same exponent.\n- **Divisors and Angles**: The divisor is computed as 10000 raised to the power of the exponent. The angles are derived by dividing the position by these divisors.\n- **Sine and Cosine Application**: Using numpy's `where` function, we efficiently apply sine to even indices and cosine to odd indices, creating the positional encoding vector.\n- **Data Conversion**: The resulting array is converted to `np.float16` to meet the output requirements and then transformed into a list for compatibility.\n\nThis approach ensures that the positional encoding is computed efficiently using vectorized operations, making it suitable for high-dimensional models typically used in Transformer architectures."}
{"task_id": 86, "completion_id": 0, "completion": "To solve this problem, we need to determine if a machine learning model is overfitting, underfitting, or performing well based on its training and test accuracy values. The solution involves checking specific conditions in a defined order to classify the model's performance.\n\n### Approach\nThe approach to determine the model's fit quality is based on two primary conditions:\n1. **Overfitting**: This occurs when the training accuracy is significantly higher than the test accuracy (difference greater than 0.2). This condition is checked first.\n2. **Underfitting**: This occurs when both the training and test accuracies are below 0.7. This condition is checked only if the overfitting condition is not met.\n\nIf neither of these conditions is met, the model is considered to have a good fit.\n\n### Solution Code\n```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if (training_accuracy - test_accuracy) > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\n```\n\n### Explanation\n1. **Overfitting Check**: The function first checks if the difference between training and test accuracy exceeds 0.2. If true, the model is overfitting, and the function returns 1.\n2. **Underfitting Check**: If the model is not overfitting, the function then checks if both training and test accuracies are below 0.7. If true, the model is underfitting, and the function returns -1.\n3. **Good Fit**: If neither condition is met, the model is considered to have a good fit, and the function returns 0.\n\nThis approach ensures that the conditions are evaluated in the correct order, prioritizing overfitting detection first, followed by underfitting, and finally defaulting to a good fit if neither condition is met."}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update first and second moment estimates\n    m_new = beta1 * m + (1 - beta1) * grad\n    v_new = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected estimates\n    m_hat = m_new / (1 - beta1 ** t)\n    v_hat = v_new / (1 - beta2 ** t)\n    \n    # Update parameters\n    parameter_updated = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round to 5 decimal places and convert to list\n    parameter_rounded = np.round(parameter_updated, 5)\n    m_rounded = np.round(m_new, 5)\n    v_rounded = np.round(v_new, 5)\n    \n    return (parameter_rounded.tolist(), m_rounded.tolist(), v_rounded.tolist())\n```"}
{"task_id": 88, "completion_id": 0, "completion": "To solve this problem, we need to implement a simplified version of the GPT-2 text generation model. The goal is to generate text autoregressively using token embeddings, positional embeddings, multi-head attention, a feed-forward network, and layer normalization. The solution will use a helper function to load dummy encoders, hyperparameters, and model parameters to simulate the behavior of a minimal GPT-2 model.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Tokenization**: Convert the input prompt into token IDs using a dummy encoder.\n2. **Embeddings**: Generate token embeddings and positional embeddings for the input sequence.\n3. **Multi-head Attention**: Apply a simplified version of multi-head attention to the embeddings.\n4. **Feed-Forward Network**: Process the attention output through a feed-forward network with a ReLU activation.\n5. **Layer Normalization**: Normalize the output using layer normalization.\n6. **Logits Calculation**: Compute the logits for the next token using the normalized output.\n7. **Token Sampling**: Select the next token using the logits and append it to the sequence.\n8. **Decoding**: Convert the generated token IDs back into text.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef layer_norm(x, g, b, eps=1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x = (x - mean) / np.sqrt(variance + eps)\n    return g * x + b\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / exp_x.sum(axis=-1, keepdims=True)\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    np.random.seed(42)  # Set the random seed for reproducibility\n\n    encoder, hparams, params = load_encoder_hparams_and_params()\n\n    # Encode the prompt into token IDs\n    input_ids = encoder.encode(prompt)\n\n    for _ in range(n_tokens_to_generate):\n        # Truncate the input if it exceeds the context size\n        current_input = input_ids[-hparams['n_ctx']:]\n        n_tokens = len(current_input)\n\n        # Token Embeddings\n        token_emb = params['wte'][current_input]  # Shape: (n_tokens, d_model)\n\n        # Positional Embeddings\n        positions = np.arange(n_tokens)\n        pos_emb = params['wpe'][positions]  # Shape: (n_tokens, d_model)\n\n        # Sum embeddings\n        embeddings = token_emb + pos_emb  # Shape: (n_tokens, d_model)\n\n        # Dummy Multi-Head Attention (simplified)\n        # For this simplified version, we'll use the embeddings directly\n        attn_output = embeddings.copy()\n\n        # Dummy Feed-Forward Network with ReLU\n        ff_output = np.maximum(attn_output, 0)  # ReLU activation\n\n        # Apply Layer Normalization using final layer norm parameters\n        norm_output = layer_norm(ff_output, params['ln_f']['g'], params['ln_f']['b'])\n\n        # Compute logits by multiplying with the transposed token embeddings\n        logits = np.dot(norm_output, params['wte'].T)  # Shape: (n_tokens, vocab_size)\n\n        # Get the logits for the last token\n        next_token_logits = logits[-1]\n\n        # Apply softmax to get probabilities\n        probs = softmax(next_token_logits)\n\n        # Sample the next token (using argmax for deterministic output)\n        next_token = np.argmax(probs)\n\n        # Append the new token to the input_ids\n        input_ids.append(int(next_token))\n\n    # Decode the tokens back to text\n    generated_text = encoder.decode(input_ids)\n    return generated_text\n```\n\n### Explanation\n1. **Tokenization**: The input prompt is converted into a list of token IDs using a dummy encoder. This encoder splits the input text by spaces and maps each token to an ID using a predefined dictionary.\n2. **Embeddings**: Token embeddings are retrieved from the `wte` matrix, and positional embeddings are retrieved from the `wpe` matrix. These embeddings are summed to combine token and positional information.\n3. **Multi-head Attention**: A simplified version of multi-head attention is applied. For this implementation, the attention output is a direct copy of the embeddings, simulating attention without actual parameters.\n4. **Feed-Forward Network**: A ReLU activation is applied to the attention output, followed by an identity transformation (due to the lack of actual parameters for the feed-forward network).\n5. **Layer Normalization**: The output is normalized using the final layer normalization parameters from the model.\n6. **Logits Calculation**: The normalized output is projected back to the vocabulary space using the transpose of the token embeddings matrix to compute logits.\n7. **Token Sampling**: The next token is selected using the argmax of the logits for simplicity, ensuring deterministic output.\n8. **Decoding**: The generated token IDs are converted back into text using the dummy encoder's decode method.\n\nThis approach provides a simplified but structured implementation of the GPT-2 text generation process, incorporating all required components even with dummy parameters."}
{"task_id": 89, "completion_id": 0, "completion": "To solve this problem, we need to implement a simplified self-attention mechanism to analyze the relationships between crystals based on their values. The goal is to compute the enhanced pattern for each crystal by considering its interactions with all other crystals using attention scores derived from the softmax function.\n\n### Approach\n1. **Compute Attention Scores**: For each crystal, calculate its relationship with every other crystal using a scaled dot product. The score between two crystals is computed as the product of their values divided by the square root of the given dimension.\n2. **Apply Softmax**: Convert these scores into probabilities using the softmax function, which ensures that the scores sum to 1 and are non-negative.\n3. **Compute Weighted Sum**: For each crystal, compute the final enhanced pattern as the weighted sum of all crystal values, using the attention scores as weights.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        e = np.exp(values - np.max(values))\n        return e / e.sum()\n    \n    crystals = np.array(crystal_values)\n    scores = np.outer(crystals, crystals) / np.sqrt(dimension)\n    attention = np.array([softmax(row) for row in scores])\n    output = attention @ crystals\n    return [round(x, 4) for x in output.tolist()]\n```\n\n### Explanation\n1. **Softmax Function**: The softmax function is applied to each row of the score matrix to convert the scores into probabilities. This function subtracts the maximum value from each element to avoid numerical instability and then exponentiates the values before normalizing them.\n2. **Score Matrix Calculation**: The score matrix is computed using the outer product of the crystal values, scaled by the square root of the given dimension. This scaling helps in stabilizing gradients during training.\n3. **Attention Weights**: Each row of the score matrix is passed through the softmax function to get the attention weights, which represent the importance of each crystal relative to others.\n4. **Weighted Sum**: The final output for each crystal is computed as the dot product of the attention weights and the crystal values, resulting in a weighted sum that forms the enhanced pattern.\n\nThis approach efficiently captures the interactions between crystals and computes their enhanced patterns using principles from self-attention mechanisms, ensuring that each crystal's value is contextualized relative to all others."}
{"task_id": 90, "completion_id": 0, "completion": "To solve this problem, we need to implement the BM25 ranking function, which is a probabilistic information retrieval algorithm used to estimate the relevance of documents to a given search query. BM25 improves upon the traditional TF-IDF approach by incorporating term frequency saturation and document length normalization.\n\n### Approach\n1. **Preprocessing**: Split each document and the query into terms (words) using whitespace as the delimiter. This converts each document and the query into lists of terms.\n2. **Document Lengths and Average Length**: Calculate the length of each document and compute the average document length across the corpus.\n3. **Term Frequency (TF) Calculation**: For each document, compute the frequency of each term using a counter.\n4. **Inverse Document Frequency (IDF) Calculation**: For each unique term in the query, compute the IDF using the formula that considers the number of documents containing the term.\n5. **BM25 Score Calculation**: For each document, compute the BM25 score by summing the contributions of each query term. The contribution of each term is determined by its TF in the document, IDF, and the document's length relative to the average document length.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    # Split each document into terms and the query into unique terms\n    docs_terms = [doc.split() for doc in corpus]\n    query_terms = list(set(query.split()))  # Use unique query terms\n    \n    N = len(docs_terms)\n    if N == 0:\n        return []\n    \n    # Calculate document lengths and average document length\n    doc_lengths = [len(doc) for doc in docs_terms]\n    avgdl = sum(doc_lengths) / N\n    \n    # Precompute term frequencies for each document\n    doc_freqs = [Counter(doc) for doc in docs_terms]\n    \n    # Precompute IDF for each query term\n    idf = {}\n    for term in query_terms:\n        n_t = sum(1 for doc in docs_terms if term in doc)\n        idf_value = np.log((N - n_t + 0.5) / (n_t + 0.5))\n        idf[term] = idf_value\n    \n    scores = []\n    for i in range(N):\n        score = 0.0\n        doc_len = doc_lengths[i]\n        for term in query_terms:\n            tf = doc_freqs[i].get(term, 0)\n            # Compute denominator components\n            if avgdl != 0:\n                denominator_part = (1 - b) + b * (doc_len / avgdl)\n            else:\n                denominator_part = 1 - b\n            denominator = tf + k1 * denominator_part\n            # Calculate term's contribution to the score\n            if denominator != 0:\n                score += idf[term] * (tf * (k1 + 1)) / denominator\n        scores.append(round(score, 3))\n    \n    return scores\n```\n\n### Explanation\n1. **Preprocessing**: Each document in the corpus and the query are split into terms. The query terms are deduplicated to ensure each term is processed once.\n2. **Document Lengths**: The length of each document and the average document length are computed to handle document length normalization.\n3. **Term Frequencies**: Using counters, we track how often each term appears in each document.\n4. **IDF Calculation**: For each unique term in the query, we calculate how many documents contain the term and use this to compute the IDF, which measures the term's importance across the corpus.\n5. **BM25 Score**: For each document, the score is computed by iterating over each query term, calculating its contribution based on TF, IDF, and document length normalization. The contributions are summed to get the document's final score, which is then rounded to three decimal places.\n\nThis approach efficiently combines term frequency and inverse document frequency with document length normalization to provide a relevance score for each document relative to the query."}
{"task_id": 91, "completion_id": 0, "completion": "To solve this problem, we need to calculate the F1 score given the true and predicted labels. The F1 score is a harmonic mean of precision and recall, which are metrics used to evaluate the performance of a classification model. Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positive predictions among all actual positive instances.\n\n### Approach\n1. **Identify Unique Labels**: Determine the unique labels present in both the true and predicted labels. This helps in identifying the positive class, which is necessary for calculating the F1 score in a binary classification context.\n2. **Determine Positive Label**: The positive label is chosen as the second label in the sorted list of unique labels. This ensures consistency in handling different types of labels (e.g., numerical, string).\n3. **Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)**: Iterate through the true and predicted labels to count TP, FP, and FN based on the identified positive label.\n4. **Compute Precision and Recall**: Using the counts of TP, FP, and FN, calculate precision and recall. Handle division by zero by setting precision or recall to zero if their denominators are zero.\n5. **Calculate F1 Score**: Use the harmonic mean formula to combine precision and recall into the F1 score, ensuring to handle cases where both precision and recall are zero to avoid division by zero.\n\n### Solution Code\n```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    unique_labels = sorted(list(set(y_true) | set(y_pred)))\n    if not unique_labels:\n        return 0.0\n    \n    positive_label = unique_labels[-1] if len(unique_labels) > 1 else unique_labels[0]\n    \n    tp, fp, fn = 0, 0, 0\n    \n    for t, p in zip(y_true, y_pred):\n        if t == positive_label:\n            if p == positive_label:\n                tp += 1\n            else:\n                fn += 1\n        else:\n            if p == positive_label:\n                fp += 1\n                \n    precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n    \n    if (precision + recall) == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n    \n    return round(f1, 3)\n```\n\n### Explanation\n1. **Unique Labels Identification**: By taking the union of unique labels from both `y_true` and `y_pred`, we ensure all possible labels are considered. Sorting these labels helps in consistently choosing the positive label.\n2. **Positive Label Determination**: The positive label is chosen as the last label in the sorted list of unique labels. This approach works for binary classification scenarios, regardless of the label types.\n3. **Counting TP, FP, FN**: By iterating through each pair of true and predicted labels, we count how many times the model correctly predicted the positive class (TP), incorrectly predicted the positive class (FP), and missed predicting the positive class (FN).\n4. **Precision and Recall Calculation**: These metrics are computed using the counts of TP, FP, and FN. Special cases where denominators are zero are handled to avoid runtime errors.\n5. **F1 Score Calculation**: The harmonic mean of precision and recall is computed, ensuring robustness against division by zero by checking if both precision and recall are zero.\n\nThis approach efficiently handles binary classification tasks and ensures accurate computation of the F1 score, even with non-numeric labels."}
{"task_id": 92, "completion_id": 0, "completion": "To solve this problem, we need to forecast the power consumption for day 15 by accounting for both a linear trend and a known daily fluctuation. The steps involve detrending the data, performing linear regression, making a prediction, and adjusting for safety margins.\n\n### Approach\n1. **Detrend the Data**: Subtract the daily fluctuation from each data point using the formula \\( f(i) = 10 \\times \\sin(2\\pi i / 10) \\), where \\( i \\) is the day number (1 through 10).\n2. **Linear Regression**: Fit a linear regression model to the detrended data to capture the underlying linear trend.\n3. **Predict Base Consumption**: Use the linear regression model to predict the base consumption for day 15.\n4. **Adjust for Fluctuation**: Add back the fluctuation for day 15, which is calculated using the same formula but results in zero for day 15.\n5. **Safety Margin**: Round the predicted value and add a 5% safety margin, ensuring the result is an integer.\n\n### Solution Code\n```python\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Step 1: Subtract daily fluctuation\n    detrended = []\n    for i in range(1, 11):\n        fluctuation = 10 * math.sin(2 * math.pi * i / 10)\n        detrended_val = consumption_data[i-1] - fluctuation\n        detrended.append(detrended_val)\n    \n    # Step 2: Perform linear regression on detrended data\n    n = 10\n    sum_x = 55  # Sum of days 1 through 10\n    sum_y = sum(detrended)\n    sum_xy = 0\n    sum_x2 = 385  # Sum of squares of days 1 through 10\n    \n    for day in range(1, 11):\n        sum_xy += day * detrended[day - 1]\n    \n    numerator = n * sum_xy - sum_x * sum_y\n    denominator = n * sum_x2 - sum_x ** 2\n    m = numerator / denominator\n    b = (sum_y - m * sum_x) / n\n    \n    # Step 3: Predict day 15's base consumption\n    base_prediction = m * 15 + b\n    \n    # Step 4: Add day 15 fluctuation\n    day15_fluctuation = 10 * math.sin(2 * math.pi * 15 / 10)\n    total = base_prediction + day15_fluctuation\n    \n    # Step 5: Round to nearest integer\n    rounded_total = round(total)\n    \n    # Step 6: Add 5% safety margin and round up\n    safety_margin = math.ceil(rounded_total * 1.05)\n    \n    return safety_margin\n```\n\n### Explanation\n1. **Detrending**: Each daily power consumption value is adjusted by subtracting the sinusoidal fluctuation to isolate the linear trend.\n2. **Linear Regression**: The linear regression model is manually computed using sums of the day numbers and detrended values to find the slope (m) and intercept (b) of the best-fit line.\n3. **Prediction**: The linear model is used to predict the base consumption for day 15, which is then adjusted by adding the fluctuation (which is zero for day 15).\n4. **Safety Margin**: The predicted value is rounded to the nearest integer, and a 5% safety margin is added, ensuring the result is an integer to meet future energy needs conservatively.\n\nThis approach ensures that both the linear growth and daily fluctuations are accounted for, providing a robust forecast for future power consumption with a safety buffer."}
{"task_id": 93, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Mean Absolute Error (MAE) between two arrays of actual and predicted values. The MAE is a metric that measures the average magnitude of errors in a set of predictions, disregarding their direction. This is done by averaging the absolute differences between the actual and predicted values.\n\n### Approach\nThe approach to calculate the MAE involves the following steps:\n1. **Compute Element-wise Differences**: Subtract each predicted value from the corresponding actual value to get the error for each prediction.\n2. **Compute Absolute Errors**: Take the absolute value of each error to convert them into positive values, which represents the magnitude of each error.\n3. **Average the Absolute Errors**: Calculate the mean of these absolute errors to get the MAE.\n4. **Round the Result**: Round the resulting MAE to three decimal places to ensure the output is formatted as required.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    return round(np.mean(np.abs(y_true - y_pred)), 3)\n```\n\n### Explanation\n1. **Element-wise Differences**: The expression `y_true - y_pred` computes the difference between each actual value and its corresponding predicted value. This results in an array of errors where each element can be positive or negative.\n2. **Absolute Errors**: Applying `np.abs` to the differences converts all error values to their absolute magnitudes, ensuring we only consider the size of the errors, not their direction.\n3. **Mean of Absolute Errors**: `np.mean` is used to compute the average of these absolute errors. This gives us the MAE, which quantifies the average error magnitude across all predictions.\n4. **Rounding**: The result is rounded to three decimal places using Python's `round` function to meet the specified precision requirement.\n\nThis approach efficiently leverages NumPy's vectorized operations for performance and clarity, ensuring the solution is both concise and computationally efficient."}
{"task_id": 94, "completion_id": 0, "completion": "To solve this problem, we need to implement the multi-head attention mechanism, a key component of the Transformer model. This mechanism allows the model to focus on different parts of the input sequence simultaneously, enhancing its ability to capture various contextual relationships.\n\n### Approach\nThe multi-head attention mechanism can be broken down into three main steps:\n\n1. **Compute Query, Key, and Value Matrices (Q, K, V):** These matrices are derived from the input using linear transformations. Each matrix is obtained by multiplying the input with respective weight matrices (W_q, W_k, W_v).\n\n2. **Split into Multiple Heads:** The Q, K, and V matrices are split into multiple heads along the feature dimension. This allows each head to attend to different parts of the input sequence.\n\n3. **Self-Attention per Head:** For each head, the scaled dot-product attention is computed. This involves calculating attention scores, applying softmax to get attention weights, and then using these weights to compute the weighted sum of the Value matrix (V).\n\n4. **Concatenate Heads and Output:** The outputs from all heads are concatenated along the feature dimension to form the final output.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    max_scores = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - max_scores)\n    sum_exp = np.sum(exp_scores, axis=-1, keepdims=True)\n    attn_weights = exp_scores / sum_exp\n    output = np.dot(attn_weights, V)\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    seq_len, d_model = Q.shape\n    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n    d_k = d_model // n_heads\n    \n    Q_split = Q.reshape(seq_len, n_heads, d_k).transpose(1, 0, 2)\n    K_split = K.reshape(seq_len, n_heads, d_k).transpose(1, 0, 2)\n    V_split = V.reshape(seq_len, n_heads, d_k).transpose(1, 0, 2)\n    \n    heads_output = []\n    for i in range(n_heads):\n        q = Q_split[i]\n        k = K_split[i]\n        v = V_split[i]\n        head_out = self_attention(q, k, v)\n        heads_output.append(head_out)\n    \n    output = np.concatenate(heads_output, axis=1)\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\n```\n\n### Explanation\n1. **compute_qkv Function:** This function computes the Query (Q), Key (K), and Value (V) matrices by applying linear transformations to the input X using the respective weight matrices W_q, W_k, and W_v.\n\n2. **self_attention Function:** This function computes the scaled dot-product attention for a single head. The attention scores are calculated by multiplying Q and K, scaled by the square root of the dimension of K. Softmax is applied to these scores to get the attention weights, which are then used to compute the weighted sum of the Value matrix V.\n\n3. **multi_head_attention Function:** This function splits the Q, K, and V matrices into multiple heads, processes each head using self-attention, and then concatenates the results. The final output is rounded to four decimal places and converted to a list for the required output format.\n\nThis approach ensures that the model can attend to different parts of the input sequence simultaneously, capturing diverse contextual relationships and improving the model's performance on various tasks."}
{"task_id": 95, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Phi coefficient, which measures the correlation between two binary variables. The Phi coefficient is equivalent to the Pearson correlation coefficient for binary data. The approach involves constructing a contingency table from the input lists and then applying the formula for the Phi coefficient.\n\n### Approach\n1. **Contingency Table Construction**: We first construct a 2x2 contingency table from the input lists. The table counts the occurrences of each combination of binary values (0s and 1s) in the two lists.\n2. **Calculate Numerator and Denominator**: Using the counts from the contingency table, we compute the numerator and denominator of the Phi coefficient formula. The numerator is the difference between the product of the diagonal elements and the product of the off-diagonal elements of the contingency table. The denominator is the square root of the product of the marginal sums of the table.\n3. **Handle Edge Cases**: If the denominator is zero (which happens when one of the variables is constant), we return 0.0 to avoid division by zero errors.\n4. **Return Result**: Finally, we return the computed Phi coefficient rounded to four decimal places.\n\n### Solution Code\n```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    a = b = c = d = 0\n    for xi, yi in zip(x, y):\n        if xi == 1:\n            if yi == 1:\n                a += 1\n            else:\n                b += 1\n        else:\n            if yi == 1:\n                c += 1\n            else:\n                d += 1\n    numerator = a * d - b * c\n    denominator = ((a + b) * (a + c) * (b + d) * (c + d)) ** 0.5\n    phi = numerator / denominator if denominator != 0 else 0.0\n    return round(phi, 4)\n```\n\n### Explanation\n1. **Contingency Table**: The variables `a`, `b`, `c`, and `d` are counts of the four possible combinations of binary values in the input lists. This is done using a loop that iterates through each pair of elements in the input lists.\n2. **Numerator Calculation**: The numerator `(a*d - b*c)` measures the difference between the product of the diagonal elements (both 1s and both 0s) and the product of the off-diagonal elements (1s in one list and 0s in the other).\n3. **Denominator Calculation**: The denominator is the square root of the product of the sums of the rows and columns of the contingency table. This normalizes the numerator to the range [-1, 1].\n4. **Edge Case Handling**: If the denominator is zero (indicating no variation in one or both variables), the Phi coefficient is undefined, and we return 0.0.\n5. **Rounding**: The result is rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the Phi coefficient using straightforward arithmetic operations and handles edge cases gracefully to ensure robustness."}
{"task_id": 96, "completion_id": 0, "completion": "To solve this problem, we need to implement the Hard Sigmoid activation function, which is a piecewise linear approximation of the standard sigmoid function. This function is computationally efficient as it avoids the use of exponential operations, making it suitable for scenarios where computational resources are limited.\n\n### Approach\nThe Hard Sigmoid function is defined using a linear transformation followed by a clipping operation. The key steps are:\n1. **Linear Transformation**: Compute the value of the input scaled by 0.2 and shifted by 0.5. This transforms the input into a range that approximates the standard sigmoid's output.\n2. **Clipping**: Ensure the result of the linear transformation lies within the interval [0, 1]. Values below 0 are clipped to 0, and values above 1 are clipped to 1. This step ensures the function's output remains within the valid range for an activation function.\n\n### Solution Code\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0.0, min(1.0, 0.2 * x + 0.5))\n```\n\n### Explanation\n1. **Linear Transformation**: The input `x` is scaled by 0.2 and then shifted by 0.5. This transformation adjusts the input to a range that approximates the standard sigmoid but using only linear operations.\n2. **Clipping**: The result of the linear transformation is clamped between 0 and 1 using the `max` and `min` functions. This ensures that the output is 0 for inputs less than -2.5, 1 for inputs greater than 2.5, and a linear ramp between these two values for inputs within the range [-2.5, 2.5].\n\nThis approach efficiently approximates the sigmoid function with minimal computational overhead, making it ideal for use in environments where performance is critical."}
{"task_id": 97, "completion_id": 0, "completion": "To solve this problem, we need to implement the ELU (Exponential Linear Unit) activation function. The ELU function is designed to mitigate the limitations of the ReLU function by providing negative outputs for negative inputs, which helps in reducing the vanishing gradient problem during neural network training.\n\n### Approach\nThe ELU activation function is defined as follows:\n- For positive input values (x > 0), the output is the input value itself.\n- For non-positive input values (x \u2264 0), the output is computed using the formula: `alpha * (exp(x) - 1)`, where `alpha` is a parameter that controls the negative output values.\n\nThe steps to implement the ELU function are:\n1. Check if the input value `x` is positive. If it is, return `x` directly.\n2. If `x` is non-positive, compute the value using the exponential formula mentioned above.\n3. Round the result to four decimal places to ensure precision and return it.\n\n### Solution Code\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to 4 decimal places.\n    \"\"\"\n    if x > 0:\n        value = x\n    else:\n        value = alpha * (math.exp(x) - 1)\n    return round(value, 4)\n```\n\n### Explanation\n- **Positive Input Handling**: When the input value `x` is positive, the ELU function behaves like the identity function, returning `x` directly. This helps in maintaining the linearity for positive values, which is beneficial for learning in neural networks.\n- **Non-Positive Input Handling**: For non-positive values of `x`, the function uses the exponential formula to compute the output. This ensures that the function outputs negative values when `x` is negative, which helps in introducing non-linearity and addressing the vanishing gradient problem.\n- **Rounding**: The result is rounded to four decimal places to ensure the output meets the precision requirements specified in the problem constraints.\n\nThis implementation efficiently handles both positive and non-positive inputs and ensures the result is precise up to four decimal places, making it suitable for use in various machine learning applications where the ELU activation function is required."}
{"task_id": 98, "completion_id": 0, "completion": "To solve this problem, we need to implement the PReLU (Parametric ReLU) activation function. This function is a variant of the ReLU activation function that introduces a learnable parameter for negative inputs. The goal is to compute the PReLU activation value for a given input value using the specified parameter for negative values.\n\n### Approach\nThe PReLU activation function is defined as follows:\n- For positive input values (x > 0), the function returns the input value itself.\n- For non-positive input values (x <= 0), the function returns the product of the input value and a learnable parameter (alpha).\n\nThe key steps to implement this function are:\n1. Check if the input value `x` is greater than zero.\n2. If `x` is positive, return `x` directly.\n3. If `x` is non-positive (zero or negative), return the product of `alpha` and `x`.\n\nThis approach ensures that the function efficiently computes the activation value using a simple conditional check.\n\n### Solution Code\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x > 0 else alpha * x\n```\n\n### Explanation\nThe provided code defines the `prelu` function which takes two parameters: `x` (the input value) and `alpha` (the slope parameter for negative values, defaulting to 0.25). The function uses a conditional expression to check if `x` is greater than zero. If true, it returns `x`; otherwise, it returns `alpha` multiplied by `x`. This implementation efficiently handles both positive and non-positive inputs, ensuring correct computation of the PReLU activation value according to the defined behavior."}
{"task_id": 99, "completion_id": 0, "completion": "To solve this problem, we need to implement the Softplus activation function, which is a smooth approximation of the ReLU function. The Softplus function is defined as log(1 + e^x). The challenge is to handle numerical stability issues for very large or very small values of x to prevent overflow or underflow.\n\n### Approach\nThe Softplus function can be numerically unstable when computed directly due to the exponential terms. To address this, we use a piecewise approach:\n1. For positive values of x, we compute the function as x + log(1 + exp(-x)). This avoids overflow by reformulating the original expression in a way that handles large positive values of x more gracefully.\n2. For non-positive values of x, we compute the function directly as log(1 + exp(x)). This avoids underflow issues for very negative values of x by leveraging the properties of logarithms and exponentials.\n\nAfter computing the result, we round it to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    if x > 0:\n        term = math.exp(-x)\n        result = x + math.log(1 + term)\n    else:\n        term = math.exp(x)\n        result = math.log(1 + term)\n    return round(result, 4)\n```\n\n### Explanation\n1. **Handling Positive Values (x > 0)**:\n   - For positive x, the term exp(-x) is computed. This term becomes very small as x increases, avoiding overflow.\n   - The result is computed as x + log(1 + exp(-x)). For large x, exp(-x) approaches zero, making the log term negligible, and the result approximates x, which is numerically stable.\n\n2. **Handling Non-Positive Values (x \u2264 0)**:\n   - For non-positive x, exp(x) is computed directly. This term becomes very small as x becomes more negative, avoiding overflow.\n   - The result is computed as log(1 + exp(x)). For very negative x, exp(x) approaches zero, making the log term approximate to exp(x), which is numerically stable.\n\n3. **Rounding**:\n   - The final result is rounded to four decimal places to meet the specified output constraints, ensuring precision and consistency in the returned values.\n\nThis approach ensures numerical stability across all ranges of x, efficiently handles edge cases, and provides results rounded to the required precision."}
{"task_id": 100, "completion_id": 0, "completion": "To solve this problem, we need to implement the Softsign activation function, which is a smooth function used in neural networks. The function is designed to take a float input and return a float output bounded between -1 and 1. The output is rounded to four decimal places for precision.\n\n### Approach\nThe Softsign activation function is defined by the formula:\n\\[ f(x) = \\frac{x}{1 + |x|} \\]\nwhere \\( |x| \\) is the absolute value of \\( x \\). This formula ensures that the output is bounded between -1 and 1. As \\( x \\) approaches positive or negative infinity, the denominator grows, causing the output to approach 1 or -1, respectively. \n\nTo implement this:\n1. Compute the absolute value of the input \\( x \\).\n2. Add 1 to this absolute value to form the denominator.\n3. Divide the original input \\( x \\) by this denominator.\n4. Round the result to four decimal places to ensure precision.\n\n### Solution Code\n\n```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\n```\n\n### Explanation\n- **Absolute Value Calculation**: The absolute value of \\( x \\) is computed to ensure the denominator is always positive, which helps in maintaining the correct sign of the output based on the input \\( x \\).\n- **Denominator Formation**: By adding 1 to the absolute value of \\( x \\), we ensure the denominator is always at least 1, avoiding division by zero.\n- **Division and Rounding**: Dividing \\( x \\) by the formed denominator gives the Softsign value, which is then rounded to four decimal places to meet the precision requirement.\n\nThis approach efficiently computes the Softsign activation function with the required precision and ensures the output is within the bounds of -1 and 1."}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Compute clipped surrogate terms\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    surrogate_terms = np.minimum(rhos * A, clipped_rhos * A)\n    surrogate_loss = np.sum(surrogate_terms)\n    \n    # Compute KL divergence between current policy (pi_theta) and reference policy (pi_theta_ref)\n    pi_theta = rhos * pi_theta_old\n    log_pi_theta = np.log(pi_theta)\n    log_pi_ref = np.log(pi_theta_ref)\n    kl_divergence = np.sum(pi_theta * (log_pi_theta - log_pi_ref))\n    \n    # Calculate the total objective value\n    total = surrogate_loss - beta * kl_divergence\n    return round(total.item(), 6)\n```"}
{"task_id": 102, "completion_id": 0, "completion": "To solve this problem, we need to implement the Swish activation function, which is defined as the product of the input value and its sigmoid. The Swish function is known for its superior performance in deep neural networks compared to ReLU. The key challenge is to compute the Swish value efficiently while ensuring numerical stability, especially for very large or very small input values.\n\n### Approach\n1. **Understanding the Swish Function**: The Swish activation function is given by \\( \\text{Swish}(x) = x \\cdot \\sigma(x) \\), where \\( \\sigma(x) \\) is the sigmoid function. The sigmoid function is defined as \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n2. **Numerical Stability**: Direct computation of the sigmoid for very large positive or negative values can lead to overflow or underflow. To handle this, we compute the sigmoid in a numerically stable manner:\n   - For non-negative \\( x \\), compute \\( \\sigma(x) \\) directly as \\( \\frac{1}{1 + e^{-x}} \\).\n   - For negative \\( x \\), compute \\( \\sigma(x) \\) as \\( \\frac{e^x}{1 + e^x} \\) to avoid overflow when \\( x \\) is very negative.\n3. **Rounding**: The result is rounded to four decimal places using Python's built-in `round` function.\n\n### Solution Code\n```python\nimport math\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal.\n    \"\"\"\n    if x >= 0:\n        sigmoid_x = 1.0 / (1.0 + math.exp(-x))\n    else:\n        exp_x = math.exp(x)\n        sigmoid_x = exp_x / (1.0 + exp_x)\n    swish_value = x * sigmoid_x\n    return round(swish_value, 4)\n```\n\n### Explanation\n1. **Sigmoid Calculation**: The sigmoid function is computed differently based on the sign of the input to ensure numerical stability. For non-negative inputs, we use the standard sigmoid formula. For negative inputs, we adjust the formula to use \\( e^x \\) directly, which avoids computing large positive exponents that can cause overflow.\n2. **Swish Value Calculation**: The Swish value is obtained by multiplying the input \\( x \\) with its sigmoid value. This product is then rounded to four decimal places to meet the specified precision requirement.\n3. **Handling Edge Cases**: The approach efficiently handles very large positive and negative values by avoiding direct computation of potentially overflow-inducing exponentials, ensuring the function works correctly across the entire range of input values."}
{"task_id": 103, "completion_id": 0, "completion": "To solve this problem, we need to implement the SELU (Scaled Exponential Linear Unit) activation function, which is a self-normalizing variant of the ELU activation function. The SELU function is designed to ensure numerical stability while scaling the outputs to maintain self-normalizing properties.\n\n### Approach\nThe SELU activation function is defined with two parameters, `alpha` and `scale`, which are provided. The function behaves differently based on whether the input value is positive or non-positive:\n1. For positive input values (`x > 0`), the output is the input scaled by the `scale` parameter.\n2. For non-positive input values (`x <= 0`), the output is computed using an exponential function scaled by both `alpha` and `scale` parameters to ensure numerical stability.\n\nThe key steps are:\n1. Check if the input value is positive.\n2. Compute the output using the respective formula based on the sign of the input.\n3. Ensure the result is rounded to the nearest 4th decimal place for consistency.\n\n### Solution Code\n\n```python\nimport math\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to 4 decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    return round(result, 4)\n```\n\n### Explanation\n- **Parameters**: The function uses predefined constants `alpha` (1.6732632423543772) and `scale` (1.0507009873554804) which are derived to ensure the self-normalizing property.\n- **Condition Check**: The input value `x` is checked to determine if it is positive. If positive, the output is simply scaled by `scale`.\n- **Exponential Handling**: For non-positive inputs, the function computes the exponential of `x`, adjusts it by subtracting 1, and then scales the result by both `alpha` and `scale` to maintain stability and normalization.\n- **Rounding**: The result is rounded to the nearest 4th decimal place to meet the output constraints, ensuring consistency and precision in the returned values.\n\nThis approach efficiently handles both positive and non-positive inputs, ensuring numerical stability and correct scaling as required by the SELU activation function."}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs, weights, and bias\n    z = np.dot(X, weights) + bias\n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-z))\n    # Convert probabilities to binary predictions using a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n    # Return the predictions as a Python list\n    return predictions.tolist()\n```"}
{"task_id": 105, "completion_id": 0, "completion": "To solve this problem, we need to implement a gradient descent-based training algorithm for Softmax regression. The goal is to compute the model parameters (coefficients) using Cross Entropy loss and return the optimized coefficients along with the loss values collected over each iteration.\n\n### Approach\n1. **Data Preprocessing**: Convert the class labels `y` into a one-hot encoded matrix. This allows us to compute the Cross Entropy loss effectively.\n2. **Initialization**: Initialize the parameter matrix `B` with zeros. The dimensions of `B` are determined by the number of classes (C) and the number of features (M).\n3. **Iterative Training**:\n   - **Softmax Calculation**: For each iteration, compute the scores using the current parameters, then apply the Softmax function to convert these scores into probabilities.\n   - **Loss Calculation**: Compute the Cross Entropy loss using the predicted probabilities and the true labels.\n   - **Gradient Descent**: Update the parameters using the computed gradient of the loss with respect to the parameters.\n4. **Rounding**: After each iteration, round the loss value to four decimal places. After all iterations, round the optimized parameters to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Determine the number of classes\n    classes = np.unique(y)\n    C = len(classes)\n    # Convert y to one-hot encoding\n    y_one_hot = np.eye(C)[y]\n    \n    N, M = X.shape\n    # Initialize parameters B (C x M)\n    B = np.zeros((C, M))\n    \n    losses = []\n    for _ in range(iterations):\n        # Compute scores\n        Z = X @ B.T  # Shape (N, C)\n        # Compute Softmax probabilities with numerical stability\n        Z_max = np.max(Z, axis=1, keepdims=True)\n        exp_Z = np.exp(Z - Z_max)\n        S = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n        \n        # Compute Cross Entropy loss\n        epsilon = 1e-15  # To avoid log(0)\n        loss = -np.mean(np.sum(y_one_hot * np.log(S + epsilon), axis=1))\n        losses.append(round(loss, 4))\n        \n        # Compute gradient\n        grad = (S - y_one_hot).T @ X / N\n        # Update parameters\n        B -= learning_rate * grad\n    \n    # Round the parameters to 4 decimal places and convert to list\n    B_rounded = np.round(B, 4)\n    return B_rounded.tolist(), losses\n```\n\n### Explanation\n1. **One-Hot Encoding**: Convert the class labels into a binary matrix where each row corresponds to a sample and each column corresponds to a class. This matrix is used to compute the loss accurately.\n2. **Parameter Initialization**: The parameter matrix `B` is initialized to zeros. This matrix will be updated iteratively during training.\n3. **Softmax Function**: The scores for each class are computed using matrix multiplication. The Softmax function is applied to these scores to obtain probabilities, ensuring numerical stability by subtracting the maximum score before exponentiation.\n4. **Cross Entropy Loss**: The loss is calculated by comparing the predicted probabilities with the true labels. A small epsilon value is added to avoid taking the logarithm of zero.\n5. **Gradient Calculation**: The gradient of the loss with respect to the parameters is computed using matrix operations. This gradient is used to update the parameters in the direction that minimizes the loss.\n6. **Iteration and Rounding**: The process is repeated for the specified number of iterations, with the loss value recorded each time. After training, the parameters are rounded to four decimal places and returned along with the loss values.\n\nThis approach efficiently trains a Softmax regression model using gradient descent, providing both the optimized parameters and the training loss history."}
{"task_id": 106, "completion_id": 0, "completion": "To solve this problem, we need to implement a logistic regression model using gradient descent optimization. The goal is to compute the model parameters (coefficients) that minimize the Binary Cross Entropy (BCE) loss and return these coefficients along with the loss values recorded at each iteration, both rounded to four decimal places.\n\n### Approach\n1. **Add Intercept Term**: To account for the bias in the logistic regression model, we add a column of ones to the feature matrix X.\n2. **Initialize Parameters**: The coefficients (theta) are initialized to zeros.\n3. **Iterative Optimization**: For each iteration, compute the predicted probabilities using the sigmoid function, calculate the BCE loss, compute the gradient of the loss with respect to the coefficients, and update the coefficients using gradient descent.\n4. **Numerical Stability**: Clamp the predicted probabilities to avoid numerical issues when computing the logarithm.\n5. **Record Loss**: After each iteration, record the loss value rounded to four decimal places.\n6. **Return Results**: After completing all iterations, return the optimized coefficients and the recorded loss values, both rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Add intercept column to X\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    m, n = X.shape\n    theta = np.zeros(n)\n    losses = []\n    \n    for _ in range(iterations):\n        z = X.dot(theta)\n        h = 1 / (1 + np.exp(-z))\n        h = np.clip(h, 1e-15, 1 - 1e-15)  # Avoid log(0)\n        loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n        losses.append(round(loss, 4))\n        gradient = (X.T.dot(h - y)) / m\n        theta -= learning_rate * gradient\n    \n    coef = np.round(theta, 4).tolist()\n    return (coef, losses)\n```\n\n### Explanation\n1. **Intercept Term**: The feature matrix X is augmented with a column of ones to include the intercept term in the model.\n2. **Sigmoid Function**: The sigmoid function is used to convert linear combinations (logits) into probabilities, which are clamped to avoid numerical instability.\n3. **Loss Calculation**: The BCE loss is computed using the predicted probabilities and true labels, then averaged over all samples.\n4. **Gradient Descent**: The gradient of the loss with respect to the coefficients is computed and used to update the coefficients iteratively.\n5. **Rounding**: Both the coefficients and loss values are rounded to four decimal places as specified, ensuring the results meet the required precision.\n\nThis approach efficiently trains the logistic regression model using gradient descent, ensuring numerical stability and precise rounding of results."}
{"task_id": 107, "completion_id": 0, "completion": "To solve this problem, we need to implement a masked self-attention mechanism, which is crucial in sequence modeling tasks like text generation. This mechanism allows each position in a sequence to attend only to previous positions, ensuring that predictions depend only on known outputs.\n\n### Approach\nThe masked self-attention mechanism involves several key steps:\n1. **Compute Scaled Dot-Product Scores**: Calculate the dot product of the query (Q) and key (K) matrices, then scale these scores by the square root of the dimension of the key vectors (d_k). This scaling helps stabilize gradients during training.\n2. **Apply Mask**: Add a mask to the scaled scores. The mask typically contains large negative values (e.g., -1e9) in positions that should be ignored, ensuring these positions have near-zero influence after the softmax operation.\n3. **Compute Softmax**: Apply the softmax function to the masked scores to obtain attention weights. This step converts the scores into probabilities that sum to 1 for each position.\n4. **Compute Output**: Multiply the attention weights with the value (V) matrix to produce the final output of the attention mechanism.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = K.shape[1]\n    # Compute scaled dot-product scores\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    # Apply the mask\n    scores += mask\n    # Compute softmax for attention weights\n    max_scores = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - max_scores)\n    sum_exp = np.sum(exp_scores, axis=-1, keepdims=True)\n    attention_weights = exp_scores / sum_exp\n    # Compute the output\n    output = np.dot(attention_weights, V)\n    return output.tolist()\n```\n\n### Explanation\n1. **Scaled Dot-Product Scores**: The dot product of Q and K matrices is scaled by the square root of the key dimension (d_k) to prevent the gradients from becoming too small.\n2. **Mask Application**: The mask is added to the scaled scores, effectively setting the scores of masked positions to a large negative value, which ensures these positions are ignored in the subsequent softmax step.\n3. **Softmax Calculation**: The softmax function is applied row-wise to convert scores into probabilities. Subtracting the maximum score before exponentiation enhances numerical stability.\n4. **Output Calculation**: The resulting attention weights are multiplied by the value matrix (V) to produce the final output, which is then converted to a Python list using `tolist()` for compatibility with non-numpy environments.\n\nThis approach efficiently computes the attention while ensuring that each position only attends to allowed positions, which is critical for tasks like autoregressive text generation."}
{"task_id": 108, "completion_id": 0, "completion": "To solve this problem, we need to measure the disorder in a basket of apples based on their colors. The disorder should be 0 when all apples are the same color and increase as the variety of colors increases. We can use the concept of entropy from information theory to quantify this disorder, as entropy effectively measures the uncertainty or diversity in a distribution.\n\n### Approach\n1. **Entropy Calculation**: Entropy is a measure of uncertainty and is calculated using the formula: \\( \\text{Entropy} = -\\sum (p_i \\times \\log_2(p_i)) \\), where \\( p_i \\) is the probability of each color in the basket. Higher entropy indicates higher disorder.\n2. **Frequency Count**: Count the occurrences of each color in the basket.\n3. **Probability Calculation**: For each color, compute its probability by dividing its count by the total number of apples.\n4. **Summing Entropy**: Calculate the entropy by summing the product of each color's probability and the logarithm of the inverse of its probability.\n\n### Solution Code\n```python\nfrom collections import Counter\nimport math\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    counts = Counter(apples)\n    total = len(apples)\n    entropy = 0.0\n    for cnt in counts.values():\n        p = cnt / total\n        entropy += p * math.log2(1 / p)\n    return round(entropy, 4)\n```\n\n### Explanation\n1. **Edge Case Handling**: If the basket is empty, the disorder is 0.0.\n2. **Counting Colors**: Using the `Counter` from the `collections` module, we count the occurrences of each color.\n3. **Probability Calculation**: For each color, compute its probability by dividing its count by the total number of apples.\n4. **Entropy Calculation**: For each color, compute the contribution to entropy using the formula \\( p \\times \\log_2(1/p) \\), then sum these contributions.\n5. **Rounding**: The result is rounded to four decimal places to meet the output constraints.\n\nThis approach ensures that the disorder measure adheres to the required properties, where more diversity in colors results in higher entropy, thus higher disorder."}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    mean = X.mean(axis=2, keepdims=True)\n    variance = X.var(axis=2, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    result = gamma * X_normalized + beta\n    return np.round(result, 5).tolist()\n```"}
{"task_id": 110, "completion_id": 0, "completion": "To solve this problem, we need to evaluate the quality of a machine translation using the METEOR score, which considers unigram matches, precision, recall, a weighted harmonic mean (F-mean), and a penalty for word order fragmentation.\n\n### Approach\n1. **Calculate Unigram Matches**: Determine the number of common words (unigrams) between the reference and candidate translations, considering the minimum occurrence of each word in both.\n2. **Compute Precision and Recall**: Precision is the ratio of matches to the number of words in the candidate translation. Recall is the ratio of matches to the number of words in the reference translation.\n3. **Calculate F-mean**: This is a weighted harmonic mean of precision and recall, using a parameter alpha to balance their contributions.\n4. **Determine Fragmentation Penalty**: This penalty accounts for the discontiguous sequences (chunks) of matched words in the candidate translation compared to the reference. The penalty is computed using the number of chunks, the number of matches, and parameters beta and gamma.\n5. **Final Score Calculation**: The final METEOR score is derived by adjusting the F-mean with the fragmentation penalty.\n\n### Solution Code\n```python\nfrom collections import Counter, defaultdict\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    \n    if not ref_words or not cand_words:\n        return 0.0\n    \n    ref_counts = Counter(ref_words)\n    cand_counts = Counter(cand_words)\n    \n    # Calculate the number of unigram matches\n    matches = 0\n    for word in cand_counts:\n        if word in ref_counts:\n            matches += min(ref_counts[word], cand_counts[word])\n    \n    if matches == 0:\n        return 0.0\n    \n    # Compute precision and recall\n    precision = matches / len(cand_words) if len(cand_words) > 0 else 0.0\n    recall = matches / len(ref_words) if len(ref_words) > 0 else 0.0\n    \n    # Calculate F-mean\n    denominator = alpha * precision + (1 - alpha) * recall\n    if denominator == 0:\n        f_mean = 0.0\n    else:\n        f_mean = (precision * recall) / denominator\n    \n    # Determine the number of chunks (contiguous matches in reference order)\n    ref_word_positions = defaultdict(list)\n    for idx, word in enumerate(ref_words):\n        ref_word_positions[word].append(idx)\n    \n    used_positions = set()\n    candidate_positions = []\n    for word in cand_words:\n        if word in ref_word_positions:\n            for pos in ref_word_positions[word]:\n                if pos not in used_positions:\n                    used_positions.add(pos)\n                    candidate_positions.append(pos)\n                    break  # Use the earliest available position\n    \n    # Calculate chunks based on candidate order's positions\n    chunks = 1\n    for i in range(1, len(candidate_positions)):\n        if candidate_positions[i] != candidate_positions[i-1] + 1:\n            chunks += 1\n    \n    # Compute penalty\n    penalty = gamma * ( (chunks / matches) ** beta ) if matches > 0 else 0.0\n    \n    # Calculate final score\n    score = f_mean * (1 - penalty)\n    score = max(0.0, score)  # Ensure score is not negative\n    \n    return round(score, 3)\n```\n\n### Explanation\n1. **Unigram Matches**: We count how many times each word in the candidate translation appears in the reference translation, taking the minimum count from both to avoid overcounting.\n2. **Precision and Recall**: These metrics measure how many of the candidate's words are relevant (precision) and how many relevant words are captured (recall).\n3. **F-mean**: This metric balances precision and recall using a weighted harmonic mean, controlled by the parameter alpha.\n4. **Chunks Calculation**: By aligning candidate words to their earliest possible positions in the reference, we track how many contiguous sequences (chunks) these positions form. More chunks mean more fragmentation.\n5. **Penalty and Final Score**: The penalty reduces the score based on fragmentation, with parameters beta and gamma controlling its severity. The final score is adjusted to ensure it is non-negative and rounded to three decimal places for readability.\n\nThis approach efficiently balances the trade-offs between precision, recall, and word order, providing a robust evaluation metric for machine translations."}
{"task_id": 111, "completion_id": 0, "completion": "To solve this problem, we need to compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events, their individual counts, and the total number of samples. PMI measures how much the actual joint occurrence of events differs from what we would expect by chance.\n\n### Approach\n1. **Understanding PMI Formula**: PMI is calculated using the logarithm of the ratio between the joint probability of two events and the product of their individual probabilities. The formula is:\n   \\[\n   PMI = \\log_2\\left(\\frac{P(x, y)}{P(x) \\cdot P(y)}\\right)\n   \\]\n   where \\(P(x, y)\\) is the joint probability, and \\(P(x)\\) and \\(P(y)\\) are the individual probabilities.\n\n2. **Conversion to Counts**: Using counts instead of probabilities, the formula can be rewritten as:\n   \\[\n   PMI = \\log_2\\left(\\frac{\\text{joint\\_counts} \\times \\text{total\\_samples}}{\\text{total\\_counts\\_x} \\times \\text{total\\_counts\\_y}}\\right)\n   \\]\n   This conversion allows us to use the given counts directly.\n\n3. **Implementation Steps**:\n   - Calculate the numerator as the product of the joint counts and the total samples.\n   - Calculate the denominator as the product of the individual counts of the two events.\n   - Compute the ratio of the numerator to the denominator.\n   - Take the base-2 logarithm of this ratio.\n   - Round the result to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    ratio = (joint_counts * total_samples) / (total_counts_x * total_counts_y)\n    pmi = np.log2(ratio)\n    return round(pmi, 3)\n```\n\n### Explanation\n- **Numerator Calculation**: The product of the joint occurrence count and the total number of samples gives the adjusted joint count considering the total sample size.\n- **Denominator Calculation**: The product of the individual counts of the two events gives the expected joint count if the events were independent.\n- **Ratio and Logarithm**: The ratio of the numerator to the denominator tells us how much more (or less) the actual joint count is compared to the expected. The logarithm (base 2) of this ratio gives the PMI, which is then rounded to three decimal places for the final result.\n\nThis approach efficiently converts the given counts into the required probabilities and computes the PMI in a straightforward manner using logarithmic functions, ensuring accuracy and adherence to the problem constraints."}
{"task_id": 112, "completion_id": 0, "completion": "To solve this problem, we need to perform Min-Max normalization on a list of integers. This process scales the values in the list to a range between 0 and 1, which is a common preprocessing step in machine learning to ensure that all features contribute equally to the model.\n\n### Approach\n1. **Identify Min and Max Values**: First, we determine the minimum and maximum values in the input list. These values are essential for scaling the data.\n2. **Handle Uniform Values**: If all values in the list are the same (i.e., the difference between the max and min values is zero), we return a list of zeros since there is no variation to scale.\n3. **Apply Normalization Formula**: For each element in the list, we apply the Min-Max normalization formula: \\((x_i - \\text{min}) / (\\text{max} - \\text{min})\\). This formula adjusts each value to lie within the range [0, 1].\n4. **Round Results**: Each normalized value is rounded to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\ndef min_max(x: list[int]) -> list[float]:\n    min_val = min(x)\n    max_val = max(x)\n    diff = max_val - min_val\n    if diff == 0:\n        return [0.0 for _ in x]\n    return [round((num - min_val) / diff, 4) for num in x]\n```\n\n### Explanation\n1. **Finding Min and Max**: The `min` and `max` functions are used to find the minimum and maximum values in the input list. These values define the range of the data.\n2. **Uniform Value Check**: If the difference between the max and min values (denominator) is zero, all values in the list are the same. In this case, each value is normalized to 0.0.\n3. **Normalization Calculation**: For each element in the list, the difference from the minimum value is computed and divided by the range (max - min). This scales the value to the interval [0, 1].\n4. **Rounding**: The result of each normalization is rounded to four decimal places using Python's `round` function to ensure the output meets the specified precision.\n\nThis approach efficiently handles both typical cases with varying values and edge cases where all values are identical, ensuring robust and correct normalization."}
{"task_id": 113, "completion_id": 0, "completion": "To solve this problem, we need to implement a residual block using NumPy that processes a 1D input array through two weight layers with ReLU activations, adds the original input via a shortcut connection, and applies a final ReLU activation. The residual block structure is designed to allow the input to bypass the main processing layers, which helps in training deeper neural networks by mitigating the vanishing gradient problem.\n\n### Approach\n1. **First Linear Layer**: The input array is processed through the first weight layer using matrix multiplication followed by a ReLU activation.\n2. **Second Linear Layer**: The output from the first ReLU activation is processed through the second weight layer using matrix multiplication followed by another ReLU activation.\n3. **Shortcut Connection**: The original input is added to the output from the second ReLU activation.\n4. **Final Activation**: A final ReLU activation is applied to the result of the shortcut connection to produce the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    # Process through first weight layer and apply ReLU\n    a1 = np.maximum(0, np.dot(x, w1))\n    # Process through second weight layer and apply ReLU\n    a2 = np.maximum(0, np.dot(a1, w2))\n    # Add the original input via shortcut connection\n    shortcut = a2 + x\n    # Apply final ReLU activation\n    output = np.maximum(0, shortcut)\n    # Round and convert to list\n    return np.round(output, 4).tolist()\n```\n\n### Explanation\n1. **First Linear Layer**: The input `x` is multiplied by the first weight matrix `w1` using `np.dot(x, w1)`. The result is then passed through a ReLU activation function using `np.maximum(0, ...)` to introduce non-linearity.\n2. **Second Linear Layer**: The output from the first ReLU activation `a1` is multiplied by the second weight matrix `w2` using `np.dot(a1, w2)`. This result is again passed through a ReLU activation function.\n3. **Shortcut Connection**: The original input `x` is added to the output from the second ReLU activation `a2`, creating a shortcut connection that helps in preserving the original information.\n4. **Final Activation**: The result of the shortcut connection is passed through a final ReLU activation to produce the output. This output is then rounded to four decimal places and converted to a list for the final result.\n\nThis approach ensures that the input can bypass the non-linear transformations if needed, facilitating easier gradient flow during training and improving the learning of deep networks."}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    return np.mean(x, axis=(0, 1))\n```"}
{"task_id": 115, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format (batch, channels, height, width). The function will normalize the input across the batch and spatial dimensions for each channel, then apply scale (gamma) and shift (beta) parameters, ensuring numerical stability with a provided epsilon value.\n\n### Approach\n1. **Compute Mean and Variance**: For each channel, compute the mean and variance across the batch and spatial dimensions (height and width). This is done by collapsing the dimensions except for the channel dimension.\n2. **Normalize the Input**: Subtract the computed mean from each element and divide by the square root of the variance plus epsilon to ensure numerical stability.\n3. **Apply Scale and Shift**: Multiply the normalized input by the gamma parameter (scale) and add the beta parameter (shift) to each channel.\n4. **Output Formatting**: Round the result to 4 decimal places and convert it to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Compute mean and variance for each channel across batch and spatial dimensions\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize the input\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape gamma and beta for broadcasting\n    gamma = gamma.reshape(1, -1, 1, 1)\n    beta = beta.reshape(1, -1, 1, 1)\n    \n    # Apply scale and shift\n    Y = gamma * X_normalized + beta\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(Y, 4).tolist()\n```\n\n### Explanation\n1. **Mean and Variance Calculation**: The mean and variance are computed for each channel by collapsing the batch, height, and width dimensions. This is efficiently done using NumPy's `mean` and `var` functions with the `axis` parameter set to (0, 2, 3) to target the batch and spatial dimensions. The `keepdims` parameter ensures the result retains the necessary dimensions for broadcasting.\n2. **Normalization**: The input is normalized by subtracting the mean and dividing by the square root of the variance plus epsilon. This step ensures the data has zero mean and unit variance, which helps in stabilizing the training process.\n3. **Scale and Shift**: The normalized data is scaled by the gamma parameter and shifted by the beta parameter. These parameters are reshaped to match the channel dimension, allowing proper broadcasting across the entire input tensor.\n4. **Output Formatting**: The final result is rounded to 4 decimal places and converted to a list using `tolist()` to meet the specified output constraints.\n\nThis approach efficiently leverages NumPy's vectorized operations to perform the necessary computations, ensuring both correctness and performance."}
{"task_id": 116, "completion_id": 0, "completion": "To solve this problem, we need to compute the derivative of a polynomial term of the form \\( c \\times x^n \\) at a given point \\( x \\). The derivative is calculated using the power rule from calculus, which states that the derivative of \\( x^n \\) is \\( n \\times x^{(n-1)} \\). By applying this rule, we can determine the derivative of the polynomial term and evaluate it at the given point \\( x \\).\n\n### Approach\n1. **Power Rule Application**: The power rule in calculus tells us that the derivative of \\( x^n \\) with respect to \\( x \\) is \\( n \\times x^{(n-1)} \\). For a polynomial term \\( c \\times x^n \\), the derivative is obtained by multiplying the coefficient \\( c \\) with the result of applying the power rule to \\( x^n \\).\n2. **Formula Derivation**: Using the power rule, the derivative of \\( c \\times x^n \\) is \\( c \\times n \\times x^{(n-1)} \\). This formula is valid for all real numbers \\( n \\) (including zero and fractional exponents) and all non-zero values of \\( x \\).\n3. **Rounding**: The result is rounded to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * (x ** (n - 1))\n    return round(derivative, 4)\n```\n\n### Explanation\n- **Power Rule Application**: The function first computes the derivative using the formula \\( c \\times n \\times x^{(n-1)} \\). This handles all cases, including when \\( n \\) is zero (resulting in the derivative being zero) and when \\( n \\) is a fractional exponent.\n- **Rounding**: The computed derivative value is then rounded to four decimal places using Python's built-in `round` function to ensure the output meets the specified precision requirement.\n\nThis approach efficiently computes the derivative for any valid input values of \\( c \\), \\( x \\), and \\( n \\), providing an accurate and precise result."}
{"task_id": 117, "completion_id": 0, "completion": "To solve this problem, we need to compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process. This process ensures that the resulting vectors are both orthogonal to each other and of unit length, which is crucial for various applications in linear algebra and machine learning.\n\n### Approach\nThe Gram-Schmidt process is a method to orthogonalize a set of vectors in an inner product space, typically Euclidean space. The steps involved in this process are as follows:\n\n1. **Initialization**: Start with an empty list to hold the orthonormal basis vectors.\n2. **Orthogonalization**: For each input vector, subtract its projection onto each of the previously computed orthonormal basis vectors. This step ensures that the vector is orthogonal to all previous vectors in the basis.\n3. **Normalization**: Compute the norm of the resulting orthogonal vector. If the norm is above a given tolerance (indicating the vector is not linearly dependent), normalize it to unit length and add it to the basis.\n4. **Rounding and Conversion**: After processing all vectors, round each basis vector to 4 decimal places and convert it to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    basis = []\n    for v in vectors:\n        current = np.array(v, dtype=float)\n        for u in basis:\n            proj = np.dot(current, u) * u\n            current -= proj\n        norm = np.linalg.norm(current)\n        if norm > tol:\n            e = current / norm\n            basis.append(e)\n    return [vec.round(4).tolist() for vec in basis]\n```\n\n### Explanation\n1. **Conversion to Numpy Arrays**: Each input vector is converted to a numpy array to facilitate efficient numerical operations.\n2. **Projection Subtraction**: For each vector, we subtract its projection onto each of the previously computed orthonormal vectors. This ensures orthogonality with respect to the existing basis vectors.\n3. **Norm Check and Normalization**: After orthogonalization, the norm of the resulting vector is checked. If it exceeds the tolerance, the vector is normalized to unit length and added to the basis. This step ensures that only linearly independent vectors are included in the basis.\n4. **Rounding and Conversion**: The final basis vectors are rounded to 4 decimal places for readability and converted to lists, as required by the problem constraints.\n\nThis approach efficiently handles linear dependencies and ensures numerical stability through the use of a tolerance parameter, making it robust for practical applications."}
{"task_id": 118, "completion_id": 0, "completion": "To solve this problem, we need to compute the cross product of two 3-dimensional vectors. The cross product of two vectors results in a third vector that is perpendicular to both of the original vectors, following the right-hand rule. This is a fundamental operation in various fields such as physics, engineering, and 3D graphics.\n\n### Approach\nThe cross product of two vectors \\( \\mathbf{a} = [a_1, a_2, a_3] \\) and \\( \\mathbf{b} = [b_1, b_2, b_3] \\) can be computed using the determinant of a matrix formed by the unit vectors \\( \\mathbf{i}, \\mathbf{j}, \\mathbf{k} \\) and the components of the input vectors. The formula for the cross product components is:\n- \\( \\mathbf{i} \\) component: \\( a_2 \\cdot b_3 - a_3 \\cdot b_2 \\)\n- \\( \\mathbf{j} \\) component: \\( a_3 \\cdot b_1 - a_1 \\cdot b_3 \\)\n- \\( \\mathbf{k} \\) component: \\( a_1 \\cdot b_2 - a_2 \\cdot b_1 \\)\n\nTo simplify the implementation, we can use the `numpy.cross` function, which directly computes the cross product of two vectors. After computing the cross product using this function, we round the result to 4 decimal places and convert it to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    product = np.cross(a, b)\n    rounded = np.round(product, decimals=4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Importing Numpy**: The `numpy` library is imported to utilize its `cross` function for computing the cross product efficiently.\n2. **Compute Cross Product**: Using `np.cross(a, b)`, we compute the cross product of the input vectors `a` and `b`.\n3. **Rounding**: The result from the cross product is rounded to 4 decimal places using `np.round` to ensure precision.\n4. **Conversion to List**: The rounded result, which is a numpy array, is converted to a list using the `tolist()` method to meet the output format requirements.\n\nThis approach efficiently leverages existing numerical libraries to perform the computation accurately and concisely, ensuring the result is formatted as specified."}
{"task_id": 119, "completion_id": 0, "completion": "To solve this problem, we need to solve a system of linear equations using Cramer's Rule. This method involves calculating determinants of matrices formed by replacing each column of the coefficient matrix with the constant vector. The solution vector is derived from these determinants.\n\n### Approach\n1. **Check Determinant**: First, compute the determinant of the coefficient matrix \\( A \\). If the determinant is zero (or very close to zero considering floating-point precision), the system does not have a unique solution, so we return -1.\n2. **Form Modified Matrices**: For each variable in the solution vector, replace the corresponding column in \\( A \\) with the constant vector \\( b \\) to form a new matrix.\n3. **Compute Determinants**: Calculate the determinant of each modified matrix.\n4. **Calculate Solutions**: Each solution component is the ratio of the determinant of the modified matrix to the determinant of the original matrix.\n5. **Round and Reshape**: Round each solution component to four decimal places, reshape the solution vector into a column vector, and convert it to a list for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    A = np.array(A)\n    b = np.array(b)\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0.0):\n        return -1\n    n = A.shape[0]\n    solution = []\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        xi = det_Ai / det_A\n        solution.append(xi)\n    solution_array = np.array(solution)\n    rounded_solution = np.round(solution_array, 4)\n    reshaped_solution = rounded_solution.reshape(-1, 1)\n    return reshaped_solution.tolist()\n```\n\n### Explanation\n1. **Determinant Check**: The determinant of matrix \\( A \\) is computed. If it is close to zero (using `np.isclose` for numerical stability), the function returns -1 as there is no unique solution.\n2. **Matrix Modification**: For each column in \\( A \\), a copy of \\( A \\) is made, and the column is replaced with \\( b \\). This forms matrices \\( A_i \\) for each variable \\( x_i \\).\n3. **Determinant Calculation**: The determinant of each \\( A_i \\) is computed, and each solution component \\( x_i \\) is found by dividing this determinant by the determinant of \\( A \\).\n4. **Rounding and Reshaping**: The solution components are rounded to four decimal places, reshaped into a column vector, and converted to a list using `tolist()` to meet the output requirements.\n\nThis approach ensures that we follow Cramer's Rule accurately, handle numerical stability issues, and format the output as specified."}
{"task_id": 120, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Bhattacharyya distance between two discrete probability distributions. The Bhattacharyya distance is a measure of similarity between two probability distributions and is derived from the Bhattacharyya coefficient, which measures the amount of overlap between the two distributions.\n\n### Approach\n1. **Input Validation**: Check if the input lists `p` and `q` are either empty or of different lengths. If so, return 0.0 as the distance.\n2. **Convert to Arrays**: Convert the input lists to NumPy arrays for efficient element-wise operations.\n3. **Element-wise Product**: Compute the element-wise product of the two arrays.\n4. **Square Root and Sum**: Take the square root of each element in the product array and sum these values to get the Bhattacharyya coefficient.\n5. **Calculate Distance**: Use the natural logarithm of the Bhattacharyya coefficient to compute the Bhattacharyya distance. If the coefficient is zero, the logarithm will be handled by NumPy to return infinity, which is a valid float in Python.\n6. **Round Result**: Round the computed distance to 4 decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or not q:\n        return 0.0\n    p_array = np.array(p)\n    q_array = np.array(q)\n    product = p_array * q_array\n    sum_bc = np.sum(np.sqrt(product))\n    distance = -np.log(sum_bc)\n    return round(distance, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input lists are valid (non-empty and of the same length). If not, it returns 0.0 immediately.\n2. **Conversion to Arrays**: The input lists are converted to NumPy arrays to leverage efficient array operations.\n3. **Element-wise Product**: Each element in the first distribution is multiplied by the corresponding element in the second distribution.\n4. **Square Root and Sum**: The square root of each product is computed and summed to get the Bhattacharyya coefficient. This coefficient measures the overlap between the two distributions.\n5. **Distance Calculation**: The natural logarithm of the Bhattacharyya coefficient is taken and negated to compute the distance. If the coefficient is zero, the logarithm results in negative infinity, which when negated gives positive infinity, a valid float in Python.\n6. **Rounding**: The result is rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently handles valid inputs and edge cases, ensuring accurate and reliable computation of the Bhattacharyya distance."}
{"task_id": 121, "completion_id": 0, "completion": "To solve this problem, we need to compute the element-wise sum of two vectors (lists) in Python. The function should return a new list containing the sums of corresponding elements if the vectors are of the same length. If the vectors have different lengths, the function should return -1.\n\n### Approach\n1. **Check Vector Lengths**: First, we verify if the two input vectors have the same length. If they do not, we return -1 immediately as the operation cannot be performed.\n2. **Element-wise Sum**: If the vectors are of the same length, we use the `zip` function to pair elements from both vectors and then compute their sums. This is done efficiently using a list comprehension.\n\n### Solution Code\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for x, y in zip(a, b)]\n```\n\n### Explanation\n1. **Length Check**: The function starts by checking if the lengths of the two input vectors `a` and `b` are equal. If they are not, it returns -1 to indicate an error.\n2. **Element-wise Addition**: If the lengths are equal, the `zip` function pairs elements from both vectors. The list comprehension then iterates over these pairs, adding each pair of elements together to form the resulting vector. This approach ensures that each element in the resulting vector is the sum of the corresponding elements from the input vectors.\n\nThis solution efficiently handles both valid and invalid cases, ensuring correctness and optimal performance."}
{"task_id": 122, "completion_id": 0, "completion": "To solve this problem, we need to implement the policy gradient estimator using the REINFORCE algorithm. The policy is parameterized by a 2D NumPy array `theta`, and the goal is to compute the average gradient of the log-policy multiplied by the return at each time step across all given episodes.\n\n### Approach\n1. **Compute Returns for Each Time Step**: For each episode, calculate the return (sum of rewards from the current time step to the end of the episode) for each time step.\n2. **Compute Gradient Contributions**: For each time step in each episode, compute the gradient of the log-policy with respect to the policy parameters `theta`. This gradient is derived using the softmax probabilities of the actions in the current state.\n3. **Sum and Average Gradients**: Accumulate the gradients for each episode, then compute the average gradient across all episodes.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]):\n    num_states, num_actions = theta.shape\n    total_gradient = np.zeros_like(theta)\n    num_episodes = len(episodes)\n    \n    if num_episodes == 0:\n        return np.zeros_like(theta).tolist()\n    \n    for episode in episodes:\n        # Extract rewards and compute returns for each step\n        rewards = [r for (s, a, r) in episode]\n        episode_length = len(rewards)\n        returns = [sum(rewards[t:]) for t in range(episode_length)]\n        \n        episode_gradient = np.zeros_like(theta)\n        \n        for t in range(episode_length):\n            s, a, _ = episode[t]\n            G_t = returns[t]\n            \n            # Compute softmax probabilities for state s\n            logits = theta[s, :]\n            max_logit = np.max(logits)\n            logits_shifted = logits - max_logit  # Numerical stability\n            exp_logits = np.exp(logits_shifted)\n            probs = exp_logits / np.sum(exp_logits)\n            \n            # Compute gradient of log-policy for action a\n            one_hot = np.zeros(num_actions)\n            one_hot[a] = 1\n            grad_log_pi = one_hot - probs\n            \n            # Multiply by G_t and add to the episode gradient\n            episode_gradient[s, :] += grad_log_pi * G_t\n        \n        total_gradient += episode_gradient\n    \n    # Average over all episodes and round\n    average_gradient = total_gradient / num_episodes\n    average_gradient_rounded = np.round(average_gradient, 4)\n    \n    return average_gradient_rounded.tolist()\n```\n\n### Explanation\n1. **Returns Calculation**: For each episode, we first compute the returns for each time step by summing the rewards from that time step to the end of the episode. This gives us the return values needed for each step.\n2. **Softmax Probabilities**: For each state encountered in the episode, we compute the softmax probabilities of the actions using the current policy parameters `theta`. This ensures numerical stability by shifting the logits before exponentiation.\n3. **Gradient Calculation**: The gradient of the log-policy for a given action in a state is computed using the difference between a one-hot encoded vector of the taken action and the softmax probabilities. This gradient is then scaled by the return value for that time step.\n4. **Accumulation and Averaging**: Gradients for each episode are accumulated, and the final result is obtained by averaging these accumulated gradients across all episodes. The result is rounded to four decimal places and converted to a list for the output.\n\nThis approach efficiently computes the policy gradient using the REINFORCE algorithm, ensuring numerical stability and correct handling of multiple episodes and time steps."}
{"task_id": 123, "completion_id": 0, "completion": "To solve this problem, we need to calculate the computational cost savings of a Mixture-of-Experts (MoE) layer compared to a dense layer. The key idea is to determine how much computational efficiency is gained by using the MoE layer, which activates only a subset of experts per input, as opposed to a dense layer that uses all parameters for every input.\n\n### Approach\n1. **Understanding FLOPs Calculation**:\n   - **Dense Layer**: The computational cost (FLOPs) of a dense layer is calculated as \\(2 \\times d_{in} \\times d_{out}\\), where \\(d_{in}\\) and \\(d_{out}\\) are the input and output dimensions respectively.\n   - **MoE Layer**: The MoE layer consists of multiple experts, each being a dense layer. However, only a subset of these experts (specified by \\(k_{active}\\)) are activated per input. The FLOPs for the MoE layer include the cost of the gating network and the active experts. The gating network's FLOPs are \\(2 \\times d_{in} \\times n_{experts}\\), and the active experts contribute \\(2 \\times k_{active} \\times d_{in} \\times d_{out}\\) FLOPs. Additionally, combining the outputs of the active experts adds \\(2 \\times k_{active} \\times d_{out}\\) FLOPs.\n\n2. **Savings Calculation**:\n   - The computational savings are determined by comparing the FLOPs of the dense layer to the FLOPs of the MoE layer. The savings percentage is calculated as \\(\\frac{\\text{Dense FLOPs} - \\text{MoE FLOPs}}{\\text{Dense FLOPs}} \\times 100\\).\n\n### Solution Code\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = 2 * d_in * d_out\n    gating_flops = 2 * d_in * n_experts\n    expert_flops = 2 * k_active * d_in * d_out\n    combine_flops = 2 * k_active * d_out\n    moe_flops = gating_flops + expert_flops + combine_flops\n    savings = (dense_flops - moe_flops) / dense_flops * 100\n    return round(savings, 1)\n```\n\n### Explanation\n- **Dense Layer FLOPs**: The dense layer's FLOPs are straightforward, calculated as \\(2 \\times d_{in} \\times d_{out}\\).\n- **MoE Layer FLOPs**: This includes three components:\n  - **Gating Network**: Determines which experts to activate, costing \\(2 \\times d_{in} \\times n_{experts}\\) FLOPs.\n  - **Active Experts**: Each active expert contributes \\(2 \\times d_{in} \\times d_{out}\\) FLOPs, multiplied by \\(k_{active}\\).\n  - **Combining Outputs**: Summing the weighted outputs of the active experts, costing \\(2 \\times k_{active} \\times d_{out}\\) FLOPs.\n- **Savings Calculation**: The savings percentage is derived by comparing the total FLOPs of the dense layer to the MoE layer. If the result is negative, it indicates that the MoE layer is more computationally expensive, which can happen if many experts are activated.\n\nThis approach provides a clear and detailed comparison of computational efficiency between the MoE and dense layers, considering all components involved in the MoE layer's computation."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Compute clean logits and noise logits\n    clean_logits = X @ W_g.T\n    noise_contrib = (X @ W_noise.T) * N\n    H = clean_logits + noise_contrib\n    \n    # Determine the top-k indices for each row\n    top_k_indices = np.argpartition(H, -k, axis=1)[:, -k:]\n    batch_indices = np.arange(H.shape[0])[:, np.newaxis]\n    \n    # Create a mask for top-k elements\n    mask = np.zeros_like(H, dtype=bool)\n    mask[batch_indices, top_k_indices] = True\n    \n    # Apply mask: set non-top-k elements to -infinity\n    masked_H = np.where(mask, H, -np.inf)\n    \n    # Compute softmax probabilities\n    shifted = masked_H - np.max(masked_H, axis=1, keepdims=True)\n    exp_H = np.exp(shifted)\n    probabilities = exp_H / exp_H.sum(axis=1, keepdims=True)\n    \n    # Round and convert to list\n    rounded_prob = np.round(probabilities, 4)\n    return rounded_prob.tolist()\n```"}
{"task_id": 125, "completion_id": 0, "completion": "To solve this problem, we need to implement a Sparse Mixture of Experts (MoE) layer using softmax gating and top-k routing. This layer will select the top-k experts for each input token, apply their transformations, and aggregate the results weighted by normalized gating probabilities.\n\n### Approach\n1. **Compute Gate Logits and Probabilities**: The input tensor is multiplied by the gating weight matrix to get gate logits. These logits are then converted to probabilities using the softmax function, ensuring numerical stability by subtracting the maximum logit before exponentiation.\n2. **Select Top-k Experts**: For each input token, the top-k experts are selected based on the highest probabilities from the softmax output.\n3. **Normalize Probabilities**: The probabilities of the selected top-k experts are normalized so that their sum is 1.\n4. **Compute Contributions**: For each selected expert, the input tensor is transformed using the expert's weight matrix. The contributions from each expert are weighted by their normalized probabilities and summed to produce the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Compute gate logits and probabilities\n    gate_logits = x.dot(Wg)\n    max_logits = np.max(gate_logits, axis=1, keepdims=True)\n    exp_logits = np.exp(gate_logits - max_logits)\n    gate_probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n    \n    # Get top-k expert indices and their probabilities\n    sorted_indices = np.argsort(-gate_probs, axis=1)\n    top_k_indices = sorted_indices[:, :top_k]\n    \n    batch_size = x.shape[0]\n    batch_idx = np.arange(batch_size)[:, None]\n    top_k_values = gate_probs[batch_idx, top_k_indices]\n    \n    # Normalize the top-k probabilities\n    sum_topk = top_k_values.sum(axis=1, keepdims=True)\n    normalized_probs = top_k_values / sum_topk\n    \n    # Initialize output\n    output_dim = We.shape[2]\n    output = np.zeros((batch_size, output_dim))\n    \n    # Accumulate contributions from each top-k expert\n    for j in range(top_k):\n        expert_idx = top_k_indices[:, j]\n        expert_weights = We[expert_idx]\n        contrib = np.einsum('bi,bio->bo', x, expert_weights)\n        contrib *= normalized_probs[:, j, np.newaxis]\n        output += contrib\n    \n    # Round to 4 decimal places and convert to list\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\n```\n\n### Explanation\n1. **Gate Logits and Probabilities**: The input tensor `x` is multiplied by the gating weight matrix `Wg` to produce gate logits. These logits are converted to probabilities using the softmax function, ensuring numerical stability by subtracting the maximum logit before exponentiation.\n2. **Top-k Selection**: The top-k experts for each input token are identified by sorting the probabilities in descending order and selecting the indices of the top-k values.\n3. **Probability Normalization**: The probabilities of the selected top-k experts are normalized so that they sum to 1. This ensures that the contributions from these experts are appropriately weighted.\n4. **Expert Contributions**: For each of the top-k experts, the input tensor is transformed using the expert's weight matrix. The contributions from each expert are scaled by their normalized probabilities and summed to produce the final output. The results are rounded to four decimal places and converted to a list for the final output.\n\nThis approach efficiently combines the contributions of multiple experts, ensuring that the most relevant experts (based on the input) have the most influence on the output, while maintaining computational efficiency through vectorized operations."}
{"task_id": 126, "completion_id": 0, "completion": "To solve this problem, we need to implement Group Normalization on a 4D input tensor. Group Normalization is a technique that normalizes the activations of a neural network by dividing the channels into groups and normalizing each group separately. This helps in stabilizing the training process and improving model performance.\n\n### Approach\n1. **Reshape the Input Tensor**: The input tensor of shape (B, C, H, W) is reshaped into (B, num_groups, G, H, W), where G is the number of channels per group. This allows us to process each group separately.\n2. **Compute Mean and Variance**: For each group, compute the mean and variance over the channels and spatial dimensions (H, W). This gives us per-group statistics.\n3. **Normalize**: Subtract the mean and divide by the standard deviation (computed from the variance plus a small epsilon to avoid division by zero) for each group.\n4. **Reshape Back**: After normalization, reshape the tensor back to the original shape (B, C, H, W).\n5. **Apply Scale and Shift**: Multiply each channel by a learned scale (gamma) and add a learned shift (beta) to the normalized values.\n6. **Round and Convert**: Finally, round the results to the nearest 4th decimal place and convert the numpy array to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    B, C, H, W = X.shape\n    G = C // num_groups  # Each group has G channels\n    \n    # Reshape the input into groups\n    X_grouped = X.reshape((B, num_groups, G, H, W))\n    \n    # Calculate mean and variance for each group\n    mean = X_grouped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_grouped.var(axis=(2, 3, 4), keepdims=True)\n    \n    # Normalize the grouped data\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape back to original dimensions\n    X_normalized = X_normalized.reshape((B, C, H, W))\n    \n    # Apply gamma and beta\n    result = X_normalized * gamma[np.newaxis, :, np.newaxis, np.newaxis] + beta[np.newaxis, :, np.newaxis, np.newaxis]\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(result, 4).tolist()\n```\n\n### Explanation\n1. **Reshape the Input**: The input tensor is reshaped to group the channels into the specified number of groups. This allows us to handle each group independently.\n2. **Compute Statistics**: For each group, we compute the mean and variance over the channels and spatial dimensions. These statistics are used to normalize the activations within each group.\n3. **Normalization**: The activations are normalized by subtracting the mean and dividing by the standard deviation. This centers the activations around zero and scales them to unit variance.\n4. **Reshape Back**: After normalization, the tensor is reshaped back to its original dimensions to maintain consistency with the input shape.\n5. **Scale and Shift**: Learnable parameters gamma and beta are applied to each channel to scale and shift the normalized values, allowing the model to learn the optimal scaling and shifting during training.\n6. **Rounding and Conversion**: The final result is rounded to 4 decimal places for precision and converted to a Python list as specified.\n\nThis approach ensures that the input tensor is normalized efficiently in groups, leveraging numpy's capabilities for array manipulation and broadcasting, while maintaining numerical stability and precision."}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    x = start_x\n    for _ in range(max_iters):\n        gradient = 4 * x**3 - 9 * x**2\n        if abs(gradient) < tolerance:\n            # Check second derivative to confirm if it's a minimum\n            second_derivative = 12 * x**2 - 18 * x\n            if second_derivative > 0:\n                break\n            else:\n                # Perturb x to escape saddle point or maximum\n                x += learning_rate\n        else:\n            x -= learning_rate * gradient\n    return round(x, 4)\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    transformed = gamma * x + beta\n    tanh_values = np.tanh(transformed)\n    scaled = alpha * tanh_values\n    rounded = np.round(scaled, decimals=4)\n    return rounded.tolist()\n```"}
