{"task_id": 4, "completion_id": 0, "completion": "To solve this problem, we need to write a function that calculates the mean of a matrix either by row or by column, based on a specified mode. The function should handle both scenarios efficiently and return the appropriate means in a list.\n\n### Approach\n1. **Understanding the Problem**: The task is to compute the mean of each row or each column of a given matrix based on the specified mode ('row' or 'column'). The matrix is a list of lists of floats, and the mode determines whether we compute the mean for each row or each column.\n\n2. **Handling Rows**: For each row in the matrix, the mean is computed by summing all the elements in the row and dividing by the number of elements in that row. This straightforward approach works for each row individually.\n\n3. **Handling Columns**: For each column, since the number of columns might not be known beforehand, we can transpose the matrix. Transposing converts columns into rows, allowing us to compute the mean of each transposed row (which was originally a column) using the same method as for rows.\n\n4. **Efficiency and Edge Cases**: The solution efficiently handles both row and column calculations using list comprehensions and built-in functions. It assumes the matrix is valid and non-empty, though edge cases like empty rows or columns are handled gracefully.\n\n### Solution Code\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode"}
{"task_id": 5, "completion_id": 0, "completion": "To solve this problem, we need to write a function that multiplies each element of a given matrix by a scalar value and returns the resulting matrix. The function should handle various cases, including empty matrices and different types of elements (int or float).\n\n### Approach\n1. **Problem Analysis**: The task involves multiplying each element of a matrix by a scalar. This operation is straightforward and can be efficiently performed using nested list comprehensions.\n2. **Intuition**: For each row in the matrix, create a new row where each element is the product of the corresponding element in the original row and the scalar value. This can be achieved using list comprehensions within list comprehensions.\n3. **Algorithm Selection**: Using list comprehensions is efficient and concise for this problem. It allows us to iterate over each element of the matrix and apply the scalar multiplication in a clean manner.\n4. **Edge Cases**: The function should handle empty matrices gracefully, returning an empty matrix if the input is empty. It should also handle matrices with rows of varying lengths, though this is beyond the scope of the problem as each row is expected to have the same number of elements.\n\n### Solution Code\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    return [[element * scalar for element in row] for row in matrix]\n```\n\n### Explanation\n- **Matrix Multiplication**: The function uses list comprehensions to iterate over each row of the matrix and then over each element in the row. Each element is multiplied by the scalar value.\n- **Efficiency**: The approach is efficient with a time complexity of O(n*m), where n is the number of rows and m is the number of columns in the matrix. This is optimal for this problem as it processes each element exactly once.\n- **Type Handling**: The function handles both integers and floats, ensuring that the result maintains the appropriate numeric type for each element.\n\nThis solution is both concise and efficient, leveraging Python's list comprehensions for clarity and performance."}
{"task_id": 8, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that calculates the inverse of a 2x2 matrix if it exists. If the matrix is not invertible (i.e., its determinant is zero), the function should return `None`.\n\n### Approach\n1. **Extract Matrix Elements**: Extract the elements `a`, `b`, `c`, and `d` from the input matrix.\n2. **Calculate Determinant**: Compute the determinant of the matrix using the formula `det = a*d - b*c`.\n3. **Check Invertibility**: If the determinant is zero, the matrix is not invertible, so return `None`.\n4. **Compute Inverse Matrix**: If the determinant is non-zero, compute each element of the inverse matrix using the formula `1/det` multiplied by the submatrix [[d, -b], [-c, a]].\n5. **Return Inverse Matrix**: Return the computed inverse matrix.\n\n### Solution Code\n\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    a, b, c, d = matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv_a = d / determinant\n    inv_b = -b / determinant\n    inv_c = -c / determinant\n    inv_d = a / determinant\n    return [[inv_a, inv_b], [inv_c, inv_d]]\n```\n\n### Explanation\n1. **Extract Matrix Elements**: The elements `a`, `b`, `c`, and `d` are extracted from the input matrix. These represent the elements of the matrix [[a, b], [c, d]].\n2. **Calculate Determinant**: The determinant is calculated using the formula for a 2x2 matrix, which is `ad - bc`.\n3. **Check Invertibility**: If the determinant is zero, the matrix is singular and not invertible, so the function returns `None`.\n4. **Compute Inverse Matrix**: If the determinant is non-zero, the inverse matrix elements are calculated using the formula for the inverse of a 2x2 matrix, which involves dividing each element of the submatrix [[d, -b], [-c, a]] by the determinant.\n5. **Return Inverse Matrix**: The computed inverse matrix is returned as the result.\n\nThis approach ensures that the function efficiently and correctly computes the inverse of a 2x2 matrix or identifies that the matrix is not invertible."}
{"task_id": 22, "completion_id": 0, "completion": "To solve this problem, we need to compute the output of the sigmoid activation function given an input value \\( z \\). The sigmoid function is a fundamental function in machine learning and neural networks, often used in binary classification problems. The function is defined as \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\). \n\n### Approach\nThe sigmoid function can be computed using the formula \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\). The steps to compute this function are as follows:\n1. Compute the exponential of \\(-z\\), which is \\( e^{-z} \\).\n2. Add 1 to the result from step 1.\n3. Take the reciprocal of the result from step 2.\n4. Round the result to four decimal places.\n\nThis approach ensures that we accurately compute the sigmoid value and that the result is precise to four decimal places.\n\n### Solution Code\n\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    exp_z = math.exp(-z)\n    denominator = 1 + exp_z\n    result = 1 / denominator\n    return round(result, 4)\n```\n\n### Explanation\n1. **Import the Math Module**: The `math` module is imported to use the `exp` function, which computes the exponential of a number.\n2. **Compute Exponential of -z**: Using `math.exp(-z)`, we compute \\( e^{-z} \\) where \\( z \\) is the input.\n3. **Calculate Denominator**: Add 1 to the result of step 2 to get the denominator for the sigmoid function.\n4. **Compute Result**: Take the reciprocal of the denominator to get the sigmoid value.\n5. **Round Result**: The result is rounded to four decimal places using the `round` function to meet the problem's requirements.\n\nThis method efficiently computes the sigmoid function and ensures the result is precise and formatted correctly."}
{"task_id": 23, "completion_id": 0, "completion": "To solve this problem, we need to compute the softmax activation function for a given list of scores. The softmax function converts each score into a probability value between 0 and 1, which is useful in various machine learning applications for normalizing raw scores into probabilities.\n\n### Approach\nThe softmax function is defined as follows:\nFor each score \\( x_i \\) in the input list, the softmax value is computed as:\n\\[ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\]\nwhere \\( e \\) is the base of the natural logarithm, and the denominator is the sum of the exponentials of all scores.\n\nThe steps to compute the softmax function are as follows:\n1. Compute the exponential of each score in the input list.\n2. Sum all these exponential values to get the normalization denominator.\n3. Divide each exponential value by the sum to get the probability values.\n4. Round each probability value to four decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    if not scores:\n        return []\n    exponentials = [math.exp(score) for score in scores]\n    sum_exp = sum(exponentials)\n    probabilities = [exp / sum_exp for exp in exponentials]\n    rounded = [round(prob, 4) for prob in probabilities]\n    return rounded\n```\n\n### Explanation\n1. **Input Handling**: The function takes a list of float values as input. If the list is empty, the function returns an empty list.\n2. **Exponential Calculation**: Using the `math.exp` function, each score in the list is exponentiated.\n3. **Sum of Exponentials**: The sum of all exponentiated values is computed to serve as the normalization denominator.\n4. **Probability Calculation**: Each exponentiated value is divided by the sum to get the probability value for that score.\n5. **Rounding**: Each probability value is rounded to four decimal places using Python's `round` function.\n6. **Return**: The function returns the list of rounded probability values.\n\nThis approach ensures that the scores are converted to probabilities in a normalized manner, which is essential for many machine learning applications."}
{"task_id": 29, "completion_id": 0, "completion": "To solve this problem, we need to write a function that takes two numpy arrays, X and y, and returns them in a random shuffled order while maintaining the corresponding order between them. The function should have an optional seed parameter for reproducibility.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Create a List of Tuples**: We start by creating a list of tuples where each tuple contains the index, X value, and corresponding Y value. This list helps us keep track of the original indices of the elements.\n\n2. **Shuffle the List**: Using numpy's `shuffle` function, we shuffle the list of tuples. The `shuffle` function modifies the list in place, so after shuffling, the order of the tuples is randomized.\n\n3. **Extract New Values**: After shuffling, we extract the new X and Y values from the shuffled list. This ensures that the elements are in a random order while maintaining their original pairing.\n\n4. **Convert to Lists**: Finally, we convert the shuffled X and Y arrays into lists using `tolist()` and return them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    # Create a list of tuples containing indices, X, and Y values\n    data = list(zip(np.arange(len(X)), X, y))\n    \n    # Set the seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Shuffle the list to ensure randomness\n    np.random.shuffle(data)\n    \n    # Extract the new X and Y values\n    new_X = [item[1] for item in data]\n    new_y = [item[2] for item in data]\n    \n    # Convert to numpy arrays and then to lists\n    new_X = np.array(new_X).tolist()\n    new_y = np.array(new_y).tolist()\n    \n    # Return as a list of arrays\n    return [new_X, new_y]\n```\n\n### Explanation\n- **List of Tuples**: The list `data` is created by zipping together an array of indices and the corresponding X and Y values. This allows us to"}
{"task_id": 31, "completion_id": 0, "completion": "To solve this problem, we need to write a function that divides a given dataset into two subsets based on a specified feature and a threshold value. The function will return these subsets as two separate numpy arrays.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Iterate Through Each Sample**: Loop through each row of the dataset to check the value of the specified feature against the given threshold.\n2. **Condition Check**: For each row, determine if the feature's value is greater than or equal to the threshold. If it is, the row is added to the condition subset; otherwise, it goes to the other subset.\n3. **Convert to numpy Arrays**: After collecting all the rows that meet the condition, convert these rows into a numpy array. The same process is repeated for the rows that do not meet the condition.\n4. **Return the Result**: Return the two subsets as a list of two numpy arrays.\n\nThis approach ensures that we efficiently split the dataset using vectorized operations, which are both concise and efficient.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    condition_subset = X[np.where(X[:, feature_i] >= threshold)[0]]\n    other_subset = X[np.where(X[:, feature_i] < threshold)[0]]\n    return [condition_subset, other_subset]\n```\n\n### Explanation\n- **Condition Check**: The code uses boolean indexing to efficiently check each feature value against the threshold. The `np.where` function is used to find indices where the condition is true, and these indices are used to extract the corresponding rows from the dataset.\n- **Efficiency**: By leveraging numpy's vectorized operations, the solution efficiently handles the dataset, ensuring that the operations are performed in a concise and computationally efficient manner.\n- **Return Value**: The result is returned as a list of two numpy arrays, where each array represents one of the subsets. The first element of the list is the subset of rows where the feature meets or exceeds the threshold, and the second element is the subset where the feature is below the threshold.\n\nThis approach ensures clarity, efficiency, and conciseness in handling the dataset and performing the required operations."}
{"task_id": 35, "completion_id": 0, "completion": "To solve this problem, we need to convert a 1D numpy array into a diagonal matrix. A diagonal matrix is a square matrix where all the elements except those on the main diagonal are zero. The elements on the main diagonal are the elements of the input array, and the rest of the elements are zeros.\n\n### Approach\n1. **Understanding the Problem**: Given a 1D numpy array, we need to create a 2D numpy array where the diagonal elements are the elements of the input array, and all other elements are zero.\n2. **Using numpy's diag Function**: The numpy function `np.diag` can be used to extract the diagonal elements from the input array and create a 2D array where these elements form the diagonal. For example, if the input array is [1, 2, 3], `np.diag([1, 2, 3])` will return a 2D array with 1, 2, and 3 on the main diagonal.\n3. **Converting to List of Lists**: The result from `np.diag` is a 2D numpy array. To convert this into a list of lists (a Python list), we use the `tolist()` method.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    return np.array(np.diag(x)).tolist()\n```\n\n### Explanation\n- **Function Definition**: The function `make_diagonal` takes a 1D numpy array `x` as input.\n- **Extracting Diagonal Elements**: Using `np.diag(x)`, we get a 2D numpy array where the diagonal elements are the elements of `x`.\n- **Conversion to List**: The result from `np.diag(x)` is converted into a list of lists using `.tolist()`, which is then returned as the output.\n\nThis approach efficiently constructs the desired diagonal matrix using numpy functions, ensuring both correctness and performance."}
{"task_id": 36, "completion_id": 0, "completion": "To solve this problem, we need to calculate the accuracy score of a model's predictions. The accuracy score is a measure of how often the model's predictions match the true labels. This score is computed as the proportion of correct predictions out of the total number of predictions.\n\n### Approach\n1. **Problem Analysis**: The task is to compute the accuracy score given two 1D numpy arrays, `y_true` (true labels) and `y_pred` (predicted labels). The accuracy is calculated by comparing each element of `y_true` with the corresponding element in `y_pred` and determining the proportion of correct predictions.\n\n2. **Intuition**: Accuracy is the ratio of correct predictions to the total number of predictions. This can be efficiently computed using numpy operations, which are optimized for performance.\n\n3. **Algorithm Selection**: We use numpy to compare the two arrays for equality. The sum of these comparisons gives the count of correct predictions. Dividing this count by the total number of predictions (length of the arrays) gives the accuracy. The result is then rounded to four decimal places to meet the problem's requirements.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)\n```\n\n### Explanation\n1. **Comparison of Arrays**: The line `y_true == y_pred` creates a boolean array where each element is `True` if the corresponding elements in `y_true` and `y_pred` are equal, and `False` otherwise.\n\n2. **Count of Correct Predictions**: The sum of this boolean array gives the number of correct predictions.\n\n3. **Total Predictions**: The length of the `y_true` array (or `y_pred`, since they are of the same length) gives the total number of predictions.\n\n4. **Accuracy Calculation**: The accuracy is the ratio of correct predictions to the total predictions. This ratio is then rounded to four decimal places using the `round` function"}
{"task_id": 42, "completion_id": 0, "completion": "To solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function. The ReLU function is a commonly used activation function in neural networks due to its simplicity and effectiveness in introducing non-linearity to the network.\n\n### Approach\nThe ReLU function is defined such that it returns the input value if it is greater than zero, and returns zero otherwise. This function helps in introducing non-linearity in the network by allowing the output to be zero for negative inputs, which helps in preventing the vanishing gradient problem in neural networks.\n\nThe approach to implement the ReLU function is straightforward:\n1. Check if the input value is greater than zero.\n2. If the input value is greater than zero, return the input value as it is.\n3. If the input value is zero or negative, return zero.\n\nThis approach ensures that the function efficiently computes the output in constant time, O(1).\n\n### Solution Code\n```python\ndef relu(z: float) -> float:\n    if z > 0:\n        return z\n    else:\n        return 0\n```\n\n### Explanation\nThe provided solution defines a function `relu` that takes a single float input `z` and returns the output of the ReLU activation function. The function checks if `z` is greater than zero. If it is, the function returns `z` as it is. If `z` is zero or negative, the function returns zero. This implementation efficiently handles all possible cases of the input value, ensuring correct computation of the ReLU function."}
{"task_id": 43, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that calculates the Ridge Regression loss function. The function takes a feature matrix, coefficient vector, true labels, and a regularization parameter as inputs and returns the loss value rounded to four decimal places.\n\n### Approach\n\n1. **Problem Analysis**: \n   - The goal is to compute the Ridge Regression loss, which combines the Mean Squared Error (MSE) and a regularization term to prevent overfitting.\n   - The Ridge Regression loss function is given by:\n     \\[\n     \\text{Loss} = \\sum{(y_{\\text{pred}} - y_{\\text{true}})^2} + \\alpha \\sum{w_i^2}\n     \\]\n     where \\( y_{\\text{pred}} \\) is the predicted value, \\( y_{\\text{true}} \\) is the true label, \\( w \\) is the coefficient vector, and \\( \\alpha \\) is the regularization parameter.\n\n2. **Intuition**:\n   - The first term, \\( \\sum{(y_{\\text{pred}} - y_{\\text{true}})^2} \\), represents the MSE, which measures the average squared difference between predicted and true values.\n   - The second term, \\( \\alpha \\sum{w_i^2} \\), is the regularization term that penalizes large values in the coefficients vector \\( w \\).\n\n3. **Algorithm**:\n   - Compute the predicted values using matrix multiplication of the feature matrix \\( X \\) and the coefficient vector \\( w \\).\n   - Calculate the squared errors between the predicted values and the true labels.\n   - Sum these squared errors to get the MSE contribution.\n   - Compute the regularization term by summing the squares of the coefficients and multiplying by the regularization parameter \\( \\alpha"}
{"task_id": 44, "completion_id": 0, "completion": "To solve this problem, we need to implement a function called `leaky_relu` that computes the Leaky Rectified Linear Unit (Leaky ReLU) activation function. This function is a variation of the standard ReLU (Rectified Linear Unit) function, which is commonly used in neural networks.\n\n### Approach\nThe Leaky ReLU function is defined as:\n- `max(alpha * z, z)` where `alpha` is a small positive slope (default value is 0.01).\n- This means that for negative values of `z`, the function returns `z` (i.e., it does not reset negative values to zero), and for positive values, it returns `alpha * z`.\n\nThe approach to solve this problem involves:\n1. Checking if the input `z` is less than or equal to zero. If it is, the function returns `z` as is.\n2. If `z` is positive, the function returns `alpha * z`.\n\nThis approach ensures that the function behaves correctly for both positive and negative inputs, with the slope applied only to positive values.\n\n### Solution Code\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    if z <= 0:\n        return z\n    else:\n        return alpha * z\n```\n\n### Explanation\n- The function `leaky_relu` takes two parameters: `z` (a float) and `alpha` (an optional float with a default value of 0.01).\n- The function first checks if `z` is less than or equal to zero. If true, it returns `z` as it is, since the Leaky ReLU function does not reset negative values to zero.\n- If `z` is positive, the function returns `alpha * z`, which applies the Leaky ReLU slope to the input value.\n\nThis implementation efficiently handles both positive and negative values of `z`, ensuring that the output is correct according to the Leaky ReLU activation function's definition."}
{"task_id": 45, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function called `kernel_function` that computes the linear kernel between two input vectors `x1` and `x2`. The linear kernel is defined as the dot product of these two vectors.\n\n### Approach\nThe linear kernel is a fundamental concept in machine learning, particularly in algorithms like Support Vector Machines (SVM). The dot product of two vectors is calculated by summing the products of their corresponding elements. This can be efficiently computed using numpy's `dot` function, which handles both 1D and 2D arrays.\n\nThe steps to solve this problem are as follows:\n1. **Import numpy**: This library is used to compute the dot product efficiently.\n2. **Define the function**: The function `kernel_function` takes two parameters, `x1` and `x2`, which are the input vectors.\n3. **Compute the dot product**: Use numpy's `dot` function to compute the dot product of `x1` and `x2` and return the result.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)\n```\n\n### Explanation\nThe function `kernel_function` starts by importing the numpy library, which is necessary for efficient computation of the dot product. The function then takes two arguments, `x1` and `x2`, representing the input vectors. The dot product of these vectors is computed using numpy's `dot` function, which is both efficient and concise. The result of this computation is returned as the output of the function.\n\nThis approach ensures that the function works efficiently even for larger vectors, leveraging numpy's optimized internal functions for performance. The `dot` function handles both 1D and 2D arrays, making it versatile for various applications in machine learning and data analysis."}
{"task_id": 64, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Gini Impurity for a set of class labels. Gini Impurity is a measure used in decision tree algorithms to assess the quality of a split. It helps in determining how well a given split can separate the data into classes.\n\n### Approach\nThe Gini Impurity is calculated using the formula:\n\\[ Gini = 1 - \\sum \\left( \\frac{p_i^2}{n} \\right) \\]\nwhere \\( p_i \\) is the proportion of samples in class \\( i \\) and \\( n \\) is the total number of samples.\n\n1. **Count Frequencies**: Use a dictionary to count the occurrences of each class label in the input list.\n2. **Calculate Proport"}
{"task_id": 73, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Dice Score between two binary classification arrays. The Dice Score is a measure of similarity between two sets and is particularly useful in tasks like image segmentation and binary classification. It ranges from 0 (no overlap) to 1 (perfect overlap).\n\n### Approach\n1. **Calculate Intersection**: The intersection of two arrays is the count of elements that are 1 in both arrays. This can be efficiently computed using the logical AND operation followed by summation.\n2. **Compute Sums**: Calculate the sum of elements in `y_true` and `y_pred`. These sums will be used to compute the denominator of the Dice Score formula.\n3. **Handle Edge Cases**: If the sum of `y_true` or `y_pred` is zero, the Dice Score is zero because there are no elements to compare. If both sums are zero,"}
{"task_id": 75, "completion_id": 0, "completion": "To solve this problem, we need to generate a confusion matrix for a binary classification problem. The confusion matrix provides a visual summary of the performance of a classification model, showing the number of correct and incorrect predictions made by the model.\n\n### Approach\nThe approach to generate the confusion matrix involves the following steps:\n\n1. **Initialization**: Create a 2x2 matrix initialized with zeros. This matrix will represent the counts of correct and incorrect predictions.\n2. **Iteration**: Loop through each pair of actual and predicted labels provided in the input data.\n3. **Update Matrix**: For each pair, determine if the prediction is correct or incorrect. If the prediction matches the actual label, increment the corresponding cell in the matrix. If not, increment the cell where the actual label is on the row and the predicted label is on the"}
{"task_id": 80, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that calculates the probability density function (PDF) of a normal distribution for a given value, mean, and standard deviation. The function should return the PDF value rounded to five decimal places.\n\n### Approach\nThe probability density function (PDF) of a normal distribution is given by the formula:\n\\[ f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{ -\\frac{(x - \\mu)^2}{2\\sigma^2} } \\]\nwhere:\n- \\( x \\) is the value at which the PDF is evaluated,\n- \\( \\mu \\) is the mean of the distribution,\n- \\( \\sigma \\) is the standard deviation of the distribution.\n\nThe steps to calculate the PDF are:\n1. Compute the exponent term, which is \\(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\).\n2. Calculate the exponential of the exponent term.\n3. Multiply the result by \\(\\frac{1}{\\sqrt{2\\pi}}\\).\n4. Round the result to five decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    exponent = (x - mean) ** 2 / (2 * (std_dev ** 2))\n    value = (1 / math.sqrt(2 * math.pi)) * math.exp(-exponent)\n    return round(value, 5)\n```\n\n### Explanation\nThe function `normal_pdf` calculates the PDF of a normal distribution using the given formula. Here's a breakdown of the steps:\n1. **Exponent Calculation**: The term \\((x - \\mu)^2 / (2 \\sigma^2)\\) is computed to determine the exponent part of the PDF formula.\n2. **Exponential Calculation**: The exponential of the computed exponent is taken using the `math.exp` function.\n3. **PDF Value**: The exponential value is multiplied by \\(1 / \\sqrt{2\\pi}\\) to get the PDF value.\n4. **Rounding**: The result is rounded to five decimal places using the `round` function to meet the problem requirements.\n\nThis approach ensures that the function accurately computes the PDF value for any given \\(x\\), mean, and standard deviation, handling both normal and degenerate distributions appropriately."}
{"task_id": 81, "completion_id": 0, "completion": "To solve this problem, we need to calculate the probability of observing exactly k events in a fixed interval using the Poisson distribution. The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space. It is characterized by the fact that the probability of an event occurring is constant and independent of the time since the last event.\n\n### Approach\nThe approach to solve this problem involves using the Poisson probability formula, which is given by:\n\n\\[ P(k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\]\n\nWhere:\n- \\( \\lambda \\) is the average rate (mean) of occurrences in the interval.\n- \\( k \\) is the number of occurrences we are interested in.\n- \\( e \\) is the base of the natural logarithm (approximately equal to 2.71828).\n- \\( k! \\) is the factorial of \\( k \\).\n\nThe steps to compute the probability are as follows:\n1. Compute \\( e^{-\\lambda} \\) using the exponential function.\n2. Compute \\( \\lambda^k \\) by raising \\( \\lambda \\) to the power of \\( k \\).\n3. Compute the factorial of \\( k \\) using the `math.factorial` function.\n4. Combine these values into the Poisson probability formula.\n5. Round the result to 5 decimal places to get the final probability.\n\n### Solution Code\n\n```python\nimport math"}
{"task_id": 82, "completion_id": 0, "completion": "To solve this problem, we need to calculate the contrast of a grayscale image using the difference between the maximum and minimum pixel values. This measure of contrast helps in understanding the intensity differences in an image, with higher contrast indicating more vibrant images and lower contrast indicating more uniform images.\n\n### Approach\n1. **Understand the Problem**: The task is to compute the contrast of a grayscale image, which is defined as the difference between the maximum and minimum pixel values in the image.\n2. **Input Handling**: The input is a 2D numpy array where each element represents a pixel value ranging from 0 (black) to 255 (white).\n3. **Compute Max and Min Values**: Using numpy functions, we can efficiently find the maximum and minimum pixel values in the image.\n4. **Calculate Contrast**: Subtract the minimum value from the maximum value to get the contrast.\n5. **Return the Result**: The computed contrast is returned as the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: The contrast value of the image.\n    \"\"\"\n    max_val = np.max(img)\n    min_val = np.min(img)\n    return max_val - min_val\n```\n\n### Explanation\n- **Importing numpy**: The numpy library is used for efficient array operations, including finding the maximum and minimum values.\n- **Function Definition**: The function `calculate_contrast` takes a 2D numpy array `img` as input.\n- **Finding Max and Min**: Using `np.max(img)` and `np.min(img)`, we determine the maximum and minimum pixel values in the image.\n- **Contrast Calculation**: The contrast is computed by subtracting the minimum value from the maximum value.\n- **Return Value**: The function returns the computed contrast value, which represents the intensity difference in the image.\n\nThis approach efficiently computes the contrast using numpy functions, making the solution both concise and performant."}
{"task_id": 83, "completion_id": 0, "completion": "To solve this problem, we need to write a function that calculates the dot product of two 1D NumPy arrays. The dot product is a fundamental operation in linear algebra, where each element of one array is multiplied by the corresponding element of another array, and the results are summed up.\n\n### Approach\nThe approach to solve this problem involves using NumPy's built-in function `np.dot()`, which efficiently computes the dot product of two arrays. The steps are as follows:\n\n1. **Import NumPy**: This is necessary to use the `np.dot()` function.\n2. **Define the Function**: Create a function that takes two 1D NumPy arrays as input parameters.\n3. **Compute the Dot Product**: Utilize `np.dot()` to compute the dot product of the two input arrays.\n4. **Return the Result**: The result of the dot product is returned as a single number.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    return np.dot(vec1, vec2)\n```\n\n### Explanation\nThe function `calculate_dot_product` takes two arguments, `vec1` and `vec2`, which are 1D NumPy arrays. It uses the `np.dot()` function to compute the dot product, which is efficient and handles the computation internally. The result is then returned as a single number, which is the dot product of the two input vectors.\n\nThis approach ensures that the function is both concise and efficient, leveraging NumPy's optimized operations for numerical computations."}
{"task_id": 86, "completion_id": 0, "completion": "To solve this problem, we need to determine whether a machine learning model is overfitting, underfitting, or performing well based on its training and test accuracy values. The function should"}
{"task_id": 93, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Mean Absolute Error (MAE) between two arrays of actual and predicted values. MAE is a measure of the average magnitude of errors in a set of predictions, without considering their direction. This problem requires us to implement a function that computes this metric efficiently using numpy.\n\n### Approach\n1. **Calculate Absolute Differences**: Compute the absolute difference between each corresponding element in the actual values (y_true) and predicted values (y_pred). This gives us an array of absolute errors.\n2. **Compute Mean of Absolute Errors**: Calculate the mean of the absolute errors array. This provides the average magnitude of the errors.\n3. **Round the Result**: Round the computed mean to three decimal places to get the final MAE value.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    absolute_diffs = np.abs(y_true - y_pred)\n    mae = np.mean(absolute_diffs)\n    return round(mae, 3)\n```\n\n### Explanation\n1. **Absolute Differences Calculation**: Using numpy's absolute value function `np.abs()`, we compute the absolute differences between each element of y_true and y_pred. This step ensures that each error is non-negative.\n2. **Mean Calculation**: The mean of these absolute differences is computed using numpy's `mean()` function. This gives us the average magnitude of the errors.\n3. **Rounding**: The result is rounded to three decimal places using Python's `round()` function to meet the problem's requirement for precision.\n\nThis approach efficiently computes the MAE using vectorized operations provided by numpy, ensuring both accuracy and performance."}
{"task_id": 96, "completion_id": 0, "completion": "To solve this problem, we need to implement the Hard Sigmoid activation function, which is an efficient approximation of the standard sigmoid function. The Hard Sigmoid function is designed to be computationally efficient by using clamping to ensure the output values are within the range [0, 1].\n\n### Approach\nThe Hard Sigmoid function is defined as follows:\n- For any input value \\( x \\), compute \\( 0.5 \\times (1 + x) \\).\n- If the result is less than 0, clamp it to 0.\n- If the result is greater than 1, clamp it to 1.\n- Otherwise, return the computed value.\n\nThis approach ensures that the output is always within the range [0, 1], which is the same as the standard sigmoid function but with a faster computation time.\n\n### Solution Code\n\n```python\ndef hard_sigmoid(x: float) -> float:\n    return max(0.0, min(1.0, 0.5 * (1 + x)))\n```\n\n### Explanation\n1. **Compute the Linear Transformation**: The input \\( x \\) is first transformed using the formula \\( 0.5 \\times (1 + x) \\). This step linearly scales the input into a range that is suitable for the standard sigmoid function.\n\n2. **Clamp the Result**: The result of the linear transformation is then clamped to ensure it lies within the range [0, 1]. This is done using Python's built-in `max` and `min` functions. If the result is less than 0, it is set to 0. If the result is more than 1, it is set to 1.\n\n3. **Return the Result**: The clamped value is returned as the output of the Hard Sigmoid function.\n\nThis implementation efficiently computes the output in constant time \\( O(1) \\) by avoiding any complex mathematical operations, making it very suitable for applications where computational efficiency is important."}
{"task_id": 98, "completion_id": 0, "completion": "To solve this problem, we need to implement the PReLU (Parametric ReLU) activation function, which is a variant of the ReLU activation function. The PReLU function introduces a learnable parameter for negative inputs, allowing it to scale these inputs differently than the standard ReLU function.\n\n### Approach\nThe PReLU function works as follows:\n1. For positive input values, the function returns the input value as is.\n2. For negative input values, the function scales the input by a parameter called `alpha`, which is a learnable slope.\n3. The function returns 0 for zero input values.\n\nThe approach involves checking each input value individually and applying the respective transformation based on whether the input is positive, negative, or zero. This can be efficiently done using a simple conditional check for each value.\n\n### Solution Code\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    if x > 0:\n        return x\n    else:\n        return x * alpha\n```\n\n### Explanation\n- The function `prelu` takes two arguments: `x` (the input value) and `alpha` (the slope parameter for negative inputs, with a default value of 0.25).\n- If `x` is positive, the function returns `x` directly.\n- If `x` is negative, the function returns `x` multiplied by `alpha`.\n- If `x` is zero, the function returns 0, which is the same as the standard ReLU function.\n\nThis implementation ensures that the function is efficient and handles all possible cases, including positive, negative, and zero inputs. The conditional check ensures that each input is processed correctly according to the PReLU activation rule."}
{"task_id": 100, "completion_id": 0, "completion": "To solve this problem, we need to implement the Softsign activation function, which is a smooth function that maps any real number to the interval [-1, 1]. The Softsign function is defined as \\( \\text{Softsign}(x) = \\frac{x}{1 + |x|} \\). The output of this function is always bounded between -1 and 1.\n\n### Approach\n1. **Understanding the Softsign Function**: The Softsign function takes an input \\( x \\) and returns \\( \\frac{x}{1 + |x|} \\). This ensures that the output is always within the range [-1, 1]. For positive values of \\( x \\), the function decreases towards 0 as \\( x \\) increases, and for negative values, it increases towards -1 as \\( x \\) becomes more negative.\n2. **Implementation Steps**:\n   - Compute the absolute value of \\( x \\).\n   - Add 1 to this absolute value.\n   - Divide \\( x \\) by this sum to get the Softsign value.\n   - Round the result to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\ndef softsign(x: float) -> float:\n    result = x / (1 + abs(x))\n    return round(result, 4)\n```\n\n### Explanation\n- **Step 1**: The function takes a float input \\( x \\).\n- **Step 2**: It calculates the absolute value of \\( x \\) to handle both positive and negative inputs uniformly.\n- **Step 3**: The absolute value is added to 1 to avoid division by zero and to ensure the denominator is always positive.\n- **Step 4**: The result of the division is computed, which gives the Softsign value.\n- **Step 5**: The result is rounded to four decimal places using Python's `round` function to meet the problem's output requirements.\n\nThis approach ensures that the function is efficient and correctly handles both positive and negative inputs, providing the desired bounded output."}
{"task_id": 102, "completion_id": 0, "completion": "To solve this problem, we need to implement the Swish activation function, which is a self-gated activation function that has shown superior performance compared to other activation functions like ReLU in deep neural networks. The Swish function is defined as \\( x \\times \\text{sigmoid}(x) \\), where \\(\\text{sigmoid}(x)\\) is the sigmoid function.\n\n### Approach\n1. **Understand the Swish Function**: The Swish function computes the product of the input value \\( x \\) and the sigmoid of \\( x \\). The sigmoid function is defined as \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n2. **Mathematical Computation**: For a given input \\( x \\), compute the sigmoid value using the formula and then multiply it by \\( x \\).\n3. **Edge Cases Handling**: Consider cases where \\( x \\) is very large or very small. However, the current implementation handles these cases correctly using Python's math library.\n4. **Rounding**: The result is rounded to the nearest 4th decimal place to meet the problem's output constraints.\n\n### Solution Code\n```python\nimport math\n\ndef swish(x: float) -> float:\n    sigmoid = 1 / (1 + math.exp(-x))\n    swish_val = x * sigmoid\n    return round(swish_val, 4)\n```\n\n### Explanation\n- **Importing Math Module**: The `math` module is imported to use the exponential function `exp` for computing the sigmoid value.\n- **Sigmoid Calculation**: For a given input \\( x \\), the sigmoid value is calculated as \\( 1 / (1 + e^{-x}) \\).\n- **Swish Value Calculation**: The Swish value is then computed by multiplying \\( x \\) with the sigmoid value.\n- **Rounding**: The result is rounded to 4 decimal places using Python's `round` function to meet the specified precision.\n\nThis approach ensures that the Swish function is implemented efficiently and correctly handles all edge cases, providing the desired output constraints."}
{"task_id": 104, "completion_id": 0, "completion": "To solve this problem, we need to implement a function for binary classification using logistic regression. The function will compute the class probabilities using the sigmoid function and return binary predictions based on a threshold of 0.5.\n\n### Approach\n1. **Compute Linear Combination**: First, we compute the linear combination of the input features and the model weights, then add the bias term.\n2. **Apply Sigmoid Function**: The linear combination is passed through the sigmoid function to convert it into probabilities between 0 and 1.\n3. **Threshold for Binary Prediction**: Convert these probabilities into binary predictions by checking if each probability is greater than or equal to 0.5. This gives us 1 if the probability is above 0.5 and 0 otherwise.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using logistic regression.\n    \n    Args:\n        X: Input feature matrix (shape: NxD)\n        weights: Model weights (shape: D)\n        bias: Model bias term\n    \n    Returns:\n        Binary predictions (0 or 1) as a list after converting the probabilities.\n    \"\"\"\n    z = X @ weights + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0"}
{"task_id": 114, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer. The function should take an input of shape (height, width, channels) and return a 1D array of shape (channels,), where each element is the average of all values in the corresponding feature map.\n\n### Approach\nThe approach involves computing the average of each feature map across all height and width dimensions. This is efficiently achieved using NumPy's built-in functions. Specifically, we can use the `mean` function with the `axis` parameter set to `(0, 1)`, which computes the mean across the first two dimensions (height and width) for each channel, resulting in a 1D array of length equal to the number of channels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    return x.mean(axis=(0, 1))\n```\n\n### Explanation\n1. **Input Shape**: The input is a 3D NumPy array with shape (height, width, channels). Here, `height` and `width` are the dimensions of each feature map, and `channels` represent the number of feature maps (or channels).\n2. **Compute Averages**: Using `x.mean(axis=(0, 1))`, we compute the average of all elements across the height and width dimensions for each channel. This results in a 1D array where each element is the average of a feature map.\n3. **Efficiency**: The use of NumPy's `mean` function with the specified axis ensures efficient computation, leveraging optimized C-based operations for performance.\n\nThis approach efficiently computes the Global Average Pooling by reducing the dimensionality of the feature maps while maintaining the number of channels, which is essential for further processing in convolutional neural networks."}
{"task_id": 116, "completion_id": 0, "completion": "To solve this problem, we need to compute the derivative of a polynomial term of the form `c * x^n` at a given point `x`. The derivative of this term is given by the power rule in calculus, which states that the derivative of `c * x^n` with respect to `x` is `c * n * x^(n-1)`. \n\n### Approach\nThe approach to solve this problem involves the following steps:\n1"}
{"task_id": 118, "completion_id": 0, "completion": "To solve this problem, we need to compute the cross product of two 3-dimensional vectors. The cross product of two vectors results in a third vector that is perpendicular to both of the original vectors, following the right-hand rule. This concept is fundamental in physics, engineering, and 3D graphics.\n\n### Approach\nThe cross product of two vectors \\( a = [a_1, a_2, a_3] \\)"}
{"task_id": 121, "completion_id": 0, "completion": "To solve this problem, we need to write a function that computes the element-wise sum of two vectors. If the vectors have incompatible dimensions, the function should return -1.\n\n### Approach\n1. **Check Length Compatibility**: The first step is to check if the lengths of the two vectors are the same. If they are not, the function should return -1.\n2. **Element-wise Sum**: If the vectors have the same length, we create a new vector where each element is the sum of the corresponding elements from the input vectors.\n3. **Return Result**: The result is returned as a new list. If the vectors are empty, the result will also be an empty list.\n\n### Solution Code\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    if len(a) != len(b):\n        return -1\n    return [a[i] + b[i] for i in range(len(a))]\n```\n\n### Explanation\n- **Check Length Compatibility**: The function first checks if the lengths of the two input vectors `a` and `b` are different. If they are, it immediately returns -1.\n- **Element-wise Sum**: If the lengths are the same, the function uses a list comprehension to iterate over each index. For each index, it adds the corresponding elements from both vectors and collects these sums into a new list.\n- **Return Result**: The resulting list of sums is returned. This ensures that the function handles both compatible and incompatible vector dimensions correctly.\n\nThis approach efficiently handles the problem requirements and ensures that the solution is both correct and optimal."}
