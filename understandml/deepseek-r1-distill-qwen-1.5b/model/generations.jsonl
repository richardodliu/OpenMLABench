{"task_id": 16, "completion_id": 0, "completion": "To solve this problem, we need to generate all contiguous N-grams from a given sequence. An N-gram is a sliding window of length N taken from consecutive elements of the sequence. The sequence can be a list, tuple, or string, and each element (or character) is treated as an atomic token.\n\n### Approach\n1. **Check for Valid N**: If the given value of N is 0, we return an empty list, although the problem guarantees that N is at least 1.\n2. **Edge Case for Empty Sequence**: If the sequence is empty, we immediately return an empty list.\n3. **Iterate Through the Sequence**: For each starting index in the sequence, take a window of size N and convert it into a tuple. Continue this until the window can no longer be formed (i.e., when the starting index exceeds the length of the sequence minus N).\n4. **Return the Result**: Collect all the generated N-grams and return them as a list of tuples.\n\n### Solution Code\n```python\nfrom typing import Any, Iterable, List, Tuple\n\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    if N == 0:\n        return []\n    L = len(sequence)\n    if N > L:\n        return []\n    ngrams = []\n    for i in range(L - N + 1):\n        window = sequence[i:i + N]\n        ngrams.append(tuple(window))\n    return ngrams\n```\n\n### Explanation\n- **Check for Valid N**: The function first checks if N is 0 and returns an empty list if true. This avoids unnecessary processing and handles the edge case.\n- **Edge Case for Empty Sequence**: If the sequence is empty or N is greater than the length of the sequence, the function returns an empty list immediately.\n- **Iterate Through the Sequence**: The loop runs"}
{"task_id": 18, "completion_id": 0, "completion": "To solve this problem, we need to create a lightweight replacement for NumPy's built-in `zeros` constructor. The function should return a NumPy array filled with zeros, with the specified shape and optional data type.\n\n### Approach\n1. **Understand the Problem Requirements**: The function `zero` should accept a shape which can be an integer (for a 1-D array) or a tuple/list (for a multi-dimensional array). It should also accept an optional `dtype` argument, defaulting to `float` if not provided.\n2. **Use NumPy's `zeros` Function**: NumPy's `zeros` function can create an array filled with zeros. We can utilize this function to generate the desired array.\n3. **Handle Optional Parameters**: The function should extract the `dtype` from keyword arguments if provided, defaulting to `float` if not.\n4. **Return the Result**: Ensure the result is returned as a NumPy array.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n    \n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape. If an int is provided, a 1-D array of that length is created. If a tuple/list is provided, it specifies the dimensions of the resulting array.\n    **kwargs : dict, optional\n        Additional keyword arguments."}
{"task_id": 71, "completion_id": 0, "completion": "To solve this problem, we need to count how many times each class label appears in a tabular dataset. Each sample in the dataset is represented as a list, where the last element of each list is the class label. The goal is to return a dictionary where each key is a unique label and the corresponding value is the count of occurrences of that label.\n\n### Approach\n1. **Check for Empty Data**: If the input list is empty, return an empty dictionary immediately.\n2. **Initialize an Empty Dictionary**: This will hold the counts of each class label.\n3. **Iterate Through Each Sample**: For each sample in the dataset, extract the last element which represents the class label.\n4. **Count Occurrences**: Use a loop to go through each sample, check if the label is already in the dictionary, and update its count accordingly.\n5. **Return the Result**: After processing all samples, return the dictionary containing the counts.\n\n### Solution Code\n```python\ndef label_uniq_cnt(data: list[list]) -> dict:\n    if not data:\n        return {}\n    counts = {}\n    for sample in data:\n        label = sample[-1]\n        if label in counts:\n            counts[label] += 1\n        else:\n            counts[label] = 1\n    return counts\n```\n\n### Explanation\n- **Empty Data Check**: The function first checks if the input list `data` is empty. If it is, an empty dictionary is returned.\n- **Dictionary Initialization**: An empty dictionary `counts` is initialized to keep track of the occurrences of each label.\n- **Loop Through Samples**: For each sample in the list `data`, the last element is extracted to get the class label.\n- **Update Count**: Each label is checked against the `counts` dictionary. If it exists, the count is incremented; otherwise, a new entry is added with an initial count of 1.\n- **Return Result**: Finally, the dictionary `counts` is returned, which contains all unique labels and their respective counts.\n\nThis approach ensures that each label is processed exactly once, making the solution efficient with a time complexity of O(n), where n is the number of samples in the dataset."}
{"task_id": 87, "completion_id": 0, "completion": "To solve this problem, we need to determine the sizes of the input, hidden, and output layers in a feed-forward neural network. The hidden layer size is fixed at 10, while the input and output layer sizes are derived from the dimensions of the input and output data matrices provided.\n\n### Approach\n1. **Understand the Problem Requirements**: \n   - The input data matrix `X` is arranged column-wise, where each column represents a training example.\n   - The output data matrix `Y` is also arranged column-wise, where each column represents a sample's output.\n   - We need to return a tuple `(n_x, n_h, n_y)` where:\n     - `n_x` is the number of input features (rows of `X`).\n     - `n_h` is the fixed size of the hidden layer, which is 10.\n     - `n_y` is the number of output units (rows of `Y`).\n\n2. **Extract Dimensions from Input and Output Data**:\n   - The number of input features `n_x` is the number of rows in the input matrix `X`, which is obtained using `X.shape[0]`.\n   - The number of output units `n_y` is the number of rows in the output matrix `Y`, which is obtained using `Y.shape[0]`.\n\n3. **Construct the Result Tuple**:\n   - Using the extracted values, we construct the tuple with `n_x` as the first element, `n_h` as the second element, and `n_y` as the third element.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    n_x = X.shape[0]\n    n_y = Y.shape[0]\n    n_h = 10\n    return (n_x, n_h, n_y)\n```\n\n### Explanation\n- The function `layer_sizes` takes two NumPy arrays `X` and `Y` as input.\n- `X` is an input matrix where each column represents a training example, so the number of rows in `X` gives the number of input features, which is `n_x`.\n- `Y` is an output matrix where each column represents a sample's output, so the number of rows in `Y` gives the number of output units, which is `n_y`.\n- The hidden layer size `n_h` is fixed at 10 as per the problem requirements.\n- The function returns a tuple `(n_x, n_h, n_y)` which represents the sizes of the input, hidden, and output layers respectively.\n\nThis approach ensures that we correctly determine the necessary dimensions for the neural network layers based on the provided input and output data matrices."}
{"task_id": 91, "completion_id": 0, "completion": "To solve this problem, we need to compute the gradient of the ReLU (Rectified Linear Unit) activation function during backpropagation. The ReLU function is defined as \\( \\text{ReLU}(z) = \\max(0, z) \\), and its derivative is 1 if \\( z > 0 \\) and 0 otherwise. \n\n### Approach\n1. **Input Shape Check**: First, we need to ensure that the shapes of the input gradients (dA) and cached pre-activations (Z) are the same. If they are not, we return -1.\n2. **Convert to NumPy Arrays**: Convert the input lists into NumPy arrays for efficient"}
{"task_id": 92, "completion_id": 0, "completion": "To solve this problem, we need to determine whether a given input value is a numeric type, excluding boolean values. A numeric type includes integers, floats, complex numbers, fractions, and decimals. \n\n### Approach\nThe approach to solve this problem involves using the `numbers` module in Python, which provides a `numbers.Number` class that includes various numeric types such as int, float, complex, etc. However, this class also includes boolean values because in Python, `bool` is a subclass of `int`. Therefore, to exclude boolean values, we need to check the type of the input value explicitly.\n\nThe steps to solve the problem are as follows:\n1. Import the `numbers` module.\n2. Check if the input value is an instance of `numbers.Number`.\n3. Ensure that the input value is not a boolean type.\n4. Return `True` if both conditions are met, otherwise return `False`.\n\n### Solution Code\n```python\nimport numbers\n\ndef is_number(a) -> bool:\n    return isinstance(a, numbers.Number) and not isinstance(a, bool)\n```\n\n### Explanation\n- The function `is_number` takes an input `a`.\n- It first checks if `a` is an instance of `numbers.Number`, which includes various numeric types.\n- It then checks if `a` is not a boolean. This is done using `isinstance(a, bool)`, which returns `True` if `a` is a boolean.\n- The function returns `True` only if both conditions are satisfied, meaning `a` is a numeric type and not a boolean. Otherwise, it returns `False`.\n\nThis approach efficiently checks for the numeric type while excluding boolean values, ensuring accurate results for all edge cases."}
{"task_id": 94, "completion_id": 0, "completion": "To solve this problem, we need to determine the majority class label among the k nearest neighbors using the k-Nearest Neighbours (k-NN) algorithm. The function should return the label that appears most frequently, and in case of a tie, the smallest label according to Python\u2019s default ordering.\n\n### Approach\n1. **Count Occurrences**: Use the `Counter` class from the `collections` module to count the occurrences of each label in the list of neighbor targets.\n2. **Sort by Frequency and Lexicographic Order**: Convert the count dictionary into a list of tuples and sort this list primarily by the count in descending order. For labels with the same count, sort them in ascending order lexicographically.\n3. **Select the Majority Label**: The first element in the sorted list will be the label with the highest frequency, breaking ties by lexicographic order.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef knn_majority_vote(neighbors_targets: list[str | int]) -> str | int:\n    counts = Counter(neighbors_targets)\n    sorted_items = sorted(counts.items(), key=lambda x: (-x[1], x[0]))\n    return sorted_items[0][0]\n```\n\n### Explanation\n1. **Counting Occurrences**: The `Counter` class efficiently counts the occurrences of each label in the list of neighbor targets.\n2. **Sorting by Frequency and Lexicographic Order**: The list of tuples (label, count) is sorted first by the negative count (to get descending order) and then by the label itself (to handle ties lexicographically).\n3. **Returning the Result**: The first element in the sorted list is the label with the highest frequency, ensuring that in case of a tie, the smallest label is chosen.\n\nThis approach ensures that we efficiently and correctly determine the majority label, handling both numerical and string labels appropriately."}
{"task_id": 113, "completion_id": 0, "completion": "To solve this problem, we need to create a function that automates the process of running a trained model on multiple input examples and collecting the predicted outputs. The function will call another helper function for each example, process the output, and collect all predictions.\n\n### Approach\nThe function `run_examples` takes four arguments: a model callable, an input vocabulary mapping characters to indices, an inverse output vocabulary mapping indices to characters, and an iterable of input strings. The function processes each input string by calling the helper function `run_example`, which returns the predicted string. The predicted string is converted into a list of characters and added to a predictions list. The function then prints the input and output for each example and returns the list of predictions.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    if examples is None:\n        examples = globalConstant  # Assuming globalConstant is a predefined list of example strings\n    predictions = []\n    for example in examples:\n        output = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        predictions.append(''.join(output))\n        print(f\"Input: {example}, Output: {output}\")\n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The function checks if the examples argument is"}
{"task_id": 119, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs one step of Stochastic Gradient Descent (SGD) update. SGD is a popular optimization algorithm used to train artificial neural networks. The goal is to update each parameter by subtracting the gradient of the loss function multiplied by the learning rate.\n\n### Approach\n1. **Check for Empty or Mismatched Inputs**: The function first checks if the input lists for parameters (`params`) and gradients (`grads`) are empty or if their lengths differ. If any of these conditions are met, the function returns an empty list.\n2. **Update Parameters**: For each parameter and its corresponding gradient, compute the new parameter value by subtracting the product of the learning rate and the gradient from the current parameter.\n3. **Rounding**: Each updated parameter is rounded to four decimal places using numpy's `round` function.\n4. **Return Result**: The function returns a new list of parameters with the updated values, each converted to a Python list.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef sgd_update(params: list, grads: list, alpha: float = 0.01) -> list:\n    if len(params) == 0 or len(grads) == 0 or len(params) != len(grads):\n        return []\n    new_params = [np.round(p - alpha * g, 4) for p, g in zip(params, grads)]\n    return [list(p) for p in new_params]\n```\n\n### Explanation\n- **Input Handling**: The function begins by checking if the inputs are valid. If any of the conditions (empty lists, mismatched lengths) are true, it returns an empty list immediately.\n- **Parameter Update**: Using a list comprehension, the function iterates over each pair of parameter and gradient, computes the update using the formula `theta = theta - alpha * gradient`, and rounds the result to four decimal places.\n- **Output**: The updated parameters are converted to Python lists and returned as the result.\n\nThis approach ensures that the function efficiently updates the parameters in a single SGD step, handles edge cases appropriately, and returns the result in the required format."}
{"task_id": 129, "completion_id": 0, "completion": "To solve this problem, we need to create a function that mimics the behavior of an infinite sequence generator, providing a finite number of elements repeatedly. The function should return the first `samples` elements of this infinite sequence.\n\n### Approach\n1. **Problem Analysis**: The task is to cycle through a given finite sequence repeatedly and return the first `samples` elements. If the sequence is empty or `samples` is non-positive, the function should return an empty list.\n2. **Intuition**: Using Python's `itertools.cycle` function allows us to create an infinite loop over the given sequence. This function is efficient and handles the cyclic nature of the sequence.\n3. **Algorithm Selection**: We will use `itertools.cycle` to create an infinite iterator from the sequence. Then, we will use `islice` to extract the first `samples` elements from this iterator.\n4. **Edge Cases Handling**: The function checks if the sequence is empty or `samples` is non-positive and returns an empty list immediately to handle these edge cases.\n\n### Solution Code\n```python\nfrom itertools import cycle\n\ndef cycle_sequence(sequence: list, samples: int) -> list:\n    \"\"\"Return the first `samples` items from an infinite cycle over `sequence`.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing `samples` items collected by looping over\n              `sequence` repeatedly. If `sequence` is empty or `samples`\n              is not positive, an empty list is returned.\n    \"\"\"\n    if not sequence or samples <= 0:\n        return []\n    iterator = cycle(sequence)\n    return list(islice(iterator, samples))\n```\n\n### Explanation\n- **Handling Edge Cases**: The function first checks if the input sequence is empty or if `samples` is non-positive. If either condition is true, it returns an empty list immediately.\n- **Creating Infinite Cycle**: Using `itertools.cycle(sequence)`, we create an infinite iterator that cycles through the given sequence.\n- **Extracting Elements**: The `islice` function is used to extract the first `samples` elements from the infinite iterator. This ensures that we efficiently get the required elements without generating an infinite list.\n- **Returning the Result**: The result is converted to a list to match the expected return type and is returned.\n\nThis approach efficiently handles the problem requirements and edge cases, providing a clean and concise solution."}
{"task_id": 134, "completion_id": 0, "completion": "To solve this problem, we need to identify the optimal arm in a Bernoulli bandit. The goal is to find the arm with the highest expected reward and return both the maximum expected reward and the corresponding arm index. The solution must handle cases where the input list is empty or contains invalid probabilities, returning a specific tuple in those cases.\n\n### Approach\n1. **Check for Empty List**: If the input list is empty, return `(-1.0, -1)` as indicated by the problem constraints.\n2. **Validate Probabilities**: Ensure all probabilities in the list are within the valid range [0, 1]. If any probability is outside this range, return `(-1.0, -1)`.\n3. **Find Maximum Probability**: Determine the maximum probability value from the"}
{"task_id": 173, "completion_id": 0, "completion": "To solve this problem, we need to calculate the total alphabetical distance between two strings. The alphabetical distance is defined as the sum of the absolute differences of the ASCII values of corresponding characters in the two strings. If the strings are of different lengths, we return -1.\n\n### Approach\n1. **Check Lengths:** First, we check if the lengths of the two strings are equal. If they are not, we return -1 immediately.\n2. **Iterate Through Characters:** If the lengths are equal, we iterate through each pair of corresponding characters from both strings.\n3. **Calculate Alphabetical Distance:** For each pair of characters, we compute the absolute difference of their ASCII values and accumulate this difference to a total sum.\n4. **Return Result:** Finally, we return the accumulated total sum.\n\n### Solution Code\n```python\ndef alphabetical_distance(candidate: str, target: str) -> int:\n    if len(candidate) != len(target):\n        return -1\n    total = 0\n    for c1, c2 in zip(candidate, target):\n        total += abs(ord(c1) - ord(c2))\n    return total\n```\n\n### Explanation\n1. **Check Lengths:** The function first checks if the lengths of the candidate and target strings are the same. If they are not, it returns -1.\n2. **Iterate Through Characters:** Using a loop, the function iterates through each character pair from the candidate and target strings.\n3. **Calculate Alphabetical Distance:** For each character pair, it calculates the absolute difference between their ASCII values using `ord(c1) - ord(c2)` and accumulates these differences into a total sum.\n4. **Return Result:** The function returns the accumulated total sum after processing all corresponding character pairs.\n\nThis approach ensures that we efficiently compute the alphabetical distance by leveraging Python's built-in functions for handling strings and their character values."}
{"task_id": 199, "completion_id": 0, "completion": "To solve this problem, we need to split a one-dimensional feature vector `X` into two parts based on a given threshold value. The goal is to partition the label vector `y` into two sublists: `y_left` and `y_right`. `y_left` contains elements from `y` where their corresponding feature in `X` is strictly smaller than the threshold, while `y_right` contains elements where their corresponding feature is greater than or equal to the threshold. The function should return these sublists as regular Python lists.\n\n### Approach\n1. **Initialization**: Start by initializing two empty lists, `y_left` and `y_right`, to store the elements of `y` that meet the respective conditions.\n2. **Iterate through Elements**: Loop through each element of `y` using its indices. For each element, check if the corresponding value in `X` is strictly less than the threshold value.\n3. **Condition Check**: If the condition is met (i.e., `X[i] < value`), append the corresponding element from `y` to `y_left`. Otherwise, append it to `y_right`.\n4. **Return Result**: After processing all elements, return the tuples `(y_left, y_right)`.\n\nThis approach ensures that the elements are processed in their original order, and the result preserves the order of appearance from `y`.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    y_left = []\n    y_right = []\n    for i in range(len(y)):\n        if X[i] < value:\n            y_left.append(y[i])\n        else:\n            y_right.append(y[i])\n    return (y_left, y_right)\n```\n\n### Explanation\n- **Initialization**: Two lists, `y_left` and `y_right`, are created to store the results.\n- **Loop Through Elements**: Each element of `y` is checked against the corresponding element in `X`. The index is used to ensure the original order is preserved.\n- **Condition Check**: For each element, the condition `X[i] < value` is evaluated. If true, the element is added to `y_left`; otherwise, it is added to `y_right`.\n- **Return**: The function returns a tuple of the two lists, preserving the order of elements as they appeared in `y`.\n\nThis method efficiently processes each element exactly once, resulting in a time complexity of O(n), where n is the length of the vectors. This ensures the solution is optimal for large datasets."}
{"task_id": 205, "completion_id": 0, "completion": "To solve this problem, we need to extract unique environment IDs from a given registry, which is a list of Python objects. The goal is to return a list of unique IDs in the order they first appear in the registry.\n\n### Approach\n\n1. **Problem Analysis**:\n   - Each dictionary in the registry may contain an \"id\" key, which we need to extract.\n   - We must consider only dictionaries that contain the \"id\" key.\n   - If an \"id\" value appears more than once, we should retain only the first occurrence.\n   - The result should be a list of these unique IDs in the order they first appeared.\n\n2. **Intuition**:\n   - We can iterate through each item in the registry.\n   - For each item, check if it is a dictionary and if it contains the \"id\" key.\n   - If both conditions are met, add the \"id\" to a result list only if it hasn't been added before, using a set to track seen IDs for efficient lookup.\n\n3. **Algorithm Selection**:\n   - Use a list to store the result and a set to track seen IDs for O(1) average-time complexity lookups.\n   - Iterate through each item in the registry, checking for the conditions mentioned.\n\n4. **Complexity Analysis**:\n   - Time Complexity: O(n), where n is the number of items in the registry. Each item is checked once.\n   - Space Complexity: O(k), where k is the number of unique IDs, as we store each unique ID once.\n\n### Solution Code\n\n```python\ndef get_gym_environs(registry: list) -> list:\n    result = []\n    seen = set()\n    for item in registry:\n        if isinstance(item, dict) and 'id' in item:\n            id = item['id']\n            if id not in seen:\n                result.append(id)\n                seen.add(id)\n    return result\n```\n\n### Explanation\n\n1. **Initialization**:\n   - We initialize an empty list `result` to store the unique IDs in the order they first appear.\n   - We initialize an empty set `seen` to track which IDs have already been added to the result list for efficient lookups.\n\n2. **Iteration and Processing**:\n   - For each item in the registry, we check if it is a dictionary and contains the \"id\" key.\n   - If both conditions are met, we extract the \"id\" value.\n   - We check if this \"id\" has already been added to the `seen` set. If not, we add it to both the `result` list and the `seen` set.\n\n3. **Return Result**:\n   - After processing all items, the `result` list, which contains the unique IDs in the order of their first appearance, is returned.\n\nThis approach ensures that we efficiently extract unique IDs while preserving the order of their first occurrence, meeting the problem requirements."}
{"task_id": 231, "completion_id": 0, "completion": "To solve this problem, we need to determine if a given list of integers represents a natural blackjack in the game of Blackjack. A natural blackjack occurs when the hand contains exactly two cards: one"}
{"task_id": 269, "completion_id": 0, "completion": "To solve this problem, we need to calculate the total number of trainable parameters in a two-layer DQN (Deep Q-Network) architecture. The architecture consists of a fully-connected layer followed by another fully-connected layer, each contributing to the total number of parameters.\n\n### Approach\nThe problem can be broken down into understanding the structure of the DQN network:\n1. **First Layer**: This layer maps the input state vector (of length `state_dimensions`) to 164 hidden neurons. The number of trainable parameters in this layer is calculated as the product of the input dimension and the output dimension (164) plus the 164 biases.\n2. **Second Layer**: This layer maps the 164 hidden neurons to the number of possible actions (of length `action_dimensions`). The number of trainable parameters here is calculated as the product of the input dimension (164) and the output dimension (action_dimensions) plus the action_dimensions for biases.\n\nThe total number of trainable parameters is the sum of the parameters from both layers. \n\n### Solution Code\n```python\ndef count_trainable_params(state_dimensions: int, action_dimensions: int) -> int:\n    return 164 * (state_dimensions + 1) + 165 * action_dimensions\n```\n\n### Explanation\n1. **First Layer Calculation**: The first layer has `state_dimensions` input features and 164 hidden neurons. The number of parameters in this layer is given by `164 * (state_dimensions + 1)`. This is because each of the `state_dimensions` inputs connects to 164 neurons, and each neuron has a weight and a bias, leading to `state_dimensions * 164` weights and 164 biases.\n\n2. **Second Layer Calculation**: The second layer has 164 hidden neurons and `action_dimensions` output actions. The number of parameters here is given by `165 * action_dimensions`. This is because each of the 164 hidden neurons connects to `action_dimensions` actions, and each connection includes a weight and a bias, leading to `164 * action_dimensions` weights and `action_dimensions` biases.\n\nBy summing the parameters from both layers, we get the total number of trainable parameters in the network. This approach efficiently computes the result using mathematical formulas, ensuring optimal performance."}
{"task_id": 274, "completion_id": 0, "completion": "To solve this problem, we need to create a function that clips each element of a numeric vector to a specified interval. The interval is defined as [-5, 5], meaning any value smaller than -5 should be set to -5 and any value larger than 5 should be set to 5. The original order and dimensionality of the input data must be preserved.\n\n### Approach\n1. **Check Input Type**: The function first checks if the input is a list. If it is, the input is converted to a NumPy array to enable vectorized operations.\n2. **Vectorized Clipping**: Using NumPy's `clip` function, each element of the array is clipped to the specified interval [-5, 5]. This function is efficient for handling large arrays and ensures that all elements are within the desired range.\n3. **Convert Result to List**: The clipped NumPy array is then converted back to a Python list to match the required output format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef small_norm_clip(values):\n    if isinstance(values, list):\n        values = np.array(values)\n    clipped = np.clip(values, -5, 5)\n    return clipped.tolist()\n```\n\n### Explanation\n1. **Input Handling**: The function begins by checking if the input is a list. If it is, the list is converted to a NumPy array to leverage NumPy's efficient vector operations.\n2. **Clipping Operation**: The `np.clip` function is used to limit each element of the array to the interval [-5, 5]. This function is designed to handle element-wise operations efficiently.\n3. **Result Conversion**: The clipped NumPy array is converted to a Python list using `tolist()` to ensure the output matches the required format.\n\nThis approach ensures that the function works efficiently for both list and NumPy array inputs, providing a clean and concise solution to the problem."}
{"task_id": 277, "completion_id": 0, "completion": "To solve this problem, we need to determine if the \"action_space\" and \"observation_space\" keys in a given environment dictionary represent composite spaces. A composite space is defined as a built-in Python `tuple` or `dict`. The function should return a tuple containing two boolean values: one indicating whether the action space is composite and another indicating whether the observation space is composite.\n\n### Approach\n1. **Identify Composite Spaces**: For each of the two keys (\"action_space\" and \"observation_space\"), check if the corresponding value is an instance of either `tuple` or `dict`.\n2. **Return Results**: Return a tuple where the first element is `True` if the action space is a composite, and the second element is `True` if the observation space is a composite.\n\n### Solution Code\n```python\ndef is_tuple(env: dict) -> tuple:\n    action_space = env[\"action_space\"]\n    obs_space = env[\"observation_space\"]\n    tuple_action = isinstance(action_space, (tuple, dict))\n    tuple_obs = isinstance(obs_space, (tuple, dict))\n    return (tuple_action, tuple_obs)\n```\n\n### Explanation\n1. **Check for Composite Spaces**: The function uses `isinstance()` to check if the values associated with the keys \"action_space\" and \"observation_space\" are instances of `tuple` or `dict`.\n2. **Return Results**: The function returns a tuple where:\n   - The first element (`tuple_action`) is `True` if `env[\"action_space\"]` is a `tuple` or `dict`, otherwise `False`.\n   - The second element (`tuple_obs`) is `True` if `env[\"observation_space\"]` is a `tuple` or `dict`, otherwise `False`.\n\nThis approach ensures that we correctly identify composite spaces for both action and observation spaces, providing the required boolean results efficiently."}
{"task_id": 289, "completion_id": 0, "completion": "To solve this problem, we need to format a list of tuples into a single string where each tuple's field and value are separated by a colon and a space, and each formatted pair is on a new line. The formatted string should not end with an extra newline.\n\n### Approach\n1. **Problem Analysis**: Each tuple in the input list contains two elements: a field name and a value. Our task is to process each consecutive pair of tuples, converting them into lines formatted as \"field: value\".\n2. **Handling Edge Cases**: If the input list is empty, we return an empty string. If there are an odd number of tuples, we process pairs until the end, ignoring any remaining unprocessed tuple.\n3. **Iterative Processing**: We iterate through the list in steps of two, processing each pair of consecutive tuples. For each pair, we format the field and value into the required string format and collect these lines.\n4. **String Construction**: The collected formatted lines are joined into a single string with each line separated by a newline character.\n\n### Solution Code\n```python\ndef format_entries(entries: list[tuple[str, str]]) -> str:\n    if not entries:\n        return \"\"\n    lines = []\n    for i in range(0, len(entries), 2):\n        field = entries[i][0]\n        value = entries[i + 1][1]\n        lines.append(f\"{field}: {value}\")\n    return '\\n'.join(lines)\n```\n\n### Explanation\n- **Input Handling**: The function first checks if the input list is empty and returns an empty string if true.\n- **Loop Through List**: Using a for loop with a step of 2, the function processes each pair of consecutive tuples. For each pair, it extracts the field and value.\n- **Formatting**: Each formatted pair is converted into a string with \"field: value\" and added to the `lines` list.\n- **String Construction**: The `lines` list is joined into a single string with each line separated by a newline character, which is then returned.\n\nThis approach ensures that the input list is processed efficiently, handling both even and odd lengths appropriately, and ensuring the output format meets the specified requirements."}
{"task_id": 293, "completion_id": 0, "completion": "To solve this problem, we need to create a function that maps an agent name to its corresponding environment and access type for use in an RL (Reinforcement Learning) environment. The function should handle case-insensitivity and ignore leading/trailing spaces in the agent name.\n\n### Approach\n1. **Normalize the Input**: Convert the input agent name to lowercase and remove any"}
{"task_id": 307, "completion_id": 0, "completion": "To solve this problem, we need to generate a bootstrap sample from a given feature matrix `X` and target vector `y`. A bootstrap sample is created by drawing indices with replacement from the original dataset, which allows for the possibility of repeated indices in the sample.\n\n### Approach\n1. **Problem Analysis**: Bootstrap sampling is a method"}
{"task_id": 328, "completion_id": 0, "completion": "To solve this problem, we need to write a function that identifies the first word in a list of words (called a corpus) where the first character is an uppercase letter. If no such word exists, the function should return an empty list. \n\n### Approach\n1. **Iterate through each word**: The function will loop through each word in the given corpus.\n2. **Check the first character**: For each word, it will check if the first character is an uppercase letter using the `isupper()` method.\n3. **Return the first valid word**: If a word meets the condition of having an uppercase first character, it will be returned immediately.\n4. **Handle edge cases**: If the corpus is empty or contains no words with uppercase first characters, the function will return an empty list.\n\nThis approach ensures that we efficiently find the first word meeting the criteria by iterating through the list only once.\n\n### Solution Code\n```python\nfrom typing import List\n\ndef first_capitalized_word(corpus: List[str]) -> List[str]:\n    for word in corpus:\n        if not word:\n            continue\n        if word[0].isupper():\n            return word\n    return []\n```\n\n### Explanation\n- The function `first_capitalized_word` takes a list of strings (corpus) as input.\n- It iterates through each word in the corpus.\n- For each word, it checks if the word is non-empty and if the first character is uppercase.\n- The first word that meets these conditions is returned immediately.\n- If no such word is found after checking all words, an empty list is returned.\n\nThis solution efficiently handles all edge cases, including empty input and cases where no words start with uppercase letters, ensuring correct results for any valid input."}
{"task_id": 342, "completion_id": 0, "completion": "To solve this problem, we need to compute the Manhattan distance between two real-valued vectors of equal length. The Manhattan distance is the sum of the absolute differences between corresponding elements of the vectors. If the vectors have different lengths, we return -1.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the two input vectors are equal. If they are not, we return -1.\n2. **Compute Manhattan Distance**: If the vectors have the same length, we compute the Manhattan distance by iterating through each corresponding element of the vectors, taking the absolute difference, and summing these differences.\n3. **Round the Result**: Finally, we round the computed distance to the nearest four decimal places and return it as a float.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef manhattan(x: list[float | int], y: list[float | int]) -> float:\n    if len(x) != len(y):\n        return -1\n    total = 0.0\n    for i in range(len(x)):\n        total += abs(x[i] - y[i])\n    return round(total, 4)\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the lengths of the two input lists `x` and `y` are the same. If they are not, it returns -1.\n- **Compute Manhattan Distance**: If the lengths are equal, the function initializes a variable `total` to 0.0. It then iterates through each index of the vectors, computes the absolute difference between the elements at each index, and adds this difference to `total`.\n- **Round the Result**: The computed total distance is rounded to four decimal places using Python's `round` function and returned as a float.\n\nThis approach ensures that we efficiently compute the Manhattan distance while handling cases where the vectors are of unequal length by returning -1. The use of a loop ensures that each element is processed exactly once, making the solution both efficient and straightforward."}
{"task_id": 356, "completion_id": 0, "completion": "To solve this problem, we need to determine the prediction stored in a leaf node of a decision-tree model. The leaf node's prediction depends on whether the tree is a classification or regression tree. For classification, the leaf node holds a list or tuple of class probabilities, while for regression, it holds a single scalar value representing the mean of the target values in that region.\n\n### Approach\n1. **Check the Type of Leaf Node**: Determine if the leaf node's value is a list or tuple. If it is, the leaf node corresponds to a classification tree.\n2. **Classify the Leaf Node**: For classification trees, use the numpy `argmax` function to find the index of the maximum probability value. In case of ties, the smaller index is chosen.\n3. **Handle Regression Trees**: For regression trees, directly return the scalar value stored in the leaf node.\n\n### Solution Code\n```python\nimport numpy as np\n\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A Leaf object whose `value` attribute is either a sequence of\n               class probabilities (classification) or a single number\n               (regression).\n        classifier: When True, treat the leaf as a classification tree.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if isinstance(leaf.value, (list, tuple)):\n        return np.argmax(leaf.value)\n    else:\n        return leaf.value\n```\n\n### Explanation\n- **Classification Handling**: The `argmax` function from the numpy library is used to find the index of the maximum value in a list or tuple. This function efficiently handles ties by returning the smallest index in case of ties.\n- **Regression Handling**: For regression trees, the leaf node's value is a scalar, so it is returned directly without any modification.\n\nThis approach ensures that the function correctly identifies the type of tree and returns the appropriate prediction, whether it is a class index for classification or a raw scalar value for regression."}
{"task_id": 364, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that returns the sign of a given real number. The function should return 1 if the number is positive, -1 if the number is negative, and 0 if the number is exactly zero.\n\n### Approach\nThe approach to solve this problem is straightforward. We will check the value of the input number against zero using simple conditional statements. Here's a step-by-step breakdown of the approach:\n\n1. **Check if the number is positive**: If the input number is greater than zero, return 1.\n2. **Check if the number is negative**: If the input number is less than zero, return -1.\n3. **Check if the number is zero**: If the input number is neither positive nor negative, return 0.\n\nThis approach ensures that all possible cases are handled correctly, including integers and floating-point numbers.\n\n### Solution Code\n\n```python\ndef sign(x: int | float) -> int:\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else:\n        return 0\n```\n\n### Explanation\n- The function `sign` takes a single argument `x`, which can be an integer or a floating-point number.\n- The function first checks if `x` is greater than zero. If true, it returns 1.\n- If `x` is not greater than zero, it then checks if `x` is less than zero. If true, it returns -1.\n- If neither of the above conditions is met, it means `x` is zero, and the function returns 0.\n\nThis implementation efficiently handles all possible cases for the input number, ensuring correct sign determination."}
{"task_id": 378, "completion_id": 0, "completion": "To solve this problem, we need to create a function that acts as an indicator function. This function will return 1 if the input integer is 1 and 0 for any other value. The function should be straightforward and efficient, as it only needs to check the value of the input and return the appropriate result.\n\n### Approach\nThe approach to solving this problem is quite simple. The function will take an integer as input and use a conditional check to determine if the integer is equal to 1. If it is, the function will return 1. Otherwise, it will return 0. This approach ensures that the function is both efficient and easy to understand.\n\n### Solution Code\n```python\ndef indicator(flag: int) -> int:\n    if flag == 1:\n        return 1\n    else:\n        return 0\n```\n\n### Explanation\n1. **Function Definition**: The function `indicator` is defined to take a single parameter `flag` of type `int` and return an integer.\n2. **Conditional Check**: Inside the function, an `if` statement is used to check if `flag` is equal to 1. If this condition is true, the function returns 1.\n3. **Default Case**: If the condition is false (i.e., `flag` is not 1), the function returns 0. This covers all other possible integer values except 1.\n\nThis solution efficiently handles all possible integer inputs and returns the correct result based on whether the input is 1 or not. The time complexity of this function is O(1) since it involves a single conditional check and constant time operations."}
{"task_id": 379, "completion_id": 0, "completion": "To solve this problem, we need to sort a list of priority-queue nodes based on specific rules. Each node has a key, priority, and an entry ID. The sorting must prioritize nodes with lower priority first, and if two nodes have the same priority, the one with the smaller entry ID comes first.\n\n### Approach\n1. **Problem Analysis**: Each node in the priority queue must be sorted based on two criteria: priority and entry ID. The key point is that the sorting must be done in a way that lower priority nodes come first, and for nodes with the same priority, the one with the smaller entry ID comes first.\n2. **Custom Sorting Key**: For each node, we create a tuple `(priority, entry_id)`. This tuple will be used to sort the nodes. When sorted, the tuple will be ordered such that lower priorities come first, and for equal priorities, the smaller entry ID comes first.\n3. **Implementation Steps**:\n   - Check if the input list is empty and return an empty list if it is.\n   - Use the `sorted` function with a custom key to sort the nodes based on the tuple `(priority, entry_id)`.\n   - Extract the 'key' field from each node in the sorted order and return them as a list.\n\n### Solution Code\n```python\ndef sort_priority_nodes(nodes: list[dict]) -> list[str]:\n    if not nodes:\n        return []\n    # Create a list of tuples (priority, entry_id) for each node\n    sorted_nodes = sorted(nodes, key=lambda x: (x['priority'], x['entry_id']))\n    # Extract the 'key' from each node in the sorted order\n    return [node['key'] for node in sorted_nodes]\n```\n\n### Explanation\n1. **Check for Empty List**: The function first checks if the input list is empty and returns an empty list if true."}
{"task_id": 415, "completion_id": 0, "completion": "To solve this problem, we need to implement the one-dimensional Discrete Cosine Transform Type-II (DCT-II) for a given input signal. The DCT-II is a widely used transformation in signal processing and image compression, and it helps in converting a signal from the time domain to the frequency domain.\n\n### Approach\nThe approach to solve this problem involves using the NumPy library, which provides a built-in function `numpy.dct` to compute the DCT-II. The `numpy.dct` function returns the un-normalized DCT-II coefficients, which is exactly what we need according"}
{"task_id": 420, "completion_id": 0, "completion": "To solve this problem, we need to generate two Boolean masks from a 2-D NumPy array based on a given column index and a split value. These masks will help split the rows of the array into two groups: those with values strictly less than the split value (left mask) and those with values greater than or equal to the split value (right mask).\n\n### Approach\n1. **"}
{"task_id": 427, "completion_id": 0, "completion": "To solve this problem, we need to compute the identity activation function and its derivative for a given input. The identity activation function is the simplest form, which returns the input unchanged. The derivative of the identity function is a constant value of 1 for each element.\n\n### Approach\n1. **Input Handling**: The function should accept both list, tuple, and NumPy array inputs. If the input is not a NumPy array, it is converted to one using NumPy.\n2. **Identity Activation**: For any input, the identity activation function simply returns the input as it is.\n3. **Derivative Calculation**: The derivative of the identity function is a constant value of 1 for each element. This is computed using NumPy to create an array of ones with the same shape and data type as the input.\n4. **Conversion to Lists**: The results (activation and derivative) are converted to Python lists using NumPy's `tolist()` method to ensure the output format is correct.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef identity_activation(z):\n    if not isinstance(z, np.ndarray):\n        z = np.array(z)\n    activation = z\n    derivative = np.ones_like(z)\n    return (activation.tolist(), derivative.tolist())\n```\n\n### Explanation\n- **Input Handling**: The function first checks if the input `z` is a NumPy array. If not, it converts `z` to a NumPy array. This allows vectorized"}
{"task_id": 448, "completion_id": 0, "completion": "To solve this problem, we need to create a baseline regressor that always predicts the mean of the training targets. This approach is simple yet effective, serving as a baseline for more complex regression models.\n\n### Approach\nThe task is to implement a function that:\n1. Computes the arithmetic mean of the training targets.\n2. Creates predictions for each row in the test feature matrix using this mean.\n3. Rounds each prediction to the nearest 4th decimal place.\n4. Returns the predictions as a list of floats.\n\nThe steps involved are as follows:\n1. Check if the training targets are empty. If they are, return an empty list.\n2. Compute the mean of the training targets.\n3. Generate predictions for each row in the test feature matrix using the computed mean.\n4. Round each prediction to 4 decimal places.\n5. Return the rounded predictions as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mean_baseline_predict(X_train, y_train, X_test):\n    \"\"\"Baseline regressor that predicts the mean of the training targets.\n    \n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training"}
{"task_id": 454, "completion_id": 0, "completion": "To solve this problem, we need to generate one-hot label arrays used in the training of a Deep Convolutional Generative Adversarial Network (DCGAN). These arrays are essential for training both the discriminator and generator networks in a DCGAN. The labels are structured such that real images are labeled as class 0 ([1, 0]) and generated (fake) images are labeled as class 1 ([0, 1]).\n\n### Approach\n1. **Problem Analysis**: \n   - The task requires generating labels for a batch of images. The batch size is even, which means it can be split into two equal halves.\n   - The first half of the batch should contain real images labeled as [1, 0].\n   - The second half should contain generated images labeled as [0, 1].\n   - The full batch should contain images labeled as [1, 0], which is used to test the discriminator.\n\n2. **Intuitions and Insights**:\n   - Since the batch size is even, we can split it into two equal parts.\n   - Each part will have the same structure: the first half will have labels [1, 0], and the second half will have labels [0, 1].\n   - The full batch will maintain the labels [1, 0] as it contains all the images.\n\n3. **Algorithm Selection**:\n   - Use list comprehensions to create the labels for each part of the batch.\n   - Convert the lists to numpy arrays to meet the problem"}
{"task_id": 466, "completion_id": 0, "completion": "To solve this problem, we need to write a function that removes every punctuation character from a given string and also trims any leading or trailing whitespace from the resulting string. Punctuation characters are defined by the constant `string.punctuation`, and if the input string contains only punctuation and/or whitespace, the function should return an empty string.\n\n### Approach\n1. **Create Translation Table**: We first create a translation table that maps every punctuation character to `None`. This table will be used to translate each character in the input string, effectively removing all punctuation.\n2. **Translate the String**: Using the translation table, we translate each character in the input string. Characters that are in the `string.punctuation` will be replaced with `None`, effectively removing them from the string.\n3. **Strip Whitespace**: After translating the string, we strip any leading and trailing whitespace from the resulting string using the `strip()` method.\n\n### Solution Code\n\n```python\nimport string\n\ndef strip_punctuation(line: str) -> str:\n    translation_table = str.maketrans('', '', string.punctuation)\n    translated_line = line.translate(translation_table)\n    stripped_line = translated_line.strip()\n    return stripped_line\n```\n\n### Explanation\n1. **Translation Table**: The `str.maketrans('', '', string.punctuation)` creates a translation table where each character in `string.punctuation` is mapped to `None`. This means that any character that is considered punctuation will be removed when the string is translated.\n2. **String Translation**: The `translate()` method is used to apply the translation table to each character in the input string. This effectively removes all punctuation characters.\n3. **Whitespace Stripping**: The `strip()` method is called on the translated string to remove any leading or trailing whitespace, ensuring the result is clean and properly formatted.\n\nThis approach efficiently handles the removal of punctuation and stripping of whitespace in a single pass, making it both concise and effective."}
{"task_id": 467, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that computes the L1 and L2 regularizations for a given list of weights. Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. The two"}
{"task_id": 501, "completion_id": 0, "completion": "To solve this problem, we need to implement a constant learning-rate scheduler. This scheduler returns a fixed learning rate regardless of the current training step. The function should take two arguments: the initial learning rate and the current step, but the step value should not influence the result.\n\n### Approach\nThe approach to solve this problem is straightforward. The constant learning-rate scheduler simply returns the initial learning rate value for every call, regardless of the current step. This ensures that the learning rate does not change during training, which is a common practice in optimization algorithms like stochastic gradient descent.\n\n### Solution Code\n\n```python\ndef constant_scheduler(initial_lr: float, step: int) -> float:\n    \"\"\"Constant learning-rate scheduler.\n\n    Parameters\n    ----------\n    initial_lr : float\n        The fixed learning rate that should be returned on every call.\n    step : int\n        The current training step (ignored).\n\n    Returns\n    -------\n    float\n        The same value as `initial_lr`.\n    \"\"\"\n    return initial_lr\n```\n\n### Explanation\nThe function `constant_scheduler` takes two parameters: `initial_lr` (a float representing the fixed learning rate) and `step` (an integer representing the current training step). The function simply returns the value of `initial_lr` every time it is called, regardless of the value of `step`. This ensures that the learning rate remains constant throughout the training process. \n\n- The function starts by defining the parameters and their types.\n- It then returns the `initial_lr` value, which is the fixed learning rate to be used for all training steps.\n- The step parameter is ignored, as it is meant for API compatibility and does not influence the result.\n\nThis solution is efficient and straightforward, ensuring that the learning rate does not change during the training process, which can be beneficial in certain optimization scenarios."}
{"task_id": 503, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that updates only the existing keys in a dictionary `hparams` with values from another dictionary `updates`, while ignoring any keys that are not present in `hparams`. The function should handle both nested structures and provide a deep copy of `hparams` to ensure the original data remains unchanged.\n\n### Approach\n1. **Check for None**: If `hparams` is `None`, the function should immediately return `-1`.\n2. **Deep Copy**: Create a deep copy of `hparams` using the `deepcopy` function from the `copy` module. This ensures that any modifications do not affect the original dictionary.\n3. **Update Existing Keys**: If `updates` is not `None`, iterate through its key-value pairs. For each key, check if it exists in the deep-copied `hparams`. If it does, update the value in the copied dictionary.\n4. **Return Result**: Return the modified copied dictionary. If `updates` is `None`, return the deep-copied `hparams` as it is.\n\n### Solution Code\n```python\nfrom copy import deepcopy\n\ndef selective_update(hparams: dict | None, updates: dict | None) -> dict | int:\n    if hparams is None:\n        return -1\n    copied = deepcopy(hparams)\n    if updates is not None:\n        for key, value in updates.items():\n            if key in copied:\n                copied[key] = value\n    return copied\n```\n\n### Explanation\n- **Handling None**: The function first checks if `hparams` is `None` and returns `-1` if true.\n- **Deep Copy**: The `deepcopy` function is used to create a deep copy of `hparams`, which is essential to avoid modifying the original dictionary"}
{"task_id": 508, "completion_id": 0, "completion": "To solve this problem, we need to extract values from a dictionary whose keys contain a given substring. The solution must preserve the original insertion order of the dictionary.\n\n### Approach\n1. **Problem Analysis**: We are given a dictionary where keys are strings and values are any Python data type. Our task is to filter out the values whose keys contain a specified substring. The solution must maintain the order of keys as they appear in the dictionary.\n2. **Intuition**: By iterating over each key-value pair in the dictionary, we can check if the key contains the given substring. If it does, we include the corresponding value in our result list.\n3. **Algorithm Selection**: We will use Python's dictionary iteration which automatically maintains the insertion order. For each key-value pair, we check if the key contains the substring using the `in` keyword. If it does, we append the value to the result list.\n4. **Complexity Analysis**: The time complexity is O(n), where n is the number of key-value pairs in the dictionary. This is because we need to iterate through each element once.\n\n### Solution Code\n```python\nfrom typing import Any, Dict, List\n\ndef params_with_name(params: Dict[str, Any], name: str) -> List[Any]:\n    result = []\n    for key, value in params.items():\n        if name in key:\n            result.append(value)\n    return result\n```\n\n### Explanation\n- **Initialization**: We start by initializing an empty list `result` to store the values that meet the criteria.\n- **Iteration**: Using `params.items()`, we iterate over each key-value pair in the dictionary. The `items()` method returns the keys and values in the order they were inserted into the dictionary.\n- **Substring Check**: For each key, we check if it contains the given substring `name` using the `in` keyword. If true, the corresponding value is added to the `result` list.\n- **Return**: Finally, the `result` list, which now contains all values whose keys contain the substring, is returned.\n\nThis approach ensures that we efficiently and correctly extract the required values while maintaining the original order of the dictionary's keys."}
{"task_id": 511, "completion_id": 0, "completion": "To solve this problem, we need to generate a random Bernoulli multi-armed bandit environment. This involves creating a list of success probabilities for each arm, ensuring the probabilities sum to 1. The solution must handle edge cases and return a properly formatted list of probabilities.\n\n### Approach\n1. **Check for Valid Input**: If the number of arms (`n_arms`) is less than 1, return an empty list.\n2. **Generate Random Samples**: Draw `n_arms` independent samples from a uniform distribution on the interval [0, 1). This can be efficiently done using NumPy's `random.uniform` function.\n3. **Normalize Probabilities**: Convert the generated samples into probabilities by normalizing them such that their sum is 1. This is achieved by dividing each sample by the sum of all samples.\n4. **Round Probabilities**: Each probability is rounded to four decimal places to ensure precision and consistency.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_bernoulli_mab(n_arms: int = 10) -> list[float]:\n    if n_arms < 1:\n        return []\n    samples = np.random.uniform(0, 1, size=n_arms)\n    total = np.sum(samples)\n    probabilities = samples / total\n    rounded = [round(x, 4) for x in probabilities]\n    return rounded\n```\n\n### Explanation\n- **Input Validation**: The function first checks if `n_arms` is a positive integer. If not, it returns an empty list.\n- **Random Sampling**: Using NumPy, we generate `n_arms` random numbers between 0 and 1. These represent the raw probabilities.\n- **Normalization**: The raw probabilities are normalized by dividing each by the sum of all samples. This ensures that the probabilities sum to 1, which is a requirement for a valid probability distribution.\n- **Rounding**: Each normalized probability is rounded to four decimal places to meet the output constraints.\n\nThis approach ensures that the generated environment is both valid and meets the precision requirements specified in the problem statement."}
{"task_id": 515, "completion_id": 0, "completion": "To solve this problem, we need to flatten a nested dictionary into a hierarchical structure where each key is a concatenation of the original key and the nested key, separated by a dot. The goal is to transform the input dictionary into a more readable format without mutating the original data.\n\n### Approach\nThe approach involves using recursion to traverse each level of the nested dictionary. For each key-value pair, we check if the value is a dictionary. If it is, we recursively flatten that dictionary and then build a new key by concatenating the current key with each sub-key from the nested dictionary. If the value is not a dictionary, it remains unchanged, and we simply add it to the result.\n\n1. **Iterate through each key**: For each key in the input dictionary, retrieve its value.\n2. **Check if the value is a dictionary**: If the value is a dictionary, recursively process it.\n3. **Build new keys**: For each sub-key in the nested dictionary, construct a new key by appending the sub-key to the current key and add it to the result.\n4. **Handle non-dictionary values**: If the value is not a dictionary, add it directly to the result.\n\nThis approach ensures that all nested structures are flattened into a single hierarchical structure, making it easier to read and understand the data.\n\n### Solution Code\n```python\nfrom typing import Any, Dict\n\ndef"}
{"task_id": 530, "completion_id": 0, "completion": "To solve this problem, we need to compute the mean Binary Cross-Entropy (BCE) loss for a batch of predictions. The BCE loss is commonly used in binary classification tasks where the model predicts probabilities for the positive class. The loss function helps in training the model by measuring the difference between the predicted probabilities and the actual ground-truth labels.\n\n### Approach\n1. **Clipping Predictions**: Since the predicted probabilities might be very close to 0 or 1, which can cause numerical instability when taking logarithms, we clip the predictions to a range [1e-7, 1 - 1e-7]. This ensures that we avoid taking logarithms of zero or very small numbers.\n2. **Loss Calculation**: For each sample, the BCE loss is calculated using the formula:\n   \\[\n   \\ell = -[ y \\cdot \\log(p) + (1 - y) \\cdot \\log(1 - p) ]\n   \\]\n   where \\( y \\) is the ground-truth label and \\( p \\) is the predicted probability. This formula is applied to each element in the arrays, and the result is averaged to get the mean loss.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculates the mean Binary Cross-Entropy loss.\n    \n    Parameters\n    ----------\n    y_true : np.ndarray\n        Ground-truth binary labels (0 or 1).\n    y_pred : np.ndarray\n        Predicted probabilities for the positive class. Must have the same shape\n        as *y_true.\n    \n    Returns\n    -------\n    float\n        Mean BCE loss rounded to 4 decimal places.\n    \"\"\"\n    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n    loss = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n    return np.mean(loss)\n```\n\n### Explanation\n1. **Clipping Predictions**: The `np.clip` function is used to ensure that the predicted probabilities \\( y \\) are within a safe range [1e-7, 1 - 1e-7]. This"}
{"task_id": 532, "completion_id": 0, "completion": "To solve this problem, we need to determine the total number of trainable parameters in a fully-connected (dense) neural network. The parameters include both the weights and biases for each layer transition.\n\n### Approach\n1. **Problem Analysis**: A fully-connected neural network's parameters consist of weights and biases for each pair of consecutive layers. For layers with `n_in` and `n_out` neurons respectively, the number of weights is `n_in * n_out` and the number of biases is `n_out`. The total parameters for each consecutive layer pair are the sum of these values.\n\n2. **Edge Cases**: If the list of layers has fewer than two elements, there are no trainable parameters, and the function should return 0.\n\n3. **Algorithm**:\n   - Check if the number of layers is less than 2. If so, return 0.\n   - Initialize a total parameter counter.\n   - Iterate through each consecutive pair of layers, compute the weights and biases for each pair, and add their sum to the total parameter counter.\n   - Return the total parameter count.\n\n### Solution Code\n```python\ndef count_fc_parameters(layers: list[int]) -> int:\n    if len(layers) < 2:\n        return 0\n    total = 0\n    for i in range(len(layers) - 1):\n        n_in = layers[i]\n        n_out = layers[i + 1]\n        total += n_in * n_out + n_out\n    return total\n```\n\n### Explanation\n- **Edge Case Handling**: The function first checks if the number of layers is less than 2. If so, it returns 0 immediately since no layers mean no trainable parameters.\n- **Iteration and Calculation**: For each consecutive pair of layers, the number of weights is calculated as the product of the number of input neurons (`n_in`) and output neurons (`n_out`). The number of biases is simply the number of output neurons (`n_out`). These values are summed for each pair and added to the total parameter count.\n- **Efficiency**: The algorithm efficiently processes each layer pair exactly once, resulting in a time complexity of O(n), where n is the number of layers. This ensures the solution is optimal even for larger lists of layers.\n\nThis approach ensures that we accurately compute the total number of trainable parameters in a neural network by considering both weights and biases for each layer transition."}
{"task_id": 539, "completion_id": 0, "completion": "To solve this problem, we need to calculate the binary cross-entropy (BCE) cost for a neural network's predictions. The BCE cost is a common loss function used in binary classification problems, and it helps in measuring the performance of a classification model where the output is a probability value between 0 and 1.\n\n### Approach\n1. **Problem Understanding**: The binary cross-entropy cost function is given by:\n   \\[\n   J = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(a_i) + (1 - y_i) \\log(1 - a_i) \\right]\n   \\]\n   where \\(a_i\\) is the predicted probability and \\(y_i\\) is the true label (0 or 1) for each sample. We need to compute this cost for all samples in the dataset.\n\n2. **Clipping Probabilities**: To avoid numerical issues when \\(a_i\\) is very close to 0 or 1, we clip the predicted probabilities to the range \\([1 \\times 10^{-15}, 1 - 1 \\times 10^{-15}]\\). This ensures that we do not take the logarithm of zero or one, which would be undefined.\n\n3. **Summing and Averaging**: After clipping, we compute the cost for each sample, sum these values, multiply by \\(-1/m\\) (where \\(m\\) is the number of samples), and return the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    m = AL.size\n    clipped_AL = np.clip(AL,"}
{"task_id": 540, "completion_id": 0, "completion": "To solve this problem, we need to compute the element-wise derivative of the sigmoid function for each element in a given NumPy array. The"}
{"task_id": 541, "completion_id": 0, "completion": "To solve this problem, we need to compute the gradient of the Binary Cross-Entropy (BCE) loss with respect to the logits (logits) of a GAN. This gradient is essential for updating the discriminator's weights during training, as it provides the direction in which the loss should be adjusted to minimize the error.\n\n### Approach\n1. **Understanding BCE Loss and Gradient**: The BCE loss is a common loss function used in binary classification problems. The loss for a single example is given by:\n   \\[\n   \\text{BCE}(y, \\hat{y}) = -y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})\n   \\]\n   where \\( y \\) is the ground-truth label (0 or 1) and \\( \\hat{y} \\) is the predicted probability. The gradient of this loss with respect to the logits \\( z \\) (before applying the sigmoid function) is a closed-form expression:\n   \\[\n   \\frac{\\partial \\text{BCE}(y, \\hat{y})}{\\partial z} = \\hat{y} - y\n   \\]\n\n2. **Generalization to Multiple Examples**: For an array-like input \\( y \\) and \\( \\hat{y} \\) of the same shape, the gradient is computed element-wise as \\( \\hat{y} - y \\) for each corresponding element. This gradient is then rounded to four decimal places.\n\n3. **Handling Different Data Types**: The function should handle both scalar values and multi-dimensional arrays, ensuring that the gradient is computed correctly for any shape of \\( y \\) and \\( \\hat{y} \\).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef return_input_grads(y, y_hat):\n    gradient = y_hat - y\n    gradient_rounded = np.round(gradient, 4)\n    return gradient_rounded.tolist()\n```\n\n### Explanation\n1. **Compute the Gradient**: The code first calculates the element-wise difference between the predicted probabilities \\( \\hat{y} \\) and the ground-truth labels \\( y \\). This difference represents the gradient of the BCE loss with respect to the logits.\n\n2. **Round the Gradient**: Each element of the computed gradient is rounded to four decimal places to ensure precision and stability in the gradient updates.\n\n3. **Convert to Python List**: The rounded gradient is then converted into a Python list to maintain the original structure of the input arrays, whether they are scalars, 1-D, or 2-D arrays.\n\nThis approach efficiently computes the necessary gradient for updating the discriminator's weights in a GAN, ensuring correct and stable training."}
{"task_id": 543, "completion_id": 0, "completion": "To solve this problem, we need to calculate the Gini index for a list of class labels. The Gini index is a measure of how often a randomly chosen element from the set would be mis classified if it were randomly labeled according to the class distribution in the set. \n\n### Approach\n1. **Check for Empty List**: If the input list is empty, return 0.0 immediately.\n2. **Count Class Occurrences**: Use the `Counter` class from the `collections` module to count the occurrences of each class in the list.\n3. **Calculate Proportions**: Convert the counts into proportions of the total number of samples.\n4. **Compute Gini Index**: Use the formula \\( \\text{Gini}(D) = 1 - \\sum_{i=1}^{m} p_i^2 \\), where \\( p_i \\) is the proportion of samples in class \\( i \\). This involves summing the squares of each proportion and subtracting the result from 1.\n5. **Round the Result**: Round the computed Gini index to four decimal places.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef cal_gini_index(data: list) -> float:\n    if not data:\n        return"}
{"task_id": 544, "completion_id": 0, "completion": "To solve this problem, we need to partition a given list of samples into two subsets based on a specified feature and threshold value. The goal is to create two new lists where each list contains samples that meet the condition relative to the feature and threshold.\n\n### Approach\nThe task is to split the data list into two subsets: `set1` and `set2`. `set1` contains all samples where the feature at a specified index is greater than or equal to a given threshold. `set2` contains all other samples, which have a feature value strictly less than the threshold. The order of samples within each subset must follow the original order of the data list.\n\nThe approach involves iterating through each sample in the data list, checking the feature value at the specified index against the threshold. Based on this check, each sample is added to either `set1` or `set2`.\n\n### Solution Code\n```python\ndef split_tree(data: list, fea: int, value: float):\n    set1 = []\n    set2 = []\n    for i, s in enumerate(data):\n        if s[fea] >= value:\n            set1.append(s)\n        else:\n            set2.append(s)\n    return (set1, set2)\n```\n\n### Explanation\n1. **Initialization**: Two empty lists, `set1` and `set2`, are initialized to store the samples based on the given condition.\n2. **Iteration**: Using a loop, each sample in the data list is processed. For each sample, the feature value at the specified index is checked.\n3. **Condition Check**: If the feature value is greater than or equal to the threshold, the sample is added to `set1`. Otherwise, it is added to `set2`.\n4. **Return**: The function returns a tuple of the two lists (`set1`, `set2`).\n\nThis approach ensures that the original order of the samples is preserved in both subsets, and each subset is built based on the specified condition efficiently."}
{"task_id": 559, "completion_id": 0, "completion": "To solve this problem, we need to collect the values of all leaf nodes in a binary tree and return them as a list in left-to-right order. Leaf nodes are defined as nodes that have no children; only these nodes contain a value. The tree can be empty, and in that case, we should return an empty list.\n\n### Approach\nThe approach to solve this problem involves performing a depth-first traversal of the binary tree. Specifically, we will use a pre-order traversal, which ensures that we visit each node before its children. This traversal will allow us to collect the values of all leaf nodes in the correct order.\n\n1. **Base Case**: If the tree is empty (i.e., the root is `None`), return an empty list.\n2. **Leaf Node Check**: For each node, check if it is a leaf node by verifying that both its left and right children are `None`.\n3. **Recursive Traversal**: If the current node is a leaf node, add its value to the result list. Then recursively traverse its left and right subtrees, appending the results in left-to-right order.\n\n### Solution Code\n```python\nclass Node:\n    \"\"\"Binary-tree node used by the task.\n\n    Attributes:\n        val:     The value stored in the node. Only leaf nodes have a value.\n        right:   Reference to the right child or ``None``.\n        left:    Reference to the left child or ``None``.\n    \"\"\"\n\n    def __init__(self, val: Optional[int] = None, right: 'Optional[Node]' = None,"}
{"task_id": 578, "completion_id": 0, "completion": "To solve this problem, we need to generate a list of consecutive pairs (bigrams) from a given list of elements. Each pair consists of two consecutive elements from the list, and the order of these pairs must follow the original order of the elements in the list.\n\n### Approach\n1. **Edge Case Handling**: If the length of the input list is less than 2, return an empty list immediately since there can be no consecutive pairs.\n2. **Iterate Through the List**: Loop through the list from the start to the second-to-last element. For each element, form a tuple with the next element in the list.\n3. **Collect Pairs**: Store each formed tuple in a result list which is then returned as the final output.\n\nThis approach ensures that we efficiently generate the required pairs by iterating through the list only once, making the solution both time and space efficient.\n\n### Solution Code\n```python\nfrom typing import List, Tuple\n\ndef make_pairs(corpus: list) -> list[tuple]:\n    if len(corpus) < 2:\n        return []\n    result = []\n    for i in range(len(corpus) - 1):\n        result.append((corpus[i], corpus[i+1]))\n    return result\n```\n\n### Explanation\n- **Edge Case Handling**: The function first checks if the length of the input list is less than 2. If true, it returns an empty list.\n- **Loop Through Elements**: Using a loop, the function iterates from the start of the list to the second-to-last element. For each index `i`, it forms a tuple with the element at `i` and the element at `i+1`.\n- **Collect and Return Pairs**: Each formed tuple is collected into a result list, which is then returned. This ensures that the result is a list of tuples, each representing a consecutive pair from the input list.\n\nThis method efficiently processes the input list in linear time, O(n), where n is the length of the list, ensuring optimal performance."}
{"task_id": 585, "completion_id": 0, "completion": "To solve this problem, we need to replicate the functionality of Python 2's built-in `cmp` function. This function takes two comparable objects and returns an integer indicating their ordering: 1 if the first object is greater than the second, 0 if they are equal, and -1 if the first object is less than the second.\n\n### Approach\nThe approach involves creating a function that directly compares the two input objects using their default comparison operators. This function will return 1 if the first object is greater than the second, -1 if the first object is less than the second, and 0 if both objects are equal. The solution is straightforward because it leverages Python's built-in comparison operators to determine the result efficiently.\n\n### Solution Code\n```python\ndef cmp(a, b):\n    if a > b:\n        return 1\n    elif a < b:\n        return -1\n    else:\n        return 0\n```\n\n### Explanation\n1. **Comparison Using Default Operators**: The function uses the default comparison operators `>` and `<` to compare the two input objects `a` and `b`.\n2. **Return Values**: \n   - If `a` is greater than `b`, the function returns 1.\n   - If `a` is less than `b`, the function returns -1.\n   - If `a` is equal to `b`, the function returns 0.\n3. **Handling All Comparable Objects**: The function is designed to handle any objects that support the comparison operators, making it versatile for different data types including numbers, strings, tuples, lists, etc.\n\nThis approach ensures that the function works correctly for all comparable objects, providing the desired ordering result efficiently."}
{"task_id": 588, "completion_id": 0, "completion": "To solve this problem, we need to determine the maximum depth of a binary tree rooted at a given node. The depth of a node is defined such that the root node has a depth of 0, and each subsequent level increases the depth by 1. An empty tree, which has no nodes, has a depth of -1.\n\n### Approach\nThe approach to solve this problem involves using recursion to traverse the tree and compute the maximum depth. Here\u2019s a detailed breakdown of the approach:\n\n1. **Base Case**: If the tree is empty (i.e., the root node is `None`), the depth is -1.\n2. **Recursive Case**: For a non-empty tree, the depth of the current node is 1 plus the maximum depth of its left and right subtrees. This is because each node's depth is one more than its parent's depth.\n\nThe recursive function will:\n1. Check if the root is `None`. If it is, return -1.\n2. Otherwise, recursively compute the depth of the left and right subtrees.\n3. Return 1 plus the maximum depth of the left and right subtrees.\n\nThis approach ensures that we correctly compute the depth for each node by leveraging the structure of the tree and the properties of recursion.\n\n### Solution Code\n```python\nfrom typing import Optional\nfrom typing import Optional\n\nclass Node:\n    \"\"\"A simple binary tree node.\"\"\"\n\n    def __init__(self,\n                 data=None,\n                 split_variable=None,\n                 split_variable_value=None,\n                 left: Optional['Node'] = None,\n                 right: Optional['Node'] = None,\n                 depth: int = 0,\n                 criterion_value=None):\n        self.data = data\n        self.split_variable = split_variable\n        self.split_variable_value = split_variable_value\n        self.left = left\n        self.right = right\n        self.depth = depth\n        self.criterion_value = criterion_value\n\n\ndef tree_max_depth(root: Optional[Node]) -> int:\n    \"\"\"Computes the maximum depth of a binary tree.\n\n    The depth of the root node is defined to be 0.  The depth of an empty\n    tree is defined to be -1.\n\n    Args:\n        root: The root of the binary tree (Node or None).\n\n    Returns:\n        The maximum depth as an integer.\n    \"\"\"\n    if root is None:\n        return -1\n    else:\n        left_depth = tree_max_depth(root.left)\n        right_depth = tree_max_depth(root.right)\n        return 1 + max(left_depth, right_depth)\n```\n\n### Explanation\n- **Base Case Handling**: The function first checks if the root is `None`. If it is, the function returns -1, indicating an empty tree.\n- **Recursive Depth Calculation**: For a non-empty tree, the function recursively calculates the depth of the left and right subtrees. The depth of the current node is then 1 plus the maximum of the depths of the left and right subtrees.\n- **Efficiency**: This approach efficiently computes the maximum depth using a linear traversal of the tree, which is optimal for this problem.\n\nThis solution effectively leverages recursion to traverse the tree and compute the required depth, ensuring that each node's depth is correctly calculated based on its parent's depth."}
