{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given matrix (list of lists) into a specified shape using numpy.\n\n    Args:\n        a: The input matrix as a list of lists.\n        new_shape: A tuple (rows, columns) representing the desired new shape.\n\n    Returns:\n        The reshaped matrix as a list of lists if reshaping is possible,\n        otherwise an empty list [].\n    \"\"\"\n    try:\n        np_array = np.array(a)\n    except Exception:\n        return []\n    original_size = np_array.size\n    new_size = new_shape[0] * new_shape[1]\n    if original_size != new_size:\n        return []\n    try:\n        reshaped_array = np_array.reshape(new_shape)\n    except Exception:\n        return []\n    return reshaped_array.tolist()"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar.\n\n    Args:\n        matrix: A list of lists representing the matrix.\n                Each inner list is a row of the matrix.\n                Elements can be integers or floats.\n        scalar: The scalar value to multiply the matrix by.\n                Can be an integer or a float.\n\n    Returns:\n        A new matrix (list of lists) where each element of the\n        original matrix is multiplied by the scalar.\n    \"\"\"\n    result_matrix = [[element * scalar for element in row] for row in matrix]\n    return result_matrix"}
{"task_id": 8, "completion_id": 0, "solution": "import math\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n\n    Args:\n        matrix: A 2x2 matrix represented as a list of lists of floats.\n                Expected format: [[a, b], [c, d]]\n\n    Returns:\n        The inverse matrix as a list of lists of floats, or None if the\n        matrix is not invertible.\n    \"\"\"\n    if not isinstance(matrix, list) or len(matrix) != 2:\n        return None\n    if not all((isinstance(row, list) and len(row) == 2 for row in matrix)):\n        return None\n    if not all((isinstance(elem, (int, float)) for row in matrix for elem in row)):\n        return None\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    determinant = a * d - b * c\n    if abs(determinant) < 1e-09:\n        return None\n    inv_determinant = 1 / determinant\n    inverse_matrix = [[d * inv_determinant, -b * inv_determinant], [-c * inv_determinant, a * inv_determinant]]\n    return inverse_matrix"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n  Computes the sigmoid activation function for a given input z.\n\n  Args:\n    z: The input value (float).\n\n  Returns:\n    The output of the sigmoid function, rounded to four decimal places (float).\n  \"\"\"\n    sigmoid_value = 1 / (1 + math.exp(-z))\n    rounded_sigmoid_value = round(sigmoid_value, 4)\n    return rounded_sigmoid_value"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a list of scores.\n\n    Args:\n        scores: A list of floating-point numbers representing the scores.\n\n    Returns:\n        A list of floating-point numbers representing the softmax probabilities,\n        each rounded to four decimal places. Returns an empty list if the\n        input list is empty.\n    \"\"\"\n    if not scores:\n        return []\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    if sum_exp_scores == 0:\n        return [0.0] * len(scores)\n    softmax_probs = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_probs"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Performs a random shuffle of the samples in two numpy arrays, X and y,\n    while maintaining the corresponding order between them.\n\n    Args:\n        X (np.ndarray): The feature data array.\n        y (np.ndarray): The target data array. Must have the same number\n                        of samples as X.\n        seed (int, optional): Seed for the random number generator for\n                              reproducible shuffling. Defaults to None.\n\n    Returns:\n        tuple: A tuple containing two Python lists:\n               - The shuffled X data.\n               - The shuffled y data.\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples.')\n    n_samples = X.shape[0]\n    if seed is not None:\n        rng = np.random.RandomState(seed)\n        permutation = rng.permutation(n_samples)\n    else:\n        permutation = np.random.permutation(n_samples)\n    shuffled_X = X[permutation]\n    shuffled_y = y[permutation]\n    return (shuffled_X.tolist(), shuffled_y.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n  Converts a 1D numpy array into a diagonal matrix.\n\n  Args:\n    x: A 1D numpy array.\n\n  Returns:\n    A Python list representing the diagonal matrix (2D numpy array\n    converted to list).\n  \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Calculates the accuracy score between true labels and predicted labels.\n\n    Args:\n        y_true: A 1D numpy array containing the true labels.\n        y_pred: A 1D numpy array containing the predicted labels.\n\n    Returns:\n        The accuracy score as a float, rounded to 4 decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays y_true and y_pred must have the same shape.')\n    if y_true.ndim != 1:\n        raise ValueError('Input arrays y_true and y_pred must be 1-dimensional.')\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    if total_predictions == 0:\n        return 0.0\n    accuracy = correct_predictions / total_predictions\n    rounded_accuracy = round(accuracy, 4)\n    return rounded_accuracy"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for a given dataset(s).\n\n    Args:\n        X (np.ndarray): A 2D numpy array representing the first dataset\n                        (rows are observations, columns are variables).\n        Y (np.ndarray, optional): A 2D numpy array representing the second dataset\n                                  (rows are observations, columns are variables).\n                                  If None, calculates the correlation matrix of X with itself.\n\n    Returns:\n        list: The correlation matrix as a 2D Python list, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        corr_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        corr_matrix = np.corrcoef(X, Y, rowvar=False)\n    rounded_corr_matrix = np.round(corr_matrix, 4)\n    return rounded_corr_matrix.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n\n    The ReLU function returns the input if it's greater than 0,\n    otherwise, it returns 0.\n\n    Args:\n        z: The input value (float).\n\n    Returns:\n        The output after applying the ReLU function (float).\n        Returns z if z > 0, otherwise returns 0.\n    \"\"\"\n    return max(0.0, z)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n  Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n  Args:\n    z: The input value (float).\n    alpha: The slope for negative inputs (float, default is 0.01).\n\n  Returns:\n    The output after applying the Leaky ReLU function (float or int).\n  \"\"\"\n    if z > 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n  Computes the linear kernel between two input vectors x1 and x2.\n\n  The linear kernel is defined as the dot product (inner product) of the two vectors.\n\n  Args:\n    x1: A numpy array representing the first vector.\n    x2: A numpy array representing the second vector.\n\n  Returns:\n    A scalar value representing the linear kernel (dot product) of x1 and x2.\n  \"\"\"\n    x1 = np.asarray(x1)\n    x2 = np.asarray(x2)\n    if x1.shape != x2.shape:\n        raise ValueError('Input vectors must have the same dimension.')\n    if x1.ndim > 1 or x2.ndim > 1:\n        raise ValueError('Input must be 1-dimensional vectors.')\n    linear_kernel = np.dot(x1, x2)\n    return linear_kernel"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculates the precision metric.\n\n    Precision is defined as the ratio of true positives to the sum of\n    true positives and false positives.\n\n    Args:\n        y_true (np.ndarray): A numpy array containing the true binary labels (0 or 1).\n        y_pred (np.ndarray): A numpy array containing the predicted binary labels (0 or 1).\n\n    Returns:\n        float: The precision score. Returns 0 if the denominator (TP + FP) is zero.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    denominator = true_positives + false_positives\n    if denominator == 0:\n        return 0.0\n    else:\n        return true_positives / denominator"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Implements the Adam optimization algorithm.\n\n    Args:\n        f: The objective function to be optimized.\n        grad: A function that computes the gradient of f.\n        x0: Initial parameter values (can be a list or numpy array).\n        learning_rate: The step size.\n        beta1: Exponential decay rate for the first moment estimates.\n        beta2: Exponential decay rate for the second moment estimates.\n        epsilon: A small constant for numerical stability.\n        num_iterations: Number of iterations to run the optimizer.\n\n    Returns:\n        A list containing the optimized parameters, rounded to 4 decimal places.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for i in range(num_iterations):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    optimized_params = np.round(x, 4).tolist()\n    return optimized_params"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculates the recall metric for binary classification.\n\n    Recall is the ratio of true positives to the total number of actual positives.\n\n    Args:\n        y_true: A list or numpy array of true binary labels (0 or 1).\n        y_pred: A list or numpy array of predicted binary labels (0 or 1).\n\n    Returns:\n        The recall value rounded to three decimal places. Returns 0.0 if there\n        are no actual positive instances.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if not np.all(np.isin(y_true, [0, 1])) or not np.all(np.isin(y_pred, [0, 1])):\n        raise ValueError('Inputs y_true and y_pred must contain only 0s and 1s')\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    actual_positives = np.sum(y_true == 1)\n    if actual_positives == 0:\n        return 0.0\n    else:\n        recall_value = true_positives / actual_positives\n        return round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implements the self-attention mechanism.\n\n    Args:\n        X (np.ndarray): Input matrix (e.g., word embeddings) of shape (seq_len, embed_dim).\n        W_q (np.ndarray): Weight matrix for queries of shape (embed_dim, d_k).\n        W_k (np.ndarray): Weight matrix for keys of shape (embed_dim, d_k).\n        W_v (np.ndarray): Weight matrix for values of shape (embed_dim, d_v).\n\n    Returns:\n        list: The self-attention output as a Python list, rounded to 4 decimal places.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    d_k = K.shape[-1]\n    scores = Q @ K.T\n    scaled_scores = scores / np.sqrt(d_k)\n    exp_scores = np.exp(scaled_scores - np.max(scaled_scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = attention_weights @ V\n    rounded_output = np.round(output, 4)\n    return rounded_output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculates the KL divergence between two univariate normal distributions.\n\n    The KL divergence D(P || Q) between two normal distributions\n    P ~ N(mu_p, sigma_p^2) and Q ~ N(mu_q, sigma_q^2) is given by:\n    D(P || Q) = 0.5 * (log(sigma_q^2 / sigma_p^2) + (sigma_p^2 + (mu_p - mu_q)^2) / sigma_q^2 - 1)\n\n    Args:\n        mu_p (float): Mean of the first normal distribution P.\n        sigma_p (float): Standard deviation of the first normal distribution P.\n        mu_q (float): Mean of the second normal distribution Q.\n        sigma_q (float): Standard deviation of the second normal distribution Q.\n\n    Returns:\n        float: The KL divergence D(P || Q).\n               Returns np.nan if sigma_p or sigma_q are non-positive,\n               as standard deviation must be positive for a normal distribution.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        return np.nan\n    sigma_p_sq = sigma_p ** 2\n    sigma_q_sq = sigma_q ** 2\n    mu_diff_sq = (mu_p - mu_q) ** 2\n    kl_div = 0.5 * (np.log(sigma_q_sq / sigma_p_sq) + (sigma_p_sq + mu_diff_sq) / sigma_q_sq - 1)\n    return kl_div"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef tanh(x):\n    \"\"\"Tanh activation function.\"\"\"\n    return np.tanh(x)\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states at each time step,\n        as well as the final hidden state and final cell state.\n\n        Args:\n            x (np.ndarray): Input sequence of shape (sequence_length, input_size).\n            initial_hidden_state (np.ndarray): Initial hidden state of shape (hidden_size, 1).\n            initial_cell_state (np.ndarray): Initial cell state of shape (hidden_size, 1).\n\n        Returns:\n            tuple: A tuple containing:\n                - hidden_states_list (list): List of hidden states at each time step,\n                                             each element is a list of shape (hidden_size, 1)\n                                             converted to list and rounded.\n                - final_hidden_state (list): The final hidden state, a list of shape (hidden_size, 1)\n                                             converted to list and rounded.\n                - final_cell_state (list): The final cell state, a list of shape (hidden_size, 1)\n                                           converted to list and rounded.\n        \"\"\"\n        sequence_length = x.shape[0]\n        ht = initial_hidden_state\n        ct = initial_cell_state\n        hidden_states_at_each_step = []\n        for t in range(sequence_length):\n            xt = x[t].reshape(-1, 1)\n            combined_input = np.vstack((ht, xt))\n            ft = sigmoid(np.dot(self.Wf, combined_input) + self.bf)\n            it = sigmoid(np.dot(self.Wi, combined_input) + self.bi)\n            ct_candidate = tanh(np.dot(self.Wc, combined_input) + self.bc)\n            ct"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels (binary, e.g., 0 or 1)\n    :param y_pred: Numpy array of predicted labels (binary, e.g., 0 or 1)\n    :param beta: The weight of precision in the harmonic mean. beta > 0.\n                 beta=1 gives the F1-Score.\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    beta_squared = beta ** 2\n    denominator = beta_squared * precision + recall\n    if denominator == 0:\n        f_score_val = 0.0\n    else:\n        f_score_val = (1 + beta_squared) * (precision * recall) / denominator\n    return round(f_score_val, 3)"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    y_np = np.array(y)\n    (classes, counts) = np.unique(y_np, return_counts=True)\n    total_instances = len(y_np)\n    proportions = counts / total_instances\n    sum_sq_proportions = np.sum(proportions ** 2)\n    gini = 1 - sum_sq_proportions\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointers = []\n    nnz_count = 0\n    row_pointers.append(0)\n    for row in dense_matrix:\n        for (j, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                col_indices.append(j)\n                nnz_count += 1\n        row_pointers.append(nnz_count)\n    return (values, col_indices, row_pointers)"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix:\n        return ([], [], [0])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for j in range(num_cols):\n        for i in range(num_rows):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                row_indices.append(i)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculates the basis for the column space (image) of a matrix A.\n\n    Args:\n        A: A NumPy array representing the matrix.\n\n    Returns:\n        A list of lists representing the basis vectors for the column space\n        of A, rounded to 8 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    (m, n) = A.shape\n    A_copy = A.copy()\n    pivot_columns = []\n    r = 0\n    for j in range(n):\n        pivot_row = r\n        for i in range(r + 1, m):\n            if abs(A_copy[i, j]) > abs(A_copy[pivot_row, j]):\n                pivot_row = i\n        if abs(A_copy[pivot_row, j]) > 1e-09:\n            A_copy[[r, pivot_row]] = A_copy[[pivot_row, r]]\n            pivot_element = A_copy[r, j]\n            A_copy[r, :] /= pivot_element\n            for i in range(m):\n                if i != r:\n                    factor = A_copy[i, j]\n                    A_copy[i, :] -= factor * A_copy[r, :]\n            pivot_columns.append(j)\n            r += 1\n            if r == m:\n                break\n    basis_vectors = A[:, pivot_columns]\n    return np.round(basis_vectors, 8).tolist()"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculates the R-squared value for regression analysis.\n\n    Args:\n        y_true (np.ndarray): Array of true values.\n        y_pred (np.ndarray): Array of predicted values.\n\n    Returns:\n        float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    sst = np.sum((y_true - np.mean(y_true)) ** 2)\n    ssr = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - ssr / sst\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image.\n\n    Args:\n        img: A 2D list representing the grayscale image.\n\n    Returns:\n        The average brightness rounded to two decimal places, or -1 if\n        there are edge cases (empty image, inconsistent row lengths,\n        invalid pixel values).\n    \"\"\"\n    if not img:\n        return -1\n    num_rows = len(img)\n    num_cols = len(img[0]) if num_rows > 0 else 0\n    if num_cols == 0 and num_rows > 0:\n        return -1\n    total_brightness = 0\n    num_pixels = 0\n    for row in img:\n        if len(row) != num_cols:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    if num_pixels == 0:\n        return -1\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculates the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Args:\n        y_true (array-like): Array of actual values.\n        y_pred (array-like): Array of predicted values.\n\n    Returns:\n        float: The calculated RMSE rounded to three decimal places.\n\n    Raises:\n        ValueError: If input arrays have different shapes or are empty.\n        TypeError: If input arrays contain non-numeric elements that cannot be\n                   converted to a numeric type by numpy.\n    \"\"\"\n    try:\n        y_true_np = np.asarray(y_true, dtype=float)\n        y_pred_np = np.asarray(y_pred, dtype=float)\n    except ValueError:\n        raise TypeError('Input arrays must contain numeric values.')\n    if y_true_np.shape != y_pred_np.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if y_true_np.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    squared_errors = (y_true_np - y_pred_np) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    rmse_value = np.sqrt(mean_squared_error)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index between two binary arrays.\n\n    Args:\n        y_true (np.ndarray): True binary labels.\n        y_pred (np.ndarray): Predicted binary labels.\n\n    Returns:\n        float: The Jaccard Index, rounded to three decimal places.\n    \"\"\"\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    else:\n        return round(intersection / union, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Dice Score (S\u00f8rensen-Dice coefficient or F1-score) for binary classification.\n\n    Args:\n        y_true (np.ndarray): A binary numpy array representing the true labels.\n        y_pred (np.ndarray): A binary numpy array representing the predicted labels.\n\n    Returns:\n        float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true).astype(np.bool_)\n    y_pred = np.asarray(y_pred).astype(np.bool_)\n    intersection = np.sum(y_true & y_pred)\n    sum_of_sets = np.sum(y_true) + np.sum(y_pred)\n    if sum_of_sets == 0:\n        return 1.0\n    dice = 2.0 * intersection / sum_of_sets\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Creates a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Each feature in the row is represented by binding hypervectors for the feature name and its value.\n    The hypervectors for the values are created using the same feature seed provided in the `random_seeds`\n    dictionary to ensure reproducibility.\n    All feature hypervectors are then bundled to create a composite hypervector for the row.\n\n    Args:\n        row: A dictionary representing a dataset row, where keys are feature names and values are their\n             corresponding values.\n        dim: The dimensionality of the hypervectors.\n        random_seeds: A dictionary where keys are feature names and values are seeds to ensure\n                      reproducibility of hypervectors.\n\n    Returns:\n        A composite hypervector representing the entire row, converted to a list.\n    \"\"\"\n    row_hv = np.zeros(dim, dtype=int)\n    for (feature_name, feature_value) in row.items():\n        np.random.seed(hash(feature_name) % (2 ** 32 - 1))\n        feature_name_hv = np.random.randint(0, 2, dim) * 2 - 1\n        if feature_name in random_seeds:\n            np.random.seed(random_seeds[feature_name])\n            feature_value_hv = np.random.randint(0, 2, dim) * 2 - 1\n        else:\n            np.random.seed(hash(str(feature_value)) % (2 ** 32 - 1))\n            feature_value_hv = np.random.randint(0, 2, dim) * 2 - 1\n        feature_hv = feature_name_hv * feature_value_hv\n        row_hv += feature_hv\n    row_hv = np.where(row_hv > 0, 1, -1)\n    return row_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for a binary classification problem.\n\n    Args:\n        data: A list of lists, where each inner list represents a pair\n              [y_true, y_pred] for one observation.\n\n    Returns:\n        A 2x2 confusion matrix represented as a list of lists.\n        The matrix is structured as:\n        [[True Negative, False Positive],\n         [False Negative, True Positive]]\n    \"\"\"\n    true_positive = 0\n    true_negative = 0\n    false_positive = 0\n    false_negative = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            true_positive += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negative += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positive += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negative += 1\n    matrix = [[true_negative, false_positive], [false_negative, true_positive]]\n    return matrix"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculates the cosine similarity between two vectors.\n\n    Args:\n        v1: A numpy array representing the first vector.\n        v2: A numpy array representing the second vector.\n\n    Returns:\n        A float representing the cosine similarity, rounded to three decimal places.\n\n    Raises:\n        ValueError: If the input vectors have different shapes, are empty,\n                    or have zero magnitude.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculates various performance metrics for a binary classification problem.\n\n    Args:\n        actual: The actual class labels (1 for positive, 0 for negative).\n        predicted: The predicted class labels from the model.\n\n    Returns:\n        A tuple containing:\n            - confusion_matrix: A 2x2 matrix.\n            - accuracy: A float representing the accuracy of the model.\n            - f1_score: A float representing the F1 score of the model.\n            - specificity: A float representing the specificity of the model.\n            - negative_predictive_value: A float representing the negative predictive value.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('Actual and predicted lists must have the same length.')\n    for label in actual + predicted:\n        if label not in [0, 1]:\n            raise ValueError('Labels must be either 0 or 1.')\n    tp = 0\n    tn = 0\n    fp = 0\n    fn = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            tp += 1\n        elif a == 0 and p == 0:\n            tn += 1\n        elif a == 0 and p == 1:\n            fp += 1\n        elif a == 1 and p == 0:\n            fn += 1\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    total_samples = len(actual)\n    accuracy = (tp + tn) / total_samples if total_samples > 0 else 0.0\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0.0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0.0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates various descriptive statistics metrics for a given dataset.\n\n    Args:\n        data: A list or NumPy array of numerical values.\n\n    Returns:\n        A dictionary containing mean, median, mode, variance, standard deviation,\n        percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n    data = np.asarray(data)\n    if data.size == 0:\n        return {'mean': None, 'median': None, 'mode': None, 'variance': None, 'standard_deviation': None, '25th_percentile': None, '50th_percentile': None, '75th_percentile': None, 'interquartile_range': None}\n    mean = np.mean(data)\n    median = np.median(data)\n    counts = Counter(data)\n    if counts:\n        max_count = max(counts.values())\n        mode = [item for (item, count) in counts.items() if count == max_count]\n        if len(mode) == len(data):\n            mode = None\n        elif len(mode) == 1:\n            mode = mode[0]\n    else:\n        mode = None\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    q1 = np.percentile(data, 25)\n    q2 = np.percentile(data, 50)\n    q3 = np.percentile(data, 75)\n    iqr = q3 - q1\n    stats = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(q1, 4), '50th_percentile': round(q2, 4), '75th_percentile': round(q3, 4), 'interquartile_range': round(iqr, 4)}\n    return stats"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    if not 0 <= k <= n:\n        return 0.0\n    n_choose_k = math.comb(n, k)\n    prob_successes = p ** k\n    prob_failures = (1 - p) ** (n - k)\n    probability = n_choose_k * prob_successes * prob_failures\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be positive.')\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = 1 / denominator * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    if k < 0 or not isinstance(k, int):\n        raise ValueError('k must be a non-negative integer.')\n    if lam < 0:\n        raise ValueError('lam must be non-negative.')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise TypeError('Input must be a numpy array.')\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D grayscale image.')\n    if img.dtype != np.uint8:\n        pass\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = float(max_pixel - min_pixel)\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    Raises:\n        ValueError: If the input arrays are not 1D or do not have the same length.\n    \"\"\"\n    if not (isinstance(vec1, np.ndarray) and isinstance(vec2, np.ndarray)):\n        raise TypeError('Inputs must be NumPy arrays.')\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Input arrays must be 1D.')\n    if vec1.shape != vec2.shape:\n        raise ValueError('Input arrays must have the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        transformed_point = [round(x ** i, 8) for i in range(degree + 1)]\n        transformed_data.append(transformed_point)\n    return transformed_data"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates positional encodings for a given position and model dimensionality.\n\n    Args:\n        position: The position in the sequence (integer).\n        d_model: The dimensionality of the model (integer).\n\n    Returns:\n        A list of positional encodings (np.float16) for the given position,\n        or -1 if position is 0 or d_model is less than or equal to 0.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((d_model,), dtype=np.float32)\n    for i in range(0, d_model, 2):\n        denominator = np.power(10000, 2 * i / d_model)\n        pe[i] = np.sin(position / denominator)\n        if i + 1 < d_model:\n            pe[i + 1] = np.cos(position / denominator)\n    return pe.astype(np.float16).tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(updated_parameter, 5)\n    updated_m = np.round(m, 5)\n    updated_v = np.round(v, 5)\n    return (updated_parameter.tolist(), updated_m.tolist(), updated_v.tolist())"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    queries = np.array(crystal_values).reshape(-1, 1)\n    keys = np.array(crystal_values).reshape(-1, 1)\n    values = np.array(crystal_values).reshape(-1, 1)\n    attention_scores = np.dot(queries, keys.T)\n    scaled_attention_scores = attention_scores / np.sqrt(dimension)\n    attention_weights = np.apply_along_axis(softmax, axis=1, arr=scaled_attention_scores)\n    weighted_patterns = np.dot(attention_weights, values)\n    flattened_patterns = weighted_patterns.flatten()\n    rounded_patterns = [round(float(pattern), 4) for pattern in flattened_patterns]\n    return rounded_patterns"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculates BM25 scores for each document in a corpus given a query.\n\n    Args:\n        corpus: A list of strings, where each string is a document.\n        query: A string representing the query.\n        k1: A parameter controlling term frequency saturation (default is 1.5).\n        b: A parameter controlling document length normalization (default is 0.75).\n\n    Returns:\n        A list of floats, where each float is the BM25 score of the corresponding\n        document in the corpus, rounded to three decimal places.\n    \"\"\"\n    N = len(corpus)\n    avgdl = np.mean([len(doc.split()) for doc in corpus])\n    tokenized_corpus = [doc.split() for doc in corpus]\n    tokenized_query = query.split()\n    idf = {}\n    for term in tokenized_query:\n        doc_count = sum((1 for doc in tokenized_corpus if term in doc))\n        idf[term] = np.log((N - doc_count + 0.5) / (doc_count + 0.5) + 1)\n    scores = []\n    for (i, doc) in enumerate(tokenized_corpus):\n        doc_score = 0\n        doc_len = len(doc)\n        term_counts = Counter(doc)\n        for term in tokenized_query:\n            if term in term_counts:\n                tf = term_counts[term]\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * (doc_len / avgdl))\n                doc_score += idf.get(term, 0) * (numerator / denominator)\n        scores.append(round(doc_score, 3))\n    return scores"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input lists must have the same length.')\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (true_label, pred_label) in zip(y_true, y_pred):\n        if true_label == 1 and pred_label == 1:\n            true_positives += 1\n        elif true_label == 0 and pred_label == 1:\n            false_positives += 1\n        elif true_label == 1 and pred_label == 0:\n            false_negatives += 1\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0.0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    return round(f1_score, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nimport numpy as np\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data):\n        day = i + 1\n        fluctuation = 10 * math.sin(2 * PI * day / 10)\n        detrended_data.append(consumption - fluctuation)\n    days = np.arange(1, len(consumption_data) + 1).reshape(-1, 1)\n    detrended_data_np = np.array(detrended_data)\n    (slope, intercept) = np.polyfit(days.flatten(), detrended_data_np, 1)\n    day_15 = 15\n    predicted_base_consumption_day_15 = slope * day_15 + intercept\n    fluctuation_day_15 = 10 * math.sin(2 * PI * day_15 / 10)\n    predicted_total_consumption_day_15 = predicted_base_consumption_day_15 + fluctuation_day_15\n    rounded_prediction = round(predicted_total_consumption_day_15)\n    safety_margin = math.ceil(rounded_prediction * 0.05)\n    final_forecast = rounded_prediction + safety_margin\n    return int(final_forecast)"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Inputs must be numpy arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention output.\n\n    Args:\n        Q: Query matrix (batch_size, seq_len, d_k).\n        K: Key matrix (batch_size, seq_len, d_k).\n        V: Value matrix (batch_size, seq_len, d_v).\n\n    Returns:\n        The self-attention output matrix (batch_size, seq_len, d_v).\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements the multi-head attention mechanism.\n\n    Args:\n        X: Input matrix (batch_size, seq_len, d_model).\n        W_q: Weight matrix for Query (d_model, d_model).\n        W_k: Weight matrix for Key (d_model, d_model).\n        W_v: Weight matrix for Value (d_model, d_model).\n        n_heads: Number of attention heads.\n\n    Returns:\n        A list representing the concatenated output of all attention heads\n        (batch_size, seq_len, d_model).\n    \"\"\"\n    (batch_size, seq_len, d_model) = X.shape\n    d_k = d_model // n_heads\n    d_v = d_model // n_heads\n    W_q_heads = W_q.reshape(d_model, n_heads, d_k)\n    W_k_heads = W_k.reshape(d_model, n_heads, d_k)\n    W_v_heads = W_v.reshape(d_model, n_heads, d_v)\n    outputs = []\n    for i in range(n_heads):\n        Q_head = np.dot(X, W_q_heads[:, i, :])\n        K_head = np.dot(X, W_k_heads[:, i, :])\n        V_head = np.dot(X, W_v_heads[:, i, :])\n        output_head = self_attention(Q_head, K_head, V_head)\n        outputs.append(output_head)\n    concatenated_output = np.concatenate(outputs, axis=-1)\n    rounded_output = np.round(concatenated_output, 4)\n    return rounded_output.tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "import math\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length.')\n    n11 = sum((1 for i in range(len(x)) if x[i] == 1 and y[i] == 1))\n    n10 = sum((1 for i in range(len(x)) if x[i] == 1 and y[i] == 0))\n    n01 = sum((1 for i in range(len(x)) if x[i] == 0 and y[i] == 1))\n    n00 = sum((1 for i in range(len(x)) if x[i] == 0 and y[i] == 0))\n    numerator = n11 * n00 - n10 * n01\n    denominator_squared = (n11 + n10) * (n01 + n00) * (n11 + n01) * (n10 + n00)\n    if denominator_squared == 0:\n        return 0.0\n    denominator = math.sqrt(denominator_squared)\n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 20:\n        return round(x, 4)\n    elif x < -20:\n        return round(0.0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    pi_theta_old = np.maximum(pi_theta_old, 1e-08)\n    pi_theta_ref = np.maximum(pi_theta_ref, 1e-08)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    policy_objective_term = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    kl_penalty_term = -beta * kl_divergence\n    grpo_obj = policy_objective_term + kl_penalty_term\n    return round(grpo_obj, 6)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x * (1 / (1 + math.exp(-x))), 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * (alpha * (math.exp(x) - 1)), 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_model = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_model))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_classes = len(np.unique(y))\n    X = np.hstack((np.ones((n_samples, 1)), X))\n    n_features += 1\n    B = np.zeros((n_features, n_classes))\n    losses = []\n    for i in range(iterations):\n        scores = np.dot(X, B)\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        softmax_probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        one_hot_y = np.zeros((n_samples, n_classes))\n        one_hot_y[np.arange(n_samples), y] = 1\n        loss = -np.mean(np.sum(one_hot_y * np.log(softmax_probs + 1e-09), axis=1))\n        losses.append(round(loss, 4))\n        gradient = -np.dot(X.T, one_hot_y - softmax_probs) / n_samples\n        B -= learning_rate * gradient\n    B = np.round(B, 4)\n    return (B.tolist(), losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n\n    Args:\n        X: Input features (numpy array of shape (n_samples, n_features)).\n        y: Target labels (numpy array of shape (n_samples,)).\n        learning_rate: The learning rate for gradient descent.\n        iterations: The number of iterations for gradient descent.\n\n    Returns:\n        A tuple containing:\n            - A list of the optimized coefficients (including the intercept).\n            - A list of the loss values collected over iterations.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    X = np.insert(X, 0, 1, axis=1)\n    weights = np.zeros(n_features + 1)\n    loss_history = []\n    for i in range(iterations):\n        linear_model = X @ weights\n        y_predicted = 1 / (1 + np.exp(-linear_model))\n        epsilon = 1e-15\n        loss = -np.mean(y * np.log(y_predicted + epsilon) + (1 - y) * np.log(1 - y_predicted + epsilon))\n        loss_history.append(round(loss, 4))\n        gradient = 1 / n_samples * X.T @ (y_predicted - y)\n        weights -= learning_rate * gradient\n    rounded_weights = [round(w, 4) for w in weights]\n    return (rounded_weights, loss_history)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T)\n    d_k = Q.shape[-1]\n    scaled_scores = scores / np.sqrt(d_k)\n    masked_scores = scaled_scores + (1 - mask) * -1000000000.0\n    attention_weights = np.exp(masked_scores - np.max(masked_scores, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Uses Shannon entropy as the measure of disorder.\n    \"\"\"\n    n = len(apples)\n    if n == 0:\n        return 0.0\n    color_counts = Counter(apples)\n    disorder_value = 0.0\n    for count in color_counts.values():\n        probability = count / n\n        if probability > 0:\n            disorder_value -= probability * math.log2(probability)\n    return round(disorder_value, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Layer Normalization on an input tensor.\n\n    Args:\n        X: Input tensor of shape (batch_size, sequence_length, feature_dimension).\n        gamma: Scaling parameter of shape (feature_dimension,).\n        beta: Shifting parameter of shape (feature_dimension,).\n        epsilon: A small constant to avoid division by zero.\n\n    Returns:\n        A list representing the normalized tensor, rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n    output = gamma * normalized_X + beta\n    return np.round(output, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Computes the METEOR score for evaluating machine translation quality.\n\n    Args:\n        reference (str): The reference translation.\n        candidate (str): The candidate translation.\n        alpha (float): Parameter for the F-mean calculation (default: 0.9).\n        beta (float): Parameter for the penalty calculation (default: 3).\n        gamma (float): Parameter for the penalty calculation (default: 0.5).\n\n    Returns:\n        float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    ref_tokens = reference.lower().split()\n    cand_tokens = candidate.lower().split()\n    ref_counts = Counter(ref_tokens)\n    cand_counts = Counter(cand_tokens)\n    matches = 0\n    matched_cand_indices = set()\n    matched_ref_indices = set()\n    for (i, token) in enumerate(cand_tokens):\n        if token in ref_counts and ref_counts[token] > 0:\n            try:\n                ref_index = ref_tokens.index(token, 0 if not matched_ref_indices else max(matched_ref_indices) + 1)\n                if ref_index not in matched_ref_indices:\n                    matches += 1\n                    ref_counts[token] -= 1\n                    matched_cand_indices.add(i)\n                    matched_ref_indices.add(ref_index)\n            except ValueError:\n                pass\n    precision = matches / len(cand_tokens) if len(cand_tokens) > 0 else 0\n    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0\n    f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall) if alpha * precision + (1 - alpha) * recall > 0 else 0\n    chunks = 0\n    if matches > 0:\n        sorted_matched_cand_indices = sorted(list(matched_cand_indices))\n        if sorted_matched_cand_indices:\n            chunks = 1\n            for i in range(1, len(sorted_matched_cand_indices)):\n                if sorted_matched_cand_indices[i] != sorted_matched_cand_indices[i - 1] + 1:\n                    chunks += 1\n    penalty = gamma * (chunks / matches) ** beta if matches > 0 else 0\n    meteor = f_mean * (1 - penalty)\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) between two events.\n\n    Args:\n        joint_counts: The number of times both events occurred together.\n        total_counts_x: The number of times the first event occurred.\n        total_counts_y: The number of times the second event occurred.\n        total_samples: The total number of samples or observations.\n\n    Returns:\n        The Pointwise Mutual Information (PMI) as a float, rounded to 3 decimal places.\n        Returns 0.0 if any of the counts are zero to avoid division by zero or log(0).\n    \"\"\"\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0 or (total_samples == 0):\n        return 0.0\n    prob_xy = joint_counts / total_samples\n    prob_x = total_counts_x / total_samples\n    prob_y = total_counts_y / total_samples\n    expected_prob_xy = prob_x * prob_y\n    if expected_prob_xy == 0:\n        return 0.0\n    pmi = np.log2(prob_xy / expected_prob_xy)\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Performs Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n\n    Args:\n        x: A list of integers.\n\n    Returns:\n        A list of floats representing the normalized values, rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized_x = []\n    for val in x:\n        normalized_val = (val - min_val) / (max_val - min_val)\n        normalized_x.append(round(normalized_val, 4))\n    return normalized_x"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with a shortcut connection using NumPy.\n\n    Args:\n        x: A 1D NumPy array representing the input.\n        w1: A 2D NumPy array representing the weights for the first layer.\n        w2: A 2D NumPy array representing the weights for the second layer.\n\n    Returns:\n        A list representing the output of the residual block, rounded to 4 decimal places.\n    \"\"\"\n    if x.ndim != 1:\n        raise ValueError(\"Input 'x' must be a 1D array.\")\n    if w1.shape[1] != x.shape[0]:\n        raise ValueError(f'Weight matrix w1 has incompatible dimensions. Expected {x.shape[0]} columns, got {w1.shape[1]}.')\n    if w2.shape[1] != w1.shape[0]:\n        raise ValueError(f'Weight matrix w2 has incompatible dimensions. Expected {w1.shape[0]} columns, got {w2.shape[1]}.')\n    if w2.shape[0] != x.shape[0]:\n        raise ValueError(f'Output dimension of w2 ({w2.shape[0]}) must match input dimension of x ({x.shape[0]}) for shortcut connection.')\n    h1 = np.dot(w1, x)\n    h1_relu = np.maximum(0, h1)\n    h2 = np.dot(w2, h1_relu)\n    output_before_relu = h2 + x\n    output = np.maximum(0, output_before_relu)\n    return np.round(output, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n\n    Args:\n        x: A NumPy array of shape (height, width, channels) representing feature maps.\n\n    Returns:\n        A 1D NumPy array of shape (channels,) where each element is the average\n        of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input array must be 3-dimensional (height, width, channels)')\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Batch Normalization on a 4D NumPy array in BCHW format.\n\n    Args:\n        X: Input NumPy array of shape (batch, channels, height, width).\n        gamma: Scale parameter (NumPy array of shape (channels,)).\n        beta: Shift parameter (NumPy array of shape (channels,)).\n        epsilon: Small value for numerical stability.\n\n    Returns:\n        A list representing the normalized output array, rounded to 4 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    gamma_reshaped = gamma.reshape(1, -1, 1, 1)\n    beta_reshaped = beta.reshape(1, -1, 1, 1)\n    output = gamma_reshaped * X_normalized + beta_reshaped\n    return np.round(output, 4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n\n    Args:\n        c: The coefficient of the polynomial term.\n        x: The point at which to evaluate the derivative.\n        n: The exponent of the polynomial term.\n\n    Returns:\n        The value of the derivative of the polynomial term at x, rounded to 4 decimal places.\n    \"\"\"\n    if n == 0:\n        derivative_value = 0.0\n    else:\n        derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n\n    Args:\n        vectors: A list of 2D vectors (list of lists of floats).\n        tol: A tolerance value to determine linear independence.\n\n    Returns:\n        A list of orthonormal vectors (list of lists of floats) that span the\n        same subspace, rounded to 4 decimal places.\n    \"\"\"\n    vectors_np = [np.array(v) for v in vectors]\n    orthonormal_basis_list = []\n    for v in vectors_np:\n        u = v.copy()\n        for b in orthonormal_basis_list:\n            projection = np.dot(v, b) * b\n            u -= projection\n        if np.linalg.norm(u) > tol:\n            orthonormal_basis_list.append(u / np.linalg.norm(u))\n    rounded_basis = [np.round(b, 4).tolist() for b in orthonormal_basis_list]\n    return rounded_basis"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3-dimensional vectors.\n\n    Args:\n        a (list or np.ndarray): The first 3D vector.\n        b (list or np.ndarray): The second 3D vector.\n\n    Returns:\n        list: The cross product vector, rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError('Input vectors must be 3-dimensional.')\n    cross_prod = np.cross(a, b)\n    rounded_cross_prod = np.round(cross_prod, 4)\n    return rounded_cross_prod.tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n\n    Args:\n        A: A square numpy array representing the coefficient matrix.\n        b: A numpy array representing the constant vector.\n\n    Returns:\n        A list representing the solution vector x, rounded to 4 decimal places,\n        or -1 if the determinant of A is zero.\n    \"\"\"\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    n = A.shape[0]\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    return np.round(x, 4).tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two discrete probability distributions.\n\n    Args:\n        p: A list of floats representing the first probability distribution.\n        q: A list of floats representing the second probability distribution.\n\n    Returns:\n        The Bhattacharyya distance rounded to 4 decimal places, or 0.0 if the\n        inputs have different lengths or are empty.\n    \"\"\"\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    bc = np.sum(np.sqrt(np.array(p) * np.array(q)))\n    if bc <= 0:\n        return 0.0\n    else:\n        distance = -np.log(bc)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Args:\n        a: The first vector (list of numbers).\n        b: The second vector (list of numbers).\n\n    Returns:\n        A new vector representing the sum if the dimensions are compatible,\n        otherwise -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = []\n    for i in range(len(a)):\n        result.append(a[i] + b[i])\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Computes the policy gradient using the REINFORCE algorithm.\n\n    Args:\n        theta: A 2D NumPy array of shape (num_states, num_actions) representing\n               the policy parameters.\n        episodes: A list of episodes, where each episode is a list of\n                  (state, action, reward) tuples.\n\n    Returns:\n        A NumPy array of the same shape as theta, representing the average\n        policy gradient.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    num_episodes = len(episodes)\n    for episode in episodes:\n        episode_gradient = np.zeros_like(theta)\n        returns = []\n        G = 0\n        for t in reversed(range(len(episode))):\n            (state, action, reward) = episode[t]\n            G = reward + G\n            returns.insert(0, G)\n        for t in range(len(episode)):\n            (state, action, reward) = episode[t]\n            G_t = returns[t]\n            exp_theta = np.exp(theta[state, :] - np.max(theta[state, :]))\n            policy_probs = exp_theta / np.sum(exp_theta)\n            log_policy_gradient = np.zeros(num_actions)\n            log_policy_gradient[action] = 1 - policy_probs[action]\n            log_policy_gradient -= policy_probs\n            episode_gradient[state, :] += log_policy_gradient * G_t\n        gradient += episode_gradient\n    average_gradient = gradient / num_episodes\n    rounded_gradient = np.round(average_gradient, 4)\n    return rounded_gradient.tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculates the computational cost savings of an MoE layer compared to a dense layer.\n\n    Args:\n        n_experts (int): The total number of experts in the MoE layer.\n        k_active (int): The number of active experts per input.\n        d_in (int): The input dimension.\n        d_out (int): The output dimension.\n\n    Returns:\n        tuple: A tuple containing:\n            - float: FLOPs for the dense layer.\n            - float: FLOPs for the MoE layer.\n            - float: Percentage of FLOPs saved by the MoE layer.\n    \"\"\"\n    dense_flops = d_in * d_out * 2\n    moe_flops = k_active * d_in * d_out * 2\n    flops_saved = dense_flops - moe_flops\n    savings_percentage = flops_saved / dense_flops * 100 if dense_flops > 0 else 0\n    return (round(dense_flops, 1), round(moe_flops, 1), round(savings_percentage, 1))"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism for Mixture-of-Experts.\n\n    Args:\n        X: Input matrix (batch_size, input_dim).\n        W_g: Weight matrix for the gating function (input_dim, num_experts).\n        W_noise: Weight matrix for the noise (input_dim, num_experts).\n        N: Pre-sampled noise matrix (batch_size, num_experts).\n        k: Sparsity constraint (number of experts to select).\n\n    Returns:\n        A list representing the gating probabilities matrix (batch_size, num_experts),\n        rounded to the nearest 4th decimal.\n    \"\"\"\n    clean_logits = np.dot(X, W_g)\n    noisy_logits = clean_logits + np.dot(X, W_noise) * N\n    topk_indices = np.argsort(noisy_logits, axis=-1)[:, -k:]\n    mask = np.zeros_like(noisy_logits, dtype=bool)\n    for i in range(mask.shape[0]):\n        mask[i, topk_indices[i]] = True\n    masked_logits = np.where(mask, clean_logits, -np.inf)\n    max_logits = np.max(masked_logits, axis=-1, keepdims=True)\n    exp_logits = np.exp(masked_logits - max_logits)\n    sum_exp_logits = np.sum(exp_logits, axis=-1, keepdims=True)\n    gating_probabilities = exp_logits / sum_exp_logits\n    gating_probabilities = np.nan_to_num(gating_probabilities, nan=0.0)\n    gating_probabilities = np.round(gating_probabilities, 4)\n    return gating_probabilities.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Sparse Mixture of Experts (MoE) layer with softmax gating and top-k routing.\n\n    Args:\n        x: Input tensor of shape (batch_size, sequence_length, input_dim).\n        We: Expert weight matrices of shape (n_experts, input_dim, output_dim).\n        Wg: Gating weight matrix of shape (input_dim, n_experts).\n        n_experts: The total number of experts.\n        top_k: The number of top experts to select for each token.\n\n    Returns:\n        A list representing the output tensor of shape (batch_size, sequence_length, output_dim),\n        with values rounded to the nearest 4th decimal.\n    \"\"\"\n    (batch_size, sequence_length, input_dim) = x.shape\n    output_dim = We.shape[2]\n    x_flat = x.reshape(-1, input_dim)\n    num_tokens = x_flat.shape[0]\n    gating_scores = np.dot(x_flat, Wg)\n    gating_probs = np.exp(gating_scores - np.max(gating_scores, axis=-1, keepdims=True))\n    gating_probs = gating_probs / np.sum(gating_probs, axis=-1, keepdims=True)\n    top_k_indices = np.argsort(gating_probs, axis=-1)[:, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=-1)\n    top_k_probs_normalized = top_k_probs / np.sum(top_k_probs, axis=-1, keepdims=True)\n    output = np.zeros((num_tokens, output_dim))\n    for i in range(num_tokens):\n        for j in range(top_k):\n            expert_index = top_k_indices[i, j]\n            expert_weight = We[expert_index]\n            expert_output = np.dot(x_flat[i], expert_weight)\n            output[i] += top_k_probs_normalized[i, j] * expert_output\n    output = output.reshape(batch_size, sequence_length, output_dim)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Performs Group Normalization on a 4D input tensor.\n\n    Args:\n        X: Input tensor of shape (B, C, H, W).\n        gamma: Learned scale parameter of shape (C,).\n        beta: Learned shift parameter of shape (C,).\n        num_groups: Number of groups to divide channels into.\n        epsilon: Small value to avoid division by zero.\n\n    Returns:\n        Normalized tensor as a Python list after reshaping and rounding.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by the number of groups.'\n    channels_per_group = C // num_groups\n    X_reshaped = X.reshape(B, num_groups, channels_per_group, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    variance = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(variance + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    gamma_reshaped = gamma.reshape(1, C, 1, 1)\n    beta_reshaped = beta.reshape(1, C, 1, 1)\n    output = gamma_reshaped * X_normalized + beta_reshaped\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Finds the minimum of the function f(x) = x^4 - 3x^3 + 2 using gradient descent.\n\n    Args:\n        start_x: The initial guess for the value of x.\n        learning_rate: The step size for each iteration of gradient descent.\n        tolerance: The convergence criterion. The algorithm stops when the absolute\n                   change in x is less than this value.\n        max_iters: The maximum number of iterations to perform.\n\n    Returns:\n        The value of x where the function f(x) is minimized, rounded to 4 decimal places.\n    \"\"\"\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        gradient = df(x)\n        new_x = x - learning_rate * gradient\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) activation function.\n\n    Args:\n        x: Input numpy array.\n        alpha: Scaling parameter.\n        gamma: Shifting parameter.\n        beta: Slope parameter.\n\n    Returns:\n        A list of floats representing the output of the DyT function,\n        rounded to the nearest 4th decimal.\n    \"\"\"\n    dy_tanh = alpha * np.tanh(beta * x + gamma)\n    rounded_dy_tanh = np.round(dy_tanh, 4)\n    return rounded_dy_tanh.tolist()"}
