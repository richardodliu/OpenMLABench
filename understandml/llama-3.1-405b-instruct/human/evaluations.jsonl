{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Compute the dot product of a matrix and a vector.\n\n    Args:\n    a (list[list[int|float]]): A 2D list representing the matrix.\n    b (list[int|float]): A list representing the vector.\n\n    Returns:\n    list[int|float]: A list representing the resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function computes the transpose of a given matrix.\n\n    Args:\n        a (list[list[int|float]]): A 2D list representing the matrix.\n\n    Returns:\n        list[list[int|float]]: The transpose of the input matrix.\n    \"\"\"\n    transposed = list(zip(*a))\n    transposed = [list(row) for row in transposed]\n    return transposed\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ntransposed = transpose_matrix(matrix)\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given matrix into a specified shape.\n\n    Args:\n    a (list[list[int|float]]): The input matrix.\n    new_shape (tuple[int, int]): The desired shape of the output matrix.\n\n    Returns:\n    list[list[int|float]]: The reshaped matrix, or an empty list if the matrix cannot be reshaped.\n    \"\"\"\n    arr = np.array(a)\n    total_elements = arr.size\n    required_elements = new_shape[0] * new_shape[1]\n    if total_elements != required_elements:\n        return []\n    reshaped_arr = arr.reshape(new_shape)\n    return reshaped_arr.tolist()\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport statistics\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Args:\n    matrix (list[list[float]]): A 2D list of floats representing the matrix.\n    mode (str): A string indicating whether to calculate the mean by 'row' or 'column'.\n\n    Returns:\n    list[float]: A list of means according to the specified mode.\n\n    Raises:\n    ValueError: If the mode is neither 'row' nor 'column'.\n    \"\"\"\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")\n    if mode == 'row':\n        return [statistics.mean(row) for row in matrix]\n    elif mode == 'column':\n        transposed_matrix = list(map(list, zip(*matrix)))\n        return [statistics.mean(col) for col in transposed_matrix]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the result.\n\n    Args:\n        matrix (list[list[int|float]]): A 2D list representing the matrix.\n        scalar (int|float): The scalar value to multiply the matrix by.\n\n    Returns:\n        list[list[int|float]]: The resulting matrix after scalar multiplication.\n\n    Example:\n        >>> matrix = [[1, 2, 3], [4, 5, 6]]\n        >>> scalar = 2\n        >>> result = scalar_multiply(matrix, scalar)\n        >>> print(result)\n        [[2, 4, 6], [8, 10, 12]]\n    \"\"\"\n    if not matrix:\n        raise ValueError('Matrix cannot be empty')\n    result = [[0 for _ in range(len(matrix[0]))] for _ in range(len(matrix))]\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            result[i][j] = matrix[i][j] * scalar\n    return result\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float|int]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    list[float]: A list containing the eigenvalues of the matrix, sorted from highest to lowest.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2:\n        raise ValueError('Input matrix must be a 2x2 matrix')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    eigenvalue1 = (trace + math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue2 = (trace - math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function transforms a given matrix A using the operation T^-1AS.\n    \n    Args:\n    A (list[list[int|float]]): The input matrix to be transformed.\n    T (list[list[int|float]]): The transformation matrix T.\n    S (list[list[int|float]]): The transformation matrix S.\n    \n    Returns:\n    list[list[int|float]]: The transformed matrix T^-1AS. Returns -1 if T or S is not invertible.\n    \"\"\"\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    T_inv = np.linalg.inv(T)\n    transformed_matrix = np.dot(np.dot(T_inv, A), S)\n    transformed_matrix = np.round(transformed_matrix, 4)\n    return transformed_matrix.tolist()\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    list[list[float]]: The inverse of the input matrix, or None if the matrix is not invertible.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2:\n        raise ValueError('Input matrix must be a 2x2 matrix')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices.\n\n    Args:\n    a (list[list[int|float]]): The first matrix.\n    b (list[list[int|float]]): The second matrix.\n\n    Returns:\n    list[list[int|float]]: The product of the two matrices, or -1 if they cannot be multiplied.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": false, "result": "failed: name 'calculate_covariance_matrix' is not defined", "solution": "\nimport unittest\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve a system of linear equations using the Jacobi method.\n\n    Args:\n    A (np.ndarray): A 2D numpy array representing the coefficients of the linear equations.\n    b (np.ndarray): A 1D numpy array representing the constants of the linear equations.\n    n (int): The number of iterations.\n\n    Returns:\n    list: The approximate solution x after n iterations, rounded to four decimal places.\n    \"\"\"\n    num_equations = len(A)\n    x = np.zeros(num_equations)\n    for _ in range(n):\n        temp = np.zeros(num_equations)\n        for i in range(num_equations):\n            sum_other_terms = sum((A[i, j] * x[j] for j in range(num_equations) if j != i))\n            temp[i] = (b[i] - sum_other_terms) / A[i, i]\n        x = temp\n        x = np.round(x, 4)\n    return x.tolist()\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method.\n\n    Args:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the matrices U, \u03a3, and V^T.\n    \"\"\"\n    assert A.shape == (2, 2), 'Matrix A must be 2x2'\n    U = np.eye(2)\n    V = np.eye(2)\n    \u03a3 = np.zeros((2, 2))\n    for _ in range(100):\n        B = np.dot(np.dot(U.T, A), V)\n        (c1, s1) = _givens_rotation(B[0, 0], B[1, 0])\n        G1 = np.array([[c1, -s1], [s1, c1]])\n        (c2, s2) = _givens_rotation(B[0, 0], B[0, 1])\n        G2 = np.array([[c2, -s2], [s2, c2]])\n        U = np.dot(U, G1)\n        V = np.dot(V, G2)\n        \u03a3 = np.dot(np.dot(G1.T, B), G2)\n    U = np.round(U, 4)\n    \u03a3 = np.round(\u03a3, 4)\n    V = np.round(V, 4)\n    return (U, \u03a3, V.T)\ndef _givens_rotation(a, b):\n    \"\"\"\n    Computes the Givens rotation matrix.\n\n    Args:\n    a (float): The first element.\n    b (float): The second element.\n\n    Returns:\n    tuple: A tuple containing the cosine and sine of the rotation angle.\n    \"\"\"\n    if b == 0:\n        return (1, 0)\n    elif a == 0:\n        return (0, 1)\n    else:\n        r = np.sqrt(a ** 2 + b ** 2)\n        c = a / r\n        s = b / r\n        return (c, s)\nA = np.array([[1, 2], [3, 4]])\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion method.\n\n    Args:\n    matrix (list[list[int|float]]): A 4x4 matrix represented as a list of lists.\n\n    Returns:\n    float: The determinant of the matrix.\n    \"\"\"\n\n    def determinant_2x2(matrix: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculate the determinant of a 2x2 matrix.\n\n        Args:\n        matrix (list[list[int|float]]): A 2x2 matrix represented as a list of lists.\n\n        Returns:\n        float: The determinant of the matrix.\n        \"\"\"\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n\n    def determinant_3x3(matrix: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculate the determinant of a 3x3 matrix using Laplace's Expansion method.\n\n        Args:\n        matrix (list[list[int|float]]): A 3x3 matrix represented as a list of lists.\n\n        Returns:\n        float: The determinant of the matrix.\n        \"\"\"\n        det = 0\n        for i in range(3):\n            minor = [row[:i] + row[i + 1:] for row in matrix[1:]]\n            det_minor = determinant_2x2(minor)\n            det += (-1) ** i * matrix[0][i] * det_minor\n        return det\n    det = 0\n    for i in range(4):\n        minor = [row[:i] + row[i + 1:] for row in matrix[1:]]\n        det_minor = determinant_3x3(minor)\n        det += (-1) ** i * matrix[0][i] * det_minor\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]\ndet = determinant_4x4(matrix)\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": false, "result": "failed: Singular matrix", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    This function performs linear regression using the normal equation.\n\n    Parameters:\n    X (list[list[float]]): A 2D list of features.\n    y (list[float]): A list of target values.\n\n    Returns:\n    list[float]: A list of coefficients of the linear regression model.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n    coefficients = np.round(coefficients, 4)\n    return coefficients.tolist()\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Performs linear regression using gradient descent.\n\n    Parameters:\n    X (np.ndarray): Features with a column of ones for the intercept.\n    y (np.ndarray): Target variable.\n    alpha (float): Learning rate.\n    iterations (int): Number of iterations.\n\n    Returns:\n    np.ndarray: Coefficients of the linear regression model as a list.\n    \"\"\"\n    coefficients = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = np.dot(X, coefficients)\n        errors = predictions - y\n        gradients = 2 / X.shape[0] * np.dot(X.T, errors)\n        coefficients -= alpha * gradients\n    coefficients = np.round(coefficients, 4)\n    return coefficients.tolist()\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    This function performs feature scaling on a dataset using both standardization and min-max normalization.\n\n    Args:\n        data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n\n    Returns:\n        tuple[list[list[float]], list[list[float]]]: Two 2D lists, one scaled by standardization and one by min-max normalization.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    standardized_data = np.round(standardized_data, 4)\n    standardized_data = standardized_data.tolist()\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized_data = (data - min_val) / (max_val - min_val)\n    normalized_data = np.round(normalized_data, 4)\n    normalized_data = normalized_data.tolist()\n    return (standardized_data, normalized_data)\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": false, "result": "failed: too many values to unpack (expected 2)", "solution": "import numpy as np\nimport numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    This function implements the k-Means clustering algorithm.\n\n    Args:\n    points (list[tuple[float, float]]): A list of points, where each point is a tuple of coordinates (e.g., (x, y) for 2D points)\n    k (int): An integer representing the number of clusters to form\n    initial_centroids (list[tuple[float, float]]): A list of initial centroid points, each a tuple of coordinates\n    max_iterations (int): An integer representing the maximum number of iterations to perform\n\n    Returns:\n    list[tuple[float, float]]: A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n    \"\"\"\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points_array[:, np.newaxis] - centroids_array, axis=2)\n        cluster_assignments = np.argmin(distances, axis=1)\n        new_centroids = np.array([points_array[cluster_assignments == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids_array == new_centroids):\n            break\n        centroids_array = new_centroids\n    final_centroids = [(round(x, 4), round(y, 4)) for (x, y) in centroids_array]\n    return final_centroids\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Args:\n    X (np.ndarray): Feature matrix.\n    y (np.ndarray): Target vector.\n    k (int, optional): Number of folds. Defaults to 5.\n    shuffle (bool, optional): Whether to shuffle the data before splitting. Defaults to True.\n    random_seed (int, optional): Seed for random number generation. Defaults to None.\n\n    Returns:\n    list: List of tuples containing train and test indices for each fold.\n    \"\"\"\n    if k < 2:\n        raise ValueError('Number of folds must be at least 2')\n    if len(X) < k:\n        raise ValueError('Dataset size is too small for the number of folds')\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if shuffle:\n        indices = np.arange(len(X))\n        np.random.shuffle(indices)\n        X = X[indices]\n        y = y[indices]\n    fold_size = len(X) // k\n    remaining = len(X) % k\n    train_test_indices = []\n    start = 0\n    for i in range(k):\n        end = start + fold_size + (1 if i < remaining else 0)\n        test_indices = np.arange(start, end)\n        train_indices = np.concatenate((np.arange(0, start), np.arange(end, len(X))))\n        train_test_indices.append((train_indices, test_indices))\n        start = end\n    return train_test_indices\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\ny = np.array([0, 0, 1, 1, 1])\nk = 3\ntrain_test_indices = k_fold_cross_validation(X, y, k)\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the input data.\n\n    Args:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - k (int): The number of principal components to return.\n\n    Returns:\n    - principal_components (list[list[float]]): A list of k principal components, where each component is a list of floats representing the eigenvector.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = eigenvectors[:, :k].T\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Learn a decision tree from a list of examples.\n\n    Args:\n    - examples: A list of dictionaries, where each dictionary represents an example\n        and contains attribute-value pairs.\n    - attributes: A list of attribute names.\n    - target_attr: The name of the target attribute.\n\n    Returns:\n    - A nested dictionary representing the decision tree.\n    \"\"\"\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return {'label': target_values[0]}\n    if not attributes:\n        most_common_value = Counter(target_values).most_common(1)[0][0]\n        return {'label': most_common_value}\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    tree = {'attribute': best_attr, 'children': {}}\n    for value in get_attribute_values(examples, best_attr):\n        subset = [example for example in examples if example[best_attr] == value]\n        subtree = learn_decision_tree(subset, [attr for attr in attributes if attr != best_attr], target_attr)\n        tree['children'][value] = subtree\n    return tree\ndef choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n    \"\"\"\n    Choose the attribute with the highest information gain.\n\n    Args:\n    - examples: A list of dictionaries, where each dictionary represents an example\n        and contains attribute-value pairs.\n    - attributes: A list of attribute names.\n    - target_attr: The name of the target attribute.\n\n    Returns:\n    - The name of the attribute with the highest information gain.\n    \"\"\"\n    best_attr = None\n    best_gain = 0\n    for attr in attributes:\n        gain = information_gain(examples, attr, target_attr)\n        if gain > best_gain:\n            best_attr = attr\n            best_gain = gain\n    return best_attr\ndef information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n    \"\"\"\n    Calculate the information gain of an attribute.\n\n    Args:\n    - examples: A list of dictionaries, where each dictionary represents an example\n        and contains attribute-value pairs.\n    - attribute: The name of the attribute.\n    - target_attr: The name of the target attribute.\n\n    Returns:\n    - The information gain of the attribute.\n    \"\"\"\n    target_entropy = entropy([example[target_attr] for example in examples])\n    conditional_entropy = 0\n    for value in get_attribute_values(examples, attribute):\n        subset = [example for example in examples if example[attribute] == value]\n        conditional_entropy += len(subset) / len(examples) * entropy([example[target_attr] for example in subset])\n    gain = target_entropy - conditional_entropy\n    return gain\ndef entropy(values: list[str]) -> float:\n    \"\"\"\n    Calculate the entropy of a list of values.\n\n    Args:\n    - values: A list of values.\n\n    Returns:\n    - The entropy of the values.\n    \"\"\"\n    freqs = Counter(values)\n    entropy = 0\n    for freq in freqs.values():\n        prob = freq / len(values)\n        entropy -= prob * math.log2(prob)\n    return entropy\ndef get_attribute_values(examples: list[dict], attribute: str) -> list[str]:\n    \"\"\"\n    Get the unique values of an attribute.\n\n    Args:\n    - examples: A list of dictionaries, where each dictionary represents an example\n        and contains attribute-value pairs.\n    - attribute: The name of the attribute.\n\n    Returns:\n    - A list of unique values of the attribute.\n    \"\"\"\n    return list(set((example[attribute] for example in examples)))\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Train a kernel SVM classifier using the Pegasos algorithm.\n\n    Parameters:\n    data (np.ndarray): 2D NumPy array where each row represents a data sample and each column represents a feature.\n    labels (np.ndarray): 1D NumPy array where each entry corresponds to the label of the sample.\n    kernel (str): Choice of kernel (linear or RBF). Default is 'linear'.\n    lambda_val (float): Regularization parameter. Default is 0.01.\n    iterations (int): Number of iterations. Default is 100.\n    sigma (float): Parameter for RBF kernel. Default is 1.0.\n\n    Returns:\n    alpha (list): Model's alpha coefficients.\n    bias (float): Model's bias.\n    \"\"\"\n    alpha = np.zeros(data.shape[0])\n    bias = 0.0\n    if kernel == 'linear':\n        kernel_matrix = np.dot(data, data.T)\n    elif kernel == 'rbf':\n        kernel_matrix = np.exp(-np.linalg.norm(data[:, np.newaxis] - data, axis=2) ** 2 / (2 * sigma ** 2))\n    else:\n        raise ValueError('Invalid kernel choice')\n    for _ in range(iterations):\n        margin = np.dot(kernel_matrix, alpha * labels) + bias\n        alpha = (1 - lambda_val) * alpha + lambda_val * np.maximum(0, 1 - margin * labels) * labels\n        bias = bias + lambda_val * np.mean(np.maximum(0, 1 - margin * labels) * labels)\n    alpha = np.round(alpha, 4)\n    bias = np.round(bias, 4)\n    return (alpha.tolist(), bias)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function given an input value z.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: The output of the sigmoid activation function rounded to four decimal places.\n    \"\"\"\n    sigmoid_output = 1 / (1 + math.exp(-z))\n    sigmoid_output = round(sigmoid_output, 4)\n    return sigmoid_output\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n\n    Args:\n        scores (list[float]): A list of scores to compute the softmax for.\n\n    Returns:\n        list[float]: The softmax values as a list, each rounded to four decimal places.\n    \"\"\"\n    max_score = max(scores)\n    sum_exp_scores = sum((math.exp(score - max_score) for score in scores))\n    softmax_values = [math.exp(score - max_score) / sum_exp_scores for score in scores]\n    softmax_values = [round(value, 4) for value in softmax_values]\n    return softmax_values\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Args:\n    features (list[list[float]]): A list of feature vectors, where each vector represents multiple features for an example.\n    labels (list[int]): A list of true binary labels associated with the feature vectors.\n    weights (list[float]): A list of the neuron's weights, one for each feature.\n    bias (float): The neuron's bias.\n\n    Returns:\n    tuple[list[float], float]: A tuple containing the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places.\n    \"\"\"\n    features_array = np.array(features)\n    labels_array = np.array(labels)\n    weights_array = np.array(weights)\n    logits = np.dot(features_array, weights_array) + bias\n    predicted_probabilities = 1 / (1 + np.exp(-logits))\n    mean_squared_error = np.mean((predicted_probabilities - labels_array) ** 2)\n    predicted_probabilities = np.round(predicted_probabilities, 4).tolist()\n    mean_squared_error = np.round(mean_squared_error, 4)\n    return (predicted_probabilities, mean_squared_error)\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Sigmoid activation function\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    \"\"\"Derivative of sigmoid activation function\"\"\"\n    return x * (1 - x)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Train a single neuron with sigmoid activation using backpropagation.\n\n    Args:\n    - features (np.ndarray): List of feature vectors.\n    - labels (np.ndarray): Associated true binary labels.\n    - initial_weights (np.ndarray): Initial weights.\n    - initial_bias (float): Initial bias.\n    - learning_rate (float): Learning rate for gradient descent.\n    - epochs (int): Number of epochs.\n\n    Returns:\n    - updated_weights (np.ndarray): Updated weights.\n    - updated_bias (float): Updated bias.\n    - mse_values (list[float]): List of MSE values for each epoch.\n    \"\"\"\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        output = sigmoid(linear_output)\n        mse_loss = np.mean((output - labels) ** 2)\n        mse_values.append(round(mse_loss, 4))\n        error = output - labels\n        d_output = error * sigmoid_derivative(output)\n        d_weights = np.dot(features.T, d_output)\n        d_bias = np.sum(d_output)\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    return (weights, bias, mse_values)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 1])\ninitial_bias = 0.5\nlearning_rate = 0.1\nepochs = 1000\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def _tracer(self, op, *args):\n        args = [self] + list(args)\n        return Value(_op=op, _children=args, data=getattr(self.data, op)(*map(lambda x: x.data, args)))\n\n    def add(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def mul(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += out.grad if self.data > 0 else 0\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        self.grad = 1\n        visited = set()\n\n        def rec_backward(node):\n            if node not in visited:\n                visited.add(node)\n                node._backward()\n                for child in node._prev:\n                    rec_backward(child)\n        rec_backward(self)\n\n    def __add__(self, other):\n        return self.add(other)\n\n    def __mul__(self, other):\n        return self.mul(other)\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P from basis B to C.\n\n    Args:\n    B (list[list[int]]): Basis vectors in basis B.\n    C (list[list[int]]): Basis vectors in basis C.\n\n    Returns:\n    list[list[float]]: Transformation matrix P from basis B to C.\n    \"\"\"\n    B = np.array(B)\n    C = np.array(C)\n    C_inv = np.linalg.inv(C)\n    P = np.dot(C_inv, B)\n    P = np.round(P, 4)\n    return P.tolist()\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n    \n    Parameters:\n    A (np.ndarray): 2x2 matrix\n    \n    Returns:\n    U (list): 2x2 orthogonal matrix\n    S (list): 2x2 diagonal matrix containing the singular values\n    V (list): 2x2 orthogonal matrix\n    \"\"\"\n    A_T_A = np.dot(A.T, A)\n    (eigen_values, eigen_vectors) = np.linalg.eig(A_T_A)\n    idx = np.argsort(eigen_values)[::-1]\n    eigen_values = eigen_values[idx]\n    eigen_vectors = eigen_vectors[:, idx]\n    singular_values = np.sqrt(eigen_values)\n    U = np.dot(A, eigen_vectors) / singular_values\n    S = np.diag(singular_values)\n    V = eigen_vectors\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U = U.tolist()\n    S = S.tolist()\n    V = V.tolist()\n    return (U, S, V)\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n\n    Args:\n        X (numpy array): The first numpy array to be shuffled.\n        y (numpy array): The second numpy array to be shuffled.\n        seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        list: A list containing the shuffled X and y arrays.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    X_shuffled_list = X_shuffled.tolist()\n    y_shuffled_list = y_shuffled.tolist()\n    return [X_shuffled_list, y_shuffled_list]\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Batch iterator for dataset.\n\n    Args:\n    X (numpy array): Input data.\n    y (numpy array, optional): Target data. Defaults to None.\n    batch_size (int, optional): Batch size. Defaults to 64.\n\n    Yields:\n    list or tuple of lists: Batch of X or (X, y) pairs.\n    \"\"\"\n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_indices = indices[start_idx:end_idx]\n        X_batch = X[batch_indices].tolist()\n        if y is not None:\n            y_batch = y[batch_indices].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide a dataset based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n    Parameters:\n    X (numpy array): The dataset to be divided.\n    feature_i (int): The index of the feature to be used for division.\n    threshold (float): The threshold value for division.\n\n    Returns:\n    list: Two subsets of the dataset: one with samples that meet the condition and another with samples that do not.\n    \"\"\"\n    mask = X[:, feature_i] >= threshold\n    X_geq_threshold = X[mask]\n    X_lt_threshold = X[~mask]\n    X_geq_threshold_list = X_geq_threshold.tolist()\n    X_lt_threshold_list = X_lt_threshold.tolist()\n    return (X_geq_threshold_list, X_lt_threshold_list)\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n\n    Parameters:\n    X (2D numpy array): Input dataset.\n    degree (int): Maximum degree of polynomial features.\n\n    Returns:\n    list: A list of polynomial features up to the specified degree.\n    \"\"\"\n    n_features = X.shape[1]\n    poly_features = []\n    for deg in range(1, degree + 1):\n        for combo in combinations_with_replacement(range(n_features), deg):\n            feature = np.prod(X[:, combo], axis=1)\n            poly_features.append(feature)\n    poly_features = [X[:, i] for i in range(n_features)] + poly_features\n    poly_features = np.array(poly_features).T\n    return poly_features.tolist()\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (2D numpy array): Feature matrix.\n    y (1D numpy array): Target vector.\n    n_subsets (int): Number of random subsets to generate.\n    replacements (bool, optional): Whether to create subsets with replacements. Defaults to True.\n    seed (int, optional): Random seed for reproducibility. Defaults to 42.\n\n    Returns:\n    list: List of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices].tolist()\n        y_subset = y[indices].tolist()\n        subsets.append((X_subset, y_subset))\n    return subsets\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    One-hot encoding of nominal values.\n\n    Parameters:\n    x (1D numpy array): Input array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. If not provided, it will be automatically determined from the input array.\n\n    Returns:\n    list: One-hot encoded array as a Python list.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((len(x), n_col))\n    one_hot[np.arange(len(x)), x] = 1\n    return one_hot.tolist()\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n\n    Parameters:\n    x (numpy array): A 1D numpy array.\n\n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (1D numpy array): True labels.\n    y_pred (1D numpy array): Predicted labels.\n\n    Returns:\n    float: Accuracy score.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    return accuracy\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Parameters:\n    X (2D numpy array): The input dataset.\n    Y (2D numpy array, optional): The second dataset. If not provided, the function calculates the correlation matrix of X with itself.\n\n    Returns:\n    correlation_matrix (2D numpy array): The correlation matrix as a 2D numpy array.\n    \"\"\"\n    if Y is None:\n        correlation_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        correlation_matrix = np.corrcoef(np.vstack((X, Y)), rowvar=False)[:X.shape[1], X.shape[1]:]\n    correlation_matrix = np.round(correlation_matrix, 4)\n    correlation_matrix = correlation_matrix.tolist()\n    return correlation_matrix\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implement the fit method for an AdaBoost classifier.\n\n    Parameters:\n    X (2D numpy array): Dataset of shape (n_samples, n_features)\n    y (1D numpy array): Labels of shape (n_samples,)\n    n_clf (int): Number of classifiers\n\n    Returns:\n    list: List of classifiers with their parameters\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        (best_feature, best_threshold, best_polarity) = find_best_split(X, y, sample_weights)\n        error = calculate_error(X, y, best_feature, best_threshold, best_polarity, sample_weights)\n        alpha = 0.5 * np.log((1 - error) / error)\n        sample_weights = update_weights(sample_weights, alpha, X, y, best_feature, best_threshold, best_polarity)\n        classifiers.append((best_feature, best_threshold, best_polarity, alpha))\n    return classifiers\ndef find_best_split(X, y, sample_weights):\n    \"\"\"\n    Find the best threshold for each feature.\n\n    Parameters:\n    X (2D numpy array): Dataset of shape (n_samples, n_features)\n    y (1D numpy array): Labels of shape (n_samples,)\n    sample_weights (1D numpy array): Sample weights of shape (n_samples,)\n\n    Returns:\n    tuple: Best feature, best threshold, and best polarity\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    (best_feature, best_threshold, best_polarity) = (None, None, None)\n    best_gini = np.inf\n    for feature in range(n_features):\n        thresholds = np.unique(X[:, feature])\n        for threshold in thresholds:\n            for polarity in [-1, 1]:\n                parent_gini = gini_impurity(y, sample_weights)\n                (left_gini, right_gini) = gini_gain(X, y, feature, threshold, polarity, sample_weights)\n                gini = parent_gini - (left_gini + right_gini)\n                if gini < best_gini:\n                    best_gini = gini\n                    best_feature = feature\n                    best_threshold = threshold\n                    best_polarity = polarity\n    return (best_feature, best_threshold, best_polarity)\ndef calculate_error(X, y, feature, threshold, polarity, sample_weights):\n    \"\"\"\n    Calculate the error.\n\n    Parameters:\n    X (2D numpy array): Dataset of shape (n_samples, n_features)\n    y (1D numpy array): Labels of shape (n_samples,)\n    feature (int): Feature index\n    threshold (float): Threshold value\n    polarity (int): Polarity of the threshold\n    sample_weights (1D numpy array): Sample weights of shape (n_samples,)\n\n    Returns:\n    float: Error value\n    \"\"\"\n    n_samples = X.shape[0]\n    predictions = np.ones(n_samples)\n    predictions[X[:, feature] * polarity < threshold * polarity] = -1\n    errors = np.where(predictions != y, sample_weights, 0)\n    return np.sum(errors) / np.sum(sample_weights)\ndef update_weights(sample_weights, alpha, X, y, feature, threshold, polarity):\n    \"\"\"\n    Update weights.\n\n    Parameters:\n    sample_weights (1D numpy array): Sample weights of shape (n_samples,)\n    alpha (float): Alpha value\n    X (2D numpy array): Dataset of shape (n_samples, n_features)\n    y (1D numpy array): Labels of shape (n_samples,)\n    feature (int): Feature index\n    threshold (float): Threshold value\n    polarity (int): Polarity of the threshold\n\n    Returns:\n    1D numpy array: Updated sample weights\n    \"\"\"\n    n_samples = X.shape[0]\n    predictions = np.ones(n_samples)\n    predictions[X[:, feature] * polarity < threshold * polarity] = -1\n    errors = np.where(predictions != y, 1, 0)\n    return sample_weights * np.exp(-alpha * (errors * 2 - 1))\ndef gini_impurity(y, sample_weights):\n    \"\"\"\n    Calculate the Gini impurity.\n\n    Parameters:\n    y (1D numpy array): Labels of shape (n_samples,)\n    sample_weights (1D numpy array): Sample weights of shape (n_samples,)\n\n    Returns:\n    float: Gini impurity value\n    \"\"\"\n    classes = np.unique(y)\n    gini = 1\n    for c in classes:\n        class_mask = y == c\n        class_weight = np.sum(sample_weights[class_mask])\n        gini -= class_weight ** 2\n    return gini\ndef gini_gain(X, y, feature, threshold, polarity, sample_weights):\n    \"\"\"\n    Calculate the Gini gain.\n\n    Parameters:\n    X (2D numpy array): Dataset of shape (n_samples, n_features)\n    y (1D numpy array): Labels of shape (n_samples,)\n    feature (int): Feature index\n    threshold (float): Threshold value\n    polarity (int): Polarity of the threshold\n    sample_weights (1D numpy array): Sample weights of shape (n_samples,)\n\n    Returns:\n    tuple: Left and right Gini gain values\n    \"\"\"\n    parent_gini = gini_impurity(y, sample_weights)\n    left_mask = X[:, feature] * polarity < threshold * polarity\n    left_weights = sample_weights[left_mask]\n    left_y = y[left_mask]\n    left_gini = gini_impurity(left_y, left_weights)\n    right_mask = ~left_mask\n    right_weights = sample_weights[right_mask]\n    right_y = y[right_mask]\n    right_gini = gini_impurity(right_y, right_weights)\n    return (left_gini, right_gini)\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log softmax of a 1D numpy array of scores.\n\n    Args:\n        scores (list): A 1D list of scores.\n\n    Returns:\n        list: The log softmax of the input scores, rounded to the nearest 4th decimal.\n    \"\"\"\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    log_softmax = scores - max_score - np.log(np.sum(np.exp(scores - max_score)))\n    log_softmax = np.round(log_softmax, 4)\n    return log_softmax.tolist()\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nimport numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((self.n_units,))\n        self.optimizer = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        grad_wrt_input = np.dot(accum_grad, self.W.T)\n        grad_W = np.dot(self.layer_input.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0)\n        if self.trainable:\n            self.W = self.optimizer.update(self.W, grad_W)\n            self.w0 = self.optimizer.update(self.w0, grad_w0)\n        return grad_wrt_input\n\n    def output_shape(self):\n        return (self.n_units,)\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Applies a 2D convolutional layer to the input matrix.\n\n    Args:\n    input_matrix (np.ndarray): Input matrix to be convolved.\n    kernel (np.ndarray): Convolutional kernel.\n    padding (int): Number of pixels to pad the input matrix with.\n    stride (int): Step size of the convolution operation.\n\n    Returns:\n    list: Convolved output matrix as a Python list.\n    \"\"\"\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    padded_input_matrix = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    for i in range(output_height):\n        for j in range(output_width):\n            receptive_field_top = i * stride\n            receptive_field_left = j * stride\n            receptive_field = padded_input_matrix[receptive_field_top:receptive_field_top + kernel_height, receptive_field_left:receptive_field_left + kernel_width]\n            output_matrix[i, j] = np.sum(receptive_field * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: The output value after applying the ReLU function.\n    \"\"\"\n    return max(0, z)\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss function.\n\n    Parameters:\n    X (np.ndarray): Feature matrix (2D array)\n    w (np.ndarray): Coefficients (1D array)\n    y_true (np.ndarray): True labels (1D array)\n    alpha (float): Regularization parameter\n\n    Returns:\n    float: Ridge loss value\n    \"\"\"\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    loss = mse + reg_term\n    loss = round(loss, 4)\n    return loss\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Args:\n        z (float): Input value.\n        alpha (float, optional): Slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        float|int: Output value after applying Leaky ReLU.\n    \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors.\n\n    Parameters:\n    x1 (numpy array): First input vector.\n    x2 (numpy array): Second input vector.\n\n    Returns:\n    float: The linear kernel value.\n    \"\"\"\n    if not isinstance(x1, np.ndarray) or not isinstance(x2, np.ndarray):\n        raise ValueError('Input vectors must be numpy arrays')\n    if x1.shape[0] != x2.shape[0]:\n        raise ValueError('Input vectors must have the same dimension')\n    kernel_value = np.dot(x1, x2)\n    return kernel_value\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true and predicted binary labels.\n\n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n\n    Returns:\n    float: Precision metric value.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    return precision\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: non-broadcastable output operand with shape (2,1) doesn't match the broadcast shape (2,4)", "solution": "import numpy as np\nimport numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization using MSE loss.\n\n    Parameters:\n    X (numpy array): Feature matrix\n    y (numpy array): Target vector\n    weights (numpy array): Initial weights\n    learning_rate (float): Learning rate for gradient descent\n    n_iterations (int): Number of iterations\n    batch_size (int, optional): Batch size for mini-batch gradient descent (default=1)\n    method (str, optional): Gradient descent variant ('batch', 'sgd', or 'mini-batch') (default='batch')\n\n    Returns:\n    list: Optimized weights after n_iterations\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = weights.reshape(n_features, 1)\n\n    def mse_loss(y_pred, y_true):\n        return np.mean((y_pred - y_true) ** 2)\n\n    def gradient(X, y, weights):\n        return -2 * np.dot(X.T, y - np.dot(X, weights)) / n_samples\n    for _ in range(n_iterations):\n        if method == 'batch':\n            weights -= learning_rate * gradient(X, y, weights)\n        elif method == 'sgd':\n            for i in range(n_samples):\n                weights -= learning_rate * gradient(X[i].reshape(1, n_features), y[i], weights)\n        elif method == 'mini-batch':\n            for i in range(0, n_samples, batch_size):\n                batch_X = X[i:i + batch_size]\n                batch_y = y[i:i + batch_size]\n                weights -= learning_rate * gradient(batch_X, batch_y, weights)\n    return np.round(weights.flatten().tolist(), 4)\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert a given matrix into its Reduced Row Echelon Form (RREF).\n\n    Args:\n        matrix (numpy array): The input matrix.\n\n    Returns:\n        list: The RREF of the input matrix as a Python list.\n    \"\"\"\n    matrix = np.array(matrix, dtype=float)\n    (num_rows, num_cols) = matrix.shape\n    current_row = 0\n    current_col = 0\n    while current_row < num_rows and current_col < num_cols:\n        max_row = np.argmax(np.abs(matrix[current_row:, current_col])) + current_row\n        matrix[[current_row, max_row]] = matrix[[max_row, current_row]]\n        if matrix[current_row, current_col] == 0:\n            current_col += 1\n            continue\n        matrix[current_row] /= matrix[current_row, current_col]\n        for row in range(num_rows):\n            if row != current_row:\n                matrix[row] -= matrix[row, current_col] * matrix[current_row]\n        current_row += 1\n        current_col += 1\n    return matrix.tolist()\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Adam optimization algorithm.\n\n    Parameters:\n    f (function): Objective function to be optimized\n    grad (function): Gradient of the objective function\n    x0 (numpy array): Initial parameter values\n    learning_rate (float, optional): Step size (default: 0.001)\n    beta1 (float, optional): Exponential decay rate for the first moment estimates (default: 0.9)\n    beta2 (float, optional): Exponential decay rate for the second moment estimates (default: 0.999)\n    epsilon (float, optional): Small constant for numerical stability (default: 1e-8)\n    num_iterations (int, optional): Number of iterations to run the optimizer (default: 10)\n\n    Returns:\n    list: Optimized parameters\n    \"\"\"\n    x = x0.copy()\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        gradient = grad(x)\n        m = beta1 * m + (1 - beta1) * gradient\n        v = beta2 * v + (1 - beta2) * np.square(gradient)\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implement Lasso Regression using Gradient Descent with L1 Regularization.\n\n    Parameters:\n    X (np.array): Feature matrix\n    y (np.array): Target vector\n    alpha (float, optional): Regularization parameter. Defaults to 0.1.\n    learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.01.\n    max_iter (int, optional): Maximum number of iterations. Defaults to 1000.\n    tol (float, optional): Tolerance for convergence. Defaults to 1e-4.\n\n    Returns:\n    tuple: A tuple containing the optimized weights and bias.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        weights_grad = -2 / n_samples * np.dot(X.T, y - y_pred) + alpha * np.sign(weights)\n        bias_grad = -2 / n_samples * np.sum(y - y_pred)\n        weights -= learning_rate * weights_grad\n        bias -= learning_rate * bias_grad\n        if np.linalg.norm(weights_grad) < tol and np.abs(bias_grad) < tol:\n            break\n    weights = np.round(weights, 4)\n    bias = np.round(bias, 4)\n    return (weights.tolist(), [bias])\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n    \n    The OSA distance represents the minimum number of edits required to transform \n    one string into another. The allowed edit operations are:\n    - Insert a character\n    - Delete a character\n    - Substitute a character\n    - Transpose two adjacent characters\n    Each of these operations costs 1 unit.\n    \n    Parameters:\n    source (str): The source string.\n    target (str): The target string.\n    \n    Returns:\n    int: The minimum number of edits needed to convert the source string into the target string.\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    dp[0, :] = np.arange(n + 1)\n    dp[:, 0] = np.arange(m + 1)\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i, j] = dp[i - 1, j - 1]\n            else:\n                transposition_cost = dp[i - 2, j - 2] + 1 if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]) else float('inf')\n                dp[i, j] = min(dp[i, j - 1] + 1, dp[i - 1, j] + 1, dp[i - 1, j - 1] + 1, transposition_cost)\n    return dp[m, n]\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric in binary classification.\n\n    Parameters:\n    y_true (list): A list of true binary labels (0 or 1) for the dataset.\n    y_pred (list): A list of predicted binary labels (0 or 1) from the model.\n\n    Returns:\n    float: The recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        recall_value = 0.0\n    else:\n        recall_value = tp / denominator\n    recall_value = round(recall_value, 3)\n    return recall_value\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": false, "result": "failed: self_attention() takes 3 positional arguments but 4 were given", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    This function implements the self-attention mechanism.\n    \n    Parameters:\n    Q (numpy array): Query matrix\n    K (numpy array): Key matrix\n    V (numpy array): Value matrix\n    \n    Returns:\n    self_attention_output (list): The output of the self-attention mechanism\n    \"\"\"\n    dot_product = np.matmul(Q, K.T)\n    scaling_factor = np.sqrt(Q.shape[-1])\n    attention_weights = np.exp(dot_product / scaling_factor)\n    attention_weights_sum = np.sum(attention_weights, axis=-1, keepdims=True)\n    normalized_attention_weights = attention_weights / attention_weights_sum\n    self_attention_output = np.matmul(normalized_attention_weights, V)\n    self_attention_output = np.round(self_attention_output, 4)\n    self_attention_output = self_attention_output.tolist()\n    return self_attention_output\nQ = np.array([[1, 2, 3], [4, 5, 6]])\nV = np.array([[13, 14, 15], [16, 17, 18]])\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple Recurrent Neural Network (RNN) cell.\n\n    Args:\n    - input_sequence (list[list[float]]): Sequence of input vectors.\n    - initial_hidden_state (list[float]): Initial hidden state.\n    - Wx (list[list[float]]): Weight matrix for input-to-hidden connections.\n    - Wh (list[list[float]]): Weight matrix for hidden-to-hidden connections.\n    - b (list[float]): Bias vector.\n\n    Returns:\n    - final_hidden_state (list[float]): Final hidden state after processing the entire sequence.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_state = initial_hidden_state\n    for input_vector in input_sequence:\n        input_contribution = np.dot(Wx, input_vector)\n        hidden_contribution = np.dot(Wh, hidden_state)\n        hidden_state = np.tanh(input_contribution + hidden_contribution + b)\n    final_hidden_state = np.round(hidden_state, 4)\n    return final_hidden_state.tolist()\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Args:\n        points (list): A list of [x, y] coordinates.\n        tx (float): The translation distance in the x direction.\n        ty (float): The translation distance in the y direction.\n\n    Returns:\n        list: A new list of points after applying the translation matrix.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    ones_column = np.ones((points_array.shape[0], 1))\n    points_array_homogeneous = np.hstack((points_array, ones_column))\n    translated_points_homogeneous = np.dot(points_array_homogeneous, translation_matrix.T)\n    translated_points = translated_points_homogeneous[:, :2].tolist()\n    return translated_points\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n\n    Returns:\n    float: KL divergence between the two normal distributions.\n    \"\"\"\n    kl_div = 0.5 * (np.log(sigma_q ** 2 / sigma_p ** 2) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / sigma_q ** 2 - 1)\n    return kl_div\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Gauss-Seidel Method for Solving Linear Systems.\n\n    Parameters:\n    A (numpy array): Square matrix of coefficients.\n    b (numpy array): Right-hand side vector.\n    n (int): Number of iterations.\n    x_ini (numpy array, optional): Initial guess for the solution vector. Defaults to None.\n\n    Returns:\n    list: Approximated solution vector after n iterations.\n    \"\"\"\n    num_eq = A.shape[0]\n    if x_ini is None:\n        x = np.zeros(num_eq)\n    else:\n        x = x_ini.copy()\n    for _ in range(n):\n        for i in range(num_eq):\n            s = sum((A[i, j] * x[j] for j in range(num_eq) if j != i))\n            x[i] = (b[i] - s) / A[i, i]\n    return np.round(x, 4).tolist()\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Gaussian Elimination with partial pivoting to solve the system Ax = b.\n\n    Parameters:\n    A (numpy array): Coefficient matrix\n    b (numpy array): Right-hand side vector\n\n    Returns:\n    x (list): Solution vector\n    \"\"\"\n    n = len(A)\n    A = np.copy(A)\n    b = np.copy(b)\n    for i in range(n - 1):\n        max_row = np.argmax(np.abs(A[i:, i])) + i\n        A[[i, max_row]] = A[[max_row, i]]\n        b[[i, max_row]] = b[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, :] -= factor * A[i, :]\n            b[j] -= factor * b[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return np.round(x, 4).tolist()\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": false, "result": "failed: not enough values to unpack (expected 3, got 2)", "solution": "import numpy as np\nimport numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        \"\"\"Computes the sigmoid of x\"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        \"\"\"Computes the hyperbolic tangent of x\"\"\"\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        (seq_len, batch_size, _) = x.shape\n        hidden_states = np.zeros((seq_len, batch_size, self.hidden_size))\n        cell_states = np.zeros((seq_len, batch_size, self.hidden_size))\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        for t in range(seq_len):\n            concat = np.concatenate((x[t], hidden_state), axis=1)\n            forget_gate = self.sigmoid(np.dot(concat, self.Wf.T) + self.bf.T)\n            input_gate = self.sigmoid(np.dot(concat, self.Wi.T) + self.bi.T)\n            candidate_cell_state = self.tanh(np.dot(concat, self.Wc.T) + self.bc.T)\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell_state\n            output_gate = self.sigmoid(np.dot(concat, self.Wo.T) + self.bo.T)\n            hidden_state = output_gate * self.tanh(cell_state)\n            hidden_states[t] = hidden_state\n            cell_states[t] = cell_state\n        return (np.round(hidden_states, 4).tolist(), np.round(hidden_state, 4).tolist(), np.round(cell_state, 4).tolist())\nx = np.random.randn(5, 1, 10)\ninitial_hidden_state = np.zeros((1, 20))\ninitial_cell_state = np.zeros((1, 20))\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a given corpus of documents.\n\n    Args:\n    - corpus (list of lists): A list of documents, where each document is a list of words.\n    - query (list): A list of words for which to compute the TF-IDF scores.\n\n    Returns:\n    - tf_idf_scores (list of lists): A list of lists containing the TF-IDF scores for the query words in each document.\n    \"\"\"\n    if not corpus:\n        raise ValueError('Corpus cannot be empty')\n    num_docs = len(corpus)\n    df = {}\n    for doc in corpus:\n        unique_terms = set(doc)\n        for term in unique_terms:\n            df[term] = df.get(term, 0) + 1\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_scores = []\n        for term in query:\n            tf = doc.count(term) / len(doc) if doc else 0\n            idf = np.log((num_docs + 1) / (df.get(term, 0) + 1)) + 1\n            tf_idf = tf * idf\n            doc_scores.append(round(tf_idf, 5))\n        tf_idf_scores.append(doc_scores)\n    tf_idf_scores = np.round(np.array(tf_idf_scores), 4).tolist()\n    return tf_idf_scores\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall) if precision + recall != 0 else 0\n    return round(f_score, 3)\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: name 'SimpleRNN' is not defined", "solution": "import numpy as np\nimport numpy as np\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    r = b - np.dot(A, x)\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return np.round(x, 8).tolist()\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(np.square(probabilities))\n    return round(gini, 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    col_indices = []\n    row_ptr = [0]\n    for (row_idx, row) in enumerate(dense_matrix):\n        for (col_idx, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                col_indices.append(col_idx)\n        row_ptr.append(len(values))\n    return (values, col_indices, row_ptr)\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    L_normalized = L / np.linalg.norm(L)\n    dot_product = np.dot(v, L_normalized)\n    projection = dot_product * L_normalized\n    projection = np.round(projection, 3)\n    return projection.tolist()\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        col_pointer[col] = len(values)\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n    col_pointer[-1] = len(values)\n    return (values, row_indices, col_pointer)\ndense_matrix = [[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]]\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Compute the column space of a matrix A.\n\n    Parameters:\n    A (numpy.ndarray): The input matrix.\n\n    Returns:\n    list: A list of basis vectors that span the column space of A.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    (num_rows, num_cols) = A.shape\n    current_row = 0\n    current_col = 0\n    independent_cols = []\n    while current_col < num_cols and current_row < num_rows:\n        max_row = np.argmax(np.abs(A[current_row:, current_col])) + current_row\n        if np.abs(A[max_row, current_col]) < 1e-08:\n            current_col += 1\n            continue\n        A[[current_row, max_row]] = A[[max_row, current_row]]\n        A[current_row] /= A[current_row, current_col]\n        for row in range(num_rows):\n            if row != current_row:\n                A[row] -= A[row, current_col] * A[current_row]\n        independent_cols.append(current_col)\n        current_row += 1\n        current_col += 1\n    basis_vectors = A[:, independent_cols]\n    basis_vectors = np.round(basis_vectors, 8).tolist()\n    return basis_vectors\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Compute the R-squared value in regression analysis.\n\n    Parameters:\n    y_true (array-like): True values of the dependent variable.\n    y_pred (array-like): Predicted values of the dependent variable.\n\n    Returns:\n    float: R-squared value rounded to three decimal places.\n    \"\"\"\n    y_mean = np.mean(y_true)\n    sst = np.sum((y_true - y_mean) ** 2)\n    sse = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - sse / sst\n    return round(r2, 3)\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Args:\n        img (list of lists): A 2D matrix representing the image, where each element is a pixel value between 0 (black) and 255 (white).\n\n    Returns:\n        float: The average brightness of the image rounded to two decimal places, or -1 if an edge case is encountered.\n    \"\"\"\n    if not img:\n        return -1\n    row_lengths = [len(row) for row in img]\n    if len(set(row_lengths)) > 1:\n        return -1\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n    total_brightness = sum((sum(row) for row in img))\n    avg_brightness = total_brightness / (len(img) * len(img[0]))\n    return round(avg_brightness, 2)\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n\n    Parameters:\n    y_true (numpy array): Actual values.\n    y_pred (numpy array): Predicted values.\n\n    Returns:\n    float: RMSE value rounded to three decimal places.\n\n    Raises:\n    ValueError: If the input arrays have mismatched shapes or are empty.\n    TypeError: If the input arrays are not of type numpy.ndarray.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Both inputs must be of type numpy.ndarray.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    differences = y_true - y_pred\n    squared_differences = differences ** 2\n    mean_squared_differences = np.mean(squared_differences)\n    rmse_value = np.sqrt(mean_squared_differences)\n    return round(rmse_value, 3)\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) labels.\n    y_pred (array-like): Estimated labels.\n\n    Returns:\n    float: Jaccard Index, rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if not np.all(np.isin(y_true, [0, 1])) or not np.all(np.isin(y_pred, [0, 1])):\n        raise ValueError('Both y_true and y_pred must be binary arrays.')\n    intersection = np.logical_and(y_true, y_pred)\n    union = np.logical_or(y_true, y_pred)\n    jaccard = np.sum(intersection) / np.sum(union)\n    if np.sum(union) == 0:\n        jaccard = 1.0\n    jaccard = round(jaccard, 3)\n    return jaccard\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Compute the Dice Score (S\u00f8rensen-Dice coefficient or F1-score) for binary classification.\n\n    Parameters:\n    y_true (array-like): Ground truth binary labels.\n    y_pred (array-like): Predicted binary labels.\n\n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if not np.all(np.isin(y_true, [0, 1])) or not np.all(np.isin(y_pred, [0, 1])):\n        raise ValueError('Inputs must be binary arrays')\n    intersection = np.logical_and(y_true, y_pred).sum()\n    sum_true_pos = y_true.sum()\n    sum_pred_pos = y_pred.sum()\n    if sum_true_pos + sum_pred_pos == 0:\n        return 0.0\n    dice = 2 * intersection / (sum_true_pos + sum_pred_pos)\n    return round(dice, 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given dataset row using Hyperdimensional Computing.\n\n    Parameters:\n    row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    dim (int): The dimensionality of the hypervectors.\n    random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n\n    Returns:\n    list: A composite hypervector representing the entire row.\n    \"\"\"\n    composite_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        seed = random_seeds[feature]\n        np.random.seed(seed)\n        feature_hv = np.random.choice([-1, 1], size=dim)\n        value_hv = np.random.choice([-1, 1], size=dim)\n        bound_hv = feature_hv * value_hv\n        composite_hv += bound_hv\n    return composite_hv.tolist()\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for a binary classification problem.\n\n    Args:\n        data (list of lists): A list of pairs, where each pair contains the actual label (y_true) and the predicted label (y_pred).\n\n    Returns:\n        list of lists: A 2x2 confusion matrix, where the rows represent the actual labels and the columns represent the predicted labels.\n    \"\"\"\n    matrix = [[0, 0], [0, 0]]\n    counts = Counter(((y_true, y_pred) for (y_true, y_pred) in data))\n    matrix[0][0] = counts.get((0, 0), 0)\n    matrix[0][1] = counts.get((0, 1), 0)\n    matrix[1][0] = counts.get((1, 0), 0)\n    matrix[1][1] = counts.get((1, 1), 0)\n    return matrix\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Args:\n        v1 (numpy.ndarray): The first input vector.\n        v2 (numpy.ndarray): The second input vector.\n\n    Returns:\n        float: The cosine similarity between the two vectors, rounded to three decimal places.\n\n    Raises:\n        ValueError: If the input vectors have different shapes or are empty.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude')\n    cosine_similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_similarity, 3)\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "from collections import Counter\nfrom collections import Counter\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Compute performance metrics for a binary classification model.\n\n    Args:\n    actual (list[int]): Actual class labels (1 for positive, 0 for negative).\n    predicted (list[int]): Predicted class labels from the model.\n\n    Returns:\n    tuple: A tuple containing the confusion matrix, accuracy, F1 score, specificity, and negative predictive value.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('Actual and predicted lists must have the same length.')\n    if not all((i in [0, 1] for i in actual)) or not all((i in [0, 1] for i in predicted)):\n        raise ValueError('All elements in actual and predicted lists must be either 0 or 1.')\n    confusion_matrix = np.array([[sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0)), sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))], [sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0)), sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))]])\n    accuracy = sum((1 for (a, p) in zip(actual, predicted) if a == p)) / len(actual)\n    tp = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    fp = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    fn = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    f1_score = tp / (tp + 0.5 * (fp + fn)) if tp + 0.5 * (fp + fn) != 0 else 0\n    tn = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    specificity = tn / (tn + fp) if tn + fp != 0 else 0\n    negative_predictive_value = tn / (tn + fn) if tn + fn != 0 else 0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": false, "result": "failed: invalid index to scalar variable.", "solution": "import numpy as np\nimport numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate descriptive statistics metrics for a given dataset.\n\n    Parameters:\n    data (list or NumPy array): A list or NumPy array of numerical values.\n\n    Returns:\n    dict: A dictionary containing mean, median, mode, variance, standard deviation,\n          percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n    data = np.asarray(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentiles = np.percentile(data, [25, 50, 75])\n    interquartile_range = percentiles[2] - percentiles[0]\n    mean = round(mean, 4)\n    median = round(median, 4)\n    variance = round(variance, 4)\n    standard_deviation = round(standard_deviation, 4)\n    percentiles = [round(p, 4) for p in percentiles]\n    interquartile_range = round(interquartile_range, 4)\n    stats_dict = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentiles[0], '50th_percentile': percentiles[1], '75th_percentile': percentiles[2], 'interquartile_range': interquartile_range}\n    return stats_dict\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binomial_coefficient = math.comb(n, k)\n    probability = binomial_coefficient * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError('k must be a non-negative integer')\n    if lam < 0:\n        raise ValueError('lam must be a non-negative number')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)\nk = 3\nlam = 2.5\nprobability = poisson_probability(k, lam)\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: The contrast of the grayscale image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    img = np.asarray(img)\n    if len(img.shape) != 2:\n        raise ValueError('Input image must be a 2D array')\n    if np.any(img < 0) or np.any(img > 255):\n        raise ValueError('Pixel values must be between 0 and 255')\n    min_pixel_value = np.min(img)\n    max_pixel_value = np.max(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n\n    Returns:\n        float: The dot product of the two input vectors.\n\n    Raises:\n        ValueError: If the input vectors are not 1D arrays or have different lengths.\n    \"\"\"\n    if not (vec1.ndim == 1 and vec2.ndim == 1):\n        raise ValueError('Input vectors must be 1D arrays')\n    if len(vec1) != len(vec2):\n        raise ValueError('Input vectors must have the same length')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product\nvec1 = np.array([1, 2, 3])\nvec2 = np.array([4, 5, 6])\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        poly_features = [1.0]\n        for i in range(1, degree + 1):\n            poly_features.append(round(x ** i, 8))\n        transformed_data.append(poly_features)\n    return transformed_data\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates positional encodings for a sequence length (`position`) and model dimensionality (`d_model`)\n    using sine and cosine functions as specified in the Transformer architecture.\n\n    Args:\n        position (int): Sequence length.\n        d_model (int): Model dimensionality.\n\n    Returns:\n        list: Positional encoding array of dtype np.float16, or -1 if `position` is 0 or `d_model` is less than or equal to 0.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    angles = np.arange(position).reshape(-1, 1)\n    frequencies = np.arange(d_model // 2).reshape(1, -1)\n    sine_components = np.sin(angles / 10000 ** (2 * frequencies / d_model))\n    cosine_components = np.cos(angles / 10000 ** (2 * frequencies / d_model))\n    encoding = np.zeros((position, d_model), dtype=np.float16)\n    encoding[:, 0::2] = sine_components\n    encoding[:, 1::2] = cosine_components\n    return encoding.tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: module 'numpy' has no attribute 'relu'", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\nimport numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    prompt_token_ids = encoder.encode(prompt)\n    output_sequence = prompt_token_ids[:]\n    for _ in range(n_tokens_to_generate):\n        seq_len = len(output_sequence)\n        mask = np.tril(np.ones((seq_len, seq_len)))\n        token_embeddings = params['wte'][output_sequence]\n        positional_embeddings = params['wpe'][:seq_len]\n        embeddings = token_embeddings + positional_embeddings\n        embeddings = embeddings * params['ln_f']['g'] + params['ln_f']['b']\n        attention_weights = np.matmul(embeddings, embeddings.T) / np.sqrt(hparams['n_head'])\n        attention_weights = attention_weights * mask\n        attention_output = np.matmul(attention_weights, embeddings)\n        ff_output = np.relu(np.matmul(attention_output, np.random.rand(10, 10)))\n        output_token_id = np.argmax(np.matmul(ff_output, params['wte'].T))\n        output_sequence.append(output_token_id)\n    generated_text = encoder.decode(output_sequence)\n    return generated_text\nprompt = 'hello world'\ngenerated_text = gen_text(prompt, n_tokens_to_generate=20)\nnp.random.seed(42)\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nnp.random.seed(42)\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nnp.random.seed(42)\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: cannot reshape array of size 3 into shape (3,2)", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    This function implements a simplified self-attention mechanism \n    to reveal the enhanced patterns of the crystals.\n\n    Args:\n    n (int): The number of crystals.\n    crystal_values (list): A list of numeric values representing the crystals.\n    dimension (int): The dimension of the crystal values.\n\n    Returns:\n    list: A list of floats representing the final weighted pattern for each crystal.\n    \"\"\"\n\n    def softmax(values):\n        \"\"\"\n        This function implements the softmax function.\n\n        Args:\n        values (list): A list of numeric values.\n\n        Returns:\n        list: A list of floats representing the softmax scores.\n        \"\"\"\n        exp_values = np.exp(values)\n        sum_exp_values = np.sum(exp_values)\n        return exp_values / sum_exp_values\n    query = np.array(crystal_values).reshape(n, dimension)\n    key = np.array(crystal_values).reshape(n, dimension)\n    value = np.array(crystal_values).reshape(n, dimension)\n    attention_scores = np.dot(query, key.T) / np.sqrt(dimension)\n    softmax_scores = np.apply_along_axis(softmax, 1, attention_scores)\n    weighted_pattern = np.dot(softmax_scores, value)\n    return [round(float(x), 4) for x in weighted_pattern.flatten()]\nn = 3\ndimension = 1\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a query in an information retrieval context.\n\n    Args:\n    - corpus (list of lists): A list of documents, where each document is a list of words.\n    - query (list): A list of words in the query.\n    - k1 (float, optional): A hyperparameter for term frequency saturation. Defaults to 1.5.\n    - b (float, optional): A hyperparameter for document length normalization. Defaults to 0.75.\n\n    Returns:\n    - scores (list): A list of BM25 scores for each document in the corpus, rounded to three decimal places.\n    \"\"\"\n    avg_doc_len = np.mean([len(doc) for doc in corpus])\n    scores = []\n    for doc in corpus:\n        doc_len = len(doc)\n        score = 0\n        doc_tf = Counter(doc)\n        query_tf = Counter(query)\n        for term in query_tf:\n            tf = doc_tf.get(term, 0)\n            idf = np.log((len(corpus) + 1) / (1 + sum((1 for doc in corpus if term in doc))))\n            term_score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / avg_doc_len))\n            score += term_score\n        scores.append(round(score, 3))\n    return scores\ncorpus = [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['the', 'sun', 'is', 'shining', 'brightly', 'in', 'the', 'clear', 'blue', 'sky'], ['the', 'big', 'red', 'car', 'is', 'driving', 'down', 'the', 'highway']]\nquery = ['the', 'quick', 'brown', 'fox']\nscores = calculate_bm25_scores(corpus, query)\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = sum((1 for (yt, yp) in zip(y_true, y_pred) if yt == 1 and yp == 1))\n    fp = sum((1 for (yt, yp) in zip(y_true, y_pred) if yt == 0 and yp == 1))\n    fn = sum((1 for (yt, yp) in zip(y_true, y_pred) if yt == 1 and yp == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "import math\nPI = 3.14159\nimport math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = [data - 10 * math.sin(2 * PI * (i + 1) / 10) for (i, data) in enumerate(consumption_data)]\n    X = np.array(range(1, len(detrended_data) + 1)).reshape(-1, 1)\n    y = np.array(detrended_data)\n    model = LinearRegression()\n    model.fit(X, y)\n    day_15_base_consumption = model.predict(np.array([[15]]))[0]\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base_consumption + day_15_fluctuation\n    final_consumption = math.ceil(day_15_consumption * 1.05)\n    return final_consumption\nconsumption_data = [105.2, 110.5, 115.8, 121.1, 126.4, 131.7, 137, 142.3, 147.6, 152.9]\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    mae_value = round(mean_absolute_error, 3)\n    return mae_value\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: multi_head_attention() takes 4 positional arguments but 5 were given", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute self-attention.\n\n    Args:\n    - Q (np.array): Query matrix\n    - K (np.array): Key matrix\n    - V (np.array): Value matrix\n\n    Returns:\n    - attention_values (np.array): Self-attention values\n    \"\"\"\n    attention_scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    attention_values = np.dot(attention_weights, V)\n    return attention_values\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Compute multi-head attention.\n\n    Args:\n    - Q (np.array): Query matrix\n    - K (np.array): Key matrix\n    - V (np.array): Value matrix\n    - n_heads (int): Number of attention heads\n\n    Returns:\n    - multi_head_attention_values (np.array): Multi-head attention values\n    \"\"\"\n    Q_heads = np.split(Q, n_heads, axis=-1)\n    K_heads = np.split(K, n_heads, axis=-1)\n    V_heads = np.split(V, n_heads, axis=-1)\n    attention_values_heads = [self_attention(Q_head, K_head, V_head) for (Q_head, K_head, V_head) in zip(Q_heads, K_heads, V_heads)]\n    multi_head_attention_values = np.concatenate(attention_values_heads, axis=-1)\n    return multi_head_attention_values\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n = len(x)\n    if len(y) != n:\n        raise ValueError('Input lists must have the same length')\n    n11 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 1))\n    n10 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 0))\n    n01 = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 1))\n    n00 = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 0))\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n01 + n00) * (n11 + n01) * (n10 + n00)) ** 0.5\n    if denominator == 0:\n        raise ValueError('Cannot calculate Phi coefficient due to division by zero')\n    phi = numerator / denominator\n    return round(phi, 4)\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)\nx = -2.5\nalpha = 1.0\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return x\n    if x < -100:\n        return math.exp(x)\n    softplus_value = math.log(1 + math.exp(x))\n    return round(softplus_value, 4)\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    softsign_value = x / (1 + abs(x))\n    softsign_value = round(softsign_value, 4)\n    return softsign_value\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: can't multiply sequence by non-int of type 'list'", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    grpo_objective_without_penalty = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    kl_divergence = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_objective_with_penalty = grpo_objective_without_penalty - beta * kl_divergence\n    return round(grpo_objective_with_penalty, 6)\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x * (1 / (1 + math.exp(-x))), 4)\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * alpha * (math.exp(x) - 1)\n    selu_value = np.clip(selu_value, -100, 100)\n    selu_value = round(selu_value, 4)\n    return selu_value\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    logits = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-logits))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    num_classes = np.unique(y).size\n    num_features = X.shape[1]\n    weights = np.random.rand(num_features, num_classes)\n    y_onehot = np.eye(num_classes)[y]\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, weights)\n        probabilities = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n        loss = -np.mean(np.sum(y_onehot * np.log(probabilities), axis=1))\n        losses.append(round(loss, 4))\n        gradient = np.dot(X.T, probabilities - y_onehot) / X.shape[0]\n        weights -= learning_rate * gradient\n    B = np.round(weights, 4).tolist()\n    return (B, losses)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the sigmoid of x\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\ndef binary_cross_entropy(y: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute Binary Cross Entropy loss\n    \"\"\"\n    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n    loss_history = []\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = sigmoid(linear_model)\n        loss = binary_cross_entropy(y, y_pred)\n        loss_history.append(round(loss, 4))\n        dw = np.dot(X.T, y_pred - y)\n        db = np.mean(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    return (np.round(weights, 4).tolist(), round(bias, 4), loss_history)\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 1])\nlearning_rate = 0.01\niterations = 1000\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    attention_scores = np.matmul(Q, K.T) / np.sqrt(K.shape[-1])\n    attention_scores = attention_scores + (1 - mask) * -10000.0\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, V)\n    return output.tolist()\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([1,1,0,0])", "solution": "\nimport math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    color_counts = Counter(apples)\n    total_apples = sum(color_counts.values())\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    return round(entropy, 4)\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Layer Normalization on the input tensor X.\n\n    Args:\n    X (np.ndarray): Input tensor with shape (batch_size, sequence_length, feature_dim)\n    gamma (np.ndarray): Scaling parameter with shape (feature_dim,)\n    beta (np.ndarray): Shifting parameter with shape (feature_dim,)\n    epsilon (float, optional): Small value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    list: Normalized X rounded to 5 decimal places and converted to a list.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    std = np.sqrt(variance + epsilon)\n    normalized_X = (X - mean) / std\n    normalized_X = gamma * normalized_X + beta\n    normalized_X = np.round(normalized_X, 5).tolist()\n    return normalized_X\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Args:\n    - reference (str): The reference translation.\n    - candidate (str): The candidate translation.\n    - alpha (float, optional): Weight for unigram matches. Defaults to 0.9.\n    - beta (float, optional): Weight for precision and recall. Defaults to 3.\n    - gamma (float, optional): Weight for word order fragmentation. Defaults to 0.5.\n\n    Returns:\n    - float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    reference_unigrams = reference.split()\n    candidate_unigrams = candidate.split()\n    unigram_matches = sum((min(Counter(reference_unigrams)[word], Counter(candidate_unigrams)[word]) for word in set(reference_unigrams)))\n    precision = unigram_matches / len(candidate_unigrams)\n    recall = unigram_matches / len(reference_unigrams)\n    f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall)\n    chunk_count = 0\n    for i in range(len(candidate_unigrams)):\n        if i == 0 or candidate_unigrams[i] != reference_unigrams[i]:\n            chunk_count += 1\n    penalty = gamma * (chunk_count / len(candidate_unigrams)) ** beta\n    meteor_score = f_mean * (1 - penalty)\n    return round(meteor_score, 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute Pointwise Mutual Information (PMI) between two events.\n\n    Parameters:\n    joint_counts (int): Joint occurrence count of two events.\n    total_counts_x (int): Total count of event X.\n    total_counts_y (int): Total count of event Y.\n    total_samples (int): Total number of samples.\n\n    Returns:\n    float: Pointwise Mutual Information (PMI) rounded to 3 decimal places.\n    \"\"\"\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    p_xy_expected = p_x * p_y\n    pmi = np.log2(p_xy / p_xy_expected)\n    return round(pmi, 3)\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    This function performs Min-Max Normalization on a list of integers.\n    \n    Args:\n        x (list[int]): A list of integers to be normalized.\n    \n    Returns:\n        list[float]: A list of normalized floats in the range [0, 1].\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized = [(i - min_val) / (max_val - min_val) for i in x]\n    normalized = [round(i, 4) for i in normalized]\n    return normalized\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    Creates a simple residual block using NumPy.\n\n    Args:\n    x (np.ndarray): 1D input array.\n    w1 (np.ndarray): Weights for the first layer.\n    w2 (np.ndarray): Weights for the second layer.\n\n    Returns:\n    list: Output of the residual block, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    out = np.matmul(x, w1)\n    out = np.maximum(out, 0)\n    out = np.matmul(out, w2)\n    out = np.maximum(out, 0)\n    out = out + x\n    out = np.maximum(out, 0)\n    return np.round(out, 4).tolist()\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer.\n\n    Args:\n        x (np.ndarray): Input array of shape (height, width, channels)\n\n    Returns:\n        np.ndarray: Output array of shape (channels,), where each element is the average of all values in the corresponding feature map\n    \"\"\"\n    assert len(x.shape) == 3, 'Input must be a 3D array'\n    pooled = np.mean(x, axis=(0, 1))\n    return pooled\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n    output = gamma * normalized_X + beta\n    output = np.round(output, 4).tolist()\n    return output\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of a polynomial term of the form c * x^n at a given point x.\n\n    Args:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative at the given point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    derivative = round(derivative, 4)\n    return derivative\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n\n    Args:\n    vectors (list[list[float]]): A list of 2D vectors.\n    tol (float, optional): Tolerance value to determine linear independence. Defaults to 1e-10.\n\n    Returns:\n    list[list[float]]: A list of orthonormal vectors (unit length and orthogonal to each other) that span the same subspace.\n    \"\"\"\n    vectors = [np.array(v) for v in vectors]\n    basis = []\n    for v in vectors:\n        proj = np.zeros_like(v)\n        for b in basis:\n            proj += np.dot(v, b) * b\n        w = v - proj\n        if np.linalg.norm(w) > tol:\n            w = w / np.linalg.norm(w)\n            basis.append(w)\n    basis = [np.round(b.tolist(), 4) for b in basis]\n    return basis\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n    \n    Parameters:\n    a (list or numpy.ndarray): The first 3D vector.\n    b (list or numpy.ndarray): The second 3D vector.\n    \n    Returns:\n    list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n    a = np.asarray(a)\n    b = np.asarray(b)\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError('Both vectors must be 3-dimensional.')\n    cross_product_vector = np.cross(a, b)\n    result = np.round(cross_product_vector, 4).tolist()\n    return result\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'shape'", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations Ax = b using Cramer's Rule.\n\n    Parameters:\n    A (numpy array): Square coefficient matrix.\n    b (numpy array): Constant vector.\n\n    Returns:\n    x (list): Solution vector if the system has a unique solution, otherwise -1.\n    \"\"\"\n    assert A.shape[0] == A.shape[1], 'Coefficient matrix A must be square'\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = np.zeros_like(b)\n    for i in range(len(b)):\n        Ai = np.copy(A)\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x[i] = det_Ai / det_A\n    x = np.round(x, 4)\n    return x.tolist()\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two discrete probability distributions.\n\n    Args:\n    p (list[float]): The first discrete probability distribution.\n    q (list[float]): The second discrete probability distribution.\n\n    Returns:\n    float: The Bhattacharyya distance between p and q, rounded to 4 decimal places.\n    If the inputs have different lengths or are empty, return 0.0.\n    \"\"\"\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    distance = -np.log(bc)\n    distance = round(distance, 4)\n    return distance\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Args:\n        a (list[int|float]): The first vector.\n        b (list[int|float]): The second vector.\n\n    Returns:\n        list[int|float]: A new vector representing the resulting sum if the operation is valid, or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Compute the average gradient of the log-policy multiplied by the return at each time step.\n\n    Args:\n    theta (np.ndarray): 2D NumPy array of shape (num_states, num_actions) parameterizing the policy.\n    episodes (list[list[tuple[int, int, float]]]): List of episodes, each a list of (state, action, reward) tuples.\n\n    Returns:\n    list[float]: The average gradient of the log-policy multiplied by the return at each time step, reshaped as a list.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    policy_gradients = np.zeros_like(theta)\n    for episode in episodes:\n        (states, actions, rewards) = zip(*episode)\n        returns = np.cumsum(rewards[::-1])[::-1]\n        for (t, (state, action, _)) in enumerate(episode):\n            policy = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n            log_policy_grad = np.zeros(num_actions)\n            log_policy_grad[action] = 1 - policy[action]\n            log_policy_grad = -log_policy_grad / policy[action]\n            policy_gradients[state, :] += returns[t] * log_policy_grad\n    policy_gradients /= len(episodes)\n    policy_gradients = np.round(policy_gradients, 4)\n    return policy_gradients.tolist()\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Args:\n    - n_experts (int): The number of experts in the MoE layer.\n    - k_active (int): The number of active experts (sparsity).\n    - d_in (int): The input dimension.\n    - d_out (int): The output dimension.\n\n    Returns:\n    - savings_percentage (float): The percentage of FLOPs saved by using the MoE layer instead of a dense layer.\n    \"\"\"\n    dense_flops = 2 * d_in * d_out\n    moe_flops = k_active * (2 * d_in * d_out / n_experts) + n_experts * d_in\n    savings_percentage = (1 - moe_flops / dense_flops) * 100\n    savings_percentage = round(savings_percentage, 1)\n    return savings_percentage\nn_experts = 8\nk_active = 2\nd_in = 1024\nd_out = 1024\nsavings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models.\n\n    Args:\n    X (np.ndarray): Input matrix.\n    W_g (np.ndarray): Weight matrix for the gating mechanism.\n    W_noise (np.ndarray): Weight matrix for the noise.\n    N (np.ndarray): Pre-sampled noise matrix.\n    k (int): Sparsity constraint.\n\n    Returns:\n    list: Final gating probabilities matrix, rounded to the nearest 4th decimal.\n    \"\"\"\n    logits = np.dot(X, W_g)\n    noisy_logits = logits + np.dot(N, W_noise)\n    topk_logits = np.sort(noisy_logits, axis=-1)[..., -k:]\n    threshold = np.min(topk_logits, axis=-1, keepdims=True)\n    probabilities = np.where(noisy_logits >= threshold, noisy_logits, -1000000000.0)\n    probabilities = np.exp(probabilities) / np.sum(np.exp(probabilities), axis=-1, keepdims=True)\n    probabilities = np.round(probabilities, 4)\n    return probabilities.reshape(-1).tolist()\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (4,) (3,4,2) ", "solution": "import numpy as np\nimport numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n\n    Args:\n    - x (np.ndarray): Input tensor.\n    - We (np.ndarray): Set of expert weight matrices.\n    - Wg (np.ndarray): Gating weight matrix.\n    - n_experts (int): Number of experts.\n    - top_k (int): Value of k for top-k routing.\n\n    Returns:\n    - MoE output as a Python list.\n    \"\"\"\n    gating_logits = np.dot(x, Wg)\n    gating_softmax = np.exp(gating_logits) / np.sum(np.exp(gating_logits), axis=-1, keepdims=True)\n    top_k_indices = np.argsort(-gating_softmax, axis=-1)[:, :top_k]\n    top_k_gating_probs = np.sort(-gating_softmax, axis=-1)[:, :top_k]\n    top_k_gating_probs = -top_k_gating_probs / np.sum(-top_k_gating_probs, axis=-1, keepdims=True)\n    output = np.zeros_like(x)\n    for i in range(x.shape[0]):\n        for j in range(top_k):\n            expert_index = top_k_indices[i, j]\n            output[i] += top_k_gating_probs[i, j] * np.dot(x[i], We[expert_index])\n    output = np.round(output, 4)\n    return output.tolist()\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Performs Group Normalization on a 4D input tensor.\n\n    Args:\n        X (np.ndarray): Input tensor with shape (B, C, H, W)\n        gamma (np.ndarray): Learned scale with shape (1, C, 1, 1)\n        beta (np.ndarray): Learned shift with shape (1, C, 1, 1)\n        num_groups (int): Number of groups to normalize over\n        epsilon (float, optional): Small value added for numerical stability. Defaults to 1e-5.\n\n    Returns:\n        list: Normalized tensor reshaped to a list\n    \"\"\"\n    (B, C, H, W) = X.shape\n    channels_per_group = C // num_groups\n    X_reshaped = X.reshape(B, num_groups, channels_per_group, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    variance = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    normalized_X = (X_reshaped - mean) / np.sqrt(variance + epsilon)\n    normalized_X = normalized_X.reshape(B, C, H, W)\n    normalized_X = gamma * normalized_X + beta\n    normalized_X = np.round(normalized_X, 4)\n    return normalized_X.tolist()\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Find the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum.\n\n    Parameters:\n    start_x (float): Initial position to start the search from.\n    learning_rate (float, optional): Step size for each iteration. Defaults to 0.1.\n    tolerance (float, optional): Convergence tolerance. Defaults to 1e-6.\n    max_iters (int, optional): Maximum number of iterations. Defaults to 10000.\n\n    Returns:\n    float: The value of x where f(x) reaches its minimum, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def f_prime(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    iter_count = 0\n    while iter_count < max_iters:\n        gradient = f_prime(x)\n        x -= learning_rate * gradient\n        if abs(gradient) < tolerance:\n            break\n        iter_count += 1\n    return round(x, 4)\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function: a normalization-free transformation inspired by the Tanh function.\n    \n    Parameters:\n    x (np.ndarray): input array\n    alpha (float): scaling factor\n    gamma (float): shifting factor\n    beta (float): slope factor\n    \n    Returns:\n    list[float]: output of DyT function, rounded to the nearest 4th decimal\n    \"\"\"\n    dyt = (1 - np.exp(-2 * alpha * (x + gamma))) / (1 + np.exp(-2 * alpha * (x + gamma))) * beta\n    dyt = np.round(dyt, 4)\n    return dyt.tolist()\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
