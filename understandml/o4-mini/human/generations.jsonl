{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Compute the dot product of matrix `a` (n x m) with vector `b` (length m).\n    Return a list of length n, where result[i] = sum(a[i][j] * b[j] for j in range(m)).\n    If dimensions mismatch, return -1.\n    \"\"\"\n    # If matrix has no rows, return empty result\n    if not a:\n        return []\n    \n    # Number of columns in matrix a\n    m = len(a[0])\n    \n    # All rows must have the same number of columns\n    for row in a:\n        if len(row) != m:\n            return -1\n    \n    # The vector length must match number of columns\n    if len(b) != m:\n        return -1\n    \n    # Compute dot product for each row\n    result = []\n    for row in a:\n        dot = 0\n        for j in range(m):\n            dot += row[j] * b[j]\n        result.append(dot)\n    \n    return result\n```\n\nExplanation of the steps:\n1. If `a` is an empty list, there are no rows, so we return an empty list.\n2. Determine `m = len(a[0])`, the number of columns.\n3. Verify each row has exactly `m` elements; if not, return -1.\n4. Check that `len(b) == m`; if not, return -1.\n5. For each row in `a`, compute the sum of element-wise products with `b` and append it to `result`.\n6. Return the resulting list."}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n       Compute and return the transpose of the matrix `a`.\n    Each element at position (i, j) in the input becomes (j, i) in the output.\n\n    :param a: A matrix represented as a list of lists of numbers (int or float).\n    :return: The transposed matrix as a list of lists.\n       # Handle empty matrix\n    if not a:\n        return []\n\n    # Using zip to transpose: zip(*a) groups elements by column, then convert to lists\n    return [list(col) for col in zip(*a)]\n```\n\nExplanation:\n1. If the input matrix `a` is empty, we immediately return an empty list.\n2. The expression `zip(*a)` takes all rows of `a` and aggregates elements by their column index.\n3. Since `zip` returns tuples, we convert each tuple to a list to match the expected return type."}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshape the input 2D list 'a' into the shape specified by 'new_shape'.\n    If the total number of elements does not match, returns an empty list [].\n    \"\"\"\n    # Convert input list to numpy array\n    arr = np.array(a)\n    \n    # Unpack the target dimensions\n    try:\n        rows, cols = new_shape\n    except (TypeError, ValueError):\n        # new_shape is not a tuple of two ints\n        return []\n    \n    # Check if the total number of elements matches\n    if arr.size != rows * cols:\n        return []\n    \n    # Perform the reshape and convert back to Python list\n    return arr.reshape(rows, cols).tolist()\n```\n\nExplanation:\n1. We first convert the input list `a` into a NumPy array.\n2. We extract `rows` and `cols` from `new_shape`, guarding against invalid shapes.\n3. We check if the total number of elements (`arr.size`) matches `rows * cols`. If not, we immediately return `[]`.\n4. Otherwise, we reshape the array and call `.tolist()` to return a native Python list of lists."}
{"task_id": 4, "completion_id": 0, "completion": "```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a 2D matrix by rows or by columns.\n\n    Parameters:\n        matrix: A list of lists of floats representing the matrix.\n        mode: 'row' to compute the mean of each row,\n              'column' to compute the mean of each column.\n\n    Returns:\n        A list of floats containing the means in the specified mode.\n\n    Raises:\n        ValueError: If mode is not 'row' or 'column',\n                    if the matrix is irregular,\n                    or if any row/column is empty.\n    \"\"\"\n    # Validate mode\n    if mode not in ('row', 'column'):\n        raise ValueError(f\"Mode must be 'row' or 'column', got '{mode}'\")\n\n    # Empty matrix => empty result\n    if not matrix:\n        return []\n\n    if mode == 'row':\n        means = []\n        for idx, row in enumerate(matrix):\n            if not row:\n                raise ValueError(f\"Row {idx} is empty; cannot compute mean.\")\n            means.append(sum(row) / len(row))\n        return means\n\n    # mode == 'column'\n    # Ensure all rows have the same length\n    n_cols = len(matrix[0])\n    if n_cols == 0:\n        # First row empty => no columns\n        return []\n    for idx, row in enumerate(matrix):\n        if len(row) != n_cols:\n            raise ValueError(\n                f\"Row {idx} length ({len(row)}) does not match \"\n                f\"expected column count ({n_cols}).\"\n            )\n\n    n_rows = len(matrix)\n    means = []\n    for col_idx in range(n_cols):\n        col_sum = sum(matrix[row_idx][col_idx] for row_idx in range(n_rows))\n        means.append(col_sum / n_rows)\n    return means\n```\n\nExplanation of key points:\n- We first validate that `mode` is either `\"row\"` or `\"column\"`.\n- An empty matrix immediately returns an empty list of means.\n- In `\"row\"` mode, each row must be nonempty; we compute `sum(row)/len(row)` per row.\n- In `\"column\"` mode, we check that all rows share the same length (no jagged edges). If the first row is empty, there are no columns to average and we return `[]`. Otherwise we compute the mean of each column by summing over all rows and dividing by the number of rows.\n- On any invalid input (bad mode, irregular matrix, empty row/column), a `ValueError` is raised."}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiply each element of the given matrix by the provided scalar.\n    \n    Parameters:\n    - matrix: A list of lists of numbers (int or float), representing the matrix.\n    - scalar: A number (int or float) by which to scale each element of the matrix.\n    \n    Returns:\n    - A new matrix (list of lists) where each element is the original element multiplied by the scalar.\n    \n    Example:\n        >>> m = [\n        ...     [1, 2, 3],\n        ...     [4, 5, 6]\n        ... ]\n        >>> scalar_multiply(m, 2)\n        [[2, 4, 6], [8, 10, 12]]\n    \"\"\"\n    # Use nested list comprehensions to build the scaled matrix\n    return [\n        [element * scalar for element in row]\n        for row in matrix\n    ]\n\n\n# Example usage\nif __name__ == \"__main__\":\n    original = [\n        [1.5, -2, 3],\n        [0, 4,  -1]\n    ]\n    scaled = scalar_multiply(original, 3)\n    print(\"Original:\", original)\n    print(\"Scaled  :\", scaled)\n```\n\nExplanation:\n1. We define `scalar_multiply` with appropriate type hints.\n2. A nested list comprehension loops over each row and each element within the row.\n3. Each element is multiplied by `scalar`, producing a brand-new matrix."}
{"task_id": 6, "completion_id": 0, "completion": "```python\nfrom math import sqrt\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2\u00d72 matrix.\n    The matrix must be of the form [[a, b], [c, d]].\n    Returns a list of the two eigenvalues sorted from largest to smallest.\n    Raises ValueError if the matrix is not 2\u00d72 or if the eigenvalues are complex.\n    \"\"\"\n    # Check that the input is 2x2\n    if len(matrix) != 2 or any(len(row) != 2 for row in matrix):\n        raise ValueError(\"Input must be a 2\u00d72 matrix.\")\n    \n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Compute trace and determinant\n    trace = a + d\n    det = a * d - b * c\n    \n    # Discriminant of the characteristic polynomial \u03bb^2 \u2013 trace\u00b7\u03bb + det = 0\n    discriminant = trace * trace - 4 * det\n    \n    if discriminant < 0:\n        raise ValueError(\"This matrix has complex eigenvalues; only real eigenvalues are supported.\")\n    \n    # Compute the two eigenvalues\n    root = sqrt(discriminant)\n    eig1 = (trace + root) / 2\n    eig2 = (trace - root) / 2\n    \n    # Return them sorted descending\n    return sorted([eig1, eig2], reverse=True)\n```\n\nExample usage:\n\n```python\nm = [[4,  1],\n     [2,  3]]\n\nprint(calculate_eigenvalues(m))\n# Output: [5.0, 2.0]\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]],\n                     T: list[list[int|float]],\n                     S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Computes T^{-1} * A * S, validating that T and S are invertible\n    and that the shapes align for multiplication. Rounds result\n    to 4 decimal places. Returns -1 if there's no valid solution.\n    \"\"\"\n    # Convert inputs to numpy arrays of dtype float\n    try:\n        A_mat = np.array(A, dtype=float)\n        T_mat = np.array(T, dtype=float)\n        S_mat = np.array(S, dtype=float)\n    except Exception:\n        # Conversion failed\n        return -1\n\n    # Check shapes: T must be square, S must be square\n    if T_mat.ndim != 2 or S_mat.ndim != 2:\n        return -1\n    m, m2 = T_mat.shape\n    n, n2 = S_mat.shape\n\n    # A must be m x n to multiply T^{-1} A S\n    if A_mat.ndim != 2 or A_mat.shape != (m, n):\n        return -1\n\n    # Check invertibility of T and S via determinant or try/except\n    try:\n        T_inv = np.linalg.inv(T_mat)\n        S_inv = np.linalg.inv(S_mat)  # not used, but checks invertibility\n    except np.linalg.LinAlgError:\n        return -1\n\n    # Compute the transformation\n    result = T_inv.dot(A_mat).dot(S_mat)\n\n    # Round to nearest 4th decimal\n    result_rounded = np.round(result, decimals=4)\n\n    # Convert back to python list\n    return result_rounded.tolist()\n```\n\nExplanation of key steps:\n1. Convert inputs to NumPy arrays of float dtype.\n2. Verify that `T` is an m\u00d7m matrix, `S` is an n\u00d7n matrix, and `A` is m\u00d7n so the products are well-defined.\n3. Attempt to invert `T` and `S`; if either is singular, catch `LinAlgError` and return `-1`.\n4. Compute `T^{-1} * A * S`, round to 4 decimal places, and return as a Python list of lists."}
{"task_id": 8, "completion_id": 0, "completion": "```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n        matrix: A 2x2 matrix represented as a list of two lists, each containing two floats.\n                For example: [[a, b], [c, d]]\n    \n    Returns:\n        A new 2x2 matrix (list of lists) representing the inverse,\n        or None if the matrix is not invertible (determinant is zero).\n    \n    Raises:\n        ValueError: If the input is not a 2x2 matrix.\n    \"\"\"\n    # Validate input dimensions\n    if len(matrix) != 2 or any(len(row) != 2 for row in matrix):\n        raise ValueError(\"Input must be a 2x2 matrix (list of two lists of length 2).\")\n    \n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Compute determinant\n    det = a * d - b * c\n    if det == 0:\n        return None  # Not invertible\n    \n    inv_det = 1.0 / det\n    \n    # Inverse of [[a, b], [c, d]] is (1/det) * [[d, -b], [-c, a]]\n    return [\n        [ d * inv_det, -b * inv_det ],\n        [-c * inv_det,  a * inv_det ]\n    ]\n\n# Example usage:\n# >>> inverse_2x2([[4, 7], [2, 6]])\n# [[0.6, -0.7], [-0.2, 0.4]]\n```\n\nExplanation:\n1. We validate that the input is exactly a 2\u00d72 matrix.\n2. We compute the determinant `det = a*d - b*c`.\n3. If `det` is zero, the matrix is singular and not invertible, so we return `None`.\n4. Otherwise we multiply the adjugate matrix `[[d, -b], [-c, a]]` by `1/det` to get the inverse."}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int | float]],\n              b: list[list[int | float]]) -> list[list[int | float]]:\n       Multiply two matrices a and b and return the result.\n    If they cannot be multiplied (incompatible dimensions), return -1.\n       # Check that a and b are non-empty\n    if not a or not b:\n        return -1\n\n    # Number of rows and cols in a\n    rows_a = len(a)\n    cols_a = len(a[0])\n    # Ensure all rows in a have the same length\n    for row in a:\n        if len(row) != cols_a:\n            return -1\n\n    # Number of rows and cols in b\n    rows_b = len(b)\n    cols_b = len(b[0])\n    # Ensure all rows in b have the same length\n    for row in b:\n        if len(row) != cols_b:\n            return -1\n\n    # Check dimension compatibility: cols of a must equal rows of b\n    if cols_a != rows_b:\n        return -1\n\n    # Initialize result matrix with zeros\n    result: list[list[int | float]] = [\n        [0 for _ in range(cols_b)]\n        for _ in range(rows_a)\n    ]\n\n    # Perform multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            sum_ij = 0\n            for k in range(cols_a):\n                sum_ij += a[i][k] * b[k][j]\n            result[i][j] = sum_ij\n\n    return result\n```\n\nExample usage:\n\n```python\nm1 = [[1, 2, 3],\n      [4, 5, 6]]\nm2 = [[7,   8],\n      [9,  10],\n      [11, 12]]\n\nprint(matrixmul(m1, m2))\n# Output: [[58, 64], [139, 154]]\n```\n\nIf you try to multiply incompatible matrices:\n\n```python\nprint(matrixmul([[1, 2]], [[3, 4], [5, 6], [7, 8]]))\n# Returns -1 because 2 (cols of a) != 3 (rows of b)\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the sample covariance matrix for a set of feature vectors.\n    Each inner list in `vectors` represents all observations of one feature.\n    \n    Cov(X_i, X_j) = sum_k [(X_i[k] - mean_i) * (X_j[k] - mean_j)] / (n - 1)\n    \n    Arguments:\n    vectors -- list of m lists, each of length n, where m = number of features,\n               n = number of observations per feature.\n               \n    Returns:\n    m x m covariance matrix as a list of lists.\n    \n    Raises:\n    ValueError if input is empty, features have differing lengths, or < 2 observations.\n    \"\"\"\n    # Validate input\n    if not vectors:\n        raise ValueError(\"Input vector list is empty.\")\n    \n    m = len(vectors)\n    n = len(vectors[0])\n    if n < 2:\n        raise ValueError(\"At least two observations per feature are required.\")\n    \n    for vec in vectors:\n        if len(vec) != n:\n            raise ValueError(\"All feature vectors must have the same number of observations.\")\n    \n    # Compute means of each feature\n    means = [sum(vec) / n for vec in vectors]\n    \n    # Initialize covariance matrix (m x m) with zeros\n    cov_matrix = [[0.0] * m for _ in range(m)]\n    \n    # Compute upper triangle (and mirror to lower)\n    for i in range(m):\n        for j in range(i, m):\n            # Compute covariance between feature i and feature j\n            cov_ij = 0.0\n            for k in range(n):\n                cov_ij += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            cov_ij /= (n - 1)  # sample covariance denominator\n            \n            cov_matrix[i][j] = cov_ij\n            cov_matrix[j][i] = cov_ij  # symmetry\n    \n    return cov_matrix\n\n\n# ----------------------------\n# Test cases\n# ----------------------------\nif __name__ == \"__main__\":\n    import math\n    \n    def assert_matrix_almost_equal(mat1, mat2, tol=1e-9):\n        assert len(mat1) == len(mat2), \"Matrix dimension mismatch\"\n        for row1, row2 in zip(mat1, mat2):\n            assert len(row1) == len(row2), \"Matrix dimension mismatch\"\n            for a, b in zip(row1, row2):\n                assert math.isclose(a, b, rel_tol=tol, abs_tol=tol), f\"{a} != {b}\"\n    \n    # 1) Two identical features\n    X1 = [1.0, 2.0, 3.0]\n    X2 = [1.0, 2.0, 3.0]\n    cov = calculate_covariance_matrix([X1, X2])\n    # Cov(X1,X1)=1, Cov(X1,X2)=1, Cov(X2,X2)=1\n    expected = [[1.0, 1.0],\n                [1.0, 1.0]]\n    assert_matrix_almost_equal(cov, expected)\n    print(\"Test 1 passed.\")\n    \n    # 2) Perfect negative correlation\n    Y = [3.0, 2.0, 1.0]\n    cov = calculate_covariance_matrix([X1, Y])\n    # Cov(X1,X1)=1, Cov(Y,Y)=1, Cov(X1,Y)=-1\n    expected = [[1.0, -1.0],\n                [-1.0, 1.0]]\n    assert_matrix_almost_equal(cov, expected)\n    print(\"Test 2 passed.\")\n    \n    # 3) Three features with known values\n    F1 = [2.0, 4.0, 6.0, 8.0]\n    F2 = [1.0, 3.0, 5.0, 7.0]\n    F3 = [2.5, 2.5, 2.5, 2.5]  # zero variance\n    cov = calculate_covariance_matrix([F1, F2, F3])\n    # Compute by hand:\n    #   mean1=5, mean2=4, mean3=2.5\n    #   var1 = ((-3)^2+(-1)^2+1^2+3^2)/3 = (9+1+1+9)/3 = 20/3\n    #   var2 = ((-3)^2+(-1)^2+1^2+3^2)/3 = same = 20/3\n    #   cov12 = ((-3)*(-3)+(-1)*(-1)+(1)*(1)+(3)*(3))/3 = 20/3\n    #   cov13 = cov23 = 0\n    expected = [\n        [20.0/3.0, 20.0/3.0, 0.0],\n        [20.0/3.0, 20.0/3.0, 0.0],\n        [0.0,        0.0,      0.0]\n    ]\n    assert_matrix_almost_equal(cov, expected)\n    print(\"Test 3 passed.\")\n    \n    # 4) Error on mismatched lengths\n    try:\n        calculate_covariance_matrix([[1,2,3], [4,5]])\n        assert False, \"Expected ValueError for mismatched lengths\"\n    except ValueError:\n        print(\"Test 4 passed (mismatched lengths error).\")\n    \n    # 5) Error on too few observations\n    try:\n        calculate_covariance_matrix([[1.0]])\n        assert False, \"Expected ValueError for too few observations\"\n    except ValueError:\n        print(\"Test 5 passed (too few observations error).\")\n    \n    print(\"All tests passed.\")\n```\n\nExplanation of the approach:\n1. **Input validation**: Ensure the list of vectors is nonempty, all features have the same number of observations, and there are at least two observations for a meaningful covariance.\n2. **Mean calculation**: Compute the mean of each feature.\n3. **Covariance computation**: Use the sample covariance formula with denominator `(n - 1)`. Fill only the upper triangle of the matrix and mirror it to enforce symmetry.\n4. **Testing**: Several test cases cover identical features, perfect negative correlation, a zero\u2010variance feature, and error conditions."}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve the linear system A x = b using the Jacobi iterative method.\n\n    Parameters:\n    A : np.ndarray, shape (N, N)\n        Coefficient matrix (should be diagonally dominant for convergence).\n    b : np.ndarray, shape (N,) or (N,1)\n        Right\u2010hand side vector.\n    n : int\n        Number of Jacobi iterations to perform.\n\n    Returns:\n    x : list of float\n        Approximate solution vector after n iterations, each entry rounded to 4 decimals.\n    \"\"\"\n    # Ensure b is a flat 1D array\n    b_vec = b.flatten()\n    N = A.shape[0]\n    \n    # Initial guess x^(0) = zero vector\n    x_prev = np.zeros(N, dtype=float)\n    \n    # Jacobi iterations\n    for _ in range(n):\n        x_curr = np.zeros_like(x_prev)\n        for i in range(N):\n            # Sum of A[i,j] * x_prev[j] for j != i\n            sigma = 0.0\n            for j in range(N):\n                if j != i:\n                    sigma += A[i, j] * x_prev[j]\n            # Jacobi update\n            x_curr[i] = (b_vec[i] - sigma) / A[i, i]\n        \n        # Round each component to 4 decimal places\n        x_curr = np.round(x_curr, 4)\n        \n        # Prepare for next iteration\n        x_prev = x_curr\n\n    # Return as a Python list\n    return x_prev.tolist()\n```\n\nExplanation of key steps:\n1. We flatten `b` to ensure it\u2019s a one\u2010dimensional array.\n2. We initialize the iterate `x_prev` to zeros.\n3. In each iteration we compute each `x_curr[i]` by isolating it in the ith equation:\n   x_i = (b_i \u2212 \u2211_{j\u2260i} A[i,j] * x_prev[j]) / A[i,i].\n4. After each full update we round the vector to 4 decimal places.\n5. After `n` iterations we convert the final NumPy array to a Python list and return it."}
{"task_id": 12, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximate the SVD of a 2x2 matrix A using the Jacobi (one-step) method\n    on A^T A to get V and the singular values, then compute U = A V \u03a3^{-1}.\n    \n    Returns:\n        U (2\u00d72 ndarray), \n        S (2\u00d72 ndarray with singular values on the diagonal), \n        Vt (2\u00d72 ndarray, V transposed),\n    all entries rounded to 4 decimal places.\n    \"\"\"\n    assert A.shape == (2, 2), \"Input must be a 2x2 matrix.\"\n    \n    # 1) Form the symmetric matrix B = A^T A\n    B = A.T @ A\n    a = B[0, 0]\n    b = B[1, 1]\n    c = B[0, 1]  # = B[1, 0]\n    \n    # 2) Compute the Jacobi rotation angle to diagonalize B\n    if abs(c) < 1e-12:\n        phi = 0.0\n    else:\n        phi = 0.5 * np.arctan2(2 * c, a - b)\n    \n    cphi = np.cos(phi)\n    sphi = np.sin(phi)\n    \n    # 3) Build V from the rotation\n    V = np.array([[ cphi, -sphi],\n                  [ sphi,  cphi]])\n    \n    # 4) Eigenvalues of B\n    t  = (a + b) / 2.0\n    d  = np.hypot((a - b) / 2.0, c)\n    lam1 = t + d\n    lam2 = t - d\n    \n    # 5) Singular values are sqrt of eigenvalues (non-negative)\n    sigma1 = np.sqrt(max(lam1, 0.0))\n    sigma2 = np.sqrt(max(lam2, 0.0))\n    \n    # 6) Sort in descending order\n    if sigma1 < sigma2:\n        sigma1, sigma2 = sigma2, sigma1\n        V = V[:, ::-1]\n    \n    S = np.diag([sigma1, sigma2])\n    \n    # 7) Compute U = A V \u03a3^{-1}\n    U = np.zeros((2, 2))\n    # first column\n    if sigma1 > 1e-12:\n        U[:, 0] = (A @ V[:, 0]) / sigma1\n    # second column\n    if sigma2 > 1e-12:\n        U[:, 1] = (A @ V[:, 1]) / sigma2\n    \n    # 8) Transpose V for V^T\n    Vt = V.T\n    \n    # 9) Round to 4 decimal places\n    U  = np.round(U,  4)\n    S  = np.round(S,  4)\n    Vt = np.round(Vt, 4)\n    \n    return U, S, Vt\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([[3.0, 1.0],\n                  [1.0, 3.0]])\n    U, S, Vt = svd_2x2_singular_values(A)\n    print(\"U =\\n\", U)\n    print(\"S =\\n\", S)\n    print(\"Vt=\\n\", Vt)\n```\n\nExplanation of the steps:\n1. We form the symmetric matrix B = A\u1d40A.\n2. We compute a single Jacobi rotation angle \u03c6 that diagonalizes B.\n3. From \u03c6 we build the orthonormal matrix V.\n4. The eigenvalues \u03bb\u2081, \u03bb\u2082 of B are computed analytically.\n5. The singular values \u03c3\u2081, \u03c3\u2082 are the square roots of \u03bb\u2081, \u03bb\u2082.\n6. We sort \u03c3\u2081 \u2265 \u03c3\u2082 and reorder V accordingly.\n7. We get U by U = A\u00b7V\u00b7\u03a3\u207b\u00b9.\n8. We output V\u1d40 rather than V.\n9. Finally, all matrix entries are rounded to 4 decimal places."}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion recursively.\n    :param matrix: a list of 4 lists, each of length 4, representing the matrix\n    :return: determinant as a float\n    \"\"\"\n    # Validate input is 4x4\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"The input must be a 4x4 matrix.\")\n\n    def det_recursive(mat: list[list[float]]) -> float:\n        n = len(mat)\n        # Base cases\n        if n == 1:\n            return mat[0][0]\n        if n == 2:\n            return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n\n        # Recursive Laplace expansion along first row\n        total = 0.0\n        for col in range(n):\n            # Build the minor matrix by excluding row 0 and column `col`\n            minor = [\n                row[:col] + row[col+1 :]\n                for row in mat[1:]\n            ]\n            sign = (-1) ** col\n            total += sign * mat[0][col] * det_recursive(minor)\n        return total\n\n    # Call the recursive helper on the 4x4 matrix\n    return det_recursive(matrix)\n```\n\nExample usage:\n\n```python\nm = [\n    [3,  2, 0, 1],\n    [4,  0, 1, 2],\n    [3, -1, 2, 1],\n    [9,  2, 3, 1]\n]\nprint(determinant_4x4(m))  # Outputs the determinant of matrix m\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n       Performs linear regression using the normal equation.\n    Adds an intercept term automatically.\n    \n    Args:\n        X: A list of n samples, each sample is a list of p features.\n        y: A list of n target values.\n    \n    Returns:\n        A list of p+1 regression coefficients [theta0, theta1, ..., thetap],\n        rounded to 4 decimal places.\n       # Convert inputs to numpy arrays\n    X_mat = np.array(X, dtype=float)        # shape (n, p)\n    y_vec = np.array(y, dtype=float)        # shape (n,)\n\n    # Number of samples\n    n_samples = X_mat.shape[0]\n\n    # Add a column of ones for the intercept term\n    ones = np.ones((n_samples, 1), dtype=float)\n    X_aug = np.hstack((ones, X_mat))        # shape (n, p+1)\n\n    # Normal equation: theta = (X^T X)^(-1) X^T y\n    XtX = X_aug.T @ X_aug                   # shape (p+1, p+1)\n    Xty = X_aug.T @ y_vec                   # shape (p+1,)\n\n    theta = np.linalg.inv(XtX) @ Xty        # shape (p+1,)\n\n    # Round to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n\n    # Convert to a Python list and return\n    return theta_rounded.tolist()\n```\n\nExplanation of key steps:\n1. Convert the input lists to NumPy arrays for linear algebra operations.  \n2. Prepend a column of ones to `X` so that the first coefficient corresponds to the intercept term.  \n3. Compute \u03b8 = (X\u1d40X)\u207b\u00b9 X\u1d40y via the normal equation.  \n4. Round each coefficient to 4 decimal places (NumPy\u2019s `round` will produce `-0.0` if appropriate).  \n5. Return the coefficients as a regular Python list."}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, \n                                       y: np.ndarray, \n                                       alpha: float, \n                                       iterations: int) -> np.ndarray:\n       Performs linear regression using gradient descent.\n    \n    Parameters:\n    X          -- (m x n) NumPy array of features, where the first column is all ones\n    y          -- (m,) or (m x 1) NumPy array of targets\n    alpha      -- learning rate\n    iterations -- number of iterations to run gradient descent\n    \n    Returns:\n    A Python list of the learned parameters (theta), each rounded to 4 decimal places.\n       # Ensure y is a column vector of shape (m, 1)\n    m = y.shape[0]\n    y = y.reshape(m, 1)\n    \n    # Initialize theta to zeros (n x 1)\n    n = X.shape[1]\n    theta = np.zeros((n, 1))\n    \n    # Gradient descent loop\n    for _ in range(iterations):\n        predictions = X.dot(theta)          # (m x 1)\n        errors = predictions - y            # (m x 1)\n        gradient = (X.T.dot(errors)) / m    # (n x 1)\n        theta = theta - alpha * gradient    # (n x 1)\n    \n    # Round to 4 decimal places\n    theta = np.round(theta, 4)\n    \n    # Convert to a Python list and return\n    return theta.flatten().tolist()\n```\n\nExplanation of key steps:\n1. We reshape `y` into a column vector so matrix operations align.\n2. `theta` is initialized to zeros with as many rows as there are features (including the intercept).\n3. In each iteration, we compute predictions, measure the errors, compute the gradient, and update `theta`.\n4. After all iterations, we round each coefficient to four decimal places and return them as a Python list."}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n       Perform feature scaling on a dataset.\n\n    Parameters:\n    -----------\n    data : np.ndarray\n        A 2D array where each row is a sample and each column is a feature.\n\n    Returns:\n    --------\n    standardized_list : list[list[float]]\n        The data standardized (zero mean, unit variance), rounded to 4 decimals.\n    min_max_list : list[list[float]]\n        The data normalized to [0,1], rounded to 4 decimals.\n       # Standardization\n    means = np.mean(data, axis=0)\n    stds  = np.std(data, axis=0)           # population std (ddof=0)\n    # Prevent division by zero for constant features\n    stds_safe = np.where(stds == 0, 1, stds)\n    standardized = (data - means) / stds_safe\n    standardized = np.round(standardized, 4)\n\n    # Min-Max Normalization\n    mins   = np.min(data, axis=0)\n    maxs   = np.max(data, axis=0)\n    ranges = maxs - mins\n    # Prevent division by zero for constant features\n    ranges_safe = np.where(ranges == 0, 1, ranges)\n    min_max = (data - mins) / ranges_safe\n    min_max = np.round(min_max, 4)\n\n    # Convert to Python lists\n    standardized_list = standardized.tolist()\n    min_max_list      = min_max.tolist()\n\n    return standardized_list, min_max_list\n```"}
{"task_id": 17, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]],\n                       k: int,\n                       initial_centroids: list[tuple[float, float]],\n                       max_iterations: int) -> list[tuple[float, float]]:\n       Performs k-means clustering on 2D points.\n\n    Args:\n        points: List of (x, y) tuples.\n        k: Number of clusters.\n        initial_centroids: List of k initial (x, y) tuples.\n        max_iterations: Maximum number of iterations.\n\n    Returns:\n        List of k final centroids as (x, y) tuples, each rounded to 4 decimal places.\n       # Convert inputs to numpy arrays\n    X = np.array(points, dtype=float)                    # shape: (n_points, 2)\n    centroids = np.array(initial_centroids, dtype=float)  # shape: (k, 2)\n\n    for _ in range(max_iterations):\n        # Compute distances from each point to each centroid: shape (n_points, k)\n        # broadcasting: X[:, None, :] - centroids[None, :, :]\n        dists = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n\n        # Assign each point to the nearest centroid\n        labels = np.argmin(dists, axis=1)  # shape: (n_points,)\n\n        # Compute new centroids\n        new_centroids = np.zeros_like(centroids)\n        for cluster_idx in range(k):\n            members = X[labels == cluster_idx]\n            if len(members) > 0:\n                new_centroids[cluster_idx] = members.mean(axis=0)\n            else:\n                # No points assigned: keep the old centroid\n                new_centroids[cluster_idx] = centroids[cluster_idx]\n\n        # Check for convergence (if centroids do not change)\n        if np.allclose(new_centroids, centroids, atol=1e-8):\n            break\n\n        centroids = new_centroids\n\n    # Round final centroids to 4 decimal places and convert to list of tuples\n    centroids = np.round(centroids, 4)\n    final_centroids = [(float(c[0]), float(c[1])) for c in centroids]\n\n    return final_centroids\n```"}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray,\n                            y: np.ndarray,\n                            k: int = 5,\n                            shuffle: bool = True,\n                            random_seed: int = None):\n       Generate train/test indices for K-Fold cross-validation.\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Target vector of shape (n_samples,).\n    k : int, default=5\n        Number of folds.\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting into folds.\n    random_seed : int or None, default=None\n        Random seed for reproducibility (only used if shuffle=True).\n    \n    Returns\n    -------\n    folds : List[Tuple[np.ndarray, np.ndarray]]\n        A list of length k. Each element is a tuple (train_idx, test_idx)\n        where train_idx and test_idx are integer arrays of indices.\n       # Basic checks\n    n_samples = X.shape[0]\n    if y.shape[0] != n_samples:\n        raise ValueError(fX and y must have same number of samples:                         fn_samples} != {y.shape[0]    if k < 2 or k > n_samples:\n        raise ValueError(f must be >= 2 and <= n_samples ({n_samples}), got k={k    \n    # Generate indices and shuffle if requested\n    indices = np.arange(n_samples)\n    if shuffle:\n        rng = np.random.RandomState(random_seed)\n        rng.shuffle(indices)\n    \n    # Compute fold sizes (some folds may be 1 sample larger to account for remainder)\n    base_size = n_samples // k\n    remainder = n_samples % k\n    \n    folds = []\n    start = 0\n    for fold_idx in range(k):\n        # Distribute the remainder among the first `remainder` folds\n        fold_size = base_size + (1 if fold_idx < remainder else 0)\n        stop = start + fold_size\n        \n        test_idx = indices[start:stop]\n        train_idx = np.concatenate((indices[:start], indices[stop:]), axis=0)\n        \n        folds.append((train_idx, test_idx))\n        start = stop\n    \n    return folds\n\n\n# Example usage:\nif __name__ ==main    X = np.arange(20).reshape(10, 2)\n    y = np.arange(10)\n    splits = k_fold_cross_validation(X, y, k=4, shuffle=True, random_seed=42)\n    for i, (train_ix, test_ix) in enumerate(splits):\n        print(fFold {i+1        print  Train indices train_ix)\n        print  Test indices: test_ix)\n        print()\n```\n\nExplanation of main steps:\n\n1. **Input validation**: Check that `X` and `y` have the same number of samples, and that `k` is in a valid range.  \n2. **Index generation & shuffling**: Create an array `indices = np.arange(n_samples)` and optionally shuffle it with a fixed seed for reproducibility.  \n3. **Fold size calculation**: Compute a base fold size `n_samples // k` and distribute any leftover samples (`n_samples % k`) one by one to the first few folds, ensuring sizes differ by at most one.  \n4. **Split generation**: For each fold, pick a contiguous slice of `indices` as the test set and use the remainder as the training set. Append `(train_idx, test_idx)` to the result list.  \n\nThe returned `folds` list can then be used to train and evaluate any model in a loop."}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform PCA on the input data from scratch.\n    \n    Parameters:\n    - data: 2D numpy array of shape (n_samples, n_features)\n      where each row is a sample and each column is a feature.\n    - k: int\n      Number of principal components to return.\n      \n    Returns:\n    - List of k principal components, each being a list of length n_features.\n      Components are sorted by descending explained variance and rounded to\n      4 decimal places.\n    \"\"\"\n    # 1. Check inputs\n    if not isinstance(data, np.ndarray):\n        raise TypeError(\"data must be a numpy ndarray\")\n    if data.ndim != 2:\n        raise ValueError(\"data must be a 2D array of shape (n_samples, n_features)\")\n    n_samples, n_features = data.shape\n    if k <= 0 or k > n_features:\n        raise ValueError(\"k must be between 1 and n_features\")\n\n    # 2. Standardize the data (zero mean, unit variance per feature)\n    #    Mean:\n    mean = np.mean(data, axis=0)\n    #    Standard deviation (population std, ddof=0)\n    std = np.std(data, axis=0)\n    #    Avoid division by zero for constant features:\n    std_replaced = np.where(std == 0, 1.0, std)\n    data_std = (data - mean) / std_replaced\n\n    # 3. Compute covariance matrix (features \u00d7 features)\n    #    Using the unbiased estimator (divide by n_samples - 1)\n    cov_matrix = (data_std.T @ data_std) / (n_samples - 1)\n\n    # 4. Eigen decomposition of the covariance matrix\n    #    Since cov_matrix is symmetric, we can use eigh\n    eigvals, eigvecs = np.linalg.eigh(cov_matrix)\n    # eigvals: shape (n_features,)\n    # eigvecs: shape (n_features, n_features), columns are eigenvectors\n\n    # 5. Sort eigenvalues (and eigenvectors) in descending order\n    idx_desc = np.argsort(eigvals)[::-1]\n    topk_idx = idx_desc[:k]\n    topk_vecs = eigvecs[:, topk_idx]   # shape (n_features, k)\n\n    # 6. Round to 4 decimal places and convert to list of lists\n    #    We want a list of k components, each of length n_features\n    topk_vecs_rounded = np.round(topk_vecs, 4)\n    principal_components = [\n        topk_vecs_rounded[:, i].tolist() for i in range(k)\n    ]\n\n    return principal_components\n```\n\nExplanation of key steps:\n1. **Standardization**  \n   - Subtract mean of each feature.  \n   - Divide by standard deviation (population \u03c3).  \n   - Replace zeros in \u03c3 with 1 to avoid division by zero.\n\n2. **Covariance Matrix**  \n   - Computed as (X\u1d40\u00b7X) / (n_samples\u22121) on the standardized data.\n\n3. **Eigen Decomposition**  \n   - Use `np.linalg.eigh` for the symmetric covariance matrix to get eigenvalues and eigenvectors.\n\n4. **Sorting and Selection**  \n   - Sort eigenvalues in descending order; pick the top-k indices.  \n   - Extract corresponding eigenvectors as the principal components.\n\n5. **Output**  \n   - Round each component to 4 decimal places.  \n   - Return as a list of `k` lists, each list representing one principal component."}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n       Implements the ID3 decision tree learning algorithm.\n    examples: a list of records, each record is a dict mapping attribute names to values (including target_attr)\n    attributes: list of attribute names (excluding target_attr) to consider for splits\n    target_attr: the key in each example whose value is the class label\n    Returns a nested dictionary representing the learned tree:\n      - A leaf is represented simply by its class label (e.g.Yes orNo      - An internal node is { best_attr: { value1: subtree1, value2: subtree2, ... } }\n       def majority_value(exs: list[dict]):\n       Return the most common target value in exs, or None if exs is empty        if not exs:\n            return None\n        counter = Counter(e[target_attr] for e in exs)\n        return counter.most_common(1)[0][0]\n\n    def entropy(exs: list[dict]) -> float:\n       Compute entropy of the target attribute over the examples        total = len(exs)\n        if total == 0:\n            return 0.0\n        counter = Counter(e[target_attr] for e in exs)\n        ent = 0.0\n        for cnt in counter.values():\n            p = cnt / total\n            ent -= p * math.log2(p)\n        return ent\n\n    def information_gain(exs: list[dict], attr: str) -> float:\n       Compute the information gain of splitting on attribute attr        base_ent = entropy(exs)\n        total = len(exs)\n        # Partition by each value of attr\n        subsets = {}\n        for e in exs:\n            subsets.setdefault(e[attr], []).append(e)\n        # Weighted sum of entropies\n        cond_ent = sum((len(sub) / total) * entropy(sub) for sub in subsets.values())\n        return base_ent - cond_ent\n\n    def dt(exs: list[dict], attrs: list[str], default) -> dict:\n               The recursive ID3 routine.\n        exs: current subset of examples\n        attrs: attributes left to split on\n        default: the default classification if exs is empty\n               # 1. If no examples left, return the default\n        if not exs:\n            return default\n        # 2. If all examples have the same label, return that label\n        labels = [e[target_attr] for e in exs]\n        if labels.count(labels[0]) == len(labels):\n            return labels[0]\n        # 3. If no attributes left to split, return majoritarian label\n        if not attrs:\n            return majority_value(exs)\n        # 4. Otherwise, pick the attribute with highest information gain\n        gains = [(information_gain(exs, a), a) for a in attrs]\n        _, best_attr = max(gains, key=lambda x: x[0])\n        tree = {best_attr: {}}\n        # 5. For each possible value of best_attr, build a subtree\n        values = set(e[best_attr] for e in exs)\n        remaining = [a for a in attrs if a != best_attr]\n        for v in values:\n            subset = [e for e in exs if e[best_attr] == v]\n            subtree = dt(subset, remaining, majority_value(exs))\n            tree[best_attr][v] = subtree\n        return tree\n\n    # Compute the overall majority label to use as default\n    default_label = majority_value(examples)\n    return dt(examples, attributes, default_label)\n```\n\nExplanation of the main parts:\n\n1. **entropy**:  \n   Computes \u2013\u2211 p\u1d62 log\u2082 p\u1d62 over the distribution of class labels in the current example set.\n\n2. **information_gain**:  \n   Measures how much uncertainty is reduced by splitting on a given attribute.\n\n3. **dt (recursive ID3)**:\n   - If no examples remain, returns a default label.\n   - If all examples have the same class label, returns that label (leaf).\n   - If no splitting attributes remain, returns the majority label.\n   - Otherwise, picks the attribute with maximum information gain, splits the data by each of its values, and recurses.\n\nThe returned structure is a nested dictionary.  Leaves are simply the class label strings; internal nodes are dictionaries whose single key is the splitting attribute and whose value is a dict mapping each attribute-value to a subtree."}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray,\n                       labels: np.ndarray,\n                       kernel='linear',\n                       lambda_val=0.01,\n                       iterations=100,\n                       sigma=1.0):\n    \"\"\"\n    Deterministic Pegasos kernel SVM.\n    Returns:\n      alpha_list: list of alpha coefficients (length N)\n      bias_list:  single\u2010element list [bias]\n    All values rounded to 4 decimal places.\n    \"\"\"\n    n_samples, n_features = data.shape\n    y = labels.astype(float)\n\n    # Precompute Kernel matrix\n    if kernel == 'linear':\n        K = data.dot(data.T)\n    elif kernel == 'rbf':\n        # pairwise squared distances\n        sq = np.sum(data**2, axis=1, keepdims=True)\n        dist2 = sq + sq.T - 2 * data.dot(data.T)\n        K = np.exp(-dist2 / (2 * sigma**2))\n    else:\n        raise ValueError(\"Unsupported kernel. Choose 'linear' or 'rbf'.\")\n\n    # Initialize alpha and bias\n    alpha = np.zeros(n_samples, dtype=float)\n    b = 0.0\n\n    for t in range(1, iterations + 1):\n        # learning rate\n        eta = 1.0 / (lambda_val * t)\n\n        # compute decision values f_i = sum_j alpha_j * y_j * K[j,i] + b\n        # vectorized: (alpha * y) @ K  --> shape (n_samples,)\n        f = (alpha * y).dot(K) + b\n\n        # find margin-violating indices: y_i * f_i < 1\n        viol = (y * f) < 1\n\n        # gradient step on alpha\n        alpha = (1 - eta * lambda_val) * alpha\n        # add contribution from violators\n        alpha[viol] += (eta / n_samples) * y[viol]\n\n        # update bias (unregularized)\n        b += (eta / n_samples) * np.sum(y[viol])\n\n        # projection step: ensure ||w|| <= 1/sqrt(lambda)\n        # ||w||^2 = (alpha * y)^T K (alpha * y)\n        ay = alpha * y\n        norm_sq = ay.dot(K.dot(ay))\n        if norm_sq > 0:\n            norm = np.sqrt(norm_sq)\n            factor = min(1.0, 1.0 / (np.sqrt(lambda_val) * norm))\n            alpha *= factor\n            # bias is not part of w so not scaled\n\n    # round results to 4 decimal places\n    alpha = np.round(alpha, 4)\n    b = round(b, 4)\n\n    # return as python lists\n    return alpha.tolist(), [b]\n```\n\nExplanation of the main steps:\n\n1. **Kernel matrix**:  \n   - For linear: simply `X\u00b7X\u1d40`.  \n   - For RBF: compute pairwise squared distances and apply `exp(-||x\u2013x'||\u00b2 / (2\u03c3\u00b2))`.\n\n2. **Alpha & bias initialization**:  \n   - `alpha` starts at zero (length = number of samples).  \n   - `b` starts at 0.\n\n3. **Iterative updates** (full\u2010batch every iteration):  \n   - Learning rate \u03b7\u209c = 1/(\u03bb\u00b7t).  \n   - Compute `f = (\u03b1 \u2218 y)\u00b7K + b`.  \n   - Find violators: those with `y\u00b7f < 1`.  \n   - Gradient step:  \n       \u03b1 \u2190 (1 \u2013 \u03b7\u03bb)\u00b7\u03b1 + (\u03b7/N)\u00b7y for violators.  \n       b \u2190 b + (\u03b7/N)\u00b7\u03a3 y_over_violators.  \n   - Projection: ensure the implicit weight vector w (in feature space) obeys \u2016w\u2016 \u2264 1/\u221a\u03bb by scaling \u03b1 if necessary.\n\n4. **Rounding & output**:  \n   - \u03b1 and b are rounded to 4 decimal places.  \n   - Returned as Python lists via `.tolist()`.\n"}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: Sigmoid output rounded to four decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)\n```\n\nExample usage:\n```python\nprint(sigmoid(0))    # 0.5\nprint(sigmoid(2.0))  # 0.8808\nprint(sigmoid(-3.5)) # 0.0293\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n       Compute the softmax of a list of scores.\n\n    Args:\n        scores (list[float]): Input scores.\n\n    Returns:\n        list[float]: Softmax probabilities rounded to four decimal places.\n       if not scores:\n        return []\n\n    # Numerical stability: subtract max score\n    max_score = max(scores)\n    exps = [math.exp(s - max_score) for s in scores]\n    sum_exps = sum(exps)\n\n    return [round(exp_i / sum_exps, 4) for exp_i in exps]\n\n\n# Example usage\nif __name__ ==main    scores = [2.0, 1.0, 0.1]\n    print(softmax(scores))  # Output: [0.6590, 0.2424, 0.0986]\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]],\n                        labels: list[int],\n                        weights: list[float],\n                        bias: float) -> (list[float], float):\n       Simulates a single neuron with sigmoid activation.\n\n    Args:\n      features: List of m examples, each a list of n features.\n      labels:   List of m true binary labels (0 or 1).\n      weights:  List of n weights, one per feature.\n      bias:     Scalar bias term.\n\n    Returns:\n      preds:    An m\u00d71 Python list of predicted probabilities (rounded to 4 d.p.).\n      mse:      Mean squared error between predicted probabilities and labels\n                (rounded to 4 d.p.).\n       # Convert inputs to numpy arrays\n    X = np.array(features)        # shape (m, n)\n    w = np.array(weights)         # shape (n,)\n    y = np.array(labels)          # shape (m,)\n\n    # Linear combination z = X\u00b7w + b\n    z = X.dot(w) + bias           # shape (m,)\n\n    # Sigmoid activation\n    probs = 1.0 / (1.0 + np.exp(-z))\n\n    # Round probabilities to 4 decimal places\n    probs_rounded = np.round(probs, 4)\n\n    # Compute mean squared error (use the un\u2010rounded probs for the MSE calc)\n    mse = float(np.mean((probs - y) ** 2))\n    mse_rounded = round(mse, 4)\n\n    # Reshape to (m,1) and convert to pure Python list\n    preds_list = probs_rounded.reshape(-1, 1).tolist()\n\n    return preds_list, mse_rounded\n```\n\nExplanation of key steps:\n1. We load the features, weights, and labels into NumPy arrays for easy vectorized operations.  \n2. We compute the linear output `z = X\u00b7w + b`.  \n3. We apply the sigmoid function to obtain predicted probabilities.  \n4. Predictions are rounded to 4 decimal places via `np.round`.  \n5. The mean squared error is computed on the unrounded probabilities, then rounded to 4 decimal places.  \n6. Finally, we reshape the rounded predictions to `(m,1)` and call `.tolist()` to return a pure Python nested list, as requested."}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_neuron(\n    features: np.ndarray,\n    labels: np.ndarray,\n    initial_weights: np.ndarray,\n    initial_bias: float,\n    learning_rate: float,\n    epochs: int\n) -> (list[float], float, list[float]):\n       Trains a single neuron with sigmoid activation using backpropagation\n    and MSE loss.\n\n    Args:\n      features:     shape (n_samples, n_features)\n      labels:       shape (n_samples,)\n      initial_weights: shape (n_features,)\n      initial_bias: float\n      learning_rate: float\n      epochs:        int\n\n    Returns:\n      weights_list: final weights as a Python list (rounded to 4 decimals)\n      bias:         final bias as float (rounded to 4 decimals)\n      mse_history:  list of MSE values per epoch (each rounded to 4 decimals)\n       # Copy initial parameters\n    w = initial_weights.astype(float).copy()\n    b = float(initial_bias)\n    n = features.shape[0]\n\n    mse_history = []\n\n    # Sigmoid function\n    def sigmoid(x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    for epoch in range(epochs):\n        # Forward pass\n        z = np.dot(features, w) + b               # shape (n,)\n        y_pred = sigmoid(z)                       # shape (n,)\n\n        # Compute MSE\n        errors = y_pred - labels                  # shape (n,)\n        mse = np.mean(errors**2)\n        mse_history.append(round(mse, 4))\n\n        # Backpropagation (gradients)\n        # dL/dy_pred = 2*(y_pred - y_true)/n\n        dL_dy = 2 * errors / n                    # shape (n,)\n        # dy/dz = y_pred*(1 - y_pred)\n        dy_dz = y_pred * (1 - y_pred)             # shape (n,)\n        # dL/dz = dL/dy * dy/dz\n        dL_dz = dL_dy * dy_dz                     # shape (n,)\n\n        # Gradient w.r.t. weights: features.T @ dL_dz\n        grad_w = np.dot(features.T, dL_dz)        # shape (n_features,)\n        # Gradient w.r.t. bias: sum of dL_dz\n        grad_b = np.sum(dL_dz)                    # scalar\n\n        # Update parameters\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n    # Round final parameters\n    w_rounded = np.round(w, 4)\n    b_rounded = float(np.round(b, 4))\n\n    # Convert weights to python list\n    weights_list = w_rounded.tolist()\n\n    return weights_list, b_rounded, mse_history\n```\n\nExplanation of key steps:\n1. **Forward pass**: compute `z = X\u00b7w + b` and `y_pred = sigmoid(z)`.\n2. **Loss**: MSE = mean((y_pred \u2013 y_true)\u00b2).\n3. **Gradients**:\n   - dL/dy_pred = 2*(y_pred \u2013 y_true)/n\n   - dy_pred/dz = y_pred*(1 \u2013 y_pred)\n   - dL/dz = (dL/dy_pred) * (dy_pred/dz)\n   - grad_w = X\u1d40\u00b7(dL/dz), grad_b = sum(dL/dz)\n4. **Parameter update** via gradient descent.\n5. **Rounding** all final results to 4 decimals and returning weights as a Python list."}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        # The scalar value\n        self.data = data\n        # The gradient (derivative of some scalar loss w.r.t. this node)\n        self.grad = 0.0\n        # A function that, when called, will propagate gradients to this node's children\n        self._backward = lambda: None\n        # The set of child nodes (operands) that produced this node\n        self._prev = set(_children)\n        # The operation that produced this node (for graph visualization/debugging)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            # \u2202(a+b)/\u2202a = 1, \u2202(a+b)/\u2202b = 1\n            self.grad += out.grad * 1.0\n            other.grad += out.grad * 1.0\n\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other):\n        # support other + self when other is not a Value\n        return self + other\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            # \u2202(a*b)/\u2202a = b, \u2202(a*b)/\u2202b = a\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        # support other * self when other is not a Value\n        return self * other\n\n    def __neg__(self):\n        # -self\n        return self * -1\n\n    def __sub__(self, other):\n        # self - other = self + (-other)\n        return self + (-other)\n\n    def __rsub__(self, other):\n        # other - self\n        return other + (-self)\n\n    def relu(self):\n        # ReLU activation: max(0, x)\n        out = Value(self.data if self.data > 0 else 0.0, (self,), 'ReLU')\n\n        def _backward():\n            # gradient is passed only if input > 0\n            self.grad += (out.grad * (1.0 if self.data > 0 else 0.0))\n\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        # Topological ordering of nodes in the graph (dependencies first)\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n\n        # The gradient of the output w.r.t. itself is 1\n        self.grad = 1.0\n        # Traverse in reverse topological order and call each node's _backward\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return fValue(data={self.data}, grad={self.grad})```\n\nExplanation of key parts:\n\n1. Addition (`__add__`):  \n   - Forward pass computes `self.data + other.data`.  \n   - Backward pass distributes the upstream gradient to both operands.\n\n2. Multiplication (`__mul__`):  \n   - Forward pass computes `self.data * other.data`.  \n   - Backward pass applies the product rule.\n\n3. ReLU (`relu`):  \n   - Forward pass outputs `max(0, x)`.  \n   - Backward pass only passes gradient when the input was positive.\n\n4. `backward()`:  \n   - Builds a topologically sorted list of all nodes in the computation graph.  \n   - Initializes the gradient of the final output to 1.  \n   - Walks the graph in reverse order, calling each node\u2019s `_backward` to propagate gradients.\n\nYou can now use this `Value` class to build scalar computation graphs and compute gradients automatically. Here\u2019s a quick test:\n\n```python\n# Example usage\na = Value(2.0)\nb = Value(-3.0)\nc = a * b\nd = c + a\ne = d.relu()\ne.backward()\n\nprint(a, b, c, d, e)\n# a.grad, b.grad, c.grad, d.grad, e.grad should be populated correctly\n```"}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the matrix P that transforms coordinate vectors from basis B to basis C in R^3.\n    If v_B are the coords of v in basis B, and v_C the coords in basis C, then\n        v = B_mat @ v_B  and  v = C_mat @ v_C\n    implies C_mat @ v_C = B_mat @ v_B  =>  v_C = C_mat^{-1} @ B_mat @ v_B.\n    Hence P = C_mat^{-1} @ B_mat.\n\n    Args:\n      B: a list of three basis vectors for B, each a list of 3 ints.\n      C: a list of three basis vectors for C, each a list of 3 ints.\n\n    Returns:\n      P as a 3x3 list of floats, rounded to 4 decimal places.\n    \"\"\"\n    # Form the 3x3 matrices whose columns are the basis vectors\n    B_mat = np.array(B, dtype=float).T\n    C_mat = np.array(C, dtype=float).T\n\n    # Solve for P: C_mat @ P = B_mat  =>  P = inv(C_mat) @ B_mat\n    P = np.linalg.inv(C_mat).dot(B_mat)\n\n    # Round to nearest 4th decimal and return as python list\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()\n```\n\nExample usage:\n\n```python\nB = [[1, 0, 1],\n     [0, 1, 2],\n     [1, 1, 1]]\nC = [[2, 0, 1],\n     [0, 1, 0],\n     [1, 1, 1]]\n\nP = transform_basis(B, C)\nprint(P)\n# e.g. [[...], [...], [...]]  # each entry rounded to 4 decimal places\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix A\n    using eigen-decomposition of A^T A.\n\n    Returns U, S, V such that A = U @ S @ V,\n    with all entries rounded to the nearest 1e-4 and returned as Python lists.\n    \"\"\"\n    # Check shape\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix must be 2x2.\")\n\n    # Compute A^T A\n    ATA = A.T @ A\n\n    # Eigen-decomposition of A^T A\n    # eigh returns eigenvalues in ascending order\n    eigvals, eigvecs = np.linalg.eigh(ATA)\n    # Sort by descending eigenvalue\n    idx = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[idx]\n    V = eigvecs[:, idx]   # columns are the eigenvectors v1, v2\n\n    # Singular values are square roots of the eigenvalues\n    sigma1 = np.sqrt(max(eigvals[0], 0.0))\n    sigma2 = np.sqrt(max(eigvals[1], 0.0))\n\n    # Build U by normalizing A @ v_i\n    U = np.zeros((2, 2))\n    # First column\n    if sigma1 > 1e-12:\n        U[:, 0] = (A @ V[:, 0]) / sigma1\n    else:\n        # when sigma1 is zero, pick a default unit vector\n        U[:, 0] = np.array([1.0, 0.0])\n    # Second column\n    if sigma2 > 1e-12:\n        U[:, 1] = (A @ V[:, 1]) / sigma2\n    else:\n        # pick a vector orthogonal to U[:,0]\n        U[:, 1] = np.array([-U[1, 0], U[0, 0]])\n\n    # Ensure U is a proper rotation (determinant = +1)\n    if np.linalg.det(U) < 0:\n        U[:, 1] *= -1\n        V[:, 1] *= -1\n\n    # Assemble the singular value matrix\n    S = np.zeros((2, 2))\n    S[0, 0] = sigma1\n    S[1, 1] = sigma2\n\n    # Round to 4 decimal places\n    U_rounded  = np.round(U, 4)\n    S_rounded  = np.round(S, 4)\n    Vt_rounded = np.round(V.T, 4)\n\n    # Return as Python lists\n    return U_rounded.tolist(), S_rounded.tolist(), Vt_rounded.tolist()\n```\n\nExplanation of the steps:\n\n1.  We compute \\(A^T A\\) and perform an eigen-decomposition using `np.linalg.eigh`.  \n2.  The eigenvalues \\(\\lambda_1,\\lambda_2\\) (sorted descending) give singular values \\(\\sigma_i = \\sqrt{\\lambda_i}\\).  \n3.  The columns of \\(V\\) are the corresponding normalized eigenvectors.  \n4.  We form \\(U\\) by normalizing the columns \\(A\\,v_i / \\sigma_i\\).  If a singular value is zero, we pick an arbitrary orthonormal direction.  \n5.  We enforce \\(\\det(U)=+1\\) by possibly flipping a column, keeping the decomposition consistent.  \n6.  Finally, we round every entry to 4 decimal places and return `U`, `S`, and `V^T` as Python lists so that  \n   \\[\n     A \\;=\\; U \\;\\times\\; S \\;\\times\\; V\\,.\n   \\]"}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle two numpy arrays in unison, preserving correspondence between X and y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        Feature matrix (n_samples, ...).\n    y : numpy.ndarray\n        Label array (n_samples, ...) aligned with X.\n    seed : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    X_shuffled_list : list\n        Shuffled X converted to a Python list.\n    y_shuffled_list : list\n        Shuffled y converted to a Python list.\n    \"\"\"\n    # Optionally set the random seed\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Ensure X and y have the same first dimension\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples.\")\n\n    # Generate a permutation of indices and apply to both arrays\n    permuted_idx = np.random.permutation(X.shape[0])\n    X_shuffled = X[permuted_idx]\n    y_shuffled = y[permuted_idx]\n\n    # Convert to Python lists and return\n    return X_shuffled.tolist(), y_shuffled.tolist()\n```\n\nExample usage:\n\n```python\nimport numpy as np\n\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([0, 1, 0, 1])\n\nX_shuf, y_shuf = shuffle_data(X, y, seed=42)\nprint(X_shuf)  # e.g. [[7, 8], [1, 2], [5, 6], [3, 4]]\nprint(y_shuf)  # e.g. [1, 0, 0, 1]\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    A batch iterator that yields batches of X (and y if provided).\n    Each batch is returned as a Python list (via ndarray.tolist()).\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, ...)\n        The input data.\n    y : array-like of shape (n_samples,), optional\n        The target values. If provided, must have the same length as X.\n    batch_size : int, default=64\n        The number of samples per batch.\n\n    Yields\n    ------\n    batch_X : list\n        A batch of X converted to a Python list.\n    batch_y : list, optional\n        A batch of y converted to a Python list (only if y is provided).\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X = np.asarray(X)\n    if y is not None:\n        y = np.asarray(y)\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(f\"X and y must have the same number of samples; \"\n                             f\"got len(X)={X.shape[0]} and len(y)={y.shape[0]}.\")\n\n    n_samples = X.shape[0]\n    # Iterate over batches\n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        batch_X = X[start:end]\n        if y is not None:\n            batch_y = y[start:end]\n            yield batch_X.tolist(), batch_y.tolist()\n        else:\n            yield batch_X.tolist()\n```\n\nExample usage:\n\n```python\nX = np.arange(20).reshape(20, 1)\ny = np.arange(20)\n\nfor bx, by in batch_iterator(X, y, batch_size=6):\n    print(\"X batch:\", bx)\n    print(\"y batch:\", by)\n```\n\nThis will print batches of up to 6 samples each, and each `bx`/`by` is a plain Python list."}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide dataset X based on whether the value of feature at index feature_i\n    is greater than or equal to a given threshold.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        The input dataset.\n    feature_i : int\n        The index of the feature (column) to split on.\n    threshold : float\n        The threshold value to compare the feature against.\n\n    Returns\n    -------\n    X_ge : list\n        List of samples (as lists) where X[:, feature_i] >= threshold.\n    X_lt : list\n        List of samples (as lists) where X[:, feature_i] < threshold.\n    \"\"\"\n    # Ensure X is a NumPy array\n    X = np.array(X)\n    \n    # Create boolean mask for samples where feature >= threshold\n    ge_mask = X[:, feature_i] >= threshold\n    \n    # Split the data\n    X_ge = X[ge_mask]\n    X_lt = X[~ge_mask]\n    \n    # Convert to Python lists and return\n    return X_ge.tolist(), X_lt.tolist()\n```\n\nExample usage:\n\n```python\nif __name__ == \"__main__\":\n    # Sample dataset: 5 samples, 3 features each\n    data = [\n        [2.5, 1.0, 0.5],\n        [1.2, 3.4, 2.1],\n        [4.8, 0.9, 3.3],\n        [3.0, 2.2, 1.1],\n        [0.5, 4.5, 2.2]\n    ]\n\n    # Split on feature index 0 with threshold 3.0\n    above_or_equal, below = divide_on_feature(data, feature_i=0, threshold=3.0)\n\n    print(\">= 3.0 on feature 0:\", above_or_equal)\n    print(\"<  3.0 on feature 0:\", below)\n```\n\nThis will output:\n\n```\n>= 3.0 on feature 0: [[4.8, 0.9, 3.3], [3.0, 2.2, 1.1]]\n<  3.0 on feature 0: [[2.5, 1.0, 0.5], [1.2, 3.4, 2.1], [0.5, 4.5, 2.2]]\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n       Generate polynomial features up to a given degree for input array X.\n\n    Parameters:\n    X : array-like, shape (n_samples, n_features)\n        Input data.\n    degree : int\n        The maximum degree of polynomial features.\n\n    Returns:\n    list of lists, shape (n_samples, n_output_features)\n        The matrix of features, where each column is a polynomial combination\n        of the input features up to the specified degree.\n       X = np.asarray(X)\n    if X.ndim != 2:\n        raise ValueError(fX must be a 2D array, got array with shape {X.shape    if not isinstance(degree, int) or degree < 0:\n        raise ValueError(fdegree must be a non-negative integer, got {degree    n_samples, n_features = X.shape\n    # Start with bias term (degree 0)\n    features = [np.ones((n_samples, 1), dtype=X.dtype)]\n\n    # For each degree from 1 to `degree`, build combinations\n    for deg in range(1, degree + 1):\n        for comb in combinations_with_replacement(range(n_features), deg):\n            # X[:, comb] has shape (n_samples, deg); take product along axis=1\n            col = np.prod(X[:, comb], axis=1).reshape(n_samples, 1)\n            features.append(col)\n\n    # Stack all feature columns horizontally\n    X_poly = np.hstack(features)\n    return X_poly.tolist()\n```\n\nExample usage:\n```python\nif __name__ ==main    X = np.array([[2, 3],\n                  [4, 5],\n                  [6, 7]])\n    # degree = 2 will produce columns: [1, x1, x2, x1^2, x1*x2, x2^2]\n    poly = polynomial_features(X, 2)\n    for row in poly:\n        print(row)\n```\nThis will output:\n```\n[1.0, 2.0, 3.0, 4.0, 6.0, 9.0]\n[1.0, 4.0, 5.0, 16.0, 20.0, 25.0]\n[1.0, 6.0, 7.0, 36.0, 42.0, 49.0]\n```"}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets (bootstrapped or without replacement) of a dataset.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Feature matrix.\n    y : np.ndarray, shape (n_samples,)\n        Target vector.\n    n_subsets : int\n        Number of random subsets to generate.\n    replacements : bool, default=True\n        If True, sample with replacement (bootstrap). If False, sample without replacement.\n    seed : int, default=42\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    List[Tuple[List[List], List]]\n        A list of length `n_subsets`. Each element is a tuple:\n          (X_subset_as_list_of_lists, y_subset_as_list)\n    \"\"\"\n    # basic sanity checks\n    X = np.asarray(X)\n    y = np.asarray(y)\n    n_samples = X.shape[0]\n    if X.ndim != 2:\n        raise ValueError(f\"X must be 2D, but got shape {X.shape}\")\n    if y.ndim != 1:\n        raise ValueError(f\"y must be 1D, but got shape {y.shape}\")\n    if y.shape[0] != n_samples:\n        raise ValueError(\"Number of samples in X and y must match\")\n    if n_subsets < 1 or not isinstance(n_subsets, int):\n        raise ValueError(\"n_subsets must be a positive integer\")\n\n    rng = np.random.RandomState(seed)\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            # bootstrap sampling\n            idx = rng.randint(0, n_samples, size=n_samples)\n        else:\n            # subsampling without replacement\n            idx = rng.choice(n_samples, size=n_samples, replace=False)\n\n        X_sub = X[idx]\n        y_sub = y[idx]\n\n        # convert to native Python lists\n        subsets.append((X_sub.tolist(), y_sub.tolist()))\n\n    return subsets\n```\n\nExample usage:\n\n```python\nif __name__ == \"__main__\":\n    X = np.arange(20).reshape(10, 2)\n    y = np.arange(10)\n    subs = get_random_subsets(X, y, n_subsets=3, replacements=False, seed=0)\n    for i, (X_s, y_s) in enumerate(subs):\n        print(f\"Subset #{i}\")\n        print(\"X:\", X_s)\n        print(\"y:\", y_s)\n        print()\n```\n"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n       One-hot encode a 1D array of non-negative integer class labels.\n\n    Parameters\n    ----------\n    x : array-like of int\n        1D array of integer class labels (0, 1, 2, ...).\n    n_col : int, optional\n        Number of columns (classes) for one-hot encoding. If None, it will be set to max(x) + 1.\n\n    Returns\n    -------\n    List of lists\n        The one-hot encoded representation of `x`.\n       # Convert to numpy array and flatten\n    x = np.asarray(x, dtype=int).ravel()\n\n    # Determine number of columns if not provided\n    if n_col is None:\n        n_col = x.max() + 1\n\n    # Sanity checks\n    if np.any(x < 0):\n        raise ValueErrorAll class labels must be non-negative integers    if np.any(x >= n_col):\n        raise ValueError(fClass labels must be less than n_col={n_col}    # Create the one-hot encoded matrix\n    one_hot = np.zeros((x.shape[0], n_col), dtype=int)\n    one_hot[np.arange(x.shape[0]), x] = 1\n\n    # Return as a Python list of lists\n    return one_hot.tolist()\n``` \n\nExample usage:\n\n```python\n>>> import numpy as np\n>>> labels = np.array([0, 2, 1, 3])\n>>> to_categorical(labels)\n[[1, 0, 0, 0],\n [0, 0, 1, 0],\n [0, 1, 0, 0],\n [0, 0, 0, 1]]\n\n>>> to_categorical(labels, n_col=5)\n[[1, 0, 0, 0, 0],\n [0, 0, 1, 0, 0],\n [0, 1, 0, 0, 0],\n [0, 0, 0, 1, 0]]\n```"}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n\n    Parameters\n    ----------\n    x : array-like\n        Input 1D array or sequence.\n\n    Returns\n    -------\n    list of lists\n        Diagonal matrix as a nested Python list.\n    \"\"\"\n    # Convert input to numpy array\n    arr = np.array(x)\n    # Check that it's one\u2010dimensional\n    if arr.ndim != 1:\n        raise ValueError(\"Input must be a 1D array.\")\n    # Build diagonal matrix and convert to nested list\n    diag_matrix = np.diag(arr)\n    return diag_matrix.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    v = np.array([1, 2, 3, 4])\n    print(make_diagonal(v))\n    # Output:\n    # [[1, 0, 0, 0],\n    #  [0, 2, 0, 0],\n    #  [0, 0, 3, 0],\n    #  [0, 0, 0, 4]]\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Compute the accuracy classification score.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True labels.\n    y_pred : array-like of shape (n_samples,)\n        Predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy score, defined as the fraction of correct predictions:\n        (number of correct predictions) / (total number of predictions).\n\n    Raises\n    ------\n    ValueError\n        If y_true and y_pred have different shapes.\n    \"\"\"\n    # Convert inputs to numpy arrays (in case they aren't already)\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Verify that the shapes match\n    if y_true.shape != y_pred.shape:\n        raise ValueError(f\"Shape mismatch: y_true has shape {y_true.shape}, \"\n                         f\"but y_pred has shape {y_pred.shape}\")\n\n    # Total number of samples\n    n_samples = y_true.size\n    if n_samples == 0:\n        # No samples to compare; define accuracy as 0.0 (or raise an error)\n        return 0.0\n\n    # Count how many predictions match the true labels\n    n_correct = np.sum(y_true == y_pred)\n\n    # Compute accuracy\n    return float(n_correct) / n_samples\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix of X with itself or with Y.\n\n    Parameters:\n    -----------\n    X : 2D array-like of shape (n_samples, n_features_X)\n    Y : 2D array-like of shape (n_samples, n_features_Y), optional\n        If None, compute the correlation matrix of X with itself.\n\n    Returns:\n    --------\n    corr_list : list of lists\n        The correlation matrix, rounded to 4 decimal places, as a Python list.\n    \"\"\"\n    # Convert inputs to float numpy arrays\n    X = np.asarray(X, dtype=float)\n    if Y is None:\n        Y = X\n    else:\n        Y = np.asarray(Y, dtype=float)\n\n    # Check that the number of samples match\n    if X.ndim != 2 or Y.ndim != 2:\n        raise ValueError(\"Both X and Y must be 2D arrays.\")\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError(\"X and Y must have the same number of rows (samples).\")\n\n    # Number of samples\n    n_samples = X.shape[0]\n    if n_samples < 2:\n        raise ValueError(\"Need at least 2 samples to compute correlation.\")\n\n    # Center the data (subtract the mean)\n    X_centered = X - X.mean(axis=0)\n    Y_centered = Y - Y.mean(axis=0)\n\n    # Compute the sample covariance matrix (with ddof=1)\n    cov_matrix = (X_centered.T @ Y_centered) / (n_samples - 1)\n\n    # Compute standard deviations (with ddof=1)\n    std_X = X_centered.std(axis=0, ddof=1)\n    std_Y = Y_centered.std(axis=0, ddof=1)\n\n    # Outer product of standard deviations to get the denominator\n    denom = np.outer(std_X, std_Y)\n\n    # Compute correlation matrix\n    corr = cov_matrix / denom\n\n    # Round to 4 decimal places\n    corr_rounded = np.round(corr, 4)\n\n    # Convert to Python list and return\n    return corr_rounded.tolist()\n```\n\nExplanation of the key steps:\n1. Ensure `X` and `Y` are 2D float arrays and have the same number of rows.\n2. Subtract the column\u2010wise means from `X` and `Y` to center the data.\n3. Compute the sample covariance matrix using `(n_samples - 1)` in the denominator.\n4. Compute column\u2010wise standard deviations of `X` and `Y` with `ddof=1`.\n5. Divide the covariance matrix by the outer product of standard deviations to get Pearson correlation coefficients.\n6. Round each coefficient to 4 decimal places and convert the result to a nested Python list."}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n       Fit an AdaBoost ensemble of decision stumps.\n    \n    Parameters:\n    X      -- numpy array of shape (n_samples, n_features)\n    y      -- numpy array of shape (n_samples,), labels should be \u00b11\n    n_clf  -- number of weak classifiers to use\n    \n    Returns:\n    A list of weak classifiers. Each classifier is a dict with keys:\n      'feature'   : index of the feature used (int)\n      'threshold' : threshold value (float, rounded to 4 decimals)\n      'polarity'  : polarity of the inequality (1 or -1)\n      'alpha'     : classifier weight in the final ensemble (float, rounded to 4 decimals)\n       n_samples, n_features = X.shape\n    \n    # Ensure labels are \u00b11\n    y_ = y.copy()\n    y_[y_ == 0] = -1\n    \n    # Initialize weights uniformly\n    w = np.ones(n_samples) / n_samples\n    \n    classifiers = []\n    \n    for _ in range(n_clf):\n        best = {\n            'feature': None,\n            'threshold': None,\n            'polarity': None,\n            'error': np.inf\n        }\n        \n        # Find best decision stump\n        for feature_i in range(n_features):\n            X_col = X[:, feature_i]\n            unique_vals = np.unique(X_col)\n            # Build candidate thresholds: midpoints between consecutive unique feature values\n            if unique_vals.size == 1:\n                thresholds = unique_vals.copy()\n            else:\n                thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n            \n            for thresh in thresholds:\n                for polarity in [1, -1]:\n                    # Make predictions: h(x) = 1 if polarity * x < polarity * thresh else -1\n                    preds = np.ones(n_samples)\n                    if polarity == 1:\n                        preds[X_col >= thresh] = -1\n                    else:  # polarity == -1\n                        preds[X_col < thresh] = -1\n                    \n                    # Weighted error\n                    misclassified = (preds != y_)\n                    error = np.dot(w, misclassified.astype(float))\n                    \n                    # Select the stump with minimal error\n                    if error < best['error']:\n                        best.update({\n                            'feature': feature_i,\n                            'threshold': thresh,\n                            'polarity': polarity,\n                            'error': error\n                        })\n        \n        # Compute alpha, avoid division by zero\n        eps = 1e-10\n        error = best['error']\n        alpha = 0.5 * np.log((1 - error + eps) / (error + eps))\n        \n        # Store classifier parameters, rounded to 4 decimals\n        clf = {\n            'feature': best['feature'],\n            'threshold': float(np.round(best['threshold'], 4)),\n            'polarity': best['polarity'],\n            'alpha': float(np.round(alpha, 4))\n        }\n        classifiers.append(clf)\n        \n        # Update sample weights\n        feature_i = best['feature']\n        thresh    = best['threshold']\n        polarity  = best['polarity']\n        \n        X_col = X[:, feature_i]\n        preds = np.ones(n_samples)\n        if polarity == 1:\n            preds[X_col >= thresh] = -1\n        else:\n            preds[X_col < thresh] = -1\n        \n        # Weight update rule\n        w *= np.exp(-alpha * y_ * preds)\n        w /= np.sum(w)  # normalize\n    \n    return classifiers\n```\n\nExplanation of key steps:\n\n1. **Weight Initialization**  \n   We start with uniform weights `w_i = 1/n_samples`.\n\n2. **Finding the Best Stump**  \n   For each feature:\n     a. Generate threshold candidates as midpoints between sorted unique values.  \n     b. For each threshold and polarity (1 or -1), predict labels and compute weighted error.  \n     c. Keep the stump with the lowest error.\n\n3. **Compute Alpha**  \n   \u03b1 = \u00bd \u00b7 ln((1 \u2013 error) / error). A small \u03b5 is added to avoid division by zero.\n\n4. **Update Weights**  \n   w\u1d62 \u2190 w\u1d62 \u00b7 exp(\u2212\u03b1 \u00b7 y\u1d62 \u00b7 h(x\u1d62)), then normalize so that \u2211w\u1d62 = 1.\n\n5. **Store Classifier**  \n   We store a dict with keys `feature`, `threshold`, `polarity`, and `alpha`, rounding numeric values to 4 decimal places.\n\nThe function returns a Python list of these classifier dicts."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a 1D list of scores, returning a Python list\n    with each value rounded to 4 decimal places.\n\n    Args:\n        scores (list): A list of numerical scores.\n\n    Returns:\n        list: Log-softmax values (rounded to 4 decimal places).\n    \"\"\"\n    # Convert input to numpy array (float64 for precision)\n    x = np.array(scores, dtype=np.float64)\n\n    # For numerical stability, subtract the max before exponentiating\n    x_shifted = x - np.max(x)\n\n    # Compute log-sum-exp\n    log_sum_exp = np.log(np.sum(np.exp(x_shifted)))\n\n    # Compute log-softmax: (x_i - max) - log_sum_exp\n    result = x_shifted - log_sum_exp\n\n    # Round to 4 decimal places and convert to Python list\n    return np.round(result, 4).tolist()\n```\n\nExplanation of key steps:\n1. **Shift for stability**: We subtract `max(x)` from each score to avoid large exponents.\n2. **Log-Sum-Exp**: `log_sum_exp = log(sum(exp(x_shifted)))`.\n3. **Log-Softmax**: `log_softmax_i = x_shifted_i - log_sum_exp`.\n4. **Rounding & Output**: Round each element to 4 decimal places and return as a pure Python list.\n\nExample usage:\n```python\nif __name__ == \"__main__\":\n    scores = [2.0, 1.0, 0.1]\n    print(log_softmax(scores))\n    # Example output: [-0.4170, -1.4170, -2.3170]\n```"}
{"task_id": 40, "completion_id": 0, "completion": "Here\u2019s a self\u2010contained implementation of a simple Dense layer with SGD optimizers built in.  All intermediate and final tensor outputs are rounded to 4 decimals and returned as Python lists via `.tolist()`:\n\n```python\nimport numpy as np\nimport copy\nimport math\n\nnp.random.seed(42)\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\n\n# Simple SGD optimizer\nclass SGD:\n    def __init__(self, learning_rate=0.01):\n        self.lr = learning_rate\n\n    def update(self, param, grad):\n        return param - self.lr * grad\n\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None, learning_rate=0.01):\n        # number of neurons in this layer\n        self.n_units = n_units\n        # shape of one input sample (will be set via constructor or set_input_shape)\n        self.input_shape = input_shape\n        # placeholders\n        self.W = None\n        self.w0 = None\n        self.optimizerW = None\n        self.optimizerw0 = None\n        # to cache input during forward for backprop\n        self.layer_input = None\n        # allow weight updates\n        self.trainable = True\n        # store lr\n        self.learning_rate = learning_rate\n\n    def initialize(self):\n               Initialize weights with uniform(-lim, +lim) where\n        lim = 1 / sqrt(fan_in), fan_in = input_shape[0],\n        biases to zero.  Also create two SGD optimizers.\n               if self.input_shape is None:\n            raise ValueErrorCannot initialize Dense layer: input_shape is not set        fan_in = self.input_shape[0]\n        limit = 1.0 / math.sqrt(fan_in)\n        # W shape: (fan_in, n_units)\n        self.W = np.random.uniform(-limit, limit, size=(fan_in, self.n_units))\n        # bias shape: (n_units, )\n        self.w0 = np.zeros((self.n_units,))\n        # optimizers\n        self.optimizerW = SGD(learning_rate=self.learning_rate)\n        self.optimizerw0 = SGD(learning_rate=self.learning_rate)\n\n    def parameters(self):\n               Returns total number of trainable params: fan_in * n_units + n_units\n               if self.W is None or self.w0 is None:\n            return 0\n        return int(self.W.size + self.w0.size)\n\n    def forward_pass(self, X, training=True):\n               X: np.array of shape (batch_size, fan_in)\n        returns: python list of shape (batch_size, n_units)\n               # cache for backprop\n        self.layer_input = X\n        # Z = X dot W + b\n        Z = X.dot(self.W) + self.w0\n        # round to 4 decimals\n        Z = np.round(Z, 4)\n        return Z.tolist()\n\n    def backward_pass(self, accum_grad):\n               accum_grad: incoming gradient, python list or np.array of shape (batch_size, n_units)\n        returns: grad w.r.t. inputs, python list of shape (batch_size, fan_in)\n        Also updates W and w0 if trainable.\n               # ensure numpy array\n        dZ = np.array(accum_grad, dtype=float)\n        X = self.layer_input  # (batch_size, fan_in)\n        batch_size = X.shape[0]\n\n        # gradients\n        dW = X.T.dot(dZ)                 # (fan_in, n_units)\n        dw0 = np.sum(dZ, axis=0)         # (n_units,)\n\n        # gradient w.r.t. inputs for previous layer\n        dX = dZ.dot(self.W.T)            # (batch_size, fan_in)\n\n        # update parameters\n        if self.trainable:\n            self.W = self.optimizerW.update(self.W, dW)\n            self.w0 = self.optimizerw0.update(self.w0, dw0)\n\n        # round and return\n        dX = np.round(dX, 4)\n        return dX.tolist()\n\n    def output_shape(self):\n               Returns the shape of the output for one sample: (n_units,)\n               return (self.n_units,)\n\n\n# Example usage\nif __name__ ==main    # create a Dense layer expecting inputs of size 5 and producing size 3\n    dense = Dense(n_units=3, input_shape=(5,), learning_rate=0.1)\n    dense.initialize()\n\n    # random input batch of 2 samples\n    X = np.array([[0.1, 0.2, 0.3, 0.4, 0.5],\n                  [0.5, 0.4, 0.3, 0.2, 0.1]])\n    out = dense.forward_pass(X, training=True)\n    printForward output out)\n    printParam count dense.parameters())\n\n    # pretend gradient from next layer\n    grad_next = [[0.1, -0.2, 0.3],\n                 [-0.3, 0.2, -0.1]]\n    grad_input = dense.backward_pass(grad_next)\n    printGrad w.r.t input grad_input)\n    printUpdated forward output dense.forward_pass(X, training=True))\n```\n\nExplanation of key points:\n\n1. **Initialization**  \n   - `W` is drawn uniformly in `[\u22121/\u221afan_in, +1/\u221afan_in]`.  \n   - `w0` (bias) is initialized to zeros.  \n   - Two small SGD optimizers are created (one for `W`, one for `w0`).\n\n2. **Forward pass**  \n   - Computes `Z = X\u00b7W + w0`.  \n   - Rounds to 4 decimals and converts to a Python list.\n\n3. **Backward pass**  \n   - Computes `dW`, `dw0`, and `dX`.  \n   - Updates weights/biases if `trainable`.  \n   - Rounds `dX` and returns as a Python list.\n\n4. **Parameter count**  \n   - Simply the total size of `W` plus size of `w0`.\n\n5. **Output shape**  \n   - For a single sample, the layer outputs a vector of length `n_units`."}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray,\n                  kernel: np.ndarray,\n                  padding: int,\n                  stride: int):\n       Perform a simple 2D convolution (no bias, no activation) on a single-channel input.\n\n    Args:\n        input_matrix (np.ndarray): 2D input array of shape (H, W).\n        kernel (np.ndarray): 2D convolution kernel of shape (kH, kW).\n        padding (int): Number of zero\u2010padding layers to add around the input.\n        stride (int): Stride of the convolution.\n\n    Returns:\n        List[List[float]]: The convolved output (rounded to 4 decimal places) as a nested list.\n       # Validate dimensions\n    if input_matrix.ndim != 2 or kernel.ndim != 2:\n        raise ValueErrorBoth input_matrix and kernel must be 2D arrays    # Input dimensions\n    H, W = input_matrix.shape\n    kH, kW = kernel.shape\n\n    # Pad the input\n    padded = np.pad(input_matrix,\n                    pad_width=((padding, padding), (padding, padding)),\n                    mode='constant',\n                    constant_values=0)\n\n    # Output dimensions\n    out_h = (H + 2 * padding - kH) // stride + 1\n    out_w = (W + 2 * padding - kW) // stride + 1\n\n    if out_h <= 0 or out_w <= 0:\n        raise ValueErrorKernel size or padding is incompatible with input dimensions    # Prepare output\n    output = np.zeros((out_h, out_w), dtype=float)\n\n    # Perform convolution\n    for i in range(out_h):\n        for j in range(out_w):\n            start_i = i * stride\n            start_j = j * stride\n            window = padded[start_i:start_i + kH, start_j:start_j + kW]\n            conv_value = np.sum(window * kernel)\n            # Round to nearest 4th decimal\n            output[i, j] = np.round(conv_value, 4)\n\n    # Convert to nested Python list and return\n    return output.tolist()\n```\n\nExplanation of key steps:\n1. **Padding**: We use `np.pad` to add zeros around the input.\n2. **Output size**: Computed as  \n   out_h = (H + 2\u00b7padding \u2013 kH) // stride + 1  \n   out_w = (W + 2\u00b7padding \u2013 kW) // stride + 1\n3. **Sliding window**: For each output cell `(i, j)`, extract the corresponding patch from the padded input, multiply element\u2010wise with the kernel, sum the result, and round to 4 decimal places.\n4. **Format**: The final output is cast to a nested Python list via `tolist()`."}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Compute the Rectified Linear Unit (ReLU) activation of the input.\n\n    Parameters:\n    z (float): The input value.\n\n    Returns:\n    float: z if z > 0, otherwise 0.\n    \"\"\"\n    return z if z > 0.0 else 0.0\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    inputs = [-2.0, 0.0, 3.5]\n    for x in inputs:\n        print(f\"relu({x}) = {relu(x)}\")\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, \n               w: np.ndarray, \n               y_true: np.ndarray, \n               alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss:\n        loss = MSE(y_true, X @ w) + alpha * ||w||^2\n\n    where\n        MSE = (1/n) * sum_i (y_true[i] - (X @ w)[i])^2\n        ||w||^2 = sum_j w[j]^2\n\n    All results are rounded to the nearest 4th decimal.\n    \n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        The feature matrix.\n    w : np.ndarray, shape (n_features,)\n        The coefficient vector.\n    y_true : np.ndarray, shape (n_samples,)\n        The true target values.\n    alpha : float\n        Regularization strength.\n    \n    Returns\n    -------\n    float\n        The Ridge regression loss, rounded to 4 decimals.\n    \"\"\"\n    # Predicted values\n    y_pred = X.dot(w)\n    \n    # Mean Squared Error term\n    mse = np.mean((y_true - y_pred) ** 2)\n    \n    # L2 regularization term\n    reg = alpha * np.sum(w ** 2)\n    \n    # Total loss\n    loss = mse + reg\n    \n    # Round to nearest 4th decimal and return as Python float\n    return float(np.round(loss, 4))\n```\n\nExample of usage:\n```python\nif __name__ == \"__main__\":\n    # sample data\n    X = np.array([[1.0, 2.0],\n                  [3.0, 4.0],\n                  [5.0, 6.0]])\n    w = np.array([0.1, -0.2])\n    y_true = np.array([1.0, 2.0, 3.0])\n    alpha = 0.5\n\n    print(ridge_loss(X, w, y_true, alpha))  # prints the loss rounded to 4 decimals\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float|int:\n    \"\"\"\n    Compute the Leaky ReLU activation of the input.\n\n    Parameters:\n    z (float): The input value.\n    alpha (float): Slope for negative inputs (default is 0.01).\n\n    Returns:\n    float|int: z if z is positive; otherwise alpha * z.\n    \"\"\"\n    return z if z >= 0 else alpha * z\n```\n\nExample usage:\n```python\nprint(leaky_relu(5.0))    # Outputs: 5.0\nprint(leaky_relu(-3.0))   # Outputs: -0.03\nprint(leaky_relu(-3.0, 0.1))  # Outputs: -0.3\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n       Compute the linear kernel (dot product) between two vectors x1 and x2.\n\n    Parameters\n    ----------\n    x1 : array-like of shape (n,) or (n,1)\n        First input vector.\n    x2 : array-like of shape (n,) or (n,1)\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The dot product of x1 and x2.\n\n    Raises\n    ------\n    ValueError\n        If x1 and x2 have different shapes or are not 1-D arrays after flattening.\n       # Convert inputs to numpy arrays\n    x1_arr = np.asarray(x1)\n    x2_arr = np.asarray(x2)\n\n    # Flatten to 1-D\n    x1_flat = x1_arr.ravel()\n    x2_flat = x2_arr.ravel()\n\n    # Ensure same dimensionality\n    if x1_flat.shape != x2_flat.shape:\n        raise ValueError(fShape mismatch: x1 has shape {x1_flat.shape}                         fbut x2 has shape {x2_flat.shape    # Compute and return the dot product\n    return float(np.dot(x1_flat, x2_flat))\n\n\n# Example usage\nif __name__ ==main    a = [1, 2, 3]\n    b = [4, 5, 6]\n    printLinear kernel (dot product) of a,and b,is kernel_function(a, b))\n    # Expected output: 32.0\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Compute the precision metric:\n        precision = true_positives / (true_positives + false_positives)\n\n    Parameters\n    ----------\n    y_true : array\u2010like of shape (n_samples,)\n        True binary labels (0 or 1).\n    y_pred : array\u2010like of shape (n_samples,)\n        Predicted binary labels (0 or 1).\n\n    Returns\n    -------\n    float\n        Precision score. If there are no positive predictions (tp + fp == 0),\n        returns 0.0 to avoid division by zero.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y_true = np.asarray(y_true).flatten()\n    y_pred = np.asarray(y_pred).flatten()\n\n    if y_true.shape != y_pred.shape:\n        raise ValueError(f\"Shape mismatch: y_true {y_true.shape} vs y_pred {y_pred.shape}\")\n\n    # True positives: predicted 1 and actually 1\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    # False positives: predicted 1 but actually 0\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n\n    # Avoid division by zero\n    if tp + fp == 0:\n        return 0.0\n\n    return tp / float(tp + fp)\n```\n\nExample usage:\n\n```python\nimport numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 0])\ny_pred = np.array([1, 1, 1, 0, 0, 0])\n\nprint(precision(y_true, y_pred))  # Output: 2 / (2 + 1) = 0.666666...\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n       Performs gradient descent to minimize MSE loss. Supports batch, SGD, and mini-batch.\n\n    Parameters:\n    - X: numpy array of shape (m, n)\n    - y: numpy array of shape (m,) or (m, 1)\n    - weights: numpy array of shape (n,) or (n, 1)\n    - learning_rate: float\n    - n_iterations: int, number of updates\n    - batch_size: int, size of mini-batch (only used if method='mini-batch')\n    - method: 'batch', 'sgd', or 'mini-batch'\n\n    Returns:\n    - list of weights (rounded to 4 decimals), same shape as input `weights`\n       X = np.array(X, dtype=float)\n    y = np.array(y, dtype=float)\n    w = np.array(weights, dtype=float)\n    \n    # remember original shape to reshape at end\n    orig_shape = w.shape\n    w = w.reshape(-1)         # flatten to (n,)\n    y_flat = y.reshape(-1)    # flatten to (m,)\n    m, n = X.shape            # m samples, n features\n\n    method = method.lower()\n    if method not in {'batch', 'sgd', 'mini-batch'}:\n        raise ValueErrormethod must be 'batch', 'sgd', or 'mini-batch'    for _ in range(n_iterations):\n        if method == 'batch':\n            # full-batch gradient\n            preds = X.dot(w)                   # shape (m,)\n            errors = preds - y_flat            # shape (m,)\n            grad = (2.0 / m) * X.T.dot(errors) # shape (n,)\n        \n        elif method == 'sgd':\n            # pick one random sample\n            idx = np.random.randint(0, m)\n            xi = X[idx]                        # shape (n,)\n            yi = y_flat[idx]                   # scalar\n            error = xi.dot(w) - yi             # scalar\n            grad = 2.0 * error * xi            # shape (n,)\n        \n        else:  # mini-batch\n            # pick a random mini-batch of size batch_size\n            if batch_size > m:\n                raise ValueErrorbatch_size cannot exceed number of samples            idxs = np.random.choice(m, batch_size, replace=False)\n            Xb = X[idxs]                       # shape (batch_size, n)\n            yb = y_flat[idxs]                  # shape (batch_size,)\n            preds = Xb.dot(w)                  # shape (batch_size,)\n            errors = preds - yb                # shape (batch_size,)\n            grad = (2.0 / batch_size) * Xb.T.dot(errors)  # shape (n,)\n\n        # gradient step\n        w = w - learning_rate * grad\n\n    # reshape back to original shape, round and convert to python list\n    w = w.reshape(orig_shape)\n    w = np.round(w, 4)\n    return w.tolist()\n```\n\nExplanation of key points:\n- We flatten `weights` and `y` for easy vector operations, but remember the original shape so we can reshape before returning.\n- For **batch** GD, the gradient is computed over all `m` examples.\n- For **SGD**, we pick a single random sample each iteration.\n- For **mini-batch**, we sample `batch_size` examples without replacement each iteration.\n- After the final update, we round the weights to 4 decimal places and return them as a Python list."}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Convert a matrix to its Reduced Row Echelon Form (RREF).\n\n    Parameters:\n    matrix : array-like, shape (m, n)\n        The input matrix, can be a list of lists or a numpy array.\n\n    Returns:\n    rref_matrix : list of lists\n        The RREF of the input matrix, returned as a Python list of lists.\n    \"\"\"\n    # Convert input to float numpy array\n    A = np.array(matrix, dtype=float)\n    m, n = A.shape\n    row = 0\n    tol = 1e-12  # tolerance for detecting zeros\n\n    for col in range(n):\n        if row >= m:\n            break\n\n        # Find the pivot row for this column\n        pivot = np.argmax(np.abs(A[row:, col])) + row\n        if abs(A[pivot, col]) < tol:\n            # No pivot in this column, skip it\n            continue\n\n        # Swap current row with pivot row if needed\n        if pivot != row:\n            A[[row, pivot], :] = A[[pivot, row], :]\n\n        # Normalize pivot row (make pivot = 1)\n        A[row, :] = A[row, :] / A[row, col]\n\n        # Eliminate all other entries in this column\n        for r in range(m):\n            if r != row:\n                factor = A[r, col]\n                if abs(factor) > tol:\n                    A[r, :] = A[r, :] - factor * A[row, :]\n\n        row += 1\n\n    # Zero out tiny values that are essentially zero\n    A[np.abs(A) < tol] = 0.0\n\n    return A.tolist()\n```\n\nExplanation of key steps:\n\n1. **Conversion to NumPy array**: We convert the input to a float NumPy array so that arithmetic operations are straightforward and in-place row swaps are easy.\n2. **Pivot selection**: For each column, we search from the current `row` downwards to find the largest (by absolute value) entry to reduce numerical instability.\n3. **Row swapping**: If the best pivot is not already on the \u201ccurrent\u201d row, we swap rows.\n4. **Normalization**: We divide the entire pivot row by the pivot element so that the pivot becomes 1.\n5. **Elimination**: We make every other entry in the pivot column zero by subtracting a suitable multiple of the pivot row.\n6. **Tolerance cleanup**: Very small floating-point artifacts (below `tol`) are set exactly to zero.\n7. **Output**: Finally, we convert the NumPy array back to a native Python list of lists with `tolist()`.\n\nThis handles zero rows, columns without pivots, and produces a true reduced row echelon form."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n       Performs Adam optimization on function f with gradient grad starting from x0.\n    \n    Parameters:\n    - f: Objective function to be optimized (not strictly required for parameter updates here)\n    - grad: Function that returns the gradient of f at a given x\n    - x0: Initial parameters (array-like)\n    - learning_rate: Step size (alpha)\n    - beta1: Exponential decay rate for first moment estimates\n    - beta2: Exponential decay rate for second moment estimates\n    - epsilon: Small constant for numerical stability\n    - num_iterations: Number of iterations to run\n    \n    Returns:\n    - A python list of the optimized parameters, rounded to 4 decimal places\n       \n    # Convert initial guess to numpy array\n    x = np.array(x0, dtype=float)\n    \n    # Initialize first and second moment vectors\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    \n    # Run the optimization\n    for t in range(1, num_iterations + 1):\n        g = np.array(grad(x), dtype=float)        # compute gradient\n        \n        # Update biased first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n        # Update biased second moment estimate\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        # Compute bias-corrected first and second moment estimates\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        \n        # Update parameters\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round the final result to the nearest 4th decimal and return as a Python list\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\n```\n\nExplanation of the key steps:\n\n1. **Initialization**  \n   We convert `x0` into a NumPy array `x`. We also initialize the first moment vector `m` and second moment vector `v` to zeros matching the shape of `x`.\n\n2. **Iteration**  \n   For each iteration `t` from 1 to `num_iterations`:\n   - Compute the gradient `g` at the current `x`.\n   - Update the biased first moment estimate `m`.\n   - Update the biased second moment estimate `v`.\n   - Correct the bias in `m` and `v` to obtain `m_hat` and `v_hat`.\n   - Update the parameters `x` using the Adam update rule.\n\n3. **Post-processing**  \n   After completing the iterations, we round each component of `x` to 4 decimal places and convert the final NumPy array into a Python list using `.tolist()` before returning."}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(\n    X: np.array,\n    y: np.array,\n    alpha: float = 0.1,\n    learning_rate: float = 0.01,\n    max_iter: int = 1000,\n    tol: float = 1e-4\n) -> tuple:\n       Performs Lasso (L1) linear regression using batch gradient descent.\n\n    Args:\n        X (np.array): Feature matrix of shape (n_samples, n_features).\n        y (np.array): Target vector of shape (n_samples,) or (n_samples, 1).\n        alpha (float): L1 regularization strength.\n        learning_rate (float): Step size for gradient descent updates.\n        max_iter (int): Maximum number of iterations to run GD.\n        tol (float): Convergence tolerance for maximum parameter change.\n\n    Returns:\n        tuple: (weights_list, bias_scalar)\n            - weights_list: Python list of learned weights (rounded to 4 decimals).\n            - bias_scalar: Learned bias term (rounded to 4 decimals).\n       # Ensure y is a 1d array\n    y = y.reshape(-1,)\n    \n    n_samples, n_features = X.shape\n    \n    # Initialize parameters\n    w = np.zeros(n_features, dtype=float)\n    b = 0.0\n\n    for iteration in range(max_iter):\n        # Predictions\n        y_pred = X.dot(w) + b      # shape: (n_samples,)\n\n        # Residuals\n        residuals = y - y_pred     # shape: (n_samples,)\n\n        # Gradient of MSE part\n        grad_w_mse = - (1.0 / n_samples) * (X.T.dot(residuals))  # shape: (n_features,)\n        grad_b = - (1.0 / n_samples) * residuals.sum()           # scalar\n\n        # Subgradient of L1 penalty\n        subgrad_w_l1 = alpha * np.sign(w)  # sign(0) == 0\n\n        # Full gradient\n        grad_w = grad_w_mse + subgrad_w_l1\n\n        # Parameter updates\n        w_new = w - learning_rate * grad_w\n        b_new = b - learning_rate * grad_b\n\n        # Check convergence\n        if np.max(np.abs(w_new - w)) < tol and abs(b_new - b) < tol:\n            w, b = w_new, b_new\n            break\n\n        w, b = w_new, b_new\n\n    # Round the results to 4 decimal places\n    w = np.round(w, 4)\n    b = round(b, 4)\n\n    # Return weights as python list and bias as a python float\n    return w.tolist(), b\n```\n\nExplanation of key steps:\n1. We initialize weights `w` and bias `b` to zero.\n2. In each iteration:\n   - Compute predictions `y_pred = X\u00b7w + b`.\n   - Compute the residuals `y - y_pred`.\n   - Compute the gradient of the MSE term with respect to `w` and `b`.\n   - Add the subgradient of the L1 penalty, `\u03b1\u00b7sign(w)`, to the weight\u2010gradient.\n   - Perform a gradient descent step on both `w` and `b`.\n3. Check for convergence: if the maximum change in any parameter is below `tol`, we stop.\n4. Finally, round all parameters to 4 decimal places and return them."}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Compute the Optimal String Alignment (OSA) distance between two strings.\n    This distance allows insertions, deletions, substitutions, and\n    adjacent transpositions, each at cost 1.\n    \"\"\"\n    m, n = len(source), len(target)\n    # DP table of size (m+1) x (n+1)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n\n    # Base cases: empty source \u2192 j insertions; empty target \u2192 i deletions\n    for i in range(1, m + 1):\n        dp[i, 0] = i\n    for j in range(1, n + 1):\n        dp[0, j] = j\n\n    # Fill the table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n\n            # substitution or match\n            v_sub = dp[i - 1, j - 1] + cost\n            # deletion\n            v_del = dp[i - 1, j] + 1\n            # insertion\n            v_ins = dp[i, j - 1] + 1\n\n            dp[i, j] = min(v_sub, v_del, v_ins)\n\n            # transposition\n            if (\n                i > 1 and j > 1 and\n                source[i - 1] == target[j - 2] and\n                source[i - 2] == target[j - 1]\n            ):\n                dp[i, j] = min(dp[i, j], dp[i - 2, j - 2] + 1)\n\n    return int(dp[m, n])\n\n\n# Example usage\nif __name__ == \"__main__\":\n    s1 = \"caper\"\n    s2 = \"acer\"\n    print(f\"OSA('{s1}', '{s2}') = {OSA(s1, s2)}\")  # Expected output: 2\n```"}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Compute the recall (true positive rate) for binary classification.\n\n    Args:\n        y_true (list or array-like): True binary labels (0 or 1).\n        y_pred (list or array-like): Predicted binary labels (0 or 1).\n\n    Returns:\n        float: Recall rounded to three decimal places. Returns 0.0 if there are\n               no actual positives (to avoid division by zero).\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # True Positives:   y_true=1 and y_pred=1\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    # False Negatives:  y_true=1 and y_pred=0\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n\n    denom = tp + fn\n    if denom == 0:\n        return 0.0\n\n    rec = tp / denom\n    # Round to three decimal places and return as a Python float\n    return float(np.round(rec, 3))\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = [1, 0, 1, 1, 0, 1, 0]\n    y_pred = [1, 0, 0, 1, 0, 1, 1]\n\n    print(\"Recall:\", recall(y_true, y_pred))  # Recall: 0.75\n```\n\nExplanation of key steps:\n- We convert `y_true` and `y_pred` to NumPy arrays to leverage vectorized boolean masking.\n- `tp` counts the true positives where both the true label and the prediction are 1.\n- `fn` counts the false negatives where the true label is 1 but the prediction is 0.\n- Recall is computed as `tp / (tp + fn)`. If there are no actual positives (denominator = 0), we return 0.0 to avoid division by zero.\n- Finally we round the computed recall to three decimal places."}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute the self-attention output.\n    \n    Args:\n        Q: numpy array of shape (n_q, d_k) representing queries\n        K: numpy array of shape (n_k, d_k) representing keys\n        V: numpy array of shape (n_k, d_v) representing values\n\n    Returns:\n        A python list of shape (n_q, d_v) containing the self-attention output,\n        with each value rounded to 4 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    Q = np.array(Q, dtype=float)\n    K = np.array(K, dtype=float)\n    V = np.array(V, dtype=float)\n\n    # Dimension of the keys (for scaling)\n    d_k = Q.shape[1]\n\n    # Compute raw attention scores: Q \u00d7 K\u1d40\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n\n    # Apply row\u2010wise softmax in a numerically stable way\n    # subtract max per row for stability\n    scores_exp = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights = scores_exp / np.sum(scores_exp, axis=1, keepdims=True)\n\n    # Weight the values\n    output = np.dot(attention_weights, V)\n\n    # Round to 4 decimal places and convert to python list\n    output = np.round(output, 4).tolist()\n    return output\n```\n\nExplanation of key steps:\n1. **Scaling**: We divide the dot\u2010product `Q @ K.T` by \u221ad_k to keep gradients well\u2010conditioned.  \n2. **Stable softmax**: We subtract the maximal score in each row prior to exponentiation to avoid large exponentials.  \n3. **Aggregation**: We multiply the normalized attention weights by `V` to get the final context vectors.  \n4. **Rounding & output**: We round every element to 4 decimal places and return a native Python list via `tolist()`."}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]],\n                initial_hidden_state: list[float],\n                Wx: list[list[float]],\n                Wh: list[list[float]],\n                b: list[float]) -> list[float]:\n    \"\"\"\n    A simple RNN forward pass that processes an input sequence and returns\n    the final hidden state, using tanh activation.\n\n    Arguments:\n    input_sequence         -- list of input vectors (each a list of floats), length T\n    initial_hidden_state   -- list of floats, the initial hidden state (size H)\n    Wx                     -- input-to-hidden weights, shape (H, D)\n    Wh                     -- hidden-to-hidden weights, shape (H, H)\n    b                      -- bias vector, shape (H,)\n\n    Returns:\n    final_hidden_state     -- Python list (length H) of the final hidden state,\n                              rounded to 4 decimal places\n    \"\"\"\n    # Convert inputs to numpy arrays\n    h = np.array(initial_hidden_state, dtype=float)      # shape (H,)\n    Wx = np.array(Wx, dtype=float)                       # shape (H, D)\n    Wh = np.array(Wh, dtype=float)                       # shape (H, H)\n    b  = np.array(b,  dtype=float)                       # shape (H,)\n\n    # Iterate through the input sequence\n    for x in input_sequence:\n        x_vec = np.array(x, dtype=float)                 # shape (D,)\n        # Compute next hidden state: h = tanh(Wx x + Wh h + b)\n        h = np.tanh(Wx.dot(x_vec) + Wh.dot(h) + b)\n\n    # Round the final hidden state to 4 decimal places\n    h_rounded = np.round(h, 4)\n\n    # Return as a Python list\n    return h_rounded.tolist()\n```"}
{"task_id": 55, "completion_id": 0, "completion": "Here\u2019s an implementation that uses homogeneous coordinates and a 3\u00d73 translation matrix under the hood, but still returns a plain Python list of translated 2D points via `tolist()`:\n\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translates a list of 2D points by (tx, ty) using a 3x3 homogeneous translation matrix.\n\n    Parameters:\n    - points: list of [x, y] coordinates, e.g. [[x1, y1], [x2, y2], ...]\n    - tx: translation distance along the x axis\n    - ty: translation distance along the y axis\n\n    Returns:\n    - A new list of translated points, in the same format as the input.\n    \"\"\"\n    # Convert to an (N, 2) numpy array\n    pts = np.array(points, dtype=float)\n    if pts.ndim != 2 or pts.shape[1] != 2:\n        raise ValueError(\"points must be a list of [x, y] coordinates\")\n\n    # Build homogeneous coordinates by appending a column of 1s \u2192 shape (N, 3)\n    ones = np.ones((pts.shape[0], 1), dtype=float)\n    pts_homog = np.hstack([pts, ones])\n\n    # Construct the 3\u00d73 translation matrix\n    T = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0,  1]\n    ], dtype=float)\n\n    # Apply the translation: (N\u00d73) @ (3\u00d73).T \u2192 (N\u00d73)\n    translated_homog = pts_homog @ T.T\n\n    # Extract the first two columns and convert back to a Python list\n    translated_pts = translated_homog[:, :2]\n    return translated_pts.tolist()\n```\n\nExample usage:\n\n```python\noriginal = [[0, 0], [1, 2], [-3, 5]]\nmoved = translate_object(original, tx=2, ty=-1)\nprint(moved)   # \u2192 [[2.0, -1.0], [3.0, 1.0], [-1.0, 4.0]]\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler divergence KL(P || Q) between two univariate normal distributions:\n      P ~ N(mu_p,  sigma_p^2)\n      Q ~ N(mu_q,  sigma_q^2)\n\n    Formula:\n      KL(P || Q) = log(sigma_q / sigma_p)\n                   + (sigma_p^2 + (mu_p - mu_q)**2) / (2 * sigma_q^2)\n                   - 1/2\n\n    Args:\n      mu_p    (float): mean of P\n      sigma_p (float): standard deviation of P (must be > 0)\n      mu_q    (float): mean of Q\n      sigma_q (float): standard deviation of Q (must be > 0)\n\n    Returns:\n      float: the KL divergence KL(P || Q)\n\n    Raises:\n      ValueError: if sigma_p <= 0 or sigma_q <= 0\n    \"\"\"\n    # Validate inputs\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError(\"Standard deviations must be positive.\")\n\n    # compute term by term\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2)\n    kl = term1 + term2 - 0.5\n\n    return float(kl)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    mu_p, sigma_p = 0.0, 1.0\n    mu_q, sigma_q = 1.0, 2.0\n    print(\"KL(P||Q) = \", kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n```\n\nExplanation of the terms:\n- term1 = log(\u03c3_q / \u03c3_p)  \n- term2 = [\u03c3_p\u00b2 + (\u03bc_p \u2013 \u03bc_q)\u00b2] / (2 \u03c3_q\u00b2)  \n- subtract 1/2 at the end\n\nThis matches the standard closed\u2010form expression for the KL divergence between two univariate normal distributions."}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the linear system A x = b using the Gauss\u2013Seidel iterative method.\n\n    Parameters:\n    A     : square coefficient matrix (array-like)\n    b     : right-hand side vector (array-like)\n    n     : number of iterations (int)\n    x_ini : optional initial guess for x (array-like), defaults to zero vector\n\n    Returns:\n    x_approx : list of solution components rounded to 4 decimal places\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).flatten()\n    m, m2 = A.shape\n    if m != m2:\n        raise ValueError(\"Matrix A must be square\")\n    if b.size != m:\n        raise ValueError(\"Vector b must have the same size as A's dimensions\")\n\n    # Initialize x\n    if x_ini is None:\n        x = np.zeros(m, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float).flatten()\n        if x.size != m:\n            raise ValueError(\"Initial guess x_ini must have length {}\".format(m))\n\n    # Gauss\u2013Seidel iterations\n    for _ in range(n):\n        for i in range(m):\n            # sum over j < i using updated values x[j]\n            sum1 = np.dot(A[i, :i], x[:i])\n            # sum over j > i using old values x[j]\n            sum2 = np.dot(A[i, i+1:], x[i+1:])\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n\n    # Round to 4 decimal places and return as Python list\n    x_approx = np.round(x, 4).tolist()\n    return x_approx\n```"}
{"task_id": 58, "completion_id": 0, "completion": "import numpy as np\n\ndef gaussian_elimination(A, b):\n    # Convert inputs to numpy arrays of type float\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).flatten()\n    \n    # Check dimensions\n    n, m = A.shape\n    if n != m:\n        raise ValueErrorMatrix A must be square    if b.size != n:\n        raise ValueErrorVector b must have the same number of rows as A    \n    # Forward elimination with partial pivoting\n    for i in range(n):\n        # Partial pivoting: find the row with max abs value in column i\n        pivot_row = np.argmax(np.abs(A[i:, i])) + i\n        if np.isclose(A[pivot_row, i], 0.0):\n            raise ValueErrorMatrix is singular or nearly singular        # Swap current row with pivot_row\n        if pivot_row != i:\n            A[[i, pivot_row], :] = A[[pivot_row, i], :]\n            b[i], b[pivot_row] = b[pivot_row], b[i]\n        # Eliminate entries below pivot\n        for j in range(i+1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n            b[j]      -= factor * b[i]\n    \n    # Backward substitution\n    x = np.zeros(n, dtype=float)\n    for i in range(n-1, -1, -1):\n        if np.isclose(A[i, i], 0.0):\n            raise ValueErrorMatrix is singular or nearly singular        sum_ax = np.dot(A[i, i+1:], x[i+1:])\n        x[i] = (b[i] - sum_ax) / A[i, i]\n    \n    # Round to 4 decimal places and return as Python list\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()"}
{"task_id": 59, "completion_id": 0, "completion": "import numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        # Each W? has shape (hidden_size, input_size + hidden_size)\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        # Biases are (hidden_size, 1)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns:\n          - hidden_states_list: list of hidden states at each time step\n          - final_hidden: final hidden state\n          - final_cell: final cell state\n        All returned values are Python lists (via tolist()) with entries rounded to 4 decimals.\n        \n        x: array-like of shape (seq_len, input_size)\n        initial_hidden_state: array-like of shape (hidden_size, 1) or (hidden_size,)\n        initial_cell_state: array-like of shape (hidden_size, 1) or (hidden_size,)\n        \"\"\"\n        # Convert inputs to numpy\n        x_arr = np.array(x, dtype=float)\n        seq_len = x_arr.shape[0]\n        # check input dimensions\n        if x_arr.ndim != 2 or x_arr.shape[1] != self.input_size:\n            raise ValueError(f\"x must be of shape (seq_len, {self.input_size})\")\n\n        # initialize previous states\n        h_prev = np.array(initial_hidden_state, dtype=float)\n        if h_prev.ndim == 1:\n            h_prev = h_prev.reshape(-1, 1)\n        c_prev = np.array(initial_cell_state, dtype=float)\n        if c_prev.ndim == 1:\n            c_prev = c_prev.reshape(-1, 1)\n        if h_prev.shape != (self.hidden_size, 1) or c_prev.shape != (self.hidden_size, 1):\n            raise ValueError(f\"Initial hidden/cell must be shape ({self.hidden_size},1)\")\n\n        hidden_states = []\n\n        for t in range(seq_len):\n            x_t = x_arr[t].reshape(self.input_size, 1)  # (input_size, 1)\n            # concat h_prev and x_t into one vector of shape (hidden_size+input_size, 1)\n            concat = np.vstack((h_prev, x_t))\n\n            # Forget gate\n            f_t = self.sigmoid(self.Wf.dot(concat) + self.bf)\n            # Input gate\n            i_t = self.sigmoid(self.Wi.dot(concat) + self.bi)\n            # Candidate cell state\n            c_tilde = np.tanh(self.Wc.dot(concat) + self.bc)\n            # New cell state\n            c_t = f_t * c_prev + i_t * c_tilde\n            # Output gate\n            o_t = self.sigmoid(self.Wo.dot(concat) + self.bo)\n            # New hidden state\n            h_t = o_t * np.tanh(c_t)\n\n            # save and step forward\n            hidden_states.append(h_t)\n            h_prev = h_t\n            c_prev = c_t\n\n        # Round and convert to lists\n        hidden_states_list = [np.round(h, 4).tolist() for h in hidden_states]\n        final_hidden = np.round(h_prev, 4).tolist()\n        final_cell = np.round(c_prev, 4).tolist()\n\n        return hidden_states_list, final_hidden, final_cell"}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n       Computes the TF-IDF score for each query term in each document of the corpus.\n    \n    Parameters:\n    - corpus: List[List[str]] -- a list of documents, each document is a list of words.\n    - query: List[str]       -- a list of query words.\n    \n    Returns:\n    - List[List[float]] -- a matrix (as a list of lists) of shape (num_documents, len(query)),\n                           where entry [i][j] is the TF-IDF score of query[j] in document i,\n                           rounded to 4 decimal places.\n       # Number of documents\n    N = len(corpus)\n    \n    # If corpus is empty, return empty result\n    if N == 0:\n        return []\n    \n    # Compute document frequency (df) for each term in the query\n    df = []\n    for term in query:\n        # count how many documents contain 'term' at least once\n        doc_count = sum(1 for doc in corpus if term in doc)\n        df.append(doc_count)\n    \n    # Compute smoothed IDF for each query term:\n    #    idf(t) = log((N + 1) / (df_t + 1))\n    # This avoids division by zero when df_t == 0.\n    idf = [np.log((N + 1) / (df_t + 1)) for df_t in df]\n    \n    # Prepare TF-IDF matrix: shape = (N documents, len(query) terms)\n    tf_idf_matrix = np.zeros((N, len(query)), dtype=float)\n    \n    # Compute TF-IDF for each document-term pair\n    for i, doc in enumerate(corpus):\n        doc_len = len(doc)\n        for j, term in enumerate(query):\n            # Term Frequency (TF):\n            #    count of term in doc / total number of terms in doc\n            tf = doc.count(term) / doc_len if doc_len > 0 else 0.0\n            score = tf * idf[j]\n            # Round to 4 decimal places\n            tf_idf_matrix[i, j] = round(score, 4)\n    \n    # Convert to nested Python list and return\n    return tf_idf_matrix.tolist()\n```\n\nExplanation of key points:\n1. **Smoothing in IDF**  \n   We use `idf(t) = log((N + 1) / (df_t + 1))` so that when `df_t = 0`, the denominator is 1 (no division by zero).\n2. **Empty corpus**  \n   If `corpus` is empty, the function returns `[]` immediately.\n3. **Edge cases**  \n   - Documents of length zero yield TF = 0 for all terms.  \n   - Query terms not in any document get `df_t = 0` and thus a valid (smoothed) IDF.  \n4. **Rounding & Output**  \n   Scores are rounded to 4 decimal places and the final NumPy array is converted to a nested Python list via `tolist()`."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels (0 or 1)\n    :param y_pred: Numpy array of predicted labels (0 or 1)\n    :param beta:  The weight of precision in the harmonic mean (beta >= 0)\n    :return:       F-Score rounded to three decimal places\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.asarray(y_true).flatten()\n    y_pred = np.asarray(y_pred).flatten()\n\n    # Sanity check: same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n\n    # True positives, false positives, false negatives\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n\n    # Precision and Recall (guard against division by zero)\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n\n    # F-beta score formula\n    beta_sq = beta ** 2\n    denom = beta_sq * precision + recall\n    if denom == 0:\n        f_beta = 0.0\n    else:\n        f_beta = (1 + beta_sq) * (precision * recall) / denom\n\n    # Round to three decimal places\n    return round(f_beta, 3)\n```\n\nExplanation of key steps:\n1. Convert inputs to 1-D numpy arrays and check they match in length.  \n2. Compute true positives (tp), false positives (fp), and false negatives (fn) via elementwise boolean masks.  \n3. Calculate precision = tp / (tp + fp) and recall = tp / (tp + fn), handling zero denominators.  \n4. Apply the standard F\u03b2 formula:  \n   F\u03b2 = (1 + \u03b2\u00b2)\u00b7(precision\u00b7recall) / (\u03b2\u00b2\u00b7precision + recall),  \n   with a guard against division by zero.  \n5. Return the result rounded to three decimal places."}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef initialize_weights(input_size, hidden_size, output_size):\n       Initialize the RNN weight matrices and bias vectors.\n    Weights are small random values, biases are zero.\n    Returns: W_xh, W_hh, W_hy, b_h, b_y\n       W_xh = np.random.randn(hidden_size, input_size) * 0.01  # input to hidden\n    W_hh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden\n    W_hy = np.random.randn(output_size, hidden_size) * 0.01  # hidden to output\n    b_h  = np.zeros((hidden_size, 1))  # hidden bias\n    b_y  = np.zeros((output_size, 1))  # output bias\n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n       Forward pass through the RNN for a full input sequence.\n    input_sequence: list or array of shape (T, input_size) or list of length T\n    Returns:\n      outputs      : list of output vectors (each shape (output_size,))\n      last_inputs  : list of inputs as column vectors (each shape (input_size,1))\n      last_hiddens : list of hidden states (length T+1, each shape (hidden_size,1)),\n                     where last_hiddens[0] is the initial zero state.\n       T = len(input_sequence)\n    # initialize\n    h_prev = np.zeros((hidden_size, 1))\n    last_hiddens = [h_prev]\n    last_inputs  = []\n    outputs      = []\n    \n    for t in range(T):\n        # get x_t as column vector\n        x_t = np.array(input_sequence[t]).reshape(-1, 1)  # (input_size,1)\n        last_inputs.append(x_t)\n        \n        # hidden state update: h_t = tanh(W_xh x_t + W_hh h_prev + b_h)\n        h_t = np.tanh(W_xh @ x_t + W_hh @ h_prev + b_h)\n        last_hiddens.append(h_t)\n        \n        # output: y_t = W_hy h_t + b_y\n        y_t = W_hy @ h_t + b_y  # (output_size,1)\n        outputs.append(y_t.ravel())  # store as 1D array\n        \n        h_prev = h_t\n    \n    return outputs, last_inputs, last_hiddens\n\ndef rnn_backward(W_xh, W_hh, W_hy, b_h, b_y,\n                 hidden_size,\n                 input_sequence, expected_output,\n                 outputs, last_inputs, last_hiddens,\n                 learning_rate):\n       Backpropagation Through Time (BPTT) to update the RNN parameters.\n    expected_output: list or array of shape (T, output_size) or list of length T\n    outputs        : list of predicted outputs from forward (each shape (output_size,))\n    last_inputs    : list of inputs (column vectors)\n    last_hiddens   : list of hidden states (length T+1)\n    Returns updated W_xh, W_hh, W_hy, b_h, b_y\n       T = len(input_sequence)\n    output_size = b_y.shape[0]\n    input_size  = b_h.shape[0]  # careful: this is hidden_size, we only need input from last_inputs\n    \n    # initialize gradients\n    dW_xh = np.zeros_like(W_xh)\n    dW_hh = np.zeros_like(W_hh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h   = np.zeros_like(b_h)\n    db_y   = np.zeros_like(b_y)\n    \n    # carry-over gradient for hidden state (BPTT)\n    dh_next = np.zeros((hidden_size, 1))\n    \n    # BPTT through time (reverse order)\n    for t in reversed(range(T)):\n        # --- output layer gradient ---\n        y_pred = outputs[t].reshape(-1, 1)                        # (output_size,1)\n        y_true = np.array(expected_output[t]).reshape(-1, 1)     # (output_size,1)\n        dy = y_pred - y_true  # dLoss/dy_t  for 1/2 MSE\n        \n        # gradients for W_hy, b_y\n        h_t = last_hiddens[t+1]  # current hidden\n        dW_hy += dy @ h_t.T\n        db_y   += dy\n        \n        # --- hidden layer gradient ---\n        # backprop into h_t: from output + from next hidden\n        dh = (W_hy.T @ dy) + dh_next  # (hidden_size,1)\n        \n        # backprop through tanh nonlinearity\n        h_raw = h_t  # actually tanh output\n        dtanh = dh * (1 - h_raw * h_raw)  # (hidden_size,1)\n        \n        # gradients for W_xh, W_hh, b_h\n        x_t = last_inputs[t]      # (input_size,1)\n        h_prev = last_hiddens[t]  # (hidden_size,1)\n        dW_xh += dtanh @ x_t.T\n        dW_hh += dtanh @ h_prev.T\n        db_h   += dtanh\n        \n        # propagate gradient to previous hidden\n        dh_next = W_hh.T @ dtanh\n    \n    # --- parameter update ---\n    W_xh -= learning_rate * dW_xh\n    W_hh -= learning_rate * dW_hh\n    W_hy -= learning_rate * dW_hy\n    b_h  -= learning_rate * db_h\n    b_y  -= learning_rate * db_y\n    \n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef entrypoint(input_sequence, expected_output,\n               input_size, hidden_size, output_size,\n               learning_rate=0.01, epochs=100, random_seed=42):\n\n    np.random.seed(random_seed)  # reproducibility\n    W_xh, W_hh, W_hy, b_h, b_y = initialize_weights(input_size, hidden_size, output_size)\n    \n    for epoch in range(epochs):\n        # forward pass\n        outputs, last_inputs, last_hiddens = rnn_forward(\n            W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence\n        )\n        # backward pass (BPTT)\n        W_xh, W_hh, W_hy, b_h, b_y = rnn_backward(\n            W_xh, W_hh, W_hy, b_h, b_y,\n            hidden_size,\n            input_sequence, expected_output,\n            outputs, last_inputs, last_hiddens,\n            learning_rate\n        )\n    # round the outputs for readability\n    return np.round(np.array(outputs), 5).tolist()\n```\n\nExplanation of key steps:\n\n1. **Weight Initialization**  \n   - Small random values (\u00d70.01) for weight matrices:  \n     - `W_xh` (input\u2192hidden)  \n     - `W_hh` (hidden\u2192hidden)  \n     - `W_hy` (hidden\u2192output)  \n   - Zero biases `b_h`, `b_y`.\n\n2. **Forward Pass (`rnn_forward`)**  \n   - Start with hidden state `h\u2080 = 0`.  \n   - For each time step `t`:  \n     - Convert `x\u209c` to a column vector.  \n     - Compute `h\u209c = tanh(W_xh\u00b7x\u209c + W_hh\u00b7h\u209c\u208b\u2081 + b_h)`.  \n     - Compute `y\u209c = W_hy\u00b7h\u209c + b_y`.  \n     - Store `x\u209c`, `h\u209c`, and flattened `y\u209c`.\n\n3. **Backward Pass (`rnn_backward`) / BPTT**  \n   - Work backwards through time steps.  \n   - For each `t` (in reverse):  \n     - Compute output error `dy = y\u209c - target\u209c` (derivative of \u00bd MSE).  \n     - Accumulate gradients for `W_hy`, `b_y`.  \n     - Backpropagate into hidden layer, add any carry-over gradient from future (`dh_next`).  \n     - Multiply by derivative of `tanh`.  \n     - Accumulate gradients for `W_xh`, `W_hh`, `b_h`.  \n     - Compute `dh_next = W_hh\u1d40\u00b7(\u2026)` to carry to earlier time step.  \n   - After summing gradients over all steps, update all parameters with gradient descent.\n\n4. **Entrypoint**  \n   - Initializes weights and biases, then runs the specified number of epochs of forward + backward passes.\n   - Returns the final sequence of predicted outputs, rounded for readability.\n\nThis simple RNN with BPTT can learn to predict the next item in a univariate or multivariate sequence."}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded to 8 decimals, as a list)\n    \"\"\"\n    # Ensure inputs are float arrays\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).flatten()\n    m = b.size\n\n    # Initial guess\n    if x0 is None:\n        x = np.zeros(m, dtype=float)\n    else:\n        x = np.array(x0, dtype=float).flatten()\n    \n    # Initial residual and direction\n    r = b - A.dot(x)\n    p = r.copy()\n    rsold = np.dot(r, r)\n    \n    for i in range(n):\n        Ap = A.dot(p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        \n        # Check convergence\n        if np.sqrt(rsnew) < tol:\n            break\n        \n        # Update direction\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n\n    # Round to 8 decimals and return as list\n    x_rounded = np.round(x, 8)\n    return x_rounded.tolist()\n```\n\nExplanation of key steps:\n1. Initialize `x` to the zero vector if no initial guess is provided.\n2. Compute the initial residual `r = b - A x` and set the initial search direction `p = r`.\n3. In each iteration:\n   - Compute step size `alpha`.\n   - Update the solution `x` and residual `r`.\n   - Check convergence via the residual norm.\n   - Compute `beta` to update the search direction `p`.\n4. Stop early if the residual norm falls below `tol`, or after `n` iterations.\n5. Return the solution rounded to 8 decimal places as a Python list."}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # If the list is empty or has only one class, impurity is zero\n    if not y or len(set(y)) <= 1:\n        return 0.0\n\n    # Get unique classes and their counts\n    classes, counts = np.unique(y, return_counts=True)\n    # Convert counts to probabilities\n    probs = counts / counts.sum()\n    # Compute Gini impurity\n    gini = 1.0 - np.sum(probs ** 2)\n    # Round to three decimal places\n    return round(float(gini), 3)\n\n\n# Example usage and simple tests\nif __name__ == \"__main__\":\n    print(gini_impurity([1, 1, 1, 1]))    # 0.0   (pure)\n    print(gini_impurity([0, 1, 0, 1]))    # 0.5   (50/50 split)\n    print(gini_impurity([0, 1, 2, 2, 2])) # 0.48  (p0=0.2,p1=0.2,p2=0.6 => 1 - (0.04+0.04+0.36)=0.56)\n    print(gini_impurity([]))              # 0.0   (empty)\n```\n\nExplanation:\n1. We first check if the input list is empty or contains just one unique class. In both cases the impurity is zero.\n2. We use `np.unique` to get class labels and their counts.\n3. We normalize counts into probabilities.\n4. Gini impurity is calculated as \\(1 - \\sum p_i^2\\).\n5. We round the result to three decimal places before returning."}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointer = [0]\n\n    # Iterate over each row\n    for row in dense_matrix:\n        # Count of non-zero elements encountered so far in this row\n        nnz_in_row = 0\n        for j, elem in enumerate(row):\n            if elem != 0:\n                values.append(elem)\n                col_indices.append(j)\n                nnz_in_row += 1\n        # The next row pointer is the last one plus the non\u2010zeros in this row\n        row_pointer.append(row_pointer[-1] + nnz_in_row)\n\n    return values, col_indices, row_pointer\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    dense = [\n        [0, 5, 0, 0],\n        [1, 0, 0, 2],\n        [0, 0, 0, 0],\n        [3, 0, 4, 0]\n    ]\n\n    vals, cols, ptrs = compressed_row_sparse_matrix(dense)\n    print(\"Values array:       \", vals)   # [5, 1, 2, 3, 4]\n    print(\"Column indices:     \", cols)   # [1, 0, 3, 0, 2]\n    print(\"Row pointer array:  \", ptrs)   # [0, 1, 3, 3, 5]\n```\n\nExplanation:\n\n1. **values** collects all non-zero entries in row-major order.  \n2. **col_indices** stores the column index for each non-zero entry in the same order.  \n3. **row_pointer** has length `(number_of_rows + 1)`.  \n   - `row_pointer[i]` is the starting index in `values` (and `col_indices`) for row `i`.  \n   - The final element `row_pointer[-1]` equals the total number of non-zero elements."}
{"task_id": 66, "completion_id": 0, "completion": "```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (list of numbers)\n    :param L: The line vector defining the direction of projection (list of numbers)\n    :return: List representing the projection of v onto L, rounded to three decimal places\n    \"\"\"\n    # Ensure both vectors have the same dimension\n    if len(v) != len(L):\n        raise ValueError(\"v and L must have the same dimension\")\n    \n    # Compute dot products\n    dot_vL = sum(vi * Li for vi, Li in zip(v, L))\n    dot_LL = sum(Li * Li for Li in L)\n    \n    if dot_LL == 0:\n        raise ValueError(\"The line vector L must be non-zero\")\n    \n    # Projection scalar factor\n    factor = dot_vL / dot_LL\n    \n    # Compute the projection vector and round components\n    projection = [round(factor * Li, 3) for Li in L]\n    return projection\n```\n\nExplanation of the steps:\n\n1. Check that `v` and `L` have the same dimensionality.  \n2. Compute the dot product `v \u00b7 L`.  \n3. Compute the dot product `L \u00b7 L` (the squared norm of `L`).  \n4. Raise an error if `L` is the zero vector (since we cannot project onto a zero-length direction).  \n5. The projection of `v` onto `L` is  \n   projection = ((v \u00b7 L) / (L \u00b7 L)) * L  \n6. Round each component of the resulting vector to three decimal places and return it.\n\nExample usage:\n\n```python\nv = [3, 4]\nL = [1, 2]\nprint(orthogonal_projection(v, L))  # Output: [1.0, 2.0]\n```\n"}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n       Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n       # Handle empty matrix\n    if not dense_matrix or not dense_matrix[0]:\n        return [], [], [0]\n\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n\n    # Check that the matrix is well-formed (all rows same length)\n    for row in dense_matrix:\n        if len(row) != num_cols:\n            raise ValueErrorAll rows in the dense_matrix must have the same length    values = []\n    row_indices = []\n    col_ptr = [0]  # col_ptr[j] will be the starting index in values for column j\n\n    nnz_count = 0  # total non-zeros seen so far\n\n    # Iterate column by column\n    for j in range(num_cols):\n        # Record the start of this column\n        col_ptr.append(nnz_count)\n\n        # Scan down the column\n        for i in range(num_rows):\n            v = dense_matrix[i][j]\n            if v != 0:\n                values.append(v)\n                row_indices.append(i)\n                nnz_count += 1\n\n    # After the loop, we appended num_cols entries to col_ptr,\n    # but col_ptr should have length num_cols+1, with col_ptr[0]=0\n    # and col_ptr[k] = start index of column k in values.\n    # Currently col_ptr = [0, c0_start, c1_start, ..., c_{m-1}_start]\n    # where c_k_start is same as sum of nnz in columns 0..k-1.\n    # We need to append the final nnz_count for col_ptr[num_cols].\n    col_ptr[-1] = 0               # currently overwritten by first append\n    # Let's rebuild col_ptr properly:\n    col_ptr = [0] * (num_cols + 1)\n    idx = 0\n    for j in range(num_cols):\n        col_ptr[j] = idx\n        for i in range(num_rows):\n            if dense_matrix[i][j] != 0:\n                idx += 1\n    col_ptr[num_cols] = idx\n\n    return values, row_indices, col_ptr\n\n\n# Example usage:\nif __name__ ==main    A = [\n        [0, 5, 0, 0],\n        [3, 0, 0, 4],\n        [0, 0, 7, 0],\n        [0, 0, 0, 6],\n    ]\n    vals, rows, ptrs = compressed_col_sparse_matrix(A)\n    printvalues      vals)    # [3, 5, 7, 4, 6]\n    printrow_indices rows)    # [1, 0, 2, 1, 3]\n    printcol_ptr     ptrs)    # [0, 1, 2, 3, 5]\n```\n\nExplanation:\n\n1. We first guard against empty input.\n2. We validate the matrix is well-formed (all rows same length).\n3. We scan column by column (`j` from 0 to num_cols\u20131). For each nonzero entry at `(i, j)`, we append the value to `values` and `i` to `row_indices`.\n4. We build `col_ptr` with length num_cols+1, where:\n   - `col_ptr[j]` is the starting index in `values` for column `j`.\n   - `col_ptr[num_cols]` is the total number of nonzeros.\n5. The example demonstrates the CSC output for a small 4\u00d74 matrix."}
{"task_id": 68, "completion_id": 0, "completion": "Here\u2019s one way to do it with plain NumPy.  We perform a simple Gaussian\u2010elimination\u2013style pass to locate the pivot columns, then grab those columns from the original matrix as a basis for the column\u2010space:\n\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    # ensure A is an np.ndarray of floats\n    A = np.array(A, dtype=float)\n    m, n = A.shape\n\n    # make a working copy for elimination\n    M = A.copy()\n    pivot_cols = []\n    row = 0\n    tol = 1e-12\n\n    # Gaussian elimination to find pivots\n    for col in range(n):\n        # find the row with largest |M[i,col]| for i>=row\n        if row >= m:\n            break\n        sub_col = np.abs(M[row:, col])\n        max_i = np.argmax(sub_col) + row\n        if abs(M[max_i, col]) < tol:\n            continue  # no pivot in this column\n\n        # swap into place\n        M[[row, max_i]] = M[[max_i, row]]\n        pivot_cols.append(col)\n\n        # normalize pivot row (optional)\n        pivot_val = M[row, col]\n        M[row] /= pivot_val\n\n        # eliminate below\n        for i in range(row + 1, m):\n            M[i] -= M[i, col] * M[row]\n\n        row += 1\n\n    # extract the independent columns from the original A\n    basis = A[:, pivot_cols]\n    basis = np.round(basis, 8)\n\n    return basis.tolist()\n```\n\nExplanation of the main steps:\n\n1.  Convert input to a float NumPy array and copy it so we don\u2019t destroy the original.\n2.  March through each column, looking for a pivot in or below the current `row`.\n3.  When we find a non\u2010zero pivot, swap it into the \u201crow\u201d position, record the column index, normalize and eliminate downward.\n4.  After the loop, `pivot_cols` holds the indices of independent columns.\n5.  Return those columns of the *original* matrix, rounded to eight decimals, as a Python list of lists."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the coefficient of determination, R-squared, for a set of true vs. predicted values.\n\n    Parameters:\n    - y_true: array-like of shape (n_samples,)\n    - y_pred: array-like of shape (n_samples,)\n\n    Returns:\n    - R\u00b2 value rounded to three decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Sum of squares of residuals\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    # Total sum of squares\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n\n    # Handle the edge case where variance is zero\n    if ss_tot == 0:\n        # Perfect fit or zero variability in y_true\n        return 1.000\n\n    # Compute R-squared\n    r2 = 1 - (ss_res / ss_tot)\n\n    # Round to three decimal places and return\n    return round(r2, 3)\n```\n\nExample usage:\n\n```python\ny_true = [3, 5, 2, 7]\ny_pred = [2.8, 4.9, 2.1, 7.2]\n\nprint(r_squared(y_true, y_pred))  # e.g., 0.992\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n       Calculate the average brightness of a grayscale image.\n\n    img: 2D list of integers between 0 and 255 inclusive.\n    Returns the average brightness rounded to two decimal places,\n    or -1 if:\n      - img is empty,\n      - any row is empty or rows have inconsistent lengths,\n      - any pixel is not an int in [0, 255].\n       # Check that img is a non-empty list of rows\n    if not isinstance(img, list) or len(img) == 0:\n        return -1\n\n    # Determine the expected row length from the first row\n    first_row = img[0]\n    if not isinstance(first_row, list) or len(first_row) == 0:\n        return -1\n    expected_len = len(first_row)\n\n    total = 0\n    count = 0\n\n    for row in img:\n        # Each row must be a list of the same length\n        if not isinstance(row, list) or len(row) != expected_len:\n            return -1\n\n        for pixel in row:\n            # Each pixel must be an integer in [0, 255]\n            if not isinstance(pixel, int) or pixel < 0 or pixel > 255:\n                return -1\n            total += pixel\n            count += 1\n\n    # Just in case, guard against division by zero\n    if count == 0:\n        return -1\n\n    average = total / count\n    return round(average, 2)\n```\n\nExplanation of key steps:\n\n1. **Empty or malformed image**  \n   - If `img` is not a list or is empty \u2192 return -1.  \n   - If the first row is not a list or is empty \u2192 return -1.\n\n2. **Row length consistency**  \n   - Record the length of the first row (`expected_len`).  \n   - For each subsequent row, if it\u2019s not a list or its length differs \u2192 return -1.\n\n3. **Pixel validation**  \n   - Each pixel must be an integer between 0 and 255.  \n   - If any pixel fails this test \u2192 return -1.\n\n4. **Average computation**  \n   - Sum all valid pixels and count them.  \n   - If `count` is zero (shouldn\u2019t happen after the checks) \u2192 return -1.  \n   - Otherwise compute `total / count` and round to two decimal places."}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual values and predictions.\n\n    Parameters:\n    ----------\n    y_true : array-like\n        Ground truth (correct) target values.\n    y_pred : array-like\n        Estimated target values.\n\n    Returns:\n    -------\n    float\n        The RMSE rounded to three decimal places.\n\n    Raises:\n    ------\n    TypeError\n        If inputs cannot be converted to numeric arrays.\n    ValueError\n        If arrays have mismatched shapes or are empty.\n    \"\"\"\n    # Attempt to convert inputs to numpy arrays of float dtype\n    try:\n        y_true_arr = np.asarray(y_true, dtype=float)\n        y_pred_arr = np.asarray(y_pred, dtype=float)\n    except Exception as e:\n        raise TypeError(f\"Inputs must be array-like of numbers. Conversion error: {e}\")\n\n    # Check for empty arrays\n    if y_true_arr.size == 0 or y_pred_arr.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n\n    # Check that shapes match\n    if y_true_arr.shape != y_pred_arr.shape:\n        raise ValueError(\n            f\"Shape mismatch: y_true has shape {y_true_arr.shape}, \"\n            f\"but y_pred has shape {y_pred_arr.shape}.\"\n        )\n\n    # Compute the RMSE\n    diff = y_true_arr - y_pred_arr\n    mse = np.mean(diff ** 2)\n    result = np.sqrt(mse)\n\n    # Round to three decimal places and return as plain float\n    return float(np.round(result, 3))\n```\n\nExplanation of key steps:\n\n1. **Input Conversion & Validation**  \n   - We wrap the conversion in a `try`/`except` to catch any non\u2010numeric or ill\u2010formed inputs, raising a `TypeError` if conversion fails.  \n   - We ensure neither array is empty (`size == 0`).  \n   - We enforce that the two arrays have identical shapes, raising a `ValueError` otherwise.\n\n2. **RMSE Computation**  \n   - Difference: `diff = y_true_arr - y_pred_arr`  \n   - Mean Squared Error (MSE): `mse = np.mean(diff ** 2)`  \n   - Root MSE: `result = np.sqrt(mse)`\n\n3. **Rounding & Return**  \n   - We round the final result to three decimal places via `np.round` and cast to a native Python `float`.\n\nThis implementation handles edge cases cleanly, provides informative error messages, and returns the RMSE rounded as specified."}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index (Intersection over Union) for binary labels.\n\n    Parameters:\n    - y_true: array-like of shape (n_samples,), ground truth binary labels (0 or 1)\n    - y_pred: array-like of shape (n_samples,), predicted binary labels (0 or 1)\n\n    Returns:\n    - Jaccard Index as a float rounded to three decimal places.\n      Returns 1.0 if both y_true and y_pred contain only zeros.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Compute intersection and union\n    intersection = np.logical_and(y_true == 1, y_pred == 1).sum()\n    union = np.logical_or(y_true == 1, y_pred == 1).sum()\n\n    # Handle edge case: both arrays are all zeros\n    if union == 0:\n        return 1.0\n\n    j_index = intersection / union\n    return round(j_index, 3)\n```\n\nExplanation of key steps:\n1. We cast `y_true` and `y_pred` to NumPy arrays to leverage vectorized logical operations.\n2. `np.logical_and(y_true == 1, y_pred == 1).sum()` counts the number of positions where both arrays have a 1 (intersection).\n3. `np.logical_or(y_true == 1, y_pred == 1).sum()` counts the number of positions where at least one array has a 1 (union).\n4. If `union` is zero (meaning both arrays are all zeros), we define the Jaccard Index to be 1.0 (perfect overlap of empty sets).\n5. Otherwise, we compute `intersection/union` and round it to three decimal places before returning."}
{"task_id": 73, "completion_id": 0, "completion": "import numpy as np\n\ndef dice_score(y_true, y_pred):\n       Compute the Dice Score (S\u00f8rensen\u2013Dice coefficient) for binary classification.\n\n    Parameters:\n    - y_true: array-like of shape (n_samples,), ground truth binary labels (0 or 1)\n    - y_pred: array-like of shape (n_samples,), predicted binary labels (0 or 1)\n\n    Returns:\n    - dice: float, Dice Score rounded to 3 decimal places.\n       # Convert inputs to boolean numpy arrays\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n\n    if y_true.shape != y_pred.shape:\n        raise ValueError(fShape mismatch: y_true shape {y_true.shape} vs y_pred shape {y_pred.shape    # Count positives in each\n    true_pos = np.sum(y_true)\n    pred_pos = np.sum(y_pred)\n\n    # If no positives in both, define score as 1.0 (perfect overlap)\n    if true_pos + pred_pos == 0:\n        return 1.0\n\n    # Compute intersection\n    intersection = np.sum(y_true & y_pred)\n\n    # Dice formula\n    dice = 2 * intersection / (true_pos + pred_pos)\n\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport hashlib\n\ndef _stable_hash(x):\n    \"\"\"\n    Compute a stable integer hash for the string representation of x.\n    Uses MD5 and returns a 32\u2010bit integer.\n    \"\"\"\n    h = hashlib.md5(str(x).encode('utf-8')).hexdigest()\n    # Take the low 8 hex digits \u2192 32 bits\n    return int(h[-8:], 16)\n\ndef _make_hv(seed, dim):\n    \"\"\"\n    Generate a bipolar hypervector of dimension `dim`\n    using a fixed integer seed.\n    Entries are +1 or -1 with equal probability.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    return rng.choice([-1, 1], size=dim)\n\ndef _bind(hv1, hv2):\n    \"\"\"\n    Bind two bipolar hypervectors by elementwise multiplication.\n    \"\"\"\n    return hv1 * hv2\n\ndef _bundle(hv_list):\n    \"\"\"\n    Bundle (i.e. sum and bipolarize) a list of hypervectors.\n    After summation, components >= 0 go to +1, else -1.\n    \"\"\"\n    s = np.sum(hv_list, axis=0)\n    return np.where(s >= 0, 1, -1)\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Given:\n      - row: dict of {feature_name: value}\n      - dim: hypervector dimensionality\n      - random_seeds: dict of {feature_name: seed_int}\n    Returns:\n      - a composite bipolar hypervector (as a Python list)\n        representing the entire row.\n    \"\"\"\n    bound_hvs = []\n    \n    for feat, val in row.items():\n        if feat not in random_seeds:\n            raise KeyError(f\"No seed provided for feature '{feat}'\")\n        \n        seed = int(random_seeds[feat])\n        # 1) feature\u2010name hypervector\n        hv_feat = _make_hv(seed, dim)\n        \n        # 2) feature\u2010value hypervector\n        #    combine the feature seed with a stable hash of the value\n        val_hash = _stable_hash(val)\n        hv_val = _make_hv((seed ^ val_hash) & 0xFFFFFFFF, dim)\n        \n        # 3) bind them\n        bound = _bind(hv_feat, hv_val)\n        bound_hvs.append(bound)\n    \n    # 4) bundle all bound hypervectors into one composite vector\n    composite = _bundle(bound_hvs)\n    \n    # 5) return as a plain Python list\n    return composite.tolist()\n```\n\nExplanation of the steps:\n\n1. **Stable hashing**  \n   We need a reproducible way to map arbitrary feature values (e.g. strings or numbers) to seeds. We use an MD5\u2010based hash truncated to 32 bits.\n\n2. **Generating bipolar hypervectors**  \n   `_make_hv(seed, dim)` creates a random \u00b11 vector of length `dim` by seeding NumPy\u2019s RandomState.\n\n3. **Binding**  \n   We bind a feature name and its value by elementwise multiplication of their bipolar hypervectors. This mimics XOR in binary HD computing.\n\n4. **Bundling**  \n   We sum all bound vectors component\u2010wise and then bipolarize the sum: components \u2265 0 go to +1, else \u22121.\n\n5. **Putting it all together**  \n   The function loops over each feature, generates its name\u2010HV and value\u2010HV, binds them, collects all bound HVS, bundles them, and finally returns the result as a list.\n"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for binary classification.\n    Assumes the two classes are labeled 0 (negative) and 1 (positive).\n\n    The returned matrix is:\n        [[TN, FP],\n         [FN, TP]]\n\n    where\n      TN = count of true negatives  (y_true=0, y_pred=0)\n      FP = count of false positives (y_true=0, y_pred=1)\n      FN = count of false negatives (y_true=1, y_pred=0)\n      TP = count of true positives  (y_true=1, y_pred=1)\n    \"\"\"\n    # Count occurrences of each (true, pred) pair\n    counts = Counter((y_true, y_pred) for y_true, y_pred in data)\n\n    # Extract the four entries (Counter will return 0 if a key is missing)\n    tn = counts[(0, 0)]\n    fp = counts[(0, 1)]\n    fn = counts[(1, 0)]\n    tp = counts[(1, 1)]\n\n    return [[tn, fp],\n            [fn, tp]]\n```\n\nExample usage:\n\n```python\ndata = [\n    [0, 0], [0, 1], [1, 0], [1, 1],\n    [1, 1], [0, 0], [1, 0], [0, 1]\n]\ncm = confusion_matrix(data)\nprint(cm)   # [[2, 2],\n            #  [2, 2]]\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n       Calculate the cosine similarity between two non-zero vectors.\n\n    Parameters:\n    - v1: numpy array of shape (n,)\n    - v2: numpy array of shape (n,)\n\n    Returns:\n    - float: cosine similarity rounded to three decimal places\n\n    Raises:\n    - ValueError: if vectors have different shapes, are empty, or have zero magnitude\n       # Ensure same shape\n    if v1.shape != v2.shape:\n        raise ValueError(fShape mismatch: v1.shape={v1.shape}, v2.shape={v2.shape    # Compute norms\n    norm1 = np.linalg.norm(v1)\n    norm2 = np.linalg.norm(v2)\n\n    # Check for zero magnitude\n    if norm1 == 0 or norm2 == 0:\n        raise ValueErrorInput vectors must have non-zero magnitude    # Compute cosine similarity\n    dot_product = np.dot(v1, v2)\n    cos_sim = dot_product / (norm1 * norm2)\n\n    # Round to three decimal places\n    return round(float(cos_sim), 3)\n```\n\nExplanation of key steps:\n1. **Shape check**: Ensure `v1` and `v2` have identical dimensions.  \n2. **Zero-magnitude check**: Cosine similarity is undefined if either vector has length zero.  \n3. **Dot product & norms**: Compute the numerator (`v1\u00b7v2`) and denominator (`||v1||\u00b7||v2||`).  \n4. **Normalization**: Divide dot product by the product of norms.  \n5. **Rounding**: Return the result rounded to three decimal places.\n\nExample usage:\n```python\nif __name__ ==main    a = np.array([1, 2, 3])\n    b = np.array([4, 5, 6])\n    print(cosine_similarity(a, b))  # Output: 0.975\n```"}
{"task_id": 77, "completion_id": 0, "completion": "from collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n       Compute confusion matrix, accuracy, F1 score, specificity, and negative predictive value\n    for a binary classification task where labels are 0 (negative) or 1 (positive).\n    Returns:\n      - confusion_matrix: [[TN, FP], [FN, TP]]\n      - accuracy: float\n      - f1_score: float\n      - specificity: float\n      - negative_predictive_value: float\n    All floating\u2010point metrics are rounded to three decimal places.\n       # Input validation\n    if len(actual) != len(predicted):\n        raise ValueErrorLength mismatch between actual and predicted lists    for x in actual + predicted:\n        if x not in (0, 1):\n            raise ValueErrorAll labels must be 0 or 1    # Count TP, TN, FP, FN via Counter\n    pairs = list(zip(actual, predicted))\n    cnt = Counter(pairs)\n    TP = cnt[(1, 1)]\n    TN = cnt[(0, 0)]\n    FP = cnt[(0, 1)]\n    FN = cnt[(1, 0)]\n\n    # Confusion matrix\n    confusion_matrix = [[TN, FP],\n                        [FN, TP]]\n\n    # Basic rates\n    total = TP + TN + FP + FN\n    accuracy = (TP + TN) / total if total else 0.0\n\n    # Precision and recall\n    precision = TP / (TP + FP) if (TP + FP) else 0.0\n    recall = TP / (TP + FN) if (TP + FN) else 0.0\n\n    # F1 score\n    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0.0\n\n    # Specificity = TN / (TN + FP)\n    specificity = TN / (TN + FP) if (TN + FP) else 0.0\n\n    # Negative predictive value = TN / (TN + FN)\n    negative_predictive_value = TN / (TN + FN) if (TN + FN) else 0.0\n\n    # Round to three decimal places\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n\n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n\n# Example usage:\n# actual   = [1, 0, 1, 1, 0, 0, 1]\n# predicted= [1, 0, 0, 1, 1, 0, 1]\n# cm, acc, f1, spec, npv = performance_metrics(actual, predicted)\n# printCM cm,Acc acc,F1 f1,Spec spec,NPV npv)"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate descriptive statistics for a list or NumPy array of numerical values.\n\n    Parameters:\n        data (list or np.ndarray): numerical data\n\n    Returns:\n        dict: {\n            'mean': float,\n            'median': float,\n            'mode': float or int,\n            'variance': float,\n            'standard_deviation': float,\n            '25th_percentile': float,\n            '50th_percentile': float,\n            '75th_percentile': float,\n            'interquartile_range': float\n        }\n    \"\"\"\n    # Convert to NumPy array\n    arr = np.asarray(data, dtype=float)\n    if arr.size == 0:\n        raise ValueError(\"Input data must contain at least one value.\")\n\n    # Mean and median\n    mean = np.mean(arr)\n    median = np.median(arr)\n\n    # Mode: find the value(s) with highest frequency; if tie, pick the smallest\n    counts = Counter(arr)\n    max_count = max(counts.values())\n    modes = [val for val, cnt in counts.items() if cnt == max_count]\n    mode = min(modes)\n\n    # Variance and standard deviation (population, ddof=0)\n    variance = np.var(arr, ddof=0)\n    std_dev = np.std(arr, ddof=0)\n\n    # Percentiles\n    p25, p50, p75 = np.percentile(arr, [25, 50, 75])\n\n    # Interquartile range\n    iqr = p75 - p25\n\n    # Build result, rounding all computed metrics to 4 decimals except 'mode'\n    result = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,\n        'variance': round(variance, 4),\n        'standard_deviation': round(std_dev, 4),\n        '25th_percentile': round(p25, 4),\n        '50th_percentile': round(p50, 4),\n        '75th_percentile': round(p75, 4),\n        'interquartile_range': round(iqr, 4)\n    }\n\n    return result\n```\n\nExample usage:\n\n```python\ndata = [1, 2, 2, 3, 4, 5, 5, 5, 6]\nstats = descriptive_statistics(data)\nprint(stats)\n# {\n#   'mean': 3.6667,\n#   'median': 4.0,\n#   'mode': 5.0,\n#   'variance': 2.9605,\n#   'standard_deviation': 1.7200,\n#   '25th_percentile': 2.0,\n#   '50th_percentile': 4.0,\n#   '75th_percentile': 5.0,\n#   'interquartile_range': 3.0\n# }\n```"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n       Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n       # Input validation\n    if not (0 <= k <= n):\n        raise ValueErrork must be an integer between 0 and n inclusive    if not (0.0 <= p <= 1.0):\n        raise ValueErrorp must be between 0 and 1    \n    # Compute the binomial coefficient C(n, k)\n    # Note: math.comb is available in Python 3.8+\n    comb = math.comb(n, k)\n    \n    # Calculate probability\n    prob = comb * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Round to five decimal places and return\n    return round(prob, 5)\n\n# Example usage:\nif __name__ ==main    n = 10\n    k = 3\n    p = 0.5\n    print(f(X = {k}) binomial_probability(n, k, p))\n```\n\nExplanation of key steps:\n- We use `math.comb(n, k)` to compute the binomial coefficient C(n, k) = n! / (k!(n-k)!).\n- We then multiply by `p**k` (the probability of k successes) and `(1-p)**(n-k)` (the probability of n-k failures).\n- Finally, we round the result to five decimal places before returning."}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    # Coefficient term: 1 / (\u03c3 * sqrt(2\u03c0))\n    coeff = 1.0 / (std_dev * math.sqrt(2 * math.pi))\n    \n    # Exponent term: exp( - (x - \u03bc)^2 / (2\u03c3^2) )\n    exponent = -((x - mean) ** 2) / (2 * (std_dev ** 2))\n    \n    pdf_value = coeff * math.exp(exponent)\n    \n    # Round result to 5 decimal places\n    return round(pdf_value, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(normal_pdf(0, 0, 1))     # Standard normal at 0; should be \u2248 0.39894\n    print(normal_pdf(1, 0, 1))     # Standard normal at 1; should be \u2248 0.24197\n    print(normal_pdf(5, 5, 2))     # Normal with mean=5, \u03c3=2 at x=5; should be \u2248 0.19947\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    # Validate inputs\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"k must be a non-negative integer\")\n    if lam < 0:\n        raise ValueError(\"lam must be non-negative\")\n\n    # Poisson formula: P(X = k) = (e^(-lam) * lam^k) / k!\n    prob = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    return round(prob, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(poisson_probability(3, 2.5))   # Probability of exactly 3 events when \u03bb=2.5\n    print(poisson_probability(0, 1.2))   # Probability of zero events when \u03bb=1.2\n    print(poisson_probability(5, 5))     # Probability of exactly 5 events when \u03bb=5\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n       Calculate the contrast of a grayscale image.\n    \n    Contrast is defined here as the difference between the maximum and minimum pixel \n    values in the image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel \n                             values between 0 and 255.\n    \n    Returns:\n        int: The contrast value (max_pixel_value - min_pixel_value).\n    \n    Raises:\n        TypeError: If img is not a numpy ndarray.\n        ValueError: If img is not 2D or is empty.\n       # Validate input type\n    if not isinstance(img, np.ndarray):\n        raise TypeErrorInput must be a numpy.ndarray    \n    # Validate dimensionality\n    if img.ndim != 2:\n        raise ValueError(fExpected a 2D array for a grayscale image, got {img.ndim}D instead    \n    # Validate non-emptiness\n    if img.size == 0:\n        raise ValueErrorInput image is empty    \n    # Optionally, you can also check pixel value range:\n    # if img.min() < 0 or img.max() > 255:\n    #     raise ValueErrorPixel values should be in the range [0, 255].    \n    # Compute min and max pixel values\n    min_val = np.min(img)\n    max_val = np.max(img)\n    \n    # Calculate contrast\n    contrast = int(max_val) - int(min_val)\n    \n    return contrast\n\n\n# Example usage\nif __name__ ==main    # 1) A simple 3x3 test image\n    test_img = np.array([\n        [  0,  50, 100],\n        [150, 200, 255],\n        [123,  76,  30]\n    ], dtype=np.uint8)\n    printContrast of test_img calculate_contrast(test_img))  # Should print 255 - 0 = 255\n\n    # 2) A uniform image (contrast = 0)\n    uniform_img = np.full((5, 5), 128, dtype=np.uint8)\n    printContrast of uniform_img calculate_contrast(uniform_img))  # Should print 0\n\n    # 3) Random image\n    random_img = np.random.randint(0, 256, size=(100, 100), dtype=np.uint8)\n    printContrast of random_img calculate_contrast(random_img))\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float or numpy scalar: The dot product of vec1 and vec2.\n    \n    Raises:\n        ValueError: If inputs are not 1D arrays or their lengths do not match.\n    \"\"\"\n    # Convert inputs to numpy arrays (in case lists or other sequences are passed)\n    arr1 = np.asarray(vec1)\n    arr2 = np.asarray(vec2)\n    \n    # Validate dimensions\n    if arr1.ndim != 1 or arr2.ndim != 1:\n        raise ValueError(\"Both inputs must be 1D arrays.\")\n    \n    # Validate length\n    if arr1.shape[0] != arr2.shape[0]:\n        raise ValueError(\"Input vectors must have the same length.\")\n    \n    # Compute dot product\n    result = np.dot(arr1, arr2)\n    \n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    v1 = np.array([1, 2, 3])\n    v2 = np.array([4, 5, 6])\n    print(\"Dot product:\", calculate_dot_product(v1, v2))\n    # Expected output: Dot product: 32\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space\n    by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial\n                           features [1, x, x^2, ..., x^degree] of the corresponding\n                           data point, rounded to 8 decimal places. Returns an empty\n                           list if degree < 0.\n    \"\"\"\n    # If degree is negative, return empty list\n    if degree < 0:\n        return []\n\n    # Convert input data to a NumPy array\n    x = np.array(data, dtype=float)\n\n    # Generate the Vandermonde matrix with columns [x^0, x^1, ..., x^degree]\n    # 'increasing=True' ensures powers go from 0 up to degree\n    phi_matrix = np.vander(x, N=degree + 1, increasing=True)\n\n    # Round each element to 8 decimal places\n    phi_matrix = np.round(phi_matrix, 8)\n\n    # Convert to a nested Python list and return\n    return phi_matrix.tolist()\n```\n\nExplanation of key steps:\n\n1. **Degree check**: If `degree < 0`, we immediately return an empty list as specified.\n2. **NumPy conversion**: We cast the input list to a NumPy array for efficient numerical operations.\n3. **Vandermonde matrix**: `np.vander(x, N=degree+1, increasing=True)` constructs a matrix whose columns are powers of `x` from 0 up to `degree`. Thus each row corresponds to one data point\u2019s features `[x^0, x^1, \u2026, x^degree]`.\n4. **Rounding**: We use `np.round(..., 8)` to ensure each element is rounded to 8 decimal places.\n5. **Return format**: Finally, `.tolist()` converts the NumPy matrix into the requested nested Python list."}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n       Compute the positional encoding matrix for a Transformer.\n    \n    Args:\n        position (int): the maximum sequence length (number of positions).\n        d_model (int): the dimensionality of the model.\n    \n    Returns:\n        list or int: a nested list of shape (position, d_model) containing\n                     the positional encodings of dtype float16, or -1 if\n                     position is 0 or d_model <= 0.\n       # Edge cases as specified\n    if position == 0 or d_model <= 0:\n        return -1\n\n    # Create a (position, 1) vector of position indices\n    pos = np.arange(position).reshape(position, 1).astype(np.float32)\n    # Create a (1, d_model) vector of dimension indices\n    i = np.arange(d_model).reshape(1, d_model).astype(np.float32)\n\n    # Compute the angle rates: 1 / (10000^(2*(i//2) / d_model))\n    angle_rates = 1.0 / np.power(10000, (2 * (i // 2)) / d_model)\n\n    # Compute the angle radians\n    angle_rads = pos * angle_rates  # shape: (position, d_model)\n\n    # Apply sin to even indices (0-based), cos to odd indices\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    # Cast to float16 and convert to nested list\n    pos_encoding_matrix = angle_rads.astype(np.float16).tolist()\n\n    return pos_encoding_matrix\n```\n\nExplanation of key steps:\n\n1. **Edge-case handling**: immediately return `-1` if `position == 0` or `d_model <= 0`.  \n2. **Index preparation**:  \n   - `pos` is a column vector of shape `(position, 1)` containing position indices.  \n   - `i` is a row vector of shape `(1, d_model)` containing dimension indices.  \n3. **Angle rate calculation**:  \n   \\[\n     \\text{angle\\_rates}[p,i] = \\frac{1}{10000^{2\\lfloor i/2\\rfloor / d\\_model}}\n   \\]  \n4. **Angle radians**: matrix multiplication `pos * angle_rates` yields shape `(position, d_model)`.  \n5. **Sine and cosine application**: sine on even dimensions, cosine on odd dimensions.  \n6. **Output formatting**: cast to `np.float16` for the required dtype, then `.tolist()` to produce a Python nested list."}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of  1  (Overfitting),\n                           -1 (Underfitting),\n                            0 (Good fit).\n    \"\"\"\n    # Validate inputs\n    if not (0.0 <= training_accuracy <= 1.0):\n        raise ValueError(f\"training_accuracy must be between 0 and 1. Got {training_accuracy}\")\n    if not (0.0 <= test_accuracy <= 1.0):\n        raise ValueError(f\"test_accuracy must be between 0 and 1. Got {test_accuracy}\")\n\n    # Overfitting: Training much better than testing\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n\n    # Underfitting: Both train and test are low\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n\n    # Good fit otherwise\n    return 0\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    examples = [\n        (0.95, 0.70),  # Overfitting\n        (0.65, 0.60),  # Underfitting\n        (0.85, 0.80),  # Good fit\n        (0.75, 0.55),  # Overfitting (difference 0.20 exactly \u2192 good fit by rule, but > 0.2 needed for overfit)\n        (0.75, 0.54),  # Overfitting (difference 0.21)\n    ]\n\n    for train_acc, test_acc in examples:\n        result = model_fit_quality(train_acc, test_acc)\n        label = {1: \"Overfitting\", -1: \"Underfitting\", 0: \"Good fit\"}[result]\n        print(f\"Train: {train_acc:.2f}, Test: {test_acc:.2f} \u2192 {label}\")\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n       Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value (scalar or array-like)\n    :param grad: Current gradient (scalar or array-like)\n    :param m: First moment estimate (scalar or array-like)\n    :param v: Second moment estimate (scalar or array-like)\n    :param t: Current timestep (integer)\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n             where each is rounded to 5 decimals and converted to a list\n       # convert inputs to numpy arrays\n    param = np.array(parameter, dtype=float)\n    grad = np.array(grad, dtype=float)\n    m = np.array(m, dtype=float)\n    v = np.array(v, dtype=float)\n\n    # update time step\n    t = t + 1\n\n    # update biased first and second moment estimates\n    m_t = beta1 * m + (1 - beta1) * grad\n    v_t = beta2 * v + (1 - beta2) * (grad ** 2)\n\n    # compute bias-corrected moment estimates\n    m_hat = m_t / (1 - beta1 ** t)\n    v_hat = v_t / (1 - beta2 ** t)\n\n    # update parameters\n    param_updated = param - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # round to 5 decimals and convert to list\n    p_out = np.round(param_updated, 5).tolist()\n    m_out = np.round(m_t, 5).tolist()\n    v_out = np.round(v_t, 5).tolist()\n\n    return p_out, m_out, v_out\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    # Dummy parameters\n    params = {\n        \"wte\": np.random.rand(3, 10),       # vocab_size=3, d_model=10\n        \"wpe\": np.random.rand(1024, 10),    # max_pos=1024, d_model=10\n        \"blocks\": [],                       # we'll fill one block later\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    \"\"\"\n    Simplified GPT-2-like autoregressive text generator.\n    \"\"\"\n    np.random.seed(42)  # reproducibility\n\n    # Load dummy encoder, hyperparameters, and model params\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    wte, wpe = params[\"wte\"], params[\"wpe\"]\n    d_model = wte.shape[1]\n\n    # Build a single transformer block if none exist\n    if not params[\"blocks\"]:\n        block = {}\n        # LayerNorm 1 params\n        block[\"ln_1\"] = {\n            \"g\": np.ones(d_model),\n            \"b\": np.zeros(d_model)\n        }\n        # Self-attention params (single-head for simplicity)\n        block[\"attn\"] = {\n            \"wq\": np.random.randn(d_model, d_model),\n            \"wk\": np.random.randn(d_model, d_model),\n            \"wv\": np.random.randn(d_model, d_model),\n            \"wo\": np.random.randn(d_model, d_model),\n        }\n        # LayerNorm 2 params\n        block[\"ln_2\"] = {\n            \"g\": np.ones(d_model),\n            \"b\": np.zeros(d_model)\n        }\n        # MLP params\n        block[\"mlp\"] = {\n            \"w1\": np.random.randn(d_model, 4 * d_model),\n            \"b1\": np.zeros(4 * d_model),\n            \"w2\": np.random.randn(4 * d_model, d_model),\n            \"b2\": np.zeros(d_model),\n        }\n        params[\"blocks\"] = [block]\n\n    # Tokenize prompt\n    past_tokens = encoder.encode(prompt)\n\n    # Autoregressive generation loop\n    for _ in range(n_tokens_to_generate):\n        seq = past_tokens\n        seq_len = len(seq)\n\n        # Embedding + positional\n        token_emb = wte[seq]                     # shape: (seq_len, d_model)\n        pos_emb   = wpe[np.arange(seq_len)]      # shape: (seq_len, d_model)\n        x = token_emb + pos_emb                  # input to transformer\n\n        # Pass through all blocks (only one here)\n        for block in params[\"blocks\"]:\n            # LayerNorm 1\n            mu = x.mean(axis=-1, keepdims=True)\n            var = x.var(axis=-1, keepdims=True)\n            x_norm = (x - mu) / np.sqrt(var + 1e-5)\n            x_norm = x_norm * block[\"ln_1\"][\"g\"] + block[\"ln_1\"][\"b\"]\n\n            # Self-attention (single-head)\n            wq, wk, wv, wo = block[\"attn\"].values()\n            q = x_norm @ wq  # (seq_len, d)\n            k = x_norm @ wk\n            v = x_norm @ wv\n\n            # Scaled dot-product with causal mask\n            scores = q @ k.T / np.sqrt(d_model)\n            mask = np.tril(np.ones_like(scores))\n            scores = scores * mask + (1 - mask) * (-1e9)\n            attn_w = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n            attn_w = attn_w / attn_w.sum(axis=-1, keepdims=True)\n\n            attn_out = attn_w @ v                # (seq_len, d)\n            attn_proj = attn_out @ wo           # (seq_len, d)\n            x = x + attn_proj                   # residual\n\n            # LayerNorm 2\n            mu2 = x.mean(axis=-1, keepdims=True)\n            var2 = x.var(axis=-1, keepdims=True)\n            x_norm2 = (x - mu2) / np.sqrt(var2 + 1e-5)\n            x_norm2 = x_norm2 * block[\"ln_2\"][\"g\"] + block[\"ln_2\"][\"b\"]\n\n            # Feed-Forward\n            w1, b1 = block[\"mlp\"][\"w1\"], block[\"mlp\"][\"b1\"]\n            w2, b2 = block[\"mlp\"][\"w2\"], block[\"mlp\"][\"b2\"]\n            ff = x_norm2 @ w1 + b1\n            # GELU activation\n            ff = 0.5 * ff * (1 + np.tanh(np.sqrt(2/np.pi) * (ff + 0.044715 * ff**3)))\n            ff = ff @ w2 + b2\n            x = x + ff  # residual\n\n        # Final layer norm\n        ln_f = params[\"ln_f\"]\n        mu_f = x.mean(axis=-1, keepdims=True)\n        var_f = x.var(axis=-1, keepdims=True)\n        x_normf = (x - mu_f) / np.sqrt(var_f + 1e-5)\n        x_normf = x_normf * ln_f[\"g\"] + ln_f[\"b\"]  # (seq_len, d_model)\n\n        # Take the last token representation, project to vocab logits\n        last_hidden = x_normf[-1]             # (d_model,)\n        logits = last_hidden @ wte.T          # (vocab_size,)\n\n        # Softmax to probabilities\n        probs = np.exp(logits - np.max(logits))\n        probs = probs / probs.sum()\n\n        # Greedy pick\n        next_token = int(np.argmax(probs))\n        past_tokens.append(next_token)\n\n    # Decode full sequence (prompt + generated)\n    return encoder.decode(past_tokens)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    out = gen_text(\"hello world\", n_tokens_to_generate=10)\n    print(\"Generated:\", out)\n```"}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(x):\n        # Numerically stable softmax\n        exps = np.exp(x - np.max(x))\n        return exps / np.sum(exps)\n\n    # 1) Build simple embeddings by repeating each scalar value `dimension` times\n    #    embeddings shape: (n, dimension)\n    vals = np.array(crystal_values, dtype=float).reshape(n, 1)\n    embeddings = np.repeat(vals, dimension, axis=1)\n\n    # 2) Compute pairwise \u201cattention scores\u201d via dot-product\n    #    scores[i, j] = embeddings[i] \u00b7 embeddings[j]\n    scores = embeddings.dot(embeddings.T)  # shape: (n, n)\n\n    # 3) For each crystal i, compute softmax over its scores and\n    #    form the weighted sum of all embeddings\n    output = []\n    for i in range(n):\n        attn_weights = softmax(scores[i])         # shape: (n,)\n        weighted_emb = attn_weights.dot(embeddings)  # shape: (dimension,)\n        # 4) Collapse to a single float by summing over dimensions\n        pattern_value = float(np.sum(weighted_emb))\n        output.append(round(pattern_value, 4))\n\n    return output\n\n# Example usage:\nif __name__ == \"__main__\":\n    n = 4\n    crystal_values = [1.0, 3.5, -2.0, 0.5]\n    dimension = 5\n    patterns = pattern_weaver(n, crystal_values, dimension)\n    print(patterns)\n```\n\nExplanation of steps:\n\n1. **Softmax**  \n   We subtract the maximum of the vector before exponentiating to ensure numerical stability.\n\n2. **Embeddings**  \n   We create a simple embedding for each crystal by repeating its scalar value `dimension` times. This yields an (n \u00d7 dimension) matrix.\n\n3. **Attention scores**  \n   We compute a dot-product score between every pair of embeddings. This gives us an (n \u00d7 n) score matrix.\n\n4. **Weighted patterns**  \n   For each crystal index `i`, we:\n   - Apply softmax to row `i` of the score matrix to get attention weights.\n   - Take a weighted sum of all embeddings.\n   - Sum over the embedding dimensions to a single float.\n   - Round to the 4th decimal place.\n\nThe function returns a list of `n` floats, each rounded to four decimals."}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n       Compute BM25 scores for each document in the corpus relative to the query.\n    Args:\n      corpus: List of documents (each document is a string).\n      query:  Query string.\n      k1:     Term-frequency saturation parameter.\n      b:      Length-normalization parameter.\n    Returns:\n      List of BM25 scores (one per document), each rounded to 3 decimal places.\n       # 1. Tokenize documents and query (simple whitespace tokenizer, lowercase)\n    docs_tokens = [doc.lower().split() for doc in corpus]\n    query_terms = query.lower().split()\n    \n    # 2. Precompute document lengths and average document length\n    doc_lengths = np.array([len(tokens) for tokens in docs_tokens], dtype=float)\n    avgdl = np.mean(doc_lengths) if len(doc_lengths) > 0 else 0.0\n    \n    # 3. Build term\u2010frequency counters for each document\n    doc_counters = [Counter(tokens) for tokens in docs_tokens]\n    \n    # 4. For each unique query term, compute document frequency (in how many docs it appears)\n    N = len(corpus)\n    unique_query_terms = set(query_terms)\n    df = {}\n    for term in unique_query_terms:\n        df[term] = sum(1 for tokens in docs_tokens if term in tokens)\n    \n    # 5. Compute IDF for each query term using standard BM25 formula\n    #    IDF(term) = log( (N - df + 0.5)/(df + 0.5) + 1 )\n    idf = {}\n    for term, freq in df.items():\n        idf_term = np.log((N - freq + 0.5) / (freq + 0.5) + 1)\n        idf[term] = idf_term\n    \n    # 6. Initialize score vector\n    scores = np.zeros(N, dtype=float)\n    \n    # 7. Accumulate BM25 score contributions for each term\n    for term in unique_query_terms:\n        if idf[term] <= 0:\n            continue  # no contribution\n        # term frequencies in each doc\n        f_qi_D = np.array([cnt[term] for cnt in doc_counters], dtype=float)\n        \n        # BM25 term\u2010frequency component\n        numer = f_qi_D * (k1 + 1)\n        denom = f_qi_D + k1 * (1 - b + b * doc_lengths / avgdl)\n        tf_component = numer / denom\n        \n        # add to scores, weighted by IDF\n        scores += idf[term] * tf_component\n    \n    # 8. Round each score to 3 decimal places and return as list\n    return [round(float(s), 3) for s in scores]\n```\n\nExplanation of key steps:\n1. **Tokenization**: Lower\u2010case and split on whitespace for both documents and query.  \n2. **Document statistics**: Compute each document\u2019s length and the average document length (`avgdl`).  \n3. **Document frequencies**: For each unique term in the query, count how many documents contain it (`df`).  \n4. **Inverse document frequency** (`idf`): Using the BM25 formula  \n5. **Term\u2010frequency normalization**: For each term, compute  \n      (`f_qi_D * (k1 + 1)`)  \n      /  \n      (`f_qi_D + k1 * (1 - b + b * |D|/avgdl)`)  \n   and multiply by the term\u2019s `idf`.  \n6. **Score accumulation**: Sum contributions across all query terms for each document.  \n7. **Output**: Round each document\u2019s BM25 score to three decimal places."}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n       Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n       # Basic validation\n    if len(y_true) != len(y_pred):\n        raise ValueErrorThe length of y_true and y_pred must be the same    \n    # Here we assume a binary classification with1 as the positive class.\n    POSITIVE = 1\n    \n    # Count true positives (TP), false positives (FP), and false negatives (FN)\n    tp = fp = fn = 0\n    for true_label, pred_label in zip(y_true, y_pred):\n        if pred_label == POSITIVE:\n            if true_label == POSITIVE:\n                tp += 1\n            else:\n                fp += 1\n        else:  # pred_label != POSITIVE\n            if true_label == POSITIVE:\n                fn += 1\n    \n    # Precision: TP / (TP + FP)\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    # Recall: TP / (TP + FN)\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    # F1 score: 2 * (precision * recall) / (precision + recall)\n    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    # Round to 3 decimal places\n    return round(f1, 3)\n\n\n# Example usage:\nif __name__ ==main    y_true = [1, 0, 1, 1, 0, 1, 0, 0]\n    y_pred = [1, 0, 0, 1, 0, 1, 1, 0]\n    printF1 score calculate_f1_score(y_true, y_pred))  # Output: F1 score: 0.727\n```\n\nExplanation:\n1. We loop through paired true and predicted labels to count TP, FP, and FN.\n2. We compute precision and recall using standard formulas, guarding against zero division.\n3. We compute the F1 score as the harmonic mean of precision and recall.\n4. Finally, we round the F1 score to three decimal places and return it."}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n       consumption_data: list of 10 daily total power consumptions (days 1 through 10)\n    Returns an integer forecast for day 15 including fluctuation and a 5% safety margin.\n       n = len(consumption_data)\n    if n != 10:\n        raise ValueErrorExpected 10 days of data    \n    # 1) Compute detrended values: y_i = observed_i - fluctuation_i\n    xs = list(range(1, n+1))  # days 1..10\n    ys = []\n    for i, obs in zip(xs, consumption_data):\n        fluct = 10 * math.sin(2 * PI * i / 10)\n        ys.append(obs - fluct)\n    \n    # 2) Perform simple linear regression on (x, y)\n    sum_x  = sum(xs)\n    sum_y  = sum(ys)\n    sum_x2 = sum(x*x for x in xs)\n    sum_xy = sum(x*y for x, y in zip(xs, ys))\n    \n    # slope (m) and intercept (b)\n    m = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)\n    b = (sum_y - m * sum_x) / n\n    \n    # 3) Predict day 15's base consumption\n    day15 = 15\n    base_pred = m * day15 + b\n    \n    # 4) Add day 15 fluctuation back\n    fluct15 = 10 * math.sin(2 * PI * day15 / 10)\n    total15 = base_pred + fluct15\n    \n    # 5) Round to nearest integer\n    rounded = round(total15)\n    \n    # 6) Add a 5% safety margin (rounded up)\n    margin = math.ceil(0.05 * rounded)\n    final_forecast = rounded + margin\n    \n    return final_forecast\n```\n\nExplanation of the steps:\n1. We subtract the known sine\u2010wave fluctuation from each of the first 10 data points.\n2. We compute the least\u2010squares slope and intercept for the detrended data.\n3. We use that line to predict the \u201cbase\u201d (trend\u2010only) consumption on day 15.\n4. We add back the sinusoidal fluctuation for day 15.\n5. We round to the nearest integer.\n6. We compute a 5% safety buffer (using `math.ceil` to round it up) and add it to the rounded forecast."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Check that the shapes match\n    if y_true.shape != y_pred.shape:\n        raise ValueError(f\"Shape mismatch: y_true has shape {y_true.shape}, \"\n                         f\"but y_pred has shape {y_pred.shape}\")\n\n    # Compute absolute errors and mean\n    abs_errors = np.abs(y_true - y_pred)\n    mean_abs_error = np.mean(abs_errors)\n\n    # Return rounded to 3 decimal places\n    return float(np.round(mean_abs_error, 3))\n```\n\nExample usage:\n```python\nif __name__ == \"__main__\":\n    y_true = np.array([3.0, -0.5, 2.0, 7.0])\n    y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n    print(mae(y_true, y_pred))  # Output: 0.5\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the query, key, and value matrices.\n    Arguments:\n      X   : (m, n) input sequence\n      W_q : (n, n) weights for query\n      W_k : (n, n) weights for key\n      W_v : (n, n) weights for value\n    Returns:\n      Q, K, V each of shape (m, n)\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Scaled dot-product self-attention for one head.\n    Arguments:\n      Q : (m, d) queries\n      K : (m, d) keys\n      V : (m, d) values\n    Returns:\n      out : (m, d) attended output\n    \"\"\"\n    m, d = Q.shape\n    # Compute scaled scores\n    scores = (Q @ K.T) / np.sqrt(d)\n    # Numeric stability: subtract max per row\n    scores = scores - np.max(scores, axis=1, keepdims=True)\n    # Softmax\n    exp_scores = np.exp(scores)\n    attn = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    # Weighted sum of values\n    out = attn @ V\n    return out\n\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Multi-head attention.\n    Arguments:\n      Q      : (m, n) queries\n      K      : (m, n) keys\n      V      : (m, n) values\n      n_heads: number of attention heads\n    Returns:\n      output: (m, n) concatenated multi-head output\n    \"\"\"\n    m, n = Q.shape\n    if n % n_heads != 0:\n        raise ValueError(f\"Embedding dimension n={n} not divisible by n_heads={n_heads}\")\n    d_k = n // n_heads\n    \n    # Split Q, K, V into heads\n    # After reshape: (n_heads, m, d_k)\n    Qh = Q.reshape(m, n_heads, d_k).transpose(1, 0, 2)\n    Kh = K.reshape(m, n_heads, d_k).transpose(1, 0, 2)\n    Vh = V.reshape(m, n_heads, d_k).transpose(1, 0, 2)\n    \n    # Compute attention per head\n    heads_out = []\n    for i in range(n_heads):\n        out_i = self_attention(Qh[i], Kh[i], Vh[i])  # (m, d_k)\n        heads_out.append(out_i)\n    \n    # Stack and concat heads: heads_out is list of (m, d_k)\n    # After stack: (n_heads, m, d_k) -> transpose -> (m, n_heads, d_k) -> reshape -> (m, n)\n    heads_stacked = np.stack(heads_out, axis=0)\n    heads_stacked = heads_stacked.transpose(1, 0, 2)\n    concat = heads_stacked.reshape(m, n)\n    return concat\n\ndef entrypoint(m, n, n_heads, random_seed):\n    np.random.seed(random_seed)\n    # Generate input data\n    X = np.arange(m*n).reshape(m,n)\n    X = np.random.permutation(X.flatten()).reshape(m, n)\n    # Generate weight matrices\n    W_q = np.random.randint(0, 4, size=(n,n))\n    W_k = np.random.randint(0, 5, size=(n,n))\n    W_v = np.random.randint(0, 6, size=(n,n))\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    return np.round(multi_head_attention(Q, K, V, n_heads), 4).tolist()\n```\n\nExplanation of the key parts:\n\n1. **compute_qkv**  \n   Simply multiplies the input `X` by the query/key/value weight matrices to produce `Q, K, V`.\n\n2. **self_attention**  \n   - Computes the raw attention scores `Q @ K.T`.  \n   - Scales by \u221a(d_k) for stability.  \n   - Applies a row-wise stable softmax.  \n   - Uses the attention weights to blend the value vectors `V`.\n\n3. **multi_head_attention**  \n   - Checks `n` divisible by `n_heads` and computes each head dimension `d_k`.  \n   - Reshapes and transposes `Q, K, V` into separate heads.  \n   - Runs `self_attention` on each head independently.  \n   - Concatenates the per-head outputs back into shape `(m, n)`.\n\nYou can now call `entrypoint(m, n, n_heads, random_seed)` to get your multi-head attention output."}
{"task_id": 95, "completion_id": 0, "completion": "```python\nimport math\n\ndef phi_corr(x: list[int], y: list[int]) -> float:\n       Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n       if len(x) != len(y):\n        raise ValueErrorInput lists must have the same length    \n    # Initialize contingency counts\n    n11 = n10 = n01 = n00 = 0\n    \n    for xi, yi in zip(x, y):\n        if xi not in (0, 1) or yi not in (0, 1):\n            raise ValueErrorAll elements must be 0 or 1        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        else:  # xi == 0 and yi == 0\n            n00 += 1\n    \n    # Marginal sums\n    n1_ = n11 + n10  # total x = 1\n    n0_ = n01 + n00  # total x = 0\n    n_1 = n11 + n01  # total y = 1\n    n_0 = n10 + n00  # total y = 0\n\n    # Denominator for Phi\n    denom = math.sqrt(n1_ * n0_ * n_1 * n_0)\n    if denom == 0:\n        # No variation in at least one variable => correlation undefined\n        return 0.0\n\n    phi = (n11 * n00 - n10 * n01) / denom\n    return round(phi, 4)\n```\n\nExample usage:\n\n```python\nif __name__ ==main    a = [1, 0, 1, 1, 0, 0, 1, 0]\n    b = [0, 1, 1, 0, 0, 1, 1, 0]\n    print(phi_corr(a, b))  # e.g., 0.0xxx\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n       Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input, defined as:\n               hard_sigmoid(x) = max(0.0, min(1.0, 0.2 * x + 0.5))\n       # Compute the linear component\n    y = 0.2 * x + 0.5\n    # Clip to [0, 1]\n    if y <= 0.0:\n        return 0.0\n    elif y >= 1.0:\n        return 1.0\n    else:\n        return y\n\n\n# Example usage and simple tests\nif __name__ ==main    test_values = [-10.0, -3.0, -2.5, 0.0, 2.0, 2.5, 10.0]\n    for val in test_values:\n        print(fhard_sigmoid({val}) = {hard_sigmoid(val)}```\n\nExplanation:\n- We first compute the linear approximation `y = 0.2 * x + 0.5`.\n- We then clamp `y` to the interval [0.0, 1.0] using simple comparisons for maximum efficiency.\n- Inputs less than or equal to -2.5 map to 0.0, inputs greater than or equal to 2.5 map to 1.0, and inputs in between follow the line."}
{"task_id": 97, "completion_id": 0, "completion": "import math\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x\n\n\n# Example usage and simple tests\nif __name__ == \"__main__\":\n    inputs = [-5.0, -1.0, 0.0, 1.0, 5.0]\n    alphas = [0.01, 0.1, 0.25]\n\n    for a in alphas:\n        print(f\"alpha = {a}\")\n        for x in inputs:\n            y = prelu(x, alpha=a)\n            print(f\" prelu({x:.1f}) = {y:.4f}\")\n        print()\n```\n\nExplanation:\n- If the input `x` is non-negative, PReLU returns `x` (same as ReLU).\n- If `x` is negative, it returns `alpha * x`, where `alpha` is a small, learnable slope parameter.\n- The example block at the bottom runs the function on a few test inputs with different `alpha` values."}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function in a numerically stable way.\n\n    Softplus(x) = log(1 + e^x)\n    We rewrite it as:\n      softplus(x) = max(x, 0) + log1p(exp(-abs(x)))\n    to avoid overflow when x is large and underflow when x is very negative.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value rounded to 4 decimal places.\n    \"\"\"\n    # stable computation\n    res = max(x, 0.0) + math.log1p(math.exp(-abs(x)))\n    return round(res, 4)\n```\n\nExplanation of the key trick:\n- For large x \u226b 0, exp(x) overflows, but exp(-abs(x)) = exp(\u2013x) \u2192 0, so `max(x,0) + log1p(exp(-abs(x))) \u2248 x + 0`.\n- For very negative x \u226a 0, exp(\u2013abs(x)) = exp(x) \u2192 0, so the result \u2248 0 + exp(x), which is the correct tiny value.\n- `math.log1p(y)` computes `log(1 + y)` accurately even when y is very small."}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n       Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n       result = x / (1 + abs(x))\n    return round(result, 4)\n```\n\nExplanation:\n1. The Softsign function is defined as x / (1 + |x|).\n2. We compute `result = x / (1 + abs(x))`.\n3. We then round the result to the 4th decimal place with `round(result, 4)`."}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value (rounded to 6 decimal places).\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized ops\n    rhos = np.array(rhos, dtype=np.float64)\n    A = np.array(A, dtype=np.float64)\n    pi_old = np.array(pi_theta_old, dtype=np.float64)\n    pi_ref = np.array(pi_theta_ref, dtype=np.float64)\n\n    # 1) Compute the clipped surrogate term (PPO\u2010style)\n    clipped_rhos = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon)\n    surrogate_unclipped = rhos * A\n    surrogate_clipped = clipped_rhos * A\n    # PPO objective takes the elementwise minimum:\n    surrogate_terms = np.minimum(surrogate_unclipped, surrogate_clipped)\n    surrogate = np.mean(surrogate_terms)\n\n    # 2) Compute the KL divergence between old policy and reference policy\n    #    KL(\u03c0_old || \u03c0_ref) = sum \u03c0_old * (log \u03c0_old - log \u03c0_ref)\n    #    add a small epsilon inside log to avoid numerical issues\n    tiny = 1e-12\n    kl_div = np.sum(pi_old * (np.log(pi_old + tiny) - np.log(pi_ref + tiny)))\n\n    # 3) Combine with penalty\n    obj = surrogate - beta * kl_div\n\n    # 4) Round to 6 decimal places\n    return float(round(obj, 6))\n```\n\nExplanation of steps:\n\n1. **Surrogate term**  \n   We compute two versions of the policy\u2010ratio multiplied by advantage:  \n   - `rhos * A` (unclipped)  \n   - `clip(rhos, 1-\u03b5, 1+\u03b5) * A` (clipped)  \n   Then we take the element\u2010wise minimum, as in PPO, and average over all samples.\n\n2. **KL penalty**  \n   We compute the KL divergence between the old policy distribution and the reference policy:  \n   `KL = \u03a3 \u03c0_old * (log \u03c0_old - log \u03c0_ref)`.  \n\n3. **Objective**  \n   The final GRPO objective is the surrogate minus `\u03b2` times the KL divergence.  \n\n4. **Rounding**  \n   The result is rounded to 6 decimal places before returning."}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal.\n    \"\"\"\n    # Compute sigmoid(x)\n    sigmoid = 1 / (1 + math.exp(-x))\n    # Compute swish = x * sigmoid(x)\n    swish_val = x * sigmoid\n    # Round to 4 decimal places\n    return round(swish_val, 4)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    inputs = [-3.0, -1.0, 0.0, 1.0, 3.0]\n    for v in inputs:\n        print(f\"swish({v}) = {swish(v)}\")\n```\n\nExplanation:\n1. We compute the sigmoid of `x` using the standard definition 1 / (1 + e\u207b\u02e3).\n2. We multiply the input `x` by its sigmoid to get the Swish activation.\n3. We round the result to the nearest fourth decimal place before returning."}
{"task_id": 103, "completion_id": 0, "completion": "import math\nimport numpy as np\n\ndef selu(x: float) -> float:\n       Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value (rounded to 4 decimal places)\n       alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    # For numerical stability, math.exp handles large negative inputs by underflowing to 0.\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * (alpha * (math.exp(x) - 1.0))\n\n    # Round to the nearest 4th decimal place\n    return float(round(result, 4))\n\n\n# Example usage\nif __name__ ==main    test_values = [-3.0, -1.0, 0.0, 0.5, 2.0]\n    for v in test_values:\n        print(fselu({v}) = {selu(v)}"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n       Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1) as a Python list\n       # Linear combination\n    z = np.dot(X, weights) + bias\n\n    # Sigmoid to get probabilities\n    probs = 1 / (1 + np.exp(-z))\n\n    # Threshold at 0.5 to get binary predictions\n    preds = (probs >= 0.5).astype(int)\n\n    return preds.tolist()\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray,\n                     y: np.ndarray,\n                     learning_rate: float,\n                     iterations: int) -> tuple[list[float], list[float]]:\n       Gradient-descent training algorithm for Softmax regression,\n    optimizing parameters with Cross Entropy loss.\n\n    Args:\n        X: Input data, shape (N, M)\n        y: Integer class labels, shape (N,), values in {0,...,C-1}\n        learning_rate: Step size for gradient descent\n        iterations: Number of GD steps\n\n    Returns:\n        B : list of list of floats, shape (C, M),\n            final weight matrix rounded to 4 decimals\n        losses : list of floats,\n            cross\u2010entropy loss at each iteration rounded to 4 decimals\n       N, M = X.shape\n    # number of classes\n    C = int(np.max(y)) + 1\n\n    # initialize weights: shape (C, M)\n    W = np.zeros((C, M), dtype=float)\n\n    # one\u2010hot encode y: shape (N, C)\n    Y = np.zeros((N, C), dtype=float)\n    Y[np.arange(N), y] = 1.0\n\n    eps = 1e-15\n    losses: list[float] = []\n\n    for it in range(iterations):\n        # compute scores: shape (N, C)\n        scores = X.dot(W.T)\n\n        # numeric stability\n        scores -= np.max(scores, axis=1, keepdims=True)\n\n        # softmax probabilities: shape (N, C)\n        exp_s = np.exp(scores)\n        P = exp_s / np.sum(exp_s, axis=1, keepdims=True)\n\n        # cross\u2010entropy loss\n        loss = -np.sum(Y * np.log(P + eps)) / N\n        losses.append(round(loss, 4))\n\n        # gradient: shape (C, M)\n        grad = (P - Y).T.dot(X) / N\n\n        # gradient descent update\n        W -= learning_rate * grad\n\n    # round final weights\n    B_rounded = np.round(W, 4).tolist()\n\n    return B_rounded, losses\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray,\n                 y: np.ndarray,\n                 learning_rate: float,\n                 iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression,\n    optimizing parameters with Binary Cross Entropy loss.\n\n    Parameters\n    ----------\n    X : np.ndarray of shape (m, n)\n        Feature matrix (without bias term).\n    y : np.ndarray of shape (m,)\n        Binary labels (0 or 1).\n    learning_rate : float\n        Step size for gradient descent.\n    iterations : int\n        Number of gradient descent steps.\n\n    Returns\n    -------\n    weights_list : list of float\n        Learned parameters [bias, w1, w2, ..., wn], rounded to 4 decimals.\n    losses_list : list of float\n        Binary cross-entropy loss at each iteration, rounded to 4 decimals.\n    \"\"\"\n    m, n = X.shape\n    # Add bias term\n    X_bias = np.hstack((np.ones((m, 1)), X))        # shape = (m, n+1)\n    # Initialize parameters\n    w = np.zeros(n + 1)                             # shape = (n+1,)\n    losses = []\n\n    for _ in range(iterations):\n        # Linear combination\n        z = X_bias @ w                               # shape = (m,)\n        # Sigmoid prediction\n        p = 1 / (1 + np.exp(-z))\n        # Avoid log(0)\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        # Compute binary cross entropy loss\n        loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n        losses.append(loss)\n        # Gradient of loss wrt w\n        grad = (X_bias.T @ (p - y)) / m              # shape = (n+1,)\n        # Parameter update\n        w -= learning_rate * grad\n\n    # Round parameters and losses to 4 decimal places and convert to Python lists\n    weights_list = np.round(w, 4).tolist()\n    losses_list = np.round(np.array(losses), 4).tolist()\n\n    return weights_list, losses_list\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    X: (seq_len, d_model)\n    W_q, W_k, W_v: (d_model, d_k), (d_model, d_k), (d_model, d_v)\n    Returns:\n      Q: (seq_len, d_k)\n      K: (seq_len, d_k)\n      V: (seq_len, d_v)\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> list:\n    \"\"\"\n    Compute masked self-attention.\n    Q: (seq_len, d_k)\n    K: (seq_len, d_k)\n    V: (seq_len, d_v)\n    mask: (seq_len, seq_len) array of 1s and 0s,\n          where mask[i,j]==1 means position j is visible to i,\n          and 0 means it is masked out.\n\n    Returns:\n      A Python list of shape (seq_len, d_v) containing the attention output.\n    \"\"\"\n    # Dimensions\n    seq_len, d_k = Q.shape\n\n    # 1) Compute raw attention scores\n    #    scores[i,j] = Q[i] \u00b7 K[j] / sqrt(d_k)\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n\n    # 2) Apply the mask: set masked positions to a large negative value\n    #    so their softmax becomes ~0\n    #    We assume mask is 1 where allowed, 0 where masked.\n    NEG_INF = -1e9\n    scores = np.where(mask == 1, scores, NEG_INF)\n\n    # 3) Softmax over the last axis (over keys for each query)\n    #    For numerical stability, subtract the max before exponentiating.\n    max_per_row = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - max_per_row)\n    sum_exp = np.sum(exp_scores, axis=-1, keepdims=True)\n    attn_weights = exp_scores / sum_exp  # shape: (seq_len, seq_len)\n\n    # 4) Weighted sum of values\n    #    output[i] = sum_j attn_weights[i,j] * V[j]\n    output = np.dot(attn_weights, V)  # shape: (seq_len, d_v)\n\n    # Return as a Python list\n    return output.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # toy inputs\n    seq_len, d_model, d_k, d_v = 5, 8, 8, 8\n    X = np.random.randn(seq_len, d_model)\n    W_q = np.random.randn(d_model, d_k)\n    W_k = np.random.randn(d_model, d_k)\n    W_v = np.random.randn(d_model, d_v)\n\n    # causal mask: allow j <= i only\n    causal_mask = np.tril(np.ones((seq_len, seq_len), dtype=np.int32))\n\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    out = masked_attention(Q, K, V, causal_mask)\n    print(out)  # Python list of shape [seq_len][d_v]\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    We use Shannon entropy (in bits), which is 0 if all apples are the same color,\n    and increases as the distribution of colors becomes more uniform.\n    The result is rounded to the nearest 4th decimal place.\n    \"\"\"\n    n = len(apples)\n    if n == 0:\n        return 0.0\n\n    freq = Counter(apples)\n    entropy = 0.0\n    for count in freq.values():\n        p = count / n\n        entropy -= p * math.log2(p)\n\n    return round(entropy, 4)\n\n\n# Example usages / sanity checks:\nif __name__ == \"__main__\":\n    tests = [\n        ([0,0,0,0],      0.0),\n        ([1,1,0,0],      1.0),\n        ([0,1,2,3],      2.0),\n        ([0,0,1,1,2,2,3,3],   2.0),\n        ([0,0,0,0,0,1,2,3],   round(\n            - (5/8)*math.log2(5/8)\n            - 3*(1/8)*math.log2(1/8),\n            4\n        )),\n    ]\n    for apples, expected in tests:\n        got = disorder(apples)\n        print(f\"{apples} -> disorder = {got} (expected \u2248 {expected})\")\n```\n\nExplanation of choice:\n- Shannon entropy H = \u2013\u2211 p\u1d62 log\u2082(p\u1d62) is zero when there\u2019s only one color (p=1), \n  grows to 1 bit for an even 2\u2010color split, to 2 bits for an even 4\u2010color split, etc.\n- It naturally orders baskets by \u201cvariety\u201d and \u201cevenness\u201d of the distribution.\n- We round the final result to 4 decimal places as required."}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray,\n                        gamma: np.ndarray,\n                        beta: np.ndarray,\n                        epsilon: float = 1e-5):\n    \"\"\"\n    Applies Layer Normalization over the last dimension of a 3D tensor.\n\n    Parameters:\n    - X: Input array of shape (batch_size, seq_length, feature_dim)\n    - gamma: Scale parameters of shape (feature_dim,)\n    - beta: Shift parameters of shape (feature_dim,)\n    - epsilon: Small constant for numerical stability\n\n    Returns:\n    - A Python list corresponding to the layer-normalized tensor,\n      rounded to 5 decimal places.\n    \"\"\"\n    # Compute mean and variance across the feature dimension\n    mean = np.mean(X, axis=-1, keepdims=True)          # shape: (B, S, 1)\n    var = np.mean((X - mean) ** 2, axis=-1, keepdims=True)  # shape: (B, S, 1)\n\n    # Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)       # shape: (B, S, F)\n\n    # Scale and shift\n    # gamma and beta are (F,), broadcast to (B, S, F)\n    out = gamma * X_norm + beta\n\n    # Round to 5 decimal places and convert to list\n    return np.round(out, 5).tolist()\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score between a reference and a candidate translation.\n    \n    Args:\n        reference (str): The reference (ground truth) translation.\n        candidate (str): The candidate (machine) translation.\n        alpha (float): Weighting of precision vs. recall in the F-mean.\n        beta (float): Exponent in the fragmentation penalty.\n        gamma (float): Weight of the penalty.\n        \n    Returns:\n        float: METEOR score rounded to 3 decimal places.\n    \"\"\"\n    # 1. Tokenize (lowercased whitespace split)\n    ref_tokens = reference.lower().split()\n    cand_tokens = candidate.lower().split()\n    ref_len = len(ref_tokens)\n    cand_len = len(cand_tokens)\n    \n    # 2. Count unigram matches (multiset intersection)\n    ref_counts = Counter(ref_tokens)\n    cand_counts = Counter(cand_tokens)\n    matches = sum(min(ref_counts[w], cand_counts[w]) for w in ref_counts)\n    if matches == 0:\n        return 0.000\n\n    # 3. Build index lists for alignment to detect chunks\n    idxs = {}\n    for i, w in enumerate(ref_tokens):\n        idxs.setdefault(w, []).append(i)\n    \n    # Greedy align: for each candidate token, take the earliest unused ref index\n    aligned = []\n    for j, w in enumerate(cand_tokens):\n        if w in idxs and idxs[w]:\n            i = idxs[w].pop(0)\n            aligned.append((i, j))\n    # aligned is already sorted by candidate index j\n    \n    # 4. Count chunks (contiguous runs in both ref and cand)\n    chunks = 1\n    for k in range(1, len(aligned)):\n        prev_i, prev_j = aligned[k-1]\n        cur_i, cur_j     = aligned[k]\n        # if either ref or cand jump discontinuously, new chunk\n        if not (cur_i == prev_i + 1 and cur_j == prev_j + 1):\n            chunks += 1\n    \n    # 5. Precision & Recall\n    precision = matches / cand_len\n    recall    = matches / ref_len\n    \n    # 6. F-mean\n    f_mean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n    \n    # 7. Penalty for fragmentation\n    penalty = gamma * ( (chunks / matches) ** beta )\n    \n    # 8. Final METEOR score\n    score = f_mean * (1 - penalty)\n    return round(score, 3)\n```\n\nExplanation of key steps:\n\n1. **Tokenization**: lowercase and split on whitespace.  \n2. **Unigram Matching**: count overlaps via Counters to get `matches`.  \n3. **Alignment & Chunking**: align each matched candidate word to the earliest unused occurrence in the reference to get `(ref_idx, cand_idx)` pairs, then count how many contiguous \u201cchunks\u201d these matches form.  \n4. **Precision & Recall**:  \n   Precision = matches / len(candidate)  \n   Recall    = matches / len(reference)  \n5. **F-mean**: weighted harmonic mean controlled by `alpha`.  \n6. **Penalty**: penalizes disordered matches based on chunk fragmentation (`beta`) and weight (`gamma`).  \n7. **Score**: `F-mean * (1 \u2013 penalty)`, rounded to three decimals."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n       Compute the Pointwise Mutual Information (PMI) for two events X and Y.\n    \n    PMI(X,Y) = log2 [ P(X,Y) / (P(X) * P(Y)) ]\n    \n    Arguments:\n    joint_counts      -- count of co\u2010occurrences of X and Y\n    total_counts_x    -- total count of event X\n    total_counts_y    -- total count of event Y\n    total_samples     -- total number of samples (N)\n    \n    Returns:\n    PMI value rounded to 3 decimal places. If joint_counts == 0, returns -inf.\n    \n    Raises:\n    ValueError if any of the marginals or total_samples are non\u2010positive.\n       # Basic sanity checks\n    if total_samples <= 0:\n        raise ValueErrortotal_samples must be positive    if total_counts_x <= 0:\n        raise ValueErrortotal_counts_x must be positive    if total_counts_y <= 0:\n        raise ValueErrortotal_counts_y must be positive    \n    # Empirical probabilities\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n\n    # If the joint never occurs, PMI \u2192 -inf\n    if p_xy == 0:\n        return float('-inf')\n    \n    # Compute PMI in log2\n    pmi = np.log2(p_xy / (p_x * p_y))\n    \n    # Round and return\n    return round(float(pmi), 3)\n```\n\nExplanation of key steps:\n1. Convert counts to probabilities by dividing by `total_samples`.\n2. Handle edge cases (zero counts) explicitly.\n3. Compute PMI = log2(P(X,Y)/(P(X) P(Y))).\n4. Round the result to three decimal places before returning."}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max normalization on a list of integers,\n    scaling each value to the range [0, 1], and round to 4 decimal places.\n    If all values are identical, returns a list of 0.0s.\n    \"\"\"\n    if not x:\n        return []\n    min_x = min(x)\n    max_x = max(x)\n    range_x = max_x - min_x\n    if range_x == 0:\n        # Avoid division by zero: all values are the same\n        return [0.0 for _ in x]\n    normalized = [(val - min_x) / range_x for val in x]\n    # Round each value to 4 decimal places\n    return [round(v, 4) for v in normalized]\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    A simple 1D residual block:\n      1) Linear layer: x @ w1\n      2) ReLU activation\n      3) Linear layer: (previous) @ w2\n      4) Add the original input x (shortcut)\n      5) Final ReLU\n      6) Round to 4 decimals and convert to list\n\n    Shapes:\n      - x: (D,)\n      - w1: (D, H)\n      - w2: (H, D)\n    Returns:\n      - output as a Python list of length D\n    \"\"\"\n    # Check shapes for compatibility\n    D = x.shape[0]\n    assert w1.shape[0] == D, f\"w1 first dim must be {D}, got {w1.shape[0]}\"\n    H = w1.shape[1]\n    assert w2.shape == (H, D), f\"w2 must be shape ({H}, {D}), got {w2.shape}\"\n\n    # Define ReLU\n    def relu(z: np.ndarray) -> np.ndarray:\n        return np.maximum(0, z)\n\n    # First linear + ReLU\n    hidden = relu(x.dot(w1))      # shape (H,)\n\n    # Second linear\n    transformed = hidden.dot(w2)  # shape (D,)\n\n    # Add shortcut and final ReLU\n    out = relu(transformed + x)   # shape (D,)\n\n    # Round to 4 decimal places and convert to list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D feature map.\n\n    Parameters:\n    ----------\n    x : np.ndarray\n        Input array of shape (height, width, channels).\n\n    Returns:\n    -------\n    np.ndarray\n        1D array of shape (channels,), where each element is the average of\n        the corresponding height\u00d7width feature map.\n    \"\"\"\n    # Validate input dimensions\n    if not isinstance(x, np.ndarray):\n        raise TypeError(f\"Expected input type np.ndarray but got {type(x)}\")\n    if x.ndim != 3:\n        raise ValueError(f\"Input must be a 3D array of shape (H, W, C), got shape {x.shape}\")\n\n    # Compute the mean over the spatial dimensions (height and width)\n    # The result will have shape (channels,)\n    return x.mean(axis=(0, 1))\n\n\n# Example usage / quick test\nif __name__ == \"__main__\":\n    # Create a dummy feature map of shape (2, 3, 4)\n    # For illustration, fill it with 0, 1, 2, ..., 2*3*4-1\n    feature_map = np.arange(2 * 3 * 4).reshape(2, 3, 4).astype(float)\n\n    # Perform global average pooling\n    pooled = global_avg_pool(feature_map)\n\n    print(\"Input shape:\", feature_map.shape)\n    print(\"Output shape:\", pooled.shape)\n    print(\"Pooled values:\", pooled)\n```\n\nExplanation:\n- We first check that the input `x` is indeed a NumPy array of rank 3.\n- We then call `x.mean(axis=(0, 1))`, which averages over the first two dimensions (height and width), leaving a 1D array of size equal to the number of channels.\n- An example in the `__main__` block demonstrates how you might test the function."}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray,\n                        gamma: np.ndarray,\n                        beta: np.ndarray,\n                        epsilon: float = 1e-5):\n    \"\"\"\n    Perform Batch Normalization on a 4D input (B, C, H, W).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input array of shape (batch, channels, height, width).\n    gamma : np.ndarray\n        Scale parameters, shape (channels,).\n    beta : np.ndarray\n        Shift parameters, shape (channels,).\n    epsilon : float\n        Small constant for numerical stability.\n\n    Returns\n    -------\n    List\n        The batch-normalized output rounded to 4 decimal places, as a nested list.\n    \"\"\"\n    # compute per\u2010channel mean and variance over (batch, height, width)\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)    # shape (1, C, 1, 1)\n    var  = np.var(X,  axis=(0, 2, 3), keepdims=True)    # shape (1, C, 1, 1)\n\n    # normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)        # shape (B, C, H, W)\n\n    # reshape gamma and beta to broadcast over B,H,W\n    gamma = gamma.reshape(1, -1, 1, 1)                  # shape (1, C, 1, 1)\n    beta  = beta.reshape(1, -1, 1, 1)                   # shape (1, C, 1, 1)\n\n    # scale and shift\n    out = gamma * X_norm + beta                         # shape (B, C, H, W)\n\n    # round and convert to nested lists\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of the term c * x^n at the point x using the power rule.\n    Power rule: d/dx [c * x^n] = c * n * x^(n - 1)\n\n    Args:\n        c (float): coefficient of the term\n        x (float): point at which to evaluate the derivative\n        n (float): exponent of x in the term\n\n    Returns:\n        float: the derivative value at x, rounded to 4 decimal places\n    \"\"\"\n    # Apply the power rule\n    derivative_value = c * n * (x ** (n - 1))\n\n    # Round to 4 decimal places and return\n    return round(derivative_value, 4)\n```\n\nExample usage:\n```python\nprint(poly_term_derivative(3.0, 2.0, 4.0))  # 96.0, since d/dx[3*x^4] at x=2 is 3*4*(2^3)=96\nprint(poly_term_derivative(5.5, -1.2, 2.5))  # Computes 5.5*2.5*((-1.2)**1.5) rounded to 4 decimals\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10) -> list[list[float]]:\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram\u2013Schmidt process.\n\n    Parameters:\n    - vectors: list of 2D vectors (each a list of 2 floats)\n    - tol: tolerance below which a vector is considered zero (default: 1e-10)\n\n    Returns:\n    - A list of orthonormal 2D vectors (each rounded to 4 decimal places and as a Python list)\n    \"\"\"\n    U = []  # Will hold the orthonormal basis vectors as NumPy arrays\n\n    for v in vectors:\n        w = np.array(v, dtype=float)\n\n        # Subtract projections onto each already computed basis vector\n        for u in U:\n            proj = np.dot(w, u) * u\n            w = w - proj\n\n        norm_w = np.linalg.norm(w)\n        if norm_w > tol:\n            # Normalize and add to the basis\n            u_new = w / norm_w\n            U.append(u_new)\n\n    # Round to 4 decimal places and convert each to a Python list\n    return [np.round(u, 4).tolist() for u in U]\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    vecs = [[1, 1], [1, 0], [2, 2]]\n    basis = orthonormal_basis(vecs)\n    print(basis)\n    # Possible output: [[0.7071, 0.7071], [0.7071, -0.7071]]\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors a and b.\n    The result is rounded to 4 decimal places and returned as a Python list.\n\n    Parameters:\n    a (array-like): 3-dimensional vector\n    b (array-like): 3-dimensional vector\n\n    Returns:\n    list: The cross product vector, rounded to 4 decimal places\n    \"\"\"\n    # Convert inputs to numpy arrays of float\n    a_arr = np.array(a, dtype=float)\n    b_arr = np.array(b, dtype=float)\n\n    # Ensure both are 3-dimensional\n    if a_arr.shape != (3,) or b_arr.shape != (3,):\n        raise ValueError(\"Both input vectors must be 3-dimensional.\")\n\n    # Compute cross product\n    result = np.cross(a_arr, b_arr)\n\n    # Round to 4 decimal places and convert to list\n    return np.round(result, 4).tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    v1 = [1, 2, 3]\n    v2 = [4, 5, 6]\n    print(cross_product(v1, v2))   # [-3.0, 6.0, -3.0]\n    print(cross_product([1,0,0], [0,1,0]))  # [0.0, 0.0, 1.0]\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve the linear system A x = b using Cramer's Rule.\n    Returns the solution vector x as a Python list rounded to 4 decimal places,\n    or -1 if the system has no unique solution (det(A) == 0).\n    \"\"\"\n    # Convert inputs to NumPy arrays of type float\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).flatten()\n\n    # Check that A is square and dimensions match b\n    n_rows, n_cols = A.shape\n    if n_rows != n_cols or b.size != n_rows:\n        raise ValueError(\"Matrix A must be square and b must have matching dimensions.\")\n\n    # Compute determinant of A\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-12:\n        # Determinant is zero (or numerically very close): no unique solution\n        return -1\n\n    # Solve for each variable using Cramer's Rule\n    x = np.zeros(n_rows, dtype=float)\n    for i in range(n_rows):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x[i] = det_Ai / det_A\n\n    # Round to 4 decimal places and convert to Python list\n    x = np.round(x, 4).tolist()\n    return x\n```\n\nExplanation of key steps:\n1. We convert `A` and `b` to NumPy arrays of type `float` and ensure `b` is a one\u2010dimensional array.\n2. We check that `A` is square and that `b` has the correct length.\n3. We compute `det(A)`. If it\u2019s (near) zero, we return `-1` to signal no unique solution.\n4. For each variable \\( x_i \\), we form \\( A_i \\) by replacing the \\( i \\)-th column of \\( A \\) with `b`, compute its determinant, and divide by `det(A)`.\n5. Finally, we round each component of the solution to 4 decimal places and return it as a native Python list."}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n       Calculate the Bhattacharyya distance between two discrete distributions p and q.\n    \n    Args:\n        p (list[float]): First probability distribution.\n        q (list[float]): Second probability distribution.\n        \n    Returns:\n        float: Bhattacharyya distance rounded to 4 decimal places.\n               Returns 0.0 if inputs are empty or lengths differ.\n               Returns math.inf if there is no overlap (BC coefficient is zero).\n       # Check for valid input\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n\n    # Convert to numpy arrays\n    p_arr = np.array(p, dtype=float)\n    q_arr = np.array(q, dtype=float)\n\n    # Compute Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p_arr * q_arr))\n\n    # If bc is zero (no overlap), distance is infinite\n    if bc <= 0.0:\n        return float('inf')\n\n    # Compute Bhattacharyya distance\n    distance = -np.log(bc)\n\n    # Round to 4 decimal places\n    return round(distance, 4)\n```\n\nExplanation of key steps:\n1. Input validation: Return `0.0` if the lists differ in length or are empty.  \n2. Conversion to NumPy arrays for elementwise operations.  \n3. Compute the Bhattacharyya coefficient BC = \u03a3 sqrt(p\u1d62\u00b7q\u1d62).  \n4. If BC \u2264 0 (no overlap), the distance is infinite.  \n5. Otherwise, distance = \u2212ln(BC), rounded to 4 decimal places."}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float] | int:\n       Computes the element-wise sum of two vectors a and b.\n    Returns a new vector of the same length if compatible,\n    otherwise returns -1.\n       if len(a) != len(b):\n        return -1\n\n    return [x + y for x, y in zip(a, b)]\n\n\n# Example usage:\nif __name__ ==main    print(vector_sum([1, 2, 3], [4, 5, 6]))    # [5, 7, 9]\n    print(vector_sum([1.5, 2.5], [3.0, -1.5])) # [4.5, 1.0]\n    print(vector_sum([1, 2], [1, 2, 3]))       # -1\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray,\n                            episodes: list[list[tuple[int, int, float]]]) -> list:\n    \"\"\"\n    Compute the REINFORCE policy gradient estimator.\n    \n    Args:\n        theta: array of shape (num_states, num_actions), policy parameters.\n        episodes: list of episodes; each episode is a list of (state, action, reward) tuples.\n    \n    Returns:\n        Gradient array of same shape as theta, rounded to 4 decimals, as a nested Python list.\n    \"\"\"\n    num_states, num_actions = theta.shape\n    # Compute softmax policy for each state\n    # Subtract max for numerical stability\n    shifted = theta - np.max(theta, axis=1, keepdims=True)\n    exp_vals = np.exp(shifted)\n    policy = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)  # shape (num_states, num_actions)\n    \n    # Initialize gradient accumulator\n    grad = np.zeros_like(theta, dtype=float)\n    num_episodes = len(episodes)\n    \n    for ep in episodes:\n        # Extract rewards\n        rewards = [r for (_, _, r) in ep]\n        T = len(rewards)\n        # Compute return G_t = sum_{k=t}^{T-1} r_k\n        returns = [sum(rewards[t:]) for t in range(T)]\n        \n        # Accumulate gradient from each time-step\n        for (t_step, (s, a, _)), G in zip(enumerate(ep), returns):\n            # \u2207_\u03b8 log \u03c0(a|s) = one_hot(a) - policy[s]\n            # So the update for this step is G * (one_hot(a) - policy[s])\n            grad[s] += G * (-policy[s])\n            grad[s, a] += G\n    \n    # Average over episodes\n    grad /= num_episodes\n    # Round to 4 decimal places\n    grad = np.round(grad, 4)\n    # Convert to nested Python list\n    return grad.tolist()\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Compute the FLOPs for a dense layer of equivalent total capacity vs.\n    a sparsely-gated MoE layer, and return the savings percentage.\n\n    Parameters:\n    - n_experts (int): total number of experts in the MoE\n    - k_active (int): number of experts selected/active per example\n    - d_in (int): input feature dimension\n    - d_out (int): output feature dimension\n\n    Returns:\n    A tuple of three floats (all rounded to 1 decimal place):\n    - dense_flops: FLOPs of a dense layer with capacity = n_experts * (d_in\u2192d_out)\n    - moe_flops: FLOPs of the MoE layer with k_active experts\n    - savings_pct: percentage FLOP savings of MoE vs. the dense layer\n    \"\"\"\n    # FLOPs for one linear layer: one multiply + one add per weight\n    flops_per_expert = 2 * d_in * d_out\n\n    # Dense layer at full MoE capacity (n_experts experts worth of params)\n    dense_flops = n_experts * flops_per_expert\n\n    # Only k_active experts are used in MoE\n    moe_flops = k_active * flops_per_expert\n\n    # Compute savings\n    savings_pct = (dense_flops - moe_flops) / dense_flops * 100.0\n\n    # Round to nearest 0.1\n    dense_flops = round(dense_flops, 1)\n    moe_flops   = round(moe_flops,   1)\n    savings_pct = round(savings_pct, 1)\n\n    return dense_flops, moe_flops, savings_pct\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Suppose 16 experts total, 2 active per example, dims 1024 -> 4096\n    d_flops, m_flops, save = compute_efficiency(16, 2, 1024, 4096)\n    print(f\"Dense FLOPs: {d_flops:.1f}\")\n    print(f\"MoE FLOPs:   {m_flops:.1f}\")\n    print(f\"Savings:     {save:.1f}%\")\n```"}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism for Mixture-of-Experts.\n    \n    Arguments:\n    X       -- input matrix of shape (batch_size, input_dim)\n    W_g     -- gating weight matrix of shape (input_dim, num_experts)\n    W_noise -- noise-weight matrix of shape (input_dim, num_experts)\n    N       -- pre-sampled noise of shape (batch_size, num_experts)\n    k       -- number of experts to select per example\n    \n    Returns:\n    A Python list of shape (batch_size, num_experts) containing the\n    final gating probabilities (rounded to 4 decimal places).\n    \"\"\"\n    # 1) clean gating logits\n    clean_logits = X.dot(W_g)                          # (B, E)\n    \n    # 2) predicted noise std (or scale)\n    noise_scale  = X.dot(W_noise)                      # (B, E)\n    \n    # 3) noisy logits\n    noisy_logits = clean_logits + N * noise_scale      # (B, E)\n    \n    # 4) find top-k expert indices for each batch element\n    #    we use argpartition for efficiency, then slice out the k largest\n    topk_indices = np.argpartition(noisy_logits, -k, axis=1)[:, -k:]  # (B, k)\n    \n    # 5) gather the noisy logits of those top-k\n    batch_idx     = np.arange(X.shape[0])[:, None]                       # (B, 1)\n    topk_logits   = noisy_logits[batch_idx, topk_indices]               # (B, k)\n    \n    # 6) compute a stable softmax over those k logits\n    max_logits    = np.max(topk_logits, axis=1, keepdims=True)          # (B, 1)\n    exp_shifted   = np.exp(topk_logits - max_logits)                    # (B, k)\n    sum_exp       = np.sum(exp_shifted, axis=1, keepdims=True)          # (B, 1)\n    softmax_topk  = exp_shifted / sum_exp                               # (B, k)\n    \n    # 7) scatter the softmax probabilities back into full expert space\n    B, E = noisy_logits.shape\n    gates = np.zeros((B, E), dtype=softmax_topk.dtype)                  # (B, E)\n    gates[batch_idx, topk_indices] = softmax_topk                        # fill in top-k\n    \n    # 8) round to 4 decimal places\n    gates = np.round(gates, 4)\n    \n    # 9) convert to nested Python list\n    return gates.tolist()\n```"}
{"task_id": 125, "completion_id": 0, "completion": "import numpy as np\nnp.random.seed(42)\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Sparse Mixture of Experts layer.\n    Args:\n        x: input array of shape (batch_size, d_in)\n        We: expert weight tensor of shape (n_experts, d_in, d_out)\n        Wg: gating weight matrix of shape (d_in, n_experts)\n        n_experts: total number of experts\n        top_k: number of experts to select per token\n    Returns:\n        A Python list of shape (batch_size, d_out) with values rounded to 4 decimals.\n    \"\"\"\n    # 1. Compute gating logits and softmax\n    logits = x.dot(Wg)  # (batch_size, n_experts)\n    max_logits = np.max(logits, axis=1, keepdims=True)\n    exp_logits = np.exp(logits - max_logits)\n    sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n    probs = exp_logits / sum_exp  # (batch_size, n_experts)\n\n    batch_size = x.shape[0]\n    d_out = We.shape[2]\n    outputs = np.zeros((batch_size, d_out))\n\n    # 2. For each sample, select top-k experts, apply and aggregate\n    for i in range(batch_size):\n        prob_i = probs[i]  # (n_experts,)\n        # get indices of top_k experts (highest gating probs)\n        top_indices = np.argsort(prob_i)[-top_k:][::-1]  # (top_k,)\n        top_probs = prob_i[top_indices]                  # (top_k,)\n\n        # gather expert weights for selected experts\n        We_k = We[top_indices]                           # (top_k, d_in, d_out)\n\n        # compute expert outputs: for each expert j, y_j = x_i @ We_k[j]\n        # we can use tensordot to batch this:\n        #   result shape: (top_k, d_out)\n        y_e = np.tensordot(We_k, x[i], axes=([1], [0]))\n\n        # weight by gating probabilities and sum\n        weighted = top_probs[:, None] * y_e              # (top_k, d_out)\n        outputs[i] = np.sum(weighted, axis=0)            # (d_out,)\n\n    # 3. Round to 4th decimal and convert to list\n    outputs = np.round(outputs, 4)\n    return outputs.tolist()"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\nnp.random.seed(42)\n\ndef group_normalization(X: np.ndarray,\n                        gamma: np.ndarray,\n                        beta: np.ndarray,\n                        num_groups: int,\n                        epsilon: float = 1e-5):\n    \"\"\"\n    Perform Group Normalization on a 4D tensor.\n    \n    Args:\n      X          : Input tensor of shape (B, C, H, W)\n      gamma      : Scale parameter of shape (C,)\n      beta       : Shift parameter of shape (C,)\n      num_groups : Number of groups to split the channels into\n      epsilon    : Small constant for numerical stability\n      \n    Returns:\n      A nested Python list (B x C x H x W) of the normalized, scaled and shifted output,\n      with values rounded to 4 decimal places.\n    \"\"\"\n    B, C, H, W = X.shape\n    assert C % num_groups == 0, \"C must be divisible by num_groups\"\n    \n    # reshape into groups: (B, G, C//G, H, W)\n    G = num_groups\n    X_grouped = X.reshape(B, G, C // G, H, W)\n    \n    # compute mean/variance per group over (channels_in_group, H, W)\n    mean = X_grouped.mean(axis=(2, 3, 4), keepdims=True)\n    var  = X_grouped.var(axis=(2, 3, 4), keepdims=True)\n    \n    # normalize\n    X_norm = (X_grouped - mean) / np.sqrt(var + epsilon)\n    \n    # reshape back to (B, C, H, W)\n    X_norm = X_norm.reshape(B, C, H, W)\n    \n    # apply per\u2010channel scale and shift\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta  = beta.reshape(1, C, 1, 1)\n    Y = X_norm * gamma + beta\n    \n    # round to nearest 4th decimal and convert to Python list\n    Y = np.round(Y, 4)\n    return Y.tolist()\n```"}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef find_treasure(start_x: float,\n                  learning_rate: float = 0.1,\n                  tolerance: float = 1e-6,\n                  max_iters: int = 10000) -> float:\n       Uses gradient descent to locate the local minimum of f(x) = x^4 - 3x^3 + 2.\n    Returns the x-value of the minimum, rounded to 4 decimal places.\n       x = start_x\n    for _ in range(max_iters):\n        # Gradient of f(x): f'(x) = 4x^3 - 9x^2\n        grad = 4 * x**3 - 9 * x**2\n        \n        x_next = x - learning_rate * grad\n        \n        # Check for convergence\n        if abs(x_next - x) < tolerance:\n            x = x_next\n            break\n        \n        x = x_next\n\n    return round(x, 4)\n\n\n# Example usage:\nif __name__ ==main    # Try different starting points\n    for init in [-5.0, 0.5, 10.0]:\n        loc = find_treasure(start_x=init, learning_rate=0.05)\n        print(fStarting at x={init:>5}, treasure found at x={loc```\n\nExplanation of key parts:\n1. We compute the derivative f\u2032(x) = 4x\u00b3 \u2212 9x\u00b2.\n2. We iteratively update x \u2190 x \u2212 \u03b1\u00b7f\u2032(x) until the change in x is below a small tolerance.\n3. We round the final result to four decimal places, as requested."}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) activation \u2013 a normalization\u2010free variant of tanh.\n    Here we parameterize it as:\n        y = gamma * tanh(alpha * x) + beta\n    which preserves the conventional tanh squashing but allows\n    trainable scale (gamma) and shift (beta) without any mean\u2010variance norm.\n    \n    Args:\n        x     (np.ndarray): input array of any shape\n        alpha (float)     : input\u2010scale hyperparameter\n        gamma (float)     : output\u2010scale (gain) hyperparameter\n        beta  (float)     : output\u2010bias hyperparameter\n    \n    Returns:\n        List[float]: the activation output, rounded to 4 decimal places\n                     and converted to nested Python lists.\n    \"\"\"\n    # apply the dynamic tanh elementwise\n    y = gamma * np.tanh(alpha * x) + beta\n    \n    # round to nearest 4th decimal\n    y_rounded = np.round(y, 4)\n    \n    # convert back to Python nested lists\n    return y_rounded.tolist()\n```\n\nExample usage:\n\n```python\n>>> x = np.array([[0.0, 1.0], [-2.5, 3.1415]])\n>>> dynamic_tanh(x, alpha=1.2, gamma=0.9, beta=0.1)\n[[0.1, 0.7822], [-0.8584, 0.9985]]\n```"}
