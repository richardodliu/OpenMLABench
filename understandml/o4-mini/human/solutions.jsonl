{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Compute the dot product of matrix `a` (n x m) with vector `b` (length m).\n    Return a list of length n, where result[i] = sum(a[i][j] * b[j] for j in range(m)).\n    If dimensions mismatch, return -1.\n    \"\"\"\n    if not a:\n        return []\n    m = len(a[0])\n    for row in a:\n        if len(row) != m:\n            return -1\n    if len(b) != m:\n        return -1\n    result = []\n    for row in a:\n        dot = 0\n        for j in range(m):\n            dot += row[j] * b[j]\n        result.append(dot)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": ""}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape the input 2D list 'a' into the shape specified by 'new_shape'.\n    If the total number of elements does not match, returns an empty list [].\n    \"\"\"\n    arr = np.array(a)\n    try:\n        (rows, cols) = new_shape\n    except (TypeError, ValueError):\n        return []\n    if arr.size != rows * cols:\n        return []\n    return arr.reshape(rows, cols).tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a 2D matrix by rows or by columns.\n\n    Parameters:\n        matrix: A list of lists of floats representing the matrix.\n        mode: 'row' to compute the mean of each row,\n              'column' to compute the mean of each column.\n\n    Returns:\n        A list of floats containing the means in the specified mode.\n\n    Raises:\n        ValueError: If mode is not 'row' or 'column',\n                    if the matrix is irregular,\n                    or if any row/column is empty.\n    \"\"\"\n    if mode not in ('row', 'column'):\n        raise ValueError(f\"Mode must be 'row' or 'column', got '{mode}'\")\n    if not matrix:\n        return []\n    if mode == 'row':\n        means = []\n        for (idx, row) in enumerate(matrix):\n            if not row:\n                raise ValueError(f'Row {idx} is empty; cannot compute mean.')\n            means.append(sum(row) / len(row))\n        return means\n    n_cols = len(matrix[0])\n    if n_cols == 0:\n        return []\n    for (idx, row) in enumerate(matrix):\n        if len(row) != n_cols:\n            raise ValueError(f'Row {idx} length ({len(row)}) does not match expected column count ({n_cols}).')\n    n_rows = len(matrix)\n    means = []\n    for col_idx in range(n_cols):\n        col_sum = sum((matrix[row_idx][col_idx] for row_idx in range(n_rows)))\n        means.append(col_sum / n_rows)\n    return means"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiply each element of the given matrix by the provided scalar.\n    \n    Parameters:\n    - matrix: A list of lists of numbers (int or float), representing the matrix.\n    - scalar: A number (int or float) by which to scale each element of the matrix.\n    \n    Returns:\n    - A new matrix (list of lists) where each element is the original element multiplied by the scalar.\n    \n    Example:\n        >>> m = [\n        ...     [1, 2, 3],\n        ...     [4, 5, 6]\n        ... ]\n        >>> scalar_multiply(m, 2)\n        [[2, 4, 6], [8, 10, 12]]\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "from math import sqrt\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2\u00d72 matrix.\n    The matrix must be of the form [[a, b], [c, d]].\n    Returns a list of the two eigenvalues sorted from largest to smallest.\n    Raises ValueError if the matrix is not 2\u00d72 or if the eigenvalues are complex.\n    \"\"\"\n    if len(matrix) != 2 or any((len(row) != 2 for row in matrix)):\n        raise ValueError('Input must be a 2\u00d72 matrix.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    det = a * d - b * c\n    discriminant = trace * trace - 4 * det\n    if discriminant < 0:\n        raise ValueError('This matrix has complex eigenvalues; only real eigenvalues are supported.')\n    root = sqrt(discriminant)\n    eig1 = (trace + root) / 2\n    eig2 = (trace - root) / 2\n    return sorted([eig1, eig2], reverse=True)"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes T^{-1} * A * S, validating that T and S are invertible\n    and that the shapes align for multiplication. Rounds result\n    to 4 decimal places. Returns -1 if there's no valid solution.\n    \"\"\"\n    try:\n        A_mat = np.array(A, dtype=float)\n        T_mat = np.array(T, dtype=float)\n        S_mat = np.array(S, dtype=float)\n    except Exception:\n        return -1\n    if T_mat.ndim != 2 or S_mat.ndim != 2:\n        return -1\n    (m, m2) = T_mat.shape\n    (n, n2) = S_mat.shape\n    if A_mat.ndim != 2 or A_mat.shape != (m, n):\n        return -1\n    try:\n        T_inv = np.linalg.inv(T_mat)\n        S_inv = np.linalg.inv(S_mat)\n    except np.linalg.LinAlgError:\n        return -1\n    result = T_inv.dot(A_mat).dot(S_mat)\n    result_rounded = np.round(result, decimals=4)\n    return result_rounded.tolist()"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n        matrix: A 2x2 matrix represented as a list of two lists, each containing two floats.\n                For example: [[a, b], [c, d]]\n    \n    Returns:\n        A new 2x2 matrix (list of lists) representing the inverse,\n        or None if the matrix is not invertible (determinant is zero).\n    \n    Raises:\n        ValueError: If the input is not a 2x2 matrix.\n    \"\"\"\n    if len(matrix) != 2 or any((len(row) != 2 for row in matrix)):\n        raise ValueError('Input must be a 2x2 matrix (list of two lists of length 2).')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_det = 1.0 / det\n    return [[d * inv_det, -b * inv_det], [-c * inv_det, a * inv_det]]"}
{"task_id": 9, "completion_id": 0, "solution": ""}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the sample covariance matrix for a set of feature vectors.\n    Each inner list in `vectors` represents all observations of one feature.\n    \n    Cov(X_i, X_j) = sum_k [(X_i[k] - mean_i) * (X_j[k] - mean_j)] / (n - 1)\n    \n    Arguments:\n    vectors -- list of m lists, each of length n, where m = number of features,\n               n = number of observations per feature.\n               \n    Returns:\n    m x m covariance matrix as a list of lists.\n    \n    Raises:\n    ValueError if input is empty, features have differing lengths, or < 2 observations.\n    \"\"\"\n    if not vectors:\n        raise ValueError('Input vector list is empty.')\n    m = len(vectors)\n    n = len(vectors[0])\n    if n < 2:\n        raise ValueError('At least two observations per feature are required.')\n    for vec in vectors:\n        if len(vec) != n:\n            raise ValueError('All feature vectors must have the same number of observations.')\n    means = [sum(vec) / n for vec in vectors]\n    cov_matrix = [[0.0] * m for _ in range(m)]\n    for i in range(m):\n        for j in range(i, m):\n            cov_ij = 0.0\n            for k in range(n):\n                cov_ij += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            cov_ij /= n - 1\n            cov_matrix[i][j] = cov_ij\n            cov_matrix[j][i] = cov_ij\n    return cov_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve the linear system A x = b using the Jacobi iterative method.\n\n    Parameters:\n    A : np.ndarray, shape (N, N)\n        Coefficient matrix (should be diagonally dominant for convergence).\n    b : np.ndarray, shape (N,) or (N,1)\n        Right\u2010hand side vector.\n    n : int\n        Number of Jacobi iterations to perform.\n\n    Returns:\n    x : list of float\n        Approximate solution vector after n iterations, each entry rounded to 4 decimals.\n    \"\"\"\n    b_vec = b.flatten()\n    N = A.shape[0]\n    x_prev = np.zeros(N, dtype=float)\n    for _ in range(n):\n        x_curr = np.zeros_like(x_prev)\n        for i in range(N):\n            sigma = 0.0\n            for j in range(N):\n                if j != i:\n                    sigma += A[i, j] * x_prev[j]\n            x_curr[i] = (b_vec[i] - sigma) / A[i, i]\n        x_curr = np.round(x_curr, 4)\n        x_prev = x_curr\n    return x_prev.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximate the SVD of a 2x2 matrix A using the Jacobi (one-step) method\n    on A^T A to get V and the singular values, then compute U = A V \u03a3^{-1}.\n    \n    Returns:\n        U (2\u00d72 ndarray), \n        S (2\u00d72 ndarray with singular values on the diagonal), \n        Vt (2\u00d72 ndarray, V transposed),\n    all entries rounded to 4 decimal places.\n    \"\"\"\n    assert A.shape == (2, 2), 'Input must be a 2x2 matrix.'\n    B = A.T @ A\n    a = B[0, 0]\n    b = B[1, 1]\n    c = B[0, 1]\n    if abs(c) < 1e-12:\n        phi = 0.0\n    else:\n        phi = 0.5 * np.arctan2(2 * c, a - b)\n    cphi = np.cos(phi)\n    sphi = np.sin(phi)\n    V = np.array([[cphi, -sphi], [sphi, cphi]])\n    t = (a + b) / 2.0\n    d = np.hypot((a - b) / 2.0, c)\n    lam1 = t + d\n    lam2 = t - d\n    sigma1 = np.sqrt(max(lam1, 0.0))\n    sigma2 = np.sqrt(max(lam2, 0.0))\n    if sigma1 < sigma2:\n        (sigma1, sigma2) = (sigma2, sigma1)\n        V = V[:, ::-1]\n    S = np.diag([sigma1, sigma2])\n    U = np.zeros((2, 2))\n    if sigma1 > 1e-12:\n        U[:, 0] = A @ V[:, 0] / sigma1\n    if sigma2 > 1e-12:\n        U[:, 1] = A @ V[:, 1] / sigma2\n    Vt = V.T\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    Vt = np.round(Vt, 4)\n    return (U, S, Vt)"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion recursively.\n    :param matrix: a list of 4 lists, each of length 4, representing the matrix\n    :return: determinant as a float\n    \"\"\"\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('The input must be a 4x4 matrix.')\n\n    def det_recursive(mat: list[list[float]]) -> float:\n        n = len(mat)\n        if n == 1:\n            return mat[0][0]\n        if n == 2:\n            return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n        total = 0.0\n        for col in range(n):\n            minor = [row[:col] + row[col + 1:] for row in mat[1:]]\n            sign = (-1) ** col\n            total += sign * mat[0][col] * det_recursive(minor)\n        return total\n    return det_recursive(matrix)"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 17, "completion_id": 0, "solution": ""}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform PCA on the input data from scratch.\n    \n    Parameters:\n    - data: 2D numpy array of shape (n_samples, n_features)\n      where each row is a sample and each column is a feature.\n    - k: int\n      Number of principal components to return.\n      \n    Returns:\n    - List of k principal components, each being a list of length n_features.\n      Components are sorted by descending explained variance and rounded to\n      4 decimal places.\n    \"\"\"\n    if not isinstance(data, np.ndarray):\n        raise TypeError('data must be a numpy ndarray')\n    if data.ndim != 2:\n        raise ValueError('data must be a 2D array of shape (n_samples, n_features)')\n    (n_samples, n_features) = data.shape\n    if k <= 0 or k > n_features:\n        raise ValueError('k must be between 1 and n_features')\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    std_replaced = np.where(std == 0, 1.0, std)\n    data_std = (data - mean) / std_replaced\n    cov_matrix = data_std.T @ data_std / (n_samples - 1)\n    (eigvals, eigvecs) = np.linalg.eigh(cov_matrix)\n    idx_desc = np.argsort(eigvals)[::-1]\n    topk_idx = idx_desc[:k]\n    topk_vecs = eigvecs[:, topk_idx]\n    topk_vecs_rounded = np.round(topk_vecs, 4)\n    principal_components = [topk_vecs_rounded[:, i].tolist() for i in range(k)]\n    return principal_components"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Deterministic Pegasos kernel SVM.\n    Returns:\n      alpha_list: list of alpha coefficients (length N)\n      bias_list:  single\u2010element list [bias]\n    All values rounded to 4 decimal places.\n    \"\"\"\n    (n_samples, n_features) = data.shape\n    y = labels.astype(float)\n    if kernel == 'linear':\n        K = data.dot(data.T)\n    elif kernel == 'rbf':\n        sq = np.sum(data ** 2, axis=1, keepdims=True)\n        dist2 = sq + sq.T - 2 * data.dot(data.T)\n        K = np.exp(-dist2 / (2 * sigma ** 2))\n    else:\n        raise ValueError(\"Unsupported kernel. Choose 'linear' or 'rbf'.\")\n    alpha = np.zeros(n_samples, dtype=float)\n    b = 0.0\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        f = (alpha * y).dot(K) + b\n        viol = y * f < 1\n        alpha = (1 - eta * lambda_val) * alpha\n        alpha[viol] += eta / n_samples * y[viol]\n        b += eta / n_samples * np.sum(y[viol])\n        ay = alpha * y\n        norm_sq = ay.dot(K.dot(ay))\n        if norm_sq > 0:\n            norm = np.sqrt(norm_sq)\n            factor = min(1.0, 1.0 / (np.sqrt(lambda_val) * norm))\n            alpha *= factor\n    alpha = np.round(alpha, 4)\n    b = round(b, 4)\n    return (alpha.tolist(), [b])"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: Sigmoid output rounded to four decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np"}
{"task_id": 25, "completion_id": 0, "solution": ""}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad * 1.0\n            other.grad += out.grad * 1.0\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other):\n        return self + other\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __rsub__(self, other):\n        return other + -self\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0.0, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += out.grad * (1.0 if self.data > 0 else 0.0)\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the matrix P that transforms coordinate vectors from basis B to basis C in R^3.\n    If v_B are the coords of v in basis B, and v_C the coords in basis C, then\n        v = B_mat @ v_B  and  v = C_mat @ v_C\n    implies C_mat @ v_C = B_mat @ v_B  =>  v_C = C_mat^{-1} @ B_mat @ v_B.\n    Hence P = C_mat^{-1} @ B_mat.\n\n    Args:\n      B: a list of three basis vectors for B, each a list of 3 ints.\n      C: a list of three basis vectors for C, each a list of 3 ints.\n\n    Returns:\n      P as a 3x3 list of floats, rounded to 4 decimal places.\n    \"\"\"\n    B_mat = np.array(B, dtype=float).T\n    C_mat = np.array(C, dtype=float).T\n    P = np.linalg.inv(C_mat).dot(B_mat)\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix A\n    using eigen-decomposition of A^T A.\n\n    Returns U, S, V such that A = U @ S @ V,\n    with all entries rounded to the nearest 1e-4 and returned as Python lists.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    ATA = A.T @ A\n    (eigvals, eigvecs) = np.linalg.eigh(ATA)\n    idx = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[idx]\n    V = eigvecs[:, idx]\n    sigma1 = np.sqrt(max(eigvals[0], 0.0))\n    sigma2 = np.sqrt(max(eigvals[1], 0.0))\n    U = np.zeros((2, 2))\n    if sigma1 > 1e-12:\n        U[:, 0] = A @ V[:, 0] / sigma1\n    else:\n        U[:, 0] = np.array([1.0, 0.0])\n    if sigma2 > 1e-12:\n        U[:, 1] = A @ V[:, 1] / sigma2\n    else:\n        U[:, 1] = np.array([-U[1, 0], U[0, 0]])\n    if np.linalg.det(U) < 0:\n        U[:, 1] *= -1\n        V[:, 1] *= -1\n    S = np.zeros((2, 2))\n    S[0, 0] = sigma1\n    S[1, 1] = sigma2\n    U_rounded = np.round(U, 4)\n    S_rounded = np.round(S, 4)\n    Vt_rounded = np.round(V.T, 4)\n    return (U_rounded.tolist(), S_rounded.tolist(), Vt_rounded.tolist())"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle two numpy arrays in unison, preserving correspondence between X and y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        Feature matrix (n_samples, ...).\n    y : numpy.ndarray\n        Label array (n_samples, ...) aligned with X.\n    seed : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    X_shuffled_list : list\n        Shuffled X converted to a Python list.\n    y_shuffled_list : list\n        Shuffled y converted to a Python list.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples.')\n    permuted_idx = np.random.permutation(X.shape[0])\n    X_shuffled = X[permuted_idx]\n    y_shuffled = y[permuted_idx]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    A batch iterator that yields batches of X (and y if provided).\n    Each batch is returned as a Python list (via ndarray.tolist()).\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, ...)\n        The input data.\n    y : array-like of shape (n_samples,), optional\n        The target values. If provided, must have the same length as X.\n    batch_size : int, default=64\n        The number of samples per batch.\n\n    Yields\n    ------\n    batch_X : list\n        A batch of X converted to a Python list.\n    batch_y : list, optional\n        A batch of y converted to a Python list (only if y is provided).\n    \"\"\"\n    X = np.asarray(X)\n    if y is not None:\n        y = np.asarray(y)\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(f'X and y must have the same number of samples; got len(X)={X.shape[0]} and len(y)={y.shape[0]}.')\n    n_samples = X.shape[0]\n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        batch_X = X[start:end]\n        if y is not None:\n            batch_y = y[start:end]\n            yield (batch_X.tolist(), batch_y.tolist())\n        else:\n            yield batch_X.tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide dataset X based on whether the value of feature at index feature_i\n    is greater than or equal to a given threshold.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        The input dataset.\n    feature_i : int\n        The index of the feature (column) to split on.\n    threshold : float\n        The threshold value to compare the feature against.\n\n    Returns\n    -------\n    X_ge : list\n        List of samples (as lists) where X[:, feature_i] >= threshold.\n    X_lt : list\n        List of samples (as lists) where X[:, feature_i] < threshold.\n    \"\"\"\n    X = np.array(X)\n    ge_mask = X[:, feature_i] >= threshold\n    X_ge = X[ge_mask]\n    X_lt = X[~ge_mask]\n    return (X_ge.tolist(), X_lt.tolist())"}
{"task_id": 32, "completion_id": 0, "solution": ""}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets (bootstrapped or without replacement) of a dataset.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        Feature matrix.\n    y : np.ndarray, shape (n_samples,)\n        Target vector.\n    n_subsets : int\n        Number of random subsets to generate.\n    replacements : bool, default=True\n        If True, sample with replacement (bootstrap). If False, sample without replacement.\n    seed : int, default=42\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    List[Tuple[List[List], List]]\n        A list of length `n_subsets`. Each element is a tuple:\n          (X_subset_as_list_of_lists, y_subset_as_list)\n    \"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y)\n    n_samples = X.shape[0]\n    if X.ndim != 2:\n        raise ValueError(f'X must be 2D, but got shape {X.shape}')\n    if y.ndim != 1:\n        raise ValueError(f'y must be 1D, but got shape {y.shape}')\n    if y.shape[0] != n_samples:\n        raise ValueError('Number of samples in X and y must match')\n    if n_subsets < 1 or not isinstance(n_subsets, int):\n        raise ValueError('n_subsets must be a positive integer')\n    rng = np.random.RandomState(seed)\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            idx = rng.randint(0, n_samples, size=n_samples)\n        else:\n            idx = rng.choice(n_samples, size=n_samples, replace=False)\n        X_sub = X[idx]\n        y_sub = y[idx]\n        subsets.append((X_sub.tolist(), y_sub.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": ""}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n\n    Parameters\n    ----------\n    x : array-like\n        Input 1D array or sequence.\n\n    Returns\n    -------\n    list of lists\n        Diagonal matrix as a nested Python list.\n    \"\"\"\n    arr = np.array(x)\n    if arr.ndim != 1:\n        raise ValueError('Input must be a 1D array.')\n    diag_matrix = np.diag(arr)\n    return diag_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Compute the accuracy classification score.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True labels.\n    y_pred : array-like of shape (n_samples,)\n        Predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy score, defined as the fraction of correct predictions:\n        (number of correct predictions) / (total number of predictions).\n\n    Raises\n    ------\n    ValueError\n        If y_true and y_pred have different shapes.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError(f'Shape mismatch: y_true has shape {y_true.shape}, but y_pred has shape {y_pred.shape}')\n    n_samples = y_true.size\n    if n_samples == 0:\n        return 0.0\n    n_correct = np.sum(y_true == y_pred)\n    return float(n_correct) / n_samples"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix of X with itself or with Y.\n\n    Parameters:\n    -----------\n    X : 2D array-like of shape (n_samples, n_features_X)\n    Y : 2D array-like of shape (n_samples, n_features_Y), optional\n        If None, compute the correlation matrix of X with itself.\n\n    Returns:\n    --------\n    corr_list : list of lists\n        The correlation matrix, rounded to 4 decimal places, as a Python list.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    if Y is None:\n        Y = X\n    else:\n        Y = np.asarray(Y, dtype=float)\n    if X.ndim != 2 or Y.ndim != 2:\n        raise ValueError('Both X and Y must be 2D arrays.')\n    if X.shape[0] != Y.shape[0]:\n        raise ValueError('X and Y must have the same number of rows (samples).')\n    n_samples = X.shape[0]\n    if n_samples < 2:\n        raise ValueError('Need at least 2 samples to compute correlation.')\n    X_centered = X - X.mean(axis=0)\n    Y_centered = Y - Y.mean(axis=0)\n    cov_matrix = X_centered.T @ Y_centered / (n_samples - 1)\n    std_X = X_centered.std(axis=0, ddof=1)\n    std_Y = Y_centered.std(axis=0, ddof=1)\n    denom = np.outer(std_X, std_Y)\n    corr = cov_matrix / denom\n    corr_rounded = np.round(corr, 4)\n    return corr_rounded.tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a 1D list of scores, returning a Python list\n    with each value rounded to 4 decimal places.\n\n    Args:\n        scores (list): A list of numerical scores.\n\n    Returns:\n        list: Log-softmax values (rounded to 4 decimal places).\n    \"\"\"\n    x = np.array(scores, dtype=np.float64)\n    x_shifted = x - np.max(x)\n    log_sum_exp = np.log(np.sum(np.exp(x_shifted)))\n    result = x_shifted - log_sum_exp\n    return np.round(result, 4).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None, learning_rate=0.01):\n        self.n_units = n_units\n        self.input_shape = input_shape\n        self.W = None\n        self.w0 = None\n        self.optimizerW = None\n        self.optimizerw0 = None\n        self.layer_input = None\n        self.trainable = True\n        self.learning_rate = learning_rate"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Compute the Rectified Linear Unit (ReLU) activation of the input.\n\n    Parameters:\n    z (float): The input value.\n\n    Returns:\n    float: z if z > 0, otherwise 0.\n    \"\"\"\n    return z if z > 0.0 else 0.0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss:\n        loss = MSE(y_true, X @ w) + alpha * ||w||^2\n\n    where\n        MSE = (1/n) * sum_i (y_true[i] - (X @ w)[i])^2\n        ||w||^2 = sum_j w[j]^2\n\n    All results are rounded to the nearest 4th decimal.\n    \n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        The feature matrix.\n    w : np.ndarray, shape (n_features,)\n        The coefficient vector.\n    y_true : np.ndarray, shape (n_samples,)\n        The true target values.\n    alpha : float\n        Regularization strength.\n    \n    Returns\n    -------\n    float\n        The Ridge regression loss, rounded to 4 decimals.\n    \"\"\"\n    y_pred = X.dot(w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    reg = alpha * np.sum(w ** 2)\n    loss = mse + reg\n    return float(np.round(loss, 4))"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Compute the Leaky ReLU activation of the input.\n\n    Parameters:\n    z (float): The input value.\n    alpha (float): Slope for negative inputs (default is 0.01).\n\n    Returns:\n    float|int: z if z is positive; otherwise alpha * z.\n    \"\"\"\n    return z if z >= 0 else alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Compute the precision metric:\n        precision = true_positives / (true_positives + false_positives)\n\n    Parameters\n    ----------\n    y_true : array\u2010like of shape (n_samples,)\n        True binary labels (0 or 1).\n    y_pred : array\u2010like of shape (n_samples,)\n        Predicted binary labels (0 or 1).\n\n    Returns\n    -------\n    float\n        Precision score. If there are no positive predictions (tp + fp == 0),\n        returns 0.0 to avoid division by zero.\n    \"\"\"\n    y_true = np.asarray(y_true).flatten()\n    y_pred = np.asarray(y_pred).flatten()\n    if y_true.shape != y_pred.shape:\n        raise ValueError(f'Shape mismatch: y_true {y_true.shape} vs y_pred {y_pred.shape}')\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    if tp + fp == 0:\n        return 0.0\n    return tp / float(tp + fp)"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert a matrix to its Reduced Row Echelon Form (RREF).\n\n    Parameters:\n    matrix : array-like, shape (m, n)\n        The input matrix, can be a list of lists or a numpy array.\n\n    Returns:\n    rref_matrix : list of lists\n        The RREF of the input matrix, returned as a Python list of lists.\n    \"\"\"\n    A = np.array(matrix, dtype=float)\n    (m, n) = A.shape\n    row = 0\n    tol = 1e-12\n    for col in range(n):\n        if row >= m:\n            break\n        pivot = np.argmax(np.abs(A[row:, col])) + row\n        if abs(A[pivot, col]) < tol:\n            continue\n        if pivot != row:\n            A[[row, pivot], :] = A[[pivot, row], :]\n        A[row, :] = A[row, :] / A[row, col]\n        for r in range(m):\n            if r != row:\n                factor = A[r, col]\n                if abs(factor) > tol:\n                    A[r, :] = A[r, :] - factor * A[row, :]\n        row += 1\n    A[np.abs(A) < tol] = 0.0\n    return A.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Compute the Optimal String Alignment (OSA) distance between two strings.\n    This distance allows insertions, deletions, substitutions, and\n    adjacent transpositions, each at cost 1.\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(1, m + 1):\n        dp[i, 0] = i\n    for j in range(1, n + 1):\n        dp[0, j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            v_sub = dp[i - 1, j - 1] + cost\n            v_del = dp[i - 1, j] + 1\n            v_ins = dp[i, j - 1] + 1\n            dp[i, j] = min(v_sub, v_del, v_ins)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i, j] = min(dp[i, j], dp[i - 2, j - 2] + 1)\n    return int(dp[m, n])"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Compute the recall (true positive rate) for binary classification.\n\n    Args:\n        y_true (list or array-like): True binary labels (0 or 1).\n        y_pred (list or array-like): Predicted binary labels (0 or 1).\n\n    Returns:\n        float: Recall rounded to three decimal places. Returns 0.0 if there are\n               no actual positives (to avoid division by zero).\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denom = tp + fn\n    if denom == 0:\n        return 0.0\n    rec = tp / denom\n    return float(np.round(rec, 3))"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute the self-attention output.\n    \n    Args:\n        Q: numpy array of shape (n_q, d_k) representing queries\n        K: numpy array of shape (n_k, d_k) representing keys\n        V: numpy array of shape (n_k, d_v) representing values\n\n    Returns:\n        A python list of shape (n_q, d_v) containing the self-attention output,\n        with each value rounded to 4 decimal places.\n    \"\"\"\n    Q = np.array(Q, dtype=float)\n    K = np.array(K, dtype=float)\n    V = np.array(V, dtype=float)\n    d_k = Q.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores_exp = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights = scores_exp / np.sum(scores_exp, axis=1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    output = np.round(output, 4).tolist()\n    return output"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    A simple RNN forward pass that processes an input sequence and returns\n    the final hidden state, using tanh activation.\n\n    Arguments:\n    input_sequence         -- list of input vectors (each a list of floats), length T\n    initial_hidden_state   -- list of floats, the initial hidden state (size H)\n    Wx                     -- input-to-hidden weights, shape (H, D)\n    Wh                     -- hidden-to-hidden weights, shape (H, H)\n    b                      -- bias vector, shape (H,)\n\n    Returns:\n    final_hidden_state     -- Python list (length H) of the final hidden state,\n                              rounded to 4 decimal places\n    \"\"\"\n    h = np.array(initial_hidden_state, dtype=float)\n    Wx = np.array(Wx, dtype=float)\n    Wh = np.array(Wh, dtype=float)\n    b = np.array(b, dtype=float)\n    for x in input_sequence:\n        x_vec = np.array(x, dtype=float)\n        h = np.tanh(Wx.dot(x_vec) + Wh.dot(h) + b)\n    h_rounded = np.round(h, 4)\n    return h_rounded.tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translates a list of 2D points by (tx, ty) using a 3x3 homogeneous translation matrix.\n\n    Parameters:\n    - points: list of [x, y] coordinates, e.g. [[x1, y1], [x2, y2], ...]\n    - tx: translation distance along the x axis\n    - ty: translation distance along the y axis\n\n    Returns:\n    - A new list of translated points, in the same format as the input.\n    \"\"\"\n    pts = np.array(points, dtype=float)\n    if pts.ndim != 2 or pts.shape[1] != 2:\n        raise ValueError('points must be a list of [x, y] coordinates')\n    ones = np.ones((pts.shape[0], 1), dtype=float)\n    pts_homog = np.hstack([pts, ones])\n    T = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]], dtype=float)\n    translated_homog = pts_homog @ T.T\n    translated_pts = translated_homog[:, :2]\n    return translated_pts.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler divergence KL(P || Q) between two univariate normal distributions:\n      P ~ N(mu_p,  sigma_p^2)\n      Q ~ N(mu_q,  sigma_q^2)\n\n    Formula:\n      KL(P || Q) = log(sigma_q / sigma_p)\n                   + (sigma_p^2 + (mu_p - mu_q)**2) / (2 * sigma_q^2)\n                   - 1/2\n\n    Args:\n      mu_p    (float): mean of P\n      sigma_p (float): standard deviation of P (must be > 0)\n      mu_q    (float): mean of Q\n      sigma_q (float): standard deviation of Q (must be > 0)\n\n    Returns:\n      float: the KL divergence KL(P || Q)\n\n    Raises:\n      ValueError: if sigma_p <= 0 or sigma_q <= 0\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive.')\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    kl = term1 + term2 - 0.5\n    return float(kl)"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the linear system A x = b using the Gauss\u2013Seidel iterative method.\n\n    Parameters:\n    A     : square coefficient matrix (array-like)\n    b     : right-hand side vector (array-like)\n    n     : number of iterations (int)\n    x_ini : optional initial guess for x (array-like), defaults to zero vector\n\n    Returns:\n    x_approx : list of solution components rounded to 4 decimal places\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).flatten()\n    (m, m2) = A.shape\n    if m != m2:\n        raise ValueError('Matrix A must be square')\n    if b.size != m:\n        raise ValueError(\"Vector b must have the same size as A's dimensions\")\n    if x_ini is None:\n        x = np.zeros(m, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float).flatten()\n        if x.size != m:\n            raise ValueError('Initial guess x_ini must have length {}'.format(m))\n    for _ in range(n):\n        for i in range(m):\n            sum1 = np.dot(A[i, :i], x[:i])\n            sum2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n    x_approx = np.round(x, 4).tolist()\n    return x_approx"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns:\n          - hidden_states_list: list of hidden states at each time step\n          - final_hidden: final hidden state\n          - final_cell: final cell state\n        All returned values are Python lists (via tolist()) with entries rounded to 4 decimals.\n        \n        x: array-like of shape (seq_len, input_size)\n        initial_hidden_state: array-like of shape (hidden_size, 1) or (hidden_size,)\n        initial_cell_state: array-like of shape (hidden_size, 1) or (hidden_size,)\n        \"\"\"\n        x_arr = np.array(x, dtype=float)\n        seq_len = x_arr.shape[0]\n        if x_arr.ndim != 2 or x_arr.shape[1] != self.input_size:\n            raise ValueError(f'x must be of shape (seq_len, {self.input_size})')\n        h_prev = np.array(initial_hidden_state, dtype=float)\n        if h_prev.ndim == 1:\n            h_prev = h_prev.reshape(-1, 1)\n        c_prev = np.array(initial_cell_state, dtype=float)\n        if c_prev.ndim == 1:\n            c_prev = c_prev.reshape(-1, 1)\n        if h_prev.shape != (self.hidden_size, 1) or c_prev.shape != (self.hidden_size, 1):\n            raise ValueError(f'Initial hidden/cell must be shape ({self.hidden_size},1)')\n        hidden_states = []\n        for t in range(seq_len):\n            x_t = x_arr[t].reshape(self.input_size, 1)\n            concat = np.vstack((h_prev, x_t))\n            f_t = self.sigmoid(self.Wf.dot(concat) + self.bf)\n            i_t = self.sigmoid(self.Wi.dot(concat) + self.bi)\n            c_tilde = np.tanh(self.Wc.dot(concat) + self.bc)\n            c_t = f_t * c_prev + i_t * c_tilde\n            o_t = self.sigmoid(self.Wo.dot(concat) + self.bo)\n            h_t = o_t * np.tanh(c_t)\n            hidden_states.append(h_t)\n            h_prev = h_t\n            c_prev = c_t\n        hidden_states_list = [np.round(h, 4).tolist() for h in hidden_states]\n        final_hidden = np.round(h_prev, 4).tolist()\n        final_cell = np.round(c_prev, 4).tolist()\n        return (hidden_states_list, final_hidden, final_cell)"}
{"task_id": 60, "completion_id": 0, "solution": ""}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels (0 or 1)\n    :param y_pred: Numpy array of predicted labels (0 or 1)\n    :param beta:  The weight of precision in the harmonic mean (beta >= 0)\n    :return:       F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.asarray(y_true).flatten()\n    y_pred = np.asarray(y_pred).flatten()\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    beta_sq = beta ** 2\n    denom = beta_sq * precision + recall\n    if denom == 0:\n        f_beta = 0.0\n    else:\n        f_beta = (1 + beta_sq) * (precision * recall) / denom\n    return round(f_beta, 3)"}
{"task_id": 62, "completion_id": 0, "solution": ""}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded to 8 decimals, as a list)\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).flatten()\n    m = b.size\n    if x0 is None:\n        x = np.zeros(m, dtype=float)\n    else:\n        x = np.array(x0, dtype=float).flatten()\n    r = b - A.dot(x)\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = A.dot(p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n    x_rounded = np.round(x, 8)\n    return x_rounded.tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y or len(set(y)) <= 1:\n        return 0.0\n    (classes, counts) = np.unique(y, return_counts=True)\n    probs = counts / counts.sum()\n    gini = 1.0 - np.sum(probs ** 2)\n    return round(float(gini), 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        nnz_in_row = 0\n        for (j, elem) in enumerate(row):\n            if elem != 0:\n                values.append(elem)\n                col_indices.append(j)\n                nnz_in_row += 1\n        row_pointer.append(row_pointer[-1] + nnz_in_row)\n    return (values, col_indices, row_pointer)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (list of numbers)\n    :param L: The line vector defining the direction of projection (list of numbers)\n    :return: List representing the projection of v onto L, rounded to three decimal places\n    \"\"\"\n    if len(v) != len(L):\n        raise ValueError('v and L must have the same dimension')\n    dot_vL = sum((vi * Li for (vi, Li) in zip(v, L)))\n    dot_LL = sum((Li * Li for Li in L))\n    if dot_LL == 0:\n        raise ValueError('The line vector L must be non-zero')\n    factor = dot_vL / dot_LL\n    projection = [round(factor * Li, 3) for Li in L]\n    return projection"}
{"task_id": 67, "completion_id": 0, "solution": ""}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (m, n) = A.shape\n    M = A.copy()\n    pivot_cols = []\n    row = 0\n    tol = 1e-12\n    for col in range(n):\n        if row >= m:\n            break\n        sub_col = np.abs(M[row:, col])\n        max_i = np.argmax(sub_col) + row\n        if abs(M[max_i, col]) < tol:\n            continue\n        M[[row, max_i]] = M[[max_i, row]]\n        pivot_cols.append(col)\n        pivot_val = M[row, col]\n        M[row] /= pivot_val\n        for i in range(row + 1, m):\n            M[i] -= M[i, col] * M[row]\n        row += 1\n    basis = A[:, pivot_cols]\n    basis = np.round(basis, 8)\n    return basis.tolist()"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the coefficient of determination, R-squared, for a set of true vs. predicted values.\n\n    Parameters:\n    - y_true: array-like of shape (n_samples,)\n    - y_pred: array-like of shape (n_samples,)\n\n    Returns:\n    - R\u00b2 value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    if ss_tot == 0:\n        return 1.0\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": ""}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual values and predictions.\n\n    Parameters:\n    ----------\n    y_true : array-like\n        Ground truth (correct) target values.\n    y_pred : array-like\n        Estimated target values.\n\n    Returns:\n    -------\n    float\n        The RMSE rounded to three decimal places.\n\n    Raises:\n    ------\n    TypeError\n        If inputs cannot be converted to numeric arrays.\n    ValueError\n        If arrays have mismatched shapes or are empty.\n    \"\"\"\n    try:\n        y_true_arr = np.asarray(y_true, dtype=float)\n        y_pred_arr = np.asarray(y_pred, dtype=float)\n    except Exception as e:\n        raise TypeError(f'Inputs must be array-like of numbers. Conversion error: {e}')\n    if y_true_arr.size == 0 or y_pred_arr.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true_arr.shape != y_pred_arr.shape:\n        raise ValueError(f'Shape mismatch: y_true has shape {y_true_arr.shape}, but y_pred has shape {y_pred_arr.shape}.')\n    diff = y_true_arr - y_pred_arr\n    mse = np.mean(diff ** 2)\n    result = np.sqrt(mse)\n    return float(np.round(result, 3))"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index (Intersection over Union) for binary labels.\n\n    Parameters:\n    - y_true: array-like of shape (n_samples,), ground truth binary labels (0 or 1)\n    - y_pred: array-like of shape (n_samples,), predicted binary labels (0 or 1)\n\n    Returns:\n    - Jaccard Index as a float rounded to three decimal places.\n      Returns 1.0 if both y_true and y_pred contain only zeros.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.logical_and(y_true == 1, y_pred == 1).sum()\n    union = np.logical_or(y_true == 1, y_pred == 1).sum()\n    if union == 0:\n        return 1.0\n    j_index = intersection / union\n    return round(j_index, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\nimport hashlib\ndef _stable_hash(x):\n    \"\"\"\n    Compute a stable integer hash for the string representation of x.\n    Uses MD5 and returns a 32\u2010bit integer.\n    \"\"\"\n    h = hashlib.md5(str(x).encode('utf-8')).hexdigest()\n    return int(h[-8:], 16)\ndef _make_hv(seed, dim):\n    \"\"\"\n    Generate a bipolar hypervector of dimension `dim`\n    using a fixed integer seed.\n    Entries are +1 or -1 with equal probability.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    return rng.choice([-1, 1], size=dim)\ndef _bind(hv1, hv2):\n    \"\"\"\n    Bind two bipolar hypervectors by elementwise multiplication.\n    \"\"\"\n    return hv1 * hv2\ndef _bundle(hv_list):\n    \"\"\"\n    Bundle (i.e. sum and bipolarize) a list of hypervectors.\n    After summation, components >= 0 go to +1, else -1.\n    \"\"\"\n    s = np.sum(hv_list, axis=0)\n    return np.where(s >= 0, 1, -1)\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Given:\n      - row: dict of {feature_name: value}\n      - dim: hypervector dimensionality\n      - random_seeds: dict of {feature_name: seed_int}\n    Returns:\n      - a composite bipolar hypervector (as a Python list)\n        representing the entire row.\n    \"\"\"\n    bound_hvs = []\n    for (feat, val) in row.items():\n        if feat not in random_seeds:\n            raise KeyError(f\"No seed provided for feature '{feat}'\")\n        seed = int(random_seeds[feat])\n        hv_feat = _make_hv(seed, dim)\n        val_hash = _stable_hash(val)\n        hv_val = _make_hv((seed ^ val_hash) & 4294967295, dim)\n        bound = _bind(hv_feat, hv_val)\n        bound_hvs.append(bound)\n    composite = _bundle(bound_hvs)\n    return composite.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for binary classification.\n    Assumes the two classes are labeled 0 (negative) and 1 (positive).\n\n    The returned matrix is:\n        [[TN, FP],\n         [FN, TP]]\n\n    where\n      TN = count of true negatives  (y_true=0, y_pred=0)\n      FP = count of false positives (y_true=0, y_pred=1)\n      FN = count of false negatives (y_true=1, y_pred=0)\n      TP = count of true positives  (y_true=1, y_pred=1)\n    \"\"\"\n    counts = Counter(((y_true, y_pred) for (y_true, y_pred) in data))\n    tn = counts[0, 0]\n    fp = counts[0, 1]\n    fn = counts[1, 0]\n    tp = counts[1, 1]\n    return [[tn, fp], [fn, tp]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 77, "completion_id": 0, "solution": ""}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate descriptive statistics for a list or NumPy array of numerical values.\n\n    Parameters:\n        data (list or np.ndarray): numerical data\n\n    Returns:\n        dict: {\n            'mean': float,\n            'median': float,\n            'mode': float or int,\n            'variance': float,\n            'standard_deviation': float,\n            '25th_percentile': float,\n            '50th_percentile': float,\n            '75th_percentile': float,\n            'interquartile_range': float\n        }\n    \"\"\"\n    arr = np.asarray(data, dtype=float)\n    if arr.size == 0:\n        raise ValueError('Input data must contain at least one value.')\n    mean = np.mean(arr)\n    median = np.median(arr)\n    counts = Counter(arr)\n    max_count = max(counts.values())\n    modes = [val for (val, cnt) in counts.items() if cnt == max_count]\n    mode = min(modes)\n    variance = np.var(arr, ddof=0)\n    std_dev = np.std(arr, ddof=0)\n    (p25, p50, p75) = np.percentile(arr, [25, 50, 75])\n    iqr = p75 - p25\n    result = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(std_dev, 4), '25th_percentile': round(p25, 4), '50th_percentile': round(p50, 4), '75th_percentile': round(p75, 4), 'interquartile_range': round(iqr, 4)}\n    return result"}
{"task_id": 79, "completion_id": 0, "solution": ""}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coeff = 1.0 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coeff * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError('k must be a non-negative integer')\n    if lam < 0:\n        raise ValueError('lam must be non-negative')\n    prob = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(prob, 5)"}
{"task_id": 82, "completion_id": 0, "solution": ""}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float or numpy scalar: The dot product of vec1 and vec2.\n    \n    Raises:\n        ValueError: If inputs are not 1D arrays or their lengths do not match.\n    \"\"\"\n    arr1 = np.asarray(vec1)\n    arr2 = np.asarray(vec2)\n    if arr1.ndim != 1 or arr2.ndim != 1:\n        raise ValueError('Both inputs must be 1D arrays.')\n    if arr1.shape[0] != arr2.shape[0]:\n        raise ValueError('Input vectors must have the same length.')\n    result = np.dot(arr1, arr2)\n    return result"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space\n    by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial\n                           features [1, x, x^2, ..., x^degree] of the corresponding\n                           data point, rounded to 8 decimal places. Returns an empty\n                           list if degree < 0.\n    \"\"\"\n    if degree < 0:\n        return []\n    x = np.array(data, dtype=float)\n    phi_matrix = np.vander(x, N=degree + 1, increasing=True)\n    phi_matrix = np.round(phi_matrix, 8)\n    return phi_matrix.tolist()"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of  1  (Overfitting),\n                           -1 (Underfitting),\n                            0 (Good fit).\n    \"\"\"\n    if not 0.0 <= training_accuracy <= 1.0:\n        raise ValueError(f'training_accuracy must be between 0 and 1. Got {training_accuracy}')\n    if not 0.0 <= test_accuracy <= 1.0:\n        raise ValueError(f'test_accuracy must be between 0 and 1. Got {test_accuracy}')\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    \"\"\"\n    Simplified GPT-2-like autoregressive text generator.\n    \"\"\"\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    (wte, wpe) = (params['wte'], params['wpe'])\n    d_model = wte.shape[1]\n    if not params['blocks']:\n        block = {}\n        block['ln_1'] = {'g': np.ones(d_model), 'b': np.zeros(d_model)}\n        block['attn'] = {'wq': np.random.randn(d_model, d_model), 'wk': np.random.randn(d_model, d_model), 'wv': np.random.randn(d_model, d_model), 'wo': np.random.randn(d_model, d_model)}\n        block['ln_2'] = {'g': np.ones(d_model), 'b': np.zeros(d_model)}\n        block['mlp'] = {'w1': np.random.randn(d_model, 4 * d_model), 'b1': np.zeros(4 * d_model), 'w2': np.random.randn(4 * d_model, d_model), 'b2': np.zeros(d_model)}\n        params['blocks'] = [block]\n    past_tokens = encoder.encode(prompt)\n    for _ in range(n_tokens_to_generate):\n        seq = past_tokens\n        seq_len = len(seq)\n        token_emb = wte[seq]\n        pos_emb = wpe[np.arange(seq_len)]\n        x = token_emb + pos_emb\n        for block in params['blocks']:\n            mu = x.mean(axis=-1, keepdims=True)\n            var = x.var(axis=-1, keepdims=True)\n            x_norm = (x - mu) / np.sqrt(var + 1e-05)\n            x_norm = x_norm * block['ln_1']['g'] + block['ln_1']['b']\n            (wq, wk, wv, wo) = block['attn'].values()\n            q = x_norm @ wq\n            k = x_norm @ wk\n            v = x_norm @ wv\n            scores = q @ k.T / np.sqrt(d_model)\n            mask = np.tril(np.ones_like(scores))\n            scores = scores * mask + (1 - mask) * -1000000000.0\n            attn_w = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n            attn_w = attn_w / attn_w.sum(axis=-1, keepdims=True)\n            attn_out = attn_w @ v\n            attn_proj = attn_out @ wo\n            x = x + attn_proj\n            mu2 = x.mean(axis=-1, keepdims=True)\n            var2 = x.var(axis=-1, keepdims=True)\n            x_norm2 = (x - mu2) / np.sqrt(var2 + 1e-05)\n            x_norm2 = x_norm2 * block['ln_2']['g'] + block['ln_2']['b']\n            (w1, b1) = (block['mlp']['w1'], block['mlp']['b1'])\n            (w2, b2) = (block['mlp']['w2'], block['mlp']['b2'])\n            ff = x_norm2 @ w1 + b1\n            ff = 0.5 * ff * (1 + np.tanh(np.sqrt(2 / np.pi) * (ff + 0.044715 * ff ** 3)))\n            ff = ff @ w2 + b2\n            x = x + ff\n        ln_f = params['ln_f']\n        mu_f = x.mean(axis=-1, keepdims=True)\n        var_f = x.var(axis=-1, keepdims=True)\n        x_normf = (x - mu_f) / np.sqrt(var_f + 1e-05)\n        x_normf = x_normf * ln_f['g'] + ln_f['b']\n        last_hidden = x_normf[-1]\n        logits = last_hidden @ wte.T\n        probs = np.exp(logits - np.max(logits))\n        probs = probs / probs.sum()\n        next_token = int(np.argmax(probs))\n        past_tokens.append(next_token)\n    return encoder.decode(past_tokens)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(x):\n        exps = np.exp(x - np.max(x))\n        return exps / np.sum(exps)\n    vals = np.array(crystal_values, dtype=float).reshape(n, 1)\n    embeddings = np.repeat(vals, dimension, axis=1)\n    scores = embeddings.dot(embeddings.T)\n    output = []\n    for i in range(n):\n        attn_weights = softmax(scores[i])\n        weighted_emb = attn_weights.dot(embeddings)\n        pattern_value = float(np.sum(weighted_emb))\n        output.append(round(pattern_value, 4))\n    return output"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter"}
{"task_id": 91, "completion_id": 0, "solution": ""}
{"task_id": 92, "completion_id": 0, "solution": "import math"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError(f'Shape mismatch: y_true has shape {y_true.shape}, but y_pred has shape {y_pred.shape}')\n    abs_errors = np.abs(y_true - y_pred)\n    mean_abs_error = np.mean(abs_errors)\n    return float(np.round(mean_abs_error, 3))"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Scaled dot-product self-attention for one head.\n    Arguments:\n      Q : (m, d) queries\n      K : (m, d) keys\n      V : (m, d) values\n    Returns:\n      out : (m, d) attended output\n    \"\"\"\n    (m, d) = Q.shape\n    scores = Q @ K.T / np.sqrt(d)\n    scores = scores - np.max(scores, axis=1, keepdims=True)\n    exp_scores = np.exp(scores)\n    attn = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    out = attn @ V\n    return out\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Multi-head attention.\n    Arguments:\n      Q      : (m, n) queries\n      K      : (m, n) keys\n      V      : (m, n) values\n      n_heads: number of attention heads\n    Returns:\n      output: (m, n) concatenated multi-head output\n    \"\"\"\n    (m, n) = Q.shape\n    if n % n_heads != 0:\n        raise ValueError(f'Embedding dimension n={n} not divisible by n_heads={n_heads}')\n    d_k = n // n_heads\n    Qh = Q.reshape(m, n_heads, d_k).transpose(1, 0, 2)\n    Kh = K.reshape(m, n_heads, d_k).transpose(1, 0, 2)\n    Vh = V.reshape(m, n_heads, d_k).transpose(1, 0, 2)\n    heads_out = []\n    for i in range(n_heads):\n        out_i = self_attention(Qh[i], Kh[i], Vh[i])\n        heads_out.append(out_i)\n    heads_stacked = np.stack(heads_out, axis=0)\n    heads_stacked = heads_stacked.transpose(1, 0, 2)\n    concat = heads_stacked.reshape(m, n)\n    return concat"}
{"task_id": 95, "completion_id": 0, "solution": "import math"}
{"task_id": 96, "completion_id": 0, "solution": ""}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function in a numerically stable way.\n\n    Softplus(x) = log(1 + e^x)\n    We rewrite it as:\n      softplus(x) = max(x, 0) + log1p(exp(-abs(x)))\n    to avoid overflow when x is large and underflow when x is very negative.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value rounded to 4 decimal places.\n    \"\"\"\n    res = max(x, 0.0) + math.log1p(math.exp(-abs(x)))\n    return round(res, 4)"}
{"task_id": 100, "completion_id": 0, "solution": ""}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value (rounded to 6 decimal places).\n    \"\"\"\n    rhos = np.array(rhos, dtype=np.float64)\n    A = np.array(A, dtype=np.float64)\n    pi_old = np.array(pi_theta_old, dtype=np.float64)\n    pi_ref = np.array(pi_theta_ref, dtype=np.float64)\n    clipped_rhos = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon)\n    surrogate_unclipped = rhos * A\n    surrogate_clipped = clipped_rhos * A\n    surrogate_terms = np.minimum(surrogate_unclipped, surrogate_clipped)\n    surrogate = np.mean(surrogate_terms)\n    tiny = 1e-12\n    kl_div = np.sum(pi_old * (np.log(pi_old + tiny) - np.log(pi_ref + tiny)))\n    obj = surrogate - beta * kl_div\n    return float(round(obj, 6))"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal.\n    \"\"\"\n    sigmoid = 1 / (1 + math.exp(-x))\n    swish_val = x * sigmoid\n    return round(swish_val, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression,\n    optimizing parameters with Binary Cross Entropy loss.\n\n    Parameters\n    ----------\n    X : np.ndarray of shape (m, n)\n        Feature matrix (without bias term).\n    y : np.ndarray of shape (m,)\n        Binary labels (0 or 1).\n    learning_rate : float\n        Step size for gradient descent.\n    iterations : int\n        Number of gradient descent steps.\n\n    Returns\n    -------\n    weights_list : list of float\n        Learned parameters [bias, w1, w2, ..., wn], rounded to 4 decimals.\n    losses_list : list of float\n        Binary cross-entropy loss at each iteration, rounded to 4 decimals.\n    \"\"\"\n    (m, n) = X.shape\n    X_bias = np.hstack((np.ones((m, 1)), X))\n    w = np.zeros(n + 1)\n    losses = []\n    for _ in range(iterations):\n        z = X_bias @ w\n        p = 1 / (1 + np.exp(-z))\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n        losses.append(loss)\n        grad = X_bias.T @ (p - y) / m\n        w -= learning_rate * grad\n    weights_list = np.round(w, 4).tolist()\n    losses_list = np.round(np.array(losses), 4).tolist()\n    return (weights_list, losses_list)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> list:\n    \"\"\"\n    Compute masked self-attention.\n    Q: (seq_len, d_k)\n    K: (seq_len, d_k)\n    V: (seq_len, d_v)\n    mask: (seq_len, seq_len) array of 1s and 0s,\n          where mask[i,j]==1 means position j is visible to i,\n          and 0 means it is masked out.\n\n    Returns:\n      A Python list of shape (seq_len, d_v) containing the attention output.\n    \"\"\"\n    (seq_len, d_k) = Q.shape\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    NEG_INF = -1000000000.0\n    scores = np.where(mask == 1, scores, NEG_INF)\n    max_per_row = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - max_per_row)\n    sum_exp = np.sum(exp_scores, axis=-1, keepdims=True)\n    attn_weights = exp_scores / sum_exp\n    output = np.dot(attn_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    We use Shannon entropy (in bits), which is 0 if all apples are the same color,\n    and increases as the distribution of colors becomes more uniform.\n    The result is rounded to the nearest 4th decimal place.\n    \"\"\"\n    n = len(apples)\n    if n == 0:\n        return 0.0\n    freq = Counter(apples)\n    entropy = 0.0\n    for count in freq.values():\n        p = count / n\n        entropy -= p * math.log2(p)\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Applies Layer Normalization over the last dimension of a 3D tensor.\n\n    Parameters:\n    - X: Input array of shape (batch_size, seq_length, feature_dim)\n    - gamma: Scale parameters of shape (feature_dim,)\n    - beta: Shift parameters of shape (feature_dim,)\n    - epsilon: Small constant for numerical stability\n\n    Returns:\n    - A Python list corresponding to the layer-normalized tensor,\n      rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.mean((X - mean) ** 2, axis=-1, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    out = gamma * X_norm + beta\n    return np.round(out, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score between a reference and a candidate translation.\n    \n    Args:\n        reference (str): The reference (ground truth) translation.\n        candidate (str): The candidate (machine) translation.\n        alpha (float): Weighting of precision vs. recall in the F-mean.\n        beta (float): Exponent in the fragmentation penalty.\n        gamma (float): Weight of the penalty.\n        \n    Returns:\n        float: METEOR score rounded to 3 decimal places.\n    \"\"\"\n    ref_tokens = reference.lower().split()\n    cand_tokens = candidate.lower().split()\n    ref_len = len(ref_tokens)\n    cand_len = len(cand_tokens)\n    ref_counts = Counter(ref_tokens)\n    cand_counts = Counter(cand_tokens)\n    matches = sum((min(ref_counts[w], cand_counts[w]) for w in ref_counts))\n    if matches == 0:\n        return 0.0\n    idxs = {}\n    for (i, w) in enumerate(ref_tokens):\n        idxs.setdefault(w, []).append(i)\n    aligned = []\n    for (j, w) in enumerate(cand_tokens):\n        if w in idxs and idxs[w]:\n            i = idxs[w].pop(0)\n            aligned.append((i, j))\n    chunks = 1\n    for k in range(1, len(aligned)):\n        (prev_i, prev_j) = aligned[k - 1]\n        (cur_i, cur_j) = aligned[k]\n        if not (cur_i == prev_i + 1 and cur_j == prev_j + 1):\n            chunks += 1\n    precision = matches / cand_len\n    recall = matches / ref_len\n    f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall)\n    penalty = gamma * (chunks / matches) ** beta\n    score = f_mean * (1 - penalty)\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max normalization on a list of integers,\n    scaling each value to the range [0, 1], and round to 4 decimal places.\n    If all values are identical, returns a list of 0.0s.\n    \"\"\"\n    if not x:\n        return []\n    min_x = min(x)\n    max_x = max(x)\n    range_x = max_x - min_x\n    if range_x == 0:\n        return [0.0 for _ in x]\n    normalized = [(val - min_x) / range_x for val in x]\n    return [round(v, 4) for v in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    A simple 1D residual block:\n      1) Linear layer: x @ w1\n      2) ReLU activation\n      3) Linear layer: (previous) @ w2\n      4) Add the original input x (shortcut)\n      5) Final ReLU\n      6) Round to 4 decimals and convert to list\n\n    Shapes:\n      - x: (D,)\n      - w1: (D, H)\n      - w2: (H, D)\n    Returns:\n      - output as a Python list of length D\n    \"\"\"\n    D = x.shape[0]\n    assert w1.shape[0] == D, f'w1 first dim must be {D}, got {w1.shape[0]}'\n    H = w1.shape[1]\n    assert w2.shape == (H, D), f'w2 must be shape ({H}, {D}), got {w2.shape}'\n\n    def relu(z: np.ndarray) -> np.ndarray:\n        return np.maximum(0, z)\n    hidden = relu(x.dot(w1))\n    transformed = hidden.dot(w2)\n    out = relu(transformed + x)\n    return np.round(out, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D feature map.\n\n    Parameters:\n    ----------\n    x : np.ndarray\n        Input array of shape (height, width, channels).\n\n    Returns:\n    -------\n    np.ndarray\n        1D array of shape (channels,), where each element is the average of\n        the corresponding height\u00d7width feature map.\n    \"\"\"\n    if not isinstance(x, np.ndarray):\n        raise TypeError(f'Expected input type np.ndarray but got {type(x)}')\n    if x.ndim != 3:\n        raise ValueError(f'Input must be a 3D array of shape (H, W, C), got shape {x.shape}')\n    return x.mean(axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Batch Normalization on a 4D input (B, C, H, W).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input array of shape (batch, channels, height, width).\n    gamma : np.ndarray\n        Scale parameters, shape (channels,).\n    beta : np.ndarray\n        Shift parameters, shape (channels,).\n    epsilon : float\n        Small constant for numerical stability.\n\n    Returns\n    -------\n    List\n        The batch-normalized output rounded to 4 decimal places, as a nested list.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    gamma = gamma.reshape(1, -1, 1, 1)\n    beta = beta.reshape(1, -1, 1, 1)\n    out = gamma * X_norm + beta\n    return np.round(out, 4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of the term c * x^n at the point x using the power rule.\n    Power rule: d/dx [c * x^n] = c * n * x^(n - 1)\n\n    Args:\n        c (float): coefficient of the term\n        x (float): point at which to evaluate the derivative\n        n (float): exponent of x in the term\n\n    Returns:\n        float: the derivative value at x, rounded to 4 decimal places\n    \"\"\"\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram\u2013Schmidt process.\n\n    Parameters:\n    - vectors: list of 2D vectors (each a list of 2 floats)\n    - tol: tolerance below which a vector is considered zero (default: 1e-10)\n\n    Returns:\n    - A list of orthonormal 2D vectors (each rounded to 4 decimal places and as a Python list)\n    \"\"\"\n    U = []\n    for v in vectors:\n        w = np.array(v, dtype=float)\n        for u in U:\n            proj = np.dot(w, u) * u\n            w = w - proj\n        norm_w = np.linalg.norm(w)\n        if norm_w > tol:\n            u_new = w / norm_w\n            U.append(u_new)\n    return [np.round(u, 4).tolist() for u in U]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors a and b.\n    The result is rounded to 4 decimal places and returned as a Python list.\n\n    Parameters:\n    a (array-like): 3-dimensional vector\n    b (array-like): 3-dimensional vector\n\n    Returns:\n    list: The cross product vector, rounded to 4 decimal places\n    \"\"\"\n    a_arr = np.array(a, dtype=float)\n    b_arr = np.array(b, dtype=float)\n    if a_arr.shape != (3,) or b_arr.shape != (3,):\n        raise ValueError('Both input vectors must be 3-dimensional.')\n    result = np.cross(a_arr, b_arr)\n    return np.round(result, 4).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve the linear system A x = b using Cramer's Rule.\n    Returns the solution vector x as a Python list rounded to 4 decimal places,\n    or -1 if the system has no unique solution (det(A) == 0).\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).flatten()\n    (n_rows, n_cols) = A.shape\n    if n_rows != n_cols or b.size != n_rows:\n        raise ValueError('Matrix A must be square and b must have matching dimensions.')\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-12:\n        return -1\n    x = np.zeros(n_rows, dtype=float)\n    for i in range(n_rows):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x[i] = det_Ai / det_A\n    x = np.round(x, 4).tolist()\n    return x"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 121, "completion_id": 0, "solution": ""}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> list:\n    \"\"\"\n    Compute the REINFORCE policy gradient estimator.\n    \n    Args:\n        theta: array of shape (num_states, num_actions), policy parameters.\n        episodes: list of episodes; each episode is a list of (state, action, reward) tuples.\n    \n    Returns:\n        Gradient array of same shape as theta, rounded to 4 decimals, as a nested Python list.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    shifted = theta - np.max(theta, axis=1, keepdims=True)\n    exp_vals = np.exp(shifted)\n    policy = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n    grad = np.zeros_like(theta, dtype=float)\n    num_episodes = len(episodes)\n    for ep in episodes:\n        rewards = [r for (_, _, r) in ep]\n        T = len(rewards)\n        returns = [sum(rewards[t:]) for t in range(T)]\n        for ((t_step, (s, a, _)), G) in zip(enumerate(ep), returns):\n            grad[s] += G * -policy[s]\n            grad[s, a] += G\n    grad /= num_episodes\n    grad = np.round(grad, 4)\n    return grad.tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Compute the FLOPs for a dense layer of equivalent total capacity vs.\n    a sparsely-gated MoE layer, and return the savings percentage.\n\n    Parameters:\n    - n_experts (int): total number of experts in the MoE\n    - k_active (int): number of experts selected/active per example\n    - d_in (int): input feature dimension\n    - d_out (int): output feature dimension\n\n    Returns:\n    A tuple of three floats (all rounded to 1 decimal place):\n    - dense_flops: FLOPs of a dense layer with capacity = n_experts * (d_in\u2192d_out)\n    - moe_flops: FLOPs of the MoE layer with k_active experts\n    - savings_pct: percentage FLOP savings of MoE vs. the dense layer\n    \"\"\"\n    flops_per_expert = 2 * d_in * d_out\n    dense_flops = n_experts * flops_per_expert\n    moe_flops = k_active * flops_per_expert\n    savings_pct = (dense_flops - moe_flops) / dense_flops * 100.0\n    dense_flops = round(dense_flops, 1)\n    moe_flops = round(moe_flops, 1)\n    savings_pct = round(savings_pct, 1)\n    return (dense_flops, moe_flops, savings_pct)"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism for Mixture-of-Experts.\n    \n    Arguments:\n    X       -- input matrix of shape (batch_size, input_dim)\n    W_g     -- gating weight matrix of shape (input_dim, num_experts)\n    W_noise -- noise-weight matrix of shape (input_dim, num_experts)\n    N       -- pre-sampled noise of shape (batch_size, num_experts)\n    k       -- number of experts to select per example\n    \n    Returns:\n    A Python list of shape (batch_size, num_experts) containing the\n    final gating probabilities (rounded to 4 decimal places).\n    \"\"\"\n    clean_logits = X.dot(W_g)\n    noise_scale = X.dot(W_noise)\n    noisy_logits = clean_logits + N * noise_scale\n    topk_indices = np.argpartition(noisy_logits, -k, axis=1)[:, -k:]\n    batch_idx = np.arange(X.shape[0])[:, None]\n    topk_logits = noisy_logits[batch_idx, topk_indices]\n    max_logits = np.max(topk_logits, axis=1, keepdims=True)\n    exp_shifted = np.exp(topk_logits - max_logits)\n    sum_exp = np.sum(exp_shifted, axis=1, keepdims=True)\n    softmax_topk = exp_shifted / sum_exp\n    (B, E) = noisy_logits.shape\n    gates = np.zeros((B, E), dtype=softmax_topk.dtype)\n    gates[batch_idx, topk_indices] = softmax_topk\n    gates = np.round(gates, 4)\n    return gates.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Sparse Mixture of Experts layer.\n    Args:\n        x: input array of shape (batch_size, d_in)\n        We: expert weight tensor of shape (n_experts, d_in, d_out)\n        Wg: gating weight matrix of shape (d_in, n_experts)\n        n_experts: total number of experts\n        top_k: number of experts to select per token\n    Returns:\n        A Python list of shape (batch_size, d_out) with values rounded to 4 decimals.\n    \"\"\"\n    logits = x.dot(Wg)\n    max_logits = np.max(logits, axis=1, keepdims=True)\n    exp_logits = np.exp(logits - max_logits)\n    sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n    probs = exp_logits / sum_exp\n    batch_size = x.shape[0]\n    d_out = We.shape[2]\n    outputs = np.zeros((batch_size, d_out))\n    for i in range(batch_size):\n        prob_i = probs[i]\n        top_indices = np.argsort(prob_i)[-top_k:][::-1]\n        top_probs = prob_i[top_indices]\n        We_k = We[top_indices]\n        y_e = np.tensordot(We_k, x[i], axes=([1], [0]))\n        weighted = top_probs[:, None] * y_e\n        outputs[i] = np.sum(weighted, axis=0)\n    outputs = np.round(outputs, 4)\n    return outputs.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D tensor.\n    \n    Args:\n      X          : Input tensor of shape (B, C, H, W)\n      gamma      : Scale parameter of shape (C,)\n      beta       : Shift parameter of shape (C,)\n      num_groups : Number of groups to split the channels into\n      epsilon    : Small constant for numerical stability\n      \n    Returns:\n      A nested Python list (B x C x H x W) of the normalized, scaled and shifted output,\n      with values rounded to 4 decimal places.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'C must be divisible by num_groups'\n    G = num_groups\n    X_grouped = X.reshape(B, G, C // G, H, W)\n    mean = X_grouped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_grouped.var(axis=(2, 3, 4), keepdims=True)\n    X_norm = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    Y = X_norm * gamma + beta\n    Y = np.round(Y, 4)\n    return Y.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) activation \u2013 a normalization\u2010free variant of tanh.\n    Here we parameterize it as:\n        y = gamma * tanh(alpha * x) + beta\n    which preserves the conventional tanh squashing but allows\n    trainable scale (gamma) and shift (beta) without any mean\u2010variance norm.\n    \n    Args:\n        x     (np.ndarray): input array of any shape\n        alpha (float)     : input\u2010scale hyperparameter\n        gamma (float)     : output\u2010scale (gain) hyperparameter\n        beta  (float)     : output\u2010bias hyperparameter\n    \n    Returns:\n        List[float]: the activation output, rounded to 4 decimal places\n                     and converted to nested Python lists.\n    \"\"\"\n    y = gamma * np.tanh(alpha * x) + beta\n    y_rounded = np.round(y, 4)\n    return y_rounded.tolist()"}
