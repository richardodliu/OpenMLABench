{"task_id": 0, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"\n    Compute Ridge regression weights using the closed-form solution.\n    \n    Parameters\n    ----------\n    data : np.ndarray\n        A 2D array where the last column is the target vector y\n        and the preceding columns are the feature matrix X.\n    alpha : float, default=0.0\n        Regularization strength (non-negative).\n    \n    Returns\n    -------\n    list[float]\n        Weight vector w (including bias) rounded to 4 decimal places.\n        If the regularized matrix is singular (determinant zero),\n        returns -1.\n    \"\"\"\n    # Split features and target\n    X = data[:, :-1]\n    y = data[:, -1]\n    \n    # Add bias (intercept) term\n    n_samples = X.shape[0]\n    X_bias = np.hstack([np.ones((n_samples, 1)), X])\n    \n    # Build regularized normal matrix\n    # Identity matrix of size = number of columns in X_bias\n    p = X_bias.shape[1]\n    I = np.eye(p)\n    A = X_bias.T @ X_bias + alpha * I\n    \n    # Check invertibility via determinant\n    detA = np.linalg.det(A)\n    if detA == 0:\n        return -1\n    \n    # Closed-form solution: w = (X^T X + alpha I)^{-1} X^T y\n    w = np.linalg.inv(A) @ (X_bias.T @ y)\n    \n    # Round to 4 decimal places and return as a Python list\n    w_rounded = np.round(w, 4).tolist()\n    return w_rounded\n```"}
{"task_id": 1, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or \u20111) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of updates, by default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        The weight vector (as a list) and the bias; both rounded to 4 decimals.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    # Initialize dual parameters and bias\n    alpha = np.zeros(n_samples, dtype=float)\n    b = 0.0\n\n    # Precompute Gram matrix\n    K = X_train @ X_train.T\n\n    updates = 0\n    i = 0\n    # Loop until one full pass with no updates or until n_iter updates\n    while i < n_samples and updates < n_iter:\n        # compute activation for sample i\n        activation = np.sum(alpha * y_train * K[:, i])\n        margin = y_train[i] * (activation + b)\n        if margin <= 0:\n            # misclassified, perform dual update\n            alpha[i] += eta\n            b += eta * y_train[i]\n            updates += 1\n            # restart pass\n            i = 0\n        else:\n            # correct, move to next\n            i += 1\n\n    # Compute primal weight vector from dual parameters\n    # w = sum_i alpha_i * y_i * x_i\n    w = np.zeros(n_features, dtype=float)\n    for idx in range(n_samples):\n        if alpha[idx] != 0:\n            w += alpha[idx] * y_train[idx] * X_train[idx]\n\n    # Round the results\n    w_rounded = [round(float(w_j), 4) for w_j in w]\n    b_rounded = round(float(b), 4)\n    return w_rounded, b_rounded\n\n# Example usage\nif __name__ == \"__main__\":\n    # A small toy dataset (AND logic)\n    X = np.array([[0, 0],\n                  [0, 1],\n                  [1, 0],\n                  [1, 1]], dtype=float)\n    y = np.array([-1, -1, -1, +1], dtype=int)\n\n    w, b = perceptron_dual(X, y, eta=1.0, n_iter=1000)\n    print(\"Learned w:\", w)\n    print(\"Learned b:\", b)\n```\n\nExplanation:\n1. We initialize the dual coefficients `alpha` and bias `b` to zero.\n2. We precompute the Gram matrix `K = X @ X.T` to avoid repeated dot products.\n3. We iterate over the training set, computing the activation for each sample `i` via\n       activation = \u03a3\u2c7c \u03b1\u2c7c y\u2c7c K(x\u2c7c, x\u1d62)\n   and check the margin `y\u1d62(activation + b)`.\n4. On a misclassification (margin \u2264 0), we update `\u03b1\u1d62` and `b`, increment the update counter, and restart the scan.\n5. We stop once we make a full pass with no updates (`i` reaches `n_samples`) or exceed `n_iter` updates.\n6. Finally, we recover the primal weight vector `w = \u03a3\u1d62 \u03b1\u1d62 y\u1d62 x\u1d62`, round all components to four decimals, and return `(w, b)`."}
{"task_id": 2, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary ('identity', 'log', 'logit') where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions work with numpy scalars\n        as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    # Identity link\n    def identity_link(mu):\n        return mu\n\n    def identity_inv(eta):\n        return eta\n\n    def identity_derivative(mu):\n        return np.ones_like(mu)\n\n    # Log link\n    def log_link(mu):\n        return np.log(mu)\n\n    def log_inv(eta):\n        return np.exp(eta)\n\n    def log_derivative(mu):\n        return 1.0 / mu\n\n    # Logit link\n    def logit_link(mu):\n        return np.log(mu / (1.0 - mu))\n\n    def logit_inv(eta):\n        return 1.0 / (1.0 + np.exp(-eta))\n\n    def logit_derivative(mu):\n        return 1.0 / (mu * (1.0 - mu))\n\n    return {\n        \"identity\": {\n            \"link\":       identity_link,\n            \"inv_link\":   identity_inv,\n            \"derivative\": identity_derivative\n        },\n        \"log\": {\n            \"link\":       log_link,\n            \"inv_link\":   log_inv,\n            \"derivative\": log_derivative\n        },\n        \"logit\": {\n            \"link\":       logit_link,\n            \"inv_link\":   logit_inv,\n            \"derivative\": logit_derivative\n        }\n    }\n\n\nif __name__ == \"__main__\":\n    links = glm_links()\n\n    # Example for logit\n    mu = np.array([0.2, 0.8])\n    eta = links[\"logit\"][\"link\"](mu)\n    mu2 = links[\"logit\"][\"inv_link\"](eta)\n    dlogit = links[\"logit\"][\"derivative\"](mu)\n\n    print(\"logit link( [0.2, 0.8] )      =\", np.round(eta, 4))\n    print(\"logit inv_link( eta )        =\", np.round(mu2, 4))\n    print(\"logit derivative( [0.2, 0.8] )=\", np.round(dlogit, 4))\n\n    # Example for identity\n    mu_id = np.array([1.5, -0.5])\n    eid = links[\"identity\"][\"link\"](mu_id)\n    mu_id2 = links[\"identity\"][\"inv_link\"](eid)\n    did = links[\"identity\"][\"derivative\"](mu_id)\n\n    print(\"\\nidentity link( [1.5, -0.5] )      =\", np.round(eid, 4))\n    print(\"identity inv_link( above )        =\", np.round(mu_id2, 4))\n    print(\"identity derivative( [1.5, -0.5] )=\", np.round(did, 4))\n\n    # Example for log\n    mu_p = np.array([0.5, 2.0])\n    el = links[\"log\"][\"link\"](mu_p)\n    mu_p2 = links[\"log\"][\"inv_link\"](el)\n    dl = links[\"log\"][\"derivative\"](mu_p)\n\n    print(\"\\nlog link( [0.5, 2.0] )      =\", np.round(el, 4))\n    print(\"log inv_link( above )       =\", np.round(mu_p2, 4))\n    print(\"log derivative( [0.5, 2.0] )=\", np.round(dl, 4))\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed is fixed to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length n_samples where the i-th element is the number of\n        edges from the root to the leaf that contains the i-th sample.\n    \"\"\"\n    # fix randomness\n    np.random.seed(0)\n\n    n_samples, n_features = data.shape\n    # this will hold the final depths for each sample\n    depths = [-1] * n_samples\n\n    def recurse(indices: list[int], depth: int) -> None:\n        # If fewer than 3 samples, this is a leaf:\n        if len(indices) < 3:\n            for idx in indices:\n                depths[idx] = depth\n            return\n\n        # 1) choose a random feature\n        f = np.random.randint(0, n_features)\n\n        # 2) find the min and max on that feature among the current samples\n        vals = data[indices, f]\n        down, up = vals.min(), vals.max()\n\n        # 3) choose a split value uniformly in [down, up]\n        v = np.random.uniform(down, up)\n\n        # 4) partition into left and right\n        left_indices  = [i for i in indices if data[i, f] <= v]\n        right_indices = [i for i in indices if data[i, f] >  v]\n\n        # recurse on both children, increasing depth by 1\n        recurse(left_indices,  depth + 1)\n        recurse(right_indices, depth + 1)\n\n    # start recursion from the root at depth 0\n    recurse(list(range(n_samples)), 0)\n    return depths\n\n# Example usage:\nif __name__ == \"__main__\":\n    import numpy as np\n    data = np.arange(5).reshape(-1, 1)\n    print(isolation_tree_path_length(data))  # Output: [2, 2, 2, 1, 1]\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function returns an empty dictionary.\n    \"\"\"\n    # Number of transactions\n    N = len(transactions)\n    if N == 0:\n        return {}\n    # Absolute minimum support count\n    min_cnt = math.ceil(minsup * N)\n    # Convert each transaction to a set to ignore duplicates\n    txn_sets = [set(txn) for txn in transactions]\n\n    # ------------------------------------------------------------------------\n    # 1) Find frequent 1-itemsets\n    # ------------------------------------------------------------------------\n    item_counter = Counter()\n    for t in txn_sets:\n        item_counter.update(t)\n    # Keep only those items meeting the threshold\n    L1 = [item for item, cnt in item_counter.items() if cnt >= min_cnt]\n    # Sort to have a consistent order\n    L1.sort()\n    # Map of frequent itemsets to their supports\n    freq_itemsets: Dict[Tuple[Hashable, ...], int] = {}\n    # Record 1-itemsets\n    prev_L = []\n    for item in L1:\n        freq_itemsets[(item,)] = item_counter[item]\n        prev_L.append((item,))\n\n    k = 2\n    # ------------------------------------------------------------------------\n    # 2) Iteratively build larger itemsets (Apriori)\n    # ------------------------------------------------------------------------\n    while prev_L:\n        # Generate candidate k-itemsets from (k-1)-itemsets\n        candidates = set()\n        prev_L_sorted = prev_L  # already sorted tuples\n        m = len(prev_L_sorted)\n        # Join step\n        for i in range(m):\n            for j in range(i+1, m):\n                t1, t2 = prev_L_sorted[i], prev_L_sorted[j]\n                if t1[:k-2] == t2[:k-2]:\n                    # join\n                    new_cand = tuple(sorted(t1 + (t2[-1],)))\n                    candidates.add(new_cand)\n        # Prune step: remove candidates with any (k-1)-subset not frequent\n        prev_L_set = set(prev_L)\n        pruned_cands = []\n        for c in candidates:\n            all_subsets_frequent = True\n            for subset in combinations(c, k-1):\n                if subset not in prev_L_set:\n                    all_subsets_frequent = False\n                    break\n            if all_subsets_frequent:\n                pruned_cands.append(c)\n\n        # Count supports for candidates\n        cand_counter = Counter()\n        for t in txn_sets:\n            for c in pruned_cands:\n                if set(c).issubset(t):\n                    cand_counter[c] += 1\n\n        # Select those meeting minsup\n        next_L = []\n        for c, cnt in cand_counter.items():\n            if cnt >= min_cnt:\n                freq_itemsets[c] = cnt\n                next_L.append(c)\n\n        # Prepare for next iteration\n        prev_L = sorted(next_L)\n        k += 1\n\n    return freq_itemsets\n\n# Example usage\nif __name__ == \"__main__\":\n    transactions = [\n        ['bread', 'milk'],\n        ['bread', 'diaper', 'beer', 'egg'],\n        ['milk', 'diaper', 'beer', 'coke'],\n        ['bread', 'milk', 'diaper', 'beer'],\n        ['bread', 'milk', 'diaper', 'coke']\n    ]\n    minsup = 0.6\n    result = find_frequent_itemsets(transactions, minsup)\n    # Print the result in a sorted manner\n    for itemset in sorted(result):\n        print(f\"{itemset}: {result[itemset]}\")\n```\n\nExplanation of the main steps:\n\n1. **Preprocessing**:  \n   - Convert each transaction to a `set` to ignore duplicates within the same transaction.\n   - Compute the absolute support threshold `min_cnt = ceil(minsup * N)`.\n\n2. **Frequent 1-itemsets**:  \n   - Count each item across all transactions.\n   - Keep only those items whose counts \u2265 `min_cnt`.\n\n3. **Apriori Loop** (for k = 2, 3, \u2026):  \n   - **Join step**: Pair up frequent (k\u22121)-itemsets that share the first k\u22122 items and form k-item candidates.\n   - **Prune step**: Eliminate any candidate for which at least one (k\u22121)-subset is not frequent.\n   - **Count supports** by scanning all transactions and checking subset inclusion.\n   - **Filter** candidates to keep only those meeting the support threshold.\n   - Repeat until no new frequent k-itemsets are found.\n\nAll frequent itemsets (of all sizes) and their absolute supports are collected in a single dictionary and returned."}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    # number of samples and features\n    n_samples, n_features = data.shape\n\n    # initialize centers as the first k samples\n    centers = data[:k].astype(float).copy()\n\n    # placeholder for labels\n    labels = np.zeros(n_samples, dtype=int)\n\n    for _ in range(max_iter):\n        # 1) assign each sample to the nearest center (squared Euclidean distance)\n        # distances shape: (n_samples, k)\n        diffs = data[:, np.newaxis, :] - centers[np.newaxis, :, :]  # (n_samples, k, n_features)\n        dists = np.sum(diffs**2, axis=2)                             # (n_samples, k)\n        labels = np.argmin(dists, axis=1)                            # (n_samples,)\n\n        # 2) recompute centers\n        new_centers = np.zeros_like(centers)\n        for i in range(k):\n            assigned = data[labels == i]\n            if assigned.shape[0] > 0:\n                # mean of assigned points\n                new_centers[i] = assigned.mean(axis=0)\n            else:\n                # empty cluster: reinitialize at a random sample\n                rand_idx = np.random.randint(0, n_samples)\n                new_centers[i] = data[rand_idx]\n\n        # 3) check convergence (maximum movement of any center)\n        shifts = np.linalg.norm(new_centers - centers, axis=1)\n        if np.max(shifts) < epsilon:\n            centers = new_centers\n            break\n\n        centers = new_centers\n\n    # 4) round centers to 4 decimal places and convert to Python lists\n    centers_rounded = np.round(centers, 4).tolist()\n    labels_list = labels.tolist()\n\n    return centers_rounded, labels_list\n```"}
{"task_id": 13, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function builds an item\u2013item similarity matrix and then predicts\n    the attractiveness of every yet unrated item for the specified user.\n    It finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    n_users, n_items = data.shape\n\n    # 1. Build item\u2013item similarity matrix S\n    S = np.zeros((n_items, n_items), dtype=float)\n\n    for i in range(n_items):\n        for j in range(i + 1, n_items):\n            # users who rated both items\n            mask = (data[:, i] > 0) & (data[:, j] > 0)\n            if not np.any(mask):\n                # no common raters\n                continue\n            v1 = data[mask, i].astype(float)\n            v2 = data[mask, j].astype(float)\n\n            if criterion == 'cosine':\n                # mean-center each vector if std > 1e-3\n                if v1.std(ddof=0) > 1e-3:\n                    v1 = v1 - v1.mean()\n                if v2.std(ddof=0) > 1e-3:\n                    v2 = v2 - v2.mean()\n                num = np.dot(v1, v2)\n                denom = np.linalg.norm(v1) * np.linalg.norm(v2)\n                sim = num / denom if denom > 0 else 0.0\n\n            elif criterion == 'pearson':\n                # Pearson correlation\n                if v1.size < 2:\n                    sim = 0.0\n                else:\n                    corr = np.corrcoef(v1, v2)[0, 1]\n                    sim = 0.0 if np.isnan(corr) else corr\n            else:\n                raise ValueError(\"criterion must be 'cosine' or 'pearson'\")\n\n            S[i, j] = sim\n            S[j, i] = sim\n\n    # 2. Items rated by active user\n    user_ratings = data[user_ind, :]\n    rated_inds = np.where(user_ratings > 0)[0]\n    unrated_inds = np.where(user_ratings == 0)[0]\n\n    # if user rated all items, nothing to recommend\n    if unrated_inds.size == 0:\n        return []\n\n    # 3. Predict attractiveness for each unrated item\n    scores = {}\n    for t in unrated_inds:\n        sims = S[t, rated_inds]\n        r = user_ratings[rated_inds]\n        numerator = np.dot(r, sims)\n        denominator = np.sum(np.abs(sims))\n        score = numerator / denominator if denominator > 0 else 0.0\n        scores[t] = score\n\n    # 4. Select top-k items by descending score, tie-breaking on smaller index\n    recommended = sorted(scores.keys(), key=lambda x: (-scores[x], x))\n\n    # 5. Return at most k recommendations\n    return recommended[:k]\n```\n\nExplanation of key steps:\n\n1. **Similarity matrix**  \n   For each unordered pair of items `(i, j)`, we select users who rated both and extract their rating vectors `v1` and `v2`.  \n   - If `criterion=='cosine'`, we mean\u2010center each vector only if its (population) standard deviation exceeds `1e-3`, then compute cosine similarity.  \n   - If `criterion=='pearson'`, we compute the Pearson correlation via `np.corrcoef` (falling back to 0 on `nan`).\n\n2. **Prediction**  \n   For each item `t` that the active user hasn\u2019t rated, we compute  \n   ```\n   score(t) = sum_i r_i * S[t,i] / sum_i |S[t,i]|\n   ```  \n   over all items `i` the user has rated. If the denominator is zero, the score is set to zero.\n\n3. **Recommendation**  \n   We sort the unrated items by descending score (and by ascending item index for ties) and return up to `k` item indices."}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* containing the feature\n        values of all training instances that reach the current node.\n    g : np.ndarray\n        1-D array with the first-order gradients for each training instance.\n    h : np.ndarray\n        1-D array with the second-order gradients (Hessians).\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to the denominator when computing the\n        weight of a leaf.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        A pair *(feature_index, threshold)* describing the optimal split, or\n        ``None`` if no valid split yields a positive gain.\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Precompute total gradients/hessians\n    G = g.sum()\n    H = h.sum()\n    # To store the best found split\n    best_gain = 0.0\n    best_feat = None\n    best_thr = None\n\n    # Helper to compute score contribution of a node\n    # half * G^2 / (H + lam)\n    def node_score(G_sub, H_sub):\n        return 0.5 * (G_sub * G_sub) / (H_sub + lam)\n\n    # Parent score (no split)\n    score_parent = node_score(G, H)\n\n    # Iterate over features\n    for feat in range(n_features):\n        # Sort samples by current feature\n        order = np.argsort(X[:, feat], kind=\"mergesort\")\n        x_sorted = X[order, feat]\n        g_sorted = g[order]\n        h_sorted = h[order]\n\n        # Prefix sums of gradients and hessians\n        G_cum = np.cumsum(g_sorted)\n        H_cum = np.cumsum(h_sorted)\n\n        # Try splits only where the feature value changes\n        # and both children have at least 2 samples\n        for i in range(1, n_samples):\n            # Must leave at least 2 samples on each side\n            if i < 2 or (n_samples - i) < 2:\n                continue\n            # Only consider split when x[i-1] != x[i]\n            if x_sorted[i - 1] == x_sorted[i]:\n                continue\n\n            G_L = G_cum[i - 1]\n            H_L = H_cum[i - 1]\n            G_R = G - G_L\n            H_R = H - H_L\n\n            # Compute child scores\n            score_L = node_score(G_L, H_L)\n            score_R = node_score(G_R, H_R)\n\n            # Gain = score_L + score_R - score_parent - gamma\n            gain = score_L + score_R - score_parent - gamma\n\n            if gain > best_gain:\n                best_gain = gain\n                best_feat = feat\n                # Threshold = value up to which goes to the left\n                best_thr = x_sorted[i - 1]\n\n    if best_feat is None:\n        return None\n    else:\n        return (best_feat, float(best_thr))\n```"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is a Python int or float\n        \u2022 Python list when *x* is array-like (list or ndarray)\n    \"\"\"\n    # Determine if the input is a Python scalar (int or float)\n    is_scalar = isinstance(x, (int, float))\n\n    # Convert input to a NumPy array for vectorized computation\n    arr = np.array(x, dtype=float)\n\n    # Compute sigmoid\n    sigmoid_arr = 1.0 / (1.0 + np.exp(-arr))\n    # Compute gradient: sigmoid(x) * (1 - sigmoid(x))\n    grad_arr = sigmoid_arr * (1.0 - sigmoid_arr)\n\n    # Round results to 4 decimal places\n    sigmoid_arr = np.round(sigmoid_arr, 4)\n    grad_arr = np.round(grad_arr, 4)\n\n    if is_scalar:\n        # For Python scalars, return Python floats\n        sigmoid_x = float(sigmoid_arr)\n        gradient_x = float(grad_arr)\n    else:\n        # For array-like inputs, return nested Python lists\n        sigmoid_x = sigmoid_arr.tolist()\n        gradient_x = grad_arr.tolist()\n\n    return sigmoid_x, gradient_x\n\n# Example usages:\nif __name__ == \"__main__\":\n    # Scalar input\n    s_val, g_val = sigmoid_activation(0)\n    print(\"Scalar input 0 -> sigmoid:\", s_val, \"gradient:\", g_val)\n    \n    # Python list\n    s_list, g_list = sigmoid_activation([-1, 0, 1, 2])\n    print(\"List input [-1,0,1,2] -> sigmoid:\", s_list, \"gradient:\", g_list)\n\n    # NumPy array\n    arr = np.array([[0.5, -0.5], [2.0, -2.0]])\n    s_arr, g_arr = sigmoid_activation(arr)\n    print(\"Array input [[0.5,-0.5],[2,-2]] -> sigmoid:\", s_arr)\n    print(\"                                 gradient:\", g_arr)\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\n\n    Parameters\n    ----------\n    x_train : list[float]\n        Training sample values (one-dimensional).\n    y_train : list[int]\n        Labels corresponding to *x_train* (each value must be 1 or \u20131).\n    x_test : list[float]\n        Sample values to classify after training.\n    epsilon : float, default 0.0\n        Upper bound on the allowed training error.  Training stops once\n        the ensemble\u2019s training error \u2264 *epsilon*.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (1 or \u20131) for every value in *x_test*.\n    \"\"\"\n    # Convert to numpy arrays\n    x_train = np.array(x_train, dtype=float)\n    y_train = np.array(y_train, dtype=int)\n    n = x_train.shape[0]\n    # Initial uniform weights\n    w = np.full(n, 1.0 / n, dtype=float)\n    # Sort training data (needed for threshold enumeration)\n    sort_idx = np.argsort(x_train)\n    x_sorted = x_train[sort_idx]\n    y_sorted = y_train[sort_idx]\n    w = w[sort_idx]\n    # Possible thresholds: midpoints between consecutive sorted points\n    thresholds = [\n        0.5 * (x_sorted[i] + x_sorted[i + 1])\n        for i in range(n - 1)\n    ]\n    # Storage for selected stumps and their coefficients\n    stumps = []   # list of (d, theta)\n    alphas = []   # list of alpha\n\n    # Training loop\n    train_err = 1.0\n    while train_err > epsilon:\n        best_error = float('inf')\n        best_stump = None\n        best_preds = None\n\n        # Enumerate all candidate stumps\n        for theta in thresholds:\n            # d = 0: predict +1 if x <= theta, else -1\n            preds0 = np.where(x_sorted <= theta, 1, -1)\n            err0 = np.sum(w * (preds0 != y_sorted))\n            if err0 < best_error:\n                best_error = err0\n                best_stump = (0, theta)\n                best_preds = preds0\n            # d = 1: predict +1 if x > theta, else -1\n            preds1 = np.where(x_sorted > theta, 1, -1)\n            err1 = np.sum(w * (preds1 != y_sorted))\n            if err1 < best_error:\n                best_error = err1\n                best_stump = (1, theta)\n                best_preds = preds1\n\n        # Retrieve best stump and its weighted error\n        d, theta = best_stump\n        err = best_error\n\n        # Stabilize error to avoid division by zero or log of zero\n        err_mod = min(max(err, 1e-10), 1 - 1e-10)\n        alpha = 0.5 * math.log((1 - err_mod) / err_mod)\n\n        # Save stump and its weight\n        stumps.append((d, theta))\n        alphas.append(alpha)\n\n        # Update sample weights\n        # w_i <- w_i * exp(-alpha * y_i * h(x_i))\n        w = w * np.exp(-alpha * y_sorted * best_preds)\n        w /= np.sum(w)\n\n        # Compute ensemble predictions on the training set\n        agg = np.zeros(n)\n        for (dd, th), aa in zip(stumps, alphas):\n            if dd == 0:\n                h_pred = np.where(x_sorted <= th, 1, -1)\n            else:\n                h_pred = np.where(x_sorted > th, 1, -1)\n            agg += aa * h_pred\n        # Final predicted labels (break ties as +1)\n        y_agg_pred = np.sign(agg)\n        y_agg_pred[y_agg_pred == 0] = 1\n\n        # Compute training error\n        train_err = np.mean(y_agg_pred != y_sorted)\n\n        # Stop if we've reached desired training error\n        if train_err <= epsilon:\n            break\n\n    # Now predict on x_test\n    x_test_arr = np.array(x_test, dtype=float)\n    m = x_test_arr.shape[0]\n    agg_test = np.zeros(m)\n    for (dd, th), aa in zip(stumps, alphas):\n        if dd == 0:\n            h_test = np.where(x_test_arr <= th, 1, -1)\n        else:\n            h_test = np.where(x_test_arr > th, 1, -1)\n        agg_test += aa * h_test\n\n    y_test_pred = np.sign(agg_test)\n    y_test_pred[y_test_pred == 0] = 1\n\n    return y_test_pred.astype(int).tolist()\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    X_train = np.asarray(X_train)\n    y_train = np.asarray(y_train)\n    alpha   = np.asarray(alpha)\n    X_test  = np.asarray(X_test)\n\n    # Precompute combined weights (alpha_i * y_i)\n    coeffs = alpha * y_train  # shape: (n,)\n\n    # Compute squared Euclidean distances between each test and train point:\n    # shape: (m, n)\n    # Using the identity: ||u-v||^2 = ||u||^2 + ||v||^2 - 2 u\u00b7v\n    X_test_sq = np.sum(X_test**2, axis=1)[:, np.newaxis]    # shape: (m,1)\n    X_train_sq = np.sum(X_train**2, axis=1)[np.newaxis, :]  # shape: (1,n)\n    cross_term = X_test.dot(X_train.T)                      # shape: (m,n)\n    dists_sq = X_test_sq + X_train_sq - 2 * cross_term\n\n    # Compute the RBF kernel matrix: K[i,j] = exp(-gamma * ||x_test_i - x_train_j||^2)\n    K = np.exp(-gamma * dists_sq)  # shape: (m,n)\n\n    # Decision function for each test point: g(z) = sum_j coeffs[j] * K[i,j] + b\n    decision_values = K.dot(coeffs) + b  # shape: (m,)\n\n    # Convert to final predictions: +1 if >= 0, else -1\n    preds = np.where(decision_values >= 0, 1, -1)\n\n    # Return as a Python list of ints\n    return preds.astype(int).tolist()\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error. Both are rounded to\n        four decimals. If `k` is invalid the function returns -1.\n    \"\"\"\n    # Convert input to numpy array\n    X_arr = np.array(X, dtype=float)\n    m, n = X_arr.shape\n    \n    # Check validity of k\n    if k < 1 or k > min(m, n):\n        return -1\n    \n    # Compute full SVD\n    U, s, Vt = np.linalg.svd(X_arr, full_matrices=False)\n    \n    # Truncate to rank k\n    U_k = U[:, :k]\n    s_k = s[:k]\n    Vt_k = Vt[:k, :]\n    \n    # Reconstruct X_hat = U_k * diag(s_k) * Vt_k\n    X_hat_arr = U_k @ np.diag(s_k) @ Vt_k\n    \n    # Compute mean squared error\n    mse_val = float(np.mean((X_arr - X_hat_arr) ** 2))\n    \n    # Round results\n    X_hat_rounded = np.round(X_hat_arr, 4).tolist()\n    mse_rounded = round(mse_val, 4)\n    \n    return X_hat_rounded, mse_rounded\n\n# Example usage\nif __name__ == \"__main__\":\n    X = [\n        [1.0, 2.0, 3.0],\n        [4.0, 5.0, 6.0],\n        [7.0, 8.0, 9.0]\n    ]\n    k = 2\n    result = linear_autoencoder(X, k)\n    print(result)  # Should print (X_hat, mse) with values rounded to 4 decimals\n```"}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                          y_train: list[float] | np.ndarray,\n                          x_query: float,\n                          epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\n\n    The function must:  \n    1. Determine all possible split points (mid-points between consecutive *x_train* values).  \n    2. Iteratively add the stump that minimises the squared residuals until the total\n       residual sum of squares becomes smaller than *epsilon*.  \n    3. Return the prediction for *x_query* obtained by summing the constants contributed\n       by every learnt stump.  \n\n    Args:\n        x_train: One-dimensional training features.\n        y_train: Training targets (same length as *x_train*).\n        x_query: Feature value to predict.\n        epsilon: Stopping tolerance on the residual sum of squares (default 1e-2).\n\n    Returns:\n        Single floating point number \u2013 the predicted target for *x_query*, rounded to\n        four decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    x = np.array(x_train, dtype=float)\n    y = np.array(y_train, dtype=float)\n    n = x.shape[0]\n    if n == 0:\n        return 0.0\n\n    # Sort by x\n    order = np.argsort(x)\n    x_sorted = x[order]\n    y_sorted = y[order]\n\n    # Initial residuals are the targets (initial model prediction = 0)\n    residuals = y_sorted.copy()\n\n    # Candidate splits: midpoints between consecutive x's\n    splits = (x_sorted[:-1] + x_sorted[1:]) / 2.0\n\n    # Container for learned stumps: list of tuples (split, c1, c2)\n    stumps = []\n\n    # Compute initial RSS\n    rss = np.sum(residuals**2)\n\n    # Greedy boosting loop\n    while rss > epsilon:\n        best_rss = np.inf\n        best_params = None  # to hold (split, c1, c2)\n\n        # Try every candidate split\n        for s in splits:\n            left_mask = x_sorted <= s\n            right_mask = ~left_mask\n\n            # Ensure both sides are non-empty\n            if not left_mask.any() or not right_mask.any():\n                continue\n\n            # Fit constants to residuals on each side\n            c1 = residuals[left_mask].mean()\n            c2 = residuals[right_mask].mean()\n\n            # Compute new residuals and RSS if we add this stump\n            new_residuals = residuals.copy()\n            new_residuals[left_mask] -= c1\n            new_residuals[right_mask] -= c2\n            candidate_rss = np.sum(new_residuals**2)\n\n            # Keep the best split\n            if candidate_rss < best_rss:\n                best_rss = candidate_rss\n                best_params = (s, c1, c2)\n\n        # If no valid stump found or no improvement, break\n        if best_params is None or best_rss >= rss:\n            break\n\n        # Add the best stump to the ensemble\n        s_star, c1_star, c2_star = best_params\n        stumps.append(best_params)\n\n        # Update residuals and RSS for next iteration\n        left_mask = x_sorted <= s_star\n        right_mask = ~left_mask\n        residuals[left_mask] -= c1_star\n        residuals[right_mask] -= c2_star\n        rss = best_rss\n\n    # Make prediction on x_query by summing contributions of all stumps\n    pred = 0.0\n    for s, c1, c2 in stumps:\n        pred += (c1 if x_query <= s else c2)\n\n    return round(pred, 4)\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"\n    Generate a NumPy tensor of the given shape filled with 0.0s and 1.0s,\n    where each entry is 1.0 with probability `sparsity` and 0.0 otherwise.\n\n    Parameters:\n    -----------\n    shape : tuple[int, ...]\n        The desired shape of the output tensor.\n    sparsity : float, optional (default=0.5)\n        The probability of any given entry being 1.0. Must be between 0 and 1 inclusive.\n    seed : int or None, optional (default=None)\n        If provided, used to seed NumPy's random number generator for reproducibility.\n    \n    Returns:\n    --------\n    numpy.ndarray or int\n        A NumPy array of the specified shape and dtype float, containing only 0.0 and 1.0.\n        Returns -1 if `sparsity` is outside the interval [0, 1].\n    \"\"\"\n    # Check validity of sparsity\n    if not (0.0 <= sparsity <= 1.0):\n        return -1\n\n    # Seed the RNG if requested\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Generate uniform random numbers in [0, 1)\n    rnd = np.random.rand(*shape)\n\n    # Determine threshold so that P(entry == 1.0) == sparsity\n    threshold = 1.0 - sparsity\n\n    # Build binary tensor: True where rnd >= threshold \u2192 1.0, else 0.0\n    binary = (rnd >= threshold).astype(float)\n\n    return binary\n\n\n# Example usage and simple test\nif __name__ == \"__main__\":\n    # Example from the problem statement\n    shape = (2, 3)\n    sparsity = 0.3\n    seed = 42\n\n    result = random_binary_tensor(shape, sparsity, seed)\n    print(\"Random binary tensor (shape={}, sparsity={}, seed={}):\".format(shape, sparsity, seed))\n    print(result)\n    # Expected output:\n    # [[0.0, 1.0, 1.0],\n    #  [0.0, 0.0, 0.0]]\n```\n"}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with strictly smaller values;\n        \u2022 right is a list of row indices with strictly larger values.\n    \"\"\"\n    # number of samples\n    n = data.shape[0]\n    if n == 0:\n        raise ValueError(\"Cannot split an empty dataset\")\n    # median position (for even n, this is n//2)\n    m = n // 2\n\n    # extract the d-th column\n    col = data[:, d]\n\n    # argpartition to find the index of the median value in O(n)\n    # after this, the element that would be in position m if sorted\n    # is placed at index m of the returned array\n    partitioned_indices = np.argpartition(col, m)\n    pivot_idx = int(partitioned_indices[m])\n    pivot_val = col[pivot_idx]\n\n    # collect strictly smaller and strictly larger indices\n    left_idx = np.nonzero(col < pivot_val)[0].tolist()\n    right_idx = np.nonzero(col > pivot_val)[0].tolist()\n\n    # sort for deterministic output\n    left_idx.sort()\n    right_idx.sort()\n\n    return pivot_idx, left_idx, right_idx\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # create some random 2D data\n    rng = np.random.default_rng(0)\n    data = rng.integers(0, 100, size=(10, 2))\n    dim = 1  # split along the second column\n    pivot, left, right = get_split(data, dim)\n\n    print(\"Data:\\n\", data)\n    print(f\"Splitting on column {dim}:\")\n    print(\"Pivot index:\", pivot, \"value:\", data[pivot, dim])\n    print(\"Left indices :\", left)\n    print(\"Right indices:\", right)\n```"}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n\n    Raises\n    ------\n    AssertionError\n        If input pre-conditions are violated:\n        1. `x` is not one-dimensional.\n        2. `stride < 1`.\n        3. `len(x) < frame_width`.\n    \"\"\"\n    # Preconditions\n    assert isinstance(x, np.ndarray), \"x must be a NumPy array\"\n    assert x.ndim == 1, f\"x must be one-dimensional, got {x.ndim}D\"\n    assert isinstance(frame_width, int) and frame_width > 0, \"frame_width must be a positive integer\"\n    assert isinstance(stride, int) and stride >= 1, \"stride must be a positive integer\"\n    n = x.shape[0]\n    assert n >= frame_width, \"Input length must be at least as large as frame_width\"\n\n    # Number of frames\n    n_frames = (n - frame_width) // stride + 1\n\n    # Compute new strides: \n    #   - advancing one frame moves stride steps in the original array\n    #   - advancing one element in a frame moves one step\n    item_stride = x.strides[0]\n    new_strides = (stride * item_stride, item_stride)\n\n    # Construct the view\n    frames = as_strided(x, shape=(n_frames, frame_width), strides=new_strides)\n\n    # Set writability\n    if writeable:\n        # Inherit writeability from the source array\n        frames.setflags(write=x.flags.writeable)\n    else:\n        # Force read-only\n        frames.setflags(write=False)\n\n    return frames\n\n# Example usage\nif __name__ == \"__main__\":\n    sig = np.arange(10, dtype=float)\n    # Frame width 4, hop 2 -> frames start at 0,2,4,6; trailing element '9' is dropped\n    F = to_frames(sig, frame_width=4, stride=2, writeable=False)\n    print(\"Signal:\", sig)\n    print(\"Frames shape:\", F.shape)\n    print(F)\n    print(\"Frames writeable?\", F.flags.writeable)\n```\n\nExplanation of key steps:\n\n1. **Assertions** enforce the input pre-conditions.\n2. **n_frames** is computed with floor division, dropping trailing incomplete frames.\n3. **as_strided** builds a view with shape `(n_frames, frame_width)` and strides  \n   `(stride * item_size, item_size)`, so that each row starts `stride` elements after the previous.\n4. **setflags** controls the writeability of the returned view."}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    try:\n        # Try to convert to a 2D NumPy array of floats\n        arr = np.array(X, dtype=float)\n    except (ValueError, TypeError):\n        # Conversion failed (e.g. ragged nested lists or non\u2010numeric entries)\n        return False\n\n    # Must be exactly two-dimensional\n    if arr.ndim != 2:\n        return False\n\n    # No empty matrices or zero\u2010length rows allowed\n    if arr.shape[0] == 0 or arr.shape[1] == 0:\n        return False\n\n    # All entries must lie in [0, 1]\n    if np.any(arr < 0) or np.any(arr > 1):\n        return False\n\n    # Each row must sum to 1 within tolerance\n    row_sums = arr.sum(axis=1)\n    if not np.allclose(row_sums, 1.0, atol=1e-8):\n        return False\n\n    return True\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n    n, d = X.shape\n    # Initialize parameters\n    w = np.zeros(d, dtype=float)\n    s = np.zeros(d, dtype=float)\n\n    # Early stopping check uses full gradient\n    def full_grad(w_vec):\n        return (1.0 / n) * X.T.dot(X.dot(w_vec) - y)\n\n    # Iterate\n    for it in range(max_iter):\n        # Compute batch indices\n        if n <= batch_size:\n            Xb = X\n            yb = y\n        else:\n            start = (it * batch_size) % n\n            end = start + batch_size\n            if end <= n:\n                idx = slice(start, end)\n                Xb = X[idx]\n                yb = y[idx]\n            else:\n                # wrap around\n                part1 = slice(start, n)\n                part2 = slice(0, end - n)\n                Xb = np.vstack((X[part1], X[part2]))\n                yb = np.concatenate((y[part1], y[part2]))\n\n        # Gradient on batch (use batch-size normalization)\n        mb = Xb.shape[0]\n        grad = (1.0 / mb) * Xb.T.dot(Xb.dot(w) - yb)\n\n        # RMSprop updates\n        s = rho * s + (1 - rho) * (grad ** 2)\n        w = w - eta * grad / (np.sqrt(s) + eps_station)\n\n        # Check early stopping on full gradient\n        g_full = full_grad(w)\n        if np.linalg.norm(g_full) < epsilon:\n            break\n\n    # Round and return as Python list\n    w_rounded = np.round(w, 4).tolist()\n    return w_rounded\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create some dummy data\n    np.random.seed(0)\n    n_samples, n_features = 100, 3\n    X = np.random.randn(n_samples, n_features)\n    true_w = np.array([1.5, -2.0, 0.5])\n    y = X.dot(true_w) + 0.1 * np.random.randn(n_samples)\n\n    # Train\n    learned_w = rms_prop(X, y, epsilon=1e-6, max_iter=5000, eta=0.01, rho=0.9, batch_size=20)\n    print(\"Learned weights:\", learned_w)\n    print(\"True weights:   \", true_w.tolist())\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"\n    Compute the SoftPlus activation or its derivative (sigmoid) \n    in a numerically stable way.\n    \n    Parameters:\n    - x: scalar, list, or NumPy array of real values.\n    - deriv: bool, if False return SoftPlus(x), else return sigmoid(x).\n    \n    Returns:\n    - Rounded result to 4 decimal places as a Python list,\n      or a single float if input was scalar.\n    \"\"\"\n    # Detect if input is a scalar (not list/tuple/ndarray)\n    is_scalar = np.isscalar(x)\n    \n    # Convert input to a NumPy array for vectorized operations\n    arr = np.array(x, dtype=np.float64)\n    \n    if deriv:\n        # sigmoid(x) = 1 / (1 + exp(-x)), numerically stable enough\n        result = 1.0 / (1.0 + np.exp(-arr))\n    else:\n        # SoftPlus(x) = log(1 + exp(x))\n        # use a numerically stable form:\n        # SoftPlus(x) = max(x, 0) + log1p(exp(-abs(x)))\n        # This avoids overflow for large |x|.\n        result = np.where(\n            arr > 0,\n            arr + np.log1p(np.exp(-arr)),\n            np.log1p(np.exp(arr))\n        )\n    \n    # Round to 4 decimal places\n    result = np.round(result, 4)\n    \n    # Convert back to Python types\n    if is_scalar:\n        return float(result)\n    else:\n        return result.tolist()\n\n\n# --- Example usage ---\nif __name__ == \"__main__\":\n    # Scalar input\n    print(softplus(10))           # SoftPlus(10)\n    print(softplus(-10))          # SoftPlus(-10)\n    print(softplus(0, deriv=True))  # sigmoid(0)\n    \n    # List input\n    xs = [-100, -1, 0, 1, 100]\n    print(softplus(xs))\n    print(softplus(xs, deriv=True))\n```\n\nExplanation of key points:\n1. We convert any input to a NumPy array for ease of computation.\n2. For the SoftPlus forward pass we use the stable formula  \n   SoftPlus(x) = max(x, 0) + log1p(exp(-abs(x))).  \n3. For the derivative, we compute the standard sigmoid function.\n4. We round each result to 4 decimal places with `np.round`.\n5. The output is transformed to a Python list, or a single float if the input was scalar."}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\n\n    * corpus: list of word tokens for training\n    * sequence: list of word tokens whose probability we compute\n    * N: order of the model (>=1)\n\n    Returns:\n      - total log-probability (natural log) rounded to 4 decimals\n      - float('-inf') if any required n-gram count is zero\n    \"\"\"\n    if N < 1:\n        raise ValueError(\"N must be at least 1\")\n\n    # Handle unigram case\n    if N == 1:\n        uni_counts = Counter(corpus)\n        total_tokens = len(corpus)\n        log_prob = 0.0\n        for w in sequence:\n            cnt = uni_counts.get(w, 0)\n            if cnt == 0:\n                return float('-inf')\n            log_prob += np.log(cnt / total_tokens)\n        return round(log_prob, 4)\n\n    # Build N-gram and (N-1)-gram counts from the corpus\n    ngram_counts = Counter(\n        tuple(corpus[i : i + N]) \n        for i in range(len(corpus) - N + 1)\n    )\n    prefix_counts = Counter(\n        tuple(corpus[i : i + N - 1]) \n        for i in range(len(corpus) - (N - 1) + 1)\n    )\n\n    # Slide over the target sequence\n    log_prob = 0.0\n    for i in range(len(sequence) - N + 1):\n        ngram = tuple(sequence[i : i + N])\n        prefix = tuple(sequence[i : i + N - 1])\n\n        num = ngram_counts.get(ngram, 0)\n        den = prefix_counts.get(prefix, 0)\n        if num == 0 or den == 0:\n            return float('-inf')\n\n        log_prob += np.log(num / den)\n\n    return round(log_prob, 4)\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef lasso_regression(X: list[float] | \"np.ndarray\",\n                     y: list[float] | \"np.ndarray\",\n                     degree: int,\n                     reg_factor: float,\n                     n_iterations: int = 1000,\n                     tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\n\n    The function builds polynomial features up to `degree`, optimises the\n    Lasso objective with coordinate descent, and returns the learned\n    coefficients rounded to four decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X_arr = np.asarray(X, dtype=float).flatten()\n    y_arr = np.asarray(y, dtype=float).flatten()\n    m = X_arr.shape[0]\n    if y_arr.shape[0] != m:\n        raise ValueError(\"X and y must have the same length.\")\n    \n    # Build design matrix: [1, x^1, x^2, ..., x^degree]\n    # Resulting shape: (m, degree + 1)\n    X_design = np.ones((m, degree + 1), dtype=float)\n    for j in range(1, degree + 1):\n        X_design[:, j] = X_arr ** j\n    \n    # Initialize weights (w0 is bias)\n    w = np.zeros(degree + 1, dtype=float)\n    \n    # Precompute squared norms of each feature column j>=1\n    # For bias (j=0), we don't regularise and will compute it specially.\n    xj_sq = np.sum(X_design[:, 1:] ** 2, axis=0)  # length == degree\n    \n    def soft_threshold(rho, lam):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        if rho > 0 and lam < abs(rho):\n            return rho - lam\n        if rho < 0 and lam < abs(rho):\n            return rho + lam\n        return 0.0\n    \n    # Coordinate descent\n    for it in range(n_iterations):\n        w_old = w.copy()\n        \n        # 1) Update bias term w[0] exactly\n        # w0 = mean(y - sum_{j=1}^d x_j * w_j)\n        # note: X_design[:,0] is ones\n        residual_excluding_bias = y_arr - (X_design[:, 1:] @ w[1:])\n        w[0] = np.mean(residual_excluding_bias)\n        \n        # 2) Update each coefficient w[j], j>=1\n        for j in range(1, degree + 1):\n            xj = X_design[:, j]\n            # Compute partial residual: \n            # r = y - (X_design @ w) + w[j] * xj\n            # so that we exclude the current feature's contribution\n            pred = X_design @ w\n            rho = xj.dot(y_arr - pred + w[j] * xj)\n            \n            # Soft-threshold and divide by squared norm\n            w[j] = soft_threshold(rho, reg_factor) / xj_sq[j - 1]\n        \n        # Check convergence: max change in w\n        max_change = np.max(np.abs(w - w_old))\n        if max_change < tol:\n            break\n    \n    # Round to 4 decimal places and return as list\n    w_rounded = np.round(w, 4).tolist()\n    return w_rounded\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Toy data: y = 2 + 3x - x^2 + noise\n    np.random.seed(0)\n    X = np.linspace(-3, 3, 50)\n    y = 2 + 3 * X - 1 * X**2 + 0.5 * np.random.randn(50)\n\n    # Fit a degree-2 Lasso with small regularisation\n    coeffs = lasso_regression(X, y, degree=2, reg_factor=0.1, n_iterations=5000, tol=1e-8)\n    print(\"Learned coefficients [w0, w1, w2]:\", coeffs)\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"Compute the backward probability vector \u03b2_t for a discrete HMM.\n    \n    A: transition probability matrix of shape (N, N)\n    B: emission probability matrix of shape (N, M)\n    obs: list of T observed symbols (as indices 0..M-1)\n    t: time index at which to return \u03b2_t (0 <= t < T)\n    \n    Returns a list of length N containing \u03b2_t(i) rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A)\n    B = np.array(B)\n    T = len(obs)\n    N = A.shape[0]\n\n    # Initialize \u03b2 at time T-1 to all ones\n    beta = np.zeros((T, N), dtype=float)\n    beta[T-1, :] = 1.0\n\n    # Recursively compute \u03b2 backwards from T-2 down to t\n    for time in range(T-2, t-1, -1):\n        # \u03b2_time[i] = sum_j A[i,j] * B[j, obs[time+1]] * \u03b2_{time+1}[j]\n        # We vectorize over i and j:\n        #   For each i, sum over j of A[i,j] * (B[j, obs[time+1]] * beta[time+1, j])\n        beta[time, :] = np.sum(\n            A * (B[:, obs[time+1]] * beta[time+1, :])[None, :],\n            axis=1\n        )\n\n    # Round to 4 decimal places and return as a plain Python list\n    return [round(val, 4) for val in beta[t, :]]\n```\n\nExplanation of key steps:\n1. We store all backward vectors in a 2D array `beta` of shape (T, N).\n2. The boundary condition \u03b2_{T-1}(i) = 1 is set by `beta[T-1, :] = 1.0`.\n3. We iterate `time` from T-2 down to `t` (inclusive) and fill `beta[time]` using the HMM backward equation.\n4. Finally, we round the entries of `beta[t]` to 4 decimal places and return them as a Python list."}
{"task_id": 65, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], \n                  B: list[list[float]], \n                  pi: list[float], \n                  obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    # Guard against empty inputs\n    if not A or not B or not pi or not obs:\n        return 0.0\n\n    # Convert inputs to numpy arrays for convenience\n    A = np.array(A, dtype=float)\n    B = np.array(B, dtype=float)\n    pi = np.array(pi, dtype=float)\n\n    N = A.shape[0]      # number of states\n    T = len(obs)        # length of observation sequence\n\n    # Allocate the backward variable array\n    beta = np.zeros((T, N), dtype=float)\n\n    # Initialization: \u03b2_{T-1}(i) = 1 for all i\n    beta[T-1, :] = 1.0\n\n    # Recursion: \u03b2_t(i) = sum_j A[i,j] * B[j, o_{t+1}] * \u03b2_{t+1}(j)\n    for t in range(T-2, -1, -1):\n        next_obs = obs[t+1]\n        for i in range(N):\n            beta[t, i] = np.sum(A[i, :] * B[:, next_obs] * beta[t+1, :])\n\n    # Termination: P(O|\u03bb) = sum_i \u03c0_i * B[i, o_0] * \u03b2_0(i)\n    prob = np.sum(pi * B[:, obs[0]] * beta[0, :])\n\n    # Return rounded to six decimal places\n    return round(float(prob), 6)\n```"}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Check non\u2010empty observation sequence\n    if not isinstance(observations, list) or len(observations) == 0:\n        return -1\n\n    # Check S is a list of length n\n    if not isinstance(S, list):\n        return -1\n    n = len(S)\n    if n == 0:\n        return -1\n\n    # Check A is n x n\n    if not (isinstance(A, list) and len(A) == n):\n        return -1\n    for row in A:\n        if not (isinstance(row, list) and len(row) == n):\n            return -1\n\n    # Check B is n x m\n    if not (isinstance(B, list) and len(B) == n):\n        return -1\n    m = None\n    for row in B:\n        if not isinstance(row, list):\n            return -1\n        if m is None:\n            m = len(row)\n            if m == 0:\n                return -1\n        elif len(row) != m:\n            return -1\n\n    # Check observations are in [0, m-1]\n    for o in observations:\n        if not isinstance(o, int) or o < 0 or o >= m:\n            return -1\n\n    # Convert to numpy arrays for vectorized operations\n    S_arr = np.array(S, dtype=float)         # shape (n,)\n    A_arr = np.array(A, dtype=float)         # shape (n, n)\n    B_arr = np.array(B, dtype=float)         # shape (n, m)\n\n    # Initialize forward probabilities \u03b1_i(0) = S[i] * B[i, obs_0]\n    alpha = S_arr * B_arr[:, observations[0]]  # shape (n,)\n\n    # Recursion: for each time step t = 1..T-1\n    for t in range(1, len(observations)):\n        o_t = observations[t]\n        # \u03b1_j(t) = [sum_i \u03b1_i(t-1) * A[i, j]] * B[j, o_t]\n        alpha = (alpha @ A_arr) * B_arr[:, o_t]  # shape (n,)\n\n    # Termination: sum over final alphas\n    likelihood = float(np.sum(alpha))\n    return round(likelihood, 4)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    S = [0.6, 0.4]\n    A = [[0.7, 0.3],\n         [0.4, 0.6]]\n    B = [[0.5, 0.5],\n         [0.1, 0.9]]\n    observations = [0, 1, 0]\n    result = forward_algorithm(S, A, B, observations)\n    print(\"Sequence likelihood:\", result)  # Expected output: 0.0618\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\nfrom collections import Counter, defaultdict\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings).  `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result is deterministic:\n      \u2022 Inside each item-set the items are sorted alphabetically.\n      \u2022 The outer list is sorted by `(len(itemset), itemset)`.\n    If *transactions* is empty or no item-set meets the threshold, returns [].\n    \"\"\"\n    # -----------------------------\n    # Internal FP-Tree node class\n    # -----------------------------\n    class TreeNode:\n        def __init__(self, name, count, parent):\n            self.name = name\n            self.count = count\n            self.parent = parent\n            self.children = {}\n            self.node_link = None\n        def increment(self, count):\n            self.count += count\n\n    # --------------------------------------------------\n    # Add a sorted transaction to an FP-Tree, updating\n    # header_table links and counts.\n    # --------------------------------------------------\n    def update_tree(items, node, header_table, count):\n        first = items[0]\n        # if the child exists, increment, else create new\n        if first in node.children:\n            node.children[first].increment(count)\n        else:\n            new_node = TreeNode(first, count, node)\n            node.children[first] = new_node\n            # update header table to point to this node\n            if header_table[first][1] is None:\n                header_table[first][1] = new_node\n            else:\n                current = header_table[first][1]\n                while current.node_link:\n                    current = current.node_link\n                current.node_link = new_node\n        # recurse on the remaining items\n        if len(items) > 1:\n            update_tree(items[1:], node.children[first], header_table, count)\n\n    # --------------------------------------------------\n    # Build the initial FP-Tree and its header table.\n    # --------------------------------------------------\n    def create_initial_tree(transactions, min_support):\n        # 1) Count frequency of each item\n        item_counts = Counter()\n        for trans in transactions:\n            item_counts.update(trans)\n        # 2) Remove infrequent items\n        header_table = {\n            item: [cnt, None]\n            for item, cnt in item_counts.items() if cnt >= min_support\n        }\n        if not header_table:\n            return None, None\n        # 3) Create the root of the FP-tree\n        root = TreeNode(None, 1, None)\n        freq_items = set(header_table.keys())\n        # 4) Insert transactions\n        for trans in transactions:\n            # filter out infrequent, then sort by descending support, tie by lex\n            filtered = [i for i in trans if i in freq_items]\n            if not filtered:\n                continue\n            filtered.sort(key=lambda i: (-item_counts[i], i))\n            update_tree(filtered, root, header_table, 1)\n        return root, header_table\n\n    # --------------------------------------------------\n    # Given a base item and its nodes, extract all\n    # prefix paths (conditional pattern base).\n    # --------------------------------------------------\n    def find_prefix_paths(base_item, node):\n        cond_patterns = {}\n        while node:\n            prefix = []\n            parent = node.parent\n            # climb up to root\n            while parent and parent.name is not None:\n                prefix.append(parent.name)\n                parent = parent.parent\n            if prefix:\n                key = tuple(prefix[::-1])\n                cond_patterns[key] = cond_patterns.get(key, 0) + node.count\n            node = node.node_link\n        return cond_patterns\n\n    # --------------------------------------------------\n    # Recursively mine the FP-tree to extract frequent\n    # itemsets, growing a prefix at each step.\n    # --------------------------------------------------\n    def mine_tree(header_table, min_support, prefix, freq_itemsets):\n        # iterate items in lex order to be deterministic\n        for item in sorted(header_table.keys()):\n            new_prefix = prefix + [item]\n            freq_itemsets.append(new_prefix)\n            # build conditional pattern base\n            cond_base = find_prefix_paths(item, header_table[item][1])\n            # count frequencies in the conditional base\n            cond_header = {}\n            for path, cnt in cond_base.items():\n                for itm in path:\n                    cond_header[itm] = cond_header.get(itm, 0) + cnt\n            # filter out infrequent\n            cond_header = {itm: c for itm, c in cond_header.items() if c >= min_support}\n            if not cond_header:\n                continue\n            # reformat header_table entries\n            for itm in cond_header:\n                cond_header[itm] = [cond_header[itm], None]\n            # build conditional FP-tree\n            cond_root = TreeNode(None, 1, None)\n            freq_items = set(cond_header.keys())\n            for path, cnt in cond_base.items():\n                filtered = [itm for itm in path if itm in freq_items]\n                if not filtered:\n                    continue\n                # sort by descending support in conditional header, then lex\n                filtered.sort(key=lambda i: (-cond_header[i][0], i))\n                update_tree(filtered, cond_root, cond_header, cnt)\n            # recurse deeper\n            mine_tree(cond_header, min_support, new_prefix, freq_itemsets)\n\n    # -----------------------------\n    # Main FP-Growth starts here\n    # -----------------------------\n    # edge cases\n    if not transactions or min_support < 1:\n        return []\n    # 1) Build the initial tree\n    root, header = create_initial_tree(transactions, min_support)\n    if header is None:\n        return []\n    # 2) Mine the tree\n    all_freq_itemsets = []\n    mine_tree(header, min_support, [], all_freq_itemsets)\n    # 3) Sort items in each itemset and then sort the list\n    result = [sorted(itemset) for itemset in all_freq_itemsets]\n    result.sort(key=lambda x: (len(x), x))\n    return result\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]], \n                y_train: list[int], \n                X_test: list[list[float]], \n                k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    # Check validity of k\n    n_train = len(X_train)\n    if not isinstance(k, int) or k < 1 or k > n_train:\n        return -1\n\n    # Convert inputs to numpy arrays for vectorized computation\n    X_train_arr = np.array(X_train, dtype=float)\n    X_test_arr = np.array(X_test, dtype=float)\n    y_train_arr = np.array(y_train, dtype=int)\n\n    predictions = []\n    # For each test sample\n    for x in X_test_arr:\n        # Compute squared Euclidean distances to all training samples\n        diffs = X_train_arr - x  # shape: (n_train, n_features)\n        sq_dists = np.einsum('ij,ij->i', diffs, diffs)  # shape: (n_train,)\n\n        # Find the indices of the k smallest distances\n        neigh_idx = np.argpartition(sq_dists, k-1)[:k]\n\n        # Gather the neighbor labels\n        neigh_labels = y_train_arr[neigh_idx]\n        # Count occurrences of each label\n        label_counts = Counter(neigh_labels)\n        # Determine the highest vote count\n        max_votes = max(label_counts.values())\n        # Filter labels that have the max vote, pick the smallest label in case of tie\n        best_labels = [label for label, cnt in label_counts.items() if cnt == max_votes]\n        prediction = min(best_labels)\n        predictions.append(int(prediction))\n\n    return predictions\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays of type float\n    y_arr = np.asarray(y, dtype=float)\n    y_pred_arr = np.asarray(y_pred, dtype=float)\n\n    # Machine epsilon for numerical stability\n    eps = np.finfo(float).eps\n\n    # Compute element-wise product and sum over all samples and classes\n    loss = -np.sum(y_arr * np.log(y_pred_arr + eps))\n\n    # Round to 4 decimal places and return as Python float\n    return float(round(loss, 4))\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # True labels for 3 samples, 4 classes (one-hot)\n    y_true = [\n        [1, 0, 0, 0],\n        [0, 0, 1, 0],\n        [0, 1, 0, 0]\n    ]\n    # Predicted probabilities from a softmax layer\n    y_pred = [\n        [0.7, 0.1, 0.1, 0.1],\n        [0.2, 0.2, 0.5, 0.1],\n        [0.1, 0.6, 0.2, 0.1]\n    ]\n    print(\"Cross-entropy loss:\", cross_entropy_loss(y_true, y_pred))\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(Z: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Implements the sigmoid activation.\n    \n    Returns:\n        A -- output of sigmoid(z), same shape as Z\n        cache -- returns Z for backprop\n    \"\"\"\n    A = 1 / (1 + np.exp(-Z))\n    return A, Z\n\ndef relu(Z: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Implements the ReLU activation.\n    \n    Returns:\n        A -- output of relu(z), same shape as Z\n        cache -- returns Z for backprop\n    \"\"\"\n    A = np.maximum(0, Z)\n    return A, Z\n\ndef linear_forward(A_prev: np.ndarray,\n                   W: np.ndarray,\n                   b: np.ndarray) -> tuple[np.ndarray, tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"\n    Implements the linear part of a layer's forward propagation.\n    \n    Returns:\n        Z -- the input of the activation function, also called pre-activation parameter\n        cache -- a tuple containing (A_prev, W, b) for backprop\n    \"\"\"\n    Z = W.dot(A_prev) + b\n    cache = (A_prev, W, b)\n    return Z, cache\n\ndef linear_activation_forward(A_prev: np.ndarray,\n                              W: np.ndarray,\n                              b: np.ndarray,\n                              activation: str) -> tuple[np.ndarray, tuple]:\n    \"\"\"\n    Implements the forward propagation for the LINEAR->ACTIVATION layer.\n    \n    activation can be \"relu\" or \"sigmoid\".\n    Returns:\n        A -- the output of the activation function\n        cache -- a tuple containing (linear_cache, activation_cache)\n    \"\"\"\n    Z, linear_cache = linear_forward(A_prev, W, b)\n    if activation == \"relu\":\n        A, activation_cache = relu(Z)\n    elif activation == \"sigmoid\":\n        A, activation_cache = sigmoid(Z)\n    else:\n        raise ValueError(\"Unsupported activation function\")\n    cache = (linear_cache, activation_cache)\n    return A, cache\n\ndef L_model_forward(X: np.ndarray,\n                    parameters: dict[str, np.ndarray]\n                   ) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU -> Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimals and converted to a plain\n        Python list. The shape is (1, m).\n    \"\"\"\n    caches = []\n    A = X\n    L = len(parameters) // 2  # number of layers in the neural network\n\n    # Forward propagation through layers 1 to L-1 with ReLU\n    for l in range(1, L):\n        A_prev = A\n        W = parameters[f\"W{l}\"]\n        b = parameters[f\"b{l}\"]\n        A, cache = linear_activation_forward(A_prev, W, b, activation=\"relu\")\n        caches.append(cache)\n    \n    # Forward propagation for the last layer with Sigmoid\n    W_last = parameters[f\"W{L}\"]\n    b_last = parameters[f\"b{L}\"]\n    AL, cache = linear_activation_forward(A, W_last, b_last, activation=\"sigmoid\")\n    caches.append(cache)\n    \n    # Round and convert to Python list\n    AL_rounded = np.round(AL, 4)\n    return AL_rounded.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(1)\n    X = np.random.randn(3, 5)\n    parameters = {\n        \"W1\": np.random.randn(4, 3),\n        \"b1\": np.random.randn(4, 1),\n        \"W2\": np.random.randn(1, 4),\n        \"b2\": np.random.randn(1, 1),\n    }\n    output = L_model_forward(X, parameters)\n    print(\"AL =\", output)\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as A2.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    # Clip predictions to avoid log(0)\n    epsilon = 1e-15\n    A2_clipped = np.clip(A2, epsilon, 1.0 - epsilon)\n    \n    # Flatten arrays to 1-D\n    A_flat = A2_clipped.reshape(-1)\n    Y_flat = Y.reshape(-1)\n    \n    m = Y_flat.size\n    # Compute cross-entropy\n    loss = -(1.0 / m) * np.sum(\n        Y_flat * np.log(A_flat) + (1 - Y_flat) * np.log(1 - A_flat)\n    )\n    # Round to six decimal places and return\n    return round(float(loss), 6)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # toy example\n    A2 = np.array([[0.9, 0.2, 0.1, 0.8]])\n    Y  = np.array([[1,   0,   0,   1  ]])\n    cost = compute_cost(A2, Y)\n    print(\"Cost:\", cost)  # Expected output: Cost: 0.216161\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list contains the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    # Convert to numpy array\n    Q_arr = np.asarray(Q, dtype=float)\n    n = Q_arr.size\n\n    if n == 0:\n        raise ValueError(\"The Q-values array must contain at least one element.\")\n    if not (0.0 <= epsilon <= 1.0):\n        raise ValueError(\"epsilon must be between 0 and 1 (inclusive).\")\n\n    # Base probability for each action\n    base = epsilon / n\n\n    # Index of the first maximal Q-value\n    greedy_idx = int(np.argmax(Q_arr))\n\n    # Initialize all probabilities with the base\n    probs = np.full(n, base, dtype=float)\n\n    # Give the greedy action its extra probability mass\n    probs[greedy_idx] += (1.0 - epsilon)\n\n    # Round to 4 decimal places\n    probs = np.round(probs, 4)\n\n    # Fix any tiny rounding drift to ensure sum == 1.0 within tolerance\n    total = probs.sum()\n    if not np.isclose(total, 1.0, atol=1e-12):\n        # Distribute the rounding error to the greedy action\n        diff = 1.0 - total\n        probs[greedy_idx] = np.round(probs[greedy_idx] + diff, 4)\n\n    return probs.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    Q = [1.2, 0.3, 1.2, -0.1]\n    eps = 0.1\n    print(epsilon_soft(Q, eps))  # [0.925, 0.025, 0.025, 0.025]\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # If there are no trees or no samples, return an empty list\n    if not predictions or not predictions[0]:\n        return []\n\n    # Transpose the list of lists to iterate over each sample's votes\n    # zip(*predictions) yields tuples where each tuple contains all votes for one sample\n    final_predictions = []\n    for votes in zip(*predictions):\n        # Count how many times each label was voted\n        vote_counts = Counter(votes)\n        # Determine the highest vote count\n        max_count = max(vote_counts.values())\n        # Collect labels that have the highest count (tie candidates)\n        top_labels = [label for label, cnt in vote_counts.items() if cnt == max_count]\n        # Break ties by choosing the smallest label\n        winner = min(top_labels)\n        final_predictions.append(winner)\n\n    return final_predictions\n\n\n# Example usage and basic test\nif __name__ == \"__main__\":\n    # Suppose we have 3 trees and 5 samples\n    tree_preds = [\n        [0, 1, 1, 2, 1],             # Tree 1 predictions\n        [1, 1, 2, 2, 1],             # Tree 2 predictions\n        [0, 2, 1, 2, 0]              # Tree 3 predictions\n    ]\n    # For each sample:\n    # sample 0: votes = [0,1,0] -> 0 wins (2 vs 1)\n    # sample 1: votes = [1,1,2] -> 1 wins (2 vs 1)\n    # sample 2: votes = [1,2,1] -> 1 wins (2 vs 1)\n    # sample 3: votes = [2,2,2] -> 2 wins (3 votes)\n    # sample 4: votes = [1,1,0] -> 1 wins (2 vs 1)\n    aggregated = aggregate_random_forest_votes(tree_preds)\n    print(aggregated)  # Output: [0, 1, 1, 2, 1]\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z). This implementation\n    uses `numpy.logaddexp` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python list. For scalar inputs a single float\n        is returned.\n    \"\"\"\n    # Check if the input is a scalar (Python or numpy scalar)\n    if np.isscalar(z):\n        # Compute softplus in a numerically stable way, cast to Python float,\n        # then round to 4 decimals\n        val = np.logaddexp(0.0, z)\n        return round(float(val), 4)\n    # Otherwise treat input as array-like\n    arr = np.array(z, dtype=float)\n    # Compute elementwise softplus and round\n    result = np.logaddexp(0.0, arr)\n    result = np.round(result, 4)\n    # Convert back to pure Python nested lists\n    return result.tolist()\n\n\n# Example usage and simple tests\nif __name__ == \"__main__\":\n    # Scalar inputs\n    print(softplus(0))        # 0.6931\n    print(softplus(-1000))    # 0.0\n    print(softplus(3.0))      # 3.0486\n\n    # List input\n    print(softplus([-1, 0, 5]))  \n    # Expected: [0.3133, 0.6931, 5.0067]\n\n    # NumPy array input\n    z = np.array([-1000, 0, 3])\n    print(softplus(z))        \n    # Expected: [0.0, 0.6931, 3.0486]\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute `arm_evs`.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute `ev_estimates`. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns `numpy.nan`.\n    \"\"\"\n    # Extract true arm expectations\n    try:\n        arm_evs = bandit['arm_evs']\n    except (TypeError, KeyError):\n        arm_evs = getattr(bandit, 'arm_evs', None)\n    if arm_evs is None:\n        raise AttributeError(\"bandit must have an 'arm_evs' field or key\")\n\n    # Extract policy estimates\n    try:\n        ev_estimates = policy['ev_estimates']\n    except (TypeError, KeyError):\n        ev_estimates = getattr(policy, 'ev_estimates', None)\n    # If no estimates provided or empty dict, return np.nan\n    if not isinstance(ev_estimates, dict) or len(ev_estimates) == 0:\n        return np.nan\n\n    n = len(arm_evs)\n    # Compute squared errors for each arm index 0..n-1\n    sq_errors = []\n    for i in range(n):\n        if i not in ev_estimates:\n            # missing estimate for arm i\n            return np.nan\n        diff = ev_estimates[i] - arm_evs[i]\n        sq_errors.append(diff * diff)\n\n    mse_val = float(np.mean(sq_errors))\n    return round(mse_val, 4)\n\n\nif __name__ == \"__main__\":\n    # Example from the prompt\n    bandit = {\"arm_evs\": [0.5, 0.2, 0.9]}\n    policy = {\"ev_estimates\": {0: 0.4, 1: 0.25, 2: 0.8}}\n    print(mse(bandit, policy))  # Expected output: 0.0075\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    # 1. Build vocabulary from corpus and add <UNK>\n    vocab = set()\n    for sent in corpus:\n        for w in sent.split():\n            vocab.add(w)\n    vocab.add(\"<UNK>\")\n    V = len(vocab)\n\n    # 2. Build n-gram counts for n = 1..N\n    counts = {n: Counter() for n in range(1, N+1)}\n    for sent in corpus:\n        tokens = sent.split()\n        L = len(tokens)\n        for n in range(1, N+1):\n            for i in range(L - n + 1):\n                ng = tuple(tokens[i:i+n])\n                counts[n][ng] += 1\n\n    # 3. For context counts of order 0 (empty context): total # of unigrams\n    total_unigrams = sum(counts[1].values())\n    counts[0] = Counter({(): total_unigrams})\n\n    # 4. Prepare the sequence: map unseen words to <UNK>\n    raw_tokens = sequence.split()\n    seq_tokens = [w if w in vocab else \"<UNK>\" for w in raw_tokens]\n\n    # If no N-grams exist, return 0.0\n    if len(seq_tokens) < N:\n        return 0.0\n\n    # 5. Compute log-probability\n    log_prob = 0.0\n    for i in range(len(seq_tokens) - N + 1):\n        context = tuple(seq_tokens[i:i+N-1]) if N > 1 else ()\n        word = seq_tokens[i+N-1]\n        ngram = context + (word,)\n\n        num = counts[N][ngram] + K\n        den = counts[N-1][context] + K * V\n\n        prob = num / den\n        log_prob += math.log(prob)\n\n    # 6. Round to 4 decimals\n    return round(log_prob, 4)\n```\n\nExplanation of key steps:\n\n1. **Vocabulary building**: We gather every unique token in the corpus and add a special `<UNK>` token.\n2. **Counting n-grams**: For each sentence and each order \\(1\\le n\\le N\\), we slide a window of length \\(n\\) and tally occurrences.\n3. **Zero-order context**: We store the total unigram count under the empty tuple key in `counts[0]` to handle unigram normalization.\n4. **Unknown words**: Any token in the test sentence not seen in the training vocab is replaced by `<UNK>`.\n5. **Additive smoothing**: For each contiguous \\(N\\)-gram in the test sentence, we apply\n   \\[\n     P(w_i\\mid \\text{context})\n       = \\frac{\\text{count}(\\text{context}\\circ w_i) + K}\n              {\\text{count}(\\text{context}) + K\\,|V|},\n   \\]\n   and accumulate the natural logarithm of these probabilities.\n6. **Result**: The function returns the log-probability rounded to four decimal places."}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    N, M = X.shape\n    rng = np.random.RandomState(0)\n    # Initialize W (N x K) and H (K x M)\n    W = rng.randn(N, K)\n    H = rng.randn(K, M)\n\n    # Pre-allocate identity\n    I_K = np.eye(K)\n\n    for iteration in range(max_iter):\n        # Update W: solve (H H^T + alpha I) W^T = (X H^T)\n        HHT = H @ H.T\n        A = HHT + alpha * I_K\n        XHT = X @ H.T  # shape (N, K)\n        # Solve A * (W^T) = XHT^T  => W = (solve(A, XHT.T)).T\n        W = np.linalg.solve(A, XHT.T).T\n\n        # Update H: solve (W^T W + alpha I) H = (W^T X)\n        WTW = W.T @ W\n        B = WTW + alpha * I_K\n        WTX = W.T @ X  # shape (K, M)\n        # Solve B * H = WTX  => H = solve(B, WTX)\n        H = np.linalg.solve(B, WTX)\n\n        # Compute regularised loss\n        X_hat = W @ H\n        diff = X - X_hat\n        loss_recon = np.sum(diff * diff)\n        loss_reg = alpha * (np.sum(W * W) + np.sum(H * H))\n        loss = loss_recon + loss_reg\n\n        if loss <= tol:\n            break\n\n    # Final reconstruction\n    X_hat = W @ H\n    return X_hat.round(4).tolist()\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with shape (m, n) where *m* is the number of samples and *n* is the\n        number of features.\n    K : int\n        The number of clusters to form.\n    max_iter : int, default = 100\n        Maximum number of iterations for the Lloyd refinement loop.\n    random_state : int | None, default = None\n        Seed for both Python's `random` module and NumPy's RNG to make the result\n        reproducible. If *None*, no seed is set.\n\n    Returns\n    -------\n    list[list[float]]\n        A list of K centroids (each centroid is a list of floats) rounded to 4 decimal\n        places and sorted in ascending order by their first coordinate.\n    \"\"\"\n    # Set seeds\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n    m, n = X.shape\n\n    # K-Means++ initialization\n    # 1. Pick the first centroid uniformly at random\n    first_idx = random.randrange(m)\n    centroids = [X[first_idx].astype(float).copy()]\n    # 2. Initialize the array of squared distances to the nearest centroid\n    D = np.sum((X - centroids[0])**2, axis=1)\n\n    for _ in range(1, K):\n        total = D.sum()\n        if total == 0:\n            # All points coincide; choose a random one\n            idx = random.randrange(m)\n        else:\n            # Choose next centroid with probability proportional to D\n            probs = D / total\n            idx = np.random.choice(m, p=probs)\n        new_cent = X[idx].astype(float).copy()\n        centroids.append(new_cent)\n        # Update distances\n        dist_to_new = np.sum((X - new_cent)**2, axis=1)\n        D = np.minimum(D, dist_to_new)\n\n    centroids = np.vstack(centroids)  # shape (K, n)\n\n    # Lloyd's algorithm\n    labels = np.zeros(m, dtype=int)\n    for _ in range(max_iter):\n        # Assignment step\n        # Compute distance matrix (m x K)\n        dists = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n        new_labels = dists.argmin(axis=1)\n        # Check for convergence\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n        # Update step\n        for j in range(K):\n            points_in_cluster = X[labels == j]\n            if len(points_in_cluster) > 0:\n                centroids[j] = points_in_cluster.mean(axis=0)\n            # else leave centroid unchanged\n\n    # Round centroids to 4 decimal places and sort\n    centroids_rounded = [ [round(float(val), 4) for val in centroid] for centroid in centroids ]\n    centroids_rounded.sort(key=lambda c: (c[0], tuple(c)))\n\n    return centroids_rounded\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"\n    Encode every character of `text` into its integer ID according to `vocabulary`.\n    - If a character is not in the vocabulary, encode as 0 (padding).\n    - Truncate or pad with 0 on the right to make the length exactly `time_steps`.\n    \"\"\"\n    ids: list[int] = []\n    for ch in text[:time_steps]:\n        ids.append(vocabulary.get(ch, 0))\n    # pad with zeros if shorter than time_steps\n    if len(ids) < time_steps:\n        ids.extend([0] * (time_steps - len(ids)))\n    return ids\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"\n    Decode a sequence of integer IDs back into a string using `inverse_vocab`.\n    - ID 0 is treated as padding and skipped.\n    \"\"\"\n    chars: list[str] = []\n    for idx in indices:\n        if idx == 0:\n            continue\n        # only add if in inverse vocab\n        ch = inverse_vocab.get(int(idx))\n        if ch is not None:\n            chars.append(ch)\n    return \"\".join(chars)\n\ndef run_example(model, input_vocabulary: dict[str, int],\n                inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"\n    1. Encode `text` to integer IDs.\n    2. Call model.predict on a batch of size 1.\n    3. Take argmax over the last axis to get predicted IDs.\n    4. Decode predicted IDs back to a string (skipping padding).\n    \"\"\"\n    # 1) encode\n    seq_int = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # 2) make a batch of shape (1, TIME_STEPS)\n    batch = np.array(seq_int, dtype=np.int32).reshape((1, TIME_STEPS))\n    # 3) model prediction\n    preds = model.predict(batch)  # expects shape (1, TIME_STEPS, vocab_size)\n    # take the index of the highest probability at each time step\n    pred_ids = np.argmax(preds, axis=-1)[0]  # shape (TIME_STEPS,)\n    # 4) decode\n    return int_to_string(pred_ids, inv_output_vocabulary)\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function will use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    # Fallback to global EXAMPLES if none provided\n    if examples is None:\n        try:\n            examples = EXAMPLES\n        except NameError:\n            raise ValueError(\"No examples provided and global EXAMPLES is not defined.\")\n\n    predictions = []\n    for example in examples:\n        # run_example is assumed to exist in the runtime\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        output_str = \"\".join(output_chars)\n\n        print(f\"input:  {example}\")\n        print(f\"output: {output_str}\")\n\n        predictions.append(output_str)\n\n    return predictions\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef logistic_loss_and_gradient(\n    X: np.ndarray,\n    y: np.ndarray,\n    w: np.ndarray\n) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    # Number of samples\n    m = X.shape[0]\n\n    # Ensure y is a column vector of shape (m, 1)\n    y_vec = y.reshape(m, 1)\n\n    # Compute the linear combination z = X\u00b7w and the sigmoid p = \u03c3(z)\n    # Ensure w is a column vector of shape (n, 1)\n    w_vec = w.reshape(-1, 1)\n    z = X.dot(w_vec)\n    p = 1.0 / (1.0 + np.exp(-z))\n\n    # Clip probabilities to avoid log(0)\n    eps = 1e-20\n    p_clipped = np.clip(p, eps, 1.0 - eps)\n\n    # Compute the average binary cross-entropy loss\n    loss = - (1.0 / m) * np.sum(\n        y_vec * np.log(p_clipped) + (1.0 - y_vec) * np.log(1.0 - p_clipped)\n    )\n\n    # Compute the gradient: (1/m) * X\u1d40 \u00b7 (p - y)\n    grad = (1.0 / m) * (X.T.dot(p - y_vec))\n\n    # Round the results\n    loss_rounded = float(np.round(loss, 4))\n    grad_rounded = np.round(grad, 4)\n\n    # Convert gradient to nested Python list (shape: [ [g1], [g2], ..., [gn] ])\n    grad_list = grad_rounded.reshape(-1, 1).tolist()\n\n    return loss_rounded, grad_list\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple toy dataset\n    X = np.array([[1.0, 2.0],\n                  [3.0, 4.0],\n                  [5.0, 6.0]])\n    y = np.array([0, 1, 1])\n    w = np.array([0.1, -0.2])\n\n    loss, grad = logistic_loss_and_gradient(X, y, w)\n    print(f\"Loss: {loss}\")\n    print(\"Gradient:\")\n    for row in grad:\n        print(row)\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels -1 or 1.\n        X_test:  2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    # Ensure at least one classifier\n    n_clf = max(1, n_clf)\n\n    m, n = X_train.shape\n    # Initialize weights\n    w = np.full(m, 1.0 / m)\n    # Containers for weak learners\n    stumps = []\n    alphas = []\n\n    # AdaBoost training\n    for _ in range(n_clf):\n        best_error = np.inf\n        best_stump = None\n\n        # Loop over features to find best decision stump\n        for j in range(n):\n            x_j = X_train[:, j]\n            # candidate thresholds: midpoints between sorted unique values,\n            # plus tiny margins beyond min/max\n            uniq_vals = np.unique(x_j)\n            if uniq_vals.size > 1:\n                mids = (uniq_vals[:-1] + uniq_vals[1:]) / 2.0\n                thresholds = np.concatenate((\n                    [uniq_vals[0] - 1e-10],\n                    mids,\n                    [uniq_vals[-1] + 1e-10]\n                ))\n            else:\n                # only one unique value, split just below and above\n                v = uniq_vals[0]\n                thresholds = np.array([v - 1e-10, v + 1e-10])\n\n            # Try both polarity directions\n            for thr in thresholds:\n                for polarity in [1, -1]:\n                    # Predict: h(x) = polarity if x_j < thr else -polarity\n                    pred = np.where(x_j < thr, polarity, -polarity)\n                    # Weighted error\n                    err = np.sum(w[pred != y_train])\n\n                    if err < best_error:\n                        best_error = err\n                        best_stump = {\n                            'j': j,\n                            'thr': thr,\n                            'polarity': polarity,\n                            'pred': pred.copy()\n                        }\n\n        # Avoid division by zero / log(0)\n        error = best_error\n        error = max(min(error, 1 - 1e-10), 1e-10)\n        # Compute alpha\n        alpha = 0.5 * np.log((1 - error) / error)\n\n        # Store weak learner\n        stumps.append({\n            'j': best_stump['j'],\n            'thr': best_stump['thr'],\n            'polarity': best_stump['polarity']\n        })\n        alphas.append(alpha)\n\n        # Update sample weights\n        pred = best_stump['pred']\n        w *= np.exp(-alpha * y_train * pred)\n        w /= np.sum(w)\n\n    # AdaBoost prediction on X_test\n    k = X_test.shape[0]\n    agg = np.zeros(k)\n\n    for alpha, stump in zip(alphas, stumps):\n        j = stump['j']\n        thr = stump['thr']\n        polarity = stump['polarity']\n        pred = np.where(X_test[:, j] < thr, polarity, -polarity)\n        agg += alpha * pred\n\n    y_pred = np.sign(agg)\n    # In case some are exactly zero, classify as +1\n    y_pred[y_pred == 0] = 1\n\n    return y_pred.astype(int).tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple toy dataset\n    X_train = np.array([[1], [2], [3], [4], [5]], dtype=float)\n    y_train = np.array([-1, -1, 1, 1, 1], dtype=int)\n    X_test = np.array([[1.5], [2.5], [3.5], [4.5]], dtype=float)\n\n    preds = adaboost_predict(X_train, y_train, X_test, n_clf=3)\n    print(\"Predictions:\", preds)\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 0 and\n        n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places. If n_components is 0, returns an empty list.\n    \"\"\"\n    # If no components requested, return empty list\n    if n_components == 0:\n        return []\n\n    n_samples, n_features = X.shape\n    classes = np.unique(y)\n    # Global mean\n    mean_global = np.mean(X, axis=0)\n\n    # Initialize scatter matrices\n    S_W = np.zeros((n_features, n_features), dtype=float)\n    S_B = np.zeros((n_features, n_features), dtype=float)\n\n    # Compute S_W and S_B\n    for c in classes:\n        X_c = X[y == c]\n        N_c = X_c.shape[0]\n        mean_c = np.mean(X_c, axis=0)\n\n        # Within-class scatter\n        Xc_centered = X_c - mean_c\n        S_W += Xc_centered.T @ Xc_centered\n\n        # Between-class scatter\n        mean_diff = (mean_c - mean_global).reshape(n_features, 1)\n        S_B += N_c * (mean_diff @ mean_diff.T)\n\n    # Solve the generalized eigenvalue problem via A = pinv(S_W) * S_B\n    S_W_inv = np.linalg.pinv(S_W)\n    A = S_W_inv @ S_B\n\n    # Eigen-decomposition (A is symmetric)\n    eigvals, eigvecs = np.linalg.eigh(A)\n    # Sort eigenvalues (and eigenvectors) in descending order\n    idx = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[idx]\n    eigvecs = eigvecs[:, idx]\n\n    # Select the top n_components eigenvectors\n    W = eigvecs[:, :n_components].copy()\n\n    # Deterministic sign: ensure first non-zero entry of each eigenvector is positive\n    for i in range(W.shape[1]):\n        v = W[:, i]\n        # find index of first significant (non-zero) element\n        nz = np.where(np.abs(v) > 1e-8)[0]\n        if nz.size > 0 and v[nz[0]] < 0:\n            W[:, i] = -v\n\n    # Project the data\n    X_proj = X @ W\n\n    # Round to four decimal places\n    X_proj_rounded = np.round(X_proj, 4)\n\n    # Convert to list of lists and return\n    return X_proj_rounded.tolist()\n```\n\nExample usage:\n\n```python\nif __name__ == \"__main__\":\n    X = np.array([[1,1],[1,2],[2,1],[2,2],[8,8],[9,8],[8,9],[9,9]])\n    y = np.array([0,0,0,0,1,1,1,1])\n    n_components = 1\n    projected = multi_class_lda(X, y, n_components)\n    print(projected)\n    # Expected output:\n    # [[1.4142], [2.1213], [2.1213], [2.8284],\n    #  [11.3137], [12.0208], [12.0208], [12.7279]]\n```\n"}
{"task_id": 140, "completion_id": 0, "completion": "```python\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    n = len(grid)\n    # Precompute fire arrival times by multi-source BFS\n    INF = math.inf\n    fire_time = [[INF] * n for _ in range(n)]\n    q = deque()\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_time[i][j] = 0\n                q.append((i, j))\n    dirs = [(1,0), (-1,0), (0,1), (0,-1)]\n    while q:\n        i, j = q.popleft()\n        t0 = fire_time[i][j]\n        for di, dj in dirs:\n            ni, nj = i + di, j + dj\n            if 0 <= ni < n and 0 <= nj < n:\n                if grid[ni][nj] != '#' and fire_time[ni][nj] > t0 + 1:\n                    fire_time[ni][nj] = t0 + 1\n                    q.append((ni, nj))\n\n    # If start or goal is blocked or already on fire at t=0, impossible\n    if grid[0][0] == '#' or fire_time[0][0] == 0:\n        return -1\n    if grid[n-1][n-1] == '#' or fire_time[n-1][n-1] == 0:\n        return -1\n\n    # BFS for the agent: (i, j, time)\n    visited = [[False] * n for _ in range(n)]\n    aq = deque()\n    aq.append((0, 0, 0))\n    visited[0][0] = True\n\n    while aq:\n        i, j, t = aq.popleft()\n        # Reached goal?\n        if i == n-1 and j == n-1:\n            return t\n        for di, dj in dirs:\n            ni, nj = i + di, j + dj\n            nt = t + 1\n            if 0 <= ni < n and 0 <= nj < n:\n                if not visited[ni][nj] and grid[ni][nj] != '#':\n                    # Fire spreads first, then agent moves: ensure the cell\n                    # is not on fire at arrival time nt\n                    if fire_time[ni][nj] > nt:\n                        visited[ni][nj] = True\n                        aq.append((ni, nj, nt))\n\n    # If BFS completes without reaching goal, impossible\n    return -1\n\nif __name__ == \"__main__\":\n    import sys\n    data = sys.stdin.read().strip().split()\n    if not data:\n        print(-1)\n        sys.exit(0)\n    # First token: n\n    n = int(data[0])\n    # Next n tokens: grid rows\n    grid = data[1:1+n]\n    print(escape_fire_maze(grid))\n```"}
{"task_id": 141, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"\n    k-Nearest Neighbors classifier.\n    \n    Parameters:\n    - X_train: (n_train, d) array of training samples\n    - y_train: (n_train,) array of integer labels\n    - X_test:  (n_test, d) array of test samples\n    - k:       number of neighbors to use\n    - metric:  one of 'euclidean', 'manhattan', 'cosine'\n    \n    Returns:\n    - preds:   (n_test,) array of predicted labels\n    \"\"\"\n    # Validate / fallback metric\n    metric = metric.lower()\n    if metric not in ('euclidean', 'manhattan', 'cosine'):\n        metric = 'euclidean'\n    \n    n_train = X_train.shape[0]\n    n_test = X_test.shape[0]\n    preds = np.empty(n_test, dtype=y_train.dtype)\n    \n    # Precompute norms for cosine if needed\n    if metric == 'cosine':\n        # add small eps to avoid division by zero\n        train_norms = np.linalg.norm(X_train, axis=1) + 1e-12\n    \n    for i in range(n_test):\n        x = X_test[i]\n        \n        if metric == 'euclidean':\n            # \u21132 distance\n            dists = np.linalg.norm(X_train - x, axis=1)\n        \n        elif metric == 'manhattan':\n            # \u21131 distance\n            dists = np.sum(np.abs(X_train - x), axis=1)\n        \n        else:  # 'cosine'\n            xt_norm = np.linalg.norm(x) + 1e-12\n            sims = (X_train @ x) / (train_norms * xt_norm)\n            # cosine distance = 1 - cosine similarity\n            dists = 1.0 - sims\n        \n        # find the k smallest distances\n        k2 = min(k, n_train)\n        nbr_idxs = np.argpartition(dists, k2 - 1)[:k2]\n        \n        # gather neighbor labels\n        nbr_labels = y_train[nbr_idxs]\n        \n        # majority vote, break ties by smallest label\n        labels, counts = np.unique(nbr_labels, return_counts=True)\n        max_count = counts.max()\n        top_labels = labels[counts == max_count]\n        preds[i] = top_labels.min()\n    \n    return preds\n```"}
{"task_id": 146, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    n_samples = X.shape[0]\n    k_eff = min(k, n_samples)\n    eps = 1e-12\n\n    # Precompute norms for cosine if needed\n    if metric == 'cosine':\n        X_norms = np.linalg.norm(X, axis=1)\n\n    predictions = []\n\n    for xt in X_test:\n        # Compute distances\n        if metric == 'euclidean':\n            # sqrt of sum of squared differences\n            dists = np.sqrt(np.sum((X - xt) ** 2, axis=1))\n        elif metric == 'manhattan':\n            # sum of absolute differences\n            dists = np.sum(np.abs(X - xt), axis=1)\n        elif metric == 'cosine':\n            # 1 - cos similarity\n            xt_norm = np.linalg.norm(xt)\n            # dot products\n            dots = X.dot(xt)\n            # avoid zero division\n            denom = (X_norms * xt_norm) + eps\n            cos_sim = dots / denom\n            dists = 1.0 - cos_sim\n        else:\n            raise ValueError(f\"Unsupported metric '{metric}'. Choose 'euclidean', 'manhattan', or 'cosine'.\")\n\n        # Find k nearest neighbours\n        nn_idx = np.argsort(dists)[:k_eff]\n        nn_labels = y[nn_idx]\n\n        # Majority vote with tie-breaking by smallest label\n        # Get unique labels and their counts\n        labels, counts = np.unique(nn_labels, return_counts=True)\n        max_count = counts.max()\n        # Labels that have the max count\n        winners = labels[counts == max_count]\n        # Choose the smallest label in case of tie\n        pred = winners.min()\n        predictions.append(pred)\n\n    return predictions\n\n# Example usage:\n# if __name__ == \"__main__\":\n#     X_train = np.array([[0,0], [1,1], [2,2], [3,3]])\n#     y_train = np.array([0, 0, 1, 1])\n#     X_test = np.array([[1.1, 1.0], [2.9, 3.1]])\n#     print(knn_predict(X_train, y_train, X_test, k=3, metric='euclidean'))\n```"}
{"task_id": 155, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # Convert the state to a NumPy array (1-D)\n    x = np.array(state, dtype=float)\n\n    # First affine transform: x \u2192 ReLU(W1\u1d40 x + b1)\n    W1, b1 = weights[\"W1\"], weights[\"b1\"]\n    h1 = np.dot(x, W1) + b1\n    h1 = np.maximum(h1, 0.0)\n\n    # Second affine transform: h1 \u2192 ReLU(W2\u1d40 h1 + b2)\n    W2, b2 = weights[\"W2\"], weights[\"b2\"]\n    h2 = np.dot(h1, W2) + b2\n    h2 = np.maximum(h2, 0.0)\n\n    # Third affine transform: h2 \u2192 tanh(W3\u1d40 h2 + b3)\n    W3, b3 = weights[\"W3\"], weights[\"b3\"]\n    out = np.dot(h2, W3) + b3\n    out = np.tanh(out)\n\n    # Round each action component to 4 decimal places and return as a list\n    out_rounded = np.round(out, 4)\n    return out_rounded.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Dummy dimensions\n    state_dim = 5\n    hidden1 = 8\n    hidden2 = 6\n    action_dim = 3\n\n    # Random test weights\n    np.random.seed(0)\n    weights = {\n        \"W1\": np.random.randn(state_dim, hidden1),\n        \"b1\": np.random.randn(hidden1),\n        \"W2\": np.random.randn(hidden1, hidden2),\n        \"b2\": np.random.randn(hidden2),\n        \"W3\": np.random.randn(hidden2, action_dim),\n        \"b3\": np.random.randn(action_dim),\n    }\n\n    # Random test state\n    state = np.random.randn(state_dim)\n\n    action = actor_forward(state, weights)\n    print(\"Action:\", action)\n```"}
{"task_id": 160, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\n\n    The function learns a weight vector and optional intercept that\n    minimise squared loss + \u03bb\u2006\u00b7\u2006L1-norm.  Uses only NumPy.\n\n    Args:\n        X:   2-D array of shape (m, n) \u2013 feature matrix.\n        y:   1-D array of length m \u2013 target values.\n        lambda_param: Regularisation strength \u03bb (non-negative).\n        max_iters: Number of full passes over the coordinates.\n        fit_intercept: Whether to fit an intercept term.\n\n    Returns:\n        Tuple (weights, bias) where `weights` is a list of length n and `bias`\n        is a float.  All returned numbers are rounded to 4 decimal places.\n    \"\"\"\n    m, n = X.shape\n\n    # 1) Add a column for intercept (ones) or dummy zeros\n    if fit_intercept:\n        col0 = np.ones((m, 1))\n    else:\n        col0 = np.zeros((m, 1))\n    X_enc = np.hstack([col0, X])  # shape (m, n+1)\n\n    # Initialize weights (including intercept) to zeros\n    w = np.zeros(n + 1)\n\n    # Soft-thresholding operator\n    def soft_threshold(a: float, tau: float) -> float:\n        return np.sign(a) * max(abs(a) - tau, 0.0)\n\n    # Coordinate descent\n    for _ in range(max_iters):\n        # 2) Update intercept if requested\n        if fit_intercept:\n            # residual ignoring intercept term\n            b = np.mean(y - X_enc[:, 1:] @ w[1:])\n            w[0] = b\n\n        # 3) Loop over each weight coordinate (skip intercept at j=0)\n        for j in range(1, n + 1):\n            x_j = X_enc[:, j]\n            # Temporarily zero out this coordinate\n            w_j_old = w[j]\n            w[j] = 0.0\n            # Partial residual\n            r_j = y - X_enc @ w\n            # Compute inner product\n            a = x_j.dot(r_j)\n            denom = x_j.dot(x_j)\n            # Soft-thresholding update\n            tau = lambda_param * m\n            if denom != 0.0:\n                w[j] = soft_threshold(a, tau) / denom\n            else:\n                w[j] = 0.0\n\n    # Extract final weights and bias\n    bias = float(w[0])\n    weights = w[1:]\n\n    # Round to 4 decimal places\n    weights_list = [round(float(wi), 4) for wi in weights]\n    bias_float = round(bias, 4)\n\n    return weights_list, bias_float\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple test\n    np.random.seed(0)\n    X = np.random.randn(100, 5)\n    true_w = np.array([1.5, -2.0, 0.0, 0.0, 0.5])\n    y = X @ true_w + 0.3 * np.random.randn(100)\n\n    w_est, b_est = fit_lasso(X, y, lambda_param=0.1, max_iters=200, fit_intercept=True)\n    print(\"Estimated weights:\", w_est)\n    print(\"Estimated bias:   \", b_est)\n```"}
{"task_id": 165, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    # Check for equal lengths\n    try:\n        n_x = len(x)\n        n_y = len(y)\n    except TypeError:\n        # In case inputs are not list-like\n        return -1\n\n    if n_x != n_y:\n        return -1\n\n    # Handle empty vectors\n    if n_x == 0:\n        return 0.0\n\n    # Convert to NumPy arrays for vectorized comparison\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n\n    # Compute the number of differing positions\n    diffs = np.sum(x_arr != y_arr)\n\n    # Normalize and round to 4 decimal places\n    dist = diffs / n_x\n    return round(dist, 4)\n\n\n# Example usage and simple tests\nif __name__ == \"__main__\":\n    # Should print 0.0 (identical vectors)\n    print(hamming_distance([1, 2, 3], [1, 2, 3]))\n\n    # Should print 1.0 (all elements differ)\n    print(hamming_distance([1, 2, 3], [4, 5, 6]))\n\n    # Should print 0.3333 (1 out of 3 differ)\n    print(hamming_distance([1, 2, 3], [1, 9, 3]))\n\n    # Should print -1 (different lengths)\n    print(hamming_distance([1, 2], [1, 2, 3]))\n\n    # Should print 0.0 (both empty)\n    print(hamming_distance([], []))\n```"}
{"task_id": 169, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Name of the desired window. Supported values are\n        \"hamming\", \"hann\", \"blackman_harris\" and \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate. Must be a positive integer.\n    coefficients : list[float] | None, optional\n        List of cosine\u2013series coefficients used **only** when\n        window == \"generalized_cosine\". The default is ``None``.\n\n    Returns\n    -------\n    list[float]\n        List with *N* floats rounded to 4 decimal places.\n    \"\"\"\n    # Validate N\n    if not isinstance(N, int) or N < 1:\n        raise ValueError(\"N must be a positive integer\")\n    # Special case N == 1\n    if N == 1:\n        return [1.0]\n\n    # Prepare the sample indices and denominator\n    n = np.arange(N)\n    denom = N - 1\n\n    # Compute window\n    if window == \"hamming\":\n        w = 0.54 - 0.46 * np.cos(2 * np.pi * n / denom)\n    elif window == \"hann\":\n        w = 0.5 - 0.5 * np.cos(2 * np.pi * n / denom)\n    elif window == \"blackman_harris\":\n        a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n        w = (a0\n             - a1 * np.cos(2 * np.pi * n / denom)\n             + a2 * np.cos(4 * np.pi * n / denom)\n             - a3 * np.cos(6 * np.pi * n / denom))\n    elif window == \"generalized_cosine\":\n        if coefficients is None or not isinstance(coefficients, (list, tuple, np.ndarray)):\n            raise ValueError(\"coefficients must be provided as a list for generalized_cosine\")\n        coeffs = np.asarray(coefficients, dtype=float)\n        # Sum a_k * cos(2\u03c0 k n / (N-1)) over k\n        w = np.zeros_like(n, dtype=float)\n        for k, ak in enumerate(coeffs):\n            w += ak * np.cos(2 * np.pi * k * n / denom)\n    else:\n        raise ValueError(f\"Unknown window type '{window}'\")\n\n    # Round to 4 decimal places and return as Python list\n    return np.round(w, 4).tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    for wname in [\"hamming\", \"hann\", \"blackman_harris\"]:\n        print(wname, generate_window(wname, 8))\n\n    # Generalized cosine example: a simple triangular window via [0,1,0]\n    print(\"generalized_cosine\",\n          generate_window(\"generalized_cosine\", 8, coefficients=[0.0, 1.0, 0.0]))\n```"}
{"task_id": 171, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    # Convert to numpy arrays\n    X_train_arr = np.array(X_train, dtype=float)\n    y_train_arr = np.array(y_train)\n    X_test_arr  = np.array(X_test, dtype=float)\n\n    n_train, n_feat = X_train_arr.shape\n    n_test = X_test_arr.shape[0]\n\n    # Handle special case: all labels identical\n    unique_labels = np.unique(y_train_arr)\n    if unique_labels.size == 1:\n        # Return the single label for all test samples\n        return [int(unique_labels[0])] * n_test\n    if unique_labels.size != 2:\n        raise ValueError(\"y_train must contain exactly two distinct values\")\n\n    # Map original labels to {0,1}\n    label0, label1 = unique_labels[0], unique_labels[1]\n    label_to_int = {label0: 0, label1: 1}\n    int_to_label = {0: label0, 1: label1}\n    y_train_bin = np.vectorize(label_to_int.get)(y_train_arr)\n\n    # Add intercept term\n    X_train_bias = np.hstack([np.ones((n_train, 1)), X_train_arr])\n    X_test_bias  = np.hstack([np.ones((n_test, 1)), X_test_arr])\n\n    # Initialize weights\n    w = np.zeros(X_train_bias.shape[1], dtype=float)\n\n    # Sigmoid function\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # Mini-batch Gradient Descent\n    for epoch in range(epochs):\n        # Shuffle data\n        perm = np.random.permutation(n_train)\n        X_shuf = X_train_bias[perm]\n        y_shuf = y_train_bin[perm]\n\n        # Process mini-batches\n        for start in range(0, n_train, batch_size):\n            end = start + batch_size\n            X_batch = X_shuf[start:end]\n            y_batch = y_shuf[start:end]\n            if X_batch.shape[0] == 0:\n                continue\n\n            preds = sigmoid(X_batch @ w)\n            error = preds - y_batch\n            grad = (X_batch.T @ error) / X_batch.shape[0]\n            w -= learning_rate * grad\n\n    # Predict on test set\n    probs = sigmoid(X_test_bias @ w)\n    preds_int = (probs >= 0.5).astype(int)\n    preds_orig = [int_to_label[int(p)] for p in preds_int]\n\n    return preds_orig\n```\n\nExplanation of key steps:\n- We map any two distinct numeric labels to {0,1} and remember how to invert that mapping.\n- We prepend a column of ones to both training and test feature matrices to model the bias term.\n- We run mini-batch gradient descent for the specified number of epochs, updating the weight vector to minimize the cross-entropy loss.\n- Finally, we threshold the predicted probability at 0.5 and map back to the original labels. If all training labels are the same, we skip training and always predict that label."}
{"task_id": 176, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps on (X_train, y_train) and predict on X_test.\n    Labels y_train are assumed in {0,1}. Returns a list of {0,1} predictions.\"\"\"\n    # Convert to numpy arrays\n    X = np.array(X_train, dtype=float)\n    y = np.array(y_train, dtype=int)\n    n_samples, n_features = X.shape\n\n    # Map labels {0,1} -> {-1,1}\n    y_mapped = np.where(y == 1, 1, -1)\n\n    # Initialize sample weights\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    # To store weak learners and their weights\n    stumps = []\n    alphas = []\n\n    # AdaBoost rounds\n    for _ in range(n_estimators):\n        best_error = np.inf\n        best_stump = None\n\n        # Find best decision stump\n        for feature_i in range(n_features):\n            X_column = X[:, feature_i]\n            thresholds = np.unique(X_column)\n            for threshold in thresholds:\n                for polarity in (1, -1):\n                    # Make predictions: h(x) in {-1,1}\n                    preds = np.ones(n_samples)\n                    if polarity == 1:\n                        preds[X_column < threshold] = 1\n                        preds[X_column >= threshold] = -1\n                    else:\n                        preds[X_column < threshold] = -1\n                        preds[X_column >= threshold] = 1\n\n                    # Compute weighted error\n                    misclassified = (preds != y_mapped)\n                    error = np.dot(w, misclassified)\n\n                    # Select stump if error is lower\n                    if error < best_error:\n                        best_error = error\n                        best_stump = {\n                            'feature_index': feature_i,\n                            'threshold': threshold,\n                            'polarity': polarity,\n                            'predictions': preds.copy()\n                        }\n\n        # If we found a perfect stump, we can stop early\n        # Compute stump weight alpha\n        alpha = 0.5 * np.log((1.0 - best_error) / (best_error + 1e-10))\n\n        stumps.append(best_stump)\n        alphas.append(alpha)\n\n        if best_error == 0:\n            break  # perfect classifier found\n\n        # Update sample weights\n        w *= np.exp(-alpha * y_mapped * best_stump['predictions'])\n        w /= np.sum(w)\n\n    # Now predict on X_test\n    X_test_arr = np.array(X_test, dtype=float)\n    n_test = X_test_arr.shape[0]\n    agg_predictions = np.zeros(n_test)\n\n    for stump, alpha in zip(stumps, alphas):\n        feature_i = stump['feature_index']\n        threshold = stump['threshold']\n        polarity = stump['polarity']\n        preds = np.ones(n_test)\n        col = X_test_arr[:, feature_i]\n        if polarity == 1:\n            preds[col < threshold] = 1\n            preds[col >= threshold] = -1\n        else:\n            preds[col < threshold] = -1\n            preds[col >= threshold] = 1\n        agg_predictions += alpha * preds\n\n    # Final sign and map back to {0,1}\n    final_preds = np.sign(agg_predictions)\n    final_preds = np.where(final_preds == -1, 0, 1)\n\n    return final_preds.astype(int).tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple toy dataset\n    X_train = [[1], [2], [3], [4], [5], [6]]\n    y_train = [0, 0, 1, 1, 1, 0]\n    X_test = [[1.5], [3.5], [5.5]]\n    preds = adaboost_predict(X_train, y_train, X_test, n_estimators=5)\n    print(\"Predictions:\", preds)\n```\n"}
{"task_id": 178, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Input validation\n    if not (isinstance(n_dims, int) and isinstance(num_particles, int) \n            and isinstance(num_iterations, int)):\n        return -1\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n\n    # PSO parameters\n    w = 0.5       # inertia weight\n    c1 = 1.5      # cognitive (particle) weight\n    c2 = 1.5      # social (swarm) weight\n    lb, ub = -1.0, 1.0  # search-space bounds\n\n    # RNG for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # Initialize particle positions and velocities\n    positions = rng.uniform(lb, ub, size=(num_particles, n_dims))\n    velocities = rng.uniform(lb, ub, size=(num_particles, n_dims))\n\n    # Evaluate initial fitness (Sphere function)\n    fitness = np.sum(positions**2, axis=1)\n\n    # Initialize personal bests\n    p_best_positions = positions.copy()\n    p_best_values = fitness.copy()\n\n    # Initialize global best\n    g_best_idx = np.argmin(p_best_values)\n    g_best_position = p_best_positions[g_best_idx].copy()\n    g_best_value = p_best_values[g_best_idx]\n\n    # Main PSO loop\n    for _ in range(num_iterations):\n        # Random coefficients\n        r1 = rng.random((num_particles, n_dims))\n        r2 = rng.random((num_particles, n_dims))\n\n        # Velocity update\n        velocities = (\n            w * velocities\n            + c1 * r1 * (p_best_positions - positions)\n            + c2 * r2 * (g_best_position - positions)\n        )\n\n        # Position update and clipping\n        positions += velocities\n        np.clip(positions, lb, ub, out=positions)\n\n        # Evaluate fitness\n        fitness = np.sum(positions**2, axis=1)\n\n        # Update personal bests\n        better_mask = fitness < p_best_values\n        p_best_positions[better_mask] = positions[better_mask]\n        p_best_values[better_mask] = fitness[better_mask]\n\n        # Update global best\n        current_gbest_idx = np.argmin(p_best_values)\n        current_gbest_val = p_best_values[current_gbest_idx]\n        if current_gbest_val < g_best_value:\n            g_best_value = current_gbest_val\n            g_best_position = p_best_positions[current_gbest_idx].copy()\n\n    # Return the best value rounded to 4 decimal places\n    return round(float(g_best_value), 4)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    best = particle_swarm_optimisation(n_dims=5,\n                                       num_particles=30,\n                                       num_iterations=100,\n                                       seed=42)\n    print(\"Best Sphere value found:\", best)\n```"}
{"task_id": 180, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    # Convert to numpy arrays for efficient computation\n    X_tr = np.asarray(X_train, dtype=float)\n    X_te = np.asarray(X_test, dtype=float)\n    n_train = X_tr.shape[0]\n\n    # Determine actual k\n    if k <= 0 or k > n_train:\n        k_eff = n_train\n    else:\n        k_eff = k\n\n    predictions = []\n    for x in X_te:\n        # Compute Euclidean distances to all training points\n        # (no need to take sqrt if only ranking, but we'll do it for clarity)\n        diffs = X_tr - x  # shape (n_train, n_features)\n        dists = np.linalg.norm(diffs, axis=1)  # shape (n_train,)\n\n        # Get indices of k_eff nearest neighbours\n        idx_sorted = np.argsort(dists)\n        neigh_idx = idx_sorted[:k_eff]\n        neigh_targets = [y_train[i] for i in neigh_idx]\n\n        if task == \"classification\":\n            # Majority vote; in tie, pick smallest label\n            cnt = Counter(neigh_targets)\n            max_count = max(cnt.values())\n            # All labels with max_count\n            candidates = [lab for lab, ct in cnt.items() if ct == max_count]\n            prediction = min(candidates)\n        elif task == \"regression\":\n            # Numeric average, rounded to 4 decimal places\n            # Convert targets to float in case they aren't already\n            vals = np.array(neigh_targets, dtype=float)\n            prediction = round(vals.mean(), 4)\n        else:\n            raise ValueError(f\"Unknown task '{task}'; must be 'classification' or 'regression'.\")\n\n        predictions.append(prediction)\n\n    return predictions\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Classification example\n    X_train_clf = [[1, 2], [2, 3], [3, 1], [6, 5]]\n    y_train_clf = ['a', 'a', 'b', 'b']\n    X_test_clf = [[2, 2], [5, 4]]\n    print(knn_predict(X_train_clf, y_train_clf, X_test_clf, k=3, task=\"classification\"))\n    # Regression example\n    X_train_reg = [[0], [1], [2], [3]]\n    y_train_reg = [0.0, 1.0, 4.0, 9.0]\n    X_test_reg = [[1.5], [2.5]]\n    print(knn_predict(X_train_reg, y_train_reg, X_test_reg, k=2, task=\"regression\"))\n```"}
{"task_id": 184, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Integer class labels for the training data, shape (n_samples,).\n    X_test : np.ndarray\n        Feature matrix to classify, shape (m_samples, n_features).\n    max_depth : int | None, optional\n        Maximum allowed depth of the tree. If None the depth is unlimited.\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for each row in X_test.\n    \"\"\"\n    # Define a simple tree node\n    class Node:\n        def __init__(self, *, predicted_class=None, feature_index=None, threshold=None, left=None, right=None):\n            self.predicted_class = predicted_class\n            self.feature_index = feature_index\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n\n        def is_leaf(self):\n            return self.left is None and self.right is None\n\n        def predict_one(self, x):\n            if self.is_leaf():\n                return self.predicted_class\n            # decide which branch to follow\n            if x[self.feature_index] <= self.threshold:\n                return self.left.predict_one(x)\n            else:\n                return self.right.predict_one(x)\n\n    def gini(y):\n        \"\"\"Compute Gini impurity for label array y.\"\"\"\n        if y.size == 0:\n            return 0.0\n        _, counts = np.unique(y, return_counts=True)\n        probs = counts / counts.sum()\n        return 1.0 - np.sum(probs**2)\n\n    def majority_class(y):\n        \"\"\"Return the majority class label in y (smallest label on ties).\"\"\"\n        if y.size == 0:\n            return 0\n        counts = np.bincount(y)\n        return int(np.argmax(counts))\n\n    def build_tree(X, y, depth):\n        \"\"\"Recursively build the CART tree.\"\"\"\n        # Stopping criteria\n        # 1) all labels identical\n        if np.unique(y).size == 1:\n            return Node(predicted_class=int(y[0]))\n        # 2) max depth reached\n        if max_depth is not None and depth >= max_depth:\n            return Node(predicted_class=majority_class(y))\n\n        n_samples, n_features = X.shape\n        parent_gini = gini(y)\n        best_gain = 0.0\n        best_idx = None\n        best_thr = None\n        best_splits = None\n\n        # Try all features and thresholds\n        for feature_idx in range(n_features):\n            xi = X[:, feature_idx]\n            # get sorted unique values\n            sorted_idx = np.argsort(xi)\n            xi_sorted, y_sorted = xi[sorted_idx], y[sorted_idx]\n            unique_vals = np.unique(xi_sorted)\n            # cannot split on constant feature\n            if unique_vals.size <= 1:\n                continue\n            # candidate thresholds: mid-points\n            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n            for thr in thresholds:\n                # split\n                left_mask = xi_sorted <= thr\n                y_left = y_sorted[left_mask]\n                y_right = y_sorted[~left_mask]\n                # skip empty splits\n                if y_left.size == 0 or y_right.size == 0:\n                    continue\n                # compute gini for children\n                g_left = gini(y_left)\n                g_right = gini(y_right)\n                # weighted average\n                w_left = y_left.size / n_samples\n                w_right = y_right.size / n_samples\n                gain = parent_gini - (w_left * g_left + w_right * g_right)\n                if gain > best_gain:\n                    best_gain = gain\n                    best_idx = feature_idx\n                    best_thr = thr\n                    best_splits = {\n                        'left_X': X[xi <= thr],\n                        'left_y': y[xi <= thr],\n                        'right_X': X[xi > thr],\n                        'right_y': y[xi > thr]\n                    }\n\n        # If no gain, make leaf\n        if best_gain == 0 or best_splits is None:\n            return Node(predicted_class=majority_class(y))\n\n        # Otherwise build subtrees\n        left_child = build_tree(best_splits['left_X'], best_splits['left_y'], depth + 1)\n        right_child = build_tree(best_splits['right_X'], best_splits['right_y'], depth + 1)\n        return Node(predicted_class=None, feature_index=best_idx, threshold=best_thr,\n                    left=left_child, right=right_child)\n\n    # Build the tree\n    root = build_tree(X_train, y_train, depth=0)\n\n    # Predict for test set\n    preds = [root.predict_one(x) for x in X_test]\n    return preds\n```\n\nExample usage:\n\n```python\nif __name__ == \"__main__\":\n    # Simple XOR example\n    X_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n    y_train = np.array([0,1,1,0])\n    X_test = np.array([[0,0],[0,1],[1,0],[1,1]])\n    preds = decision_tree_predict(X_train, y_train, X_test, max_depth=None)\n    print(preds)  # [0, 1, 1, 0]\n```\n"}
{"task_id": 190, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\n\n    Parameters\n    ----------\n    X : list[list[float]] or numpy.ndarray\n        A 2-D structure where each inner list/row contains the numerical\n        feature values of one sample.\n    y : list[int] or numpy.ndarray\n        A 1-D structure containing the class labels corresponding to the rows\n        of X.\n\n    Returns\n    -------\n    tuple\n        A tuple `(best_feature_index, best_threshold_value, best_gini)` where\n        `best_gini` is rounded to 4 decimal places.  If no valid split exists\n        the function returns `(-1, None, round(G_whole_dataset, 4))`.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    if X.ndim != 2 or y.ndim != 1 or X.shape[0] != y.shape[0]:\n        raise ValueError(\"X must be 2D of shape (n_samples, n_features) and y must be 1D of length n_samples.\")\n\n    n_samples, n_features = X.shape\n\n    # Gini impurity function\n    def gini(labels):\n        if labels.size == 0:\n            return 0.0\n        _, counts = np.unique(labels, return_counts=True)\n        ps = counts / labels.size\n        return 1.0 - np.sum(ps * ps)\n\n    # Gini of the whole dataset\n    G_whole = gini(y)\n\n    # If only one class present, no meaningful split\n    if np.unique(y).size <= 1:\n        return (-1, None, round(G_whole, 4))\n\n    best_j = None\n    best_v = None\n    best_gini_raw = np.inf\n    tol = 1e-12\n\n    # Iterate over features and possible thresholds\n    for j in range(n_features):\n        col = X[:, j]\n        unique_vals = np.unique(col)\n\n        for v in unique_vals:\n            # Split indices\n            left_mask = (col <= v)\n            n_left = left_mask.sum()\n            n_right = n_samples - n_left\n            # Skip invalid splits\n            if n_left == 0 or n_right == 0:\n                continue\n\n            # Compute impurities\n            G_left = gini(y[left_mask])\n            G_right = gini(y[~left_mask])\n            G_split = (n_left / n_samples) * G_left + (n_right / n_samples) * G_right\n\n            # Check if this split is better (with tie-breaking)\n            if (G_split < best_gini_raw - tol or\n               (abs(G_split - best_gini_raw) < tol and\n                (best_j is None or j < best_j or (j == best_j and float(v) < best_v)))):\n                best_gini_raw = G_split\n                best_j = j\n                best_v = float(v)\n\n    # If no valid split found\n    if best_j is None:\n        return (-1, None, round(G_whole, 4))\n\n    return (best_j, best_v, round(best_gini_raw, 4))\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple test\n    X = [[2.5, 1.0],\n         [1.0, 2.0],\n         [3.5, 0.5],\n         [2.0, 1.5]]\n    y = [0, 1, 0, 1]\n    print(best_gini_split(X, y))\n```"}
{"task_id": 191, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n    supported = {'euclidean', 'manhattan', 'chebyshev', 'cosine'}\n\n    def d(a, b):\n        # Check metric support\n        if name not in supported:\n            return -1\n        # Try converting inputs to 1D float arrays\n        try:\n            x = np.array(a, dtype=float)\n            y = np.array(b, dtype=float)\n        except Exception:\n            return -1\n        # Validate dimensions, non-empty and equal length\n        if x.ndim != 1 or y.ndim != 1:\n            return -1\n        if x.size == 0 or y.size == 0 or x.shape != y.shape:\n            return -1\n\n        # Compute the selected metric\n        if name == 'euclidean':\n            val = np.linalg.norm(x - y)\n        elif name == 'manhattan':\n            val = np.sum(np.abs(x - y))\n        elif name == 'chebyshev':\n            val = np.max(np.abs(x - y))\n        else:  # cosine\n            nx = np.linalg.norm(x)\n            ny = np.linalg.norm(y)\n            if nx == 0 or ny == 0:\n                return -1\n            val = 1.0 - (np.dot(x, y) / (nx * ny))\n\n        # Round to 4 decimal places and return\n        return float(np.round(val, 4))\n\n    return d\n\n# Example usage:\n# print(metric('euclidean')([1, 2, 3], [4, 5, 6]))  # \u279e 5.1962\n# print(metric('manhattan')([1, 2], [4, 6]))       # \u279e 7.0000\n# print(metric('chebyshev')((0, 2), (2, -1)))      # \u279e 3.0000\n# print(metric('cosine')([1, 0], [0, 1]))          # \u279e 1.0000\n# print(metric('cosine')([1, 0], [0, 0]))          # \u279e -1\n# print(metric('unknown')([1], [1]))               # \u279e -1\n```\n"}
{"task_id": 197, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any,\n                 md_action: bool,\n                 cont_action: bool\n                ) -> Tuple[List[Union[int, float]],\n                           Union[List[Tuple[int, ...]], None],\n                           int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute\n             mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if the action space is continuous\n                     (infinite number of actions).\n\n    Returns:\n        A tuple ``(n_actions_per_dim, action_ids, action_dim)`` where\n            \u2022 n_actions_per_dim: list with the number of actions in every\n                                  dimension (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product)\n                          or ``None`` if any dimension is continuous,\n            \u2022 action_dim: total number of action dimensions.\n    \"\"\"\n    # Case 1: single-dimensional action space\n    if not md_action:\n        # Discrete single dimension\n        if not cont_action:\n            n = env.action_space.n\n            n_actions_per_dim = [n]\n            # generate all discrete action tuples of length 1\n            action_ids = [(i,) for i in range(n)]\n            action_dim = 1\n        # Continuous (possibly vector) single space\n        else:\n            shape = env.action_space.shape\n            # total number of scalar dims in this Box\n            dim = int(np.prod(shape))\n            n_actions_per_dim = [np.inf] * dim\n            action_ids = None\n            action_dim = dim\n\n    # Case 2: multi-dimensional action space\n    else:\n        n_actions_per_dim: List[Union[int, float]] = []\n        any_cont = False\n        # assume env.action_space.spaces is a list-like of sub-spaces\n        for sub in env.action_space.spaces:\n            # Discrete sub-space\n            if hasattr(sub, 'n'):\n                n_actions_per_dim.append(sub.n)\n            # Continuous sub-space (Box)\n            elif hasattr(sub, 'shape'):\n                dim = int(np.prod(sub.shape))\n                n_actions_per_dim.extend([np.inf] * dim)\n                any_cont = True\n            else:\n                raise ValueError(f\"Unknown sub-space type: {sub}\")\n        action_dim = len(n_actions_per_dim)\n        # if any continuous dims, we cannot enumerate all actions\n        if cont_action or any_cont:\n            action_ids = None\n        else:\n            # fully discrete: cartesian product over each dimension\n            ranges = [range(n) for n in n_actions_per_dim]  # type: ignore\n            action_ids = list(product(*ranges))\n\n    return n_actions_per_dim, action_ids, action_dim\n```\n\nExplanation of key points:\n- We branch on `md_action` and `cont_action` to cover the four combinations:\n  1. Single discrete\n  2. Single continuous\n  3. Multi-dimensional discrete\n  4. Multi-dimensional mixed/continuous  \n- For continuous dimensions we insert `numpy.inf` in `n_actions_per_dim` and set `action_ids=None`.\n- For fully discrete spaces we enumerate all possible actions via `itertools.product`, returning tuples whose length equals the action dimensionality."}
{"task_id": 198, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    # Number of topics is the second dimension of any phi[d]\n    if not phi:\n        # No documents: return zero matrix\n        return [[0.0] * 0 for _ in range(V)]\n    T = phi[0].shape[1]\n\n    # Initialize count matrix of shape (V, T)\n    counts = np.zeros((V, T), dtype=float)\n\n    # Accumulate phi contributions\n    for d, doc in enumerate(corpus):\n        phi_d = phi[d]  # shape (N_d, T)\n        # Sanity check for matching lengths\n        if len(doc) != phi_d.shape[0]:\n            raise ValueError(f\"Document {d} length and phi[{d}] rows disagree.\")\n        for n, word in enumerate(doc):\n            # word is an index in 0..V-1\n            if 0 <= word < V:\n                counts[word, :] += phi_d[n, :]\n            else:\n                raise IndexError(f\"Word index {word} out of bounds for V={V}.\")\n\n    # Normalize each topic column to sum to 1\n    # Avoid division by zero: if a topic has zero total mass, leave column as zeros\n    col_sums = counts.sum(axis=0)  # shape (T,)\n    for t in range(T):\n        s = col_sums[t]\n        if s > 0.0:\n            counts[:, t] /= s\n\n    # Round to 4 decimal places\n    counts = np.round(counts, 4)\n\n    # Convert to nested list (V rows, each row is a list of length T)\n    beta_list = counts.tolist()\n    return beta_list\n```"}
{"task_id": 202, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    The function must follow these rules:\n    1. Use the first *k* samples of *X* as the initial medoids.\n    2. Repeatedly try all possible single swaps between a medoid and a non-\n       medoid, accepting the swap that strictly reduces the total cost the\n       most.  Stop when no swap can further decrease the cost.\n    3. After convergence label every sample with the index (0 \u2026 k\u22121) of the\n       closest medoid (ties are broken by the medoid that appears first in the\n       current medoid list).\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features).\n        k: Number of clusters to form.\n\n    Returns:\n        A Python list of length *n_samples* containing integer cluster labels.\n    \"\"\"\n    n_samples = X.shape[0]\n    if not (1 <= k <= n_samples):\n        raise ValueError(\"k must be between 1 and number of samples\")\n\n    # Precompute the full distance matrix between all samples\n    # D[i, j] = Euclidean distance between X[i] and X[j]\n    D = np.linalg.norm(X[:, None, :] - X[None, :, :], axis=2)\n\n    # Initialize medoids to the first k sample indices\n    medoids = list(range(k))\n\n    def total_cost(meds: list[int]) -> float:\n        \"\"\"Compute the sum of distances of each sample to its closest medoid.\"\"\"\n        # For each sample i, look at distances to meds, take the minimum\n        dists = D[:, meds]  # shape (n_samples, k)\n        return float(np.min(dists, axis=1).sum())\n\n    current_cost = total_cost(medoids)\n\n    # Iteratively try to improve by swapping one medoid with one non-medoid\n    while True:\n        best_cost = current_cost\n        best_swap = None\n        med_set = set(medoids)\n\n        # Try every swap: for each medoid m and each non-medoid h\n        for m in medoids:\n            for h in range(n_samples):\n                if h in med_set:\n                    continue\n                # candidate medoid set: replace m with h, keep order\n                candidate = medoids.copy()\n                pos = candidate.index(m)\n                candidate[pos] = h\n\n                c = total_cost(candidate)\n                if c < best_cost:\n                    best_cost = c\n                    best_swap = (m, h)\n\n        # If no improving swap found, we're done\n        if best_swap is None:\n            break\n\n        # Otherwise perform the best swap\n        m, h = best_swap\n        pos = medoids.index(m)\n        medoids[pos] = h\n        current_cost = best_cost\n\n    # Assign each sample to the nearest medoid (ties broken by medoid list order)\n    labels: list[int] = []\n    for i in range(n_samples):\n        # distances to current medoids\n        dists = D[i, medoids]\n        # argmin yields the first minimum in case of ties\n        label = int(np.argmin(dists))\n        labels.append(label)\n\n    return labels\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[0,0], [1,0], [0,1], [5,5], [6,5], [5,6]], dtype=float)\n    labels = pam_clustering(X, k=2)\n    print(labels)  # Expected output: [0, 0, 0, 1, 1, 1]\n```"}
{"task_id": 216, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test,\n                           degree=1, reg_factor=0.05, l1_ratio=0.5,\n                           n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic Net regression implemented with batch gradient descent.\n\n    Parameters\n    ----------\n    x_train : list[list[float]] | np.ndarray\n        Training feature matrix where each sub-list is a sample.\n    y_train : list[float] | np.ndarray\n        Target values for every row in `x_train`.\n    x_test : list[list[float]] | np.ndarray\n        Matrix of samples to predict after training.\n    degree : int, default 1\n        Degree of polynomial expansion applied to every original feature.\n    reg_factor : float, default 0.05\n        Overall regularisation strength (alpha).\n    l1_ratio : float, default 0.5\n        Portion of L1 penalty in Elastic Net (0 = pure ridge, 1 = pure lasso).\n    n_iterations : int, default 3000\n        Number of gradient descent iterations.\n    learning_rate : float, default 0.01\n        Step size used in each gradient update.\n\n    Returns\n    -------\n    list[float]\n        Predictions for `x_test` rounded to 4 decimals. Returns `-1` if the\n        inputs are invalid (different feature counts, or non-positive\n        `n_iterations`).\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X_train = np.array(x_train, dtype=float)\n    X_test = np.array(x_test, dtype=float)\n    y_train = np.array(y_train, dtype=float)\n\n    # Basic validity checks\n    if n_iterations < 1:\n        return -1\n    if X_train.ndim != 2 or X_test.ndim != 2:\n        return -1\n    n_samples, n_feats = X_train.shape\n    if y_train.ndim != 1 or y_train.shape[0] != n_samples:\n        return -1\n    if X_test.shape[1] != n_feats:\n        return -1\n    if degree < 1:\n        return -1\n\n    # Polynomial feature expansion (no cross-terms), degree >=1\n    def polynomial_features(X, degree):\n        # X is (n_samples, n_feats)\n        poly_list = []\n        for d in range(1, degree + 1):\n            poly_list.append(X ** d)\n        return np.hstack(poly_list)\n\n    X_train_poly = polynomial_features(X_train, degree)\n    X_test_poly = polynomial_features(X_test, degree)\n\n    # Standard-score (zero-mean, unit-variance) on training features\n    mu = X_train_poly.mean(axis=0)\n    sigma = X_train_poly.std(axis=0)\n    # Avoid division by zero\n    sigma[sigma == 0] = 1.0\n\n    X_train_std = (X_train_poly - mu) / sigma\n    X_test_std = (X_test_poly - mu) / sigma\n\n    # Add bias term (column of ones)\n    X_train_bd = np.hstack([np.ones((X_train_std.shape[0], 1)), X_train_std])\n    X_test_bd = np.hstack([np.ones((X_test_std.shape[0], 1)), X_test_std])\n\n    # Initialize weights (including bias)\n    n_params = X_train_bd.shape[1]\n    w = np.zeros(n_params, dtype=float)\n\n    # Batch gradient descent\n    for _ in range(n_iterations):\n        preds = X_train_bd.dot(w)               # (n_samples,)\n        errors = preds - y_train                # (n_samples,)\n        # Gradient of MSE loss: (2/n) * X^T * (pred - y)\n        grad_mse = (2.0 / n_samples) * (X_train_bd.T.dot(errors))\n        # Gradient of regularization (sub-gradient for L1, plain for L2)\n        # Combine: alpha * [ l1_ratio * sign(w) + (1-l1_ratio)*w ]\n        grad_reg = reg_factor * (l1_ratio * np.sign(w) + (1 - l1_ratio) * w)\n        # Do not regularize bias term\n        grad_reg[0] = 0.0\n        # Total gradient\n        grad = grad_mse + grad_reg\n        # Update weights\n        w -= learning_rate * grad\n\n    # Predictions on test set\n    y_pred = X_test_bd.dot(w)\n    # Round to 4 decimals and return as list\n    return np.round(y_pred, 4).tolist()\n```\n\nExplanation of key steps:\n1. Input validation ensures `x_train` and `x_test` have compatible shapes and `n_iterations` is positive.  \n2. Polynomial expansion is done feature-wise up to the given `degree` (no cross-terms).  \n3. Features are standard-scored using training-set statistics; zero-variance features are left unchanged.  \n4. A bias term is prepended to the design matrix; its weight is not regularized.  \n5. Gradient descent minimizes the mean squared error plus an Elastic Net penalty\n   L = (1/n)\u2006\u2211(\u0177\u200a\u2212\u200ay)\u00b2 + \u03b1 [ l\u2081\u2006\u2016w\u2016\u2081 + \u00bd(1\u2212l\u2081)\u2006\u2016w\u2016\u2082\u00b2 ],\n   using the sub-gradient for the L\u2081 term.  \n6. Final predictions are rounded to four decimal places."}
{"task_id": 217, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs are rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers (+1 or -1).\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y = np.asarray(actual, dtype=float)\n    f = np.asarray(predicted, dtype=float)\n\n    # Sigmoid(probability) for each predicted score\n    prob = 1.0 / (1.0 + np.exp(-f))\n\n    # Gradient: y * sigmoid(-y * f) = y / (1 + exp(y * f))\n    grad = y / (1.0 + np.exp(y * f))\n\n    # Hessian: sigmoid(f) * (1 - sigmoid(f))\n    hess = prob * (1.0 - prob)\n\n    # Round all results to 6 decimal places and convert to plain Python lists\n    grad_list = np.round(grad, 6).tolist()\n    hess_list = np.round(hess, 6).tolist()\n    prob_list = np.round(prob, 6).tolist()\n\n    return grad_list, hess_list, prob_list\n\n# Example usage:\nif __name__ == \"__main__\":\n    actual = [+1, -1, +1, -1]\n    predicted = [0.0, 1.0, -1.0, 2.0]\n    grad, hess, prob = logistic_components(actual, predicted)\n    print(\"Gradient:\", grad)\n    print(\"Hessian: \", hess)\n    print(\"Prob:    \", prob)\n```"}
{"task_id": 218, "completion_id": 0, "completion": "```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1.0, 0.0, 1.0 or 1.5.\n    \"\"\"\n    def hand_value(cards: list[int]) -> int:\n        total = sum(cards)\n        # If there's at least one Ace and upgrading one Ace from 1 to 11 keeps us \u226421\n        if 1 in cards and total + 10 <= 21:\n            return total + 10\n        return total\n\n    def is_natural(cards: list[int]) -> bool:\n        # Natural blackjack is exactly two cards: one Ace (1) and one 10-value card (10)\n        return len(cards) == 2 and 1 in cards and 10 in cards\n\n    p_val = hand_value(player)\n    d_val = hand_value(dealer)\n\n    # Determine the raw outcome: -1 (player loses), 0 (draw), +1 (player wins)\n    if p_val > 21:\n        raw = -1\n    elif d_val > 21:\n        raw = 1\n    else:\n        if p_val > d_val:\n            raw = 1\n        elif p_val < d_val:\n            raw = -1\n        else:\n            raw = 0\n\n    # If the player wins (raw == 1) with a natural blackjack, pay 1.5 instead of 1.0\n    if raw == 1 and is_natural(player):\n        return 1.5\n    return float(raw)\n```"}
{"task_id": 221, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"Return element-wise squared log errors between actual and predicted.\"\"\"\n    a = np.array(actual)\n    p = np.array(predicted)\n    # Compute (log(1 + a) - log(1 + p))^2\n    return (np.log1p(a) - np.log1p(p)) ** 2\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence of true values.  All elements must be non-negative.\n    predicted : list | tuple | np.ndarray\n        Sequence of predicted values.  Must be the same length as\n        `actual` and contain only non-negative numbers.\n\n    Returns\n    -------\n    float\n        The MSLE rounded to 4 decimals.  If the inputs are invalid the\n        function returns -1.\n    \"\"\"\n    try:\n        a_np = np.array(actual)\n        p_np = np.array(predicted)\n\n        # Check same shape\n        if a_np.shape != p_np.shape:\n            return -1\n        # Disallow empty sequences\n        if a_np.size == 0:\n            return -1\n        # Check non-negative\n        if np.any(a_np < 0) or np.any(p_np < 0):\n            return -1\n\n        errors = squared_log_error(a_np, p_np)\n        msle = np.mean(errors)\n        # Round to 4 decimal places and return as float\n        return round(float(msle), 4)\n    except Exception:\n        return -1\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = [2, 0, 5, 10]\n    y_pred = [3, 1, 4, 8]\n    print(mean_squared_log_error(y_true, y_pred))  # e.g., 0.0204\n```"}
{"task_id": 222, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function works for scalar numbers, Python lists and NumPy arrays and\n    is numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    # Convert input to NumPy array of floats (does not copy if already array)\n    z_arr = np.array(z, dtype=float, copy=False)\n    \n    # Prepare output array\n    result = np.empty_like(z_arr)\n    \n    # Masks for positive and negative values\n    pos_mask = z_arr >= 0\n    neg_mask = ~pos_mask\n    \n    # For z >= 0: sigmoid = 1 / (1 + exp(-z))\n    result[pos_mask] = 1.0 / (1.0 + np.exp(-z_arr[pos_mask]))\n    \n    # For z < 0: sigmoid = exp(z) / (1 + exp(z)) to avoid overflow\n    exp_z = np.exp(z_arr[neg_mask])\n    result[neg_mask] = exp_z / (1.0 + exp_z)\n    \n    # Round to 4 decimal places\n    result = np.round(result, 4)\n    \n    # If input was a scalar (0-d array), return a float\n    if result.ndim == 0:\n        return float(result)\n    \n    # Otherwise, return a nested Python list\n    return result.tolist()\n\n\n# Example usages:\nif __name__ == \"__main__\":\n    # Scalar inputs\n    print(sigmoid(0))        # 0.5\n    print(sigmoid(1000))     # 1.0\n    print(sigmoid(-1000))    # 0.0\n\n    # 1-D list\n    print(sigmoid([0, 2, -2, 1000, -1000]))\n    # 2-D list\n    print(sigmoid([[0, -1], [3, -4]]))\n    # NumPy array\n    arr = np.array([-10, 0, 10], dtype=float)\n    print(sigmoid(arr))\n```"}
{"task_id": 224, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or numpy.ndarray of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    # Convert input to a NumPy array (this handles scalars, lists, nested lists, etc.)\n    arr = np.array(z, dtype=float)\n    # Apply vectorized Leaky ReLU\n    activated = np.where(arr >= 0, arr, a * arr)\n    # Convert back to Python nested lists (or scalar)\n    result = activated.tolist()\n    # Ensure we always return a list, even for zero-dimensional (scalar) inputs\n    if activated.ndim == 0:\n        return [result]\n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Scalar input\n    print(leaky_relu(-3.5))             # -> [-0.035]\n    print(leaky_relu(2.0))              # -> [2.0]\n\n    # Flat list\n    print(leaky_relu([-2, -1, 0, 1, 2])) # -> [-0.02, -0.01, 0.0, 1.0, 2.0]\n\n    # Nested list\n    nested = [[-1, 2, -3], [4, -5, 6]]\n    print(leaky_relu(nested, a=0.1))\n    # -> [[-0.1, 2.0, -0.30000000000000004], [4.0, -0.5, 6.0]]\n\n    # NumPy array\n    arr = np.array([[-1.5, 0.0, 1.5]])\n    print(leaky_relu(arr))             # -> [[-0.015, 0.0, 1.5]]\n```"}
{"task_id": 226, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D numpy array of shape (n_samples, n_features) with training data.\n        y_train: 1-D numpy array of shape (n_samples,) containing class labels (-1 or 1).\n        X_test:  2-D numpy array of unseen samples to classify.\n        n_clf:   Number of weak learners (decision stumps) to use in the ensemble.\n\n    Returns:\n        A Python list with one element per row in *X_test*. Each element is\n        either -1 or 1 indicating the predicted class label.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n\n    # Initialize weights uniformly\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    # This will hold tuples of (feature_index, threshold, polarity, alpha)\n    classifiers = []\n\n    for _ in range(n_clf):\n        best_stump = {}\n        min_error = float('inf')\n\n        # Find the best decision stump\n        for feature_i in range(n_features):\n            X_column = X_train[:, feature_i]\n            unique_vals = np.unique(X_column)\n\n            # Candidate thresholds are midpoints between consecutive unique feature values\n            if unique_vals.shape[0] > 1:\n                thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n            else:\n                thresholds = unique_vals\n\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    # Predict with this stump\n                    preds = np.ones(n_samples, dtype=int)\n                    # For polarity=1: x < thresh => -1, else +1\n                    # For polarity=-1: x < thresh => +1, else -1\n                    mask = polarity * X_column < polarity * threshold\n                    preds[mask] = -1\n\n                    # Weighted classification error\n                    misclassified = preds != y_train\n                    error = np.dot(w, misclassified)\n\n                    # Keep the best stump so far\n                    if error < min_error:\n                        min_error = error\n                        best_stump = {\n                            'feature_i': feature_i,\n                            'threshold': threshold,\n                            'polarity': polarity,\n                            'predictions': preds.copy()\n                        }\n\n        # Compute the learner weight alpha\n        alpha = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n\n        # Update sample weights: increase weight on misclassified examples\n        w *= np.exp(-alpha * y_train * best_stump['predictions'])\n        w /= np.sum(w)  # Normalize\n\n        # Save this weak learner\n        classifiers.append((best_stump['feature_i'],\n                            best_stump['threshold'],\n                            best_stump['polarity'],\n                            alpha))\n\n    # Now make predictions on X_test\n    n_test = X_test.shape[0]\n    final_pred = np.zeros(n_test)\n\n    for feature_i, threshold, polarity, alpha in classifiers:\n        X_column = X_test[:, feature_i]\n        preds = np.ones(n_test)\n        mask = polarity * X_column < polarity * threshold\n        preds[mask] = -1\n        final_pred += alpha * preds\n\n    # Sign of the weighted sum; treat zeros as +1\n    final_pred = np.sign(final_pred)\n    final_pred[final_pred == 0] = 1\n\n    return final_pred.astype(int).tolist()\n```"}
{"task_id": 241, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list | int:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers. Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix. Defaults to 0.5.\n\n    Returns:\n        A python list (possibly nested) representing the tensor whose first two\n        dimensions are orthogonal up to the given scaling factor. If `shape`\n        has fewer than two dimensions the function returns -1.\n    \"\"\"\n    # We need at least two dimensions to form an orthogonal matrix\n    if len(shape) < 2:\n        return -1\n\n    # Flatten all but the first dimension\n    d0 = shape[0]\n    d_rest = int(np.prod(shape[1:], dtype=int))\n    flat_shape = (d0, d_rest)\n\n    # 1) Draw random normal matrix\n    A = np.random.randn(*flat_shape)\n\n    # 2) Compute SVD\n    #    A = U @ np.diag(s) @ Vt, with U.shape = (d0, k), Vt.shape = (k, d_rest)\n    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n\n    # 3) Pick the factor that matches A's shape\n    if U.shape == flat_shape:\n        Q = U\n    else:\n        Q = Vt  # Vt has shape (d0, d_rest) when d0 < d_rest\n\n    # 4) Reshape back to the requested tensor shape and scale\n    Q = Q.reshape(shape)\n    Q = scale * Q\n\n    # 5) Return as a nested Python list\n    return Q.tolist()\n\n\n# Example usage and sanity check\nif __name__ == \"__main__\":\n    np.random.seed(0)\n    result = orthogonal((2, 2), scale=0.5)\n    print(\"Result:\\n\", np.array(result))\n    # Expected (approximately):\n    # [[ 0.259,  0.426],\n    #  [ 0.426, -0.260]]\n\n    # Orthogonality check\n    out = np.array(result)\n    flat = out.reshape(out.shape[0], -1)\n    if flat.shape[0] <= flat.shape[1]:\n        ortho = flat @ flat.T\n        identity = (scale ** 2) * np.eye(flat.shape[0])\n    else:\n        ortho = flat.T @ flat\n        identity = (scale ** 2) * np.eye(flat.shape[1])\n    print(\"flat @ flat.T  \u2248\\n\", ortho)\n    print(\"scale^2 * I     =\\n\", identity)\n    print(\"Max abs diff   =\", np.max(np.abs(ortho - identity)))\n```\n\nExplanation of the key steps:\n\n1. Check that `shape` has at least two dimensions; otherwise return `-1`.  \n2. Flatten the tensor into a 2-D matrix of shape `(d0, d1\u22efdn)`.  \n3. Draw i.i.d. standard normal entries for this matrix.  \n4. Compute its (thin) SVD via `np.linalg.svd(..., full_matrices=False)`.  \n5. Select the SVD factor (`U` or `V\u1d40`) whose shape matches the flattened matrix.  \n6. Reshape back to the original `shape`, multiply by `scale`, and convert to nested lists.  \n\nThe result\u2019s rows or columns (whichever are fewer) form an orthonormal set up to the factor `scale`, satisfying the specified orthogonality condition within numerical tolerance."}
{"task_id": 243, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    # Convert input to numpy array\n    s = np.array(state, dtype=np.float64)\n\n    # Dense-1: z1 = ReLU(sum(s))\n    z1 = np.maximum(0.0, s.sum())\n\n    # Dense-2: z2 = 4 * z1 (since all weights=1 and 4 outputs)\n    z2 = 4.0 * z1\n\n    # Dense-3: z3 = 4 * z2\n    z3 = 4.0 * z2\n\n    # Actor head logits: 4 * z3 for each of 3 actions\n    logits = np.full((3,), 4.0 * z3, dtype=np.float64)\n\n    # Softmax\n    exps = np.exp(logits - np.max(logits))  # for numerical stability\n    probs = exps / exps.sum()\n\n    # Critic head: state value = 4 * z3\n    state_value = 4.0 * z3\n\n    # Round outputs to 4 decimals\n    probs_rounded = np.round(probs, 4).tolist()\n    value_rounded = float(round(state_value, 4))\n\n    return probs_rounded, value_rounded\n\n# Example usage\nif __name__ == \"__main__\":\n    for test_state in [[1.0, 2.0, -0.5], [0.0, 0.0, 0.0], [-1.0, -2.0, 0.5]]:\n        probs, val = actor_critic_forward(test_state)\n        print(f\"state={test_state} -> action_probs={probs}, state_value={val}\")\n```\n\nExplanation of key steps:\n1. We sum the input vector and apply ReLU to get `z1`.\n2. Each subsequent dense layer multiplies the previous activation by 4, since every weight is 1 and each layer has 4 outputs.\n3. The actor head produces 3 identical logits = `4 * z3`; softmax on identical logits yields uniform probabilities.\n4. The critic head produces a single value = `4 * z3`.\n5. Finally, we round all outputs to four decimal places."}
{"task_id": 249, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head\n        value         : float          \u2013 scalar output of the critic head\n    \"\"\"\n    # Convert state to NumPy array\n    x = np.array(state, dtype=float)\n\n    # Shared three-layer MLP with ReLU activations\n    for i in range(1, 4):\n        W = np.array(params[f'W{i}'], dtype=float)\n        b = np.array(params[f'b{i}'], dtype=float)\n        x = W.dot(x) + b\n        x = np.maximum(x, 0.0)  # ReLU\n\n    # Actor head: affine + softmax\n    Wa = np.array(params['Wa'], dtype=float)\n    ba = np.array(params['ba'], dtype=float)\n    z = Wa.dot(x) + ba\n    # Numeric stability for softmax\n    z_max = np.max(z)\n    exp_z = np.exp(z - z_max)\n    probs = exp_z / np.sum(exp_z)\n\n    # Critic head: affine \u2192 scalar\n    Wc = np.array(params['Wc'], dtype=float)\n    bc = np.array(params['bc'], dtype=float)\n    v = Wc.dot(x) + bc\n    # If v is an array of shape (1,), extract the scalar\n    v = float(v) if np.ndim(v) == 0 else float(v.squeeze())\n\n    # Round outputs to 4 decimal places\n    probs = np.round(probs, 4)\n    v = round(v, 4)\n\n    # Convert probabilities to Python floats in a list\n    probs_list = [float(p) for p in probs]\n\n    return probs_list, v\n```\n\nExplanation of key steps:\n\n1. **Shared layers**: We loop over the three dense layers (`W1, b1` to `W3, b3`), each time applying an affine transform (`W.dot(x) + b`) followed by a ReLU (`np.maximum(x, 0)`).\n\n2. **Actor head**: We compute the raw action scores `z = Wa\u00b7x + ba`. The soft-max is implemented in a numerically stable manner by subtracting `max(z)` before exponentiating, then normalizing.\n\n3. **Critic head**: We compute the scalar state value `v = Wc\u00b7x + bc`. We ensure it is returned as a plain Python `float`.\n\n4. **Rounding**: Both the action probabilities and the state value are rounded to 4 decimal places. Values that are exactly `0` or `1` remain `0.0` or `1.0` respectively.\n\nThis implementation uses only NumPy and adheres to the specified API."}
{"task_id": 253, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    # Convert w to numpy array of floats\n    w = np.array(w, dtype=float)\n    # Input validation\n    if alpha < 0:\n        raise ValueError(\"alpha must be non-negative\")\n    if not (0 <= l1_ratio <= 1):\n        raise ValueError(\"l1_ratio must be between 0 and 1\")\n\n    if gradient:\n        # \u2207R(w) = \u03b1 * [ \u03bb * sign(w) + (1\u2212\u03bb) * w ]\n        grad = alpha * (l1_ratio * np.sign(w) + (1 - l1_ratio) * w)\n        # Round each element to 4 decimals and return as list\n        return np.round(grad, 4).tolist()\n    else:\n        # R(w) = \u03b1 * [ \u03bb * ||w||\u2082 + (1\u2212\u03bb) * 0.5 * w\u1d40w ]\n        l1_term = np.linalg.norm(w, 2)\n        l2_term = 0.5 * np.dot(w, w)\n        penalty = alpha * (l1_ratio * l1_term + (1 - l1_ratio) * l2_term)\n        return round(penalty, 4)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    w = [1.0, -2.0, 3.0]\n    alpha = 0.5\n    l1_ratio = 0.7\n\n    pen = elastic_net_regularization(w, alpha, l1_ratio)\n    grad = elastic_net_regularization(w, alpha, l1_ratio, gradient=True)\n\n    print(\"Penalty:\", pen)\n    print(\"Gradient:\", grad)\n```"}
{"task_id": 256, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\n\n    Args:\n        model:        A model exposing a ``parameters`` dict, a ``forward``\n                       method (single time-step) and a ``flush_gradients``\n                       method.\n        loss_func:    Callable that maps the list of predictions to a scalar\n                       loss value.\n        param_name:   Name of the parameter to be checked.  \"Ba\" and \"Bx\" must\n                       be redirected to the lower-case keys.  If the name is\n                       \"X\" or \"y\" the function returns None.\n        n_t:          Number of time-steps to unroll the network for.\n        X:            Input tensor of shape (batch, input_dim, n_t).\n        epsilon:      Small perturbation added/subtracted to the parameter.\n\n    Returns:\n        NumPy array containing the *transposed* numerical gradient of the\n        chosen parameter or None for the non-trainable names.\n    \"\"\"\n    # Non-trainable\n    if param_name in (\"X\", \"y\"):\n        return None\n\n    # Redirect special cases\n    if param_name == \"Ba\":\n        real_name = \"ba\"\n    elif param_name == \"Bx\":\n        real_name = \"bx\"\n    else:\n        real_name = param_name\n\n    # Check parameter exists\n    if real_name not in model.parameters:\n        raise KeyError(f\"Parameter '{real_name}' not found in model.parameters\")\n\n    # Ensure inputs are float64\n    X = X.astype(np.float64)\n\n    # Original parameter array\n    param = model.parameters[real_name]\n    if not isinstance(param, np.ndarray):\n        raise ValueError(f\"Parameter '{real_name}' is not a NumPy array\")\n    param = param.astype(np.float64)\n\n    grads = np.zeros_like(param, dtype=np.float64)\n\n    # Iterate over every index in the parameter tensor\n    it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n    while not it.finished:\n        idx = it.multi_index\n\n        # --- Positive perturbation ---\n        model_p = deepcopy(model)\n        # Cast the parameter to float64 in the copied model\n        model_p.parameters[real_name] = model_p.parameters[real_name].astype(np.float64)\n        model_p.parameters[real_name][idx] += epsilon\n        model_p.flush_gradients()\n        preds_p = []\n        for t in range(n_t):\n            x_t = X[:, :, t]\n            preds_p.append(model_p.forward(x_t))\n        L_p = loss_func(preds_p)\n\n        # --- Negative perturbation ---\n        model_m = deepcopy(model)\n        model_m.parameters[real_name] = model_m.parameters[real_name].astype(np.float64)\n        model_m.parameters[real_name][idx] -= epsilon\n        model_m.flush_gradients()\n        preds_m = []\n        for t in range(n_t):\n            x_t = X[:, :, t]\n            preds_m.append(model_m.forward(x_t))\n        L_m = loss_func(preds_m)\n\n        # Finite difference\n        grads[idx] = (L_p - L_m) / (2 * epsilon)\n\n        it.iternext()\n\n    # Return the transpose of the gradient array\n    return grads.T\n```"}
{"task_id": 257, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    # Ensure at least one classifier\n    if n_clf < 1:\n        n_clf = 1\n\n    m, n = X_train.shape\n    # Initialize weights uniformly\n    D = np.full(m, 1 / m)\n    # Will hold the parameters of each weak classifier\n    classifiers = []\n\n    for _ in range(n_clf):\n        best = {\n            'feature_idx': None,\n            'threshold': None,\n            'polarity': 1,\n            'error': np.inf,\n        }\n\n        # Loop over features to find the best decision stump\n        for feature_i in range(n):\n            feature_values = X_train[:, feature_i]\n            unique_vals = np.unique(feature_values)\n\n            # Consider each unique value as a threshold\n            for threshold in unique_vals:\n                for polarity in [1, -1]:\n                    # Predict: h(x) = 1 if polarity * x < polarity * threshold else -1\n                    preds = np.ones(m)\n                    # Where the condition holds, predict -1\n                    idx = polarity * feature_values < polarity * threshold\n                    preds[idx] = -1\n\n                    # Compute weighted error\n                    misclassified = preds != y_train\n                    error = np.dot(D, misclassified)\n\n                    # If error is better than current best, update\n                    if error < best['error']:\n                        best.update({\n                            'feature_idx': feature_i,\n                            'threshold': threshold,\n                            'polarity': polarity,\n                            'error': error\n                        })\n\n        # Compute alpha (classifier weight), add a small epsilon to avoid div by zero\n        eps = 1e-10\n        err = best['error']\n        alpha = 0.5 * np.log((1 - err + eps) / (err + eps))\n\n        # Store the weak classifier parameters\n        classifiers.append({\n            'feature_idx': best['feature_idx'],\n            'threshold': best['threshold'],\n            'polarity': best['polarity'],\n            'alpha': alpha\n        })\n\n        # Update sample weights D\n        feature_vals = X_train[:, best['feature_idx']]\n        preds = np.ones(m)\n        idx = best['polarity'] * feature_vals < best['polarity'] * best['threshold']\n        preds[idx] = -1\n\n        # Exponential update\n        D *= np.exp(-alpha * y_train * preds)\n        D /= D.sum()  # Normalize to sum to 1\n\n    # Now predict on X_test\n    k = X_test.shape[0]\n    final_preds = np.zeros(k)\n\n    for clf in classifiers:\n        feature_i = clf['feature_idx']\n        thresh = clf['threshold']\n        pol = clf['polarity']\n        alpha = clf['alpha']\n\n        feat_vals_test = X_test[:, feature_i]\n        preds_test = np.ones(k)\n        idx_test = pol * feat_vals_test < pol * thresh\n        preds_test[idx_test] = -1\n\n        final_preds += alpha * preds_test\n\n    # Return the sign of the aggregated predictions\n    # If sum is 0, we can assign +1 by convention\n    out = np.where(final_preds >= 0, 1, -1)\n    return out.tolist()\n\n# Example Usage:\nif __name__ == \"__main__\":\n    # Simple toy dataset\n    X_train = np.array([[1], [2], [3], [4], [5], [6]])\n    y_train = np.array([-1, -1, -1, 1, 1, 1])\n    X_test = np.array([[1.5], [2.5], [3.5], [4.5], [5.5]])\n\n    preds = adaboost_predict(X_train, y_train, X_test, n_clf=5)\n    print(\"Predictions:\", preds)\n```"}
{"task_id": 261, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError(\"glorot_normal requires at least 2 dimensions, got shape %r\" % (shape,))\n\n    # Fully connected / 2D layer\n    if len(shape) == 2:\n        fan_in, fan_out = shape\n    # Convolutional or higher-rank tensor\n    else:\n        # shape = (out_channels, in_channels, k1, k2, ...)\n        receptive_field_size = math.prod(shape[2:])\n        fan_in  = shape[1] * receptive_field_size\n        fan_out = shape[0] * receptive_field_size\n\n    # Standard deviation for Glorot normal\n    std = math.sqrt(2.0 / (fan_in + fan_out))\n\n    # Draw samples from N(0, std^2)\n    return np.random.standard_normal(size=shape) * std\n\n\n# Example usage and sanity check\nif __name__ == \"__main__\":\n    # Fully connected layer weight: 128 inputs, 64 outputs\n    W_fc = glorot_normal((128, 64))\n    print(\"FC layer: mean=%.5f, std=%.5f (theoretical std=%.5f)\"\n          % (W_fc.mean(), W_fc.std(), math.sqrt(2/(128+64))))\n\n    # Convolutional layer: 32 filters, 3 input channels, 5x5 kernel\n    W_conv = glorot_normal((32, 3, 5, 5))\n    rf = 3 * 5 * 5\n    theo_std = math.sqrt(2/(rf * (32 + 3)))\n    print(\"Conv layer: mean=%.5f, std=%.5f (theoretical std=%.5f)\"\n          % (W_conv.mean(), W_conv.std(), theo_std))\n```"}
{"task_id": 266, "completion_id": 0, "completion": "```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list is preserved.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples `(u, v)` describing undirected edges that connect\n        vertices `u` and `v`.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list G where G[i] contains all vertices adjacent to V[i].\n        \u2022 The outer list has the same length and order as V.\n        \u2022 Inner lists contain no duplicates and are ordered by their\n          appearance order in V.\n    \"\"\"\n    # Map each vertex to its index in V\n    idx_map = {vertex: i for i, vertex in enumerate(V)}\n    \n    # Prepare a list of sets to collect neighbour-indices (to avoid duplicates)\n    neighbour_idx_sets = [set() for _ in V]\n    \n    # Process each undirected edge\n    for u, v in E:\n        if u not in idx_map or v not in idx_map:\n            # Skip edges with unknown vertices\n            continue\n        i, j = idx_map[u], idx_map[v]\n        neighbour_idx_sets[i].add(j)\n        neighbour_idx_sets[j].add(i)\n    \n    # Build the final adjacency list, sorting neighbours by their order in V\n    G: List[List[Any]] = []\n    for i, vertex in enumerate(V):\n        # Sort neighbour indices in ascending order to respect V's order\n        sorted_neigh_indices = sorted(neighbour_idx_sets[i])\n        # Map back to vertex identifiers\n        G.append([V[j] for j in sorted_neigh_indices])\n    \n    return G\n\n\nif __name__ == \"__main__\":\n    # Example 1: simple undirected graph\n    V1 = [5, 2, 3, 7]\n    E1 = [(5, 2), (2, 3), (2, 5), (7, 7)]  # note: (7,7) is a self-loop\n    # 5 <-> 2, 2 <-> 3, plus a self-loop on 7\n    # Adjacency:\n    # 5: [2]\n    # 2: [5, 3]\n    # 3: [2]\n    # 7: [7]\n    print(build_adj_list(V1, E1))\n    # Expected: [[2], [5, 3], [2], [7]]\n\n    # Example 2: string vertices, parallel edges, and an isolated vertex\n    V2 = ['a', 'b', 'c', 'd']\n    E2 = [('a', 'b'), ('b', 'c'), ('a', 'b'), ('c', 'a')]\n    # a <-> b (twice), b <-> c, c <-> a, 'd' is isolated\n    # Adjacency:\n    # a: [b, c]\n    # b: [a, c]\n    # c: [a, b]\n    # d: []\n    print(build_adj_list(V2, E2))\n    # Expected: [['b', 'c'], ['a', 'c'], ['a', 'b'], []]\n```"}
{"task_id": 267, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump.\n    \"\"\"\n    n_samples, n_features = X.shape\n    best_feature = None\n    best_threshold = None\n    best_polarity = None\n    best_error = np.inf\n\n    # Loop over all features\n    for j in range(n_features):\n        feature_values = X[:, j]\n        unique_values = np.unique(feature_values)\n        \n        # Loop over all candidate thresholds (unique feature values)\n        for theta in unique_values:\n            # Predictions for polarity = +1\n            preds_p1 = np.where(feature_values < theta, 1, -1)\n            err_p1 = np.sum(sample_weights[preds_p1 != y])\n\n            # Predictions for polarity = -1 (flip +1 predictions)\n            preds_pneg1 = np.where(feature_values < theta, -1, 1)\n            err_pneg1 = np.sum(sample_weights[preds_pneg1 != y])\n\n            # Check if these are the best so far\n            if err_p1 < best_error:\n                best_error = err_p1\n                best_feature = j\n                best_threshold = theta\n                best_polarity = 1\n            if err_pneg1 < best_error:\n                best_error = err_pneg1\n                best_feature = j\n                best_threshold = theta\n                best_polarity = -1\n\n    # Round threshold and error to 4 decimals\n    best_threshold_rounded = float(np.round(best_threshold, 4))\n    best_error_rounded = float(np.round(best_error, 4))\n\n    return {\n        \"feature_index\": int(best_feature),\n        \"threshold\": best_threshold_rounded,\n        \"polarity\": int(best_polarity),\n        \"weighted_error\": best_error_rounded\n    }\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple test\n    X = np.array([[2.3, 1.5],\n                  [1.1, 3.2],\n                  [4.0, 2.2],\n                  [3.1, 0.8]])\n    y = np.array([1, -1, 1, -1])\n    w = np.array([0.25, 0.25, 0.25, 0.25])\n    \n    stump = train_decision_stump(X, y, w)\n    print(\"Best decision stump:\", stump)\n```"}
{"task_id": 273, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimals or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to plain Python lists (handles numpy arrays too)\n    X = list(feature)\n    Y = list(target)\n    n = len(X)\n    if n == 0:\n        return None, 0.0\n\n    # Build total label counter and per-value label counters\n    total_counter = Counter(Y)\n    counts_per_value = {}\n    for x, y in zip(X, Y):\n        counts_per_value.setdefault(x, Counter()).update([y])\n\n    # Sort unique feature values\n    uniq = sorted(counts_per_value.keys())\n    # No split possible if all feature values are identical\n    if len(uniq) <= 1:\n        return None, 0.0\n\n    # Entropy function\n    def entropy(counter, total):\n        if total == 0:\n            return 0.0\n        h = 0.0\n        for cnt in counter.values():\n            if cnt > 0:\n                p = cnt / total\n                h -= p * math.log2(p)\n        return h\n\n    # Parent entropy\n    H_parent = entropy(total_counter, n)\n\n    best_gain = 0.0\n    best_threshold = None\n\n    left_counter = Counter()\n    left_count = 0\n\n    # Iterate over possible splits (between uniq[i] and uniq[i+1])\n    for i in range(len(uniq) - 1):\n        # Add all points with feature == uniq[i] to the left\n        cnt_i = counts_per_value[uniq[i]]\n        left_counter.update(cnt_i)\n        left_count += sum(cnt_i.values())\n\n        right_count = n - left_count\n        # If no points on one side, skip (no actual split)\n        if left_count == 0 or right_count == 0:\n            continue\n\n        # Compute the threshold midway between uniq[i] and uniq[i+1]\n        t = 0.5 * (uniq[i] + uniq[i+1])\n\n        # Compute entropies of the two subsets\n        H_left = entropy(left_counter, left_count)\n        # Right counter = total_counter - left_counter\n        # Build it on the fly\n        right_counter = Counter()\n        for cls, cnt in total_counter.items():\n            right_counter[cls] = cnt - left_counter.get(cls, 0)\n        H_right = entropy(right_counter, right_count)\n\n        # Information gain\n        weighted_H = (left_count / n) * H_left + (right_count / n) * H_right\n        gain = H_parent - weighted_H\n\n        # Update best if gain is higher, or equal but threshold smaller\n        if gain > best_gain or (math.isclose(gain, best_gain) and (best_threshold is None or t < best_threshold)):\n            best_gain = gain\n            best_threshold = t\n\n    # If no positive gain found, return (None, 0.0)\n    if best_threshold is None or best_gain <= 0.0:\n        return None, 0.0\n\n    # Round the results to 4 decimal places\n    bt_rounded = round(best_threshold, 4)\n    gain_rounded = round(best_gain, 4)\n    return bt_rounded, gain_rounded\n```"}
{"task_id": 286, "completion_id": 0, "completion": "```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[1.0 for _ in range(cols)] for _ in range(rows)]\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    rows, cols = shape\n    # Set seed for reproducibility\n    random.seed(seed)\n    return [[random.uniform(low, high) for _ in range(cols)] for _ in range(rows)]\n\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    obj = globals().get(name)\n    if obj is None or not callable(obj):\n        raise ValueError(\"Invalid initialization function.\")\n    return obj\n\n# Example usage:\nif __name__ == \"__main__\":\n    init_fn = get_initializer(\"ones_init\")\n    print(init_fn((2, 3)))  # [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n\n    init_fn = get_initializer(\"zeros_init\")\n    print(init_fn((2, 2)))  # [[0.0, 0.0], [0.0, 0.0]]\n\n    init_fn = get_initializer(\"random_uniform_init\")\n    print(init_fn((2, 5), low=-1.0, high=1.0, seed=123))\n    # e.g. [[0.064..., -0.123...], [...], ...]\n```\n\nExplanation of key parts:\n1. `zeros_init` and `ones_init` use nested list comprehensions to build the requested matrix.\n2. `random_uniform_init` sets the random seed for determinism, then fills the matrix using `random.uniform`.\n3. `get_initializer` fetches the object from `globals()` by name, checks that it exists and is callable, and otherwise raises the required `ValueError`."}
{"task_id": 287, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef average_pool2d(X: np.ndarray,\n                   pool_shape: tuple[int, int],\n                   stride: tuple[int, int],\n                   accum_grad: np.ndarray\n                  ) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    Args:\n      X: Input of shape (N, C, H, W).\n      pool_shape: (p_h, p_w) window size.\n      stride: (s_h, s_w) stride along height and width.\n      accum_grad: Gradient w.r.t. output, shape (N, C, H_out, W_out).\n\n    Returns:\n      A tuple (pooled_output_list, grad_input_list), both rounded to 4 decimals\n      and converted to Python lists.\n    \"\"\"\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    s_h, s_w = stride\n\n    # Compute output dimensions\n    H_out = (H - p_h) // s_h + 1\n    W_out = (W - p_w) // s_w + 1\n\n    # Initialize output and gradient w.r.t. input\n    pooled = np.zeros((N, C, H_out, W_out), dtype=X.dtype)\n    grad_input = np.zeros_like(X, dtype=X.dtype)\n\n    # Forward pass: compute average pooling\n    for n in range(N):\n        for c in range(C):\n            for i in range(H_out):\n                for j in range(W_out):\n                    i0 = i * s_h\n                    j0 = j * s_w\n                    window = X[n, c, i0:i0 + p_h, j0:j0 + p_w]\n                    pooled[n, c, i, j] = np.mean(window)\n\n    # Backward pass: distribute gradients equally\n    field_size = p_h * p_w\n    for n in range(N):\n        for c in range(C):\n            for i in range(H_out):\n                for j in range(W_out):\n                    grad_val = accum_grad[n, c, i, j] / field_size\n                    i0 = i * s_h\n                    j0 = j * s_w\n                    grad_input[n, c, i0:i0 + p_h, j0:j0 + p_w] += grad_val\n\n    # Round to 4 decimal places and convert to Python lists\n    pooled_rounded = np.round(pooled, 4).tolist()\n    grad_input_rounded = np.round(grad_input, 4).tolist()\n\n    return pooled_rounded, grad_input_rounded\n```"}
{"task_id": 290, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    # Both are leaves: compare their values\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-8, rtol=0)\n\n    # Both are internal nodes: compare feature, threshold, and subtrees\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        # Feature indices must match exactly\n        if tree_a.feature != tree_b.feature:\n            return False\n        # Thresholds must be equal within 1e-8 tolerance\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8, rtol=0):\n            return False\n        # Recursively compare left and right children\n        return (compare_trees(tree_a.left, tree_b.left) and\n                compare_trees(tree_a.right, tree_b.right))\n\n    # One is a Node and the other is a Leaf (mismatched structure)\n    return False\n```"}
{"task_id": 292, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                           parent2: list[list[int | float]],\n                           cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as parent1.\n        cutoff: Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function returns -1.\n    \"\"\"\n    # Convert to NumPy arrays\n    arr1 = np.array(parent1, dtype=float)\n    arr2 = np.array(parent2, dtype=float)\n\n    # Check shapes\n    if arr1.shape != arr2.shape:\n        return -1\n\n    n_rows, n_cols = arr1.shape\n\n    # Allocate empty child arrays\n    child1 = np.empty_like(arr1)\n    child2 = np.empty_like(arr2)\n\n    # Copy columns before cutoff from the same parent\n    child1[:, :cutoff] = arr1[:, :cutoff]\n    child2[:, :cutoff] = arr2[:, :cutoff]\n\n    # Copy columns from cutoff onward from the other parent\n    child1[:, cutoff:] = arr2[:, cutoff:]\n    child2[:, cutoff:] = arr1[:, cutoff:]\n\n    # Convert back to Python lists\n    return child1.tolist(), child2.tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    p1 = [\n        [0.1, 0.2, 0.3, 0.4],\n        [1.1, 1.2, 1.3, 1.4]\n    ]\n    p2 = [\n        [9.1, 9.2, 9.3, 9.4],\n        [8.1, 8.2, 8.3, 8.4]\n    ]\n    cutoff = 2\n\n    offspring = single_point_crossover(p1, p2, cutoff)\n    if offspring == -1:\n        print(\"Parent shapes do not match.\")\n    else:\n        child1, child2 = offspring\n        print(\"Child 1:\")\n        print(child1)\n        print(\"Child 2:\")\n        print(child2)\n```"}
{"task_id": 294, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph represented as a dictionary with the keys:\n          * 'is_directed' (bool)\n          * '_V2I'        (dict mapping vertex label -> index)\n          * '_G'          (list of adjacency lists, each edge is a\n                           tuple (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a vertex label and the value is a list\n        of `(neighbour, weight)` tuples.  For undirected graphs every edge\n        appears once in each direction and the neighbour lists are sorted\n        alphabetically.\n    \"\"\"\n    # Extract components\n    is_directed = G.get('is_directed', True)\n    v2i = G.get('_V2I', {})\n    raw_adj = G.get('_G', [])\n\n    # If there are no vertices, return empty dict\n    if not v2i:\n        return {}\n\n    # Initialize adjacency dict with all vertices, even if they end up with no edges\n    adj: Dict[Any, List[Tuple[Any, float]]] = {v: [] for v in v2i.keys()}\n\n    if is_directed:\n        # Simply add each outgoing edge\n        for edge_list in raw_adj:\n            for src, tgt, w in edge_list:\n                adj[src].append((tgt, w))\n    else:\n        # For undirected graphs, ensure exactly one copy in each direction\n        # even if internal storage already contains both.\n        added: set = set()\n        for edge_list in raw_adj:\n            for src, tgt, w in edge_list:\n                # add src -> tgt once\n                if (src, tgt, w) not in added:\n                    adj[src].append((tgt, w))\n                    added.add((src, tgt, w))\n                # add tgt -> src once\n                if (tgt, src, w) not in added:\n                    adj[tgt].append((src, w))\n                    added.add((tgt, src, w))\n\n    # Sort neighbour lists lexicographically by neighbour label\n    for u in adj:\n        adj[u].sort(key=lambda pair: pair[0])\n\n    return adj\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Directed example\n    G_dir = {\n        'is_directed': True,\n        '_V2I': {'A': 0, 'B': 1, 'C': 2},\n        '_G': [\n            [('A', 'B', 1.0), ('A', 'C', 2.0)],  # edges out of 'A'\n            [('B', 'C', 3.0)],                   # edges out of 'B'\n            []                                   # edges out of 'C'\n        ]\n    }\n    print(to_networkx(G_dir))\n    # {'A': [('B', 1.0), ('C', 2.0)], 'B': [('C', 3.0)], 'C': []}\n\n    # Undirected example, internal storage might have duplicates\n    G_undir = {\n        'is_directed': False,\n        '_V2I': {'X': 0, 'Y': 1},\n        '_G': [\n            [('X', 'Y', 5.0)],  # X \u2192 Y\n            [('Y', 'X', 5.0)]   # Y \u2192 X (duplicate internal)\n        ]\n    }\n    print(to_networkx(G_undir))\n    # {'X': [('Y', 5.0)], 'Y': [('X', 5.0)]}\n```"}
{"task_id": 296, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        True if all elements are 0 or 1, otherwise False.\n    \"\"\"\n    # Convert input to a NumPy array\n    x_arr = np.asarray(x)\n    # Compare elementwise to 0 or 1\n    try:\n        mask = (x_arr == 0) | (x_arr == 1)\n    except Exception:\n        # Incomparable dtypes, e.g. strings\n        return False\n    # mask is a boolean array that is True where x_arr is 0 or 1\n    return bool(mask.all())\n\n# Example usage and simple tests\nif __name__ == \"__main__\":\n    print(is_binary([0, 1, 1, 0]))            # True\n    print(is_binary(np.array([[True, False], [1, 0]])))  # True\n    print(is_binary([0.0, 1.0, 2.0]))         # False\n    print(is_binary(np.array([np.nan, 0])))  # False\n    print(is_binary([]))                     # True\n```"}
{"task_id": 298, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom typing import List\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        A list of training sentences. Each sentence is a plain string; tokens\n        are assumed to be separated by white-spaces.\n    query : str\n        The sentence whose probability should be calculated.\n    N : int\n        The order of the N-gram model (N \u2265 1).\n\n    Returns\n    -------\n    float\n        The sum of natural logarithms of the probabilities of every\n        consecutive N-gram occurring in *query*, rounded to 4 decimal places.\n        If any necessary N-gram is unseen in the corpus or if the query is\n        too short, returns float('-inf').\n    \"\"\"\n    # Counters for N-grams and (N-1)-gram prefixes\n    ngram_counts = Counter()\n    prefix_counts = Counter()\n    # For unigrams, track total token count\n    total_unigrams = 0\n\n    # Build counts from the corpus\n    for sentence in corpus:\n        tokens = sentence.split()\n        # Pad for N>1\n        if N > 1:\n            tokens = [\"<bol>\"] * (N - 1) + tokens + [\"<eol>\"]\n        # Unigram case\n        if N == 1:\n            for tok in tokens:\n                ngram_counts[(tok,)] += 1\n            total_unigrams += len(tokens)\n        else:\n            # Count N-grams and their (N-1)-prefixes\n            L = len(tokens)\n            for i in range(L - N + 1):\n                gram = tuple(tokens[i : i + N])\n                pref = tuple(tokens[i : i + N - 1])\n                ngram_counts[gram] += 1\n                prefix_counts[pref] += 1\n\n    # Prepare the query\n    q_tokens = query.split()\n    if N > 1:\n        q_tokens = [\"<bol>\"] * (N - 1) + q_tokens + [\"<eol>\"]\n    # If the query is too short, return -inf\n    if len(q_tokens) < N:\n        return float(\"-inf\")\n\n    # Compute total log-probability\n    log_prob = 0.0\n    for i in range(len(q_tokens) - N + 1):\n        gram = tuple(q_tokens[i : i + N])\n        count_ng = ngram_counts.get(gram, 0)\n        if count_ng == 0:\n            # Unseen N-gram: zero probability\n            return float(\"-inf\")\n\n        if N == 1:\n            denom = total_unigrams\n        else:\n            pref = tuple(q_tokens[i : i + N - 1])\n            denom = prefix_counts.get(pref, 0)\n\n        # If prefix never occurred, also zero probability\n        if denom == 0:\n            return float(\"-inf\")\n\n        prob = count_ng / denom\n        log_prob += math.log(prob)\n\n    # Round to 4 decimal places\n    return round(log_prob, 4)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    corpus = [\"a b a\", \"b a b\"]\n    print(ngram_log_prob(corpus, \"a b\", 1))  # Expected: -1.3863\n    print(ngram_log_prob(corpus, \"a b\", 2))  # Example for bigrams\n```"}
{"task_id": 302, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n, d = X.shape\n    # 1. Build the weighted adjacency matrix W\n    # Compute pairwise Euclidean distances\n    # Using the identity: ||x - y||^2 = ||x||^2 + ||y||^2 - 2 x\u00b7y\n    sq_norms = np.sum(X * X, axis=1)\n    dist_sq = sq_norms[:, None] + sq_norms[None, :] - 2 * (X @ X.T)\n    # Numerical safety\n    dist_sq = np.maximum(dist_sq, 0.0)\n    dist = np.sqrt(dist_sq)\n    # Build W\n    W = 1.0 / (1.0 + dist)\n    np.fill_diagonal(W, 0.0)\n\n    # 2. Compute the (unnormalised) graph Laplacian L = D - W\n    D = np.diag(W.sum(axis=1))\n    L = D - W\n\n    # 3. Eigen-decomposition, take the first k eigenvectors (smallest eigenvalues)\n    # Use eigh since L is symmetric\n    eigvals, eigvecs = np.linalg.eigh(L)\n    # Sort eigenvalues ascending and pick the first k\n    idx = np.argsort(eigvals)\n    E = eigvecs[:, idx[:k]]  # shape (n, k)\n\n    # 4. Run k-means on rows of E\n    # Initialize centroids as the first k rows of E\n    centroids = E[:k, :].copy()\n    labels = np.zeros(n, dtype=int)\n\n    for iteration in range(100):\n        # Compute distances from each point (row of E) to each centroid\n        # distance^2 matrix of shape (n, k)\n        # Using ||u - v||^2 = ||u||^2 + ||v||^2 - 2 u\u00b7v\n        u_norms = np.sum(E * E, axis=1)[:, None]        # shape (n,1)\n        c_norms = np.sum(centroids * centroids, axis=1)  # shape (k,)\n        # compute distances squared\n        dists_sq = u_norms + c_norms[None, :] - 2 * (E @ centroids.T)\n        new_labels = np.argmin(dists_sq, axis=1)\n\n        # Check for convergence\n        if iteration > 0 and np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # Update centroids\n        for j in range(k):\n            members = E[labels == j]\n            if members.shape[0] == 0:\n                # empty cluster: reinitialize to a random row of E\n                rand_idx = np.random.randint(0, n)\n                centroids[j] = E[rand_idx]\n            else:\n                centroids[j] = members.mean(axis=0)\n\n    return labels.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create some toy data\n    np.random.seed(0)\n    # Two blobs in 2D\n    X1 = np.random.randn(50, 2) + np.array([0, 0])\n    X2 = np.random.randn(50, 2) + np.array([5, 5])\n    X = np.vstack([X1, X2])\n    labels = spectral_clustering(X, k=2)\n    print(labels)\n```"}
{"task_id": 303, "completion_id": 0, "completion": "```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    # Determine indices for previous and current entries\n    prev_idx = ix - 1 if ix > 0 else 0\n\n    # Unpack the mine/label pairs\n    prev_mine, prev_label = params[prev_idx]\n    curr_mine, curr_label = params[ix]\n\n    # Lookup the gold outputs\n    gold_prev = golds[prev_label]\n    gold_curr = golds[curr_label]\n\n    # Build header and footer with exact dash counts\n    header = \"-\" * 25 + \" DEBUG \" + \"-\" * 25\n    footer = \"-\" * 23 + \" END DEBUG \" + \"-\" * 23\n\n    # Assemble the lines\n    lines = [\n        header,\n        f\"Mine (prev) [{prev_label}]:\",\n        prev_mine,\n        \"\",\n        f\"Theirs (prev) [{prev_label}]:\",\n        gold_prev,\n        \"\",\n        f\"Mine [{curr_label}]:\",\n        curr_mine,\n        \"\",\n        f\"Theirs [{curr_label}]:\",\n        gold_curr + warn_str,\n        footer\n    ]\n\n    # Join with newline characters\n    return \"\\n\".join(lines)\n```"}
{"task_id": 304, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu = 0.0,\n    V = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility).\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    # Ensure X and y are arrays\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2D array of shape (N, M)\")\n    N, M = X.shape\n    if y.shape[0] != N:\n        raise ValueError(\"y must have length N (number of rows in X)\")\n\n    # Add intercept column if requested\n    if fit_intercept:\n        X = np.hstack([np.ones((N, 1)), X])\n        M += 1  # updated feature count\n\n    # Build prior mean vector mu_vec\n    if np.isscalar(mu):\n        mu_vec = np.full((M,), float(mu))\n    else:\n        mu_arr = np.asarray(mu, dtype=float).ravel()\n        if mu_arr.shape[0] != M:\n            raise ValueError(f\"mu must be length {M} when fit_intercept={fit_intercept}\")\n        mu_vec = mu_arr\n\n    # Build prior covariance matrix V_mat\n    if V is None:\n        V_mat = np.eye(M)\n    elif np.isscalar(V):\n        V_mat = float(V) * np.eye(M)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 1:\n            if V_arr.shape[0] != M:\n                raise ValueError(f\"V diagonal must have length {M}\")\n            V_mat = np.diag(V_arr)\n        elif V_arr.ndim == 2:\n            if V_arr.shape != (M, M):\n                raise ValueError(f\"V must be of shape ({M}, {M})\")\n            V_mat = V_arr\n        else:\n            raise ValueError(\"V must be None, a scalar, 1-D or 2-D array\")\n\n    # Compute V^{-1}\n    try:\n        V_inv = np.linalg.inv(V_mat)\n    except np.linalg.LinAlgError:\n        raise np.linalg.LinAlgError(\"Prior covariance V is singular and non-invertible\")\n\n    # Compute posterior precision and covariance (up to \u03c3\u00b2)\n    XtX = X.T @ X\n    precision = V_inv + XtX\n    try:\n        Sigma_b = np.linalg.inv(precision)\n    except np.linalg.LinAlgError:\n        raise np.linalg.LinAlgError(\"Posterior precision matrix is singular and non-invertible\")\n\n    # Compute right-hand side: V^{-1} \u03bc + X^T y\n    rhs = V_inv @ mu_vec + X.T @ y\n\n    # Posterior mean\n    mu_b = Sigma_b @ rhs\n\n    # Round to 4 decimal places and return as list\n    return np.round(mu_b, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Simulate some data\n    np.random.seed(0)\n    N, M = 100, 3\n    X = np.random.randn(N, M)\n    true_b = np.array([1.5, -2.0, 0.5])\n    y = X @ true_b + np.random.randn(N) * 0.5\n\n    # Compute posterior mean with default priors\n    b_map = bayesian_posterior_mean(X, y, mu=0.0, V=None, fit_intercept=False)\n    print(\"MAP coefficients (no intercept):\", b_map)\n\n    # With intercept\n    b_map_int = bayesian_posterior_mean(X, y, mu=0.0, V=1.0, fit_intercept=True)\n    print(\"MAP coefficients (with intercept):\", b_map_int)\n```"}
{"task_id": 308, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    The returned matrix has n_filters rows and N//2 + 1 columns. Each row\n    is a triangular filter defined on the Mel scale.\n    \"\"\"\n    # Default max frequency is Nyquist\n    if max_freq is None:\n        max_freq = fs / 2\n\n    # Helper conversions\n    def hz_to_mel(f):\n        return 2595.0 * np.log10(1.0 + f / 700.0)\n\n    def mel_to_hz(m):\n        return 700.0 * (10.0**(m / 2595.0) - 1.0)\n\n    # Compute Mel\u2010scale points\n    mel_min = hz_to_mel(min_freq)\n    mel_max = hz_to_mel(max_freq)\n    mel_points = np.linspace(mel_min, mel_max, n_filters + 2)\n\n    # Convert back to Hertz -- these are filter corner freqs\n    freq_points = mel_to_hz(mel_points)\n\n    # Center freqs of DFT bins\n    bin_indices = np.arange(0, N // 2 + 1)\n    bin_freqs = bin_indices * (fs / N)\n\n    # Build filterbank\n    filters = []\n    for i in range(n_filters):\n        f_left = freq_points[i]\n        f_center = freq_points[i + 1]\n        f_right = freq_points[i + 2]\n\n        # Slopes of rising and falling edges\n        # Avoid division by zero in degenerate cases\n        denom_left = f_center - f_left if f_center != f_left else 1.0\n        denom_right = f_right - f_center if f_right != f_center else 1.0\n\n        # Compute triangular weights\n        slope_up = (bin_freqs - f_left) / denom_left\n        slope_down = (f_right - bin_freqs) / denom_right\n        weights = np.minimum(slope_up, slope_down)\n        weights = np.maximum(weights, 0.0)\n\n        # Normalize to unit area in Mel scale, if requested\n        if normalize:\n            weights *= 2.0 / (f_right - f_left)\n\n        # Round to 4 decimal places\n        weights = np.round(weights, 4)\n\n        filters.append(weights.tolist())\n\n    return filters\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Build a 20\u2010filter Mel bank for N=1024 FFT, sampling rate 44 kHz\n    bank = mel_filterbank(N=1024, n_filters=20, fs=44000)\n    print(f\"Filterbank shape: {len(bank)} x {len(bank[0])}\")\n    # Print the first filter\n    print(\"First filter:\", bank[0])\n```"}
{"task_id": 312, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    # Special cases\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n\n    # Blackman\u2013Harris coefficients\n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n\n    # Determine denominator for the cosine arguments\n    # Symmetric: use N = L-1\n    # Periodic: generate as if length L+1 and drop last -> N = L\n    L = window_len\n    N = L - 1 if symmetric else L\n\n    # Sample indices\n    n = np.arange(0, L)\n\n    # Compute the window\n    w = (a0\n         - a1 * np.cos(2 * np.pi * n / N)\n         + a2 * np.cos(4 * np.pi * n / N)\n         - a3 * np.cos(6 * np.pi * n / N))\n\n    # Round to 4 decimal places and convert to list of floats\n    return np.round(w, 4).tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(\"Symmetric, length=8 :\", blackman_harris(8, symmetric=True))\n    print(\"Periodic, length=8  :\", blackman_harris(8, symmetric=False))\n    print(\"Zero length        :\", blackman_harris(0))\n    print(\"Unit length        :\", blackman_harris(1))\n```\n\nExplanation of key steps:\n- Handle special cases `window_len <= 0` and `window_len == 1` as required.\n- Use `N = L-1` for the symmetric form, or `N = L` for the periodic form (which simulates an `(L+1)`-point symmetric window with the last point dropped).\n- Compute the Blackman\u2013Harris window via its cosine-sum definition.\n- Round each sample to four decimal places before returning the result as a plain Python list."}
{"task_id": 313, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function implemented below.\n    The result is rounded to 4 decimal places.\n    \"\"\"\n    def digamma(x):\n        # Use recurrence \u03c8(x) = \u03c8(x+1) \u2212 1/x to shift x up to \u2265 6\n        result = 0.0\n        while x < 6.0:\n            result -= 1.0 / x\n            x += 1.0\n        # Asymptotic expansion for large x\n        inv = 1.0 / x\n        inv2 = inv * inv\n        inv4 = inv2 * inv2\n        return (result\n                + math.log(x)\n                - 0.5 * inv\n                - inv2 / 12.0\n                + inv4 / 120.0\n                - inv4 * inv2 / 252.0)\n\n    # Extract the needed parameters, ensuring Python floats\n    row = gamma[d]\n    alpha_dt = float(row[t])\n    sum_alpha = float(sum(row))\n\n    psi_dt = digamma(alpha_dt)\n    psi_sum = digamma(sum_alpha)\n    val = psi_dt - psi_sum\n\n    # Round to 4 decimal places and return\n    return round(val, 4)\n```\n"}
{"task_id": 317, "completion_id": 0, "completion": "```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\n\n    See the task description for the exact required format.\n\n    Args:\n        params: A list where every element is a tuple (prediction, label).\n        golds:  A dictionary mapping each label to its gold/expected string.\n        ix:     The index inside `params` for which the detailed comparison\n                 is built.\n        warn_str: Optional extra text appended before the footer.\n\n    Returns:\n        A string that follows the specified layout exactly.\n    \"\"\"\n    # Determine previous index (clamp at 0)\n    prev_ix = max(ix - 1, 0)\n\n    # Unpack previous and current predictions and labels\n    prev_pred, prev_label = params[prev_ix]\n    curr_pred, curr_label = params[ix]\n\n    # Look up gold strings\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n\n    # Build header and footer\n    header = \"-\" * 25 + \" DEBUG \" + \"-\" * 25 + \"\\n\"\n    footer = \"-\" * 23 + \" END DEBUG \" + \"-\" * 23\n\n    # Assemble the debug report\n    report = []\n    report.append(header)\n    report.append(f\"Mine (prev) [{prev_label}]:\\n\")\n    report.append(prev_pred + \"\\n\\n\")\n    report.append(f\"Theirs (prev) [{prev_label}]:\\n\")\n    report.append(prev_gold + \"\\n\\n\")\n    report.append(f\"Mine [{curr_label}]:\\n\")\n    report.append(curr_pred + \"\\n\\n\")\n    report.append(f\"Theirs [{curr_label}]:\\n\")\n    report.append(curr_gold + warn_str + \"\\n\")\n    report.append(footer)\n\n    return \"\".join(report)\n```"}
{"task_id": 318, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples containing the training labels. Each label\n        is either -1 or 1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds (weak learners) to use.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 or 1) for each sample in X_test.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n\n    # Initialize weights\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    # To store the parameters of the weak learners\n    learners = []  # each element: (feature_index, threshold, polarity, alpha)\n\n    # Helper sign function: sign(z) = -1 if z < 0 else +1\n    def sign(z):\n        return np.where(z < 0, -1, 1)\n\n    for m in range(n_estimators):\n        # Find the best decision stump\n        best_error = float('inf')\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n        best_pred = None\n\n        # Loop over features\n        for feature_i in range(n_features):\n            X_col = X_train[:, feature_i]\n            # Get sorted unique values\n            unique_vals = np.unique(X_col)\n            # Potential thresholds: midpoints between consecutive unique values\n            if len(unique_vals) > 1:\n                thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n            else:\n                thresholds = unique_vals\n\n            for t in thresholds:\n                # Try both polarities\n                for p in [1, -1]:\n                    # Predict: h(x) = p * sign(x_j - t)\n                    preds = p * sign(X_col - t)\n                    # Weighted error\n                    misclassified = preds != y_train\n                    error = np.dot(w, misclassified.astype(float))\n\n                    if error < best_error:\n                        best_error = error\n                        best_feature = feature_i\n                        best_threshold = t\n                        best_polarity = p\n                        best_pred = preds.copy()\n\n        # Avoid division by zero or log of zero\n        eps = best_error\n        eps = max(eps, 1e-10)\n        eps = min(eps, 1 - 1e-10)\n\n        # Compute learner weight\n        alpha = 0.5 * math.log((1 - eps) / eps)\n\n        # Save this weak learner\n        learners.append((best_feature, best_threshold, best_polarity, alpha))\n\n        # Update sample weights\n        # w_i <- w_i * exp(-alpha * y_i * h_m(x_i))\n        w *= np.exp(-alpha * y_train * best_pred)\n        w /= np.sum(w)\n\n    # Now predict on X_test\n    n_test = X_test.shape[0]\n    agg_preds = np.zeros(n_test)\n\n    for feature_i, threshold, polarity, alpha in learners:\n        preds = polarity * sign(X_test[:, feature_i] - threshold)\n        agg_preds += alpha * preds\n\n    # Final prediction: sign of aggregated predictions\n    y_pred = sign(agg_preds)\n    # Ensure integers -1 or 1\n    return y_pred.astype(int).tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Toy dataset\n    X_train = np.array([[1.0], [2.0], [3.0], [4.0]])\n    y_train = np.array([-1, -1, 1, 1])\n    X_test = np.array([[1.5], [2.5], [3.5]])\n\n    preds = adaboost_predict(X_train, y_train, X_test, n_estimators=10)\n    print(\"Predictions:\", preds)\n```"}
{"task_id": 329, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When True, return only the non-negative frequency bins (length floor(N/2)+1);\n        when False, return all N bins as in numpy.fft.fftfreq.\n        Default is True.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.\n        Returns an empty list if N <= 0 or fs <= 0.\n    \"\"\"\n    # Guard against invalid parameters\n    if N <= 0 or fs <= 0:\n        return []\n\n    # Compute frequencies\n    if positive_only:\n        # Non-negative part: length floor(N/2)+1\n        freqs = np.fft.rfftfreq(N, d=1.0/fs)\n    else:\n        # Full spectrum, ordered same as np.fft.fftfreq\n        freqs = np.fft.fftfreq(N, d=1.0/fs)\n\n    # Round to 4 decimal places and return as Python list\n    return np.round(freqs, 4).tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(\"N=8, fs=16kHz, positive only:\")\n    print(dft_bins(8, 16000, True))\n    # [0.0, 2000.0, 4000.0, 6000.0, 8000.0]\n\n    print(\"N=8, fs=16kHz, full spectrum:\")\n    print(dft_bins(8, 16000, False))\n    # [0.0, 2000.0, 4000.0, 6000.0, -8000.0, -6000.0, -4000.0, -2000.0]\n```"}
{"task_id": 331, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y_true = np.asarray(y_true, dtype=float).ravel()\n    predictions = np.asarray(predictions, dtype=float)\n    oob_mask = np.asarray(oob_mask, dtype=bool)\n    \n    # Check shapes\n    if predictions.shape != oob_mask.shape:\n        raise ValueError(\"predictions and oob_mask must have the same shape\")\n    if predictions.shape[0] != y_true.shape[0]:\n        raise ValueError(\"Number of samples in y_true must match predictions\")\n\n    n_samples = y_true.shape[0]\n    squared_errors = []\n\n    # Iterate over each sample\n    for i in range(n_samples):\n        mask_i = oob_mask[i]\n        if not np.any(mask_i):\n            # No OOB predictions for this sample\n            continue\n        oob_preds = predictions[i, mask_i]\n        avg_pred = np.mean(oob_preds)\n        se = (avg_pred - y_true[i]) ** 2\n        squared_errors.append(se)\n    \n    # If no sample had any OOB predictions, return -1\n    if len(squared_errors) == 0:\n        return -1.0\n    \n    # Compute OOB-MSE and round to 4 decimal places\n    mse = np.mean(squared_errors)\n    return round(float(mse), 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # True values for 5 samples\n    y = [3.0, -0.5, 2.0, 7.0, 5.0]\n    # Predictions from 4 trees\n    preds = [\n        [2.5, 3.2, 2.9, 3.1],\n        [-0.3, -0.7, -0.6, -0.4],\n        [2.1, 2.0, 2.2, 1.9],\n        [7.2, 6.8, 7.1, 7.0],\n        [4.9, 5.2, 5.1, 5.0]\n    ]\n    # OOB mask: True if the tree did NOT train on the sample\n    mask = [\n        [True, False, True, False],\n        [False, False, False, False],  # sample 1 has no OOB predictions\n        [True, True, False, False],\n        [False, True, True, False],\n        [True, True, True, True]\n    ]\n    print(\"OOB-MSE:\", oob_mse(y, preds, mask))\n```"}
{"task_id": 332, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of transactions an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node contains two keys:\n        \"support\" and \"children\" (the latter mapping item \u2192 child-node).\n    \"\"\"\n    # 1. Count item support across distinct transactions\n    #    (each transaction contributes at most +1 per item)\n    item_counts = Counter()\n    for txn in transactions:\n        for item in set(txn):\n            item_counts[item] += 1\n\n    # 2. Filter out infrequent items\n    frequent_items = {\n        item: cnt for item, cnt in item_counts.items() if cnt >= min_support\n    }\n    if not frequent_items:\n        # no items meet min_support \u2192 return empty root\n        return {\"support\": 0, \"children\": {}}\n\n    # 3. Global ordering: by decreasing support, then alphabetically\n    #    We'll build a map item \u2192 its rank index\n    sorted_items = sorted(\n        frequent_items.items(),\n        key=lambda x: (-x[1], x[0])\n    )\n    # item_order maps item to its position in sorted_items\n    item_order = {item: idx for idx, (item, _) in enumerate(sorted_items)}\n\n    # 4. Build the FP-tree\n    root = {\"support\": 0, \"children\": {}}\n\n    for txn in transactions:\n        # a) keep only frequent items, remove dupes, sort by global order\n        filtered = [it for it in set(txn) if it in item_order]\n        ordered = sorted(filtered, key=lambda it: item_order[it])\n\n        # b) insert into the tree, updating support counts\n        node = root\n        node[\"support\"] += 1  # root sees every transaction\n        for item in ordered:\n            if item not in node[\"children\"]:\n                # create new child\n                node[\"children\"][item] = {\"support\": 0, \"children\": {}}\n            # descend into child\n            node = node[\"children\"][item]\n            node[\"support\"] += 1\n\n    return root\n\n\n# Example usage\nif __name__ == \"__main__\":\n    txns = [\n        [\"a\", \"b\", \"c\"],\n        [\"a\", \"b\"],\n        [\"a\", \"c\"],\n        [\"b\", \"c\"],\n        [\"a\", \"b\", \"c\"],\n    ]\n    min_sup = 2\n    tree = build_fp_tree(txns, min_sup)\n    import json\n    print(json.dumps(tree, indent=2))\n```\n\nExplanation of key steps:\n1. We count each item once per transaction to get its support.\n2. We drop any item whose support is below `min_support`.\n3. We define a total order of the remaining items by sorting on (support descending, item name ascending).\n4. For each transaction, we filter, dedupe, and sort its items by the global order. Then we traverse the tree from the root, creating or updating child nodes and bumping support counts along the path."}
{"task_id": 336, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_kernel(X: list[list[int | float]] | \"np.ndarray\",\n                  Y: list[list[int | float]] | \"np.ndarray\" | None = None,\n                  c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function must set\n        ``Y = X``.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y have\n        incompatible shapes (different number of columns).\n    \"\"\"\n    # Convert X to numpy array of floats\n    if isinstance(X, list):\n        if len(X) == 0:\n            return []  # empty X yields empty kernel matrix\n        X_arr = np.asarray(X, dtype=float)\n    elif isinstance(X, np.ndarray):\n        X_arr = X.astype(float, copy=False)\n    else:\n        return -1\n\n    # Check X is 2-D\n    if X_arr.ndim != 2:\n        return -1\n\n    # Handle Y\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        if isinstance(Y, list):\n            if len(Y) == 0:\n                # empty Y: shape (0, C) where C = X's feature dim\n                Y_arr = np.zeros((0, X_arr.shape[1]), dtype=float)\n            else:\n                Y_arr = np.asarray(Y, dtype=float)\n        elif isinstance(Y, np.ndarray):\n            Y_arr = Y.astype(float, copy=False)\n        else:\n            return -1\n\n        # Check Y is 2-D\n        if Y_arr.ndim != 2:\n            return -1\n\n    # Now we have X_arr and Y_arr both 2-D\n    N, Cx = X_arr.shape\n    M, Cy = Y_arr.shape\n\n    # Check dimensional compatibility\n    if Cx != Cy:\n        return -1\n\n    # Compute kernel matrix: (N x C) \u00b7 (C x M) = N x M\n    K = X_arr.dot(Y_arr.T) + float(c0)\n\n    # Round to 4 decimal places\n    K_rounded = np.round(K, 4)\n\n    # Convert to Python list of lists and return\n    return K_rounded.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Simple test\n    X = [[1, 2], [3, 4]]\n    Y = [[5, 6], [7, 8], [9, 10]]\n    print(linear_kernel(X, Y))       # 2x3 matrix\n    print(linear_kernel(X))          # 2x2 symmetric matrix\n    print(linear_kernel([], Y))      # empty X -> []\n    print(linear_kernel(X, [[1,2,3]]))  # incompatible dims -> -1\n```\n\nExplanation of key steps:\n1. Convert inputs `X` and `Y` (if provided) into NumPy arrays of type float.\n2. Handle the special case of an empty `X` (returns `[]` immediately).\n3. If `Y` is `None`, set `Y = X` to produce a symmetric kernel.\n4. Check that both `X` and `Y` are 2-D and have the same number of columns; otherwise return `-1`.\n5. Compute the kernel matrix `K = X\u00b7Y\u1d40 + c0`, round each entry to 4 decimal places, and return as a list of lists."}
{"task_id": 340, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform one Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as parameter).\n        t:     Time-step counter starting at 1.\n        m:     First moment estimate from the previous step.\n        v:     Second moment estimate from the previous step.\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_param, new_m, new_v) as python lists,\n        each rounded to 8 decimal places.\n    \"\"\"\n    # Update biased first moment estimate\n    m_t = beta1 * m + (1 - beta1) * grad\n    # Update biased second raw moment estimate\n    v_t = beta2 * v + (1 - beta2) * (grad ** 2)\n\n    # Compute bias-corrected first moment\n    m_hat = m_t / (1 - beta1 ** t)\n    # Compute bias-corrected second moment\n    v_hat = v_t / (1 - beta2 ** t)\n\n    # Update parameters\n    param_t = param - lr * m_hat / (np.sqrt(v_hat) + eps)\n\n    # Round to 8 decimal places and convert to python lists\n    param_out = np.round(param_t, 8).tolist()\n    m_out     = np.round(m_t, 8).tolist()\n    v_out     = np.round(v_t, 8).tolist()\n\n    return param_out, m_out, v_out\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # initialize\n    theta = np.array([[0.5, -0.2], [1.0, 0.1]])\n    grad  = np.array([[0.01, -0.02], [0.03, -0.01]])\n    m0    = np.zeros_like(theta)\n    v0    = np.zeros_like(theta)\n    t     = 1\n\n    theta_new, m1, v1 = adam_update(theta, grad, t, m0, v0)\n    print(\"Updated parameter:\", theta_new)\n    print(\"New first moment m_t:\", m1)\n    print(\"New second moment v_t:\", v1)\n```"}
{"task_id": 343, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                         rewards: list[float] | np.ndarray,\n                         retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    # Ensure inputs are arrays\n    theta_samples = np.asarray(theta_samples)\n    rewards = np.asarray(rewards)\n\n    # Number of samples\n    N = theta_samples.shape[0]\n    # How many elites to keep\n    num_elite = int(retain_prcnt * N)\n\n    # Indices of top rewards (descending)\n    elite_indices = np.argsort(-rewards)[:num_elite]\n    elites = theta_samples[elite_indices]\n\n    # Compute per-dimension mean and variance\n    mean = np.mean(elites, axis=0)\n    var = np.var(elites, axis=0)\n\n    # Round to 4 decimals and convert to Python lists\n    mean_rounded = np.round(mean, 4).tolist()\n    var_rounded = np.round(var, 4).tolist()\n\n    return mean_rounded, var_rounded\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    thetas = np.array([[1.0, 2.0],\n                       [2.0, 1.0],\n                       [0.5, 0.5],\n                       [3.0, 1.5]])\n    rews = [10, 20, 5, 15]\n    retain_fraction = 0.5  # keep top 2 out of 4\n\n    mu, sigma2 = cross_entropy_update(thetas, rews, retain_fraction)\n    print(\"Updated mean:     \", mu)\n    print(\"Updated variance: \", sigma2)\n```"}
{"task_id": 353, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    # Handle numpy arrays by flattening; otherwise assume iterable\n    try:\n        seq = labels.ravel()\n    except AttributeError:\n        seq = labels\n\n    # Materialize into a list\n    seq_list = list(seq)\n    n = len(seq_list)\n    if n == 0:\n        return 0.0\n\n    counts = Counter(seq_list)\n    ent = 0.0\n    for count in counts.values():\n        p = count / n\n        if p > 0:\n            ent -= p * math.log2(p)\n\n    return round(ent, 5)\n\n\n# Example usage and simple tests\nif __name__ == \"__main__\":\n    # Empty input\n    print(entropy([]))               # 0.0\n\n    # All same label\n    print(entropy([\"a\", \"a\", \"a\"]))  # 0.0\n\n    # Two labels equally likely\n    print(entropy([0, 1, 0, 1]))     # 1.0\n\n    # Three labels with probabilities (0.5, 0.25, 0.25)\n    labels = [\"x\"] * 50 + [\"y\"] * 25 + [\"z\"] * 25\n    print(entropy(labels))           # 1.5\n```"}
{"task_id": 354, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"Performs a 2-D convolution (cross-correlation) via im2col + GEMM.\n\n    Args:\n        X: NumPy array of shape (n_ex, in_rows, in_cols, in_ch) representing the input batch.\n        W: NumPy array of shape (kernel_rows, kernel_cols, in_ch, out_ch) containing the kernels.\n        stride: Stride of the convolution.\n        pad: Padding specification \u2013 integer, 2-tuple, 4-tuple or the string 'same'.\n        dilation: Optional dilation factor. 0 corresponds to a normal convolution.\n\n    Returns:\n        The convolution result as a Python list obtained through numpy.ndarray.tolist().\n    \"\"\"\n    n_ex, in_h, in_w, in_ch = X.shape\n    k_h, k_w, _, out_ch = W.shape\n    # effective kernel size with dilation\n    dil = dilation\n    eff_k_h = k_h + (k_h - 1) * dil\n    eff_k_w = k_w + (k_w - 1) * dil\n\n    # parse pad\n    if isinstance(pad, str) and pad.lower() == 'same':\n        # output dims for 'same' (ceil behavior)\n        out_h = int(np.ceil(in_h / stride))\n        out_w = int(np.ceil(in_w / stride))\n        pad_h = max((out_h - 1) * stride + eff_k_h - in_h, 0)\n        pad_w = max((out_w - 1) * stride + eff_k_w - in_w, 0)\n        pr1 = pad_h // 2\n        pr2 = pad_h - pr1\n        pc1 = pad_w // 2\n        pc2 = pad_w - pc1\n    else:\n        # numeric pad\n        if isinstance(pad, int):\n            pr1 = pr2 = pc1 = pc2 = pad\n        elif isinstance(pad, (list, tuple)) and len(pad) == 2:\n            pr1 = pr2 = pad[0]\n            pc1 = pc2 = pad[1]\n        elif isinstance(pad, (list, tuple)) and len(pad) == 4:\n            pr1, pr2, pc1, pc2 = pad\n        else:\n            raise ValueError(\"Invalid pad specification\")\n        # output dims\n        out_h = (in_h + pr1 + pr2 - eff_k_h) // stride + 1\n        out_w = (in_w + pc1 + pc2 - eff_k_w) // stride + 1\n\n    # pad the input\n    X_pad = np.pad(\n        X,\n        ((0, 0), (pr1, pr2), (pc1, pc2), (0, 0)),\n        mode='constant',\n        constant_values=0\n    )\n\n    # compute im2col indices\n    # number of patches per image\n    Np = out_h * out_w\n    # total patches\n    N = n_ex * Np\n    # size of each patch vector\n    K = k_h * k_w * in_ch\n\n    # offsets for kernel positions\n    # row offsets\n    row_off = np.repeat(np.arange(k_h) * (dil + 1), k_w * in_ch)\n    # col offsets\n    col_off = np.tile(np.repeat(np.arange(k_w) * (dil + 1), in_ch), k_h)\n    # channel offsets\n    ch_off  = np.tile(np.arange(in_ch), k_h * k_w)\n\n    # spatial locations in output\n    i_out = np.repeat(np.arange(out_h) * stride, out_w)\n    j_out = np.tile(np.arange(out_w) * stride, out_h)\n\n    # prepare the patch matrix\n    M = np.empty((N, K), dtype=X.dtype)\n\n    for ex in range(n_ex):\n        # extract patches for example ex\n        Xp = X_pad[ex]\n        # compute absolute indices\n        rows = row_off[:, None] + i_out[None, :]\n        cols = col_off[:, None] + j_out[None, :]\n        # gather: shape (K, Np)\n        patches = Xp[rows, cols, ch_off[:, None]]\n        # transpose to (Np, K)\n        M[ex * Np:(ex + 1) * Np, :] = patches.T\n\n    # reshape kernels into (K, out_ch)\n    W_flat = W.reshape(K, out_ch)\n\n    # GEMM\n    out = M.dot(W_flat)  # shape (N, out_ch)\n    # reshape to (n_ex, out_h, out_w, out_ch)\n    out = out.reshape(n_ex, out_h, out_w, out_ch)\n\n    # convert to Python list\n    return out.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # simple test\n    np.random.seed(0)\n    X = np.random.randn(2, 5, 5, 3)      # 2 examples, 5x5, 3 channels\n    W = np.random.randn(3, 3, 3, 4)      # 3x3 kernels, in_ch=3, out_ch=4\n    stride = 1\n    pad = 'same'\n    d = 0\n    y = conv2D(X, W, stride, pad, dilation=d)\n    print(\"Output shape:\", np.array(y).shape)\n```\n\nExplanation of the main steps:\n\n1. **Padding**  \n   Parse the `pad` argument (integer, 2\u2010tuple, 4\u2010tuple or `'same'`) to determine how many rows/columns to add on each side. The `'same'` option uses the standard formula to keep the output spatial size equal (or as close as possible with `stride>1`) to the input.\n\n2. **im2col**  \n   \u2013 Compute the *effective* kernel size by accounting for dilation.  \n   \u2013 Build index offsets (`row_off`, `col_off`, `ch_off`) so that each patch can be gathered in one fancy\u2010indexing step.  \n   \u2013 For each example in the batch, extract all sliding patches of size `k_h\u00d7k_w\u00d7in_ch` into a matrix `M` of shape `(n_ex * out_h * out_w, k_h * k_w * in_ch)`.\n\n3. **GEMM (Matrix Multiply)**  \n   Flatten the kernels `W` into a matrix `W_flat` of shape `(k_h * k_w * in_ch, out_ch)`, then compute `M.dot(W_flat)`.  The result has shape `(n_ex * out_h * out_w, out_ch)` and is reshaped back to `(n_ex, out_h, out_w, out_ch)`.\n\n4. **Return**  \n   Convert the final NumPy array to a nested Python list via `.tolist()` as required."}
{"task_id": 355, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array in which rows correspond to samples and columns to features.\n    n_components : int\n        Number of principal components to retain (must be \u22651).\n    solver : str, optional (default=\"svd\")\n        ``\"svd\"`` to use singular value decomposition or ``\"eigen\"`` to use\n        eigen-decomposition of the covariance matrix.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 The projected data (scores) as a list of lists \u2013 each inner list is a\n          sample represented in the new sub-space.\n        \u2022 The list of explained variance ratios corresponding to the kept\n          components.\n    \"\"\"\n    # Input checks\n    if solver not in (\"svd\", \"eigen\"):\n        raise ValueError(f\"Unknown solver '{solver}'. Use 'svd' or 'eigen'.\")\n    if n_components < 1:\n        raise ValueError(\"n_components must be at least 1.\")\n    if data.ndim != 2:\n        raise ValueError(\"Input data must be a 2D array.\")\n\n    # 1. Mean centering\n    X = np.array(data, dtype=float)\n    n_samples, n_features = X.shape\n    mean_ = X.mean(axis=0)\n    X_centered = X - mean_\n\n    # 2. Compute directions and explained variances\n    if solver == \"svd\":\n        # SVD on centered data\n        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n        # Principal directions (loadings)\n        directions = Vt.T  # shape (n_features, n_components_available)\n        # Eigenvalues of covariance = S^2 / (n_samples - 1)\n        eigvals = (S ** 2) / (n_samples - 1)\n    else:\n        # Covariance matrix\n        cov = np.cov(X_centered, rowvar=False, ddof=1)\n        eigvals_all, eigvecs_all = np.linalg.eigh(cov)\n        # Sort in descending order\n        idx_desc = np.argsort(eigvals_all)[::-1]\n        eigvals = eigvals_all[idx_desc]\n        directions = eigvecs_all[:, idx_desc]\n\n    # 3. Decide how many components to keep\n    max_components = directions.shape[1]\n    k = min(n_components, max_components)\n    eigvals = eigvals[:k]\n    directions = directions[:, :k]\n\n    # 5. Project data\n    scores = X_centered.dot(directions)  # shape (n_samples, k)\n\n    # 4. Deterministic sign fix\n    for j in range(k):\n        comp = directions[:, j]\n        # find first non-zero loading\n        for val in comp:\n            if val != 0:\n                if val < 0:\n                    directions[:, j] *= -1\n                    scores[:, j] *= -1\n                break\n\n    # 6. Compute explained variance ratio\n    total_var = eigvals.sum()\n    if total_var <= 0:\n        # all zero variance (e.g., constant data)\n        exp_var_ratio = np.zeros_like(eigvals)\n    else:\n        exp_var_ratio = eigvals / total_var\n\n    # Round final results\n    scores_rounded = np.round(scores, 4).tolist()\n    exp_var_ratio_rounded = np.round(exp_var_ratio, 4).tolist()\n\n    return scores_rounded, exp_var_ratio_rounded\n```\n\nExplanation of key steps:\n1. **Mean centering**: subtract the column-wise mean from each feature.  \n2. **Solver choice**:  \n   - For `\"svd\"`, use `np.linalg.svd` on the centered data; singular values yield component variances.  \n   - For `\"eigen\"`, build the sample covariance matrix (`ddof=1`) and apply `np.linalg.eigh`.  \n3. **Sorting and truncation**: sort components by descending variance, then keep up to `n_components`.  \n4. **Deterministic signs**: inspect each loading vector; if its first nonzero entry is negative, flip the sign of that vector and its corresponding score column.  \n5. **Projection**: compute the score matrix `X_centered @ directions`.  \n6. **Explained variance ratio**: each component's variance divided by total variance.  \n7. **Rounding**: only the final `scores` and `explained_variance_ratio` are rounded to four decimal places and converted to Python lists."}
{"task_id": 356, "completion_id": 0, "completion": "```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        # Expect leaf.value to be a sequence of probabilities\n        probs = leaf.value\n        # Find the index of the maximum probability (ties -> smallest index)\n        # Using a simple Python loop to avoid external dependencies\n        max_idx = 0\n        max_val = probs[0]\n        for i, p in enumerate(probs[1:], start=1):\n            if p > max_val:\n                max_val = p\n                max_idx = i\n        return max_idx\n    else:\n        # Regression: return the stored scalar (mean)\n        return leaf.value\n\n\n# Example usage and simple tests\nif __name__ == \"__main__\":\n    # Classification leaf: probabilities over 3 classes\n    leaf_cls = Leaf([0.1, 0.4, 0.4])\n    pred_cls = leaf_predict(leaf_cls, classifier=True)\n    print(f\"Classification prediction: {pred_cls}\")  # Expected: 1 (tie broken in favor of smaller index)\n\n    # Regression leaf: scalar mean\n    leaf_reg = Leaf(5.23)\n    pred_reg = leaf_predict(leaf_reg, classifier=False)\n    print(f\"Regression prediction: {pred_reg}\")      # Expected: 5.23\n\n    # Another classification leaf\n    leaf_cls2 = Leaf([0.0, 0.0, 0.0, 0.7, 0.7])\n    pred_cls2 = leaf_predict(leaf_cls2, classifier=True)\n    print(f\"Classification prediction: {pred_cls2}\")  # Expected: 3\n```"}
{"task_id": 357, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    tol = 1e-6\n    n_samples, n_features = data.shape\n\n    # Special case K=1: the centroid is the global mean\n    if K == 1:\n        centroid = data.mean(axis=0)\n        return [[round(float(x), 4) for x in centroid]]\n\n    # Initialize centroids as first K samples (make a float copy)\n    centroids = data[:K].astype(float, copy=True)\n\n    for _ in range(max_iters):\n        # Compute distances from points to centroids: shape (n_samples, K)\n        # Using broadcasting\n        diffs = data[:, np.newaxis, :] - centroids[np.newaxis, :, :]  # (n, K, d)\n        dists = np.linalg.norm(diffs, axis=2)  # (n, K)\n        # Assign each sample to the nearest centroid\n        labels = np.argmin(dists, axis=1)     # (n,)\n\n        # Recompute centroids\n        new_centroids = np.zeros_like(centroids)\n        for k in range(K):\n            members = data[labels == k]\n            if members.shape[0] > 0:\n                new_centroids[k] = members.mean(axis=0)\n            else:\n                # No members: keep old centroid\n                new_centroids[k] = centroids[k]\n\n        # Check for convergence\n        shifts = np.linalg.norm(new_centroids - centroids, axis=1)\n        if np.all(shifts <= tol):\n            centroids = new_centroids\n            break\n\n        centroids = new_centroids\n\n    # Round to 4 decimals and sort lexicographically\n    cent_list = [[round(float(x), 4) for x in c] for c in centroids]\n    cent_list.sort()\n\n    return cent_list\n\n# Example usage\nif __name__ == \"__main__\":\n    data = np.array([\n        [1, 1],\n        [1, 2],\n        [2, 1],\n        [8, 8],\n        [9, 8],\n        [8, 9]\n    ])\n    K = 2\n    centers = kmeans(data, K)\n    print(centers)\n    # Expected output:\n    # [[1.3333, 1.3333], [8.3333, 8.3333]]\n```"}
{"task_id": 362, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation. If a row has a sum of\n    0 the function returns -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]] or int\n        The row-normalised matrix rounded to 4 decimals, or -1 when\n        normalisation is impossible.\n    \"\"\"\n    # Convert input to a NumPy array of floats\n    arr = np.array(data, dtype=float)\n\n    # Ensure it's 2-D\n    if arr.ndim != 2:\n        raise ValueError(\"Input must be a 2-D structure\")\n\n    # Compute the sum of each row\n    row_sums = arr.sum(axis=1)\n\n    # If any row sums to zero, normalization is impossible\n    if np.any(row_sums == 0):\n        return -1\n\n    # Divide each element by its row sum\n    normalized = arr / row_sums[:, np.newaxis]\n\n    # Round to 4 decimal places\n    normalized = np.round(normalized, 4)\n\n    # Return as native Python list\n    return normalized.tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # A valid matrix\n    M = [\n        [1, 1, 2],\n        [0, 3, 3],\n        [5, 5, 0]\n    ]\n    print(\"Normalized M:\", row_stochastic_matrix(M))\n    # A matrix with a zero\u2010sum row\n    N = [\n        [1, 2, 3],\n        [0, 0, 0],  # zero-sum row\n        [4, 4, 2]\n    ]\n    print(\"Normalized N:\", row_stochastic_matrix(N))\n```"}
{"task_id": 363, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    def _check(space: Space, is_tuple: bool) -> bool:\n        # If it's not declared composite, it's continuous only if it's a Box\n        if not is_tuple:\n            return isinstance(space, Box)\n        # If it is composite, require it to be Tuple or Dict\n        if isinstance(space, Tuple):\n            return all(isinstance(s, Box) for s in space.spaces)\n        if isinstance(space, Dict):\n            return all(isinstance(s, Box) for s in space.spaces.values())\n        # If flagged as composite but not a Tuple/Dict, consider it non-continuous\n        return False\n\n    cont_action = _check(env.action_space, tuple_action)\n    cont_obs = _check(env.observation_space, tuple_obs)\n    return cont_action, cont_obs\n\n# --------------------------------- Example Usage ---------------------------------\nif __name__ == \"__main__\":\n    # Simple Box vs Discrete\n    e1 = Env(Box(0.0, 1.0, (3,)), Discrete(5))\n    print(is_continuous(e1, tuple_action=False, tuple_obs=False))  # (True, False)\n\n    # Composite action space: all Box\n    act = Tuple([Box(-1.0, 1.0, (2,)), Box(0.0, 5.0, (1,))])\n    obs = Dict({\"state\": Box(0.0, 10.0, (4,)), \"aux\": Discrete(3)})\n    e2 = Env(act, obs)\n    print(is_continuous(e2, tuple_action=True, tuple_obs=True))  # (True, False)\n\n    # Composite observation space: all Box\n    obs2 = Dict({\"a\": Box(0.0, 1.0, (1,)), \"b\": Box(-5.0, 5.0, (2, 2))})\n    e3 = Env(Discrete(2), obs2)\n    print(is_continuous(e3, tuple_action=False, tuple_obs=True))  # (False, True)\n```"}
{"task_id": 369, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_predict(\n    updates: list[np.ndarray],\n    learning_rate: float,\n    regression: bool\n) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for all samples. For regression the array shape is (n_samples,);\n        for classification it is (n_samples, n_classes).\n    learning_rate : float\n        The learning-rate hyper-parameter used during training. Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set True for regression problems and False for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        raise ValueError(\"The 'updates' list must contain at least one array.\")\n\n    # Initialize the running prediction to zeros of the same shape\n    # as a single update, with float dtype.\n    running_pred = np.zeros_like(updates[0], dtype=np.float64)\n\n    # Aggregate all tree outputs\n    for u in updates:\n        running_pred -= learning_rate * u\n\n    if regression:\n        # Regression: round to 4 decimal places\n        return np.round(running_pred, 4)\n\n    # Classification: running_pred are un-normalized logits\n    # Compute softmax row-wise to get probabilities\n    # subtract max per row for numerical stability\n    max_logits = np.max(running_pred, axis=1, keepdims=True)\n    exp_shifted = np.exp(running_pred - max_logits)\n    probs = exp_shifted / np.sum(exp_shifted, axis=1, keepdims=True)\n\n    # Return the class with highest probability for each sample\n    return np.argmax(probs, axis=1)\n```\n\nExplanation of key steps:\n- We start with a zero `running_pred` of the same shape as the first update.\n- For each tree update `u`, we subtract `learning_rate * u` from `running_pred`.\n- If `regression` is `True`, we round the final predictions to 4 decimals and return them.\n- Otherwise, we treat `running_pred` as logits for multi-class classification:\n  * Shift by the row-wise maximum for stability,\n  * Exponentiate and normalize to get probabilities,\n  * Use `argmax` to pick the predicted class per sample."}
{"task_id": 371, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\n\n    The function must support batches, multiple input/output channels, padding\n    (integer, tuple or \"same\"), arbitrary stride and dilation.  It returns\n    the output volume as a list via ndarray.tolist().\n\n    Args:\n        X: ndarray of shape (n_examples, signal_length, in_channels)\n        W: ndarray of shape (kernel_width, in_channels, out_channels)\n        stride: positive integer, the step size of the sliding window\n        pad:  int, 2-tuple, or the string \"same\" specifying the amount of zero\n              padding to add to the left and right of the signal\n        dilation: non-negative integer, number of points inserted between\n                   neighbouring kernel elements (0 \u21d2 standard convolution)\n\n    Returns:\n        A Python list representing the convolved volume with shape\n        (n_examples, output_length, out_channels).\n    \"\"\"\n    # Extract dimensions\n    n_examples, signal_length, in_channels = X.shape\n    kernel_width, k_in_channels, out_channels = W.shape\n    assert k_in_channels == in_channels, \\\n        f\"Input channels of X ({in_channels}) and W ({k_in_channels}) must match.\"\n\n    # Effective kernel size accounting for dilation\n    # If dilation = d, there are d zeros inserted between consecutive kernel taps,\n    # so the distance between tap k and k+1 is (d + 1).\n    eff_kernel = kernel_width + (kernel_width - 1) * dilation\n\n    # Determine padding on left and right\n    if isinstance(pad, str):\n        if pad.lower() != \"same\":\n            raise ValueError(\"If pad is a string, it must be 'same'.\")\n        # TensorFlow 'SAME' padding rule\n        out_length = math.ceil(signal_length / stride)\n        total_pad = max(0, (out_length - 1) * stride + eff_kernel - signal_length)\n        pad_left = total_pad // 2\n        pad_right = total_pad - pad_left\n    elif isinstance(pad, int):\n        pad_left = pad_right = pad\n    else:\n        # Expect a 2-tuple\n        pad_left, pad_right = pad\n        if not (isinstance(pad_left, int) and isinstance(pad_right, int)):\n            raise ValueError(\"pad tuple must contain two integers\")\n\n    # Pad the input signal\n    X_padded = np.pad(\n        X,\n        pad_width=((0, 0), (pad_left, pad_right), (0, 0)),\n        mode='constant',\n        constant_values=0\n    )\n    padded_length = signal_length + pad_left + pad_right\n\n    # Compute output length\n    out_length = (padded_length - eff_kernel) // stride + 1\n    if out_length <= 0:\n        # No valid positions\n        return np.zeros((n_examples, 0, out_channels), dtype=X.dtype).tolist()\n\n    # Prepare output array\n    # Use a dtype that can hold the products/sums\n    out_dtype = np.result_type(X.dtype, W.dtype)\n    Y = np.zeros((n_examples, out_length, out_channels), dtype=out_dtype)\n\n    # Stride between kernel taps\n    tap_spacing = dilation + 1\n\n    # Perform cross-correlation (no kernel reversal)\n    for b in range(n_examples):\n        for t in range(out_length):\n            base_idx = t * stride\n            for k in range(kernel_width):\n                x_idx = base_idx + k * tap_spacing\n                # slice of input at position x_idx: shape (in_channels,)\n                x_slice = X_padded[b, x_idx, :]  # (in_channels,)\n                # weight slice: shape (in_channels, out_channels)\n                w_slice = W[k, :, :]            # (in_channels, out_channels)\n                # accumulate across input channels\n                # result: shape (out_channels,)\n                Y[b, t, :] += x_slice @ w_slice\n\n    return Y.tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Single example, single channel, kernel size 3, stride 1, pad \"same\"\n    X = np.array([[[1], [2], [3], [4], [5]]])     # shape (1, 5, 1)\n    W = np.array([[[1]], [[0]], [[-1]]])          # shape (3, 1, 1)\n    out = conv1D(X, W, stride=1, pad=\"same\", dilation=0)\n    print(\"Output:\", out)\n    # Expected cross-correlation (edge handled by same padding):\n    # filter taps [1, 0, -1], so results ~ [2-0, 1-1, 2-2, 3-3, 4-4] = [2, 0, 0, 0, 0]\n```"}
{"task_id": 373, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    # Convert input to a NumPy array for uniform handling\n    y_arr = np.asarray(y, dtype=int)\n    \n    # Handle empty input\n    n_total = y_arr.size\n    if n_total == 0:\n        return 0.0\n    \n    # Count occurrences of each class\n    _, counts = np.unique(y_arr, return_counts=True)\n    \n    # Compute class probabilities\n    probs = counts / n_total\n    \n    # Compute Gini impurity\n    gini_val = 1.0 - np.sum(probs ** 2)\n    \n    # Round to 4 decimal places\n    return round(float(gini_val), 4)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Perfectly pure set (all labels the same) -> Gini = 0.0\n    print(gini([1, 1, 1, 1]))       # 0.0\n\n    # Binary labels, half-half -> Gini = 1 - (0.5^2 + 0.5^2) = 0.5\n    print(gini([0, 1, 0, 1]))       # 0.5\n\n    # Three classes with equal frequency -> Gini = 1 - 3*(1/3)^2 = 0.6667\n    print(gini([0, 1, 2, 0, 1, 2])) # 0.6667\n\n    # Empty input -> Gini = 0.0\n    print(gini([]))                 # 0.0\n```"}
{"task_id": 374, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Target vector of shape (m, ).\n        learning_rate (float): Step size for gradient descent.\n        n_iterations (int): Number of iterations to run gradient descent.\n\n    Returns:\n        tuple[list[float], float]: A tuple containing the list of learned weights\n        (rounded to 4 decimals) and the learned bias (rounded to 4 decimals).\n        If X and y have incompatible shapes, returns -1.\n    \"\"\"\n    # Check that X and y have compatible shapes\n    if X.ndim != 2 or y.ndim != 1 or X.shape[0] != y.shape[0]:\n        return -1\n\n    m, n = X.shape\n\n    # Initialize parameters\n    theta = np.zeros(n, dtype=float)\n    b = 0.0\n\n    # Gradient descent loop\n    for _ in range(n_iterations):\n        # Compute predictions\n        y_pred = X.dot(theta) + b  # shape: (m,)\n\n        # Compute errors\n        errors = y_pred - y        # shape: (m,)\n\n        # Compute gradients\n        dw = (2.0 / m) * X.T.dot(errors)  # shape: (n,)\n        db = (2.0 / m) * np.sum(errors)   # scalar\n\n        # Update parameters\n        theta -= learning_rate * dw\n        b     -= learning_rate * db\n\n    # Round the learned parameters to 4 decimal places\n    weights_rounded = [round(w, 4) for w in theta]\n    bias_rounded = round(b, 4)\n\n    return (weights_rounded, bias_rounded)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Simple test with 2 features and 4 samples\n    X_test = np.array([[1.0, 2.0],\n                       [2.0, 0.5],\n                       [3.0, 1.0],\n                       [4.0, 2.5]])\n    y_test = np.array([5.0, 4.0, 6.0, 8.0])\n\n    weights, bias = linear_regression_gd(X_test, y_test, learning_rate=0.01, n_iterations=10000)\n    print(\"Learned weights:\", weights)\n    print(\"Learned bias:   \", bias)\n```"}
{"task_id": 376, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose,\n    within a tolerance of 1e-8 for floating\u2010point comparisons.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Convert input to a NumPy array of floats\n    arr = np.array(X, dtype=float)\n\n    # Must be 2-D and square\n    if arr.ndim != 2 or arr.shape[0] != arr.shape[1]:\n        return False\n\n    # Compare to its transpose within the absolute tolerance 1e-8\n    return bool(np.allclose(arr, arr.T, atol=1e-8, rtol=0))\n\n\n# Example usage\nif __name__ == \"__main__\":\n    A = [[1.0, 2.0, 3.0],\n         [2.0, 5.0, 6.0],\n         [3.0, 6.0, 9.0]]\n    B = [[1, 2, 3],\n         [4, 5, 6]]\n    print(is_symmetric(A))  # True\n    print(is_symmetric(B))  # False\n```"}
{"task_id": 377, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Compute the negative gradient for logistic (binomial deviance) loss in gradient boosting.\n\n    Parameters:\n    -----------\n    y : list or array-like of int\n        True binary class labels encoded as 0 or 1.\n    f : list or array-like of float\n        Current model prediction scores (real-valued).\n\n    Returns:\n    --------\n    list of float\n        The negative gradients g_i = y'_i / (1 + exp(y'_i * f_i)), where y'_i = 2*y_i - 1,\n        rounded to 4 decimal places.\n    int\n        Returns -1 if `y` contains values other than 0 or 1.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y_arr = np.asarray(y)\n    f_arr = np.asarray(f, dtype=float)\n    \n    # Validate labels\n    unique_labels = np.unique(y_arr)\n    if not set(unique_labels).issubset({0, 1}):\n        return -1\n    \n    # Compute y' = 2*y - 1  (maps {0,1} to {-1,1})\n    y_prime = 2 * y_arr - 1\n    \n    # Compute negative gradient: g_i = y'_i / (1 + exp(y'_i * f_i))\n    # Use elementwise operations in numpy\n    exponents = np.exp(y_prime * f_arr)\n    grads = y_prime / (1 + exponents)\n    \n    # Round to 4 decimal places and convert to Python list\n    return np.round(grads, 4).tolist()\n```"}
{"task_id": 380, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    # Constants from the original SELU paper\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n\n    # Convert input to NumPy array for vectorized operations\n    x_arr = np.array(x, dtype=float)\n    positive = x_arr > 0\n\n    # Compute according to requested order\n    if order == 0:\n        # SELU activation\n        res = np.where(positive,\n                       scale * x_arr,\n                       scale * alpha * (np.exp(x_arr) - 1.0))\n    elif order == 1:\n        # First derivative\n        res = np.where(positive,\n                       scale,\n                       scale * alpha * np.exp(x_arr))\n    elif order == 2:\n        # Second derivative\n        res = np.where(positive,\n                       0.0,\n                       scale * alpha * np.exp(x_arr))\n    else:\n        raise ValueError(\"order must be 0, 1, or 2\")\n\n    # Round to 4 decimal places and convert to Python list/scalar\n    return np.around(res, 4).tolist()\n```"}
{"task_id": 387, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\n\n    Args:\n        x: A list with a single numeric feature for every sample.\n        y: Target values.\n        n_estimators: Number of boosting rounds.\n        learning_rate: Shrinkage applied to every stump (\u03b7).\n\n    Returns:\n        In-sample predictions rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n    # Convert to numpy arrays\n    x_arr = np.array(x, dtype=float)\n    y_arr = np.array(y, dtype=float)\n    n_samples = y_arr.shape[0]\n\n    # Initial prediction: mean of y\n    y_pred = np.full(n_samples, y_arr.mean(), dtype=float)\n\n    # Boosting rounds\n    for _ in range(n_estimators):\n        # Compute residuals\n        residuals = y_arr - y_pred\n\n        # Sort by feature\n        sorted_idx = np.argsort(x_arr)\n        x_sorted = x_arr[sorted_idx]\n        r_sorted = residuals[sorted_idx]\n\n        # Precompute cumulative sums and sums of squares\n        cumsum_r = np.cumsum(r_sorted)\n        cumsum_r2 = np.cumsum(r_sorted ** 2)\n        total_r = cumsum_r[-1]\n        total_r2 = cumsum_r2[-1]\n\n        # Identify valid split positions (where x changes)\n        splits = []\n        for i in range(n_samples - 1):\n            if x_sorted[i] != x_sorted[i + 1]:\n                splits.append(i)\n        \n        if not splits:\n            # No valid split: stump predicts global mean residual\n            stump_pred = np.full(n_samples, residuals.mean(), dtype=float)\n        else:\n            # Evaluate sum of squared errors (SSE) for each split\n            best_sse = np.inf\n            best_i = None\n            best_mean_L = None\n            best_mean_R = None\n\n            for i in splits:\n                count_L = i + 1\n                sum_r_L = cumsum_r[i]\n                sum_r2_L = cumsum_r2[i]\n                mean_L = sum_r_L / count_L\n                sse_L = sum_r2_L - (sum_r_L ** 2) / count_L\n\n                count_R = n_samples - count_L\n                sum_r_R = total_r - sum_r_L\n                sum_r2_R = total_r2 - sum_r2_L\n                mean_R = sum_r_R / count_R\n                sse_R = sum_r2_R - (sum_r_R ** 2) / count_R\n\n                sse = sse_L + sse_R\n                if sse < best_sse:\n                    best_sse = sse\n                    best_i = i\n                    best_mean_L = mean_L\n                    best_mean_R = mean_R\n\n            # Best threshold is midpoint between x_sorted[best_i] and x_sorted[best_i+1]\n            t_star = 0.5 * (x_sorted[best_i] + x_sorted[best_i + 1])\n\n            # Build stump predictions on original order\n            stump_pred = np.where(x_arr <= t_star, best_mean_L, best_mean_R)\n\n        # Update the ensemble prediction\n        y_pred += learning_rate * stump_pred\n\n    # Round to 4 decimals and return as list\n    return np.round(y_pred, 4).tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple test data\n    xs = [1.0, 2.0, 3.0, 4.0, 5.0]\n    ys = [1.2, 1.9, 3.2, 3.9, 5.1]\n    preds = gradient_boosting_1d(xs, ys, n_estimators=10, learning_rate=0.2)\n    print(preds)\n```"}
{"task_id": 394, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    # Small constant for numerical stability\n    eps = 1e-8\n\n    # Initialize running average if needed\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n\n    # Update the running average of squared gradients\n    Eg_next = rho * Eg + (1 - rho) * (grad ** 2)\n\n    # Compute the parameter update\n    w_next = w - learning_rate * grad / np.sqrt(Eg_next + eps)\n\n    # Round to 4 decimal places and convert to Python lists\n    w_next_rounded = np.round(w_next, 4).tolist()\n    Eg_next_rounded = np.round(Eg_next, 4).tolist()\n\n    return w_next_rounded, Eg_next_rounded\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Dummy parameters and gradient\n    w = np.array([1.0, 2.0, 3.0])\n    grad = np.array([0.1, -0.2, 0.3])\n\n    # First iteration: Eg is None\n    w1, Eg1 = rmsprop_update(w, grad)\n    print(\"After 1st update:\")\n    print(\"w1 =\", w1)\n    print(\"Eg1 =\", Eg1)\n\n    # Second iteration: pass the running average from the first update\n    grad2 = np.array([0.05, -0.1, 0.15])\n    w2, Eg2 = rmsprop_update(np.array(w1), grad2, Eg=np.array(Eg1))\n    print(\"\\nAfter 2nd update:\")\n    print(\"w2 =\", w2)\n    print(\"Eg2 =\", Eg2)\n```"}
{"task_id": 398, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If True, return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    # Convert input to numpy array of floats\n    x_arr = np.asarray(x, dtype=float)\n    # Compute ELU activation\n    elu_vals = np.where(x_arr >= 0,\n                        x_arr,\n                        alpha * (np.exp(x_arr) - 1))\n    if derivative:\n        # For x >= 0 gradient is 1; for x < 0 gradient is ELU(x) + alpha\n        grad = np.where(x_arr >= 0,\n                        1.0,\n                        elu_vals + alpha)\n        result = grad\n    else:\n        result = elu_vals\n\n    # Round to 4 decimal places and convert to Python list\n    return np.round(result, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    inputs = [-2.0, -1.0, -0.2, 0.0, 0.5, 1.5]\n    print(\"Inputs:           \", inputs)\n    print(\"ELU outputs:      \", elu(inputs, alpha=0.1, derivative=False))\n    print(\"ELU derivatives:  \", elu(inputs, alpha=0.1, derivative=True))\n```"}
{"task_id": 411, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numbers\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n\n    Parameters\n    ----------\n    actions : list\n        A list with the actions that were taken.  Each element is either a\n        scalar or an iterable of scalars (for multi-dimensional spaces).\n    observations : list\n        A list with the corresponding observations.  Same structural\n        requirements as *actions*.\n\n    Returns\n    -------\n    dict\n        A dictionary with the keys described in the task description.\n    \"\"\"\n    def analyze(data):\n        \"\"\"Analyze a list of scalars or 1-D iterables of scalars.\"\"\"\n        # Detect if any element is a tuple/list/ndarray\n        tuple_flag = any(isinstance(x, (list, tuple, np.ndarray)) for x in data)\n        # Build a 2D list: each element becomes a list of length dim\n        if tuple_flag:\n            flat_data = [np.asarray(x).flatten().tolist() for x in data]\n            dim = len(flat_data[0]) if flat_data else 0\n        else:\n            flat_data = [[x] for x in data]\n            dim = 1 if data else 0\n\n        # Flatten all values and check for non-integer floats\n        continuous = False\n        for row in flat_data:\n            for v in row:\n                # Convert numpy scalar to Python scalar\n                if isinstance(v, np.generic):\n                    v = v.item()\n                if isinstance(v, numbers.Real):\n                    # If not an integer (e.g. 1.5), mark continuous\n                    if not float(v).is_integer():\n                        continuous = True\n                        break\n            if continuous:\n                break\n\n        # For each dimension, collect sorted unique ids\n        ids = []\n        counts = []\n        for d in range(dim):\n            vals = []\n            for row in flat_data:\n                v = row[d]\n                if isinstance(v, np.generic):\n                    v = v.item()\n                vals.append(v)\n            unique = sorted(set(vals))\n            ids.append(unique)\n            counts.append(len(unique))\n\n        return {\n            'tuple': tuple_flag,\n            'dim': dim,\n            'continuous': continuous,\n            'ids': ids,\n            'counts': counts\n        }\n\n    # Analyze actions and observations\n    a_stats = analyze(actions)\n    o_stats = analyze(observations)\n\n    # Build the final dictionary\n    stats = {\n        'tuple_actions': a_stats['tuple'],\n        'tuple_observations': o_stats['tuple'],\n        'multidim_actions': a_stats['dim'] > 1,\n        'multidim_observations': o_stats['dim'] > 1,\n        'continuous_actions': a_stats['continuous'],\n        'continuous_observations': o_stats['continuous'],\n        'n_actions_per_dim': a_stats['counts'],\n        'action_dim': a_stats['dim'],\n        'n_obs_per_dim': o_stats['counts'],\n        'obs_dim': o_stats['dim'],\n        'action_ids': a_stats['ids'],\n        'obs_ids': o_stats['ids'],\n    }\n    return stats\n\n\n# Example usage\nif __name__ == '__main__':\n    actions = [(0, 1), (1, 0), (1, 1)]\n    observations = [10.0, 11.5, 12.0]\n    result = env_stats(actions, observations)\n    import pprint\n    pprint.pprint(result)\n    # Expected output:\n    # {\n    #   'tuple_actions': True,\n    #   'tuple_observations': False,\n    #   'multidim_actions': True,\n    #   'multidim_observations': False,\n    #   'continuous_actions': False,\n    #   'continuous_observations': True,\n    #   'n_actions_per_dim': [2, 2],\n    #   'action_dim': 2,\n    #   'n_obs_per_dim': [3],\n    #   'obs_dim': 1,\n    #   'action_ids': [[0, 1], [0, 1]],\n    #   'obs_ids': [[10.0, 11.5, 12.0]]\n    # }\n```"}
{"task_id": 413, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass TreeNode:\n    def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):\n        self.feature_index = feature_index\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n\n    def predict(self, X):\n        # X is (n_samples, n_features)\n        n_samples = X.shape[0]\n        y_pred = np.empty(n_samples, dtype=float)\n        # Leaf node\n        if self.feature_index is None:\n            y_pred.fill(self.value)\n        else:\n            # Split\n            mask = X[:, self.feature_index] <= self.threshold\n            # Predict left\n            y_pred[mask] = self.left.predict(X[mask]) if np.any(mask) else []\n            # Predict right\n            y_pred[~mask] = self.right.predict(X[~mask]) if np.any(~mask) else []\n        return y_pred\n\ndef build_tree(X, y, max_depth, depth=0):\n    # If pure or max depth reached, make a leaf\n    if depth >= max_depth or X.shape[0] <= 1:\n        leaf_value = y.mean()\n        return TreeNode(value=leaf_value)\n\n    n_samples, n_features = X.shape\n    # current mse\n    current_mse = np.sum((y - y.mean())**2)\n    best_mse = np.inf\n    best_idx = None\n    best_thr = None\n    best_left_idx = None\n    best_right_idx = None\n\n    # Find best split\n    for feature_index in range(n_features):\n        X_col = X[:, feature_index]\n        # consider midpoints between unique sorted values\n        unique_vals = np.unique(X_col)\n        if unique_vals.shape[0] <= 1:\n            continue\n        thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n        for thr in thresholds:\n            left_mask = X_col <= thr\n            right_mask = ~left_mask\n            if not np.any(left_mask) or not np.any(right_mask):\n                continue\n            y_left, y_right = y[left_mask], y[right_mask]\n            mse_left = np.sum((y_left - y_left.mean())**2)\n            mse_right = np.sum((y_right - y_right.mean())**2)\n            mse_total = mse_left + mse_right\n            if mse_total < best_mse:\n                best_mse = mse_total\n                best_idx = feature_index\n                best_thr = thr\n                best_left_idx = left_mask\n                best_right_idx = right_mask\n\n    # If no improvement, make a leaf\n    if best_idx is None or best_mse >= current_mse:\n        leaf_value = y.mean()\n        return TreeNode(value=leaf_value)\n\n    # Otherwise, build subtrees\n    left = build_tree(X[best_left_idx], y[best_left_idx], max_depth, depth+1)\n    right = build_tree(X[best_right_idx], y[best_right_idx], max_depth, depth+1)\n    return TreeNode(feature_index=best_idx, threshold=best_thr, left=left, right=right)\n\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray,\n                    n_estimators: int = 10, learning_rate: float = 0.1,\n                    max_depth: int = 3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the training\n        features.\n    y_train : np.ndarray\n        1-D array of length n_samples containing the training targets.\n    X_test : np.ndarray\n        2-D array with shape (m_samples, n_features) containing the test\n        features to predict.\n    n_estimators : int, default=10\n        Number of boosting iterations.\n    learning_rate : float, default=0.1\n        Shrinkage factor applied to each tree\u2019s prediction.\n    max_depth : int, default=3\n        Maximum depth of every individual regression tree.\n\n    Returns\n    -------\n    list[float]\n        Predictions for every sample in *X_test*, rounded to 4 decimal places.\n    \"\"\"\n    # Initial prediction\n    init_pred = y_train.mean()\n    y_pred_train = np.full(y_train.shape, init_pred, dtype=float)\n    trees = []\n\n    # Boosting iterations\n    for _ in range(n_estimators):\n        # Compute negative gradient (residuals)\n        residual = y_train - y_pred_train\n        # Fit a tree to residuals\n        tree = build_tree(X_train, residual, max_depth)\n        trees.append(tree)\n        # Update predictions\n        update = tree.predict(X_train)\n        y_pred_train += learning_rate * update\n\n    # Predict on test set\n    y_pred = np.full(X_test.shape[0], init_pred, dtype=float)\n    for tree in trees:\n        y_pred += learning_rate * tree.predict(X_test)\n\n    # Round and return as Python list\n    y_pred_rounded = np.round(y_pred, 4).tolist()\n    return y_pred_rounded\n```"}
{"task_id": 416, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Compute the PDF values of a multivariate Gaussian distribution\n    for given data points X, mean vector, and covariance matrix.\n\n    Parameters:\n    -----------\n    X : array-like, shape (n_samples, n_features) or (n_features,)\n        Data points where the PDF is evaluated. If 1D, treated as (n_samples, 1).\n    mean : array-like, shape (n_features,)\n        Mean vector of the distribution.\n    cov : array-like, shape (n_features, n_features)\n        Covariance matrix (must be positive-definite).\n\n    Returns:\n    --------\n    pdf_values : list of floats\n        List where the i-th element is the PDF value at X[i],\n        rounded to 4 decimal places.\n    \"\"\"\n    # Convert to numpy arrays\n    X = np.asarray(X, dtype=float)\n    mean = np.asarray(mean, dtype=float)\n    cov = np.asarray(cov, dtype=float)\n\n    # Handle the case X is 1D: treat as (n_samples, 1)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n\n    n_samples, n_features = X.shape\n\n    # Ensure mean has shape (n_features,)\n    if mean.ndim == 1 and mean.shape[0] == n_features:\n        mu = mean\n    else:\n        mu = mean.reshape(n_features)\n\n    # Ensure covariance matrix has correct shape\n    cov = cov.reshape((n_features, n_features))\n\n    # Compute determinant and inverse of covariance\n    det_cov = np.linalg.det(cov)\n    if det_cov <= 0:\n        raise ValueError(\"Covariance matrix must be positive-definite (det > 0).\")\n    inv_cov = np.linalg.inv(cov)\n\n    # Normalization constant\n    denom = math.sqrt((2 * math.pi) ** n_features * det_cov)\n    norm_const = 1.0 / denom\n\n    # Evaluate PDF for each sample\n    pdf_values = []\n    for x in X:\n        diff = x - mu\n        exponent = -0.5 * float(diff.T.dot(inv_cov).dot(diff))\n        pdf = norm_const * math.exp(exponent)\n        pdf_values.append(round(pdf, 4))\n\n    return pdf_values\n```"}
{"task_id": 419, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    The function returns the MAP predictive means for every row in\n    `X_new`.  See the task description for full details.\n    \"\"\"\n    # 1. Optionally add intercept\n    if fit_intercept:\n        X = np.hstack((np.ones((X.shape[0], 1)), X))\n        X_new = np.hstack((np.ones((X_new.shape[0], 1)), X_new))\n\n    # Dimensions\n    N, p = X.shape\n\n    # 2. Process prior mean mu\n    if np.isscalar(mu):\n        mu_vec = np.full(p, mu, dtype=float)\n    else:\n        mu_vec = np.array(mu, dtype=float).ravel()\n        if mu_vec.shape[0] != p:\n            raise ValueError(f\"mu has length {mu_vec.shape[0]}, expected {p}\")\n\n    # 2. Process prior covariance V\n    if V is None:\n        V_mat = np.eye(p)\n    elif np.isscalar(V):\n        V_mat = float(V) * np.eye(p)\n    else:\n        V_arr = np.array(V, dtype=float)\n        if V_arr.ndim == 1:\n            if V_arr.shape[0] != p:\n                raise ValueError(f\"V diag has length {V_arr.shape[0]}, expected {p}\")\n            V_mat = np.diag(V_arr)\n        elif V_arr.ndim == 2:\n            if V_arr.shape != (p, p):\n                raise ValueError(f\"V matrix has shape {V_arr.shape}, expected ({p},{p})\")\n            V_mat = V_arr\n        else:\n            raise ValueError(\"V must be scalar, 1-D or 2-D array\")\n\n    # 3. Compute posterior covariance and mean\n    # V_inv\n    V_inv = np.linalg.inv(V_mat)\n    XtX = X.T @ X\n    Sigma_p = np.linalg.inv(V_inv + XtX)        # posterior covariance\n    rhs = V_inv @ mu_vec + X.T @ y               # right\u2010hand side\n    mu_p = Sigma_p @ rhs                         # posterior mean\n\n    # 4. Predictive MAP mean\n    y_pred = X_new @ mu_p\n\n    # Round to 4 decimal places and return as Python list\n    return [round(float(val), 4) for val in y_pred]\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple test\n    X_train = np.array([[1.0], [2.0], [3.0], [4.0]])\n    y_train = np.array([2.0, 3.0, 2.5, 5.0])\n    X_test = np.array([[1.5], [3.5]])\n    preds = bayesian_linear_regression_pred(\n        X_train,\n        y_train,\n        X_test,\n        mu=0.0,\n        V=1.0,\n        fit_intercept=True\n    )\n    print(preds)  # e.g., [2.2857, 3.9286]\n```"}
{"task_id": 423, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute the Root Mean Squared Logarithmic Error (RMSLE).\n       Returns -1 if inputs are invalid (unequal lengths, empty, or contain negative values).\"\"\"\n    # 1. Validate inputs\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1\n\n    for v in actual + predicted:\n        if not isinstance(v, (int, float)) or v < 0:\n            return -1\n\n    # 2. Compute RMSLE\n    actual_arr = np.array(actual, dtype=float)\n    predicted_arr = np.array(predicted, dtype=float)\n    # use log1p for numerical stability: log1p(x) = log(1 + x)\n    log_actual = np.log1p(actual_arr)\n    log_pred   = np.log1p(predicted_arr)\n    msle       = np.mean((log_pred - log_actual) ** 2)\n    rmsle      = np.sqrt(msle)\n\n    # 3. Return rounded result\n    return round(rmsle, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true    = [3, 5, 2.5, 7]\n    y_preds   = [2.5, 5, 4,   8]\n    score     = root_mean_squared_log_error(y_true, y_preds)\n    print(f\"RMSLE: {score}\")  # Expected output: RMSLE: 0.1206\n```"}
{"task_id": 428, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    # Special cases\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n\n    # Determine the length of the internal window to compute\n    length = window_len if symmetric else window_len + 1\n\n    # Create the Hann window samples\n    n = np.arange(length)\n    w = 0.5 - 0.5 * np.cos(2.0 * math.pi * n / (length - 1))\n\n    # If a periodic window is requested, drop the last sample\n    if not symmetric:\n        w = w[:-1]\n\n    # Round to 4 decimal places and convert to a Python list\n    return np.round(w, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(\"Symmetric, N=8:\", hann(8, symmetric=True))\n    print(\"Periodic,  N=8:\", hann(8, symmetric=False))\n```\n\nExplanation of key steps:\n- Handle edge cases `window_len <= 0` and `window_len == 1`.\n- For a periodic window, compute an internal window of length `window_len + 1` and discard the last sample to ensure wrap\u2010around continuity.\n- Use the formula `0.5 - 0.5 * cos(2\u03c0n/(N\u22121))`.\n- Round each coefficient to 4 decimal places before returning.\n"}
{"task_id": 433, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Union, Tuple, List\n\ndef glorot_fan(shape: Union[Tuple[int, ...], List[int, ...]]) -> Tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    # Check for 4-D convolutional kernel case\n    if len(shape) == 4:\n        out_channels, in_channels, kh, kw = shape\n        receptive_field_size = kh * kw\n        fan_in = in_channels * receptive_field_size\n        fan_out = out_channels * receptive_field_size\n    else:\n        # Fallback: use the first two dimensions directly\n        fan_in, fan_out = shape[0], shape[1]\n\n    return float(fan_in), float(fan_out)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Dense layer weight matrix of shape (in_features, out_features)\n    shape_dense = (128, 64)\n    fi, fo = glorot_fan(shape_dense)\n    print(f\"Dense layer shape {shape_dense} -> fan_in={fi}, fan_out={fo}\")\n\n    # Conv2D kernel of shape (out_channels, in_channels, kernel_height, kernel_width)\n    shape_conv = (32, 16, 3, 3)\n    fi, fo = glorot_fan(shape_conv)\n    print(f\"Conv2D kernel shape {shape_conv} -> fan_in={fi}, fan_out={fo}\")\n\n    # Higher-dimensional tensor example\n    shape_high = (256, 512, 7)\n    fi, fo = glorot_fan(shape_high)\n    print(f\"Higher-dim tensor shape {shape_high} -> fan_in={fi}, fan_out={fo}\")\n```"}
{"task_id": 435, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    # Convert inputs to NumPy arrays of floats\n    X_arr = np.array(X, dtype=float)\n    y_arr = np.array(y, dtype=float)\n    m = y_arr.shape[0]\n\n    # Compute the base prediction (mean of y)\n    base_pred = y_arr.mean()\n\n    # Handle special cases\n    if n_estimators <= 0 or learning_rate == 0:\n        return [round(base_pred, 4)] * m\n\n    # Initialize current prediction\n    y_pred = np.full(m, base_pred, dtype=float)\n\n    # Gradient boosting iterations\n    for _ in range(n_estimators):\n        # Compute residuals\n        residual = y_arr - y_pred\n\n        # Fit OLS: add intercept term\n        X1 = np.column_stack((np.ones(m, dtype=float), X_arr))\n        # Solve for weights [intercept, coef...]\n        w = np.linalg.lstsq(X1, residual, rcond=None)[0]\n\n        # Weak learner prediction\n        h = X1.dot(w)\n\n        # Update the overall prediction\n        y_pred += learning_rate * h\n\n    # Round to 4 decimal places and convert to Python list\n    y_pred_rounded = np.round(y_pred, 4)\n    return y_pred_rounded.tolist()\n```"}
{"task_id": 437, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            list/array input returns a Python list preserving the shape.\n            If order is not 0, 1, or 2, returns -1.\n    \"\"\"\n    # Validate order\n    if order not in (0, 1, 2):\n        return -1\n\n    # Determine if input is scalar\n    is_scalar = np.isscalar(x)\n\n    # Convert to NumPy array for vectorized computation\n    arr = np.array(x, dtype=float)\n\n    # Compute sigmoid\n    sig = 1.0 / (1.0 + np.exp(-arr))\n\n    # Select computation based on order\n    if order == 0:\n        result = sig\n    elif order == 1:\n        result = sig * (1.0 - sig)\n    else:  # order == 2\n        result = sig * (1.0 - sig) * (1.0 - 2.0 * sig)\n\n    # Round to 4 decimal places\n    result = np.round(result, 4)\n\n    # Return appropriate type\n    if is_scalar:\n        return float(result)\n    else:\n        return result.tolist()\n```"}
{"task_id": 438, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]] | int:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If None, the value is inferred.\n\n    Returns\n    -------\n    list[list[int]] | int\n        A 2-D list of lists containing the one-hot encoded representation of y,\n        or -1 when num_classes is smaller than required.\n    \"\"\"\n    # Convert input to a 1-D numpy array of integers\n    y_arr = np.array(y, dtype=int).ravel()\n    \n    # If it's empty, return an empty list (no samples)\n    if y_arr.size == 0:\n        if num_classes is None:\n            return []\n        else:\n            # Even if num_classes > 0, with zero samples we return [] (0 rows)\n            return []\n    \n    # Find the maximum label to infer required number of classes\n    max_label = int(y_arr.max())\n    required = max_label + 1\n    \n    # If num_classes is given but too small, signal error\n    if num_classes is not None and num_classes < required:\n        return -1\n    \n    # Determine final number of classes\n    n_classes = required if num_classes is None else num_classes\n    \n    # Build the one-hot array\n    n_samples = y_arr.shape[0]\n    one_hot = np.zeros((n_samples, n_classes), dtype=int)\n    for i, label in enumerate(y_arr):\n        # This will automatically error out if label < 0 or label >= n_classes\n        one_hot[i, label] = 1\n    \n    # Convert to a pure Python list of lists and return\n    return one_hot.tolist()\n\n\n# Example usages and basic tests\nif __name__ == \"__main__\":\n    print(to_categorical([0, 2, 1, 3]))          \n    # Expected: [[1,0,0,0], [0,0,1,0], [0,1,0,0], [0,0,0,1]]\n    \n    print(to_categorical(np.array([1, 0, 2]), num_classes=4))\n    # Expected: [[0,1,0,0], [1,0,0,0], [0,0,1,0]]\n    \n    print(to_categorical([0, 1, 4], num_classes=4))\n    # Expected: -1  (since max label 4 requires at least 5 classes)\n    \n    print(to_categorical([], num_classes=3))\n    # Expected: []  (zero samples, three classes specified)\n    \n    print(to_categorical([], None))\n    # Expected: []  (zero samples, num_classes inferred as 0)\n```\n"}
{"task_id": 439, "completion_id": 0, "completion": "```python\nimport itertools\nimport math\nfrom collections import Counter\nfrom typing import Hashable, List, Tuple\n\ndef apriori_frequent_itemsets(transactions: List[List[Hashable]],\n                              min_sup: float) -> List[Tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        List of transactions; each transaction is itself a list of items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction (>0 and \u22641).\n\n    Returns\n    -------\n    list[tuple]\n        Frequent itemsets ordered by length and then lexicographically.\n    \"\"\"\n    if not 0 < min_sup <= 1:\n        raise ValueError(\"min_sup must be in (0, 1].\")\n    # Convert transactions to list of sets (dedupe within a transaction)\n    txn_list = [set(t) for t in transactions]\n    num_txns = len(txn_list)\n    if num_txns == 0:\n        return []\n\n    # Minimum count threshold (ceiling of min_sup * total transactions)\n    min_count = math.ceil(min_sup * num_txns)\n\n    # 1) Generate L1, the frequent 1-itemsets\n    item_counter = Counter()\n    for txn in txn_list:\n        for item in txn:\n            item_counter[item] += 1\n    # Keep only those items whose count >= min_count\n    L1 = []\n    for item, cnt in item_counter.items():\n        if cnt >= min_count:\n            L1.append((item,))\n    # Sort lexicographically\n    L1 = sorted(L1)\n\n    # Prepare to iterate\n    freq_itemsets: List[Tuple] = []\n    # Add L1 to the global list\n    freq_itemsets.extend(L1)\n\n    # Convert L1 to a set of frozensets for quick lookup\n    Lk_sets = {frozenset([item]) for (item,) in L1}\n    k = 2\n\n    # Loop to find Lk for k = 2, 3, ...\n    while True:\n        # 2) Candidate generation: join step\n        candidates_k = set()\n        Lk_list = list(Lk_sets)\n        for i in range(len(Lk_list)):\n            for j in range(i + 1, len(Lk_list)):\n                c = Lk_list[i] | Lk_list[j]\n                if len(c) == k:\n                    candidates_k.add(c)\n\n        # 3) Prune step: remove any candidate with infrequent subset\n        pruned_candidates = set()\n        prev_Lk = Lk_sets  # frequent (k-1)-itemsets from previous iteration\n        for cand in candidates_k:\n            # all (k-1)-subsets of cand must be in prev_Lk\n            all_subsets_frequent = True\n            for subset in itertools.combinations(cand, k - 1):\n                if frozenset(subset) not in prev_Lk:\n                    all_subsets_frequent = False\n                    break\n            if all_subsets_frequent:\n                pruned_candidates.add(cand)\n\n        # 4) Count support for each candidate\n        cand_count = Counter()\n        for txn in txn_list:\n            for cand in pruned_candidates:\n                if cand.issubset(txn):\n                    cand_count[cand] += 1\n\n        # 5) Keep candidates meeting min_count -> that's Lk\n        Lk_sets = {c for c, cnt in cand_count.items() if cnt >= min_count}\n        if not Lk_sets:\n            break\n\n        # Convert to sorted tuples, then sort lexicographically\n        Lk_tuples = [tuple(sorted(c)) for c in Lk_sets]\n        Lk_tuples.sort()\n        freq_itemsets.extend(Lk_tuples)\n\n        # Move to next k\n        k += 1\n\n    return freq_itemsets\n\n# Example usage\nif __name__ == \"__main__\":\n    txns = [\n        ['bread', 'milk'],\n        ['bread', 'diaper', 'beer', 'egg'],\n        ['milk', 'diaper', 'beer', 'cola'],\n        ['bread', 'milk', 'diaper', 'beer'],\n        ['bread', 'milk', 'diaper', 'cola'],\n    ]\n    min_support = 0.6\n    result = apriori_frequent_itemsets(txns, min_support)\n    print(\"Frequent itemsets (min_sup={}):\".format(min_support))\n    for itemset in result:\n        print(itemset)\n```"}
{"task_id": 440, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]\n                         ) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    # Convert to NumPy array of shape (n_estimators, n_samples, n_classes)\n    arr = np.array(predictions, dtype=float)\n\n    # Compute the mean across estimators: shape -> (n_samples, n_classes)\n    avg_probs = arr.mean(axis=0)\n\n    # Round to 4 decimal places\n    rounded_probs = np.round(avg_probs, 4)\n\n    # For each sample, take the argmax over classes\n    preds = np.argmax(rounded_probs, axis=1)\n\n    # Convert back to Python lists\n    return rounded_probs.tolist(), preds.tolist()\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    # Suppose we have 3 estimators, 4 samples, and 3 classes\n    example_preds = [\n        # Estimator 1\n        [\n            [0.1, 0.7, 0.2],\n            [0.3, 0.3, 0.4],\n            [0.0, 1.0, 0.0],\n            [0.2, 0.2, 0.6],\n        ],\n        # Estimator 2\n        [\n            [0.2, 0.6, 0.2],\n            [0.4, 0.2, 0.4],\n            [0.1, 0.8, 0.1],\n            [0.1, 0.3, 0.6],\n        ],\n        # Estimator 3\n        [\n            [0.0, 0.8, 0.2],\n            [0.2, 0.4, 0.4],\n            [0.2, 0.7, 0.1],\n            [0.3, 0.1, 0.6],\n        ],\n    ]\n\n    avg_matrix, labels = aggregate_predictions(example_preds)\n\n    print(\"Averaged probabilities (rounded to 4 decimals):\")\n    for i, probs in enumerate(avg_matrix):\n        print(f\" Sample {i}: {probs}\")\n    print(\"Predicted class labels:\", labels)\n```"}
{"task_id": 444, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]] | int:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    Given:\n      X: N \u00d7 C list of vectors\n      Y: M \u00d7 C list of vectors (if None, Y = X)\n      sigma: bandwidth parameter\n         - single positive float (isotropic)\n         - list/array of length C of positive floats (anisotropic)\n         - None \u21d2 default sigma_j = sqrt(C/2)\n\n    Returns:\n      -1 on invalid input, else returns the N\u00d7M kernel matrix as a nested list\n      with entries rounded to 4 decimal places.\n    \"\"\"\n    # Convert X to numpy array\n    try:\n        X_arr = np.array(X, dtype=float)\n    except Exception:\n        return -1\n    if X_arr.ndim != 2:\n        return -1\n    N, C = X_arr.shape\n\n    # Convert or assign Y\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        try:\n            Y_arr = np.array(Y, dtype=float)\n        except Exception:\n            return -1\n        if Y_arr.ndim != 2 or Y_arr.shape[1] != C:\n            return -1\n    M = Y_arr.shape[0]\n\n    # Process sigma\n    if sigma is None:\n        # default isotropic\n        sigma_arr = np.sqrt(C / 2.0)\n    elif isinstance(sigma, (int, float)):\n        sigma_arr = float(sigma)\n        if sigma_arr <= 0:\n            return -1\n    elif isinstance(sigma, (list, tuple, np.ndarray)):\n        if len(sigma) != C:\n            return -1\n        sigma_arr = np.array(sigma, dtype=float)\n        if sigma_arr.ndim != 1 or np.any(sigma_arr <= 0):\n            return -1\n    else:\n        return -1\n\n    # Scale the data\n    if np.isscalar(sigma_arr):\n        Xs = X_arr / sigma_arr\n        Ys = Y_arr / sigma_arr\n    else:\n        Xs = X_arr / sigma_arr[np.newaxis, :]\n        Ys = Y_arr / sigma_arr[np.newaxis, :]\n\n    # Compute squared distances: ||x/s - y/s||^2\n    X_sq = np.sum(Xs * Xs, axis=1).reshape(N, 1)   # (N,1)\n    Y_sq = np.sum(Ys * Ys, axis=1).reshape(1, M)   # (1,M)\n    D2 = X_sq + Y_sq - 2 * Xs.dot(Ys.T)\n    # Numerical stability\n    D2 = np.maximum(D2, 0.0)\n\n    # RBF kernel\n    K = np.exp(-0.5 * D2)\n\n    # Round to 4 decimal places\n    K_rounded = np.round(K, 4)\n\n    # Convert to nested Python list\n    return K_rounded.tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    X = [[1, 0], [0, 1]]\n    Y = [[1, 0], [0, 1]]\n    sigma = 1.0\n    print(rbf_kernel(X, Y, sigma))\n    # Output: [[1.0, 0.3679], [0.3679, 1.0]]\n```"}
{"task_id": 446, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\n\n    A *decision stump* is a one-level decision tree: it chooses one feature and\n    one threshold to split the dataset into two parts. This function must find\n    the split that maximises the decrease of entropy (information gain) and\n    return a summarising tuple. Read the detailed task description for exact\n    requirements, tie-breaking rules and the expected return value.\n\n    Args:\n        X: 2-D list (or array-like) of shape (n_samples, n_features) containing\n           only numeric values.\n        y: 1-D list (or array-like) with the class label of every sample.\n\n    Returns:\n        \u2022 (best_feature_index, best_threshold, left_majority_label,\n           right_majority_label)\n        \u2022 If no split can improve information gain, returns\n          (-1, None, majority_label, majority_label).\n    \"\"\"\n    # Convert to numpy arrays for ease of computation\n    X = np.array(X, dtype=float)\n    y = np.array(y)\n    n_samples, n_features = X.shape\n\n    def entropy(labels: np.ndarray) -> float:\n        \"\"\"Compute Shannon entropy of a 1D array of class labels.\"\"\"\n        if labels.size == 0:\n            return 0.0\n        vals, counts = np.unique(labels, return_counts=True)\n        probs = counts / counts.sum()\n        return -np.sum(probs * np.log2(probs))\n\n    def majority_label(labels: np.ndarray) -> int:\n        \"\"\"Return the most frequent label; break ties by taking the smallest.\"\"\"\n        if labels.size == 0:\n            return None\n        vals, counts = np.unique(labels, return_counts=True)\n        max_count = counts.max()\n        candidates = vals[counts == max_count]\n        return int(candidates.min())\n\n    # Compute parent entropy once\n    parent_entropy = entropy(y)\n\n    # Variables to track the best split\n    best_gain = 0.0\n    best_j = -1\n    best_threshold = None\n    best_left_label = None\n    best_right_label = None\n\n    # A small tolerance for floating\u2010point comparisons\n    tol = 1e-12\n\n    # Iterate over all features\n    for j in range(n_features):\n        col = X[:, j]\n        # Find distinct sorted values to generate mid\u2010point thresholds\n        distinct_vals = np.unique(col)\n        if distinct_vals.size <= 1:\n            # No possible split on this feature\n            continue\n\n        # Candidate thresholds are mid\u2010points between successive distinct values\n        thresholds = (distinct_vals[:-1] + distinct_vals[1:]) / 2.0\n\n        # Evaluate each threshold\n        for t in thresholds:\n            left_mask = col <= t\n            right_mask = ~left_mask\n            n_left = left_mask.sum()\n            n_right = n_samples - n_left\n\n            # Skip invalid splits\n            if n_left == 0 or n_right == 0:\n                continue\n\n            left_labels = y[left_mask]\n            right_labels = y[right_mask]\n\n            # Compute child entropies\n            h_left = entropy(left_labels)\n            h_right = entropy(right_labels)\n\n            # Weighted average entropy after split\n            p_left = n_left / n_samples\n            p_right = n_right / n_samples\n            gain = parent_entropy - (p_left * h_left + p_right * h_right)\n\n            # Check if this split is better\n            if gain > best_gain + tol:\n                best_gain = gain\n                best_j = j\n                best_threshold = t\n                best_left_label = majority_label(left_labels)\n                best_right_label = majority_label(right_labels)\n            elif (\n                abs(gain - best_gain) <= tol\n                and j == best_j\n                and best_threshold is not None\n                and t < best_threshold\n            ):\n                # Tie on gain for the same feature: pick smaller threshold\n                best_threshold = t\n                best_left_label = majority_label(left_labels)\n                best_right_label = majority_label(right_labels)\n\n    # If no split improves over parent entropy, return the default stump\n    if best_j == -1:\n        maj = majority_label(y)\n        return (-1, None, maj, maj)\n\n    # Otherwise return the best found split (round threshold to 4 decimals)\n    return (best_j, round(best_threshold, 4), best_left_label, best_right_label)\n```"}
{"task_id": 452, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List\nfrom numbers import Number\n\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\n\n    The function must create two disjoint subsets:\n    1. For a numeric threshold (int or float) the first subset contains every\n       sample whose value in column *feature_i* is **greater than or equal** to\n       the threshold.\n    2. For any other type of threshold the first subset contains every sample\n       whose value in column *feature_i* is **exactly equal** to the threshold.\n\n    The second subset always contains the remaining samples.  Both subsets must\n    keep the original order of appearance in *X*.\n\n    Args:\n        X: 2-D iterable (list or ndarray) where each element is a sample.\n        feature_i: Index of the feature column used for the split.\n        threshold: Value that determines how the split is performed.\n\n    Returns:\n        A list of length two.  *result[0]* is the first subset, *result[1]* is\n        the second subset.  Each subset is a list of lists.\n    \"\"\"\n    # Determine whether we treat the threshold as numeric\n    is_numeric_threshold = isinstance(threshold, Number)\n\n    # We'll collect raw rows here\n    left_rows = []\n    right_rows = []\n\n    for sample in X:\n        # sample might be a list or a numpy array\n        try:\n            value = sample[feature_i]\n        except Exception as e:\n            raise IndexError(f\"Cannot access feature index {feature_i} in sample {sample}\") from e\n\n        if is_numeric_threshold:\n            # Numeric split: >= threshold\n            if value >= threshold:\n                left_rows.append(sample)\n            else:\n                right_rows.append(sample)\n        else:\n            # Categorical split: == threshold\n            if value == threshold:\n                left_rows.append(sample)\n            else:\n                right_rows.append(sample)\n\n    # Convert each row to a plain Python list\n    def _to_list(row):\n        if isinstance(row, np.ndarray):\n            return row.tolist()\n        else:\n            # assume it's already a sequence (e.g., list or tuple)\n            return list(row)\n\n    left_list = [_to_list(row) for row in left_rows]\n    right_list = [_to_list(row) for row in right_rows]\n\n    return [left_list, right_list]\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Numeric split example\n    X_num = np.array([[1, 5], [3, 2], [4, 6], [2, 1]])\n    result_num = divide_on_feature(X_num, feature_i=0, threshold=3)\n    print(\"Numeric split result:\", result_num)\n    # Expected: [[[3, 2], [4, 6]], [[1, 5], [2, 1]]]\n\n    # Categorical split example\n    X_cat = np.array([[1, \"A\"], [2, \"B\"], [3, \"A\"], [4, \"C\"]], dtype=object)\n    result_cat = divide_on_feature(X_cat, feature_i=1, threshold=\"A\")\n    print(\"Categorical split result:\", result_cat)\n    # Expected: [[[1, \"A\"], [3, \"A\"]], [[2, \"B\"], [4, \"C\"]]]\n```"}
{"task_id": 453, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # Basic validation\n    if not (0.0 <= gamma <= 1.0):\n        return -1\n    if not isinstance(Q, np.ndarray) or not isinstance(Q_next, np.ndarray):\n        return -1\n    if Q.ndim != 2 or Q_next.shape != Q.shape:\n        return -1\n\n    batch_size, n_actions = Q.shape\n    if not (len(actions) == len(rewards) == len(dones) == batch_size):\n        return -1\n\n    # Prepare targets as a copy of current Q-values\n    y = Q.copy().astype(float)\n\n    for i in range(batch_size):\n        a = actions[i]\n        # Validate action index\n        if not (isinstance(a, int) and 0 <= a < n_actions):\n            return -1\n\n        r = rewards[i]\n        done = dones[i]\n\n        # Compute target\n        if done:\n            target = r\n        else:\n            target = r + gamma * float(np.max(Q_next[i]))\n\n        y[i, a] = target\n\n    # Round to 4 decimal places and convert to nested lists\n    y_rounded = np.round(y, 4)\n    return y_rounded.tolist()\n```"}
{"task_id": 458, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numbers import Integral\n\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    # Attempt to cast to a Python list\n    try:\n        labels_list = list(labels)\n    except Exception:\n        # If it's not iterable\n        return -1\n\n    # Handle empty label list\n    if not labels_list:\n        # Infer num_classes = 0 if not provided\n        if num_classes is None:\n            num_classes = 0\n        # Return empty list for zero rows\n        return []\n\n    # Validate each label\n    for l in labels_list:\n        # Must be an integer (built-in or numpy integer) and non-negative\n        if not isinstance(l, Integral) or l < 0:\n            return -1\n\n    max_label = int(max(labels_list))\n\n    # Infer number of classes if needed\n    if num_classes is None:\n        num_classes = max_label + 1\n    # Provided num_classes must be large enough\n    elif num_classes < max_label + 1:\n        return -1\n\n    # Build one-hot encoding\n    one_hot = []\n    for label in labels_list:\n        row = [0] * num_classes\n        row[label] = 1\n        one_hot.append(row)\n\n    return one_hot\n```\n"}
{"task_id": 461, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\n\n    Parameters\n    ----------\n    x : list[float]\n        Feature values (one-dimensional).\n    y : list[float]\n        Target values.\n    n_estimators : int, default 200\n        Number of boosting iterations.\n    learning_rate : float, default 0.5\n        Shrinkage applied to each weak learner.\n\n    Returns\n    -------\n    list[float]\n        Final predictions for the training data, rounded to four decimals.\n    \"\"\"\n    x_arr = np.array(x, dtype=float)\n    y_arr = np.array(y, dtype=float)\n    n_samples = x_arr.shape[0]\n\n    # Initial prediction: mean of y\n    y_hat = np.full(n_samples, y_arr.mean(), dtype=float)\n\n    # Special case: no boosting rounds\n    if n_estimators <= 0:\n        return np.round(y_hat, 4).tolist()\n\n    for _ in range(n_estimators):\n        # Compute residuals\n        residuals = y_arr - y_hat\n        # Early stopping if residuals are effectively zero\n        if np.all(np.abs(residuals) < 1e-12):\n            break\n\n        # Find best split among unique x values (excluding the maximum)\n        uniq = np.unique(x_arr)\n        split_candidates = uniq[:-1]  # drop the greatest\n\n        best_sse = np.inf\n        best_t = None\n        best_vL = 0.0\n        best_vR = 0.0\n\n        for t in split_candidates:\n            left_mask = x_arr <= t\n            right_mask = ~left_mask\n            if not left_mask.any() or not right_mask.any():\n                continue\n\n            rL = residuals[left_mask]\n            rR = residuals[right_mask]\n            vL = rL.mean()\n            vR = rR.mean()\n            sse = ((rL - vL) ** 2).sum() + ((rR - vR) ** 2).sum()\n\n            if sse < best_sse:\n                best_sse = sse\n                best_t = t\n                best_vL = vL\n                best_vR = vR\n\n        # If no valid split found, use global mean residual\n        if best_t is None:\n            global_mean_res = residuals.mean()\n            # stump predicts the same everywhere\n            stump_pred = np.full(n_samples, global_mean_res, dtype=float)\n        else:\n            # build stump prediction\n            stump_pred = np.where(x_arr <= best_t, best_vL, best_vR)\n\n        # Update the ensemble prediction\n        y_hat = y_hat + learning_rate * stump_pred\n\n    # Round and return\n    return np.round(y_hat, 4).tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # toy data\n    x_train = [1.0, 2.0, 3.0, 4.0, 5.0]\n    y_train = [1.2, 1.9, 3.0, 4.1, 5.1]\n    preds = gradient_boosting_regressor(x_train, y_train,\n                                        n_estimators=10,\n                                        learning_rate=0.3)\n    print(preds)\n```"}
{"task_id": 471, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One step Expected\u2013SARSA TD(0) update for a tabular Q function.\n\n    Parameters\n    ----------\n    q_table : list[list[float]]\n        Current Q-table; q_table[s][a] is Q(s,a).\n    state : int\n        Index of the state *s* where the action was taken.\n    action : int\n        Index of the action *a* taken in state *s*.\n    reward : float\n        Immediate reward received after executing the action.\n    next_state : int | None\n        The successor state *s'*.  Use ``None`` if the transition ended the\n        episode.\n    epsilon : float\n        \u03b5 in the \u03b5-soft policy used to compute the expectation.\n    lr : float\n        Learning-rate \u03b7.\n    gamma : float\n        Discount factor \u03b3.\n\n    Returns\n    -------\n    list[list[float]]\n        The updated Q-table (all entries rounded to four decimals).\n    \"\"\"\n    # Compute expected Q for the next state under the epsilon-soft policy\n    if next_state is None:\n        expected_q = 0.0\n    else:\n        # Number of actions in next_state\n        n_actions = len(q_table[next_state])\n        # Identify the greedy action (first max)\n        greedy_action = int(np.argmax(q_table[next_state]))\n        # Build the policy probabilities for next_state\n        base_prob = epsilon / n_actions\n        probs = np.full(n_actions, base_prob, dtype=float)\n        probs[greedy_action] += (1.0 - epsilon)\n        # Compute the expectation\n        q_vals = np.array(q_table[next_state], dtype=float)\n        expected_q = float(np.dot(probs, q_vals))\n\n    # TD target\n    target = reward + gamma * expected_q\n\n    # TD update\n    q_old = q_table[state][action]\n    q_table[state][action] = q_old + lr * (target - q_old)\n\n    # Round entire Q-table to 4 decimals\n    for s in range(len(q_table)):\n        for a in range(len(q_table[s])):\n            q_table[s][a] = round(q_table[s][a], 4)\n\n    return q_table\n```"}
{"task_id": 474, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None\n                 ) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes (>=2).\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                   of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where:\n          - X_train is an array of shape (m * n_train, 2)\n          - X_val   is an array of shape (m * n_val,   2)\n          - Y_train is an array of shape (m * n_train,)\n          - Y_val   is an array of shape (m * n_val,)\n        All coordinate values in X_train and X_val are rounded to 4 decimals.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    X_train_list = []\n    Y_train_list = []\n    X_val_list = []\n    Y_val_list = []\n\n    for i in range(m):\n        # Compute the horizontal (x) interval for class i\n        x_low  = (i // 2 + 0.1) * 10\n        x_high = (i // 2 + 0.9) * 10\n        # Compute the vertical (y) interval for class i\n        y_low  = ((i % 2) * interval + 0.1) * 10\n        y_high = ((i % 2) * interval + 0.9) * 10\n        # Class label as integer\n        label = int((i - 0.5) * 2)\n\n        # Generate training points for class i\n        for _ in range(n_train):\n            x = random.uniform(x_low, x_high)\n            y = random.uniform(y_low, y_high)\n            X_train_list.append((x, y))\n            Y_train_list.append(label)\n\n        # Generate validation points for class i\n        for _ in range(n_val):\n            x = random.uniform(x_low, x_high)\n            y = random.uniform(y_low, y_high)\n            X_val_list.append((x, y))\n            Y_val_list.append(label)\n\n    # Convert to NumPy arrays\n    X_train = np.array(X_train_list, dtype=float)\n    X_val   = np.array(X_val_list,   dtype=float)\n    Y_train = np.array(Y_train_list, dtype=int)\n    Y_val   = np.array(Y_val_list,   dtype=int)\n\n    # Round coordinates to 4 decimal places\n    X_train = np.round(X_train, 4)\n    X_val   = np.round(X_val,   4)\n\n    return X_train, X_val, Y_train, Y_val\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    X_tr, X_v, Y_tr, Y_v = generate_data(\n        m=4,\n        n_train=5,\n        n_val=3,\n        interval=1.5,\n        seed=42\n    )\n    print(\"X_train:\\n\", X_tr)\n    print(\"Y_train:\\n\", Y_tr)\n    print(\"X_val:\\n\", X_v)\n    print(\"Y_val:\\n\", Y_v)\n```"}
{"task_id": 475, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t,\n                learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current parameter value(s).\n    grad : float | np.ndarray\n        Gradient of the loss w.r.t. w.\n    m_prev : float | np.ndarray | None\n        Previous first moment. If None, initialized to zero.\n    v_prev : float | np.ndarray | None\n        Previous second moment. If None, initialized to zero.\n    t : int\n        Time step (should start at 1).\n    learning_rate : float\n        Step size alpha.\n    b1 : float\n        Exponential decay rate for the first moment.\n    b2 : float\n        Exponential decay rate for the second moment.\n    eps : float\n        Small constant for numerical stability.\n\n    Returns\n    -------\n    w_new : float | np.ndarray\n        Updated parameters.\n    m_t : float | np.ndarray\n        Updated first moment.\n    v_t : float | np.ndarray\n        Updated second moment.\n    \"\"\"\n    # Initialize moments if needed\n    if m_prev is None:\n        m_prev = np.zeros_like(grad, dtype=np.float64)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad, dtype=np.float64)\n\n    # Convert inputs to numpy arrays for consistent ops\n    w_arr = np.array(w, copy=False, dtype=np.float64)\n    g = np.array(grad, copy=False, dtype=np.float64)\n\n    # Update biased first moment estimate\n    m_t = b1 * m_prev + (1.0 - b1) * g\n    # Update biased second moment estimate\n    v_t = b2 * v_prev + (1.0 - b2) * (g * g)\n\n    # Compute bias-corrected moments\n    m_hat = m_t / (1.0 - b1**t)\n    v_hat = v_t / (1.0 - b2**t)\n\n    # Parameter update\n    w_arr_new = w_arr - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n\n    # If original w was a scalar, return scalars\n    if np.isscalar(w):\n        w_new = float(w_arr_new)\n        m_new = float(m_t)\n        v_new = float(v_t)\n    else:\n        w_new = w_arr_new\n        m_new = m_t\n        v_new = v_t\n\n    return w_new, m_new, v_new\n\n\n# Example usage and test\nif __name__ == \"__main__\":\n    # Scalar example\n    w0, m0, v0 = 1.0, None, None\n    grad0 = 0.1\n    t0 = 1\n    w1, m1, v1 = adam_update(w0, grad0, m0, v0, t0)\n    print(\"After step 1:\", w1, m1, v1)  # w1 \u2248 0.999000001\n\n    # Vector example\n    w = np.array([1.0, 2.0])\n    grad = np.array([0.1, -0.2])\n    m, v = None, None\n    for t in range(1, 4):\n        w, m, v = adam_update(w, grad, m, v, t)\n        print(f\"Step {t}:\", w)\n```"}
{"task_id": 479, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where rewards[t][i] is the reward for arm i at time-step t.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    # If no time steps, return empty list\n    if not rewards:\n        return []\n\n    T = len(rewards)\n    N = len(rewards[0])\n\n    # Initialize value estimates and pull counts\n    values = np.full(N, ev_prior, dtype=float)\n    counts = np.zeros(N, dtype=int)\n\n    for t in range(T):\n        # Decide whether to explore or exploit\n        if np.random.rand() < epsilon:\n            # Explore: pick random arm\n            arm = np.random.randint(N)\n        else:\n            # Exploit: pick the arm with highest estimated value (ties -> lowest index)\n            arm = int(np.argmax(values))\n\n        # Observe reward\n        r = rewards[t][arm]\n        # Increment count for the chosen arm\n        counts[arm] += 1\n        # Update estimate using incremental sample mean\n        values[arm] += (r - values[arm]) / counts[arm]\n\n    # Round to 4 decimal places and return as a Python list\n    return [round(v, 4) for v in values]\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Suppose we have 3 arms and 5 time steps\n    rewards_matrix = [\n        [1.0, 0.5, 0.0],\n        [0.8, 0.6, 0.2],\n        [1.2, 0.4, 0.3],\n        [0.9, 0.7, 0.1],\n        [1.1, 0.5, 0.4]\n    ]\n    estimates = epsilon_greedy_bandit(\n        rewards=rewards_matrix,\n        epsilon=0.1,\n        ev_prior=0.0,\n        seed=42\n    )\n    print(\"Final value estimates:\", estimates)\n```"}
{"task_id": 481, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    # Handle empty input\n    if data.size == 0:\n        return []\n\n    n_points = data.shape[0]\n    # Label states: None = unvisited, -1 = noise, >=0 = cluster ID\n    labels = [None] * n_points\n    cluster_id = 0\n\n    def region_query(idx: int) -> list[int]:\n        \"\"\"Return all point indices within eps of point idx.\"\"\"\n        # Compute Euclidean distances from data[idx] to all points\n        diff = data - data[idx]\n        dist = np.linalg.norm(diff, axis=1)\n        # Return list of neighbor indices\n        return list(np.where(dist <= eps)[0])\n\n    # Main DBSCAN loop\n    for i in range(n_points):\n        # Skip if already processed\n        if labels[i] is not None:\n            continue\n\n        # Find neighbors of i\n        neighbors = region_query(i)\n\n        # If not enough neighbors, mark as noise\n        if len(neighbors) < min_samples:\n            labels[i] = -1\n        else:\n            # Start a new cluster\n            labels[i] = cluster_id\n            # Seed set: all neighbors except the point itself\n            seed_set = [nbr for nbr in neighbors if nbr != i]\n            # Expand cluster\n            j = 0\n            while j < len(seed_set):\n                point = seed_set[j]\n                # If this point was marked noise, it's actually border\n                if labels[point] == -1:\n                    labels[point] = cluster_id\n\n                # If unvisited, assign to current cluster and maybe expand\n                if labels[point] is None:\n                    labels[point] = cluster_id\n                    point_neighbors = region_query(point)\n                    # If it's a core point, append its neighbors to seed set\n                    if len(point_neighbors) >= min_samples:\n                        for nbr in point_neighbors:\n                            # Only add if not already processed in this cluster\n                            if labels[nbr] is None or labels[nbr] == -1:\n                                seed_set.append(nbr)\n                j += 1\n\n            # Finished expanding this cluster\n            cluster_id += 1\n\n    return labels\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some sample data\n    rng = np.random.RandomState(0)\n    # Two blobs in 2D\n    blob1 = rng.randn(50, 2) + np.array([5, 5])\n    blob2 = rng.randn(50, 2) + np.array([-5, -5])\n    data = np.vstack([blob1, blob2])\n\n    # Run DBSCAN\n    labels = dbscan(data, eps=1.5, min_samples=5)\n    print(labels)\n```\n\nExplanation of key steps:\n- We keep a `labels` list initialized to `None` (unvisited).  \n- We scan each point in index order.  \n- The `region_query` function finds all neighbors within distance `eps`.  \n- If a point has fewer than `min_samples` neighbors, it is marked noise (`-1`).  \n- Otherwise, it seeds a new cluster, and we iteratively expand that cluster by checking each neighbor:\n  - If the neighbor was noise, it becomes a border point of this cluster.  \n  - If it was unvisited and has enough neighbors, its neighbors are added to the seed set (density reachability).  \n- Cluster IDs start at `0` and increment in the order clusters are discovered.  \n- Points never assigned to a cluster remain noise (`-1`)."}
{"task_id": 482, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If None (default), the number is inferred automatically as\n            max(y) + 1.\n\n    Returns:\n        np.ndarray: A matrix of shape (len(y), num_classes) filled with 0s\n        and 1s (dtype=int), representing the one-hot encoding of the input\n        labels.\n\n    Raises:\n        ValueError: If labels contain negative values or values >= num_classes.\n    \"\"\"\n    # Convert input to a 1-D integer NumPy array\n    y_arr = np.array(y, dtype=int).flatten()\n    n = y_arr.shape[0]\n    \n    # Infer num_classes if not provided\n    if num_classes is None:\n        if n > 0:\n            num_classes = int(y_arr.max()) + 1\n        else:\n            num_classes = 0\n    \n    # Validate labels\n    if np.any(y_arr < 0):\n        raise ValueError(\"Labels must be non-negative.\")\n    if n > 0 and np.any(y_arr >= num_classes):\n        raise ValueError(\n            f\"All label values must be less than num_classes ({num_classes}).\"\n        )\n\n    # Create the one-hot encoded matrix\n    one_hot = np.zeros((n, num_classes), dtype=int)\n    if n > 0 and num_classes > 0:\n        rows = np.arange(n)\n        one_hot[rows, y_arr] = 1\n\n    return one_hot\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    labels = [0, 2, 1, 3]\n    print(to_categorical(labels))\n    # Output:\n    # [[1 0 0 0]\n    #  [0 0 1 0]\n    #  [0 1 0 0]\n    #  [0 0 0 1]]\n    \n    # Specify num_classes explicitly\n    print(to_categorical(labels, num_classes=5))\n    # Output shape: (4, 5)\n```"}
{"task_id": 485, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\"\"\"\n\n    # Convert to numpy arrays\n    X_train = np.array(X_train, dtype=float)\n    y_train = np.array(y_train, dtype=float)\n    if X_train.size == 0:\n        return []\n\n    def build_tree(X, y, depth):\n        # Create a leaf node\n        def make_leaf(vals):\n            return {'is_leaf': True, 'value': float(np.mean(vals))}\n\n        # Stopping criteria\n        if depth >= max_depth or X.size < 2:\n            return make_leaf(y)\n\n        # Sort samples by X to find candidate splits\n        order = np.argsort(X)\n        Xs = X[order]\n        ys = y[order]\n\n        # Candidate splits: midpoints where adjacent Xs differ\n        mids = (Xs[:-1] + Xs[1:]) / 2.0\n        distinct = Xs[:-1] != Xs[1:]\n        if not np.any(distinct):\n            return make_leaf(y)\n\n        best_sse = np.inf\n        best_split = None\n\n        # Evaluate each candidate split\n        for split in mids[distinct]:\n            left_mask = X <= split\n            yL = y[left_mask]\n            yR = y[~left_mask]\n            if yL.size == 0 or yR.size == 0:\n                continue\n            sseL = np.sum((yL - yL.mean())**2)\n            sseR = np.sum((yR - yR.mean())**2)\n            sse = sseL + sseR\n            if sse < best_sse:\n                best_sse = sse\n                best_split = split\n\n        # If no valid split was found, make a leaf\n        if best_split is None:\n            return make_leaf(y)\n\n        # Recurse on left and right subsets\n        left_mask = X <= best_split\n        left_node = build_tree(X[left_mask], y[left_mask], depth + 1)\n        right_node = build_tree(X[~left_mask], y[~left_mask], depth + 1)\n        return {\n            'is_leaf': False,\n            'split': best_split,\n            'left': left_node,\n            'right': right_node\n        }\n\n    # Build the tree\n    tree = build_tree(X_train, y_train, depth=0)\n\n    # Prediction function for a single sample\n    def predict_one(x):\n        node = tree\n        while not node['is_leaf']:\n            if x <= node['split']:\n                node = node['left']\n            else:\n                node = node['right']\n        return node['value']\n\n    # Apply to each test sample\n    return [predict_one(float(x)) for x in X_test]\n\n\n# Example usage:\nif __name__ == '__main__':\n    Xtr = [2.3, 1.5, 3.7, 2.9, 4.0, 3.3]\n    ytr = [1.2, 0.5, 1.8, 1.3, 2.0, 1.6]\n    Xte = [2.0, 3.0, 4.5]\n    preds = regression_tree_predict(Xtr, ytr, Xte, max_depth=2)\n    print(\"Predictions:\", preds)\n```"}
{"task_id": 490, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    arr = np.array(x)\n    if arr.ndim != 1 or arr.size == 0:\n        return []\n    diag_matrix = np.diag(arr)\n    return diag_matrix.tolist()\n\n# Example usage and simple tests\nif __name__ == \"__main__\":\n    examples = [\n        [],                   # empty\n        [1, 2, 3],            # list of ints\n        (4.5, 5.5),           # tuple of floats\n        np.array([7, 8, 9.0]) # numpy array\n    ]\n    for v in examples:\n        print(f\"Input: {v!r}\")\n        print(\"Diagonal matrix:\")\n        for row in make_diagonal(v):\n            print(row)\n        print()\n```"}
{"task_id": 491, "completion_id": 0, "completion": "```python\nfrom collections import Counter, defaultdict\nimport itertools\n\nclass TreeNode:\n    def __init__(self, item, count, parent):\n        self.item = item            # the item name\n        self.count = count          # support count\n        self.parent = parent        # parent TreeNode\n        self.children = {}          # item_name -> TreeNode\n        self.node_link = None       # link to next node with same item\n\n    def increment(self, n):\n        \"\"\"Increment the count of this node by n.\"\"\"\n        self.count += n\n\ndef insert_tree(items, node, header_table, count):\n    \"\"\"Recursively insert the ordered items into the FP-tree.\"\"\"\n    if not items:\n        return\n    first = items[0]\n    # If the child exists, increment it; otherwise create a new child.\n    if first in node.children:\n        child = node.children[first]\n        child.increment(count)\n    else:\n        child = TreeNode(first, count, node)\n        node.children[first] = child\n        # Update header table to link to this new node\n        _, head = header_table[first]\n        if head is None:\n            header_table[first][1] = child\n        else:\n            # follow node_link to the end and append\n            while head.node_link:\n                head = head.node_link\n            head.node_link = child\n    # Recurse on the remaining items\n    insert_tree(items[1:], child, header_table, count)\n\ndef find_prefix_paths(base_item, header_table):\n    \"\"\"Return the conditional pattern base for base_item as a list of (prefix_path, count).\"\"\"\n    paths = []\n    node = header_table[base_item][1]  # first node of base_item\n    while node:\n        # ascend from node to root, collecting the path items\n        prefix = []\n        parent = node.parent\n        while parent and parent.item is not None:\n            prefix.append(parent.item)\n            parent = parent.parent\n        if prefix:\n            paths.append((prefix[::-1], node.count))  # reverse to get root->leaf order\n        node = node.node_link\n    return paths\n\ndef build_conditional_tree(pattern_base, min_sup):\n    \"\"\"Given a conditional pattern base, build its FP-tree and header table.\"\"\"\n    # First count frequency of items in the pattern base\n    freq = Counter()\n    for path, cnt in pattern_base:\n        for item in path:\n            freq[item] += cnt\n    # Remove items below min_sup\n    freq = {item: c for item, c in freq.items() if c >= min_sup}\n    if not freq:\n        return None, None\n\n    # Initialize header table: item -> [count, head_node]\n    header_table = {item: [cnt, None] for item, cnt in freq.items()}\n    root = TreeNode(None, 1, None)\n\n    # Insert paths into the conditional tree\n    for path, cnt in pattern_base:\n        # filter and sort path by descending freq (and lex tiebreak)\n        filtered = [i for i in path if i in freq]\n        if not filtered:\n            continue\n        sorted_items = sorted(filtered, key=lambda i: (-freq[i], i))\n        insert_tree(sorted_items, root, header_table, cnt)\n\n    return root, header_table\n\ndef is_single_path(node):\n    \"\"\"Check if the FP-tree has a single path (no node has more than one child).\"\"\"\n    num_children = len(node.children)\n    if num_children > 1:\n        return False\n    elif num_children == 0:\n        return True\n    else:\n        # exactly one child -> recurse\n        return is_single_path(next(iter(node.children.values())))\n\ndef mine_tree(root, header_table, min_sup, prefix, frequent_sets):\n    \"\"\"Recursively mine the FP-tree.\"\"\"\n    # If the tree is a single path, enumerate all combinations at once\n    if is_single_path(root):\n        # collect the single path\n        node = root\n        single_path = []\n        while len(node.children) == 1:\n            node = next(iter(node.children.values()))\n            single_path.append((node.item, node.count))\n        items = [item for item, cnt in single_path]\n        # enumerate all non-empty combinations of the items on the path\n        for r in range(1, len(items) + 1):\n            for comb in itertools.combinations(items, r):\n                new_set = prefix + list(comb)\n                frequent_sets.add(tuple(sorted(new_set)))\n    else:\n        # for each item in header_table in ascending order of frequency\n        # (to ensure smaller conditional trees first)\n        sorted_items = sorted(header_table.items(),\n                              key=lambda x: (x[1][0], x[0]))\n        for base_item, (sup, _) in sorted_items:\n            new_prefix = prefix + [base_item]\n            frequent_sets.add(tuple(sorted(new_prefix)))\n            # Build conditional pattern base\n            pattern_base = find_prefix_paths(base_item, header_table)\n            # Build conditional FP-tree\n            cond_root, cond_header = build_conditional_tree(pattern_base, min_sup)\n            if cond_header is not None:\n                # Recursively mine the conditional FP-tree\n                mine_tree(cond_root, cond_header, min_sup, new_prefix, frequent_sets)\n\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\n\n    Parameters\n    ----------\n    transactions : Iterable[Iterable[Hashable]]\n        A collection of transactions.  Each transaction is an iterable containing hashable items.\n    min_sup : int\n        Minimum number of occurrences an item-set must have to be considered frequent.\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets sorted 1) by length, 2) lexicographically.  Every\n        tuple itself is sorted lexicographically.\n    \"\"\"\n    # First scan: count item frequencies\n    item_counts = Counter()\n    for trans in transactions:\n        item_counts.update(trans)\n    # Remove infrequent items\n    freq_items = {item: cnt for item, cnt in item_counts.items() if cnt >= min_sup}\n    if not freq_items:\n        return []\n\n    # Build the initial header table\n    header_table = {item: [cnt, None] for item, cnt in freq_items.items()}\n    # Create the root of the FP-tree\n    root = TreeNode(None, 1, None)\n\n    # Second scan: construct the FP-tree\n    for trans in transactions:\n        # Filter transaction and sort by descending frequency then lex\n        filtered = [i for i in trans if i in freq_items]\n        if not filtered:\n            continue\n        ordered = sorted(filtered, key=lambda i: (-freq_items[i], i))\n        insert_tree(ordered, root, header_table, 1)\n\n    # Mine the FP-tree\n    frequent_sets = set()\n    mine_tree(root, header_table, min_sup, [], frequent_sets)\n\n    # Prepare sorted output\n    # Each tuple is already sorted internally; now sort by length and lex\n    result = list(frequent_sets)\n    result.sort(key=lambda t: (len(t),) + t)\n    return result\n\n# Example usage and test\nif __name__ == \"__main__\":\n    Transactions = [\n        [\"A\",\"B\",\"D\",\"E\"],\n        [\"B\",\"C\",\"E\"],\n        [\"A\",\"B\",\"D\",\"E\"],\n        [\"A\",\"B\",\"C\",\"E\"],\n        [\"A\",\"B\",\"C\",\"D\",\"E\"],\n        [\"B\",\"C\",\"D\"]\n    ]\n    minsup = 3\n    freq_itemsets = fp_growth(Transactions, minsup)\n    print(freq_itemsets)\n    # Expected output:\n    # [('A',), ('B',), ('C',), ('D',), ('E',),\n    #  ('A','B'), ('A','D'), ('A','E'), ('B','C'), ('B','D'),\n    #  ('B','E'), ('C','E'), ('D','E'),\n    #  ('A','B','D'), ('A','B','E'), ('A','D','E'),\n    #  ('B','C','E'), ('B','D','E'),\n    #  ('A','B','D','E')]\n```"}
{"task_id": 492, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to a given degree.\n\n    The function should take a 2-D list *X* whose rows correspond to\n    observations and columns correspond to original features.  It must\n    return a new list of lists containing, for every observation, all\n    monomials of the original features whose total degree does not exceed\n    the supplied *degree*.  The very first element in every row should be\n    1 (the degree-0 term).\n\n    If *degree* is negative the function must return -1.\n\n    Args:\n        X: Input data of shape (n_samples, n_features).\n        degree: Maximum total degree of the generated polynomial terms.\n\n    Returns:\n        A 2-D Python list with the enriched feature matrix, or -1 when\n        *degree* is negative.\n    \"\"\"\n    # If degree is negative, per specification, return -1\n    if degree < 0:\n        return -1\n\n    # Convert input to numpy array for easier indexing\n    X_arr = np.array(X, dtype=float)\n    \n    # Handle edge case: no samples\n    n_samples = X_arr.shape[0]\n    if n_samples == 0:\n        return []\n    n_features = X_arr.shape[1]\n    \n    # Generate all combinations of feature indices up to the given degree\n    # Each combination of length d corresponds to the monomials of total degree d\n    combos: list[tuple[int, ...]] = []\n    for d in range(degree + 1):\n        # combinations_with_replacement chooses indices with repetition,\n        # yielding all multisets of size d in lex order\n        for comb in combinations_with_replacement(range(n_features), d):\n            combos.append(comb)\n    \n    # Build the feature matrix\n    feature_matrix: list[list[float]] = []\n    for sample in X_arr:\n        row_feats: list[float] = []\n        for comb in combos:\n            # For the empty tuple (degree=0) we want the constant term 1\n            if not comb:\n                row_feats.append(1.0)\n            else:\n                # multiply the selected features\n                prod = 1.0\n                for idx in comb:\n                    prod *= sample[idx]\n                row_feats.append(prod)\n        feature_matrix.append(row_feats)\n    \n    return feature_matrix\n\n# Example usage:\nif __name__ == \"__main__\":\n    # A small test\n    X = [[2, 3],\n         [4, 5]]\n    d = 2\n    # Expected features: [1, x0, x1, x0^2, x0*x1, x1^2]\n    feats = polynomial_features(X, d)\n    for row in feats:\n        print(row)\n```\n\nExplanation:\n\n1. We first handle the special case where `degree < 0` by returning -1.\n2. We convert the input to a NumPy array to simplify indexing, but we build the output as a pure Python list of lists.\n3. We enumerate all index-combinations (with repetition) of lengths 0 through `degree`. A combination of length `d` corresponds exactly to a monomial of total degree `d`.\n4. For each sample and each combination, we compute the product of the corresponding feature values (using 1.0 for the empty combination).\n5. We return the resulting 2D list."}
{"task_id": 493, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean. If the input sequence is empty the function\n    returns -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n               *y* is empty.\n    \"\"\"\n    # Convert to numpy array of floats\n    arr = np.asarray(y, dtype=float)\n    # Check for empty input\n    if arr.size == 0:\n        return -1\n    # Compute MSE\n    mean_val = arr.mean()\n    mse_val = np.mean((arr - mean_val) ** 2)\n    # Round to 4 decimal places and return\n    return round(float(mse_val), 4)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    samples = [\n        [1, 2, 3, 4],\n        np.array([2.5, 3.5, 4.0, 5.5]),\n        [],\n        [10]\n    ]\n    for s in samples:\n        print(f\"mse({s}) = {mse(s)}\")\n```"}
{"task_id": 496, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list, NumPy array, or scalar containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to a NumPy array of floats, treating scalars as length-1 arrays.\n    x_arr = np.atleast_1d(np.array(x, dtype=float))\n\n    # Compute the affine activation\n    y = slope * x_arr + intercept\n    # First derivative is constant slope\n    grad = np.full_like(x_arr, fill_value=slope, dtype=float)\n    # Second derivative is zero\n    grad2 = np.zeros_like(x_arr, dtype=float)\n\n    # Round to 4 decimal places\n    y_rounded = np.round(y, 4)\n    grad_rounded = np.round(grad, 4)\n    grad2_rounded = np.round(grad2, 4)\n\n    # Convert back to Python lists\n    return y_rounded.tolist(), grad_rounded.tolist(), grad2_rounded.tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example 1: list input\n    x_list = [0.123456, -1.98765, 3.14159]\n    y, dy, d2y = apply_affine(x_list, slope=2.5, intercept=-0.5)\n    print(\"Input list:\", x_list)\n    print(\"Affine output:\", y)\n    print(\"First derivative:\", dy)\n    print(\"Second derivative:\", d2y)\n    print()\n\n    # Example 2: NumPy array input\n    x_arr = np.array([1.2345, 2.3456])\n    y2, dy2, d2y2 = apply_affine(x_arr)\n    print(\"Input array:\", x_arr)\n    print(\"Affine output (default slope=1, intercept=0):\", y2)\n    print(\"First derivative:\", dy2)\n    print(\"Second derivative:\", d2y2)\n    print()\n\n    # Example 3: scalar input\n    y3, dy3, d2y3 = apply_affine(7.7777, slope=0.1, intercept=0.9)\n    print(\"Input scalar: 7.7777\")\n    print(\"Affine output:\", y3)\n    print(\"First derivative:\", dy3)\n    print(\"Second derivative:\", d2y3)\n```"}
{"task_id": 499, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places. If *k* is outside the\n        valid range, return \u22121.\n    \"\"\"\n    # Check dimensions\n    if data.ndim != 2:\n        raise ValueError(\"Input data must be a 2D array.\")\n    n_samples, n_features = data.shape\n    # Validate k\n    if not (1 <= k <= n_features):\n        return -1\n\n    # 1. Standardise each feature: zero mean, unit population variance\n    #    If variance is zero, leave the centered feature as all zeros.\n    means = np.mean(data, axis=0)\n    Z = data - means  # centered data\n    pop_var = np.mean(Z**2, axis=0)  # population variance\n    nonzero = pop_var > 0\n    Z[:, nonzero] = Z[:, nonzero] / np.sqrt(pop_var[nonzero])\n\n    # 2. Sample covariance matrix (with Bessel's correction)\n    cov = (Z.T @ Z) / (n_samples - 1)\n\n    # 3. Eigen-decomposition\n    # Since cov is symmetric, use eigh for numerical stability\n    eigvals, eigvecs = np.linalg.eigh(cov)\n\n    # 4. Sort eigenvalues (and eigenvectors) in descending order\n    idx_desc = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[idx_desc]\n    eigvecs = eigvecs[:, idx_desc]\n\n    # 5. Fix the sign of eigenvectors: make the largest-abs entry positive\n    for i in range(n_features):\n        vec = eigvecs[:, i]\n        max_idx = np.argmax(np.abs(vec))\n        if vec[max_idx] < 0:\n            eigvecs[:, i] = -vec\n\n    # 6. Project the standardised data onto first k principal components\n    projected = Z @ eigvecs[:, :k]\n\n    # Round to 4 decimal places and convert to nested list\n    return np.round(projected, 4).tolist()\n```"}
{"task_id": 500, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_classifier(X_train: list[list[float]],\n                                 y_train: list[int],\n                                 X_test: list[list[float]],\n                                 n_estimators: int = 20,\n                                 learning_rate: float = 0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\n\n    The ensemble minimises the squared-error on the *binary* targets and turns the\n    final regression score into a class label by thresholding at 0.5.\n\n    Args:\n        X_train: Training samples, each sample being a list of feature values.\n        y_train: Binary labels (0 or 1) for the training samples.\n        X_test: Samples to predict \u2013 identical structure to X_train.\n        n_estimators: Number of boosting stages to perform.\n        learning_rate: Shrinkage factor (\u03b7) applied to each weak learner.\n\n    Returns:\n        A list containing the predicted class (0 or 1) for every sample in\n        X_test.\n    \"\"\"\n    # Convert to numpy arrays\n    X = np.array(X_train, dtype=float)\n    y = np.array(y_train, dtype=float)\n    N, D = X.shape\n\n    # Initial prediction is the average of y\n    F0 = np.mean(y)\n    F = np.full(N, F0, dtype=float)\n\n    # To store stumps: list of (feature_index, threshold, left_value, right_value)\n    stumps = []\n\n    for m in range(n_estimators):\n        # Compute residuals\n        residuals = y - F\n\n        # Find best stump minimizing squared error on residuals\n        best_error = np.inf\n        best_stump = None\n\n        for j in range(D):\n            x_j = X[:, j]\n            unique_vals = np.unique(x_j)\n            # Candidate thresholds: midpoints between sorted unique values\n            if unique_vals.size > 1:\n                thresh_candidates = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n            else:\n                thresh_candidates = unique_vals.copy()\n\n            for thresh in thresh_candidates:\n                left_mask = (x_j <= thresh)\n                right_mask = ~left_mask\n\n                # Compute mean residual in each region\n                if np.any(left_mask):\n                    left_val = residuals[left_mask].mean()\n                else:\n                    left_val = 0.0\n                if np.any(right_mask):\n                    right_val = residuals[right_mask].mean()\n                else:\n                    right_val = 0.0\n\n                # Compute squared error\n                err_left = ((residuals[left_mask] - left_val) ** 2).sum()\n                err_right = ((residuals[right_mask] - right_val) ** 2).sum()\n                total_err = err_left + err_right\n\n                if total_err < best_error:\n                    best_error = total_err\n                    best_stump = (j, thresh, left_val, right_val)\n\n        # Store the best stump\n        stumps.append(best_stump)\n        feat_idx, thresh, lv, rv = best_stump\n\n        # Update F with the new stump's predictions\n        pred = np.where(X[:, feat_idx] <= thresh, lv, rv)\n        F += learning_rate * pred\n\n    # Predict on X_test\n    X_t = np.array(X_test, dtype=float)\n    M = len(stumps)\n    # Start from F0\n    F_pred = np.full(X_t.shape[0], F0, dtype=float)\n\n    # Add contributions of each stump\n    for feat_idx, thresh, lv, rv in stumps:\n        pred = np.where(X_t[:, feat_idx] <= thresh, lv, rv)\n        F_pred += learning_rate * pred\n\n    # Threshold at 0.5 to get binary class labels\n    y_pred = (F_pred >= 0.5).astype(int)\n\n    return y_pred.tolist()\n```"}
{"task_id": 505, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\n\n    Args:\n        X: A 2-D NumPy array of shape (batch_size, n_visible) containing the\n           mini-batch of visible units. Values are expected to be in [0,1].\n        W: Weight matrix of shape (n_visible, n_hidden).\n        hbias: 1-D NumPy array (length n_hidden) with hidden unit biases.\n        vbias: 1-D NumPy array (length n_visible) with visible unit biases.\n        learning_rate: Scalar learning-rate used for the update.\n        k: Number of Gibbs sampling steps to run (CD-k).\n\n    Returns:\n        The updated weight matrix **rounded to 4 decimal places** and converted\n        to a regular Python list of lists.\n    \"\"\"\n    def sigmoid(x: np.ndarray) -> np.ndarray:\n        return 1.0 / (1.0 + np.exp(-x))\n    \n    m = X.shape[0]\n    # If the mini-batch is empty, return an empty list\n    if m == 0:\n        return []\n    \n    # Positive phase: compute hidden probabilities given data\n    h0_prob = sigmoid(X.dot(W) + hbias)  # shape (m, n_hidden)\n    \n    # Start Gibbs chain from the positive phase hidden probabilities\n    h_prob = h0_prob\n    v_prob = None\n    \n    # k full Gibbs steps (hidden -> visible -> hidden) using probabilities\n    for _ in range(k):\n        # Reconstruct visible units (probabilities)\n        v_prob = sigmoid(h_prob.dot(W.T) + vbias)  # shape (m, n_visible)\n        # Recompute hidden units from these visibles\n        h_prob = sigmoid(v_prob.dot(W) + hbias)    # shape (m, n_hidden)\n    \n    # Compute gradients\n    pos_grad = X.T.dot(h0_prob)      # shape (n_visible, n_hidden)\n    neg_grad = v_prob.T.dot(h_prob)  # shape (n_visible, n_hidden)\n    \n    # Update weights\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n    \n    # Round to 4 decimal places and convert to Python list\n    W_new_rounded = np.round(W_new, 4)\n    return W_new_rounded.tolist()\n```"}
{"task_id": 509, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length. If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    # Convert inputs to 1-D numpy arrays of floats\n    arr_x = np.asarray(x, dtype=float)\n    arr_y = np.asarray(y, dtype=float)\n\n    # Check that both are 1-D and have the same shape\n    if arr_x.shape != arr_y.shape or arr_x.ndim != 1:\n        return -1\n\n    # If zero-length, define distance as 0.0\n    if arr_x.size == 0:\n        return 0.0\n\n    # Compute elementwise absolute difference and take the maximum\n    max_diff = np.abs(arr_x - arr_y).max()\n\n    # Convert to Python float and round to 4 decimal places\n    return round(float(max_diff), 4)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    x = [1, 2, 3]\n    y = [2, 4, 6]\n    print(chebyshev(x, y))       # Expected output: 3.0\n    print(chebyshev([], []))     # Expected output: 0.0\n    print(chebyshev([1,2], [1])) # Expected output: -1\n```"}
{"task_id": 510, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function returns the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as y.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as t_mean.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    # Numerical stability for log\n    eps = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, eps, 1. - eps)\n\n    # Reconstruction loss (binary cross-entropy), summed over features\n    rec = - (y * np.log(y_pred_clipped) +\n             (1. - y) * np.log(1. - y_pred_clipped))\n    rec_loss_per_sample = np.sum(rec, axis=1)  # shape (batch_size,)\n\n    # KL divergence term, summed over latent dimensions\n    # KL = -0.5 * sum(1 + log_var - mean^2 - exp(log_var))\n    kl_element = 1. + t_log_var - np.square(t_mean) - np.exp(t_log_var)\n    kl_per_sample = -0.5 * np.sum(kl_element, axis=1)  # shape (batch_size,)\n\n    # Total loss per sample\n    total_per_sample = rec_loss_per_sample + kl_per_sample\n\n    # Average over batch\n    avg_loss = np.mean(total_per_sample)\n\n    # Round to 6 decimal places and return as Python float\n    return float(np.round(avg_loss, 6))\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Dummy data for demonstration\n    batch_size, n_features, latent_dim = 4, 10, 3\n    y = np.random.randint(0, 2, size=(batch_size, n_features)).astype(float)\n    y_pred = np.random.rand(batch_size, n_features)\n    t_mean = np.random.randn(batch_size, latent_dim)\n    t_log_var = np.random.randn(batch_size, latent_dim) - 1.0  # some negative values\n\n    loss_value = vae_loss(y, y_pred, t_mean, t_log_var)\n    print(\"VAE loss:\", loss_value)\n```"}
{"task_id": 513, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X_arr = np.array(X, dtype=float)          # shape: (n_samples, n_features)\n    w_arr = np.array(w, dtype=float)          # shape: (n_features,)\n    V_arr = np.array(V, dtype=float)          # shape: (n_features, k)\n\n    # Linear term: w0 + X \u00b7 w\n    linear_term = X_arr.dot(w_arr)            # shape: (n_samples,)\n\n    # Interaction term:\n    # For each latent factor f:\n    #   term_f = (sum_j V[j,f] * X[i,j])^2 - sum_j (V[j,f]^2 * X[i,j]^2)\n    # Sum over f and multiply by 0.5\n    XV = X_arr.dot(V_arr)                     # shape: (n_samples, k)\n    XV_squared = XV**2                        # (sum_j V[j,f] x_ij)^2\n    V_squared = V_arr**2                      # V[j,f]^2\n    X_squared = X_arr**2                      # x_ij^2\n    interaction_term = 0.5 * np.sum(\n        XV_squared - X_squared.dot(V_squared),\n        axis=1\n    )                                         # shape: (n_samples,)\n\n    # Final prediction\n    y_pred = w0 + linear_term + interaction_term\n\n    # Round to 4 decimal places and return as Python list\n    return np.round(y_pred, 4).tolist()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Toy data: 2 samples, 3 features, 2 latent factors\n    X = [\n        [1.0, 0.5, -1.2],\n        [0.3, -0.2,  2.1]\n    ]\n    w0 = 0.1\n    w = [0.5, -0.3, 0.8]\n    V = [\n        [0.2, -0.1],\n        [0.4,  0.3],\n        [-0.5, 0.6]\n    ]\n    preds = fm_predict(X, w0, w, V)\n    print(\"Predictions:\", preds)\n```"}
{"task_id": 517, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function works for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a Python scalar (int or float), otherwise a Python\n        list obtained with ``numpy.ndarray.tolist()``, where every element\n        is the sigmoid of the corresponding input value rounded to four\n        decimal places.\n    \"\"\"\n    # Convert input to a NumPy array of floats\n    arr = np.array(z, dtype=float)\n\n    # Prepare an output array of the same shape\n    result = np.empty_like(arr)\n\n    # Mask for non-negative entries\n    pos_mask = arr >= 0\n    # For z >= 0: sigmoid(z) = 1 / (1 + exp(-z))\n    result[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n\n    # For z < 0: sigmoid(z) = exp(z) / (1 + exp(z))\n    neg_mask = ~pos_mask\n    exp_z = np.exp(arr[neg_mask])\n    result[neg_mask] = exp_z / (1.0 + exp_z)\n\n    # Round to 4 decimal places\n    result = np.round(result, 4)\n\n    # If the original input was a Python scalar, return a float\n    if isinstance(z, (int, float)):\n        return float(result)\n\n    # Otherwise return a (nested) Python list\n    return result.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(sigmoid(-10))          # -> 0.0000\n    print(sigmoid(0))            # -> 0.5000\n    print(sigmoid(10))           # -> 1.0000\n    print(sigmoid([-1, 0, 1]))   # -> [0.2689, 0.5, 0.7311]\n    print(sigmoid([[1000, -1000], [10, -10]]))\n    # -> [[1.0, 0.0], [0.99995, 0.00005]]\n```"}
{"task_id": 518, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n    def wrapper(actual: np.ndarray, predicted: np.ndarray):\n        # Convert actual if it's one-hot (2-D with more than one column)\n        if isinstance(actual, np.ndarray) and actual.ndim == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        # Convert predicted if it's one-hot (2-D with more than one column)\n        if isinstance(predicted, np.ndarray) and predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        return function(actual, predicted)\n    return wrapper\n\n# Example usage\n@unhot\ndef accuracy(actual: np.ndarray, predicted: np.ndarray) -> float:\n    \"\"\"Simple accuracy rounded to 4 decimals.\"\"\"\n    return float(np.round(np.mean(actual == predicted), 4))\n\nif __name__ == \"__main__\":\n    actual    = np.array([[0,1,0], [1,0,0], [0,0,1]])  # one-hot\n    predicted = np.array([[0,1,0], [0,1,0], [0,0,1]])  # one-hot\n    print(accuracy(actual, predicted))  # 0.6667\n```\n"}
{"task_id": 520, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numbers\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise, rounded to 4 decimals.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    def _process_list(lst):\n        \"\"\"Recursively process a nested list of numbers.\"\"\"\n        result = []\n        for elem in lst:\n            if isinstance(elem, list):\n                # Recurse into sublist\n                result.append(_process_list(elem))\n            else:\n                # Treat as numeric scalar\n                val = float(elem)\n                grad = np.round(val * (1.0 - val), 4)\n                result.append(float(grad))\n        return result\n\n    # Case 1: NumPy array\n    if isinstance(z, np.ndarray):\n        # Scalar array\n        if z.ndim == 0:\n            val = float(z)\n            return float(np.round(val * (1.0 - val), 4))\n        # Vector or matrix -> convert to nested lists\n        py_list = z.tolist()\n        return _process_list(py_list)\n\n    # Case 2: Python list (possibly nested)\n    if isinstance(z, list):\n        # Empty list edge case\n        if len(z) == 0:\n            return []\n        return _process_list(z)\n\n    # Case 3: Scalar number (Python or NumPy scalar)\n    if isinstance(z, numbers.Number):\n        val = float(z)\n        return float(np.round(val * (1.0 - val), 4))\n\n    # Unsupported type\n    raise TypeError(f\"Unsupported input type: {type(z)}\")\n\n\n# Example usages:\nif __name__ == \"__main__\":\n    # Scalar\n    print(sigmoid_grad(0.5))               # 0.25\n\n    # List\n    print(sigmoid_grad([0.5, 0.1, 0.9]))    # [0.25, 0.09, 0.09]\n    print(sigmoid_grad([]))                # []\n\n    # Nested list\n    mat = [[0.5, 0.2], [0.9, 0.3]]\n    print(sigmoid_grad(mat))  # [[0.25, 0.16], [0.09, 0.21]]\n\n    # NumPy array\n    arr = np.array([[0.2, 0.8], [0.0, 1.0]])\n    print(sigmoid_grad(arr))  # [[0.16, 0.16], [0.0, 0.0]]\n```"}
{"task_id": 528, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape (grid_n \u00d7 grid_n) containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    # Convert input data to NumPy array\n    arr = np.array(X, dtype=float)\n    if arr.ndim != 2 or arr.shape[1] != 2:\n        raise ValueError(\"X must be a list of [x1, x2] pairs.\")\n    \n    # Determine the bounding box of the data\n    x1_min, x1_max = arr[:, 0].min(), arr[:, 0].max()\n    x2_min, x2_max = arr[:, 1].min(), arr[:, 1].max()\n    \n    # Build the 1D grids for each axis\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    \n    # Create the 2D meshgrid\n    X1, X2 = np.meshgrid(x1_plot, x2_plot)\n    \n    # Unpack weights\n    w1, w2 = W\n    \n    # Compute the decision function on the grid\n    Z = w1 * X1 + w2 * X2 + b\n    \n    # Apply the sign rule: 1 if >= 0, else -1\n    labels = np.where(Z >= 0, 1, -1).astype(int)\n    \n    # Convert to a nested Python list and return\n    return labels.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X_sample = [[0, 0], [1, 2], [2, 1], [-1, 1]]\n    W_sample = [0.5, -0.3]\n    b_sample = 0.1\n    grid = decision_boundary_grid(X_sample, W_sample, b_sample, grid_n=5)\n    for row in grid:\n        print(row)\n```"}
{"task_id": 537, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\n\n    Training phase: estimate mean, standard deviation and class prior for\n    every feature of both classes.\n    Prediction phase: compute the posterior probability for each class\n    using the Gaussian likelihood (with feature independence assumption)\n    and return the most probable class (0 or 1) for every test sample.\n\n    Args:\n        X_train: 2-D NumPy array of shape (n_samples, n_features) containing\n                  the training features.\n        y_train: 1-D NumPy array of length n_samples with binary labels\n                  (each entry is 0 or 1).\n        X_test:  2-D NumPy array of shape (m_samples, n_features) containing\n                  the data to classify.\n\n    Returns:\n        A Python list of length m_samples where each element is the\n        predicted class label (0 or 1).\n    \"\"\"\n    # Small constant to avoid division by zero\n    eps = 1e-9\n\n    # Identify number of features\n    n_features = X_train.shape[1]\n\n    # Prepare storage for mean, std and priors for classes 0 and 1\n    means = np.zeros((2, n_features))\n    stds = np.zeros((2, n_features))\n    priors = np.zeros(2)\n\n    # Estimate parameters for each class\n    for c in [0, 1]:\n        X_c = X_train[y_train == c]\n        means[c, :] = X_c.mean(axis=0)\n        stds[c, :] = X_c.std(axis=0) + eps\n        priors[c] = X_c.shape[0] / X_train.shape[0]\n\n    # Function to compute log Gaussian likelihood of a feature vector x under class c\n    def log_likelihood(x: np.ndarray, c: int) -> float:\n        var = stds[c] ** 2\n        # term1 = -0.5 * log(2*pi*var)\n        term1 = -0.5 * np.log(2 * np.pi * var)\n        # term2 = - (x - mu)^2 / (2*var)\n        term2 = - ( (x - means[c]) ** 2 ) / (2 * var)\n        return np.sum(term1 + term2)\n\n    # Predict class for each test sample\n    predictions: list[int] = []\n    for x in X_test:\n        # Compute posterior log-probabilities for both classes\n        log_post_c0 = np.log(priors[0]) + log_likelihood(x, 0)\n        log_post_c1 = np.log(priors[1]) + log_likelihood(x, 1)\n        # Choose the class with higher posterior probability\n        pred = 0 if log_post_c0 > log_post_c1 else 1\n        predictions.append(pred)\n\n    return predictions\n```"}
{"task_id": 539, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Compute the binary cross\u2010entropy (log\u2010loss) cost.\n\n    Arguments:\n    AL -- Predicted probabilities, numpy array of shape (m,) or (1, m)\n    Y  -- True labels (0 or 1), same shape as AL\n\n    Returns:\n    cost -- Python float, the binary cross\u2010entropy cost rounded to 4 decimal places\n    \"\"\"\n    # Number of examples\n    m = Y.shape[1] if Y.ndim == 2 else Y.shape[0]\n\n    # Clip AL to avoid log(0)\n    eps = 1e-15\n    AL_clipped = np.clip(AL, eps, 1 - eps)\n\n    # Compute cost\n    loss_terms = Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped)\n    cost = - np.sum(loss_terms) / m\n\n    # Return as plain Python float rounded to 4 decimals\n    return float(np.round(cost, 4))\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example 1\n    AL = np.array([0.8, 0.9, 0.4, 0.2])\n    Y  = np.array([1,   0,   1,   0])\n    print(\"Cost =\", compute_cost(AL, Y))  # e.g. 0.6407\n\n    # Example 2 with shape (1, m)\n    AL2 = np.array([[0.99, 0.01, 0.5]])\n    Y2  = np.array([[1,    0,    1   ]])\n    print(\"Cost =\", compute_cost(AL2, Y2))  # e.g. 0.1711\n```"}
{"task_id": 552, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation uses the forward-backward algorithm and returns the\n    resulting vector rounded to four decimal places.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'A' and 'B'.\n                    - pi: initial state distribution, shape (N,)\n                    - A: state transition matrix, shape (N, N)\n                    - B: emission probability matrix, shape (N, M)\n        observations (list[int]): Sequence of observation indices (0-based).\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t,\n                     rounded to 4 decimal places. Empty list if t is invalid.\n    \"\"\"\n    # Number of time steps\n    T = len(observations)\n    # If t is out of bounds, return empty list\n    if t < 0 or t >= T:\n        return []\n    # Extract HMM parameters and ensure they are numpy arrays\n    pi = np.array(hmm['pi'], dtype=float)      # shape (N,)\n    A  = np.array(hmm['A'], dtype=float)       # shape (N, N)\n    B  = np.array(hmm['B'], dtype=float)       # shape (A.shape[0], M)\n    N  = pi.shape[0]                           # number of hidden states\n\n    # Forward pass: alpha[t, i] = P(o_0...o_t, q_t = i)\n    alpha = np.zeros((T, N), dtype=float)\n    # Initialization\n    o0 = observations[0]\n    alpha[0, :] = pi * B[:, o0]\n    # Induction\n    for time in range(1, T):\n        oi = observations[time]\n        # alpha[time, i] = sum_j alpha[time-1, j] * A[j, i] * B[i, oi]\n        alpha[time, :] = (alpha[time-1, :][:, None] * A).sum(axis=0) * B[:, oi]\n\n    # Backward pass: beta[t, i] = P(o_{t+1}...o_{T-1} | q_t = i)\n    beta = np.zeros((T, N), dtype=float)\n    # Initialization\n    beta[T-1, :] = 1.0\n    # Induction backwards\n    for time in range(T-2, -1, -1):\n        oi1 = observations[time + 1]\n        # beta[time, i] = sum_j A[i, j] * B[j, o_{time+1}] * beta[time+1, j]\n        beta[time, :] = (A * (B[:, oi1] * beta[time+1, :])[None, :]).sum(axis=1)\n\n    # Compute gamma at time t: \u03b3_t(i) = \u03b1_t(i) * \u03b2_t(i) / sum_j \u03b1_t(j) * \u03b2_t(j)\n    numerator = alpha[t, :] * beta[t, :]\n    denominator = numerator.sum()\n    gamma_t = numerator / denominator\n\n    # Round to 4 decimal places and return as list\n    return np.round(gamma_t, 4).tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a small HMM\n    hmm = {\n        'pi': [0.6, 0.4],\n        'A':  [[0.7, 0.3],\n               [0.4, 0.6]],\n        'B':  [[0.5, 0.5],\n               [0.1, 0.9]]\n    }\n    # Observation sequence (indices into the emission matrix)\n    observations = [0, 1, 0]\n    # Compute \u03b3_1 (time step 1)\n    gamma_at_1 = gamma_probabilities(hmm, observations, 1)\n    print(\"\u03b3_1:\", gamma_at_1)\n```"}
{"task_id": 555, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample.  Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    n_samples = data.shape[0]\n\n    # 1. Compute pairwise Euclidean distances\n    #    We build a (n_samples x n_samples) distance matrix.\n    diff = data[:, np.newaxis, :] - data[np.newaxis, :, :]\n    dist_matrix = np.linalg.norm(diff, axis=2)\n\n    # 2. Build neighborhood list for each sample\n    neighbors = [list(np.where(dist_matrix[i] <= eps)[0]) for i in range(n_samples)]\n\n    # 3. Identify core points\n    is_core = [len(neighbors[i]) >= min_samples for i in range(n_samples)]\n\n    # Labels: 0 = unvisited, 1,2,... = cluster ids, -1 = noise (assigned at end)\n    labels = [0] * n_samples\n    cluster_id = 0\n\n    # 4. Expand clusters from each core point\n    for i in range(n_samples):\n        if labels[i] != 0:\n            # Already processed (either assigned to a cluster or marked noise later)\n            continue\n        if not is_core[i]:\n            # Not a core point => leave it unlabelled for now\n            continue\n\n        # Found a new cluster\n        cluster_id += 1\n        labels[i] = cluster_id\n        queue = deque([i])\n\n        # BFS to find all density-reachable points\n        while queue:\n            point = queue.popleft()\n            for nbr in neighbors[point]:\n                if labels[nbr] == 0:\n                    # Assign to current cluster\n                    labels[nbr] = cluster_id\n                    # If this neighbor is also a core point, expand from it\n                    if is_core[nbr]:\n                        queue.append(nbr)\n\n    # 5. Label all remaining unvisited points as noise\n    for idx in range(n_samples):\n        if labels[idx] == 0:\n            labels[idx] = -1\n\n    return labels\n\n# Example usage\nif __name__ == \"__main__\":\n    # A small sample dataset\n    data = np.array([\n        [1.0, 2.0], [1.1, 2.1], [0.9, 1.8],\n        [8.0, 8.0], [8.1, 7.9], [7.9, 8.2],\n        [50.0, 50.0]\n    ])\n    eps = 0.5\n    min_samples = 3\n\n    labels = dbscan(data, eps, min_samples)\n    print(\"Data points:\")\n    print(data)\n    print(\"Cluster labels:\")\n    print(labels)\n```"}
{"task_id": 561, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef gmm_em(data: np.ndarray, k: int, max_iter: int = 200, epsilon: float = 1e-4) -> list[int]:\n    \"\"\"\n    Fit a K-component Gaussian Mixture Model to `data` using the EM algorithm,\n    then assign each sample to the component with the highest posterior.\n    The returned labels are re\u2010indexed so that components are sorted by the\n    first coordinate of their mean in ascending order.\n\n    Parameters:\n    - data: np.ndarray of shape (N, D), N samples with D features\n    - k: number of mixture components\n    - max_iter: maximum number of EM iterations\n    - epsilon: convergence tolerance on mixture weights (also used for covariance regularization)\n\n    Returns:\n    - List of length N with integer labels in 0..k-1\n    \"\"\"\n    N, D = data.shape\n\n    # trivial case\n    if k == 1:\n        return [0] * N\n\n    # RNG for reproducibility within a run\n    rng = np.random.default_rng()\n\n    # Initialize mixture weights \u03c0_i\n    pi = rng.random(k)\n    pi /= pi.sum()\n\n    # Initialize means by choosing k distinct random samples\n    init_idxs = rng.choice(N, size=k, replace=False)\n    mu = data[init_idxs, :].copy()  # shape (k, D)\n\n    # Initialize covariances to the data covariance\n    base_cov = np.cov(data, rowvar=False) + epsilon * np.eye(D)\n    cov = np.array([base_cov.copy() for _ in range(k)])  # shape (k, D, D)\n\n    # EM loop\n    for iteration in range(max_iter):\n        # E-step: compute responsibilities gamma[n,i]\n        resp = np.zeros((N, k))\n        for i in range(k):\n            diff = data - mu[i]  # (N, D)\n            inv_cov = np.linalg.inv(cov[i])\n            det_cov = np.linalg.det(cov[i])\n            # exponent term of Gaussian\n            exponent = np.sum(diff @ inv_cov * diff, axis=1)  # (N,)\n            # Gaussian coefficient\n            denom = math.pow(2 * math.pi, D / 2) * math.sqrt(det_cov)\n            coeff = 1.0 / denom\n            resp[:, i] = pi[i] * coeff * np.exp(-0.5 * exponent)\n        # normalize over components\n        row_sums = resp.sum(axis=1, keepdims=True)\n        resp /= row_sums\n\n        # M-step: update parameters\n        Nk = resp.sum(axis=0)               # shape (k,)\n        pi_new = Nk / N                     # new mixture weights\n\n        # Check convergence on mixture weights\n        if np.max(np.abs(pi_new - pi)) < epsilon:\n            pi = pi_new\n            break\n        pi = pi_new\n\n        # update means\n        mu = (resp.T @ data) / Nk[:, None]  # shape (k, D)\n\n        # update covariances\n        for i in range(k):\n            diff = data - mu[i]             # (N, D)\n            wc = resp[:, i][:, None] * diff # (N, D)\n            cov[i] = (wc.T @ diff) / Nk[i] + epsilon * np.eye(D)\n\n    # Final assignment: compute responsibilities one last time\n    resp = np.zeros((N, k))\n    for i in range(k):\n        diff = data - mu[i]\n        inv_cov = np.linalg.inv(cov[i])\n        det_cov = np.linalg.det(cov[i])\n        exponent = np.sum(diff @ inv_cov * diff, axis=1)\n        denom = math.pow(2 * math.pi, D / 2) * math.sqrt(det_cov)\n        coeff = 1.0 / denom\n        resp[:, i] = pi[i] * coeff * np.exp(-0.5 * exponent)\n    hard_labels = np.argmax(resp, axis=1)  # shape (N,)\n\n    # Sort components by the first coordinate of their means\n    order = np.argsort(mu[:, 0])  # old indices in ascending order\n    # Build a remapping: old index -> new label\n    remap = np.empty(k, dtype=int)\n    for new_label, old_index in enumerate(order):\n        remap[old_index] = new_label\n\n    # Apply remapping to the hard labels\n    final_labels = [int(remap[l]) for l in hard_labels]\n    return final_labels\n```"}
{"task_id": 562, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(data: np.ndarray,\n                        n_cluster: int,\n                        gamma: float = 2.0,\n                        method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A list of length N where the i-th element is an integer label identifying the\n        cluster assignment of the i-th sample (labels are 0-based).\n    \"\"\"\n    N, d = data.shape\n    if n_cluster == 1:\n        return [0] * N\n\n    # 1. Build fully-connected similarity graph with Gaussian kernel\n    #    compute pairwise squared Euclidean distances\n    #    shape (N, N)\n    diff = data[:, np.newaxis, :] - data[np.newaxis, :, :]\n    sq_dists = np.sum(diff * diff, axis=2)\n    W = np.exp(-gamma * sq_dists)\n\n    # 2. Construct Laplacian\n    degs = np.sum(W, axis=1)\n    if method == 'unnormalized':\n        D = np.diag(degs)\n        L = D - W\n    elif method == 'normalized':\n        # symmetric normalized Laplacian: L_sym = I - D^{-1/2} W D^{-1/2}\n        # build D^{-1/2}\n        inv_sqrt_degs = 1.0 / np.sqrt(degs)\n        inv_sqrt_degs[np.isinf(inv_sqrt_degs)] = 0.0\n        D_inv_sqrt = np.diag(inv_sqrt_degs)\n        L = np.eye(N) - D_inv_sqrt @ W @ D_inv_sqrt\n    else:\n        raise ValueError(\"method must be 'unnormalized' or 'normalized'\")\n\n    # 3. Compute the first k eigenvectors of L\n    # Since L is symmetric, use eigh\n    eigen_vals, eigen_vecs = np.linalg.eigh(L)\n    # pick the indices of the smallest n_cluster eigenvalues\n    idx = np.argsort(eigen_vals)[:n_cluster]\n    H = eigen_vecs[:, idx]  # shape (N, n_cluster)\n\n    # If normalized, row-normalise H\n    if method == 'normalized':\n        norms = np.linalg.norm(H, axis=1, keepdims=True)\n        # avoid division by zero\n        norms[norms == 0] = 1.0\n        H = H / norms\n\n    # 4. Run deterministic k-means on rows of H\n    #    initialize centroids to first n_cluster rows of H\n    centroids = H[:n_cluster, :].copy()\n    labels = np.zeros(N, dtype=int)\n    max_iter = 100\n    for _ in range(max_iter):\n        # assign labels\n        # compute squared distances to centroids\n        # shape distances: (N, n_cluster)\n        dists = np.sum((H[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # update centroids\n        for j in range(n_cluster):\n            members = H[labels == j]\n            if len(members) > 0:\n                centroids[j] = np.mean(members, axis=0)\n            # if a cluster loses all points, we keep the old centroid\n\n    # 5. Relabel clusters so that the cluster containing the smallest original index is 0, next is 1, ...\n    label_map = {}\n    next_label = 0\n    final_labels = np.zeros(N, dtype=int)\n    for i in range(N):\n        c = labels[i]\n        if c not in label_map:\n            label_map[c] = next_label\n            next_label += 1\n        final_labels[i] = label_map[c]\n\n    return final_labels.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # generate toy data\n    from sklearn.datasets import make_blobs\n    X, _ = make_blobs(centers=3, n_samples=150, random_state=42)\n    labels = spectral_clustering(X, n_cluster=3, gamma=1.0, method='normalized')\n    print(labels)\n```"}
{"task_id": 563, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data:   2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data:   1-D NumPy array of length N with labels \u22121 or 1.\n        eta:      Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    N, d = X_data.shape\n\n    # Initialize dual coefficients and bias\n    alpha = np.zeros(N, dtype=float)\n    b = 0.0\n\n    # Precompute the Gram matrix G[i, j] = X_i \u00b7 X_j\n    G = X_data.dot(X_data.T)\n\n    # Perceptron in dual form\n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            # Compute prediction score in dual form: g = sum_j alpha_j y_j G[j, i]\n            g = np.dot(alpha * y_data, G[:, i])\n\n            # Perceptron update condition\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n\n        if not updated:\n            # No updates in a full pass \u2192 converged\n            break\n\n    # Recover the primal weight vector: w = sum_i alpha_i y_i x_i\n    # Equivalent to w = X_data.T @ (alpha * y_data)\n    coeff = alpha * y_data  # shape (N,)\n    w = coeff @ X_data      # shape (d,)\n\n    # Round to 4 decimal places\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n\n    return w_rounded, b_rounded\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple OR dataset\n    X = np.array([[0, 0],\n                  [0, 1],\n                  [1, 0],\n                  [1, 1]], dtype=float)\n    y = np.array([-1, 1, 1, 1], dtype=int)\n\n    w, b = perceptron_dual_train(X, y, eta=1.0, max_iter=100)\n    print(\"Learned w:\", w)\n    print(\"Learned b:\", b)\n```"}
{"task_id": 564, "completion_id": 0, "completion": "```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    node = root\n    # Traverse until we hit a leaf (which stores `y`)\n    while not hasattr(node, 'y'):\n        feat_val = features[node.label]\n        if task_type == 'classification':\n            # Try to find a child whose x matches the feature value\n            next_child = None\n            for child in node.child:\n                if child.x == feat_val:\n                    next_child = child\n                    break\n            # If none matches, default to the second child\n            if next_child is None:\n                next_child = node.child[1]\n            node = next_child\n\n        elif task_type == 'regression':\n            # Compare against split-point s\n            if feat_val <= node.s:\n                node = node.child[0]\n            else:\n                node = node.child[1]\n\n        else:\n            raise ValueError(f\"Unknown task_type: {task_type}\")\n\n    # We've reached a leaf; return its stored prediction\n    return node.y\n```"}
{"task_id": 565, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # Helper: compute entropy of a 1-D array of class labels\n    def entropy(labels: np.ndarray) -> float:\n        if labels.size == 0:\n            return 0.0\n        counts = np.bincount(labels)\n        probs = counts[counts > 0] / labels.size\n        return -np.sum(probs * np.log2(probs))\n\n    # Compute base entropy H(y)\n    base_entropy = entropy(y)\n\n    best_feat = -1\n    best_ig = -1.0\n\n    # Iterate over each feature\n    for j in range(n_features):\n        feature_vals = X[:, j]\n        unique_vals = np.unique(feature_vals)\n\n        # Compute conditional entropy H(y|A_j)\n        cond_entropy = 0.0\n        for v in unique_vals:\n            mask = (feature_vals == v)\n            subset_y = y[mask]\n            p_v = subset_y.size / n_samples\n            cond_entropy += p_v * entropy(subset_y)\n\n        # Information gain\n        ig = base_entropy - cond_entropy\n\n        # Update best feature (ties broken by smaller index)\n        if ig > best_ig + 1e-12:  # tiny epsilon for numerical stability\n            best_ig = ig\n            best_feat = j\n\n    # Round the best information gain to 6 decimal places\n    best_ig = round(best_ig, 6)\n    return best_feat, best_ig\n```"}
{"task_id": 566, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation.\n\n    Args:\n        data: 2-D NumPy array where each row is an observation and each column a feature.\n        n_trees: Number of random isolation trees that will be grown.\n        sample_size: Number of samples drawn (without replacement) to grow every tree.\n        epsilon: Fraction (0 \u2264 \u03b5 < 1) of observations that are considered outliers.\n\n    Returns:\n        A list with the zero-based indices of the detected outliers, sorted increasingly.\n    \"\"\"\n    np.random.seed(42)\n    n_samples, n_features = data.shape\n\n    # If epsilon is zero, no outliers\n    if epsilon <= 0:\n        return []\n\n    # Compute c(n) for leaf correction\n    def c(n: int) -> float:\n        if n <= 1:\n            return 0.0\n        return 2.0 * math.log(n - 1) + 0.5772156649 - 2.0 * (n - 1) / n\n\n    # Height limit for tree growth\n    height_limit = math.ceil(math.log2(sample_size)) if sample_size > 0 else 0\n\n    # Build one isolation tree on a sample of the data\n    def build_tree(idxs: np.ndarray, current_depth: int):\n        size = idxs.size\n        # Stopping conditions\n        if current_depth >= height_limit or size <= 1:\n            return {'leaf': True, 'size': size}\n        # Select a random feature\n        f = np.random.randint(0, n_features)\n        feat_vals = data[idxs, f]\n        lo, hi = feat_vals.min(), feat_vals.max()\n        # If no split is possible, make leaf\n        if lo == hi:\n            return {'leaf': True, 'size': size}\n        # Random threshold in [lo, hi)\n        t = np.random.uniform(lo, hi)\n        # Partition\n        left_mask = feat_vals < t\n        right_mask = ~left_mask\n        if not np.any(left_mask) or not np.any(right_mask):\n            return {'leaf': True, 'size': size}\n        left_idxs = idxs[left_mask]\n        right_idxs = idxs[right_mask]\n        # Recurse\n        left = build_tree(left_idxs, current_depth + 1)\n        right = build_tree(right_idxs, current_depth + 1)\n        return {'leaf': False, 'feature': f, 'threshold': t,\n                'left': left, 'right': right, 'size': size}\n\n    # Compute path length of a single point x in a tree\n    def path_length(x: np.ndarray, node: dict, depth: int) -> float:\n        if node['leaf']:\n            return depth + c(node['size'])\n        # Internal node\n        if x[node['feature']] < node['threshold']:\n            return path_length(x, node['left'], depth + 1)\n        else:\n            return path_length(x, node['right'], depth + 1)\n\n    # Build all trees\n    trees = []\n    all_indices = np.arange(n_samples)\n    for _ in range(n_trees):\n        if sample_size >= n_samples:\n            samp_idxs = all_indices.copy()\n        else:\n            samp_idxs = np.random.choice(all_indices, size=sample_size, replace=False)\n        tree = build_tree(samp_idxs, 0)\n        trees.append(tree)\n\n    # Compute average path lengths for all samples\n    avg_path_lengths = np.zeros(n_samples, dtype=float)\n    for i in range(n_samples):\n        x = data[i]\n        pl_sum = 0.0\n        for tree in trees:\n            pl_sum += path_length(x, tree, 0)\n        avg_path_lengths[i] = pl_sum / n_trees\n\n    # Compute phi for score normalization\n    # \u03c6 = 2\u00b7ln(n\u22121) \u2212 2\u00b7(n\u22121)/n\n    if n_samples > 1:\n        phi = 2.0 * math.log(n_samples - 1) - 2.0 * (n_samples - 1) / n_samples\n    else:\n        phi = 1.0  # avoid division by zero; trivial case\n\n    # Anomaly scores: s(x) = 2^{- (h_avg / \u03c6)}\n    scores = 2.0 ** (-(avg_path_lengths / phi))\n\n    # Determine number of outliers\n    k = int(math.floor(epsilon * n_samples))\n    if k <= 0:\n        return []\n\n    # Select top-k highest scores\n    # argsort(-scores) gives indices of descending scores\n    topk_desc = np.argsort(-scores)[:k]\n    # Return sorted indices\n    outliers = sorted(topk_desc.tolist())\n    return outliers\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate sample data\n    rng = np.random.RandomState(0)\n    X = np.vstack([\n        rng.normal(0, 1, size=(100, 2)),\n        rng.normal(10, 1, size=(5, 2))\n    ])\n    # Detect top 5% as outliers\n    out = detect_outliers(X, n_trees=50, sample_size=64, epsilon=0.05)\n    print(\"Outlier indices:\", out)\n```"}
{"task_id": 568, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"\n    Train a Maximum Entropy classifier using Generalized Iterative Scaling (GIS)\n    on categorical data and return predicted labels for X_test.\n    \"\"\"\n    # Number of training samples and feature dimensions\n    N, M = X_train.shape\n\n    # Unique class labels\n    classes = sorted(set(y_train.tolist()))\n    K = len(classes)\n    # Map class label to index\n    class2idx = {y: i for i, y in enumerate(classes)}\n\n    # Build feature-to-index mapping:\n    # For each feature index i, each value v seen in training, each class y\n    feat_map = {}   # (i, v, y) -> j\n    feat_i = []\n    feat_v = []\n    feat_y = []\n    j = 0\n    for i in range(M):\n        vals = set(X_train[:, i].tolist())\n        for v in vals:\n            for y in classes:\n                feat_map[(i, v, y)] = j\n                feat_i.append(i)\n                feat_v.append(v)\n                feat_y.append(y)\n                j += 1\n    J = j  # total number of feature functions\n\n    # Empirical expectations: E_emp[j] = (1/N) * count of times f_j(x_t, y_t)=1\n    E_emp = np.zeros(J, dtype=float)\n    for t in range(N):\n        x_t = X_train[t]\n        y_t = y_train[t]\n        for i in range(M):\n            v = x_t[i]\n            j = feat_map[(i, v, y_t)]\n            E_emp[j] += 1.0\n    E_emp /= N\n\n    # GIS constant: maximum number of active features per (x, y)\n    # Here each (x,y) activates exactly M features (one per feature dimension)\n    C = float(M)\n\n    # Initialize weights\n    w = np.zeros(J, dtype=float)\n\n    # GIS iterations\n    for iteration in range(n_iter):\n        # Model expectations under current w\n        E_model = np.zeros(J, dtype=float)\n\n        # For each sample, compute p(y|x), then accumulate E_model\n        for t in range(N):\n            x_t = X_train[t]\n            # Compute score s_y = sum_i w[(i, x_t[i], y)]\n            s = np.zeros(K, dtype=float)\n            for yi, y in enumerate(classes):\n                total = 0.0\n                for i in range(M):\n                    v = x_t[i]\n                    j_idx = feat_map[(i, v, y)]\n                    total += w[j_idx]\n                s[yi] = total\n            # Partition function Z\n            max_s = s.max()\n            # To improve numerical stability\n            exp_s = np.exp(s - max_s)\n            Z = exp_s.sum()\n            # Probabilities p(y|x_t)\n            p_y = exp_s / Z\n\n            # Accumulate E_model for all feature functions active\n            # For each class y and each feature dimension i\n            for yi, y in enumerate(classes):\n                p = p_y[yi] / N\n                if p == 0.0:\n                    continue\n                for i in range(M):\n                    v = x_t[i]\n                    j_idx = feat_map[(i, v, y)]\n                    E_model[j_idx] += p\n\n        # Update weights: delta_j = (1/C) * log(E_emp_j / E_model_j)\n        delta = np.zeros(J, dtype=float)\n        for j in range(J):\n            if E_model[j] > 0:\n                delta[j] = (1.0 / C) * math.log(E_emp[j] / E_model[j])\n            else:\n                # If model expectation is zero, skip update (rare if careful)\n                delta[j] = 0.0\n        # Apply updates\n        w += delta\n\n        # Check convergence\n        max_update = np.max(np.abs(delta))\n        if max_update < epsilon:\n            break\n\n    # Predict on X_test\n    preds = []\n    for x in X_test:\n        # Compute score for each class\n        scores = []\n        for y in classes:\n            total = 0.0\n            for i in range(M):\n                v = x[i]\n                key = (i, v, y)\n                if key in feat_map:\n                    total += w[feat_map[key]]\n            scores.append(total)\n        # Pick class with highest score\n        best_idx = int(np.argmax(scores))\n        preds.append(classes[best_idx])\n\n    return preds\n\n# Example usage:\nif __name__ == \"__main__\":\n    # A tiny example with two features and two classes (0/1)\n    X_train = np.array([\n        [0, 'A'],\n        [1, 'B'],\n        [0, 'B'],\n        [1, 'A'],\n    ], dtype=object)\n    y_train = np.array([0, 1, 0, 1])\n    X_test = np.array([\n        [0, 'A'],\n        [1, 'B'],\n        [0, 'B'],\n        [1, 'A'],\n    ], dtype=object)\n\n    preds = maxent_predict(X_train, y_train, X_test)\n    print(\"Predictions:\", preds)\n```"}
{"task_id": 569, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio is rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                  ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of (word, ratio) pairs and (2) a set of the most\n        frequent words as described above.\n    \"\"\"\n    total_docs = len(data)\n    # If no documents, return empty structures\n    if total_docs == 0:\n        return [], set()\n\n    # Count in how many documents each word appears\n    doc_counter: Counter[str] = Counter()\n    for _, words in data:\n        unique_words = set(words)\n        doc_counter.update(unique_words)\n\n    # Compute ratios and round to 4 decimal places\n    ratios = {\n        word: round(count / total_docs, 4)\n        for word, count in doc_counter.items()\n    }\n\n    # Sort words by ratio (descending)\n    sorted_word_ratios: List[Tuple[str, float]] = sorted(\n        ratios.items(),\n        key=lambda item: item[1],\n        reverse=True\n    )\n\n    # Determine the set of top-k words\n    if cut_off is None:\n        top_words = {word for word, _ in sorted_word_ratios}\n    else:\n        # slicing handles cut_off = 0 or cut_off > len(sorted_word_ratios)\n        top_words = {word for word, _ in sorted_word_ratios[:cut_off]}\n\n    return sorted_word_ratios, top_words\n\n\n# Example usage\nif __name__ == \"__main__\":\n    docs = [\n        (\"doc1\", [\"apple\", \"banana\", \"apple\"]),\n        (\"doc2\", [\"banana\", \"cherry\"]),\n        (\"doc3\", [\"apple\", \"durian\", \"cherry\"]),\n    ]\n    freq_list, top_set = statistic_key_word(docs, cut_off=2)\n    print(\"Frequencies:\", freq_list)\n    print(\"Top 2 words:\", top_set)\n```"}
{"task_id": 571, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_svm_parameters(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: np.ndarray\n) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    # Compute weight vector: w = X^T (alpha * y)\n    w = X.T.dot(alpha * y)\n\n    # Compute bias: b = (1/n) sum_i (y_i - w^T x_i)\n    residuals = y - X.dot(w)\n    b = np.mean(residuals)\n\n    # Round to four decimal places\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n\n    return w_rounded, b_rounded\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Toy dataset\n    X = np.array([[2.0, 3.0],\n                  [1.0, 1.0],\n                  [2.0, 1.0],\n                  [3.0, 2.0]])\n    y = np.array([1, -1, -1, 1])\n    # Suppose these are the final alphas from SMO\n    alpha = np.array([0.5, 0.1, 0.0, 0.4])\n\n    w, b = compute_svm_parameters(X, y, alpha)\n    print(\"Weight vector:\", w)\n    print(\"Bias term:\", b)\n```"}
{"task_id": 572, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"\n    Compute the Local Outlier Factor (LOF) for each sample in `data`.\n    Return a tuple: (lof_scores, outlier_indices)\n      - lof_scores is a list of LOF scores rounded to 4 decimals\n      - outlier_indices is a sorted list of indices whose LOF > epsilon\n    \"\"\"\n    data = np.asarray(data, dtype=float)\n    n = data.shape[0]\n    if not (1 <= k < n):\n        raise ValueError(\"k must be \u22651 and < number of samples\")\n\n    # 1. Compute full pairwise Euclidean distance matrix\n    #    dist_matrix[i, j] = distance from sample i to sample j\n    diff = data[:, np.newaxis, :] - data[np.newaxis, :, :]    # shape (n, n, d)\n    dist_matrix = np.sqrt(np.sum(diff**2, axis=2))            # shape (n, n)\n\n    # Exclude self-distance by setting diagonal to infinity\n    np.fill_diagonal(dist_matrix, np.inf)\n\n    # 2. For each p, find its k nearest neighbors and the k-distance(p)\n    sorted_idx = np.argsort(dist_matrix, axis=1)   # each row sorted by distance\n    neighbors = sorted_idx[:, :k]                  # shape (n, k)\n    # k-distance(p) is the distance to the k-th nearest neighbor\n    k_distances = dist_matrix[np.arange(n), neighbors[:, -1]]  # shape (n,)\n\n    # 3. Reachability distance: reach-dist_k(p, q) = max(k-distance(q), dist(p, q))\n    # Build a matrix of reachability distances for each (p, each of its k neighbors)\n    reach_dist = np.maximum(\n        k_distances[neighbors],                          # k-distance(q) for each neighbor q\n        dist_matrix[np.arange(n)[:, None], neighbors]     # dist(p, q)\n    )  # shape (n, k)\n\n    # 4. Local Reachability Density: LRD_k(p) = k / sum_q reach-dist_k(p, q)\n    lrd = k / np.sum(reach_dist, axis=1)  # shape (n,)\n\n    # 5. Local Outlier Factor: LOF_k(p) = (sum_{q in N_k(p)} lrd[q] / k) / lrd[p]\n    lrd_sum_neighbors = np.sum(lrd[neighbors], axis=1)  # sum of neighbor LRDs\n    lof = (lrd_sum_neighbors / k) / lrd                  # shape (n,)\n\n    # Round scores to 4 decimal places\n    lof_rounded = np.round(lof, 4).tolist()\n\n    # Collect indices of samples whose LOF > epsilon\n    outlier_indices = [i for i, score in enumerate(lof_rounded) if score > epsilon]\n\n    return lof_rounded, outlier_indices\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create some sample 2D data\n    data = np.array([\n        [1.0, 1.0],\n        [1.0, 2.0],\n        [2.0, 1.0],\n        [2.0, 2.0],  # small cluster\n        [8.0, 8.0],  # another cluster\n        [100.0, 100.0]  # obvious outlier\n    ])\n    k = 2\n    epsilon = 1.5\n\n    scores, outliers = local_outlier_factor(data, k, epsilon)\n    print(\"LOF scores:\", scores)\n    print(\"Outlier indices:\", outliers)\n```"}
{"task_id": 574, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # containers\n    x_train_list = []\n    y_train_list = []\n    x_val_list = []\n    y_val_list = []\n\n    for c in range(m):\n        base = 10 * (c // 2)\n        low, high = base + 1, base + 9\n\n        # training points for class c\n        for _ in range(n_train):\n            x = random.uniform(low, high)\n            y = random.uniform(low, high)\n            x_train_list.append((x, y))\n            y_train_list.append(c)\n\n        # validation points for class c\n        for _ in range(n_val):\n            x = random.uniform(low, high)\n            y = random.uniform(low, high)\n            x_val_list.append((x, y))\n            y_val_list.append(c)\n\n    # Convert to NumPy arrays\n    X_train = np.array(x_train_list, dtype=float)\n    X_val   = np.array(x_val_list,   dtype=float)\n    y_train = np.array(y_train_list, dtype=int)\n    y_val   = np.array(y_val_list,   dtype=int)\n\n    # Round coordinates to 4 decimal places\n    X_train = np.round(X_train, 4)\n    X_val   = np.round(X_val,   4)\n\n    return X_train, X_val, y_train, y_val\n```\n\nExplanation of key steps:\n- We seed Python\u2019s built-in `random` module (not NumPy\u2019s) so that repeated calls with the same seed yield identical outputs.\n- For each class `c`, we compute `base = 10 * (c // 2)` and sample uniformly on `[base+1, base+9]` for both x and y.\n- We collect training and validation points separately, then stack them into NumPy arrays.\n- Finally, we round all coordinates to 4 decimal places to meet the output constraint."}
{"task_id": 579, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres. Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached. The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    m, d = X.shape\n    # Initialize centres to the first k points\n    centres = X[:k].astype(float).copy()\n    # Placeholder for labels\n    labels = np.full(m, -1, dtype=int)\n\n    for _ in range(max_iters):\n        # Assignment step: compute distances and assign labels\n        # distances shape: (m, k)\n        diff = X[:, np.newaxis, :] - centres[np.newaxis, :, :]  # (m, k, d)\n        dist_sq = np.sum(diff * diff, axis=2)  # squared distances\n        new_labels = np.argmin(dist_sq, axis=1)  # ties -> smaller index automatically\n\n        # Check for convergence\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # Update step: recompute centres\n        for j in range(k):\n            points_in_cluster = X[labels == j]\n            if points_in_cluster.size:\n                centres[j] = points_in_cluster.mean(axis=0)\n            # else: no change if cluster is empty\n\n    # Round centres to 4 decimal places\n    centres_rounded = np.round(centres, 4)\n    return labels.tolist(), centres_rounded.tolist()\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create some sample data\n    X = np.array([\n        [1.0, 2.0],\n        [1.5, 1.8],\n        [5.0, 8.0],\n        [8.0, 8.0],\n        [1.0, 0.6],\n        [9.0, 11.0]\n    ])\n    k = 2\n    labels, centres = kmeans(X, k)\n    print(\"Labels:\", labels)\n    print(\"Centres:\", centres)\n```"}
{"task_id": 581, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation using batch gradient descent.\"\"\"\n    m, n = X.shape\n    # initialize parameters\n    w = np.zeros(n, dtype=float)\n    b = 0.0\n\n    # edge case: no training if learning rate or iterations is zero\n    if learning_rate == 0 or iterations == 0:\n        return [0.0] * n, 0.0\n\n    for _ in range(iterations):\n        # predictions and residuals\n        y_hat = X.dot(w) + b\n        r = y - y_hat  # residuals\n\n        # gradient wrt weights\n        # sign(w_j): +1 if w_j > 0, -1 if w_j <= 0\n        sign_w = np.where(w > 0, 1.0, -1.0)\n        grad_w = (-2 * X.T.dot(r) + l1_penalty * sign_w + 2 * l2_penalty * w) / m\n\n        # gradient wrt bias\n        grad_b = (-2 * np.sum(r)) / m\n\n        # parameter updates\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n    # round to four decimals\n    w_rounded = [round(val, 4) for val in w]\n    b_rounded = round(b, 4)\n\n    return w_rounded, b_rounded\n```"}
{"task_id": 591, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Perform a forward pass through a fully-connected ReLU neural network.\n\n    The network topology and parameters are encoded in *weights* where:\n      \u2022 *weights[L]* \u2013 weight matrix of layer L,\n      \u2022 each *weights[L][i]* \u2013 list of weights of neuron i of layer L,\n      \u2022 the first weight of every neuron is its bias weight.\n\n    All hidden layers use ReLU, the last layer is linear.\n\n    Args:\n        weights: A three-level nested list containing the network\u2019s weights.\n        x:       Input feature vector (without the bias term).\n\n    Returns:\n        The prediction rounded to four decimals (float if the output layer has\n        one neuron, otherwise list of floats).\n    \"\"\"\n    # Convert input to numpy array\n    a = np.array(x, dtype=float)\n\n    # Iterate through each layer\n    for L, layer in enumerate(weights):\n        # Prepare the augmented activation vector (prepend bias input = 1)\n        a_aug = np.concatenate((np.array([1.0]), a))\n        # Convert layer weights to numpy array: shape (n_neurons, prev_layer_size+1)\n        W = np.array(layer, dtype=float)\n        # Compute pre-activation\n        z = W.dot(a_aug)\n        # Apply ReLU if not the last layer\n        if L < len(weights) - 1:\n            a = np.maximum(z, 0.0)\n        else:\n            # Last layer is linear\n            a = z\n\n    # Round the outputs to 4 decimal places\n    a_rounded = np.round(a, 4)\n\n    # Return scalar if only one output neuron, else list\n    if a_rounded.size == 1:\n        return float(a_rounded[0])\n    else:\n        return [float(val) for val in a_rounded]\n\n# Example usage:\nif __name__ == \"__main__\":\n    # A small 2-layer network: 2 inputs -> 3 hidden units -> 1 output\n    example_weights = [\n        # Layer 0: 3 neurons, each has 1 bias + 2 input weights\n        [[0.1, 0.2, -0.1],\n         [0.0, 0.1,  0.1],\n         [-0.1, 0.05, 0.05]],\n        # Layer 1: 1 neuron, has 1 bias + 3 hidden weights\n        [[0.05, 0.2, -0.2, 0.1]]\n    ]\n    x_input = [1.0, -1.5]\n    print(neural_net_predict(example_weights, x_input))  # Example prediction\n```"}
{"task_id": 595, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"\n    Compute k centroids using the K-Means clustering algorithm from scratch.\n\n    Parameters:\n    - X: np.ndarray of shape (n_samples, n_features)\n    - k: int, number of clusters\n    - max_iters: int, maximum number of iterations (default 100)\n\n    Returns:\n    - List of k centroids (each a list of floats), sorted lexicographically,\n      with each coordinate rounded to 4 decimal places.\n    \"\"\"\n    # Convert to float64 for numeric stability\n    X = np.asarray(X, dtype=float)\n    n_samples, n_features = X.shape\n    tol = 1e-4\n\n    # 1. Initialize centroids as the first k samples\n    centroids = X[:k].copy()\n\n    for iteration in range(max_iters):\n        # 2a. Assign points to nearest centroid (squared Euclidean distance)\n        # distances shape: (n_samples, k)\n        diffs = X[:, np.newaxis, :] - centroids[np.newaxis, :, :]\n        dists = np.sum(diffs**2, axis=2)\n        labels = np.argmin(dists, axis=1)\n\n        # 2b. Update centroids\n        new_centroids = centroids.copy()\n        for j in range(k):\n            points_in_cluster = X[labels == j]\n            if points_in_cluster.size > 0:\n                new_centroids[j] = points_in_cluster.mean(axis=0)\n            # else: leave new_centroids[j] unchanged\n\n        # 3. Check for convergence\n        shifts = np.linalg.norm(new_centroids - centroids, axis=1)\n        centroids = new_centroids\n        if np.all(shifts < tol):\n            break\n\n    # 5. Sort centroids lexicographically and round to 4 decimals\n    cent_list = centroids.tolist()\n    cent_list.sort()  # lex order on lists\n    cent_list = [\n        [round(coord, 4) for coord in centroid]\n        for centroid in cent_list\n    ]\n    return cent_list\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data: 2D points\n    X_sample = np.array([\n        [1.0, 2.0],\n        [1.5, 1.8],\n        [5.0, 8.0],\n        [8.0, 8.0],\n        [1.0, 0.6],\n        [9.0, 11.0]\n    ])\n    k = 2\n    centroids = kmeans_centroids(X_sample, k)\n    print(\"Computed centroids:\")\n    for c in centroids:\n        print(c)\n```"}
