{"task_id": 0, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n       Fit an L2\u2010regularized (Ridge) linear regression model in closed form.\n    \n    Parameters\n    ----------\n    data : np.ndarray\n        A 2D array of shape (n_samples, n_features + 1) where the last column\n        is the target vector y and the other columns are the features X.\n    alpha : float, default=0.0\n        Regularization strength (>= 0). When alpha=0 this is ordinary least squares.\n        \n    Returns\n    -------\n    List[float] or int\n        The fitted weight vector (including intercept as the first entry),\n        with each coefficient rounded to 4 decimal places, or -1 if the\n        regularized normal matrix is singular (non-invertible).\n       # Split into features X and target y\n    X = data[:, :-1]\n    y = data[:,  -1]\n    \n    # Number of samples\n    n_samples = X.shape[0]\n    \n    # Add intercept (bias) column of ones\n    X_design = np.hstack((np.ones((n_samples, 1)), X))\n    \n    # Build the regularized normal matrix\n    # I has same size as number of columns in X_design\n    I = np.eye(X_design.shape[1])\n    M = X_design.T @ X_design + alpha * I\n    \n    # Check for invertibility via determinant\n    det = np.linalg.det(M)\n    if abs(det) < 1e-12:\n        return -1\n    \n    # Solve for weights: w = (X^T X + \u03b1 I)^{-1} X^T y\n    # More stable to use solve than explicit inv:\n    w = np.linalg.solve(M, X_design.T @ y)\n    \n    # Round each coefficient to 4 decimal places and return as list\n    w_rounded = [round(float(coef), 4) for coef in w]\n    return w_rounded\n```"}
{"task_id": 1, "completion_id": 0, "completion": "Here\u2019s an implementation of the dual\u2010form perceptron as specified. It precomputes the Gram matrix, maintains the \u03b1\u2019s and bias b, and loops until an entire pass yields no updates or until the maximum number of updates is reached. Finally it reconstructs the primal weight vector and rounds everything to four decimals.\n\n```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or \u20111) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of updates, by default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        The weight vector (as a list) and the bias; both rounded to 4 decimals.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n\n    # Precompute the Gram matrix\n    K = X_train @ X_train.T  # shape (n_samples, n_samples)\n\n    # Initialize dual variables and bias\n    alpha = np.zeros(n_samples, dtype=float)\n    b = 0.0\n\n    updates = 0\n    i = 0\n\n    # Main loop: scan until full pass without update or too many updates\n    while i < n_samples and updates < n_iter:\n        # Compute activation for sample i:\n        # activation = sum_j alpha_j * y_j * K[j, i]\n        activation = np.dot(alpha * y_train, K[:, i])\n        margin = y_train[i] * (activation + b)\n\n        if margin <= 0:\n            # mis\u2013classification \u2192 update dual parameter and bias\n            alpha[i] += eta\n            b += eta * y_train[i]\n            updates += 1\n            # restart scan\n            i = 0\n        else:\n            # correct classification \u2192 move to next sample\n            i += 1\n\n    # Reconstruct primal weight vector: w = sum_i alpha_i * y_i * x_i\n    w = (alpha * y_train) @ X_train  # shape (n_features,)\n\n    # Round to 4 decimals\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n\n    return w_rounded, b_rounded\n```\n\nExplanation of key steps:\n\n1. **Gram matrix** `K` holds all dot-products `\u27e8x\u1d62, x\u2c7c\u27e9`.  \n2. We keep the dual coefficients `\u03b1` and bias `b`.  \n3. We iterate over samples, computing  \n      activation\u1d62 = \u2211\u2c7c \u03b1\u2c7c y\u2c7c K(x\u2c7c, x\u1d62).  \n   If `y\u1d62 (activation\u1d62 + b) \u2264 0`, we update  \n      \u03b1\u1d62 \u2190 \u03b1\u1d62 + \u03b7,   b \u2190 b + \u03b7 y\u1d62,  \n   then restart the pass.  \n4. We stop when a full pass has no updates (i.e. linearly separable data) or when `updates` reaches `n_iter`.  \n5. Finally we convert back to the primal form `w = \u03a3\u1d62 \u03b1\u1d62 y\u1d62 x\u1d62` and round both `w` and `b` to four decimal places."}
{"task_id": 2, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (\"identity\", \"log\", \"logit\") where each entry is a\n        dictionary containing callables for the link, its inverse, and its\n        derivative with respect to \u03bc. All functions work with numpy scalars\n        as well as 1-D/2-D numpy arrays via element\u2010wise operations.\n    \"\"\"\n    # identity link: g(\u03bc) = \u03bc\n    def identity_link(mu):\n        return mu\n\n    def identity_inv(eta):\n        return eta\n\n    def identity_deriv(mu):\n        # derivative of \u03bc wrt \u03bc is 1\n        return np.ones_like(mu)\n\n    # log link: g(\u03bc) = log(\u03bc)\n    def log_link(mu):\n        return np.log(mu)\n\n    def log_inv(eta):\n        return np.exp(eta)\n\n    def log_deriv(mu):\n        # derivative of log(\u03bc) wrt \u03bc is 1/\u03bc\n        return 1.0 / mu\n\n    # logit link: g(\u03bc) = log(\u03bc / (1 - \u03bc))\n    def logit_link(mu):\n        return np.log(mu / (1.0 - mu))\n\n    def logit_inv(eta):\n        # stable sigmoid: 1 / (1 + exp(-\u03b7))\n        return 1.0 / (1.0 + np.exp(-eta))\n\n    def logit_deriv(mu):\n        # derivative of logit(\u03bc) wrt \u03bc: 1 / [\u03bc (1-\u03bc)]\n        return 1.0 / (mu * (1.0 - mu))\n\n    return {\n        \"identity\": {\n            \"link\": identity_link,\n            \"inv_link\": identity_inv,\n            \"derivative\": identity_deriv\n        },\n        \"log\": {\n            \"link\": log_link,\n            \"inv_link\": log_inv,\n            \"derivative\": log_deriv\n        },\n        \"logit\": {\n            \"link\": logit_link,\n            \"inv_link\": logit_inv,\n            \"derivative\": logit_deriv\n        }\n    }\n\n# Example usage:\nif __name__ == \"__main__\":\n    links = glm_links()\n    mu = np.array([0.2, 0.8])\n    eta_logit = links[\"logit\"][\"link\"](mu)\n    mu2_logit = links[\"logit\"][\"inv_link\"](eta_logit)\n    deriv_logit = links[\"logit\"][\"derivative\"](mu)\n\n    # Round to four decimal places for display\n    print(\"logit link(mu):           \", np.round(eta_logit, 4))\n    # \u2192 [-1.3863  1.3863]\n    print(\"inverse logit(link(mu)):  \", np.round(mu2_logit, 4))\n    # \u2192 [0.2 0.8]\n    print(\"derivative of logit(mu):  \", np.round(deriv_logit, 4))\n    # \u2192 [1.25    1.25  ]\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n   Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed is set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length n_samples where the i-th element is the number of\n        edges from the root to the leaf that contains the i-th sample.\n       # Make randomness reproducible\n    np.random.seed(0)\n\n    n_samples, n_features = data.shape\n    # This will hold the depth of the leaf for each sample index\n    depths = [None] * n_samples\n\n    def recurse(indices: list[int], depth: int):\n        # If fewer than 3 samples, this node is a leaf\n        if len(indices) < 3:\n            for i in indices:\n                depths[i] = depth\n            return\n\n        # Otherwise pick a random feature\n        f = np.random.randint(0, n_features)\n        feature_values = data[indices, f]\n        down = float(feature_values.min())\n        up   = float(feature_values.max())\n        # Pick a split threshold uniformly in [down, up]\n        v = np.random.uniform(down, up)\n\n        # Partition indices into left and right child\n        left_indices  = [i for i in indices if data[i, f] <= v]\n        right_indices = [i for i in indices if data[i, f] >  v]\n\n        # Recurse on non-empty branches at depth+1\n        if left_indices:\n            recurse(left_indices, depth + 1)\n        if right_indices:\n            recurse(right_indices, depth + 1)\n\n    # Build the tree starting from all samples at depth 0\n    recurse(list(range(n_samples)), depth=0)\n\n    return depths\n```\n\nExplanation of key steps:\n1. We seed the NumPy RNG inside the function with `np.random.seed(0)` so every call is reproducible.\n2. We maintain a `depths` list indexed by the original sample index.\n3. The recursive helper `recurse(indices, depth)`:\n   - If fewer than 3 samples, it\u2019s a leaf: we assign the current `depth` to all those sample indices.\n   - Otherwise we choose a random feature `f`, find its min/max over the current samples, draw a threshold `v` uniformly in `[down, up]`, and split into left/right children.\n   - We recurse on each nonempty child with `depth+1`.\n4. Finally we return the collected `depths` list."}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    # Number of transactions\n    N = len(transactions)\n    if N == 0:\n        return {}\n\n    # Absolute support threshold\n    threshold = math.ceil(minsup * N)\n    if threshold <= 0:\n        return {}\n\n    # Convert each transaction to a set (to ignore duplicate items in the same transaction)\n    txn_sets = [set(t) for t in transactions]\n\n    # 1) Find frequent 1-itemsets\n    item_counts = Counter()\n    for tset in txn_sets:\n        item_counts.update(tset)\n\n    # Keep only those meeting the threshold\n    L1 = { (item,): cnt for item, cnt in item_counts.items() if cnt >= threshold }\n    if not L1:\n        return {}\n\n    # This will collect all frequent itemsets of all sizes\n    frequent_itemsets: Dict[Tuple[Hashable, ...], int] = {}\n    frequent_itemsets.update(L1)\n\n    # Prepare for the iterative apriori steps\n    # Lk_minus_1: dict of frequent (k-1)-itemsets to their counts\n    Lk_minus_1 = L1\n    k = 2\n\n    while True:\n        # 2) Generate candidate k-itemsets from Lk_minus_1 (the join step)\n        prev_itemsets = sorted(Lk_minus_1.keys())  # list of tuples\n        Ck = set()\n        len_prev = len(prev_itemsets)\n        for i in range(len_prev):\n            for j in range(i + 1, len_prev):\n                p = prev_itemsets[i]\n                q = prev_itemsets[j]\n                # join only if first k-2 items are the same\n                if p[:k-2] == q[:k-2]:\n                    # form a candidate by adding the last item of q\n                    candidate = tuple(sorted(p + (q[-1],)))\n                    # prune: all (k-1)-subsets of candidate must be frequent\n                    all_subsets_frequent = True\n                    for subset in combinations(candidate, k-1):\n                        if subset not in Lk_minus_1:\n                            all_subsets_frequent = False\n                            break\n                    if all_subsets_frequent:\n                        Ck.add(candidate)\n\n        if not Ck:\n            break\n\n        # 3) Count support for each candidate\n        candidate_counts = Counter()\n        for tset in txn_sets:\n            for cand in Ck:\n                # if candidate is subset of transaction\n                if set(cand).issubset(tset):\n                    candidate_counts[cand] += 1\n\n        # 4) Keep those meeting minsup => Lk\n        Lk = { cand: cnt for cand, cnt in candidate_counts.items() if cnt >= threshold }\n        if not Lk:\n            break\n\n        # add to global frequent itemsets\n        frequent_itemsets.update(Lk)\n\n        # prepare next iteration\n        Lk_minus_1 = Lk\n        k += 1\n\n    return frequent_itemsets\n```\n\nExplanation of the main steps:\n\n1. **Threshold calculation**: We compute `threshold = ceil(minsup * N)` where `N` is the number of transactions.\n2. **Frequent 1-itemsets**: We count each item over all transactions (using a `Counter`), then filter by `threshold`.\n3. **Apriori iterative loop** (for k = 2, 3, \u2026):\n   a. **Candidate generation** (`join` step): We join pairs of frequent (k\u22121)-itemsets that share the same first k\u22122 items, producing k-item candidates.  \n   b. **Pruning**: For each candidate, ensure all its (k\u22121)-sized subsets are themselves frequent.  \n   c. **Support counting**: We scan the transactions and increment the count of each candidate that is a subset of a transaction.  \n   d. **Filtering**: We keep only those candidates whose support \u2265 `threshold`.  \n   e. If no candidates survive at any stage, we terminate.\n\nAll discovered frequent itemsets (of all sizes) are collected in `frequent_itemsets` and returned. If none meet the support threshold, the function returns an empty dictionary."}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    n_samples, n_features = data.shape\n    if not (1 <= k <= n_samples):\n        raise ValueError(\"k must be in the range [1, number of samples]\")\n\n    # Initialize centres: first k samples (as floats, copy to avoid side-effects)\n    centres = data[:k].astype(float).copy()\n\n    labels = np.zeros(n_samples, dtype=int)\n\n    for iteration in range(max_iter):\n        # 1) Assign each sample to the nearest centre (squared Euclidean distance)\n        # distances shape: (n_samples, k)\n        diff = data[:, np.newaxis, :] - centres[np.newaxis, :, :]  # broadcast\n        dists_sq = np.sum(diff * diff, axis=2)\n        new_labels = np.argmin(dists_sq, axis=1)\n\n        # 2) Recompute centres\n        new_centres = np.zeros_like(centres)\n        for cluster_idx in range(k):\n            members = np.where(new_labels == cluster_idx)[0]\n            if members.size == 0:\n                # Empty cluster: reinitialize its centre to a random data point\n                rand_idx = np.random.randint(0, n_samples)\n                new_centres[cluster_idx] = data[rand_idx]\n            else:\n                new_centres[cluster_idx] = np.mean(data[members], axis=0)\n\n        # 3) Check convergence: maximum shift of any centre\n        shifts = np.linalg.norm(new_centres - centres, axis=1)\n        if np.max(shifts) < epsilon:\n            centres = new_centres\n            labels = new_labels\n            break\n\n        centres = new_centres\n        labels = new_labels\n\n    # Round each coordinate of each centre to 4 decimal places\n    centres_rounded = np.round(centres, 4)\n\n    # Convert to Python types\n    centres_list = centres_rounded.tolist()\n    labels_list = labels.tolist()\n\n    return centres_list, labels_list\n```\n\nExplanation of the main steps:\n\n1. **Initialization**  \n   The first `k` rows of `data` are copied as the initial cluster centres.\n\n2. **Assignment**  \n   For each sample, compute its squared Euclidean distance to each centre (using NumPy broadcasting), and assign it to the nearest centre.\n\n3. **Update**  \n   For each cluster, compute the arithmetic mean of its assigned samples to get the new centre.  \n   If a cluster has no members, re-initialize that centre to a randomly chosen sample (using `np.random.randint`).\n\n4. **Convergence Check**  \n   Compute the shift of each centre (Euclidean distance between old and new centres).  \n   If the maximum shift is less than `epsilon`, or if `max_iter` iterations are reached, stop.\n\n5. **Rounding and Return**  \n   After convergence, round each coordinate of each centre to 4 decimal places and return them along with the cluster labels.\n"}
{"task_id": 13, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n   Item-based k-NN collaborative filtering recommender    n_user, n_item = data.shape\n    # 1) Build the item\u2013item similarity matrix S\n    S = np.zeros((n_item, n_item), dtype=float)\n    for i in range(n_item):\n        for j in range(i, n_item):\n            # find users who rated both items i and j\n            mask = (data[:, i] > 0) & (data[:, j] > 0)\n            if not np.any(mask):\n                sim = 0.0\n            else:\n                v1 = data[mask, i].astype(float)\n                v2 = data[mask, j].astype(float)\n                if criterion == 'cosine':\n                    # mean-center if std > 1e-3\n                    std1 = np.std(v1)\n                    std2 = np.std(v2)\n                    if std1 > 1e-3:\n                        v1 = v1 - v1.mean()\n                    if std2 > 1e-3:\n                        v2 = v2 - v2.mean()\n                    norm1 = np.linalg.norm(v1)\n                    norm2 = np.linalg.norm(v2)\n                    if norm1 < 1e-12 or norm2 < 1e-12:\n                        sim = 0.0\n                    else:\n                        sim = float(np.dot(v1, v2) / (norm1 * norm2))\n                elif criterion == 'pearson':\n                    # if fewer than 2 samples, corrcoef gives nan; we treat as 0\n                    if v1.size < 2:\n                        sim = 0.0\n                    else:\n                        corr = np.corrcoef(v1, v2)[0, 1]\n                        sim = 0.0 if np.isnan(corr) else float(corr)\n                else:\n                    raise ValueErrorcriterion must be 'cosine' or 'pearson            S[i, j] = sim\n            S[j, i] = sim\n\n    # 2) Find items already rated by the active user\n    user_ratings = data[user_ind, :]\n    rated_indices = np.where(user_ratings > 0)[0]\n    if rated_indices.size == n_item:\n        return []  # user has rated everything\n\n    # 3) For each unrated item, compute its predicted score\n    candidates = []\n    for t in range(n_item):\n        if user_ratings[t] > 0:\n            continue  # skip already rated\n        sims = S[t, rated_indices]\n        denom = np.sum(np.abs(sims))\n        if denom < 1e-12:\n            score = 0.0\n        else:\n            score = float(np.dot(user_ratings[rated_indices], sims) / denom)\n        candidates.append((t, score))\n\n    # 4) Sort by descending score, tie-breaking on smaller item index\n    #    and pick the top-k\n    candidates.sort(key=lambda x: (-x[1], x[0]))\n    top_k = [t for t, _ in candidates[:k]]\n    return top_k\n```\n\nExplanation of key steps:\n1. We build `S`, the item\u2013item similarity matrix, by iterating over all unordered pairs `(i, j)`. We collect only the users who rated both items and compute either cosine or Pearson similarity as specified.\n2. We extract which items the active user has already rated.\n3. For every item `t` the user has _not_ rated, we compute the usual weighted\u2010sum score  \n   `score(t) = \u03a3\u1d62 r\u1d62\u00b7S[t,i] / \u03a3\u1d62 |S[t,i]|`  \n   over all rated items `i`. If the denominator is zero, we define the score as zero.\n4. We sort candidate items by descending score and break ties by smaller item index (via the sort key `(-score, index)`) and return at most `k` recommendations."}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n   Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* containing the feature\n        values of all training instances that reach the current node.\n    g : np.ndarray\n        1-D array with the first-order gradients for each training instance.\n    h : np.ndarray\n        1-D array with the second-order gradients (Hessians).\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to the denominator when computing the\n        weight of a leaf.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        A pair *(feature_index, threshold)* describing the optimal split, or\n        ``None`` if no valid split yields a positive gain.\n       n_samples, n_features = X.shape\n    # Need at least 4 samples to have 2 on each side after a split\n    if n_samples < 4:\n        return None\n\n    # Precompute parent sums\n    G_parent = np.sum(g)\n    H_parent = np.sum(h)\n    # XGBoost gain formula uses:\n    #   gain = 0.5 * [ G_L^2/(H_L+lam) + G_R^2/(H_R+lam) - G_P^2/(H_P+lam) ] - gamma\n    parent_term = (G_parent * G_parent) / (H_parent + lam)\n\n    best_gain = 0.0\n    best_split: tuple[int, float] | None = None\n\n    # For each feature, try splitting at each unique feature value\n    for j in range(n_features):\n        x_j = X[:, j]\n        # sort the feature column and keep track of corresponding g,h if you like,\n        # but here we just use boolean masks on the fly\n        unique_vals = np.unique(x_j)\n\n        for v in unique_vals:\n            # Partition samples\n            left_mask = (x_j <= v)\n            n_left = left_mask.sum()\n            n_right = n_samples - n_left\n\n            # Must leave at least 2 samples on each side\n            if n_left < 2 or n_right < 2:\n                continue\n\n            # Compute sums on left and right\n            G_left = np.sum(g[left_mask])\n            H_left = np.sum(h[left_mask])\n            G_right = G_parent - G_left\n            H_right = H_parent - H_left\n\n            # Compute gain\n            gain = 0.5 * (\n                (G_left * G_left) / (H_left + lam)\n                + (G_right * G_right) / (H_right + lam)\n                - parent_term\n            ) - gamma\n\n            # Track the best positive gain\n            if gain > best_gain:\n                best_gain = gain\n                best_split = (j, float(v))\n\n    return best_split\n```\n\nExplanation of key steps:\n\n1. **Parent statistics**  \n   We precompute  \n   &nbsp;&nbsp;&nbsp;&nbsp;G_parent = \u03a3\u1d62 g\u1d62,  \n   &nbsp;&nbsp;&nbsp;&nbsp;H_parent = \u03a3\u1d62 h\u1d62,  \n   and the \u201cparent term\u201d G_parent\u00b2/(H_parent+\u03bb).  \n\n2. **Enumerate features and thresholds**  \n   For each feature column `x_j = X[:, j]`, we loop over each unique value `v` as a candidate splitting threshold.\n\n3. **Validity check**  \n   We require at least two samples on both left (x_j \u2264 v) and right (x_j > v).\n\n4. **Compute child statistics**  \n   Summing gradients `g` and Hessians `h` on the left side gives (G_left, H_left); the right side is the remainder.\n\n5. **Gain calculation**  \n   According to XGBoost\u2019s formula:  \n   gain = \u00bd [ G_left\u00b2/(H_left+\u03bb) + G_right\u00b2/(H_right+\u03bb) \u2013 G_parent\u00b2/(H_parent+\u03bb) ] \u2013 \u03b3.\n\n6. **Select best**  \n   We keep track of the feature and threshold that maximize the (positive) gain. If no split yields gain > 0, we return `None`."}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    # Helper to compute sigmoid and gradient\n    def _compute(arr):\n        s = 1 / (1 + np.exp(-arr))\n        g = s * (1 - s)\n        return s, g\n\n    # Case 1: Python scalar (int or float)\n    if isinstance(x, (int, float)):\n        s, g = _compute(x)\n        # cast to Python float and round\n        return round(float(s), 4), round(float(g), 4)\n\n    # Otherwise, convert list or array-like to a NumPy array\n    arr = np.array(x, dtype=float)\n\n    # If it is effectively a 0-d array, treat as scalar\n    if arr.ndim == 0:\n        s, g = _compute(arr.item())\n        return round(float(s), 4), round(float(g), 4)\n\n    # Compute for array\n    s_arr, g_arr = _compute(arr)\n    s_arr = np.round(s_arr, 4)\n    g_arr = np.round(g_arr, 4)\n\n    # Convert back to Python lists\n    return s_arr.tolist(), g_arr.tolist()\n```\n\nExplanation of key points:\n1. We first check for pure Python scalars (`int`/`float`) and handle them directly, returning rounded floats.\n2. For lists or NumPy arrays, we convert the input to a NumPy array of floats.\n3. A zero-dimensional NumPy array (`arr.ndim == 0`) is also treated as a scalar.\n4. We compute sigmoid and its gradient element-wise using vectorized NumPy operations.\n5. Results are rounded to 4 decimal places with `round` for scalars and `np.round(..., 4)` for arrays.\n6. Non-scalar outputs are converted back to nested Python lists via `.tolist()`."}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\nfrom collections import defaultdict\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\n\n    Parameters\n    ----------\n    x_train : list[float]\n        Training sample values (one-dimensional).\n    y_train : list[int]\n        Labels corresponding to *x_train* (each value must be 1 or \u20131).\n    x_test : list[float]\n        Sample values to classify after training.\n    epsilon : float, default 0.0\n        Upper bound on the allowed training error.  Training stops once\n        the ensemble\u2019s training error \u2264 *epsilon*.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (1 or \u20131) for every value in *x_test*.\n    \"\"\"\n    # Convert to numpy arrays\n    x = np.array(x_train, dtype=float)\n    y = np.array(y_train, dtype=int)\n    n = len(x)\n    # Initial weights\n    w = np.ones(n) / n\n\n    # Sort training data (we'll keep everything in sorted order for stump search)\n    sorted_idx = np.argsort(x)\n    x_s = x[sorted_idx]\n    y_s = y[sorted_idx]\n    w_s = w[sorted_idx]\n\n    # This F_s will accumulate the ensemble's raw score on the sorted training set\n    F_s = np.zeros(n)\n\n    # We will store the chosen stumps as tuples (d, theta) and their alphas\n    stumps = []\n    alphas = []\n\n    while True:\n        # 1) Find the best 1D stump under the current weights\n        best_err = float('inf')\n        best_theta = None\n        best_d = None\n        best_pred = None\n\n        # Candidate thresholds: midpoints between consecutive x_s's\n        thresholds = []\n        for i in range(n - 1):\n            thresholds.append((x_s[i] + x_s[i + 1]) / 2.0)\n        if not thresholds:\n            # all points identical, just pick that value\n            thresholds = [x_s[0]]\n\n        for theta in thresholds:\n            # d = 0: h(x)=+1 if x<=theta else -1\n            pred0 = np.where(x_s <= theta, 1, -1)\n            err0 = np.sum(w_s * (pred0 != y_s))\n            if err0 < best_err:\n                best_err = err0\n                best_theta = theta\n                best_d = 0\n                best_pred = pred0\n\n            # d = 1: h(x)=+1 if x>theta else -1\n            pred1 = np.where(x_s > theta, 1, -1)\n            err1 = np.sum(w_s * (pred1 != y_s))\n            if err1 < best_err:\n                best_err = err1\n                best_theta = theta\n                best_d = 1\n                best_pred = pred1\n\n        # 2) If the stump is no better than random, stop\n        if best_err >= 0.5:\n            break\n\n        # 3) Compute alpha, with a tiny clip to avoid zero division\n        e = max(min(best_err, 1.0 - 1e-10), 1e-10)\n        alpha = 0.5 * math.log((1.0 - e) / e)\n\n        # Save the stump\n        stumps.append((best_d, best_theta))\n        alphas.append(alpha)\n\n        # 4) Update the weights on the sorted training set\n        w_s = w_s * np.exp(-alpha * y_s * best_pred)\n        w_s = w_s / np.sum(w_s)\n\n        # 5) Update the ensemble's raw score and check training error\n        F_s += alpha * best_pred\n        train_pred = np.where(F_s >= 0, 1, -1)\n        train_err = np.mean(train_pred != y_s)\n        if train_err <= epsilon:\n            break\n\n    # --- Now predict on x_test ---\n    y_out = []\n    for xt in x_test:\n        s = 0.0\n        for (d, theta), alpha in zip(stumps, alphas):\n            if d == 0:\n                h = 1 if xt <= theta else -1\n            else:\n                h = 1 if xt > theta else -1\n            s += alpha * h\n        y_out.append(1 if s >= 0 else -1)\n\n    return y_out\n```\n\nExplanation of key steps:\n1. We sort the training points once so that threshold candidates are easy to enumerate as the midpoints between consecutive `x` values.\n2. We maintain and update the sample weights `w_s` (in sorted order) in each round, pick the stump with minimum weighted error, compute its coefficient `alpha` and update both `w_s` and the cumulative score `F_s`.\n3. We stop when the ensemble\u2019s training error drops below `epsilon` (or if the best stump cannot beat random guessing).\n4. Finally, we apply the learned stumps (and their weights) to the test inputs and return the sign of the weighted sum."}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    # Number of test samples (m) and training samples (n)\n    # d is the feature dimension, not used explicitly below\n    m, _ = X_test.shape\n    n, _ = X_train.shape\n\n    if m == 0 or n == 0:\n        # No data to predict or no support vectors => return empty or trivial\n        return []\n\n    # Pre-compute squared norms for train and test\n    # X_test_sq : shape (m,)\n    X_test_sq = np.sum(X_test**2, axis=1)\n    # X_train_sq: shape (n,)\n    X_train_sq = np.sum(X_train**2, axis=1)\n\n    # Compute the matrix of squared distances: shape (m, n)\n    # dist_sq[i,j] = ||X_test[i] - X_train[j]||^2\n    # = ||X_test[i]||^2 + ||X_train[j]||^2 - 2 * X_test[i]\u00b7X_train[j]\n    dist_sq = (\n        X_test_sq[:, np.newaxis]\n        + X_train_sq[np.newaxis, :]\n        - 2.0 * X_test.dot(X_train.T)\n    )\n\n    # Gaussian (RBF) kernel matrix: K[i,j] = exp(-gamma * dist_sq[i,j])\n    K = np.exp(-gamma * dist_sq)\n\n    # Compute the decision function values for all test points:\n    # g = K \u00b7 (alpha * y_train) + b\n    coeff = alpha * y_train  # shape (n,)\n    g = K.dot(coeff) + b     # shape (m,)\n\n    # Convert to labels: sign(g) but ensure only +1 or -1\n    preds = np.where(g >= 0, 1, -1)\n\n    # Return as a plain Python list of ints\n    return preds.astype(int).tolist()\n```\n\nExplanation of the main steps:\n1. We compute all pairwise squared Euclidean distances between test points and training points using the identity  \n   \u2016z\u2212x\u2016\u00b2 = \u2016z\u2016\u00b2 + \u2016x\u2016\u00b2 \u2212 2 z\u00b7x  \n   in a fully vectorized way.\n2. We apply the Gaussian (RBF) kernel `K = exp(-\u03b3 \u00b7 dist_sq)`.\n3. We form the weighted sum  \n   g(z) = \u03a3_i \u03b1_i\u00b7y_i\u00b7K(z, x_i) + b  \n   by a single matrix-vector product `K.dot(alpha * y_train) + b`.\n4. Finally, we threshold at zero (`>=0` \u2192 +1, else \u22121) to produce the predicted class labels."}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    # Convert input to numpy array of floats\n    A = np.array(X, dtype=float)\n    m, n = A.shape\n\n    # Check validity of k\n    if k < 1 or k > min(m, n):\n        return -1\n\n    # Full SVD\n    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n    # Truncate to rank k\n    U_k = U[:, :k]             # (m x k)\n    s_k = s[:k]                # (k,)\n    Vt_k = Vt[:k, :]           # (k x n)\n\n    # Form the diagonal matrix of singular values\n    S_k = np.diag(s_k)         # (k x k)\n\n    # Reconstruct\n    A_hat = U_k @ S_k @ Vt_k   # (m x n)\n\n    # Compute MSE\n    mse = np.mean((A - A_hat) ** 2)\n\n    # Round results to 4 decimals\n    A_hat_rounded = np.round(A_hat, 4)\n    mse_rounded = float(np.round(mse, 4))\n\n    # Convert reconstructed matrix back to list of lists\n    X_hat_list = A_hat_rounded.tolist()\n\n    return X_hat_list, mse_rounded\n```\n\nExplanation of key steps:\n\n1. **Validation**: We check that `k` is between 1 and `min(m, n)`. If not, we return `-1`.  \n2. **SVD**: Using `np.linalg.svd(A, full_matrices=False)` to get the compact factorization `A = U @ diag(s) @ Vt`.  \n3. **Truncation**: We keep only the first `k` singular values/vectors (`U_k`, `s_k`, `Vt_k`).  \n4. **Reconstruction**: Compute `A_hat = U_k @ diag(s_k) @ Vt_k`.  \n5. **Error**: The MSE is `mean((A - A_hat)^2)`.  \n6. **Rounding**: We round every entry of `A_hat` and the MSE to 4 decimal places.  \n7. **Output**: Return a tuple `(X_hat, mse)` where `X_hat` is a list of lists and `mse` a float."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                          y_train: list[float] | np.ndarray,\n                          x_query: float,\n                          epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\"\"\"\n    # Convert to numpy arrays\n    x = np.asarray(x_train, dtype=float).flatten()\n    y = np.asarray(y_train, dtype=float).flatten()\n    n = x.shape[0]\n    if n == 0:\n        return 0.0\n\n    # Sort by x\n    order = np.argsort(x)\n    x = x[order]\n    y = y[order]\n\n    # Candidate splits: midpoints between consecutive distinct x's\n    splits = []\n    for i in range(n - 1):\n        if x[i] != x[i + 1]:\n            splits.append(0.5 * (x[i] + x[i + 1]))\n    # If no valid split, we'll just predict the mean\n    if len(splits) == 0:\n        return round(float(np.mean(y)), 4)\n\n    # Initialize residuals to y (since initial prediction is zero)\n    residuals = y.copy()\n    rss = np.sum(residuals**2)\n\n    # Storage for learned stumps: list of (split, c1, c2)\n    stumps = []\n\n    # Iteratively add stumps until rss <= epsilon\n    while rss > epsilon:\n        best = None  # will hold (rss_after, split, c1, c2)\n        # Try every candidate split\n        for s in splits:\n            left_mask = x <= s\n            right_mask = ~left_mask\n            # Means of residuals on each side\n            c1 = residuals[left_mask].mean() if np.any(left_mask) else 0.0\n            c2 = residuals[right_mask].mean() if np.any(right_mask) else 0.0\n            # Compute stump predictions\n            preds = np.where(left_mask, c1, c2)\n            # RSS after adding this stump\n            rss_new = np.sum((residuals - preds)**2)\n            if best is None or rss_new < best[0]:\n                best = (rss_new, s, c1, c2)\n\n        # If we couldn't improve, break\n        if best is None:\n            break\n\n        rss_new, best_s, best_c1, best_c2 = best\n        # If no improvement, stop\n        if rss_new >= rss:\n            break\n\n        # Accept this stump\n        stumps.append((best_s, best_c1, best_c2))\n        # Update residuals and rss\n        left_mask = x <= best_s\n        preds = np.where(left_mask, best_c1, best_c2)\n        residuals -= preds\n        rss = rss_new\n\n    # Make prediction for x_query\n    pred = 0.0\n    for s, c1, c2 in stumps:\n        pred += (c1 if x_query <= s else c2)\n\n    return round(float(pred), 4)\n```\n\nExplanation of key steps:\n1. We sort the data by the single feature to generate valid candidate splits.\n2. We initialize residuals to the true targets (since the ensemble prediction starts at 0).\n3. In each boosting iteration, we scan all splits, compute the best stump (one split with constants on left/right) that minimizes the new RSS, and then update the residuals.\n4. We stop once the RSS falls below the tolerance `epsilon` (or no further improvement is possible).\n5. Finally, we sum the contributions of all learned stumps at `x_query` and return the result rounded to four decimals."}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"\n    Generate a NumPy tensor of the given shape, filled with 0.0s and 1.0s,\n    where each element is 1.0 with probability `sparsity` and 0.0 otherwise.\n\n    Parameters\n    ----------\n    shape : tuple of int\n        The desired shape of the output tensor.\n    sparsity : float, optional\n        Probability of an entry being 1.0. Must lie in [0, 1]. Default is 0.5.\n    seed : int or None, optional\n        If not None, seed for NumPy's random number generator to make the\n        result reproducible. Default is None.\n\n    Returns\n    -------\n    numpy.ndarray of floats\n        An array of shape `shape` containing only 0.0 and 1.0.\n    int\n        Returns -1 if `sparsity` is outside the interval [0, 1].\n    \"\"\"\n    # 1. Check sparsity bounds\n    if not (0.0 <= sparsity <= 1.0):\n        return -1\n\n    # 2. Optionally seed for reproducibility\n    if seed is not None:\n        np.random.seed(seed)\n\n    # 3. Draw uniform samples in [0, 1)\n    uniform_samples = np.random.rand(*shape)\n\n    # 4. Compute threshold so that P(entry == 1.0) == sparsity\n    threshold = 1.0 - sparsity\n\n    # 5. Compare and cast to float\n    binary_tensor = (uniform_samples >= threshold).astype(float)\n\n    return binary_tensor\n```\n\nExplanation of key steps:\n\n1. We first validate that `sparsity` \u2208 [0,1], returning `-1` if not.\n2. If a `seed` is provided, we call `np.random.seed(seed)` for reproducibility.\n3. We generate uniform random samples in `[0,1)` of the requested `shape` via `np.random.rand`.\n4. To ensure each entry is 1.0 with probability exactly `sparsity`, we set a threshold at `1 - sparsity`.  A uniform sample `r` will satisfy `r >= threshold` with probability `sparsity`.\n5. Finally, we convert the boolean mask `(uniform_samples >= threshold)` into floats (`0.0` and `1.0`) and return it."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger  values.\n    \"\"\"\n    n = data.shape[0]\n    if n == 0:\n        raise ValueError(\"Cannot split an empty dataset.\")\n\n    # k = median position (for even n, this is n//2 as specified)\n    k = n // 2\n\n    # extract the column of interest\n    col = data[:, d]\n\n    # find the row index of the k-th smallest element in O(n) time\n    parti = np.argpartition(col, k)\n    pivot_idx = int(parti[k])        # row index in original array\n    pivot_val = col[pivot_idx]\n\n    # collect strictly smaller and strictly larger row indices\n    left_mask = col < pivot_val\n    right_mask = col > pivot_val\n\n    left_indices = np.nonzero(left_mask)[0]\n    right_indices = np.nonzero(right_mask)[0]\n\n    # sort to make output deterministic\n    left_list = sorted(int(i) for i in left_indices)\n    right_list = sorted(int(i) for i in right_indices)\n\n    return pivot_idx, left_list, right_list\n```\n\nExplanation of the key steps:\n\n1.  Compute `k = n//2`.  \n2.  Use `np.argpartition` on the chosen column to place the k-th smallest element at position `k` without fully sorting.  \n3.  Extract `pivot_idx = parti[k]`. This is the row index in the original array whose value is the median.  \n4.  Determine `pivot_val = data[pivot_idx, d]`.  \n5.  Build boolean masks for strictly smaller (`< pivot_val`) and strictly larger (`> pivot_val`) entries, extract their row indices, and sort them.  \n6.  Return `(pivot_idx, left_list, right_list)`."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n   Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n       # Preconditions\n    assert isinstance(x, np.ndarray),Input must be a numpy array    assert x.ndim == 1,Input array must be 1-D    assert stride >= 1,Stride must be at least 1    N = x.shape[0]\n    assert N >= frame_width,Input length must be at least as large as frame_width    # Number of frames\n    n_frames = (N - frame_width) // stride + 1\n\n    # Compute the new strides\n    element_stride = x.strides[0]\n    new_strides = (stride * element_stride, element_stride)\n\n    # Build the as_strided view\n    frames = as_strided(x,\n                        shape=(n_frames, frame_width),\n                        strides=new_strides)\n\n    # Set writeability\n    if writeable:\n        # Inherit the writability of the source array\n        frames.flags.writeable = x.flags.writeable\n    else:\n        # Force read-only\n        frames.flags.writeable = False\n\n    return frames\n```\n\nExplanation of key steps:\n\n1. We assert the three required preconditions:\n   - `x` is 1-D.\n   - `stride` is at least 1.\n   - `len(x)` is at least `frame_width`.\n\n2. We compute the number of frames as\n   ```\n   n_frames = (len(x) - frame_width) // stride + 1\n   ```\n   dropping any trailing samples that wouldn't form a full frame.\n\n3. We use `as_strided` with\n   - `shape = (n_frames, frame_width)`\n   - `strides = (stride * x.strides[0], x.strides[0])`\n   so that each successive row starts `stride` samples after the previous one.\n\n4. Finally, we enforce the required writeability on the returned view."}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Attempt to convert to a 2D float array\n    try:\n        arr = np.array(X, dtype=float)\n    except Exception:\n        # Non-numeric data or ragged input\n        return False\n\n    # Must be exactly 2-dimensional\n    if arr.ndim != 2:\n        return False\n\n    # Every entry must lie in [0, 1]\n    if np.any(arr < 0) or np.any(arr > 1):\n        return False\n\n    # Each row must sum to 1 within tolerance\n    row_sums = arr.sum(axis=1)\n    if not np.allclose(row_sums, np.ones(arr.shape[0]), atol=1e-8):\n        return False\n\n    return True\n```\n\nExplanation of the steps:\n\n1. We wrap the array conversion in a `try`/`except` so that non-numeric or irregular (ragged) inputs immediately yield `False` rather than propagating an exception.  \n2. We check the number of dimensions (`arr.ndim`). Only 2D arrays are allowed.  \n3. We verify that every entry is between 0 and 1 inclusive using `np.any`.  \n4. We compute the sum of each row and compare it to 1 with `np.allclose` under an absolute tolerance of `1e-8`.  \n5. If all checks pass, we return `True`; otherwise `False`."}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n   Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n       n, d = X.shape\n    # Flatten y to shape (n,)\n    y = y.reshape(-1)\n    # Initialize parameters\n    w = np.zeros(d, dtype=float)\n    s = np.zeros(d, dtype=float)\n    \n    # If batch_size > n, use full batch each time\n    if batch_size > n:\n        batch_size = n\n\n    # For wrapping around the data in mini-batches\n    idx_start = 0\n\n    for _ in range(max_iter):\n        # Determine current mini-batch indices with wrap-around\n        if batch_size == n:\n            Xb = X\n            yb = y\n        else:\n            idx_end = idx_start + batch_size\n            if idx_end <= n:\n                Xb = X[idx_start:idx_end]\n                yb = y[idx_start:idx_end]\n            else:\n                # Wrap around\n                part1 = X[idx_start:n]\n                part2 = X[0:idx_end - n]\n                Xb = np.vstack((part1, part2))\n                yb = np.concatenate((y[idx_start:n], y[0:idx_end - n]))\n            idx_start = idx_end % n  # advance start for next iteration\n\n        m = Xb.shape[0]  # actual batch size\n        \n        # Compute gradient on the batch\n        resid = Xb.dot(w) - yb  # shape (m,)\n        g = (1.0 / m) * (Xb.T.dot(resid))  # shape (d,)\n\n        # Early stopping if gradient norm is small\n        if np.linalg.norm(g) < epsilon:\n            break\n\n        # Update the running average of squared gradients\n        s = rho * s + (1 - rho) * (g ** 2)\n\n        # Update weights\n        w -= eta * g / (np.sqrt(s) + eps_station)\n\n    # Round to 4 decimals and return as list\n    return np.round(w, 4).tolist()\n```\n\nExplanation of key steps:\n1. **Initialization**:  \n   - `w` (weights) and `s` (running squared-gradient average) are zero vectors of length `d`.  \n   - If `batch_size > n`, we reset `batch_size = n` so each update uses the full dataset.\n\n2. **Mini-batch selection with wrap-around**:  \n   - We keep an index `idx_start` that moves forward by `batch_size` each iteration, wrapping around the dataset when we reach the end.  \n   - We slice out `Xb, yb` accordingly; if the slice overruns the end of the data, we concatenate the \u201ctail\u201d and the \u201chead\u201d of the dataset.\n\n3. **Gradient and RMSprop update**:  \n   - Compute the mini-batch gradient `g = (1/m) Xb\u1d40(Xb w \u2212 yb)`.  \n   - Stop early if `\u2225g\u2225\u2082 < epsilon`.  \n   - Update the running average of squared gradients `s \u2190 \u03c1\u00b7s + (1\u2212\u03c1)\u00b7g\u00b2` (element-wise).  \n   - Update weights `w \u2190 w \u2212 \u03b7\u00b7g/(\u221as + eps_station)`.\n\n4. **Return**  \n   - Round each component of `w` to 4 decimal places and convert to a Python list."}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n       Compute the SoftPlus activation or its derivative (sigmoid) in a numerically\n    stable way, round each output to 4 decimal places, and return as a Python list\n    (or a single float if the input was a scalar).\n\n    Args:\n        x: A scalar, Python list, or NumPy array of real values.\n        deriv: If False (default), return SoftPlus(x). If True, return sigmoid(x).\n\n    Returns:\n        A Python list of floats rounded to 4 decimals, or a single float if the input\n        was a scalar.\n       # Convert input to NumPy array of floats\n    x_arr = np.array(x, dtype=float)\n    is_scalar = (x_arr.ndim == 0)\n\n    if deriv:\n        # Numerically stable sigmoid:\n        #   sigmoid(x) = 1/(1+e^-x)        if x >= 0\n        #              = e^x/(1+e^x)      if x <  0\n        out = np.where(\n            x_arr >= 0,\n            1.0 / (1.0 + np.exp(-x_arr)),\n            np.exp(x_arr) / (1.0 + np.exp(x_arr))\n        )\n    else:\n        # Numerically stable SoftPlus:\n        #   softplus(x) = x + log1p(e^-x)   if x >  0\n        #                = log1p(e^x)       if x <= 0\n        out = np.where(\n            x_arr > 0,\n            x_arr + np.log1p(np.exp(-x_arr)),\n            np.log1p(np.exp(x_arr))\n        )\n\n    # Round to 4 decimal places\n    out = np.round(out, 4)\n\n    # Return as Python list or scalar float\n    if is_scalar:\n        return float(out)\n    else:\n        return out.tolist()\n```"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\nimport math\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n   Compute the unsmoothed MLE N-gram log-probability of *sequence*.\n\n    Your task is to complete this function so that it builds N-gram count\n    tables from *corpus* and then returns the total log-probability of\n    *sequence* under the resulting unsmoothed language model.\n\n    The return value must be rounded to 4 decimal places.  If any required\n    count is zero you should immediately return ``float('-inf')``.\n       # handle trivial case: if sequence has fewer tokens than N, no windows => prob=1 => log prob = 0\n    if len(sequence) < N:\n        return round(0.0, 4)\n\n    # Unigram model\n    if N == 1:\n        uni_counts = Counter(corpus)\n        total_tokens = len(corpus)\n        log_prob = 0.0\n\n        for w in sequence:\n            count_w = uni_counts.get(w, 0)\n            if count_w == 0:\n                return float('-inf')\n            p = count_w / total_tokens\n            log_prob += math.log(p)\n\n        return round(log_prob, 4)\n\n    # N > 1: build counts for N-grams and (N-1)-gram prefixes\n    # ngram_counts[(w1,...,wN)] = count\n    # prefix_counts[(w1,...,w_{N-1})] = count\n    ngram_counts = Counter(\n        tuple(corpus[i : i + N]) for i in range(len(corpus) - N + 1)\n    )\n    prefix_counts = Counter(\n        tuple(corpus[i : i + N - 1]) for i in range(len(corpus) - (N - 1) + 1)\n    )\n\n    log_prob = 0.0\n    for i in range(len(sequence) - N + 1):\n        ngram = tuple(sequence[i : i + N])\n        prefix = tuple(sequence[i : i + N - 1])\n\n        count_ng = ngram_counts.get(ngram, 0)\n        if count_ng == 0:\n            return float('-inf')\n\n        count_pre = prefix_counts.get(prefix, 0)\n        if count_pre == 0:\n            return float('-inf')\n\n        p = count_ng / count_pre\n        log_prob += math.log(p)\n\n    return round(log_prob, 4)\n```\n\nExplanation of the main steps:\n\n1. **Base case**: If `len(sequence) < N`, there are no length-N windows, so the product of window probabilities is 1 and log-probability is 0.\n\n2. **Unigram (N=1)**:  \n   - Count each token in `corpus`.  \n   - For each token in `sequence`, if it never appeared in `corpus`, return `-inf`.  \n   - Otherwise add `log(count(token)/|corpus|)` to the total.\n\n3. **Higher-order (N>1)**:  \n   - Build a `Counter` of all N-grams in the corpus.  \n   - Build a `Counter` of all (N\u22121)-gram prefixes in the corpus.  \n   - Slide a window of length N over `sequence`.  \n     - If the N-gram count or its prefix count is zero, return `-inf`.  \n     - Otherwise add `log(count(ngram)/count(prefix))`.\n\n4. **Rounding**: Finally, round the sum of logs to 4 decimal places."}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef lasso_regression(\n    X: list[float] | \"np.ndarray\",\n    y: list[float] | \"np.ndarray\",\n    degree: int,\n    reg_factor: float,\n    n_iterations: int = 1000,\n    tol: float = 1e-6\n) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\n\n    The function builds polynomial features up to 'degree', optimises the\n    Lasso objective with coordinate descent, and returns the learned\n    coefficients rounded to four decimal places.\n\n    Parameters\n    ----------\n    X : list[float] | np.ndarray\n        One-dimensional input samples.\n    y : list[float] | np.ndarray\n        Target values, same length as X.\n    degree : int\n        Highest exponent of x to include (total coeffs = degree + 1).\n    reg_factor : float\n        L1 regularisation strength \u03bb.\n    n_iterations : int\n        Maximum number of coordinate-descent passes.\n    tol : float\n        Early-stopping tolerance on max coefficient change.\n\n    Returns\n    -------\n    list[float]\n        Weights [w0, w1, ..., w_degree], rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n    m = X_arr.shape[0]\n    if y_arr.shape[0] != m:\n        raise ValueError(\"X and y must have the same length.\")\n    if X_arr.ndim != 1:\n        raise ValueError(\"X must be one-dimensional.\")\n    if degree < 0 or not isinstance(degree, int):\n        raise ValueError(\"degree must be a non-negative integer.\")\n    if reg_factor < 0:\n        raise ValueError(\"reg_factor must be non-negative.\")\n\n    # Build design matrix: columns [1, x, x^2, ..., x^degree]\n    # np.vander with increasing=True gives [x^0, x^1, ..., x^degree]\n    X_mat = np.vander(X_arr, N=degree + 1, increasing=True)  # shape (m, degree+1)\n\n    # Initialize weights w = [w0, w1, ..., w_degree]\n    w = np.zeros(degree + 1, dtype=float)\n\n    # Precompute denom for j>=1: denom_j = sum_i x_i^j * x_i^j = sum (X_mat[:,j]**2)\n    if degree >= 1:\n        denoms = np.sum(X_mat[:, 1:] ** 2, axis=0)  # shape (degree,)\n    else:\n        denoms = np.array([])\n\n    # Soft-threshold helper\n    def soft_threshold(rho: float, lam: float) -> float:\n        if rho > lam:\n            return rho - lam\n        elif rho < -lam:\n            return rho + lam\n        else:\n            return 0.0\n\n    # Coordinate descent\n    for iteration in range(n_iterations):\n        w_old = w.copy()\n\n        # 1) Update bias (w0), unregularised\n        if degree >= 1:\n            # residual excluding bias term: y - \u03a3_{j>=1} x^j * w_j\n            # X_mat[:,1:] @ w[1:] is shape (m,)\n            w[0] = np.mean(y_arr - X_mat[:, 1:] @ w[1:])\n        else:\n            # If degree == 0, model is constant\n            w[0] = np.mean(y_arr)\n\n        # 2) Update each w_j for j = 1..degree\n        #    using coordinate descent closed-form with soft thresholding\n        for j in range(1, degree + 1):\n            x_j = X_mat[:, j]               # shape (m,)\n            # Compute partial residual: r = y - (Xw) + w_j * x_j\n            # so that r + x_j * w_j = y - X_{-j} w_{-j}\n            r = y_arr - (X_mat @ w) + w[j] * x_j\n            rho = x_j.dot(r)                # scalar\n\n            # Soft-threshold then divide by denom\n            w_j_new = soft_threshold(rho, reg_factor) / denoms[j - 1]\n            w[j] = w_j_new\n\n        # Check convergence: max absolute change in any weight\n        max_change = np.max(np.abs(w - w_old))\n        if max_change < tol:\n            break\n\n    # Round to 4 decimal places and return as list\n    w_rounded = np.round(w, 4)\n    return w_rounded.tolist()\n```\n\nExplanation of key steps:\n\n1. **Design Matrix**  \n   We use `np.vander(X_arr, N=degree+1, increasing=True)` to build `[1, x, x^2, \u2026, x^degree]`.\n\n2. **Coordinate Descent**  \n   - **Bias Update (w\u2080)**:  \n     w\u2080 \u2190 mean(y \u2212 \u2211_{j\u22651} x\u02b2\u00b7w\u2c7c). Bias is not regularised.\n   - **Feature Updates (w\u2c7c, j\u22651)**:  \n     Let r = y \u2212 Xw + w\u2c7c\u00b7x\u2c7c, compute \u03c1 = x\u2c7c\u1d40r, then  \n       w\u2c7c \u2190 soft_threshold(\u03c1, \u03bb) / (x\u2c7c\u1d40x\u2c7c)  \n     where soft_threshold(\u03c1, \u03bb) = sign(\u03c1)\u00b7max(|\u03c1|\u2212\u03bb, 0).\n\n3. **Stopping Criterion**  \n   We stop early if the maximum absolute change across all weights in one pass is below `tol`.\n\n4. **Edge Case \u03bb=0**  \n   When `reg_factor=0`, the soft-threshold becomes identity, and the algorithm reduces to ordinary least squares via coordinate descent.\n\n5. **Output**  \n   We return all coefficients `[w0, w1, \u2026, w_degree]` rounded to 4 decimal places.\n"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], \n                  B: list[list[float]], \n                  obs: list[int], \n                  t: int) -> list[float]:\n    \"\"\"\n    Compute the backward probability vector \u03b2_t for a discrete Hidden Markov Model.\n    \n    Given:\n      A   \u2013 (N\u00d7N) transition-probability matrix, A[i][j] = P(x_{k+1}=j | x_k=i)\n      B   \u2013 (N\u00d7M) emission-probability matrix, B[j][m] = P(o=m | x=j)\n      obs \u2013 length-T list of observation indices (each in 0..M-1)\n      t   \u2013 time index (0 \u2264 t < T)\n    \n    Returns:\n      A list of length N containing \u03b2_t(i) for i=0..N-1,\n      each rounded to 4 decimal places.\n    \n    Backward recursion:\n      \u03b2_{T-1}(i) = 1\n      \u03b2_t(i) = sum_j A[i][j] * B[j][ obs[t+1] ] * \u03b2_{t+1}(j)\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized ops\n    A = np.array(A, dtype=float)      # shape (N, N)\n    B = np.array(B, dtype=float)      # shape (N, M)\n    T = len(obs)\n    N = A.shape[0]\n    \n    # initialize \u03b2_{T-1}(i) = 1 for all i\n    beta = np.ones(N, dtype=float)\n    \n    # backward pass from T-2 down to t\n    # at each step k we compute \u03b2_k from \u03b2_{k+1}\n    for k in range(T - 2, t - 1, -1):\n        o_next = obs[k + 1]\n        # elementwise multiply emission * next-beta, then weight by A and sum over j\n        beta = (A * (B[:, o_next] * beta)).sum(axis=1)\n    \n    # round to 4 decimal places and return as Python list\n    beta_rounded = np.round(beta, 4)\n    return beta_rounded.tolist()\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    # If any of the inputs is empty, we cannot compute a valid probability.\n    if not A or not B or not pi or not obs:\n        return 0.0\n\n    # Convert to numpy arrays for convenience\n    A_arr = np.array(A, dtype=float)\n    B_arr = np.array(B, dtype=float)\n    pi_arr = np.array(pi, dtype=float)\n    obs_arr = np.array(obs, dtype=int)\n\n    N = A_arr.shape[0]   # number of states\n    T = obs_arr.shape[0] # length of the observation sequence\n\n    # Initialize the backward variables \u03b2\n    # \u03b2[t, i] = P(o_{t+1}, ..., o_{T-1} | q_t = i, \u03bb)\n    beta = np.zeros((T, N), dtype=float)\n    beta[T-1, :] = 1.0\n\n    # Recursively compute \u03b2 backwards from t = T-2 down to t = 0\n    for t in range(T-2, -1, -1):\n        # for each state i, \u03b2[t,i] = sum_j A[i,j] * B[j, o_{t+1}] * \u03b2[t+1, j]\n        weighted_next = B_arr[:, obs_arr[t+1]] * beta[t+1, :]\n        beta[t, :] = A_arr.dot(weighted_next)\n\n    # Finally, compute P(O | \u03bb) = sum_i \u03c0[i] * B[i, o_0] * \u03b2[0, i]\n    first_emit = B_arr[:, obs_arr[0]]\n    prob = np.dot(pi_arr * first_emit, beta[0, :])\n\n    # Round to six decimal places and return as native Python float\n    return float(round(prob, 6))\n```\n\nExplanation of the main steps:\n\n1. **Validation**: If any of the input lists (`A`, `B`, `pi`, or `obs`) is empty, return `0.0` immediately.\n2. **Initialization**:\n   - Convert the lists to NumPy arrays for vectorized computations.\n   - `N` is the number of hidden states.\n   - `T` is the length of the observation sequence.\n   - Create a `(T \u00d7 N)` array `beta` and set the last row `beta[T-1, :]` to 1.\n3. **Backward recursion**:\n   - For each time step `t` from `T-2` down to `0`, compute\n     ```\n     beta[t, i] = \u2211_j A[i, j] * B[j, o_{t+1}] * beta[t+1, j].\n     ```\n   - This is done efficiently with a matrix\u2013vector product.\n4. **Termination**:\n   - Compute the total probability\n     ```\n     P(O | \u03bb) = \u2211_i \u03c0[i] * B[i, o_0] * beta[0, i].\n     ```\n5. **Rounding**: Return the result rounded to six decimal places.\n\nThis function runs in O(N\u00b2T) time, which is the standard complexity for the backward procedure in HMMs."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # 1. Validate that S, A, B are non-empty and have compatible dimensions\n    if not isinstance(S, list) or not isinstance(A, list) or not isinstance(B, list):\n        return -1.0\n\n    n = len(S)\n    if n == 0 or len(A) != n or len(B) != n:\n        return -1.0\n\n    # A must be n x n\n    for row in A:\n        if not isinstance(row, list) or len(row) != n:\n            return -1.0\n\n    # B must be n x m for some m >= 1\n    m = None\n    for row in B:\n        if not isinstance(row, list):\n            return -1.0\n        if m is None:\n            m = len(row)\n            if m == 0:\n                return -1.0\n        elif len(row) != m:\n            return -1.0\n\n    # 2. Check that observations is non-empty and each index is in [0, m-1]\n    if not isinstance(observations, list) or len(observations) == 0:\n        return -1.0\n    for obs in observations:\n        if not isinstance(obs, int) or obs < 0 or obs >= m:\n            return -1.0\n\n    # Convert to numpy arrays for vectorized operations\n    S_arr = np.array(S, dtype=float)         # shape (n,)\n    A_arr = np.array(A, dtype=float)         # shape (n, n)\n    B_arr = np.array(B, dtype=float)         # shape (n, m)\n\n    # 3. Initialize forward probabilities (alpha) at time 0\n    t0 = observations[0]\n    alpha = S_arr * B_arr[:, t0]             # shape (n,)\n\n    # 4. Recursively compute alpha for t = 1 ... T-1\n    for t in observations[1:]:\n        # alpha_new[j] = [ sum_i alpha[i] * A[i, j] ] * B[j, t]\n        alpha = np.dot(alpha, A_arr) * B_arr[:, t]\n\n    # 5. Termination: sum of alpha over all states\n    prob = alpha.sum()\n\n    # Round to 4 decimal places\n    return round(prob, 4)\n```\n\nExplanation of key steps:\n\n1. **Validation**  \n   - Ensure `S`, `A`, `B` are lists and non\u2010empty.  \n   - Check `A` is n\u00d7n and `B` is n\u00d7m for some m\u22651.  \n   - Ensure the observation sequence is non\u2010empty and each index falls in [0, m\u22121].  \n   - On any failure, return \u22121.0.\n\n2. **Initialization**  \n   - Convert lists to numpy arrays.  \n   - Set `alpha[i] = S[i] * B[i, observations[0]]`.\n\n3. **Induction**  \n   - For each new observation `t`, update  \n     `alpha = (alpha @ A) * B[:, t]`.\n\n4. **Termination**  \n   - The total likelihood is `sum(alpha)`; round to four decimal places.\n\nThis implementation runs in O(T\u00b7n\u00b2) time where T = len(observations)."}
{"task_id": 70, "completion_id": 0, "completion": "from collections import Counter, defaultdict\n\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings).  `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result must be deterministic:\n      \u2022 Inside each item-set the items have to be sorted alphabetically.\n      \u2022 The outer list has to be sorted by `(len(itemset), itemset)`.\n    If *transactions* is empty or no item-set meets the threshold return an\n    empty list.\n    \"\"\"\n    # FP-tree node\n    class FPNode:\n        __slots__ = ('name', 'count', 'parent', 'children', 'link')\n        def __init__(self, name, parent):\n            self.name = name\n            self.count = 0\n            self.parent = parent\n            self.children = {}\n            self.link = None\n\n    def update_header(item, node, header_table):\n        # link new node into header table\n        head = header_table[item][1]\n        if head is None:\n            header_table[item][1] = node\n        else:\n            while head.link:\n                head = head.link\n            head.link = node\n\n    def insert_tree(items, node, header_table, count=1):\n        if not items:\n            return\n        first = items[0]\n        # add or update child\n        if first in node.children:\n            child = node.children[first]\n            child.count += count\n        else:\n            child = FPNode(first, node)\n            child.count = count\n            node.children[first] = child\n            update_header(first, child, header_table)\n        # recurse\n        insert_tree(items[1:], child, header_table, count)\n\n    # 1. Count frequency of single items\n    freq = Counter()\n    for tran in transactions:\n        freq.update(tran)\n    # filter infrequent\n    freq = {item: cnt for item, cnt in freq.items() if cnt >= min_support}\n    if not freq:\n        return []\n    # sort frequent items by descending freq, then lex\n    sorted_items = sorted(freq.items(), key=lambda x: (-x[1], x[0]))\n    # order map for transaction sorting\n    def sort_key(x):\n        return (-freq[x], x)\n    # build initial header table: item -> [support, head_of_node_link]\n    header_table = {item: [freq[item], None] for item, _ in sorted_items}\n    # build the main FP-tree\n    root = FPNode(None, None)\n    for tran in transactions:\n        # filter and sort\n        filtered = [i for i in tran if i in freq]\n        if not filtered:\n            continue\n        ordered = sorted(filtered, key=sort_key)\n        insert_tree(ordered, root, header_table)\n\n    frequent_itemsets = []\n\n    def mine_tree(header, prefix):\n        # items in header sorted lexicographically for deterministic order\n        for item in sorted(header.keys()):\n            new_prefix = prefix + [item]\n            frequent_itemsets.append(new_prefix.copy())\n            # build conditional pattern base\n            conditional_patterns = []\n            node = header[item][1]\n            while node:\n                count = node.count\n                path = []\n                parent = node.parent\n                while parent and parent.name is not None:\n                    path.append(parent.name)\n                    parent = parent.parent\n                if path:\n                    conditional_patterns.append((path[::-1], count))\n                node = node.link\n            # count freq in conditional base\n            cpb_freq = Counter()\n            for path, cnt in conditional_patterns:\n                for itm in path:\n                    cpb_freq[itm] += cnt\n            # filter by min_support\n            cond_items = {i for i, c in cpb_freq.items() if c >= min_support}\n            if not cond_items:\n                continue\n            # build conditional header table\n            # sort by descending cpb freq then lex\n            sorted_c = sorted(cond_items, key=lambda x: (-cpb_freq[x], x))\n            cond_header = {i: [cpb_freq[i], None] for i in sorted_c}\n            # build conditional FP-tree\n            cond_root = FPNode(None, None)\n            for path, cnt in conditional_patterns:\n                filtered_path = [i for i in path if i in cond_items]\n                if not filtered_path:\n                    continue\n                ordered_path = sorted(filtered_path, key=lambda x: (-cpb_freq[x], x))\n                insert_tree(ordered_path, cond_root, cond_header, cnt)\n            # recurse\n            mine_tree(cond_header, new_prefix)\n\n    mine_tree(header_table, [])\n\n    # sort items within each itemset (they already are), then sort all itemsets\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    return frequent_itemsets"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list[int],\n                X_test: list[list[float]],\n                k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    # Number of training samples\n    n_train = len(X_train)\n    \n    # Validate k\n    if not isinstance(k, int) or k < 1 or k > n_train:\n        return -1\n\n    # Convert to numpy arrays for vectorized distance computation\n    X_train_arr = np.array(X_train, dtype=float)\n    X_test_arr = np.array(X_test, dtype=float)\n    y_train_arr = np.array(y_train, dtype=int)\n\n    predictions = []\n    # For each test point, compute distances to all train points\n    for x in X_test_arr:\n        # squared Euclidean distances (no need for sqrt)\n        dists = np.sum((X_train_arr - x) ** 2, axis=1)\n        # indices of the k nearest neighbors\n        neigh_idx = np.argsort(dists)[:k]\n        neigh_labels = y_train_arr[neigh_idx].tolist()\n        \n        # majority vote\n        vote_counts = Counter(neigh_labels)\n        max_votes = max(vote_counts.values())\n        # get all labels with max_votes, pick the smallest label on ties\n        tied_labels = [lbl for lbl, cnt in vote_counts.items() if cnt == max_votes]\n        prediction = min(tied_labels)\n        \n        predictions.append(int(prediction))\n\n    return predictions\n```\n\nExplanation of key steps:\n1. Validate `k`: it must be an integer between 1 and `len(X_train)`. Otherwise return `-1`.  \n2. Convert inputs to NumPy arrays for efficient distance computation.  \n3. For each test sample, compute squared Euclidean distances to all training samples.  \n4. Sort distances, pick the indices of the `k` smallest.  \n5. Take a majority vote among the `k` neighbors, and in case of a tie choose the smallest class label.  \n6. Collect and return the predictions as a plain Python list of integers."}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to NumPy arrays of dtype float64\n    y = np.array(y, dtype=np.float64)\n    y_pred = np.array(y_pred, dtype=np.float64)\n\n    # Machine epsilon for float64 to avoid log(0)\n    eps = np.finfo(np.float64).eps\n\n    # Compute the elementwise log (with eps for numerical stability)\n    log_preds = np.log(y_pred + eps)\n\n    # Compute the unnormalised cross-entropy: -sum(y * log(y_pred + eps))\n    loss = -np.sum(y * log_preds)\n\n    # Round to 4 decimal places and return as Python float\n    return float(np.round(loss, 4))\n```\n\nExplanation of the steps:\n1. We first cast both `y` and `y_pred` to NumPy arrays of a floating type.\n2. We obtain machine epsilon (`eps`) for float64 to ensure `log(0)` is never evaluated.\n3. We compute `log(y_pred + eps)` element\u2010wise.\n4. We multiply by the one\u2010hot labels `y` and sum over all samples and classes, then negate to get the cross\u2010entropy.\n5. Finally, we round the result to 4 decimal places and return it as a native Python `float`."}
{"task_id": 77, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> tuple[list[list[float]], list]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    tuple[list[list[float]], list]\n        A tuple (AL, caches), where\n        - AL is the final activation rounded to 4 decimals and converted to a \n          plain Python list of shape (1, m).\n        - caches is a list of caches for each layer (for backprop).\n    \"\"\"\n    def sigmoid(Z: np.ndarray) -> np.ndarray:\n        return 1 / (1 + np.exp(-Z))\n\n    def relu(Z: np.ndarray) -> np.ndarray:\n        return np.maximum(0, Z)\n\n    caches = []\n    A = X\n    # number of layers L inferred from parameters dictionary\n    L = len(parameters) // 2  # assumes keys are W1...WL, b1...bL\n\n    # Forward pass through layers 1 to L-1 with ReLU activation\n    for l in range(1, L):\n        A_prev = A\n        W = parameters[f\"W{l}\"]\n        b = parameters[f\"b{l}\"]\n        Z = W.dot(A_prev) + b\n        A = relu(Z)\n        # Cache the linear and activation values for backprop\n        linear_cache = (A_prev, W, b)\n        activation_cache = Z\n        caches.append((linear_cache, activation_cache))\n\n    # Final layer L with sigmoid activation\n    W = parameters[f\"W{L}\"]\n    b = parameters[f\"b{L}\"]\n    ZL = W.dot(A) + b\n    AL = sigmoid(ZL)\n    linear_cache = (A, W, b)\n    activation_cache = ZL\n    caches.append((linear_cache, activation_cache))\n\n    # Round AL to 4 decimals and convert to plain Python list\n    AL_rounded = np.round(AL, 4).tolist()\n\n    return AL_rounded, caches\n```\n\nExplanation of the main steps:\n\n1. We infer the number of layers `L` from the size of the `parameters` dictionary.\n2. We loop from layer 1 to layer L\u22121, performing:\n   - Linear step: `Z = W\u00b7A_prev + b`\n   - ReLU activation: `A = relu(Z)`\n   - Store `(A_prev, W, b)` and `Z` in `caches`.\n3. For the final layer L, we use a sigmoid activation:\n   - `ZL = WL\u00b7A_(L\u22121) + bL`\n   - `AL = sigmoid(ZL)`\n   - Cache the final layer\u2019s linear and activation values.\n4. We round `AL` to 4 decimal places and convert it to a native Python list of lists before returning it, along with the `caches`."}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as A2.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    # Clip A2 to avoid log(0)\n    eps = 1e-15\n    A2_clipped = np.clip(A2, eps, 1 - eps)\n    \n    # Flatten to 1D arrays\n    A2_flat = A2_clipped.reshape(-1)\n    Y_flat = Y.reshape(-1)\n    \n    # Number of examples\n    m = Y_flat.shape[0]\n    \n    # Compute cost\n    cost = - (1.0 / m) * np.sum(\n        Y_flat * np.log(A2_flat) +\n        (1 - Y_flat) * np.log(1 - A2_flat)\n    )\n    \n    # Round to six decimal places and return as a Python float\n    return round(cost, 6)\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list contains the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    # Convert to 1D numpy array of floats\n    arr = np.array(Q, dtype=float).ravel()\n    n = arr.size\n\n    # Quick checks\n    if n == 0:\n        return []  # no actions -> empty list\n    if not (0.0 <= epsilon <= 1.0):\n        raise ValueError(\"epsilon must be in [0, 1]\")\n\n    # Compute base probability for all actions\n    base = epsilon / n\n\n    # Identify the first (smallest index) greedy action\n    greedy_idx = int(np.argmax(arr))\n\n    # Assign base probability to all, then add the extra mass to the greedy\n    probs = np.full(n, base, dtype=float)\n    probs[greedy_idx] += (1.0 - epsilon)\n\n    # Round to 4 decimals\n    rounded = np.round(probs, 4)\n\n    # Fix any tiny rounding drift so sum == 1 within 1e-12\n    total = rounded.sum()\n    drift = 1.0 - total\n    if abs(drift) > 1e-12:\n        # drift should be a multiple of 1e-4; add it back to the greedy action\n        rounded[greedy_idx] = round(rounded[greedy_idx] + drift, 4)\n\n    # Final check\n    if not np.isclose(rounded.sum(), 1.0, atol=1e-12):\n        raise RuntimeError(f\"Probabilities sum to {rounded.sum()}, not 1\")\n\n    return rounded.tolist()\n```\n\nExplanation of key steps:\n\n1. Convert `Q` to a flat NumPy array so we can leverage vector operations.\n2. Check that `epsilon` is in [0, 1] and handle the corner case of no actions.\n3. Compute the \u201cbase\u201d probability `\u03b5/n` for every action.\n4. Find the first greedy action (`argmax`) and add the extra mass \\(1\u2212\u03b5\\) to its probability.\n5. Round all probabilities to 4 decimal places.\n6. Because rounding can introduce a tiny sum\u2010to\u20101 drift (e.g. 0.9999 or 1.0001), we compute the drift and patch it back into the greedy action. This ensures the final probabilities still sum to exactly 1 (within 1\u00d710\u207b\u00b9\u00b2) and remain at 4\u2010decimal precision."}
{"task_id": 86, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Any, List, Union\n\ndef aggregate_random_forest_votes(\n    predictions: List[List[Union[int, float, str]]]\n) -> List[Union[int, float, str]]:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # If there are no trees or no samples, return an empty list\n    if not predictions or not predictions[0]:\n        return []\n\n    # Transpose so that each tuple in `columns` is the list of votes for one sample\n    columns = zip(*predictions)\n\n    final_predictions: List[Any] = []\n    for votes in columns:\n        # Count how many times each label was predicted\n        cnt = Counter(votes)\n        # Find the highest vote count\n        max_votes = max(cnt.values())\n        # Collect all labels that tied for the top vote count\n        tied_labels = [label for label, v in cnt.items() if v == max_votes]\n        # Break ties by choosing the smallest label (numeric or lexicographic)\n        winner = min(tied_labels)\n        final_predictions.append(winner)\n\n    return final_predictions\n```\n\nExplanation of key steps:\n1. We check for edge cases\u2014if there are no trees or no samples, we return an empty list.\n2. We use `zip(*predictions)` to iterate column-wise across the 2D list; each `votes` tuple holds all tree predictions for one sample.\n3. We build a `Counter` of those votes, find the maximum vote count, and gather all labels that share that top count.\n4. We break any ties by selecting the smallest label (Python\u2019s built-in `min` will do numeric comparison for numbers and lexicographic for strings).\n5. We collect and return all sample\u2010wise winners in a single list."}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z). This implementation\n    uses numpy.logaddexp to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape\n        but is converted to a pure Python list. For scalar inputs a single\n        float is returned.\n    \"\"\"\n    # Convert input to a NumPy array (or 0-d array for scalars)\n    arr = np.asarray(z, dtype=float)\n\n    # Compute softplus via the numerically stable identity\n    y = np.logaddexp(0.0, arr)\n\n    # Round to 4 decimal places\n    y_rounded = np.round(y, 4)\n\n    # If the result is 0-D (scalar), return a Python float\n    if y_rounded.ndim == 0:\n        return float(y_rounded)\n\n    # Otherwise convert to a nested Python list\n    return y_rounded.tolist()\n```\n\nExample usage:\n\n```python\nimport numpy as np\n\nprint(softplus(-1000))             # 0.0\nprint(softplus(0))                 # 0.6931\nprint(softplus([0, 1, 10]))        # [0.6931, 1.3133, 10.0000]\nprint(softplus(np.array([-1000, 0, 3])))  # [0.0, 0.6931, 3.0486]\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    # --- retrieve the policy's estimates ---\n    # try attribute-style\n    estimates = getattr(policy, 'ev_estimates', None)\n    # if not found or None, try dict-style\n    if estimates is None and isinstance(policy, dict):\n        estimates = policy.get('ev_estimates', None)\n\n    # if still missing or not a dict or empty, return nan\n    if not isinstance(estimates, dict) or len(estimates) == 0:\n        return np.nan\n\n    # --- retrieve the true arm expectations ---\n    arm_evs = getattr(bandit, 'arm_evs', None)\n    if arm_evs is None and isinstance(bandit, dict):\n        arm_evs = bandit.get('arm_evs', None)\n\n    # at this point we assume arm_evs is a sequence (list/tuple) of reals\n    # --- compute squared errors ---\n    se_sum = 0.0\n    count = 0\n    for arm_index in sorted(estimates.keys()):\n        # pull out the estimate and the true value\n        est = estimates[arm_index]\n        true = arm_evs[arm_index]\n        diff = est - true\n        se_sum += diff * diff\n        count += 1\n\n    # compute mean\n    mse_val = se_sum / count\n    # round to 4 decimal places\n    return round(mse_val, 4)\n```\n\nExplanation of key steps:\n1. We look for `ev_estimates` first via `getattr`, then (if needed) via dictionary lookup.\n2. If `ev_estimates` is missing, not a dict, or empty, we immediately return `np.nan`.\n3. We similarly obtain `arm_evs` from the bandit.\n4. We iterate over the sorted arm indices in the policy\u2019s estimates so that index _i_ aligns with `arm_evs[i]`.\n5. We accumulate squared errors, compute their average, and round the result to 4 decimal places."}
{"task_id": 96, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive\u2010smoothed log\u2010probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    # 1) Build vocabulary from corpus and add <UNK>\n    vocab = set()\n    for sent in corpus:\n        for w in sent.split():\n            vocab.add(w)\n    vocab.add(\"<UNK>\")\n    V = len(vocab)\n\n    # 2) Build n\u2010gram counts for n = 1..N\n    #    counters[n] will be a Counter over n\u2010gram tuples\n    counters = [Counter() for _ in range(N + 1)]\n    total_unigrams = 0\n    for sent in corpus:\n        tokens = sent.split()\n        total_unigrams += len(tokens)\n        for n in range(1, N + 1):\n            if len(tokens) < n:\n                continue\n            for i in range(len(tokens) - n + 1):\n                ngram = tuple(tokens[i : i + n])\n                counters[n][ngram] += 1\n\n    # 3) Map sequence tokens to <UNK> if unseen\n    seq_tokens = sequence.split()\n    if len(seq_tokens) < N:\n        # no N-grams in the sequence\n        return 0.0\n\n    seq_mapped = [w if w in vocab else \"<UNK>\" for w in seq_tokens]\n\n    # 4) Compute smoothed log-probability\n    log_prob = 0.0\n    for i in range(N - 1, len(seq_mapped)):\n        # context is the preceding N-1 words (or empty tuple if N=1)\n        if N > 1:\n            context = tuple(seq_mapped[i - (N - 1) : i])\n        else:\n            context = ()\n        word = seq_mapped[i]\n        ngram = context + (word,)\n\n        count_ngram = counters[N].get(ngram, 0)\n        if N > 1:\n            count_context = counters[N - 1].get(context, 0)\n        else:\n            # for unigrams, the \"context count\" is total number of tokens\n            count_context = total_unigrams\n\n        prob = (count_ngram + K) / (count_context + K * V)\n        log_prob += math.log(prob)\n\n    # 5) Round to 4 decimals and return\n    return round(log_prob, 4)\n```\n\nExplanation of key steps:\n1. We collect all tokens in the corpus to form our vocabulary, then explicitly add `\"<UNK>\"`.\n2. We build counters for 1-grams through N-grams using `collections.Counter`.\n3. Any word in the target `sequence` not in the training vocabulary is mapped to `\"<UNK>\"`.\n4. We iterate over all contiguous N-grams of the (possibly remapped) sequence, compute the additive-smoothed probability  \n   P(w\u1d62 | context) = (count(context\u25e6w\u1d62) + K) / (count(context) + K\u00b7|V|),  \n   accumulate the natural log, and finally round to 4 decimal places.\n5. If the sequence is shorter than N, we return 0.0 as specified."}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    # Ensure reproducibility\n    np.random.seed(0)\n    N, M = X.shape\n\n    # Initialize factor matrices\n    W = np.random.randn(N, K)\n    H = np.random.randn(K, M)\n\n    # Pre-allocate identity\n    I_K = np.eye(K)\n\n    for iteration in range(max_iter):\n        # 1) Update W by solving: (H H^T + \u03b1 I) W^T = (H X^T)\n        A = H @ H.T + alpha * I_K            # K x K\n        V = X @ H.T                          # N x K\n        # Solve A (W_i)^T = V_i^T for each row i, vectorized:\n        W = np.linalg.solve(A, V.T).T        # (A is symmetric so .T not needed, but for shape)\n\n        # 2) Update H by solving: (W^T W + \u03b1 I) H = W^T X\n        B = W.T @ W + alpha * I_K            # K x K\n        U = W.T @ X                          # K x M\n        H = np.linalg.solve(B, U)            # K x M\n\n        # 3) Compute loss\n        X_hat = W @ H\n        recon_error = np.linalg.norm(X - X_hat, ord='fro')**2\n        reg = alpha * (np.linalg.norm(W, 'fro')**2 + np.linalg.norm(H, 'fro')**2)\n        loss = recon_error + reg\n\n        if loss < tol:\n            break\n\n    # Final reconstruction, rounded and converted to list of lists\n    X_hat = W @ H\n    return X_hat.round(4).tolist()\n```\n\nExplanation of key steps:\n\n1. We initialise `W` and `H` with a fixed random seed for deterministic results.\n2. In each ALS iteration:\n   - We keep `H` fixed and solve a regularised least\u2010squares problem for `W`.  Vectorised, this is solving `(H H\u1d40 + \u03b1I) W\u1d40 = (H X\u1d40)`.\n   - We then keep the new `W` fixed and solve similarly for `H`: `(W\u1d40 W + \u03b1I) H = W\u1d40 X`.\n3. After updating both, we compute the regularised Frobenius\u2010norm loss and stop early if it falls below `tol`.\n4. Finally, we return the reconstructed matrix `W @ H`, rounded to 4 decimals and cast to a Python list of lists."}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n   Perform K-Means clustering with K-Means++ initialisation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with shape (m, n) where *m* is the number of samples and *n* is the\n        number of features.\n    K : int\n        The number of clusters to form.\n    max_iter : int, default = 100\n        Maximum number of iterations for the Lloyd refinement loop.\n    random_state : int | None, default = None\n        Seed for both Python's `random` module and NumPy's RNG to make the result\n        reproducible. If *None*, no seed is set.\n\n    Returns\n    -------\n    list[list[float]]\n        A list of K centroids (each centroid is a list of floats) rounded to 4 decimal\n        places and sorted in ascending order by their first coordinate.\n       # Set seeds for reproducibility\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n\n    m, n = X.shape\n    if K <= 0 or m == 0:\n        return []\n\n    # 1) K-Means++ initialization\n    # Choose the first centroid uniformly at random\n    first_idx = np.random.choice(m)\n    centroids = [X[first_idx].astype(float)]\n\n    # Initialize squared distances to the first centroid\n    diff = X - centroids[0]\n    dist_sq = np.sum(diff * diff, axis=1)\n\n    for _ in range(1, K):\n        # Avoid all-zero distances\n        total = dist_sq.sum()\n        if total <= 0.0:\n            # If all points coincide with centroids, pick random\n            next_idx = np.random.choice(m)\n        else:\n            # Probability proportional to distance squared\n            probs = dist_sq / total\n            next_idx = np.random.choice(m, p=probs)\n        new_cent = X[next_idx].astype(float)\n        centroids.append(new_cent)\n\n        # Update the distance-squared array\n        new_diff = X - new_cent\n        new_dist_sq = np.sum(new_diff * new_diff, axis=1)\n        dist_sq = np.minimum(dist_sq, new_dist_sq)\n\n    centroids = np.vstack(centroids)  # shape (K, n)\n\n    # 2) Lloyd's iterations\n    labels = np.zeros(m, dtype=int)\n    for _ in range(max_iter):\n        # Assign step\n        # Compute squared distances from each point to each centroid\n        # shape (m, K)\n        dists = np.sum((X[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n\n        # Check for convergence\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # Update step\n        for k in range(K):\n            members = (labels == k)\n            if np.any(members):\n                centroids[k] = X[members].mean(axis=0)\n            # else: keep the centroid unchanged for empty clusters\n\n    # 3) Round to 4 decimals and sort\n    centroids = np.round(centroids, 4)\n    # Convert to list of lists\n    cent_list = [list(map(float, centroids[k])) for k in range(K)]\n    # Sort by full tuple (first coordinate primary, then tie-breakers)\n    cent_list_sorted = sorted(cent_list, key=lambda c: tuple(c))\n\n    return cent_list_sorted\n```"}
{"task_id": 111, "completion_id": 0, "completion": "import numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"\n    Encode each character in `text` into its integer token from `vocabulary`.\n    - Unknown characters \u2192 padding token 0.\n    - If text is shorter than time_steps, pad with 0s on the right.\n    - If text is longer, truncate to time_steps.\n    Returns a list of length exactly time_steps.\n    \"\"\"\n    # Initialize with padding tokens\n    encoded = [0] * time_steps\n    for i, ch in enumerate(text[:time_steps]):\n        encoded[i] = vocabulary.get(ch, 0)\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"\n    Decode a sequence of integer token ids into a string using inverse_vocab.\n    - Skip any padding tokens (0).\n    - If an id is not found in inverse_vocab, skip it.\n    \"\"\"\n    chars = []\n    for idx in indices:\n        if idx == 0:\n            continue\n        ch = inverse_vocab.get(int(idx))\n        if ch is not None:\n            chars.append(ch)\n    return \"\".join(chars)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"\n    1) Encode `text` to integer sequence of length TIME_STEPS.\n    2) Run model.predict on batch of size 1.\n    3) Take argmax over last axis to get predicted ids per time step.\n    4) Decode the predicted ids back to a string.\n    Returns the decoded prediction.\n    \"\"\"\n    # 1) Encode input\n    seq_int = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # 2) Create batch shape (1, TIME_STEPS)\n    batch = np.array([seq_int], dtype=np.int32)\n    # 3) Predict: output shape assumed (1, TIME_STEPS, output_dim)\n    preds = model.predict(batch)\n    # 4) Argmax over the last axis \u2192 shape (1, TIME_STEPS)\n    pred_ids = np.argmax(preds, axis=-1)[0]\n    # 5) Decode to string\n    return int_to_string(pred_ids, inv_output_vocabulary)"}
{"task_id": 113, "completion_id": 0, "completion": "```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If None, the function will use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    # Fallback to global EXAMPLES if none provided\n    if examples is None:\n        try:\n            examples = EXAMPLES\n        except NameError:\n            raise NameError(\"Global EXAMPLES is not defined, and no examples were passed in.\")\n\n    predictions = []\n    for example in examples:\n        # run_example is assumed to be defined elsewhere in the environment\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        pred_str = \"\".join(output_chars)\n\n        # Print in the requested format\n        print(f\"input:  {example}\")\n        print(f\"output: {pred_str}\")\n\n        predictions.append(pred_str)\n\n    return predictions\n```\n\nExplanation of key points:\n- We give `examples` a default of `None`, and if it remains `None`, we attempt to pull from the global `EXAMPLES`.\n- For each input string, we call the existing `run_example(...)`, join its returned character list into a single string, print the input/output pair, and collect the result.\n- Finally, we return the list of all predicted strings in the original order."}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n   Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n       eps = 1e-20\n\n    # Ensure y is a flat vector of shape (m,)\n    y_flat = y.reshape(-1)\n\n    # Number of samples\n    m = X.shape[0]\n\n    # Linear combination\n    z = X @ w\n    # If z is (m,1), flatten it to (m,)\n    z = z.reshape(-1)\n\n    # Sigmoid prediction\n    p = 1 / (1 + np.exp(-z))\n\n    # Clip predictions to avoid log(0)\n    p = np.clip(p, eps, 1 - eps)\n\n    # Compute average binary cross-entropy loss\n    loss = - (1.0 / m) * np.sum(y_flat * np.log(p) + (1 - y_flat) * np.log(1 - p))\n    loss_rounded = float(np.round(loss, 4))\n\n    # Compute gradient: shape will be (n,)\n    grad = (X.T @ (p - y_flat)) / m\n    # Reshape to (n, 1) so tolist() yields a nested list\n    grad = grad.reshape(-1, 1)\n    grad_rounded = np.round(grad, 4).tolist()\n\n    return loss_rounded, grad_rounded\n```\n\nExplanation of key steps:\n1. Flatten `y` and `z` so we work with 1D arrays of length `m`.\n2. Compute the sigmoid predictions `p`.\n3. Clip `p` to `[\u03b5, 1\u2212\u03b5]` to avoid taking `log(0)`.\n4. Compute the average binary cross-entropy loss and round it to 4 decimals.\n5. Compute the gradient, reshape it to `(n, 1)` for nested-list output, round to 4 decimals, and convert to a Python list."}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray,\n                     y_train: np.ndarray,\n                     X_test: np.ndarray,\n                     n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels -1 or 1.\n        X_test:  2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    # Ensure we build at least one weak classifier\n    if n_clf < 1:\n        n_clf = 1\n\n    m, n = X_train.shape\n    # Initialize sample weights uniformly\n    D = np.full(m, 1.0 / m)\n\n    # To store the parameters of each stump:\n    # Each stump is a tuple (feature_index, threshold, polarity)\n    stumps = []\n    alphas = []\n\n    for t in range(n_clf):\n        best_error = np.inf\n        best_stump = None\n\n        # Loop over all features to find the best stump\n        for feature_i in range(n):\n            Xf = X_train[:, feature_i]\n            # sort and find candidate thresholds\n            unique_vals = np.unique(Xf)\n            # if there's only one unique value, we still create two thresholds\n            if unique_vals.size == 1:\n                thresholds = [unique_vals[0] - 1e-5, unique_vals[0] + 1e-5]\n            else:\n                # midpoints between consecutive unique feature values\n                thresholds = (\n                    (unique_vals[:-1] + unique_vals[1:]) / 2.0\n                ).tolist()\n                # also consider margins beyond min and max\n                thresholds.insert(0, unique_vals[0] - 1e-5)\n                thresholds.append(unique_vals[-1] + 1e-5)\n\n            # Try both polarities\n            for thr in thresholds:\n                for polarity in [1, -1]:\n                    # Make predictions for this stump\n                    preds = np.ones(m)\n                    if polarity == 1:\n                        preds[Xf < thr] = -1\n                    else:\n                        preds[Xf < thr] = 1\n\n                    # Weighted error\n                    miss = (preds != y_train).astype(float)\n                    error = np.dot(D, miss)\n\n                    # Select the stump with minimal weighted error\n                    if error < best_error:\n                        best_error = error\n                        best_stump = (feature_i, thr, polarity)\n\n        # Avoid division by zero or log of zero\n        eps = 1e-10\n        best_error = max(eps, min(best_error, 1 - eps))\n\n        # Compute alpha\n        alpha = 0.5 * np.log((1.0 - best_error) / best_error)\n\n        # Store the stump and its weight\n        stumps.append(best_stump)\n        alphas.append(alpha)\n\n        # Update sample weights D\n        feature_i, thr, polarity = best_stump\n        Xf = X_train[:, feature_i]\n        preds = np.ones(m)\n        if polarity == 1:\n            preds[Xf < thr] = -1\n        else:\n            preds[Xf < thr] = 1\n\n        # Exponential weight update\n        D *= np.exp(-alpha * y_train * preds)\n        # Renormalize\n        D /= D.sum()\n\n    # Now predict on X_test\n    k = X_test.shape[0]\n    agg = np.zeros(k)\n\n    for stump, alpha in zip(stumps, alphas):\n        feature_i, thr, polarity = stump\n        Xf_test = X_test[:, feature_i]\n        preds = np.ones(k)\n        if polarity == 1:\n            preds[Xf_test < thr] = -1\n        else:\n            preds[Xf_test < thr] = 1\n        agg += alpha * preds\n\n    # Final prediction is sign of the aggregated sum\n    final_preds = np.where(agg >= 0, 1, -1)\n    return final_preds.tolist()\n```\n\nExplanation of key steps:\n\n1. **Initialization**  \n   We start with uniform sample weights \\(D_i = 1/m\\).\n\n2. **Building each stump**  \n   - For each feature, we generate candidate thresholds at the midpoints between sorted unique values (plus two margins).  \n   - For each threshold and polarity (\\(+1\\) or \\(-1\\)), we compute the weighted error  \n     \\(\\varepsilon = \\sum_i D_i [h(x_i)\\neq y_i]\\).  \n   - We pick the stump (feature, threshold, polarity) with the lowest error.\n\n3. **Computing \\(\\alpha_t\\)**  \n   \\[\n     \\alpha_t \\;=\\;\\tfrac12\\ln\\!\\Bigl(\\tfrac{1 - \\varepsilon_t}{\\varepsilon_t}\\Bigr)\n   \\]\n   A small \\(\\epsilon\\) guards against division by zero.\n\n4. **Updating weights**  \n   \\[\n     D_i \\;\\propto\\; D_i \\exp\\bigl(-\\alpha_t\\,y_i\\,h_t(x_i)\\bigr)\n   \\]\n   With renormalization so they sum to 1.\n\n5. **Prediction**  \n   We aggregate stump predictions weighted by \\(\\alpha_t\\) and take the sign:\n   \\(\\hat y = \\mathrm{sign}\\bigl(\\sum_t \\alpha_t\\,h_t(x)\\bigr)\\).\n\nThis yields a list of \\(-1\\)/\\(1\\) predictions for `X_test`."}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n   Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 0 and\n        n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n       # If no components requested, return empty list\n    if n_components == 0:\n        return []\n\n    n_samples, n_features = X.shape\n    classes = np.unique(y)\n    n_classes = len(classes)\n\n    # Compute the global mean\n    mu = X.mean(axis=0)\n\n    # Within-class scatter\n    S_W = np.zeros((n_features, n_features), dtype=float)\n    for c in classes:\n        X_c = X[y == c]\n        mu_c = X_c.mean(axis=0)\n        # Centered data\n        Xc_centered = X_c - mu_c\n        S_W += Xc_centered.T @ Xc_centered\n\n    # Between-class scatter\n    S_B = np.zeros((n_features, n_features), dtype=float)\n    for c in classes:\n        X_c = X[y == c]\n        N_c = X_c.shape[0]\n        mu_c = X_c.mean(axis=0)\n        diff = (mu_c - mu).reshape(-1, 1)\n        S_B += N_c * (diff @ diff.T)\n\n    # Solve the generalised eigenvalue problem by forming A = pinv(S_W) @ S_B\n    S_W_inv = np.linalg.pinv(S_W)\n    A = S_W_inv @ S_B\n\n    # Eigen-decomposition (A is symmetric if S_W and S_B are symmetric)\n    eigvals, eigvecs = np.linalg.eigh(A)\n\n    # Sort eigenpairs in descending order of eigenvalues\n    idx_desc = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[idx_desc]\n    eigvecs = eigvecs[:, idx_desc]\n\n    # Pick the first n_components eigenvectors\n    W = eigvecs[:, :n_components].copy()\n\n    # Deterministic sign: ensure first non-zero entry is positive\n    for i in range(W.shape[1]):\n        vec = W[:, i]\n        # find first element with magnitude > eps\n        for x in vec:\n            if abs(x) > 1e-8:\n                if x < 0:\n                    W[:, i] = -vec\n                break\n\n    # Project the data\n    X_proj = X @ W  # shape (n_samples, n_components)\n\n    # Round to 4 decimal places\n    X_proj_rounded = np.round(X_proj, 4)\n\n    # Convert to list of lists and return\n    return X_proj_rounded.tolist()\n```\n\nExplanation of key steps:\n\n1. **Within-class scatter** `S_W` is built by centering each class\u2019s data and accumulating `(X_c - \u03bc_c)\u1d40 (X_c - \u03bc_c)`.  \n2. **Between-class scatter** `S_B` is accumulated by `(\u03bc_c - \u03bc)(\u03bc_c - \u03bc)\u1d40` weighted by the class size.  \n3. We compute `A = pinv(S_W) @ S_B` and perform an eigen-decomposition with `numpy.linalg.eigh`.  \n4. We sort eigenvalues/eigenvectors in descending order and select the top `n_components` eigenvectors.  \n5. To fix the sign ambiguity, each chosen eigenvector is flipped if its first non-zero entry is negative, ensuring a deterministic result.  \n6. Finally, we project `X` onto the selected directions, round to four decimals, and return the result as a list of lists."}
{"task_id": 140, "completion_id": 0, "completion": "Here is an \\(O(n^2)\\) solution.  First we do a multi\u2010source BFS from all the initial fire cells to compute, for each cell, the earliest time `fire_time[i][j]` at which fire reaches it (walls \u2018#\u2019 never burn so their time stays infinite).  Then we do a second BFS for the agent from \\((0,0)\\), keeping track of the agent\u2019s current time \\(t\\).  We only enqueue a move into cell \\((i,j)\\) at time \\(t+1\\) if:\n\n 1. \\((i,j)\\) is inside the grid and not a wall.\n 2. We have not visited \\((i,j)\\) before.\n 3. `fire_time[i][j] > t+1`, i.e.\\ the fire arrives strictly after the agent.\n\nWe also immediately reject if the start or goal is initially on fire or is a wall.  The code below returns the minimum \\(t\\) when/if the agent reaches \\((n-1,n-1)\\), or \\(-1\\) if it never can.\n\n```python\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    n = len(grid)\n    # trivial empty or 1\u00d71 cases\n    if n == 0:\n        return -1\n    if n == 1:\n        # only escape if it's a free cell '.'\n        return 0 if grid[0][0] == '.' else -1\n\n    # start and goal must be free cells '.' or we can never stand there\n    if grid[0][0] != '.' or grid[n-1][n-1] != '.':\n        return -1\n\n    # 1) Compute fire_time[i][j] = earliest minute fire reaches (i,j)\n    INF = math.inf\n    fire_time = [[INF]*n for _ in range(n)]\n    q = deque()\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_time[i][j] = 0\n                q.append((i, j))\n\n    # Spread fire\n    while q:\n        x, y = q.popleft()\n        t0 = fire_time[x][y]\n        for dx, dy in ((1,0),(-1,0),(0,1),(0,-1)):\n            nx, ny = x+dx, y+dy\n            if 0 <= nx < n and 0 <= ny < n:\n                # fire cannot go through walls\n                if grid[nx][ny] != '#' and fire_time[nx][ny] > t0 + 1:\n                    fire_time[nx][ny] = t0 + 1\n                    q.append((nx, ny))\n\n    # if start or goal is burning at time 0, it's impossible\n    if fire_time[0][0] <= 0 or fire_time[n-1][n-1] <= 0:\n        return -1\n\n    # 2) BFS for the agent\n    visited = [[False]*n for _ in range(n)]\n    visited[0][0] = True\n    q = deque([(0, 0, 0)])  # (x, y, t_agent)\n    while q:\n        x, y, t = q.popleft()\n        # reached goal?\n        if x == n-1 and y == n-1:\n            return t\n        for dx, dy in ((1,0),(-1,0),(0,1),(0,-1)):\n            nx, ny = x+dx, y+dy\n            nt = t + 1\n            if 0 <= nx < n and 0 <= ny < n:\n                # must be a free cell, not visited, and not on fire at arrival\n                if (not visited[nx][ny]\n                   and grid[nx][ny] == '.'\n                   and fire_time[nx][ny] > nt):\n                    visited[nx][ny] = True\n                    q.append((nx, ny, nt))\n\n    return -1\n```\n\nExplanation of the key points:\n\n1. We first flood\u2010fill the fire in increasing time order from all initial \u201cF\u201d cells simultaneously.  \n2. `fire_time[i][j]` ends up being the **earliest** minute at which that cell is burning (or infinity if never reachable by fire).  \n3. In the agent\u2019s BFS we keep `(x,y,t)` and only step into a neighbor at time `t+1` if `fire_time[x][y] > t+1`, because the fire spreads *first* at each minute, then the agent moves.  \n4. We also immediately reject if start or end is a wall or already burning.  \n\nThis runs in \\(O(n^2)\\) time and space."}
{"task_id": 141, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"\n    k-Nearest Neighbors classifier.\n    Parameters\n    ----------\n    X_train : np.ndarray, shape (n_train, n_features)\n        Training data.\n    y_train : np.ndarray, shape (n_train,)\n        Training labels.\n    X_test : np.ndarray, shape (n_test, n_features)\n        Test data to classify.\n    k : int\n        Number of neighbors to use.\n    metric : str, optional (default='euclidean')\n        Distance metric to use: 'euclidean', 'manhattan', or 'cosine'.\n        If unknown, falls back to 'euclidean'.\n    Returns\n    -------\n    np.ndarray, shape (n_test,)\n        Predicted labels for X_test.\n    \"\"\"\n    # Ensure metric is one of the known ones\n    metric = metric.lower()\n    if metric not in ('euclidean', 'manhattan', 'cosine'):\n        metric = 'euclidean'\n\n    X_train = np.asarray(X_train)\n    y_train = np.asarray(y_train)\n    X_test  = np.asarray(X_test)\n\n    n_train, n_feat = X_train.shape\n    n_test = X_test.shape[0]\n\n    # Compute distance matrix of shape (n_test, n_train)\n    if metric == 'euclidean':\n        # We can skip the sqrt if only ranking distances,\n        # but we'll include it for clarity.\n        # Broadcasting: (n_test,1,n_feat) - (1,n_train,n_feat)\n        diffs = X_test[:, None, :] - X_train[None, :, :]\n        dists = np.sqrt(np.sum(diffs**2, axis=2))\n\n    elif metric == 'manhattan':\n        diffs = X_test[:, None, :] - X_train[None, :, :]\n        dists = np.sum(np.abs(diffs), axis=2)\n\n    else:  # cosine\n        # cosine distance = 1 - (x\u00b7y)/(||x|| * ||y||)\n        # compute norms\n        train_norms = np.linalg.norm(X_train, axis=1)\n        test_norms  = np.linalg.norm(X_test,  axis=1)\n        # To avoid division by zero, add a tiny epsilon\n        eps = 1e-12\n        train_norms = np.maximum(train_norms, eps)\n        test_norms  = np.maximum(test_norms, eps)\n        # dot products\n        dots = X_test.dot(X_train.T)  # shape (n_test, n_train)\n        # outer product of norms\n        norm_matrix = np.outer(test_norms, train_norms)  # (n_test, n_train)\n        cos_sim = dots / norm_matrix\n        # clamp numerical errors\n        cos_sim = np.clip(cos_sim, -1.0, 1.0)\n        dists = 1.0 - cos_sim\n\n    # Prepare output array\n    y_pred = np.empty(n_test, dtype=y_train.dtype)\n\n    for i in range(n_test):\n        # indices of k nearest neighbors\n        nn_idx = np.argsort(dists[i])[:k]\n        neighbors = y_train[nn_idx]\n\n        # majority vote\n        labels, counts = np.unique(neighbors, return_counts=True)\n        max_count = counts.max()\n        # tie-break by smallest label\n        winners = labels[counts == max_count]\n        y_pred[i] = winners.min()\n\n    return y_pred\n```"}
{"task_id": 146, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n   Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n       # Basic checks\n    if metric not in ('euclidean', 'manhattan', 'cosine'):\n        raise ValueError(fUnsupported metric '{metric}'.                        Choose from 'euclidean', 'manhattan', 'cosine'.    if X.ndim != 2:\n        raise ValueErrorX must be a 2-D array    if X_test.ndim != 2:\n        raise ValueErrorX_test must be a 2-D array    if X.shape[1] != X_test.shape[1]:\n        raise ValueErrorNumber of features in X and X_test must match    n_train = X.shape[0]\n    if n_train == 0:\n        raise ValueErrorTraining set X must contain at least one sample    if k < 1:\n        raise ValueErrork must be at least 1    # If k is larger than number of training samples, use all\n    k_eff = min(k, n_train)\n\n    # Precompute norms for cosine if needed\n    eps = 1e-12\n    if metric == 'cosine':\n        # Norm of each training vector, add eps to avoid zero-division\n        train_norms = np.linalg.norm(X, axis=1) + eps\n\n    preds = []\n    for x in X_test:\n        # Compute distances from x to all training samples\n        if metric == 'euclidean':\n            # sqrt(sum((X - x)^2, axis=1))\n            diffs = X - x  # shape (n_train, n_features)\n            dists = np.sqrt(np.sum(diffs * diffs, axis=1))\n        elif metric == 'manhattan':\n            # sum(|X - x|, axis=1)\n            dists = np.sum(np.abs(X - x), axis=1)\n        else:  # cosine\n            # 1 - (X\u00b7x) / (||X|| * ||x||)\n            dot = X.dot(x)  # shape (n_train,)\n            x_norm = np.linalg.norm(x) + eps\n            cos_sim = dot / (train_norms * x_norm)\n            dists = 1.0 - cos_sim\n\n        # Find the k_eff nearest neighbours\n        nn_inds = np.argsort(dists)[:k_eff]\n        nn_labels = y[nn_inds]\n\n        # Majority vote with tie-breaking (smallest label wins on ties)\n        unique_labels, counts = np.unique(nn_labels, return_counts=True)\n        # np.unique returns sorted unique_labels, so tie -> smaller label first\n        winner_index = np.argmax(counts)\n        pred_label = unique_labels[winner_index]\n        preds.append(pred_label)\n\n    return preds\n```\n\nExplanation of key steps:\n1. **Input validation**: Check dimensions, matching feature sizes, valid `k`, and supported `metric`.  \n2. **Distance computation**:  \n   - Euclidean: vectorized difference + norm.  \n   - Manhattan: L1 norm.  \n   - Cosine: precompute `||X||`, add small `eps` to avoid division by zero, then compute `1 - cosine_similarity`.  \n3. **k neighbours**: Sort distances, take the first `k` (or all if `k` exceeds the training size).  \n4. **Majority vote**: `np.unique` with `return_counts=True` yields sorted labels; `np.argmax` picks the label with highest count, and in ties the smallest label.  \n5. **Return**: A plain Python list of predictions."}
{"task_id": 155, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is: Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n                        'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # ensure state is a NumPy array\n    x = np.array(state, dtype=float)\n\n    # first affine + ReLU\n    z1 = x.dot(weights['W1']) + weights['b1']\n    a1 = np.maximum(0, z1)\n\n    # second affine + ReLU\n    z2 = a1.dot(weights['W2']) + weights['b2']\n    a2 = np.maximum(0, z2)\n\n    # third affine + tanh\n    z3 = a2.dot(weights['W3']) + weights['b3']\n    a3 = np.tanh(z3)\n\n    # round to 4 decimal places and convert to list\n    return np.round(a3, 4).tolist()\n```"}
{"task_id": 160, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\n\n    Args:\n        X:   2-D array of shape (m, n) \u2013 feature matrix.\n        y:   1-D array of length m \u2013 target values.\n        lambda_param: Regularisation strength \u03bb (non-negative).\n        max_iters: Number of full passes over the coordinates.\n        fit_intercept: Whether to fit an intercept term.\n\n    Returns:\n        Tuple (weights, bias) where `weights` is a list of length n and `bias`\n        is a float.  All returned numbers are rounded to 4 decimal places.\n    \"\"\"\n    m, n = X.shape\n\n    # Precompute squared norms of each feature column\n    col_norms = np.sum(X * X, axis=0)  # shape (n,)\n\n    # Initialize parameter vector: w[0] is intercept, w[1:] are feature weights\n    w = np.zeros(n + 1, dtype=float)\n\n    for _ in range(max_iters):\n        # 1) Update intercept if requested\n        if fit_intercept:\n            # intercept = mean of residuals when features use current weights\n            b = np.mean(y - X.dot(w[1:]))\n            w[0] = b\n        # 2) Maintain current prediction\n        #    y_pred = w0 + X @ w[1:]\n        y_pred = w[0] + X.dot(w[1:])\n\n        # 3) Coordinate descent on each feature weight\n        #    Only w[1] through w[n] are penalised (w[0] is intercept)\n        for j in range(n):\n            old_wj = w[j + 1]\n            # partial residual for coordinate j:\n            #    r = y - (current_pred) + x_j * old_wj\n            # so that x_j^T r = x_j^T (y - (w0 + Xw)) + x_j^T x_j * old_wj\n            residual = y - y_pred + X[:, j] * old_wj\n            rho = X[:, j].dot(residual)\n\n            # Soft-thresholding parameter \u03c4 = \u03bb * m\n            tau = lambda_param * m\n\n            # Soft-thresholding update\n            if rho < -tau:\n                new_wj = (rho + tau) / col_norms[j]\n            elif rho > tau:\n                new_wj = (rho - tau) / col_norms[j]\n            else:\n                new_wj = 0.0\n\n            # Update weight and prediction incrementally\n            w[j + 1] = new_wj\n            y_pred += X[:, j] * (new_wj - old_wj)\n\n    # Final parameters\n    bias = float(np.round(w[0], 4))\n    weights = np.round(w[1:], 4).tolist()\n\n    return weights, bias\n```\n\nExplanation of key steps:\n\n1. We keep a parameter vector `w` of length `n+1`, where `w[0]` is the intercept (bias) and `w[1:]` are the feature coefficients.\n2. In each outer iteration:\n   - If fitting an intercept, we set `w[0] = mean(y \u2212 X @ w[1:])`.\n   - We compute the current predictions `y_pred = w[0] + X.dot(w[1:])`.\n3. We loop over each feature coordinate `j`:\n   - Compute the partial residual that \u201cremoves\u201d the contribution of the old `w[j+1]`.\n   - Compute the inner product `\u03c1 = x_j^T residual`.\n   - Apply the soft\u2010thresholding operator with threshold `\u03c4 = \u03bb\u00b7m`.\n   - Update `w[j+1]` accordingly and adjust `y_pred` incrementally.\n4. After `max_iters` passes, we return the rounded weights and bias.\n\nThis yields a Lasso solution via coordinate descent using only NumPy."}
{"task_id": 165, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function returns -1.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or -1 if the inputs have different lengths.\n    \"\"\"\n    # Convert inputs to NumPy arrays for uniform handling\n    xa = np.asarray(x)\n    ya = np.asarray(y)\n\n    # Check that both are 1-D and of the same length\n    if xa.ndim != 1 or ya.ndim != 1 or xa.shape[0] != ya.shape[0]:\n        return -1\n\n    n = xa.shape[0]\n    # If both are empty, define distance as 0.0\n    if n == 0:\n        return 0.0\n\n    # Compute the number of differing positions\n    diff_count = np.count_nonzero(xa != ya)\n\n    # Normalized distance\n    dist = diff_count / n\n\n    # Round to 4 decimal places\n    return round(dist, 4)\n```\n\nExplanation:\n\n1. We use `np.asarray` to handle both lists and NumPy arrays uniformly.  \n2. We check that both inputs are 1-D and have the same length\u2014if not, return `-1`.  \n3. If the length is zero (both empty), we return `0.0` to avoid division by zero.  \n4. We count the differing positions with `np.count_nonzero(xa != ya)`.  \n5. We divide by the length `n` to normalize, then round to 4 decimal places with `round()`."}
{"task_id": 169, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Name of the desired window. Supported values are\n        \"hamming\", \"hann\", \"blackman_harris\" and \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate. Must be a positive integer.\n    coefficients : list[float] | None, optional\n        List of cosine\u2013series coefficients used **only** when\n        window == \"generalized_cosine\". The default is ``None``.\n\n    Returns\n    -------\n    list[float]\n        List with *N* floats rounded to 4 decimal places.\n    \"\"\"\n    # Validate N\n    if not isinstance(N, int) or N <= 0:\n        raise ValueError(\"N must be a positive integer\")\n    # Special case N == 1\n    if N == 1:\n        return [1.0]\n    # Precompute denominator and constants\n    M = N - 1\n    two_pi_over_M = 2.0 * math.pi / M\n\n    # Dispatch by window type\n    w = []\n    if window == \"hamming\":\n        # w[n] = 0.54 - 0.46*cos(2*pi*n/(N-1))\n        for n in range(N):\n            val = 0.54 - 0.46 * math.cos(two_pi_over_M * n)\n            w.append(round(val, 4))\n\n    elif window == \"hann\":\n        # w[n] = 0.5 - 0.5*cos(2*pi*n/(N-1))\n        for n in range(N):\n            val = 0.5 - 0.5 * math.cos(two_pi_over_M * n)\n            w.append(round(val, 4))\n\n    elif window == \"blackman_harris\":\n        # 4-term Blackman\u2013Harris\n        a0 = 0.35875\n        a1 = 0.48829\n        a2 = 0.14128\n        a3 = 0.01168\n        for n in range(N):\n            x = two_pi_over_M * n\n            val = (a0\n                   - a1 * math.cos(x)\n                   + a2 * math.cos(2.0 * x)\n                   - a3 * math.cos(3.0 * x))\n            w.append(round(val, 4))\n\n    elif window == \"generalized_cosine\":\n        if coefficients is None:\n            raise ValueError(\"Must supply 'coefficients' for generalized_cosine window\")\n        # w[n] = sum_{k=0..len(coefficients)-1} a_k * cos(2*pi*k*n/(N-1))\n        for n in range(N):\n            acc = 0.0\n            for k, a_k in enumerate(coefficients):\n                acc += a_k * math.cos(two_pi_over_M * k * n)\n            w.append(round(acc, 4))\n\n    else:\n        raise ValueError(f\"Unknown window type '{window}'\")\n\n    return w\n```\n\nExplanation of key points:\n- We first validate that `N` is a positive integer; if `N == 1` we return `[1.0]` immediately.\n- For each supported window, we loop `n` from `0` to `N-1`, compute the formula, and round to 4 decimal places.\n- In `generalized_cosine`, we require the `coefficients` argument and form the sum of cosine terms.\n- If an unsupported `window` name is passed, a `ValueError` is raised.\n- The final result is a plain Python `list` of floats with each element rounded to 4 decimal places."}
{"task_id": 171, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n   Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n       # Convert to numpy arrays\n    X = np.array(X_train, dtype=float)\n    y = np.array(y_train)\n    X_t = np.array(X_test, dtype=float)\n\n    n_samples, n_features = X.shape\n\n    # Handle special case: all training labels identical\n    unique_labels = []\n    for label in y_train:\n        if label not in unique_labels:\n            unique_labels.append(label)\n    if len(unique_labels) == 1:\n        # Predict the same label for all test samples\n        return [unique_labels[0]] * X_t.shape[0]\n\n    # Require exactly two distinct labels\n    if len(unique_labels) != 2:\n        raise ValueErrory_train must contain exactly two distinct labels    # Map original labels to {0,1}\n    label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n    inv_map = {0: unique_labels[0], 1: unique_labels[1]}\n    y_bin = np.vectorize(label_map.get)(y)\n\n    # Add intercept term (column of ones) to X and X_t\n    X_bias = np.hstack([np.ones((n_samples, 1)), X])\n    X_test_bias = np.hstack([np.ones((X_t.shape[0], 1)), X_t])\n\n    # Initialize weights (including bias)\n    w = np.zeros(X_bias.shape[1], dtype=float)\n\n    # Sigmoid function\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # Mini-batch gradient descent\n    for epoch in range(epochs):\n        # Shuffle the data at the start of each epoch\n        perm = np.random.permutation(n_samples)\n        X_shuffled = X_bias[perm]\n        y_shuffled = y_bin[perm]\n\n        # Process mini-batches\n        for start in range(0, n_samples, batch_size):\n            end = start + batch_size\n            X_batch = X_shuffled[start:end]\n            y_batch = y_shuffled[start:end]\n\n            # Compute predictions\n            preds = sigmoid(X_batch.dot(w))  # shape (batch,)\n\n            # Gradient of the negative log-likelihood\n            # grad = X^T (sigmoid(Xw) - y) / m\n            error = preds - y_batch\n            grad = X_batch.T.dot(error) / y_batch.size\n\n            # Update weights\n            w -= learning_rate * grad\n\n    # After training, predict on X_test\n    probs = sigmoid(X_test_bias.dot(w))\n    y_pred_bin = (probs >= 0.5).astype(int)\n\n    # Map back to original labels\n    y_pred = [inv_map[int(b)] for b in y_pred_bin]\n\n    return y_pred\n```\n\nExplanation of key steps:\n1. Convert inputs to NumPy arrays for efficient numeric computation.\n2. Detect if all `y_train` labels are identical; if so, skip training and predict that label for all test samples.\n3. Extract exactly two unique labels in order of first appearance and build a mapping `label_map` \u2192 `{0,1}` and inverse mapping `inv_map`.\n4. Add a bias (intercept) column of ones to both training and test feature matrices.\n5. Initialize the weight vector `w` (including bias) to zeros.\n6. For a fixed number of epochs, shuffle the training data and split it into mini-batches. For each batch, compute the sigmoid predictions, form the gradient of the cross\u2010entropy loss, and update `w` by a step of size `learning_rate`.\n7. After training, compute predicted probabilities on `X_test`, threshold them at 0.5 to obtain binary predictions, and map those back to the original label values.\n8. Return the list of predicted labels."}
{"task_id": 176, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n   Train AdaBoost with decision stumps on (X_train, y_train) and predict on X_test    # Convert inputs to numpy arrays\n    X = np.array(X_train, dtype=float)    # shape (n_samples, n_features)\n    y = np.array(y_train, dtype=int)      # shape (n_samples,)\n    n_samples, n_features = X.shape\n    \n    # Map labels {0,1} -> {-1, +1}\n    y_signed = 2 * y - 1\n    \n    # Initialize sample weights uniformly\n    w = np.full(n_samples, 1.0 / n_samples, dtype=float)\n    \n    # To store weak learners and their weights\n    stumps = []      # list of tuples (feature_index, threshold, polarity)\n    alphas = []      # list of alpha_t\n    \n    # Begin boosting rounds\n    for _ in range(n_estimators):\n        best_err = float('inf')\n        best_j = None\n        best_thresh = None\n        best_pol = None\n        best_pred = None  # to store h_t(x_i) for the best stump\n        \n        # Find the best decision stump in this round\n        for j in range(n_features):\n            feature_values = X[:, j]\n            thresholds = np.unique(feature_values)\n            # Loop thresholds in ascending order\n            for thresh in thresholds:\n                # Try both polarities: 1 then -1\n                for pol in (1, -1):\n                    # Compute predictions for this stump\n                    if pol == 1:\n                        # Predict +1 if x < thresh, else -1\n                        pred = np.where(feature_values < thresh, 1, -1)\n                    else:\n                        # Predict +1 if x >= thresh, else -1\n                        pred = np.where(feature_values >= thresh, 1, -1)\n                    \n                    # Compute weighted error\n                    misclassified = (pred != y_signed).astype(float)\n                    err = np.dot(w, misclassified)\n                    \n                    # Tie-breaking:\n                    # prefer smaller error;\n                    # then smaller feature index;\n                    # then smaller threshold;\n                    # then polarity = 1 before -1\n                    is_better = False\n                    if err < best_err - 1e-12:\n                        is_better = True\n                    elif abs(err - best_err) < 1e-12:\n                        # same error, check feature index\n                        if best_j is None or j < best_j:\n                            is_better = True\n                        elif j == best_j:\n                            # same feature, check threshold\n                            if thresh < best_thresh - 1e-12:\n                                is_better = True\n                            elif abs(thresh - best_thresh) < 1e-12:\n                                # same threshold, polarity=1 preferred\n                                if pol == 1 and best_pol == -1:\n                                    is_better = True\n                    \n                    if is_better:\n                        best_err = err\n                        best_j = j\n                        best_thresh = thresh\n                        best_pol = pol\n                        best_pred = pred\n        \n        # Compute alpha for the best stump\n        # Avoid division by zero by adding a small constant\n        alpha = 0.5 * np.log((1.0 - best_err) / (best_err + 1e-10))\n        \n        # Save the stump and its weight\n        stumps.append((best_j, best_thresh, best_pol))\n        alphas.append(alpha)\n        \n        # Update sample weights\n        # w_i <- w_i * exp(-alpha * y_i * h_t(x_i))\n        w *= np.exp(-alpha * y_signed * best_pred)\n        w_sum = w.sum()\n        if w_sum <= 0:\n            # Should not happen, but just in case\n            break\n        w /= w_sum\n        \n        # Early stopping if perfect classifier found\n        if best_err <= 1e-12:\n            break\n    \n    # Now predict on X_test\n    X_t = np.array(X_test, dtype=float)\n    n_test = X_t.shape[0]\n    # Initialize combined score to zero\n    agg = np.zeros(n_test, dtype=float)\n    \n    # Sum weighted predictions of all stumps\n    for (j, thresh, pol), alpha in zip(stumps, alphas):\n        feat_vals = X_t[:, j]\n        if pol == 1:\n            pred_t = np.where(feat_vals < thresh, 1, -1)\n        else:\n            pred_t = np.where(feat_vals >= thresh, 1, -1)\n        agg += alpha * pred_t\n    \n    # Final prediction: sign(agg) -> {+1, -1}, then map to {1, 0}\n    y_pred = (agg >= 0).astype(int)  # True->1, False->0\n    \n    # Return as a plain Python list of ints\n    return y_pred.tolist()\n```\n\nExplanation of key steps:\n1. **Data preparation**: Convert inputs to NumPy arrays and map labels `{0,1}` to `{\u22121,+1}` for the AdaBoost formula.\n2. **Weight initialization**: Start with uniform weights `w_i = 1/n`.\n3. **Weak learner search**: For each feature, each unique threshold, and each polarity, compute the weighted error and apply deterministic tie\u2010breaking.\n4. **Alpha calculation**:  \n   `\u03b1_t = \u00bd \u00b7 ln((1 \u2013 error_t)/(error_t + 1e-10))`\n5. **Weight update**:  \n   `w_i \u2190 w_i \u00b7 exp(\u2212\u03b1_t \u00b7 y_i \u00b7 h_t(x_i))`, then normalize so weights sum to 1.\n6. **Early stopping** if a stump has zero weighted error.\n7. **Prediction**: Aggregate `\u03b1_t \u00b7 h_t(x)` over all stumps, take the sign, and map back to `{0,1}`."}
{"task_id": 178, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Check for invalid inputs\n    if not (isinstance(n_dims, int) and n_dims > 0 and\n            isinstance(num_particles, int) and num_particles > 0 and\n            isinstance(num_iterations, int) and num_iterations > 0):\n        return -1\n\n    # PSO hyperparameters\n    w = 0.5    # inertia weight\n    c1 = 1.5   # cognitive coefficient\n    c2 = 1.5   # social coefficient\n    lower_bound = -1.0\n    upper_bound = 1.0\n\n    # RNG for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # Initialize particle positions uniformly in [\u22121, 1]\n    positions = rng.uniform(lower_bound, upper_bound,\n                            size=(num_particles, n_dims))\n    # Initialize velocities to zero\n    velocities = np.zeros_like(positions)\n\n    # Evaluate initial fitness (Sphere function)\n    fitness = np.sum(positions**2, axis=1)\n\n    # Personal bests start at initial positions\n    pbest_positions = positions.copy()\n    pbest_fitness = fitness.copy()\n\n    # Global best is the best among personal bests\n    gbest_idx = np.argmin(pbest_fitness)\n    gbest_position = pbest_positions[gbest_idx].copy()\n    gbest_fitness = pbest_fitness[gbest_idx]\n\n    # PSO main loop\n    for _ in range(num_iterations):\n        # Draw random coefficients\n        r1 = rng.random((num_particles, n_dims))\n        r2 = rng.random((num_particles, n_dims))\n\n        # Velocity update\n        cognitive = c1 * r1 * (pbest_positions - positions)\n        social    = c2 * r2 * (gbest_position - positions)\n        velocities = w * velocities + cognitive + social\n\n        # Position update and clipping to bounds\n        positions = positions + velocities\n        np.clip(positions, lower_bound, upper_bound, out=positions)\n\n        # Evaluate fitness\n        fitness = np.sum(positions**2, axis=1)\n\n        # Update personal bests\n        better_mask = fitness < pbest_fitness\n        if np.any(better_mask):\n            pbest_positions[better_mask] = positions[better_mask]\n            pbest_fitness[better_mask] = fitness[better_mask]\n\n        # Update global best\n        min_idx = np.argmin(pbest_fitness)\n        min_val = pbest_fitness[min_idx]\n        if min_val < gbest_fitness:\n            gbest_fitness = min_val\n            gbest_position = pbest_positions[min_idx].copy()\n\n    # Return the best fitness found, rounded to four decimals\n    return round(float(gbest_fitness), 4)\n```\n\nExplanation of key steps:\n1. **Input validation:** Ensures all dimensions and counts are positive integers.  \n2. **Initialization:** Particle positions are drawn uniformly in the search bounds \\([-1,1]\\), velocities start at zero.  \n3. **Personal and global bests:** Tracked throughout, updated whenever a particle finds a new lower Sphere value.  \n4. **PSO updates:** Velocities combine inertia, cognitive (personal best), and social (global best) terms with fresh random weights each iteration. Positions are clipped back into the domain.  \n5. **Output:** The best objective value found, rounded to four decimal places, or \\(-1\\) for invalid inputs."}
{"task_id": 180, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    # Convert to numpy arrays for distance computations\n    X_train_np = np.array(X_train, dtype=float)\n    X_test_np = np.array(X_test, dtype=float)\n    y_train_list = list(y_train)\n\n    n_train = X_train_np.shape[0]\n    # Determine effective k\n    if k <= 0 or k > n_train:\n        k_eff = n_train\n    else:\n        k_eff = k\n\n    preds = []\n    for x in X_test_np:\n        # Compute Euclidean distances\n        dists = np.linalg.norm(X_train_np - x, axis=1)\n        # Get indices of the k_eff nearest neighbors\n        nn_idx = np.argsort(dists)[:k_eff]\n        neighbors = [y_train_list[i] for i in nn_idx]\n\n        if task == \"classification\":\n            # Majority vote\n            cnt = Counter(neighbors)\n            max_count = max(cnt.values())\n            # In case of tie, min() picks the smallest label lexicographically/numerically\n            top_labels = [label for label, c in cnt.items() if c == max_count]\n            pred = min(top_labels)\n        elif task == \"regression\":\n            # Average and round to 4 decimals\n            avg_val = sum(neighbors) / k_eff\n            pred = round(avg_val, 4)\n        else:\n            raise ValueError(\"task must be 'classification' or 'regression'\")\n\n        preds.append(pred)\n\n    return preds\n```"}
{"task_id": 184, "completion_id": 0, "completion": "Here is a self\u2010contained implementation of a binary CART tree using Gini impurity and recursive splitting. It builds a small tree \u201cfrom scratch\u201d and then uses it to predict on `X_test`.  \n\n```python\nimport numpy as np\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n   Build a CART decision tree on (X_train, y_train) and predict labels for X_test    \n    # Define a tree node\n    class Node:\n        __slots__ =is_leafpredictionfeaturethresholdleftright        def __init__(self, *, is_leaf, prediction=None, feature=None, threshold=None, left=None, right=None):\n            self.is_leaf = is_leaf\n            self.prediction = prediction\n            self.feature = feature\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n\n    def gini(y):\n       Compute Gini impurity for labels y        if y.size == 0:\n            return 0.0\n        counts = np.bincount(y)\n        p = counts / counts.sum()\n        return 1.0 - np.sum(p * p)\n\n    def majority_class(y):\n       Return the majority class label in y (ties broken by smaller label).        counts = np.bincount(y)\n        return int(np.argmax(counts))\n\n    def best_split(X, y):\n       Find the best (feature, threshold, gain) to split on        n_samples, n_features = X.shape\n        parent_gini = gini(y)\n        best = (None, None, 0.0)  # feature, threshold, best_gain\n\n        # If all labels are identical, no split is possible\n        if parent_gini == 0.0:\n            return best\n\n        for feat in range(n_features):\n            # sort samples by feature\n            sorted_idx = X[:, feat].argsort()\n            Xf = X[sorted_idx, feat]\n            yf = y[sorted_idx]\n            \n            # we will test thresholds between distinct consecutive Xf\n            for i in range(1, n_samples):\n                if Xf[i] == Xf[i-1]:\n                    continue\n                thr = (Xf[i] + Xf[i-1]) / 2.0\n                \n                # split\n                left_mask = X[:, feat] <= thr\n                y_left = y[left_mask]\n                y_right = y[~left_mask]\n                if y_left.size == 0 or y_right.size == 0:\n                    continue\n\n                # weighted Gini\n                g_left = gini(y_left)\n                g_right = gini(y_right)\n                w_left = y_left.size / n_samples\n                w_right = 1.0 - w_left\n                gain = parent_gini - (w_left * g_left + w_right * g_right)\n\n                if gain > best[2]:\n                    best = (feat, thr, gain)\n\n        return best  # (feature, threshold, gain)\n\n    def build_tree(X, y, depth):\n       Recursive tree builder        # stopping: all same label\n        if y.size == 0:\n            # Should not really happen in normal splits\n            return Node(is_leaf=True, prediction=0)\n        if np.all(y == y[0]):\n            return Node(is_leaf=True, prediction=int(y[0]))\n        # stopping: max depth reached\n        if max_depth is not None and depth >= max_depth:\n            return Node(is_leaf=True, prediction=majority_class(y))\n\n        # find best split\n        feat, thr, gain = best_split(X, y)\n        if feat is None or gain <= 0.0:\n            # no useful split\n            return Node(is_leaf=True, prediction=majority_class(y))\n\n        # split data\n        left_mask = X[:, feat] <= thr\n        X_left, y_left = X[left_mask], y[left_mask]\n        X_right, y_right = X[~left_mask], y[~left_mask]\n\n        # if a child is empty, make leaf\n        if y_left.size == 0 or y_right.size == 0:\n            return Node(is_leaf=True, prediction=majority_class(y))\n\n        # build subtrees\n        left_node = build_tree(X_left, y_left, depth + 1)\n        right_node = build_tree(X_right, y_right, depth + 1)\n        return Node(is_leaf=False, feature=feat, threshold=thr, left=left_node, right=right_node)\n\n    def predict_one(x, node):\n       Traverse the tree to make a single prediction        while not node.is_leaf:\n            if x[node.feature] <= node.threshold:\n                node = node.left\n            else:\n                node = node.right\n        return node.prediction\n\n    # build the tree on the training data\n    root = build_tree(X_train, y_train, depth=0)\n\n    # predict for each test sample\n    preds = [predict_one(x, root) for x in X_test]\n    return preds\n```\n\nExplanation of key points:\n\n1. **Node class**  \n   A simple object holding either a leaf prediction or a split `(feature, threshold)` and child nodes.\n\n2. **Gini impurity**  \n   `gini(y)` computes 1 \u2212 \u03a3 p\u1d62\u00b2 for the label distribution in `y`.\n\n3. **Best split search**  \n   For each feature, sort the data and try thresholds halfway between each pair of distinct values. Compute the weighted Gini of the two child groups and pick the split that maximizes \u201cGini gain.\u201d\n\n4. **Stopping criteria**  \n   - Pure node (all `y` equal)  \n   - Reached `max_depth` (if given)  \n   - No positive gain split or an empty child\n\n5. **Prediction**  \n   A simple tree\u2010walk from the root to a leaf for each test sample.\n\nThis function returns a plain Python list of integer labels for `X_test`."}
{"task_id": 190, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n   Find the best feature index and threshold that minimise the weighted Gini impurity.\n\n    Parameters\n    ----------\n    X : list[list[float]] or numpy.ndarray\n        A 2-D structure where each inner list/row contains the numerical\n        feature values of one sample.\n    y : list[int] or numpy.ndarray\n        A 1-D structure containing the class labels corresponding to the rows\n        of X.\n\n    Returns\n    -------\n    tuple\n        A tuple `(best_feature_index, best_threshold_value, best_gini)` where\n        `best_gini` is rounded to 4 decimal places.  If no valid split exists\n        the function returns `(-1, None, round(G_whole_dataset, 4))`.\n       X_arr = np.array(X, copy=False)\n    y_arr = np.array(y, copy=False)\n    n_samples, n_features = X_arr.shape\n\n    # Map class labels to 0..(n_classes-1)\n    classes, y_mapped = np.unique(y_arr, return_inverse=True)\n    n_classes = classes.size\n\n    # Compute Gini of the whole dataset\n    total_counts = np.bincount(y_mapped, minlength=n_classes)\n    probs_whole = total_counts / n_samples\n    G_whole = 1.0 - np.sum(probs_whole * probs_whole)\n\n    # If only one class present, no valid split by spec\n    if n_classes <= 1:\n        return (-1, None, round(G_whole, 4))\n\n    best_gini = np.inf\n    best_j = -1\n    best_v = None\n    tol = 1e-12\n\n    # Iterate over features\n    for j in range(n_features):\n        Xj = X_arr[:, j]\n        # sort samples by feature j\n        order = np.argsort(Xj, kind='mergesort')\n        x_sorted = Xj[order]\n        y_sorted = y_mapped[order]\n\n        # Get unique feature values, their first indices and counts\n        unique_vals, start_idxs, counts = np.unique(\n            x_sorted, return_index=True, return_counts=True\n        )\n\n        left_counts = np.zeros(n_classes, dtype=int)\n        n_left = 0\n\n        # For each unique threshold value v\n        for v, start, cnt in zip(unique_vals, start_idxs, counts):\n            grp = y_sorted[start : start + cnt]\n            # update left node counts\n            bc = np.bincount(grp, minlength=n_classes)\n            left_counts += bc\n            n_left += cnt\n            n_right = n_samples - n_left\n\n            # skip invalid splits\n            if n_left == 0 or n_right == 0:\n                continue\n\n            # Gini of left\n            p_left = left_counts / n_left\n            G_left = 1.0 - np.sum(p_left * p_left)\n            # Gini of right\n            right_counts = total_counts - left_counts\n            p_right = right_counts / n_right\n            G_right = 1.0 - np.sum(p_right * p_right)\n\n            # Weighted Gini\n            G_split = (n_left / n_samples) * G_left + (n_right / n_samples) * G_right\n\n            # Compare with best split found so far\n            if G_split + tol < best_gini:\n                best_gini = G_split\n                best_j = j\n                best_v = v\n            elif abs(G_split - best_gini) < tol:\n                # tie-breaker: smaller feature index, then smaller threshold\n                if (j < best_j) or (j == best_j and v < best_v):\n                    best_j = j\n                    best_v = v\n\n    # If no valid split found, return whole-data Gini\n    if best_j == -1:\n        return (-1, None, round(G_whole, 4))\n\n    # Ensure threshold is a native Python scalar\n    if hasattr(best_v, 'item'):\n        best_v = best_v.item()\n\n    return (best_j, best_v, round(best_gini, 4))\n```"}
{"task_id": 191, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n    # normalize metric name\n    key = name.lower()\n    supported = {'euclidean', 'manhattan', 'chebyshev', 'cosine'}\n    if key not in supported:\n        # return a stub that always fails\n        def _bad(a, b):\n            return -1\n        return _bad\n\n    def _distance(a, b):\n        # 1) Type check: must be list, tuple or np.ndarray\n        if not isinstance(a, (list, tuple, np.ndarray)) or \\\n           not isinstance(b, (list, tuple, np.ndarray)):\n            return -1\n\n        # 2) Convert to float arrays\n        try:\n            xa = np.array(a, dtype=float)\n            ya = np.array(b, dtype=float)\n        except Exception:\n            return -1\n\n        # 3) One-dimensional, non-empty, same length\n        if xa.ndim != 1 or ya.ndim != 1:\n            return -1\n        if xa.size == 0 or ya.size == 0:\n            return -1\n        if xa.shape[0] != ya.shape[0]:\n            return -1\n\n        # 4) Compute desired metric\n        if key == 'euclidean':\n            diff = xa - ya\n            val = np.linalg.norm(diff)\n        elif key == 'manhattan':\n            val = np.sum(np.abs(xa - ya))\n        elif key == 'chebyshev':\n            val = np.max(np.abs(xa - ya))\n        else:  # cosine\n            norm_x = np.linalg.norm(xa)\n            norm_y = np.linalg.norm(ya)\n            if norm_x == 0 or norm_y == 0:\n                return -1\n            cos_sim = np.dot(xa, ya) / (norm_x * norm_y)\n            val = 1.0 - cos_sim\n\n        # 5) Round result to 4 decimals and return\n        return round(float(val), 4)\n\n    return _distance\n```\n\nExplanation of key points:\n\n1. **Metric selection**: We first lowercase the provided name and check it against the supported set. If it\u2019s not supported, we immediately return a stub function that always yields -1.\n\n2. **Validation in the returned function**:\n   - Both inputs must be `list`, `tuple`, or `np.ndarray`.\n   - We attempt to cast them to NumPy arrays of dtype `float`; failure \u2192 -1.\n   - We enforce 1D arrays (`ndim == 1`), non-empty (`size > 0`), and equal lengths.\n   - For the cosine distance only, we check that neither vector has zero Euclidean norm.\n\n3. **Distance computations**: Each formula is implemented via NumPy operations. Finally, we round to four decimal places with Python\u2019s built-in `round` and return that as a `float`.\n\n4. **Error signaling**: Any violation of the above rules yields an immediate `-1` (integer) return."}
{"task_id": 197, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(\n    env: Any,\n    md_action: bool,\n    cont_action: bool\n) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional (i.e., env.action_space.spaces exists).\n        cont_action: ``True`` if the action space is continuous (infinite number of actions).\n\n    Returns:\n        A tuple ``(n_actions_per_dim, action_ids, action_dim)`` where\n            \u2022 n_actions_per_dim: list with the number of actions in every dimension\n                                  (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product) or ``None`` if\n                           any dimension is continuous,\n            \u2022 action_dim: number of action dimensions.\n    \"\"\"\n    # Multi-dimensional case: inspect each sub-space\n    if md_action:\n        subspaces = env.action_space.spaces\n        action_dim = len(subspaces)\n        n_actions_per_dim: List[Union[int, float]] = []\n        saw_cont = False\n\n        for sp in subspaces:\n            if hasattr(sp, \"n\"):\n                # discrete sub-space\n                n_actions_per_dim.append(sp.n)\n            elif hasattr(sp, \"shape\"):\n                # continuous sub-space\n                n_actions_per_dim.append(np.inf)\n                saw_cont = True\n            else:\n                raise ValueError(\"Sub-space is neither discrete nor continuous\")\n\n        # If any sub-space is continuous, we cannot enumerate all actions\n        if saw_cont:\n            action_ids = None\n        else:\n            # fully discrete: cartesian product over all dims\n            ranges = [range(int(n)) for n in n_actions_per_dim]  # cast to int just in case\n            action_ids = list(product(*ranges))\n\n        return n_actions_per_dim, action_ids, action_dim\n\n    # Single-dimensional case\n    else:\n        space = env.action_space\n\n        # Continuous single space: may itself be multi-dimensional (e.g. Box(shape=(d,)))\n        if cont_action:\n            # shape[0] tells us how many continuous dimensions\n            shape = getattr(space, \"shape\", ())\n            if not shape:\n                # fallback: treat as a single scalar dimension\n                action_dim = 1\n            else:\n                action_dim = int(shape[0])\n\n            n_actions_per_dim = [np.inf] * action_dim\n            action_ids = None\n            return n_actions_per_dim, action_ids, action_dim\n\n        # Discrete single space\n        else:\n            if not hasattr(space, \"n\"):\n                raise ValueError(\"Expected a discrete space with attribute 'n'\")\n            n = int(space.n)\n            action_dim = 1\n            n_actions_per_dim = [n]\n            # enumerate all discrete actions as 1-tuples\n            action_ids = [(i,) for i in range(n)]\n            return n_actions_per_dim, action_ids, action_dim\n```\n\nExplanation of the main steps:\n\n1. If `md_action` is True, we assume `env.action_space.spaces` exists and loop over each sub-space:\n   - If the sub-space has `.n`, it\u2019s discrete, so we append that integer.\n   - If it has `.shape`, it\u2019s continuous, so we append `np.inf` and mark that we saw a continuous dimension.\n   - If any sub-space is continuous, we set `action_ids = None`. Otherwise we build the cartesian product of all discrete ranges.\n\n2. If `md_action` is False:\n   - If `cont_action` is True, we read `space.shape` (if present) to find out how many continuous dimensions there are, set all `n_actions_per_dim` entries to `np.inf`, and return `action_ids=None`.\n   - Otherwise it\u2019s a single discrete dimension with `.n`; we return that along with the list of 1-tuples for all valid actions.\n\nIn all cases we return `(n_actions_per_dim, action_ids, action_dim)` without mutating any inputs."}
{"task_id": 198, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    # If there are no documents, there are no topics to estimate.\n    if not phi:\n        # return V rows of empty topic lists\n        return [[] for _ in range(V)]\n\n    # Number of topics inferred from phi of the first document\n    T = phi[0].shape[1]\n\n    # Accumulator for the counts: rows=vocabulary, cols=topics\n    counts = np.zeros((V, T), dtype=float)\n\n    # Sum over all documents and all tokens\n    for doc_phi, doc_words in zip(phi, corpus):\n        # doc_phi.shape == (N_d, T), len(doc_words) == N_d\n        for n, word_idx in enumerate(doc_words):\n            counts[word_idx, :] += doc_phi[n, :]\n\n    # Column-normalise: each topic t sums over all words v to 1\n    col_sums = counts.sum(axis=0)      # shape (T,)\n    for t in range(T):\n        if col_sums[t] > 0:\n            counts[:, t] /= col_sums[t]\n        # else: leave the column as all zeros (no mass for that topic)\n\n    # Round to 4 decimal places\n    beta = np.round(counts, 4)\n\n    # Convert to a nested list: beta[v][t]\n    return beta.tolist()\n```\n\nExplanation of key steps:\n\n1. **Initialization**  \n   We create a zero matrix `counts` of shape `(V, T)`.\n\n2. **Accumulation**  \n   For each document `d` and each token position `n`, we read the word index `v = corpus[d][n]` and add the vector `\u03c6^{(d)}_{n, :}` to `counts[v, :]`.\n\n3. **Normalization**  \n   After summing up all contributions, each column `t` in `counts` holds the unnormalized counts for topic `t`. We divide by the column sum so that each column sums to 1. If the column sum is zero (no mass for that topic), we leave it as all zeros.\n\n4. **Rounding and Output**  \n   We round each entry to 4 decimal places and convert the NumPy array to a nested Python list, where each inner list corresponds to one vocabulary word and its probabilities across the `T` topics."}
{"task_id": 202, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\"\"\"\n    n_samples = X.shape[0]\n    if k < 1 or k > n_samples:\n        raise ValueError(\"k must be between 1 and n_samples\")\n    # Step 1: initial medoids are the first k samples\n    medoids = list(range(k))\n    medoid_set = set(medoids)\n\n    # Precompute full pairwise distance matrix\n    # D[i, j] = distance between X[i] and X[j]\n    diff = X[:, None, :] - X[None, :, :]\n    D = np.sqrt(np.sum(diff * diff, axis=2))\n\n    # Function to compute total cost given a list of medoid indices\n    def total_cost(meds):\n        # distances of each point to its nearest medoid\n        dm = D[:, meds]           # shape (n_samples, k)\n        return float(np.min(dm, axis=1).sum())\n\n    # Initial cost\n    current_cost = total_cost(medoids)\n\n    # Step 2: try swaps until no improvement\n    while True:\n        best_delta = 0.0\n        best_swap = None  # tuple (medoid_list_index, new_medoid_idx, new_cost)\n\n        # For each medoid position i_med in the medoids list\n        for i_med, m in enumerate(medoids):\n            # For each non-medoid candidate o\n            for o in range(n_samples):\n                if o in medoid_set:\n                    continue\n                # form new medoid list by swapping medoids[i_med] -> o\n                cand = medoids.copy()\n                cand[i_med] = o\n                # compute its cost\n                c_cost = total_cost(cand)\n                delta = current_cost - c_cost\n                # keep the swap with largest positive cost reduction\n                if delta > best_delta:\n                    best_delta = delta\n                    best_swap = (i_med, o, c_cost)\n\n        # If we found a strictly improving swap, apply it\n        if best_swap is not None:\n            i_med, new_medoid, new_cost = best_swap\n            old_medoid = medoids[i_med]\n            medoids[i_med] = new_medoid\n            medoid_set.remove(old_medoid)\n            medoid_set.add(new_medoid)\n            current_cost = new_cost\n        else:\n            # no improving swap found; we're done\n            break\n\n    # Step 3: assign labels based on final medoids\n    # For each sample, find the index of the closest medoid in the medoids list\n    distances_to_meds = D[:, medoids]   # shape (n_samples, k)\n    labels = np.argmin(distances_to_meds, axis=1)\n\n    # Convert to Python list of ints\n    return [int(l) for l in labels]\n```\n\nExplanation of key steps:\n1. We initialize the medoids to the first `k` indices.\n2. We precompute the full `n\u00d7n` distance matrix `D` so that cost evaluations inside the swap loops are fast.\n3. In each iteration, we consider every possible single swap between a current medoid and a non-medoid. We pick the swap that yields the largest reduction in total cost, and apply it if that reduction is strictly positive.\n4. Once no swap can improve the cost, we assign each point to the nearest medoid (breaking ties by the medoid\u2019s position in the current list) and return the labels."}
{"task_id": 216, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test,\n                           degree=1,\n                           reg_factor=0.05,\n                           l1_ratio=0.5,\n                           n_iterations=3000,\n                           learning_rate=0.01):\n    \"\"\"Elastic Net regression implemented with batch gradient descent.\n\n    Parameters\n    ----------\n    x_train : list[list[float]] | np.ndarray\n        Training feature matrix where each sub-list is a sample.\n    y_train : list[float] | np.ndarray\n        Target values for every row in `x_train`.\n    x_test : list[list[float]] | np.ndarray\n        Matrix of samples to predict after training.\n    degree : int, default 1\n        Degree of polynomial expansion applied to every original feature.\n    reg_factor : float, default 0.05\n        Overall regularisation strength (alpha).\n    l1_ratio : float, default 0.5\n        Portion of L1 penalty in Elastic Net (0 = pure ridge, 1 = pure lasso).\n    n_iterations : int, default 3000\n        Number of gradient descent iterations.\n    learning_rate : float, default 0.01\n        Step size used in each gradient update.\n\n    Returns\n    -------\n    list[float]\n        Predictions for `x_test` rounded to 4 decimals. Returns `-1` if the\n        inputs are invalid (different feature counts, or non-positive\n        `n_iterations`).\n    \"\"\"\n    # Convert inputs to numpy arrays\n    Xtr = np.array(x_train, dtype=float)\n    ytr = np.array(y_train, dtype=float).reshape(-1)\n    Xte = np.array(x_test, dtype=float)\n\n    # Basic validation\n    if n_iterations < 1:\n        return -1\n    if Xtr.ndim != 2 or Xte.ndim != 2:\n        return -1\n    n_tr, p_tr = Xtr.shape\n    n_te, p_te = Xte.shape\n    if p_tr != p_te:\n        return -1\n    if ytr.shape[0] != n_tr:\n        return -1\n\n    # 1) Polynomial feature expansion (no interaction terms)\n    def poly_expand(X, deg):\n        if deg == 1:\n            return X.copy()\n        Xs = [X**d for d in range(1, deg+1)]\n        return np.hstack(Xs)\n\n    Xtr_poly = poly_expand(Xtr, degree)\n    Xte_poly = poly_expand(Xte, degree)\n\n    # 2) Standard-score (zero-mean, unit-variance) on non-bias features\n    means = Xtr_poly.mean(axis=0)\n    stds = Xtr_poly.std(axis=0)\n    stds_fixed = np.where(stds == 0, 1.0, stds)\n    Xtr_s = (Xtr_poly - means) / stds_fixed\n    Xte_s = (Xte_poly - means) / stds_fixed\n\n    # 3) Add bias column\n    Xtr_aug = np.hstack([np.ones((n_tr, 1)), Xtr_s])\n    Xte_aug = np.hstack([np.ones((n_te, 1)), Xte_s])\n\n    # Initialize weights (including bias)\n    n_features = Xtr_aug.shape[1]\n    w = np.zeros(n_features, dtype=float)\n\n    alpha = reg_factor\n    r = l1_ratio\n\n    # Batch gradient descent\n    for _ in range(n_iterations):\n        preds = Xtr_aug.dot(w)             # shape (n_tr,)\n        err = preds - ytr                  # shape (n_tr,)\n        # Gradient of MSE = (2/n) * X^T * err\n        grad = (2.0 / n_tr) * (Xtr_aug.T.dot(err))\n        # Add Elastic Net penalty gradient for j >= 1 (no reg on bias)\n        # d/dw_j [ alpha*( (1-r)/2 * w_j^2 + r*|w_j| ) ]\n        # = alpha * [ (1-r)*w_j + r*sign(w_j) ]\n        sign_w = np.sign(w)\n        grad[1:] += alpha * ((1 - r) * w[1:] + r * sign_w[1:])\n        # Update weights\n        w -= learning_rate * grad\n\n    # 4) Predict on test set\n    y_pred = Xte_aug.dot(w)\n    return np.round(y_pred, 4).tolist()\n```\n\nExplanation of key steps:\n1. **Validation**: Checks that the number of features in `x_train` and `x_test` match, and that `n_iterations` is positive.\n2. **Polynomial Expansion**: Raises each original feature to powers 1 through `degree` and concatenates them as new features.\n3. **Standardisation**: Computes mean and standard\u2010deviation on the expanded training features, transforms both train and test. Any feature with zero variance is left at unit scale.\n4. **Bias Term**: Prepends a column of ones so the bias weight (intercept) is learned alongside other weights.\n5. **Gradient Descent**:\n   - MSE gradient: `(2/n) * X^T (Xw - y)`\n   - Elastic Net gradient for non-bias weights: `\u03b1[(1\u2212l1_ratio)\u00b7w_j + l1_ratio\u00b7sign(w_j)]`\n6. **Prediction**: Applies learned weights to the test features and rounds results to 4 decimal places."}
{"task_id": 217, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n   Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers (+1 or -1).\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n       # Convert inputs to NumPy arrays (float)\n    actual_arr = np.asarray(actual, dtype=float)\n    pred_arr = np.asarray(predicted, dtype=float)\n\n    # Compute sigmoid(probabilities) for each predicted score\n    prob = 1.0 / (1.0 + np.exp(-pred_arr))\n\n    # Compute Hessian: sigmoid(p) * (1 - sigmoid(p))\n    hess = prob * (1.0 - prob)\n\n    # Compute gradient: actual * sigmoid(- actual * predicted)\n    # Note: sigmoid(-a*p) = 1 / (1 + exp(a * p))\n    grad = actual_arr * (1.0 / (1.0 + np.exp(actual_arr * pred_arr)))\n\n    # Round each array to 6 decimal places\n    grad_rounded = np.round(grad, 6)\n    hess_rounded = np.round(hess, 6)\n    prob_rounded = np.round(prob, 6)\n\n    # Convert back to Python lists and return\n    return grad_rounded.tolist(), hess_rounded.tolist(), prob_rounded.tolist()\n```"}
{"task_id": 218, "completion_id": 0, "completion": "```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function must follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n    def hand_value(cards: list[int]) -> int:\n        total = sum(cards)\n        # if there's at least one Ace and counting it as 11 (i.e. +10) doesn't bust,\n        # use that \"usable ace\" scenario\n        if 1 in cards and total + 10 <= 21:\n            return total + 10\n        return total\n\n    def is_natural(cards: list[int]) -> bool:\n        # exactly two cards: one Ace (1) and one 10-value card (10)\n        return len(cards) == 2 and 1 in cards and 10 in cards\n\n    p_score = hand_value(player)\n    d_score = hand_value(dealer)\n    p_nat = is_natural(player)\n    d_nat = is_natural(dealer)\n\n    # 1. Player busts -> -1\n    if p_score > 21:\n        return -1.0\n\n    # 4. Player wins with a natural blackjack -> +1.5 (unless dealer also has a natural)\n    if p_nat and not d_nat:\n        return 1.5\n\n    # 2. Dealer busts (and player did not) -> +1\n    if d_score > 21:\n        return 1.0\n\n    # 3. Compare scores\n    if p_score > d_score:\n        return 1.0\n    elif p_score < d_score:\n        return -1.0\n    else:\n        # tie\n        return 0.0\n```"}
{"task_id": 221, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n       Compute the element\u2010wise squared logarithmic errors between two arrays.\n\n    Parameters\n    ----------\n    actual : array\u2010like of shape (n,)\n        True non\u2010negative target values.\n    predicted : array\u2010like of shape (n,)\n        Predicted non\u2010negative values.\n\n    Returns\n    -------\n    np.ndarray of shape (n,)\n        (log(1 + actual) - log(1 + predicted))**2 for each element.\n       # Convert inputs to NumPy arrays (no validation here)\n    a = np.asarray(actual, dtype=float)\n    p = np.asarray(predicted, dtype=float)\n    return (np.log1p(a) - np.log1p(p)) ** 2\n\n\ndef mean_squared_log_error(actual, predicted):\n       Calculate Mean Squared Logarithmic Error (MSLE).\n\n    MSLE = mean_i [ log(1 + actual_i) \u2212 log(1 + predicted_i) ]^2\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence of true values.  All elements must be non\u2010negative.\n    predicted : list | tuple | np.ndarray\n        Sequence of predicted values.  Must be the same length as\n        `actual` and contain only non\u2010negative numbers.\n\n    Returns\n    -------\n    float\n        The MSLE rounded to 4 decimals.  If the inputs are invalid the\n        function returns -1.\n       # Convert to NumPy arrays\n    try:\n        act = np.asarray(actual, dtype=float)\n        pred = np.asarray(predicted, dtype=float)\n    except Exception:\n        return -1\n\n    # Validation checks\n    if act.shape != pred.shape:\n        return -1\n    # We expect a 1D sequence\n    if act.ndim != 1:\n        return -1\n    # Non\u2010empty\n    if act.size == 0:\n        return -1\n    # All values must be non\u2010negative\n    if np.any(act < 0) or np.any(pred < 0):\n        return -1\n\n    # Compute squared log errors\n    sq_errors = squared_log_error(act, pred)\n\n    # Compute mean and round to 4 decimal places\n    msle = float(np.mean(sq_errors))\n    return round(msle, 4)\n```\n\nExplanation of key points:\n\n1. **Input conversion**: We first coerce inputs to NumPy arrays of type `float`.  \n2. **Validation**:  \n   - Shapes must match and be 1D.  \n   - Sequences must be non\u2010empty.  \n   - All elements must be \u2265 0.  \n   If any check fails, return `-1`.  \n3. **Computation**:  \n   - The helper `squared_log_error` computes `(log1p(actual) - log1p(predicted))**2` element\u2010wise.  \n   - The main function takes the mean of these squared errors, casts to a Python `float`, and rounds to 4 decimal places."}
{"task_id": 222, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    # Check if input is a scalar (int, float, or numpy scalar)\n    if np.isscalar(z):\n        z_val = float(z)\n        # numerically stable sigmoid for a scalar\n        if z_val >= 0:\n            exp_neg = math.exp(-z_val)\n            s = 1.0 / (1.0 + exp_neg)\n        else:\n            exp_pos = math.exp(z_val)\n            s = exp_pos / (1.0 + exp_pos)\n        return round(s, 4)\n\n    # Otherwise treat input as array-like\n    arr = np.array(z, dtype=np.float64)\n    # Prepare output array same shape as arr\n    s = np.empty_like(arr)\n\n    # Masks for positive and negative entries\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n\n    # For z >= 0: sigmoid = 1 / (1 + exp(-z))\n    s[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    # For z < 0: sigmoid = exp(z) / (1 + exp(z))\n    exp_z = np.exp(arr[neg_mask])\n    s[neg_mask] = exp_z / (1.0 + exp_z)\n\n    # Round to 4 decimal places and convert to Python list\n    s_rounded = np.round(s, 4)\n    return s_rounded.tolist()\n```\n\nExplanation of key points:\n1. We detect scalars with `np.isscalar` and use Python\u2019s `math.exp` for a single value, returning a rounded float.\n2. For array-like inputs, we convert to a NumPy array and create boolean masks to split nonnegative and negative elements.  \n3. This avoids overflow/underflow when computing `exp(\u00b1z)` for large |z|.\n4. Finally, we round the results to 4 decimal places and convert back to a nested Python list."}
{"task_id": 224, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list (or for a scalar input, a Python scalar) with the same\n        structure as *z*, where each value has been transformed by the Leaky ReLU.\n    \"\"\"\n    # Convert input to a NumPy array (this works for scalars, lists, nested lists, and ndarrays)\n    arr = np.asarray(z)\n\n    # Apply Leaky ReLU elementwise: x if x >= 0 else a*x\n    activated = np.where(arr >= 0, arr, a * arr)\n\n    # Convert back to Python native types; for arrays/lists this yields a nested list,\n    # for a scalar it yields a Python scalar.\n    return activated.tolist()\n```\n\nExplanation of the key steps:\n\n1. `np.asarray(z)`  \n   \u2014 Ensures that whether `z` is a scalar, list (nested or flat), or `ndarray`, we get a NumPy array view of the same data/structure.\n\n2. `np.where(arr >= 0, arr, a * arr)`  \n   \u2014 Vectorized, element-wise application of the Leaky ReLU rule:  \n     \u2014 If `x >= 0`, keep `x`.  \n     \u2014 Otherwise, replace with `a * x`.\n\n3. `.tolist()`  \n   \u2014 Converts the NumPy result back into Python built-ins:  \n     \u2014 A nested list for any array-like input, or  \n     \u2014 A Python scalar if the original `z` was a scalar.\n\nThis satisfies the requirement of preserving the input structure and returning pure Python containers."}
{"task_id": 226, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D numpy array of shape (n_samples, n_features) with training data.\n        y_train: 1-D numpy array of shape (n_samples,) containing class labels (-1 or 1).\n        X_test:  2-D numpy array of unseen samples to classify.\n        n_clf:   Number of weak learners (decision stumps) to use in the ensemble.\n\n    Returns:\n        A Python list with one element per row in *X_test*. Each element is\n        either -1 or 1 indicating the predicted class label.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    # Initialize sample weights\n    w = np.full(n_samples, 1 / n_samples)\n\n    # To store parameters of each weak learner\n    # Each element is (feature_index, threshold, polarity, alpha)\n    classifiers = []\n\n    for _ in range(n_clf):\n        # Variables to store the best stump of this round\n        best_feature, best_thresh, best_polarity = None, None, None\n        best_error = float('inf')\n        best_pred = None\n\n        # Loop over all features\n        for feature_i in range(n_features):\n            X_col = X_train[:, feature_i]\n            # Consider all unique values in this feature as thresholds\n            thresholds = np.unique(X_col)\n            for thresh in thresholds:\n                for polarity in [1, -1]:\n                    # Generate predictions for this stump\n                    pred = np.ones(n_samples)\n                    if polarity == 1:\n                        # values below threshold -> -1, above -> +1\n                        pred[X_col < thresh] = -1\n                    else:\n                        # polarity == -1: values below threshold -> +1, above -> -1\n                        pred = -np.ones(n_samples)\n                        pred[X_col < thresh] = 1\n\n                    # Compute weighted error\n                    misclassified = (pred != y_train)\n                    error = np.dot(w, misclassified)\n\n                    # Store if this is the best stump so far\n                    if error < best_error:\n                        best_error = error\n                        best_feature = feature_i\n                        best_thresh = thresh\n                        best_polarity = polarity\n                        best_pred = pred.copy()\n\n        # Compute alpha (learner weight), add small epsilon to avoid div by zero\n        alpha = 0.5 * math.log((1.0 - best_error) / (best_error + 1e-10))\n\n        # Store this weak learner's parameters\n        classifiers.append((best_feature, best_thresh, best_polarity, alpha))\n\n        # Update sample weights\n        # w_i <- w_i * exp(-alpha * y_i * h(x_i))\n        w *= np.exp(-alpha * y_train * best_pred)\n        # Normalize to sum to 1\n        w /= np.sum(w)\n\n    # Now perform prediction on X_test\n    n_test = X_test.shape[0]\n    final_pred = np.zeros(n_test)\n\n    for feature_i, thresh, polarity, alpha in classifiers:\n        X_col = X_test[:, feature_i]\n        pred = np.ones(n_test)\n        if polarity == 1:\n            pred[X_col < thresh] = -1\n        else:\n            pred = -np.ones(n_test)\n            pred[X_col < thresh] = 1\n        # Accumulate weighted predictions\n        final_pred += alpha * pred\n\n    # Take sign, map zeros to +1\n    y_pred = np.sign(final_pred)\n    y_pred[y_pred == 0] = 1\n\n    # Convert to list of ints\n    return [int(label) for label in y_pred]\n```\n\nExplanation of key steps:\n\n1. **Initialize weights** `w` uniformly over the training samples.\n2. **For each boosting round**:\n   - Brute-force over all features, all unique thresholds, and both polarities to find the decision stump that minimizes the weighted error.\n   - Compute the stump\u2019s coefficient `alpha = 0.5 * ln((1\u2212error)/(error+\u03b5))`.\n   - Update the sample weights so that misclassified points get larger weights.\n3. **Prediction**: For each test sample, sum up `alpha * stump_prediction`. The final class is the sign of this sum (ties broken as +1).\n4. Return the predictions as a Python list of `-1` or `1`."}
{"task_id": 241, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n   Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n       # Check for minimum rank\n    if len(shape) < 2:\n        return -1\n\n    # First dimension\n    d0 = shape[0]\n    # Product of the remaining dimensions\n    flat_dim = int(np.prod(shape[1:]))\n\n    # Draw a random Gaussian matrix A of shape (d0, flat_dim)\n    A = np.random.randn(d0, flat_dim)\n\n    # Compute thethin SVD: A = U @ S @ Vt\n    # U has shape (d0, k), Vt has shape (k, flat_dim) with k = min(d0, flat_dim)\n    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n\n    # Choose the factor that matches A's shape\n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = Vt\n\n    # Reshape back to the requested shape and scale\n    Q = Q.reshape(shape) * scale\n\n    # Convert to a (nested) python list and return\n    return Q.tolist()\n```\n\nExplanation of the key steps:\n\n1. Check that `shape` has at least two dimensions; otherwise return `-1`.  \n2. Let `d0 = shape[0]` and let `flat_dim` be the product of the remaining dimensions.  \n3. Draw a `d0\u00d7flat_dim` matrix `A` with i.i.d. standard normal entries.  \n4. Compute the reduced SVD `A = U @ S @ Vt` with `full_matrices=False`.  This makes  \n   U of shape `(d0, k)` and `Vt` of shape `(k, flat_dim)`, where `k = min(d0, flat_dim)`.  \n5. Whichever of `U` or `Vt` matches `A`\u2019s shape `(d0, flat_dim)` is our orthogonal basis `Q`.  \n6. Reshape `Q` back into `shape` and multiply by `scale`.  \n7. Return `Q.tolist()` to satisfy the requirement of a pure Python nested list.  \n\nThis construction guarantees that when you flatten the result to shape `(d0, flat_dim)`, its rows or columns (whichever are fewer) form an orthonormal set up to the given `scale`. You can verify:\n\n```python\nimport numpy as np\n\nnp.random.seed(0)\nres = orthogonal((2, 2), scale=0.5)\nflat = np.array(res).reshape(2, -1)\nprint(np.round(flat @ flat.T, 6))\n# Should be approx. 0.25 * I\n```"}
{"task_id": 243, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    # Convert input to a numpy array\n    s = np.array(state, dtype=float)\n    \n    # Layer 1: z1 = ReLU(sum(s))\n    s_sum = np.sum(s)\n    z1 = max(0.0, s_sum)\n    \n    # Layers 2 & 3 and heads collapse to simple multiples:\n    # z2 = 4 * z1, z3 = 4 * z2 = 16 * z1\n    # actor logits = 4 * z3 = 64 * z1  (same for each of the 3 outputs)\n    # critic value = 4 * z3 = 64 * z1\n    logits = np.full((3,), 64.0 * z1)\n    state_value = 64.0 * z1\n\n    # Softmax over logits\n    # subtracting max for numerical stability (though here all logits are identical)\n    exp_shift = np.exp(logits - np.max(logits))\n    probs = exp_shift / np.sum(exp_shift)\n\n    # Round outputs to 4 decimal places\n    action_probs = np.round(probs, 4).tolist()\n    state_value = float(np.round(state_value, 4))\n\n    return action_probs, state_value\n```\n\nExplanation of the key steps:\n\n1. Since all weights = 1 and biases = 0:\n   - After the first dense + ReLU, every hidden unit holds max(0, s\u2080 + s\u2081 + s\u2082), call this z1.\n   - The next two hidden layers each multiply the sum of their inputs by 4, so by layer 3 each unit holds 16\u00b7z1.\n2. The actor head sums its 4 identical inputs to get 64\u00b7z1 for each of the 3 logits; softmax on equal logits yields [1/3, 1/3, 1/3].\n3. The critic head sums its 4 identical inputs to get the state value = 64\u00b7z1.\n4. Finally, we round all probabilities and the state value to 4 decimal places as required."}
{"task_id": 249, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head (rounded to 4 d.p.)\n        value         : float          \u2013 scalar output of the critic head (rounded to 4 d.p.)\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    x   = np.array(state, dtype=float)\n    W1, b1 = np.array(params['W1'], dtype=float), np.array(params['b1'], dtype=float)\n    W2, b2 = np.array(params['W2'], dtype=float), np.array(params['b2'], dtype=float)\n    W3, b3 = np.array(params['W3'], dtype=float), np.array(params['b3'], dtype=float)\n    Wa, ba = np.array(params['Wa'], dtype=float), np.array(params['ba'], dtype=float)\n    Wc, bc = np.array(params['Wc'], dtype=float), np.array(params['bc'], dtype=float)\n\n    # Shared 3-layer MLP with ReLU activations\n    def relu(z):\n        return np.maximum(0, z)\n\n    h1 = relu(W1.dot(x) + b1)\n    h2 = relu(W2.dot(h1) + b2)\n    h3 = relu(W3.dot(h2) + b3)\n\n    # Actor head: dense + softmax\n    logits = Wa.dot(h3) + ba\n    shifted = logits - np.max(logits)         # for numerical stability\n    exps    = np.exp(shifted)\n    probs   = exps / np.sum(exps)\n\n    # Critic head: dense \u2192 scalar\n    value = float(Wc.dot(h3) + bc)\n\n    # Round to 4 decimals, but keep single decimal for exact 0 or 1\n    def smart_round(num: float) -> float:\n        r = round(num, 4)\n        if r == 0.0 or r == 1.0:\n            # ensure one decimal place for exact 0 or 1\n            return float(f\"{r:.1f}\")\n        return r\n\n    probs_rounded = [smart_round(p) for p in probs.tolist()]\n    value_rounded = smart_round(value)\n\n    return probs_rounded, value_rounded\n```\n\nExplanation of key steps:\n\n1. **Parameter setup**  \n   Convert all weights and biases from the input dictionary into NumPy arrays for easy linear algebra.\n\n2. **Shared network**  \n   Apply three dense (affine) layers each followed by a ReLU activation:\n   h1 = ReLU(W1\u00b7x + b1),  \n   h2 = ReLU(W2\u00b7h1 + b2),  \n   h3 = ReLU(W3\u00b7h2 + b3).\n\n3. **Actor head**  \n   Compute action logits = Wa\u00b7h3 + ba, stabilize by subtracting max, then apply soft-max to get a probability vector.\n\n4. **Critic head**  \n   Compute state-value = Wc\u00b7h3 + bc as a single float.\n\n5. **Rounding**  \n   Use `round(..., 4)` for four decimals, but if the result is exactly 0 or 1, re-format with one decimal place (`0.0`, `1.0`) to satisfy the requirement."}
{"task_id": 253, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    # Convert input to NumPy array of floats\n    w_arr = np.asarray(w, dtype=float).ravel()\n\n    if not gradient:\n        # Penalty: \u03b1 [ \u03bb * ||w||_1 + (1\u2212\u03bb) * 0.5 * ||w||_2^2 ]\n        l1_term = l1_ratio * np.sum(np.abs(w_arr))\n        l2_term = (1.0 - l1_ratio) * 0.5 * np.sum(w_arr ** 2)\n        penalty = alpha * (l1_term + l2_term)\n        return round(penalty, 4)\n    else:\n        # Gradient: \u03b1 [ \u03bb * sign(w) + (1\u2212\u03bb) * w ]\n        grad = alpha * (\n            l1_ratio * np.sign(w_arr)\n            + (1.0 - l1_ratio) * w_arr\n        )\n        # Convert to Python list and round each element\n        return [round(float(g), 4) for g in grad]\n```\n\nExplanation of key steps:\n\n1. We cast `w` to a NumPy array (`w_arr`) for vectorized operations.\n2. When `gradient=False`, we compute  \n   R(w) = \u03b1 [ \u03bb \u00b7 \u2211|w\u1d62|  +  (1\u2212\u03bb) \u00b7 0.5 \u00b7 \u2211w\u1d62\u00b2 ]  \n   and return the scalar, rounded to 4 decimal places.\n3. When `gradient=True`, we compute  \n   \u2207R(w) = \u03b1 [ \u03bb \u00b7 sign(w) + (1\u2212\u03bb) \u00b7 w ]  \n   element\u2010wise, and return it as a list of floats each rounded to 4 decimals."}
{"task_id": 256, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n   Numerically estimate the gradient of an RNN parameter using centred finite differences.\n\n    Args:\n        model:        A model exposing a ``parameters`` dict, a ``forward``\n                       method (single time-step) and a ``flush_gradients``\n                       method.\n        loss_func:    Callable that maps the list of predictions to a scalar\n                       loss value.\n        param_name:   Name of the parameter to be checked. Ba andBx must\n                       be redirected to the lower-case keys.  If the name is\n                      X ory the function should immediately return None.\n        n_t:          Number of time-steps to unroll the network for.\n        X:            Input tensor of shape (batch, input_dim, n_t).\n        epsilon:      Small perturbation added/subtracted to the parameter.\n\n    Returns:\n        NumPy array containing the *transposed* numerical gradient of the\n        chosen parameter or None for the non-trainable names.\n       # Non-trainable\n    if param_name inXy        return None\n\n    # Map special names\n    if param_name ==Ba        key =ba    elif param_name ==Bx        key =bx    else:\n        key = param_name\n\n    # Fetch the parameter array\n    if key not in model.parameters:\n        raise KeyError(fParameter '{param_name}' (mapped to '{key}') not found in model    param = model.parameters[key]\n    \n    # We'll work in float64\n    # Create grads array\n    grads = np.zeros_like(param, dtype=np.float64)\n\n    # Ensure X is float64 for numerical stability\n    X64 = X.astype(np.float64, copy=False)\n\n    # Iterate over all indices in the parameter\n    it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n    while not it.finished:\n        idx = it.multi_index\n        # Save original value\n        orig_value = param[idx].astype(np.float64)\n\n        # + epsilon\n        param[idx] = orig_value + epsilon\n        model.flush_gradients()\n        preds_plus = []\n        for t in range(n_t):\n            xt = X64[:, :, t]\n            preds_plus.append(model.forward(xt))\n        loss_plus = loss_func(preds_plus)\n        loss_plus = np.float64(loss_plus)\n\n        # - epsilon\n        param[idx] = orig_value - epsilon\n        model.flush_gradients()\n        preds_minus = []\n        for t in range(n_t):\n            xt = X64[:, :, t]\n            preds_minus.append(model.forward(xt))\n        loss_minus = loss_func(preds_minus)\n        loss_minus = np.float64(loss_minus)\n\n        # restore original\n        param[idx] = orig_value\n\n        # finite difference\n        grads[idx] = (loss_plus - loss_minus) / (2.0 * epsilon)\n\n        it.iternext()\n\n    # return the transpose of the accumulated grads\n    return grads.T\n```\n\nExplanation:\n\n1. We immediately return `None` if asked to checkX ory`.\n2. We mapBa\u2192`ba andBx\u2192`bx`, else we use `param_name` directly.\n3. For each element in the chosen parameter array, we:\n   - Add \u03b5, run the forward loop for `n_t` steps, collect predictions, compute `loss_plus`.\n   - Subtract 2\u03b5 from the same element, repeat, compute `loss_minus`.\n   - Restore the original parameter value.\n   - Estimate the gradient by `(loss_plus - loss_minus) / (2\u03b5)`.\n4. We accumulate these finite\u2010difference estimates in a float64 array `grads` and finally return its transpose.\n\nThis satisfies the centred finite\u2010difference gradient check for any trainable RNN parameter."}
{"task_id": 257, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test:  2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    # Ensure at least one classifier\n    if n_clf < 1:\n        n_clf = 1\n\n    m, n = X_train.shape\n\n    # Initialize sample weights uniformly\n    w = np.full(m, 1 / m)\n\n    # We'll store each stump as a dict: {'feature': int, 'threshold': float, 'polarity': 1 or -1}\n    stumps = []\n    alphas = []\n\n    # --------------------------\n    # 1) Train n_clf decision stumps\n    # --------------------------\n    for _ in range(n_clf):\n        best_stump = None\n        best_pred = None\n        min_error = np.inf\n\n        # Loop over all features to find the best stump\n        for feature_i in range(n):\n            X_col = X_train[:, feature_i]\n            # Candidate thresholds: unique values in this feature\n            thresholds = np.unique(X_col)\n\n            for thresh in thresholds:\n                # Try both polarity directions\n                for polarity in [1, -1]:\n                    # Predict: for polarity=1: h(x)=1 if x < thresh else -1\n                    #          for polarity=-1: h(x)=-1 if x < thresh else 1\n                    pred = np.ones(m)\n                    if polarity == 1:\n                        pred[X_col >= thresh] = -1\n                    else:\n                        pred[X_col < thresh] = -1\n\n                    # Weighted error\n                    error = np.sum(w[pred != y_train])\n\n                    # Pick stump with minimal error\n                    if error < min_error:\n                        min_error = error\n                        best_pred = pred.copy()\n                        best_stump = {\n                            'feature': feature_i,\n                            'threshold': thresh,\n                            'polarity': polarity\n                        }\n\n        # Avoid division by zero in alpha calculation\n        eps = 1e-10\n        err = min_error\n        # Compute stump weight\n        alpha = 0.5 * np.log((1 - err + eps) / (err + eps))\n\n        # Update sample weights\n        # w_i <- w_i * exp(-alpha * y_i * h_t(x_i))\n        w *= np.exp(-alpha * y_train * best_pred)\n        w /= np.sum(w)  # normalize\n\n        # Save the stump and its alpha\n        stumps.append(best_stump)\n        alphas.append(alpha)\n\n    # --------------------------\n    # 2) Aggregate stumps on X_test\n    # --------------------------\n    k = X_test.shape[0]\n    agg = np.zeros(k)\n\n    for alpha, stump in zip(alphas, stumps):\n        feat = stump['feature']\n        thresh = stump['threshold']\n        polarity = stump['polarity']\n\n        # Make predictions of this stump on X_test\n        pred = np.ones(k)\n        if polarity == 1:\n            pred[X_test[:, feat] >= thresh] = -1\n        else:\n            pred[X_test[:, feat] < thresh] = -1\n\n        # Add weighted vote\n        agg += alpha * pred\n\n    # Final prediction by sign of the aggregate\n    y_pred = np.sign(agg)\n    # For zero, break tie as +1\n    y_pred[y_pred == 0] = 1\n\n    return y_pred.astype(int).tolist()\n```\n\nExplanation of key steps:\n\n1. **Initialization**: Start with uniform weights `w_i = 1/m`.\n2. **Stump training**  \n   - For each feature, consider splitting thresholds at each unique feature value.  \n   - For each threshold and polarity (which side predicts +1 vs. -1), compute the weighted error.  \n   - Select the stump (feature, threshold, polarity) with the lowest weighted error.\n3. **Stump weight**  \n   - Compute `alpha = 0.5 * ln((1 - err) / err)` with a small `eps` to avoid division by zero.\n4. **Re-weight examples**  \n   - Update `w_i \u2190 w_i * exp(-alpha * y_i * h_t(x_i))` and normalize so weights sum to 1.\n5. **Prediction**  \n   - For each test point, sum the weighted stump predictions and take the sign. Ties are broken in favor of +1.\n\nThe returned list contains only `-1` or `1` as required."}
{"task_id": 261, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n   Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n       if len(shape) < 2:\n        raise ValueError(fglorot_normal requires at least 2 dimensions, got {shape    # Compute fan_in and fan_out\n    if len(shape) == 2:\n        fan_in, fan_out = shape\n    else:\n        # shape: (out_channels, in_channels, k1, k2, \u2026)\n        out_channels, in_channels = shape[0], shape[1]\n        # receptive field size = product of remaining dims\n        receptive_field_size = int(np.prod(shape[2:], dtype=int))\n        fan_in = in_channels * receptive_field_size\n        fan_out = out_channels * receptive_field_size\n\n    # Standard deviation for the normal distribution\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n\n    # Sample from N(0, std^2)\n    return np.random.normal(loc=0.0, scale=std, size=shape)\n```\n\nExplanation of the key steps:\n\n1. **Dimension check**  \n   We require at least 2 dimensions to define `fan_in` and `fan_out`.  \n2. **Fan-in and fan-out**  \n   - For a 2D weight matrix `shape == (fan_in, fan_out)`, they are taken directly.  \n   - For convolutional-type tensors, the receptive field is the product of all dims after the first two.  \n3. **Standard deviation**  \n   `std = sqrt(2 / (fan_in + fan_out))` per the Glorot/Xavier normal initialization.  \n4. **Sampling**  \n   We use `np.random.normal` with the computed `std` and mean `0.0`. The returned array has the exact `shape` and dtype `float64`.\n\nThis implementation does not alter any random state beyond drawing samples from NumPy\u2019s global RNG."}
{"task_id": 266, "completion_id": 0, "completion": "```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n   Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples `(u, v)` describing undirected edges that connect\n        vertices `u` and `v`.  Edges may be repeated or duplicated in either\n        order (u, v) vs. (v, u).\n\n    Returns\n    -------\n    List[List[Any]]\n        A list `G` where `G[i]` contains all vertices adjacent to `V[i]`.\n        \u2022 The outer list has the same length and order as `V`.\n        \u2022 Inner lists contain no duplicates and are ordered by their\n          appearance order in `V`.\n       # Map each vertex to its index in V for fast lookup and sorting.\n    index_map = {vertex: idx for idx, vertex in enumerate(V)}\n\n    # Initialize an empty neighbor\u2010set for each vertex in V.\n    neighbors = {vertex: set() for vertex in V}\n\n    # Build the undirected adjacency by adding both directions.\n    for u, v in E:\n        # Only consider edges whose endpoints are in V.\n        if u in neighbors and v in neighbors:\n            neighbors[u].add(v)\n            neighbors[v].add(u)\n        # If you prefer to raise an error on unknown vertices,\n        # replace the if\u2010guard above with an assertion or exception.\n\n    # Convert each neighbor\u2010set into a sorted list according to V's order.\n    G: List[List[Any]] = []\n    for vertex in V:\n        # Extract the set, turn into list, then sort by index in V\n        nbr_list = list(neighbors[vertex])\n        nbr_list.sort(key=lambda w: index_map[w])\n        G.append(nbr_list)\n\n    return G\n```\n\nExplanation of key steps:\n\n1. **Index mapping**: We build `index_map` so that each vertex knows its position in `V`.  \n2. **Neighbor sets**: We store neighbors in a `set` to automatically remove duplicate or parallel edges (and handle self\u2010loops correctly by only storing once).  \n3. **Undirected insertion**: For each edge `(u, v)`, we add `v` to `u`\u2019s neighbor set and `u` to `v`\u2019s neighbor set.  \n4. **Ordering**: Finally, for each vertex in the original order of `V`, we convert its neighbor set into a list and sort that list by the vertices\u2019 indices in `V`, ensuring a deterministic order.  \n\nThis runs in O(|V| + |E| + \u03a3 d(v) log d(v)) time, which is efficient for typical graph sizes."}
{"task_id": 267, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump (see task description).\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    best_error = np.inf\n    best_feature = None\n    best_threshold = None\n    best_polarity = None\n\n    # Loop over all features\n    for j in range(n_features):\n        x_j = X[:, j]\n        # Consider each unique feature value as a candidate threshold\n        thresholds = np.unique(x_j)\n\n        for theta in thresholds:\n            # Polarity = +1  means: predict +1 if x < theta, else -1\n            # Polarity = -1 means: predict -1 if x < theta, else +1\n            for polarity in (1, -1):\n                # make predictions\n                if polarity == 1:\n                    # +1 for x < theta, -1 for x >= theta\n                    preds = np.where(x_j < theta, 1, -1)\n                else:\n                    # -1 for x < theta, +1 for x >= theta\n                    preds = np.where(x_j < theta, -1, 1)\n\n                # weighted error\n                miss = (preds != y)\n                error = sample_weights[miss].sum()\n\n                # update best stump if this is better\n                if error < best_error:\n                    best_error = error\n                    best_feature = j\n                    best_threshold = theta\n                    best_polarity = polarity\n\n    return {\n        \"feature_index\": int(best_feature),\n        \"threshold\": float(np.round(best_threshold, 4)),\n        \"polarity\": int(best_polarity),\n        \"weighted_error\": float(np.round(best_error, 4))\n    }\n```\n\nExplanation of key steps:\n\n1. Iterate over each feature `j`.\n2. For that feature, gather all unique values in `X[:, j]`; each serves as a candidate threshold `\u03b8`.\n3. For each `(feature, threshold)` pair, try both polarities `p = +1` and `p = -1`:\n   - If `p = +1`, we predict +1 when `x < \u03b8`, else -1.\n   - If `p = -1`, we predict -1 when `x < \u03b8`, else +1.\n4. Compute the weighted error by summing the sample weights of mis\u2010classified points.\n5. Keep track of the configuration `(feature, threshold, polarity)` that yields the smallest weighted error.\n6. Return the best stump parameters, rounding `threshold` and `weighted_error` to 4 decimal places."}
{"task_id": 273, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimals or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to lists in case they're numpy arrays\n    X = list(feature)\n    y = list(target)\n    n = len(y)\n    # No data \u2192 no split\n    if n == 0:\n        return None, 0.0\n\n    # Helper: Shannon entropy of a label-count dict\n    def entropy(counts, total):\n        h = 0.0\n        for cnt in counts.values():\n            p = cnt / total\n            if p > 0.0:\n                h -= p * math.log2(p)\n        return h\n\n    # Total label counts and parent entropy\n    total_counts = Counter(y)\n    H_parent = entropy(total_counts, n)\n\n    # Group counts by each distinct feature value\n    counts_per_value = {}\n    for xi, yi in zip(X, y):\n        if xi not in counts_per_value:\n            counts_per_value[xi] = Counter()\n        counts_per_value[xi][yi] += 1\n\n    # If all feature values are identical, no threshold\n    distinct_vals = sorted(counts_per_value.keys())\n    if len(distinct_vals) < 2:\n        return None, 0.0\n\n    # Prepare to sweep thresholds in ascending order\n    left_counts = Counter()\n    left_n = 0\n    right_counts = total_counts.copy()\n    right_n = n\n\n    best_ig = 0.0\n    best_t = None\n\n    # For each gap between distinct_vals[i] and distinct_vals[i+1]:\n    #   move all points with x == distinct_vals[i] to the left side,\n    #   then compute IG at threshold midway.\n    for i in range(len(distinct_vals) - 1):\n        v = distinct_vals[i]\n        # Move all points at value v from right to left\n        cnts_v = counts_per_value[v]\n        block_n = sum(cnts_v.values())\n        left_n += block_n\n        right_n -= block_n\n        for cls, cnt in cnts_v.items():\n            left_counts[cls] += cnt\n            right_counts[cls] -= cnt\n            if right_counts[cls] == 0:\n                del right_counts[cls]\n\n        # Compute candidate threshold\n        t = (v + distinct_vals[i + 1]) / 2.0\n\n        # Compute entropies of the splits\n        H_left = entropy(left_counts, left_n)\n        H_right = entropy(right_counts, right_n)\n\n        # Information gain\n        ig = H_parent - (left_n / n) * H_left - (right_n / n) * H_right\n\n        # Update best\n        if ig > best_ig:\n            best_ig = ig\n            best_t = t\n        elif ig == best_ig and best_ig > 0 and t < best_t:\n            best_t = t\n\n    # If no positive gain found, report nothing useful\n    if best_t is None:\n        return None, 0.0\n\n    # Round as requested\n    return round(best_t, 4), round(best_ig, 4)\n```"}
{"task_id": 286, "completion_id": 0, "completion": "```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    rows, cols = shape\n    return [[1.0 for _ in range(cols)] for _ in range(rows)]\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    rows, cols = shape\n    rnd = random.Random(seed)\n    return [[rnd.uniform(low, high) for _ in range(cols)] for _ in range(rows)]\n\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    obj = globals().get(name)\n    if obj is None or not callable(obj):\n        raise ValueError(\"Invalid initialization function.\")\n    return obj\n\n# Example usage:\nif __name__ == \"__main__\":\n    init_fn = get_initializer(\"ones_init\")\n    print(init_fn((2, 3)))\n    # Output: [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n\n    init_fn = get_initializer(\"random_uniform_init\")\n    print(init_fn((2, 2), low=-1.0, high=1.0, seed=123))\n    # Output: deterministic 2x2 matrix in [-1, 1]\n```\n\nExplanation of key parts:\n\n1. **zeros_init** and **ones_init**  \n   - Use nested list comprehensions to build a `rows \u00d7 cols` matrix.  \n   - Fill with `0.0` or `1.0` respectively.\n\n2. **random_uniform_init**  \n   - Create a local `random.Random(seed)` instance so that each call with the same seed is deterministic and does not affect global RNG state.  \n   - Draw each element with `rnd.uniform(low, high)`.\n\n3. **get_initializer**  \n   - Looks up `name` in the module\u2019s `globals()`.  \n   - Verifies the object is callable.  \n   - Returns it or raises `ValueError(\"Invalid initialization function.\")`."}
{"task_id": 287, "completion_id": 0, "completion": "Here is an implementation that does exactly what you asked: it computes the 2-D average\u2010pooling forward pass, then routes back the gradients equally to every input element in each pooling window, rounds both outputs to 4 decimals, and returns them as Python lists.\n\n```python\nimport numpy as np\n\ndef average_pool2d(\n    X: np.ndarray,\n    pool_shape: tuple[int, int],\n    stride: tuple[int, int],\n    accum_grad: np.ndarray\n) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    s_h, s_w = stride\n\n    # output spatial dims\n    H_out = (H - p_h) // s_h + 1\n    W_out = (W - p_w) // s_w + 1\n\n    # forward output\n    out = np.zeros((N, C, H_out, W_out), dtype=X.dtype)\n\n    # backward grad wrt X\n    grad_input = np.zeros_like(X, dtype=X.dtype)\n\n    pool_size = p_h * p_w\n\n    # forward and backward\n    for i in range(H_out):\n        for j in range(W_out):\n            sh = i * s_h\n            sw = j * s_w\n            window = X[:, :, sh:sh + p_h, sw:sw + p_w]\n            # forward: mean over last two dims\n            out[:, :, i, j] = window.mean(axis=(2, 3))\n\n            # backward: take incoming gradient at out[:, :, i, j],\n            # split evenly among the p_h*p_w inputs\n            grad = accum_grad[:, :, i, j] / pool_size\n            # broadcast and accumulate\n            grad_input[:, :, sh:sh + p_h, sw:sw + p_w] += grad[:, :, None, None]\n\n    # round to 4 decimals and convert to lists\n    out_rounded = np.round(out, 4).tolist()\n    grad_in_rounded = np.round(grad_input, 4).tolist()\n\n    return out_rounded, grad_in_rounded\n```\n\nExplanation of the key steps:\n\n1. **Compute output shape**  \n   H_out = (H \u2212 p_h) // s_h + 1, W_out = (W \u2212 p_w) // s_w + 1.\n\n2. **Forward pass**  \n   For each spatial offset (i, j):\n     - Slice `X[:, :, sh:sh+p_h, sw:sw+p_w]`.\n     - Take the mean over the last two axes \u2192 `out[:, :, i, j]`.\n\n3. **Backward pass**  \n   For each (i, j):\n     - Grab the incoming gradient `accum_grad[:, :, i, j]`.\n     - Divide by the number of elements in the pool window `(p_h * p_w)`.\n     - Broadcast that small `(N, C)` array to `(N, C, p_h, p_w)` and add into the corresponding slice of `grad_input`.\n\n4. **Rounding & `tolist()`**  \n   Both outputs are rounded by `np.round(..., 4)` and finally turned into Python lists via `.tolist()`."}
{"task_id": 290, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    # Case 1: both are leaves\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        # Compare leaf values with tolerance 1e-8\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-8, rtol=0)\n\n    # Case 2: both are internal nodes\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        # Check feature indices\n        if tree_a.feature != tree_b.feature:\n            return False\n        # Check thresholds with tolerance 1e-8\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8, rtol=0):\n            return False\n        # Recursively compare left and right subtrees\n        return (compare_trees(tree_a.left, tree_b.left) and\n                compare_trees(tree_a.right, tree_b.right))\n\n    # Mismatched types (one is Leaf, the other is Node)\n    return False\n```\n\nExplanation of key points:\n- We first distinguish between `Leaf` and `Node` via `isinstance`.\n- When both are leaves, we use `np.allclose` with an absolute tolerance of `1e-8` and zero relative tolerance to compare their `value`s.\n- When both are internal nodes, we check that:\n  1. Their `feature` attributes are identical.\n  2. Their `threshold`s match within `1e-8` via `np.isclose`.\n  3. Their left and right subtrees are equivalent (recursive calls).\n- If one tree is a `Leaf` and the other a `Node`, we immediately return `False`.\n- The function returns only the boolean literals `True` or `False`."}
{"task_id": 292, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                           parent2: list[list[int | float]],\n                           cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape (or the cutoff is out of bounds),\n        the function returns -1.\n    \"\"\"\n    # Convert inputs to NumPy arrays of floats\n    try:\n        p1 = np.array(parent1, dtype=float)\n        p2 = np.array(parent2, dtype=float)\n    except Exception:\n        # Bad input that cannot be converted to a 2D numeric array\n        return -1\n\n    # Validate shapes\n    if p1.shape != p2.shape or p1.ndim != 2:\n        return -1\n\n    n_rows, n_cols = p1.shape\n\n    # Validate cutoff\n    if not isinstance(cutoff, int) or cutoff < 0 or cutoff > n_cols:\n        return -1\n\n    # Prepare empty children\n    c1 = np.empty_like(p1)\n    c2 = np.empty_like(p1)\n\n    # Copy columns before cutoff\n    if cutoff > 0:\n        c1[:, :cutoff] = p1[:, :cutoff]\n        c2[:, :cutoff] = p2[:, :cutoff]\n\n    # Copy columns from cutoff onward (inclusive)\n    if cutoff < n_cols:\n        c1[:, cutoff:] = p2[:, cutoff:]\n        c2[:, cutoff:] = p1[:, cutoff:]\n\n    # Convert back to Python lists and return\n    return c1.tolist(), c2.tolist()\n```\n\nExplanation of key steps:\n1. We first convert the input nested lists into NumPy arrays with `dtype=float` so we can do fast slicing and ensure a consistent numeric type.\n2. We check that both parents are 2\u2010D arrays of the same shape; if not, we return `-1`.\n3. We make sure `cutoff` is an integer in `[0, n_cols]`. Outside that range, we also return `-1`.\n4. We allocate two empty arrays (`c1`, `c2`) with the same shape as the parents.\n5. Columns before the cutoff are copied from each parent to its corresponding child, and columns from the cutoff to the end are swapped.\n6. Finally, we use `.tolist()` to turn the resulting NumPy arrays back into ordinary nested Python lists and return them as a tuple."}
{"task_id": 294, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph represented as a dictionary with the keys:\n          * 'is_directed' (bool)\n          * '_V2I'        (dict mapping vertex label -> index)\n          * '_G'          (list of adjacency lists, each edge is a\n                           tuple (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a vertex label and the value is a list\n        of `(neighbour, weight)` tuples.  For undirected graphs every edge\n        appears once in each direction and the neighbour lists are sorted\n        alphabetically.\n    \"\"\"\n    # Initialize adjacency dict with every vertex label, even if isolated\n    adj: Dict[Any, List[Tuple[Any, float]]] = {\n        v: [] for v in G.get('_V2I', {})\n    }\n\n    # Quick exit if there are no vertices\n    if not adj:\n        return {}\n\n    is_directed = G.get('is_directed', False)\n    graph_lists = G.get('_G', [])\n\n    if is_directed:\n        # Just dump each stored directed edge\n        for edge_list in graph_lists:\n            for u, v, w in edge_list:\n                adj[u].append((v, w))\n    else:\n        # For undirected, add each edge exactly once in each direction\n        # Track seen undirected edges to avoid duplicates\n        seen = set()  # stores tuples of (frozenset({u, v}), weight)\n        for edge_list in graph_lists:\n            for u, v, w in edge_list:\n                key = (frozenset((u, v)), w)\n                if key not in seen:\n                    seen.add(key)\n                    # add u -> v\n                    adj[u].append((v, w))\n                    # add v -> u (unless it's a self-loop, in which case skip duplicate)\n                    if u != v:\n                        adj[v].append((u, w))\n\n    # Finally, sort each neighbour list by neighbour label\n    for u in adj:\n        adj[u].sort(key=lambda pair: pair[0])\n\n    return adj\n```\n\nExplanation of key points:\n1. We start by seeding `adj` with every vertex label from `G['_V2I']`, ensuring even isolated vertices get an entry.\n2. If the graph is directed, we simply copy each stored `(u, v, weight)` into `adj[u]`.\n3. If undirected, we keep a `seen` set of undirected\u2010edge identifiers `(frozenset({u, v}), weight)` so that whether the internal structure stored the edge once or twice, we only add it once in each direction.\n4. After building the adjacency lists, we sort each list by the neighbour label to ensure deterministic ordering."}
{"task_id": 296, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_binary(x):\n   Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n       # Convert input to a NumPy array\n    arr = np.asarray(x)\n\n    # Check elementwise equality to 0 or 1, then ensure all are True\n    return bool(np.all((arr == 0) | (arr == 1)))\n```\n\nExplanation:\n1. We first convert the input `x` to a NumPy array (`np.asarray(x)`) so that array-like inputs (lists, tuples) are handled transparently.  \n2. Using elementwise comparison, `(arr == 0) | (arr == 1)` produces a boolean array that is `True` where elements are 0 or 1, and `False` otherwise.  \n3. We then call `np.all(...)` which returns `True` if *every* entry in that boolean array is `True`.  \n4. Finally, we wrap the result in Python's `bool(...)` to ensure a built-in `True` or `False` is returned.  \n\nThis method works for integer, float, and boolean arrays of any shape."}
{"task_id": 298, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom typing import List\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n   Return the total log-probability of *query* under an N-gram MLE model.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        A list of training sentences. Each sentence is a plain string; tokens\n        are assumed to be separated by white-spaces.\n    query : str\n        The sentence whose probability should be calculated.\n    N : int\n        The order of the N-gram model (N \u2265 1).\n\n    Returns\n    -------\n    float\n        The sum of natural logarithms of the probabilities of every\n        consecutive N-gram occurring in *query*, rounded to 4 decimal places.\n        If any necessary N-gram is unseen in the corpus the function returns\n        float('-inf').\n       # Tokenize the query\n    q_tokens = query.split()\n    # If the query is too short to form even one N-gram, return -inf\n    if len(q_tokens) < N:\n        return float('-inf')\n\n    # --- Unigram model ---\n    if N == 1:\n        # Count all tokens in the corpus\n        unigram_counts = Counter()\n        total_tokens = 0\n        for sent in corpus:\n            toks = sent.split()\n            unigram_counts.update(toks)\n            total_tokens += len(toks)\n        # If corpus is empty, no probability mass\n        if total_tokens == 0:\n            return float('-inf')\n\n        # Compute log-prob of the query\n        log_prob = 0.0\n        for w in q_tokens:\n            cnt = unigram_counts.get(w, 0)\n            if cnt == 0:\n                return float('-inf')\n            p = cnt / total_tokens\n            log_prob += math.log(p)\n        return round(log_prob, 4)\n\n    # --- Higher-order N-gram model (N > 1) ---\n    # We'll build two Counters:\n    #   1) ngram_counts for N-grams\n    #   2) prefix_counts for (N-1)-grams\n    ngram_counts = Counter()\n    prefix_counts = Counter()\n    pad_beg =<bol> * (N - 1)\n    pad_end =<eol>    for sent in corpus:\n        toks = sent.split()\n        padded = pad_beg + toks + pad_end\n        # Slide over padded sentence\n        L = len(padded)\n        for i in range(L - N + 1):\n            ngram = tuple(padded[i : i + N])\n            prefix = tuple(padded[i : i + N - 1])\n            ngram_counts[ngram] += 1\n            prefix_counts[prefix] += 1\n\n    # Now process the query in the same way\n    padded_q = pad_beg + q_tokens + pad_end\n    Lq = len(padded_q)\n    total_log_prob = 0.0\n\n    for i in range(Lq - N + 1):\n        ngram = tuple(padded_q[i : i + N])\n        prefix = tuple(padded_q[i : i + N - 1])\n        num = ngram_counts.get(ngram, 0)\n        den = prefix_counts.get(prefix, 0)\n        # If we've never seen this N-gram or prefix, prob is zero\n        if num == 0 or den == 0:\n            return float('-inf')\n        p = num / den\n        total_log_prob += math.log(p)\n\n    return round(total_log_prob, 4)\n```\n\nExplanation of key steps:\n\n1. **Tokenization**  \n   We split each sentence on whitespace to obtain tokens.\n\n2. **Padding**  \n   - For N=1, we do not pad.  \n   - For N>1, we prepend `N\u22121` `<bol>` tokens and append one `<eol>` token to each sentence (both in corpus and in query).\n\n3. **Counting**  \n   - For unigrams, we simply count each token and keep track of the total token count.  \n   - For higher-order models, we count every N-gram (`ngram_counts`) as well as its (N\u22121)-gram prefix (`prefix_counts`).\n\n4. **Probability and Log Probability**  \n   - Unigrams: \\(P(w) = \\tfrac{\\mathrm{count}(w)}{\\text{total tokens}}\\).  \n   - N-grams: \\(P(w_N \\mid w_1\\ldots w_{N-1}) = \\tfrac{\\mathrm{count}(w_1\\ldots w_N)}{\\mathrm{count}(w_1\\ldots w_{N-1})}\\).  \n   - We sum the natural logs of these probabilities across all consecutive N-grams in the padded query.\n\n5. **Zero-Probability Handling**  \n   If any required N-gram or prefix was never seen in the corpus (count = 0), we immediately return `float('-inf')`.\n\n6. **Rounding**  \n   Finally, we round the total log-probability to 4 decimal places before returning."}
{"task_id": 302, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n, d = X.shape\n    # 1. Build adjacency matrix W with w_ij = 1 / (1 + ||xi - xj||)\n    # Compute pairwise Euclidean distances\n    # shape: (n, n)\n    diffs = X[:, np.newaxis, :] - X[np.newaxis, :, :]   # (n, n, d)\n    dist = np.sqrt(np.sum(diffs**2, axis=2))            # (n, n)\n    W = 1.0 / (1.0 + dist)\n    np.fill_diagonal(W, 0.0)  # zero diagonal\n\n    # 2. Graph Laplacian L = D - W\n    degree = np.sum(W, axis=1)          # (n,)\n    D = np.diag(degree)                 # (n, n)\n    L = D - W                           # (n, n)\n\n    # 3. Compute first k eigenvectors of L (smallest eigenvalues)\n    # eigh returns eigenvalues in ascending order for symmetric matrices\n    eigvals, eigvecs = np.linalg.eigh(L)\n    E = eigvecs[:, :k]                  # (n, k)\n\n    # 4. Run k-means on rows of E\n    # Initialize centroids to first k rows of E\n    centroids = E[:k, :].copy()         # (k, k)\n    labels = np.zeros(n, dtype=int)\n    for iteration in range(100):\n        # Compute distances from each point to each centroid\n        # shape: (n, k)\n        diff = E[:, np.newaxis, :] - centroids[np.newaxis, :, :]  # (n, k, k)\n        dists = np.sqrt(np.sum(diff**2, axis=2))                  # (n, k)\n        new_labels = np.argmin(dists, axis=1)                     # (n,)\n\n        # Check for convergence\n        if iteration > 0 and np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # Update centroids\n        for ci in range(k):\n            members = E[labels == ci]\n            if members.shape[0] == 0:\n                # Empty cluster: reinitialize to random row of E\n                rand_idx = np.random.randint(0, n)\n                centroids[ci] = E[rand_idx]\n            else:\n                centroids[ci] = np.mean(members, axis=0)\n\n    # 5. Return labels as a Python list\n    return labels.tolist()\n```\n\nExplanation of key steps:\n\n1. We compute the full pairwise Euclidean distance matrix in a vectorized fashion, then convert it to weights `W` via the provided formula and zero out the diagonal.\n\n2. We form the (unnormalized) Laplacian `L = D - W`, where `D` is the diagonal degree matrix.\n\n3. We call `np.linalg.eigh` on `L` (a symmetric matrix) to obtain all eigenvalues and eigenvectors, then take the first `k` eigenvectors corresponding to the smallest eigenvalues to form the embedding `E`.\n\n4. We implement a basic k-means:\n   - Initialize centroids to the first `k` rows of `E`.\n   - Iterate up to 100 times:\n     - Assign each point to its nearest centroid.\n     - If no labels change from the previous iteration, stop early.\n     - Recompute each centroid as the mean of its assigned points. If a cluster has no points, we reinitialize that centroid randomly.\n\n5. Finally, we convert the NumPy array of integer labels to a Python list and return it."}
{"task_id": 303, "completion_id": 0, "completion": "```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = -> str:\n   Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n       prev_ix = max(ix - 1, 0)\n    prev_mine, prev_label = params[prev_ix]\n    curr_mine, curr_label = params[ix]\n    return (\n       ------------------------- DEBUG -------------------------\\n        fMine (prev) [{prev_label}]:\\n        fprev_mine}\\n\\n        firs (prev) [{prev_label}]:\\n        fgolds[prev_label]}\\n\\n        fMine [{curr_label}]:\\n        fcurr_mine}\\n\\n        firs [{curr_label}]:\\n        fgolds[curr_label]}{warn_str}\\n       ----------------------- END DEBUG -----------------------    )\n```"}
{"task_id": 304, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu = 0.0,\n    V = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility).\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    # 1) Optionally add intercept\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    if fit_intercept:\n        ones = np.ones((X.shape[0], 1), dtype=float)\n        X = np.hstack((ones, X))\n\n    N, M = X.shape\n\n    # 2) Build prior mean vector mu_vec of length M\n    mu_arr = np.asarray(mu)\n    if mu_arr.ndim == 0:\n        mu_vec = np.full(M, float(mu_arr))\n    else:\n        mu_vec = mu_arr.ravel().astype(float)\n        if mu_vec.shape[0] != M:\n            raise ValueError(f\"Prior mean mu length {mu_vec.shape[0]} does not match number of features {M}\")\n\n    # 3) Build prior covariance V_mat (M x M)\n    if V is None:\n        V_mat = np.eye(M, dtype=float)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 0:\n            # scalar\n            V_mat = V_arr * np.eye(M, dtype=float)\n        elif V_arr.ndim == 1:\n            # diagonal\n            if V_arr.shape[0] != M:\n                raise ValueError(f\"Diagonal V length {V_arr.shape[0]} does not match number of features {M}\")\n            V_mat = np.diag(V_arr)\n        elif V_arr.ndim == 2:\n            if V_arr.shape != (M, M):\n                raise ValueError(f\"Full V shape {V_arr.shape} does not match ({M}, {M})\")\n            V_mat = V_arr.copy()\n        else:\n            raise ValueError(\"Prior scale V must be None, scalar, 1-D or 2-D array\")\n\n    # 4) Compute precision matrix: V^{-1} + X^T X\n    # Compute V^{-1}\n    try:\n        V_inv = np.linalg.inv(V_mat)\n    except np.linalg.LinAlgError:\n        raise np.linalg.LinAlgError(\"Prior covariance V is singular and not invertible\")\n\n    XtX = X.T @ X\n    precision = V_inv + XtX\n\n    # 5) Compute posterior covariance (up to \u03c3\u00b2): Sigma_b = precision^{-1}\n    try:\n        Sigma_b = np.linalg.inv(precision)\n    except np.linalg.LinAlgError:\n        raise np.linalg.LinAlgError(\"Precision matrix is singular and not invertible\")\n\n    # 6) Compute posterior mean: mu_b = Sigma_b @ (V^{-1} mu + X^T y)\n    Xty = X.T @ y\n    term = V_inv @ mu_vec + Xty\n    mu_b = Sigma_b @ term\n\n    # 7) Round to 4 decimal places and return as list\n    mu_b_rounded = np.round(mu_b, 4)\n    return [float(val) for val in mu_b_rounded]\n```\n\nExplanation of key steps:\n\n1. If `fit_intercept=True`, we prepend a column of ones to `X`, increasing the feature dimension by 1.\n2. We broadcast or validate the prior mean `mu` into a vector of length `M`.\n3. We construct the prior scale matrix `V` appropriately:\n   - `None` \u2192 identity,\n   - scalar \u2192 scalar \u00d7 identity,\n   - 1-D array \u2192 diagonal matrix,\n   - 2-D array \u2192 full matrix (validated shape).\n4. We compute the precision matrix `V^{-1} + X\u1d40X` and invert it to get the posterior covariance up to \u03c3\u00b2.\n5. We compute the posterior mean `\u03bc_b = \u03a3_b (V^{-1} \u03bc + X\u1d40y)`.\n6. Finally, we round each coefficient to 4 decimal places and return them as a Python list."}
{"task_id": 308, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    The returned matrix has *n_filters* rows and *N//2 + 1* columns.  Each row\n    is a triangular filter defined on the Mel scale.\n    \"\"\"\n    # 1. Set default max_freq to Nyquist if not provided\n    if max_freq is None:\n        max_freq = fs / 2\n\n    # 2. Convert Hz to Mel\n    def hz_to_mel(f):\n        return 2595.0 * np.log10(1.0 + f / 700.0)\n\n    def mel_to_hz(m):\n        return 700.0 * (10**(m / 2595.0) - 1.0)\n\n    mel_min = hz_to_mel(min_freq)\n    mel_max = hz_to_mel(max_freq)\n\n    # 3. Equally spaced points in Mel domain\n    mel_points = np.linspace(mel_min, mel_max, n_filters + 2)\n\n    # 4. Convert Mel points back to Hz\n    hz_points = mel_to_hz(mel_points)\n\n    # 5. DFT bin center frequencies\n    #    We only need bins from 0 to N//2 inclusive\n    bin_count = N // 2 + 1\n    freqs = np.linspace(0, fs / 2, bin_count)\n\n    # 6. Allocate filterbank matrix\n    filterbank = np.zeros((n_filters, bin_count), dtype=float)\n\n    # 7. Build each filter\n    for i in range(n_filters):\n        f_left = hz_points[i]\n        f_center = hz_points[i + 1]\n        f_right = hz_points[i + 2]\n\n        # Slopes\n        # Avoid division in the loop\n        denom_left = f_center - f_left\n        denom_right = f_right - f_center\n\n        # Compute the two slopes for all bins\n        left_slope = (freqs - f_left) / denom_left\n        right_slope = (f_right - freqs) / denom_right\n\n        # The filter is the positive part of the min of the two\n        filt = np.maximum(0.0, np.minimum(left_slope, right_slope))\n\n        # Normalize to unit area in Mel domain if requested\n        if normalize:\n            area_norm = 2.0 / (f_right - f_left)\n            filt *= area_norm\n\n        filterbank[i, :] = filt\n\n    # 8. Round to 4 decimal places and convert to plain Python list\n    fb_rounded = []\n    for i in range(n_filters):\n        row = [round(val, 4) for val in filterbank[i, :]]\n        fb_rounded.append(row)\n\n    return fb_rounded\n```\n\nExplanation of key steps:\n\n1. **Frequency/Mel conversions**  \n   We use the standard formulas  \n   &nbsp;&nbsp;&nbsp;&nbsp;mel = 2595 \u00b7 log\u2081\u2080(1 + f/700)  \n   &nbsp;&nbsp;&nbsp;&nbsp;f   = 700 \u00b7 (10^(mel/2595) \u2212 1)\n\n2. **Mel\u2010spaced corner points**  \n   We compute `n_filters + 2` points equally spaced on the Mel axis, then map back to Hertz.  These give the left, center, and right \u201ccorners\u201d for each triangular filter.\n\n3. **Bin frequencies**  \n   For an N\u2010point DFT the k-th bin\u2019s center frequency is `k\u00b7fs/N`.  We only need bins `k = 0 \u2026 N//2`.\n\n4. **Triangle weights**  \n   For filter i with corners (f\u1d62, f\u1d62\u208a\u2081, f\u1d62\u208a\u2082), the weight at bin frequency f is  \n     w = max(0, min((f \u2013 f\u1d62)/(f\u1d62\u208a\u2081 \u2013 f\u1d62), (f\u1d62\u208a\u2082 \u2013 f)/(f\u1d62\u208a\u2082 \u2013 f\u1d62\u208a\u2081)))  \n\n5. **Normalization**  \n   If `normalize=True`, each filter is scaled by `2/(f\u1d62\u208a\u2082 \u2013 f\u1d62)` so that its area in Mel\u2010space is unity.\n\n6. **Output**  \n   We round each weight to 4 decimal places and return a plain Python list of lists, shape `(n_filters, N//2+1)`."}
{"task_id": 312, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    # Special cases\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n\n    # Coefficients\n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n\n    # For the periodic case we generate length = window_len + 1 and drop the last sample\n    length = window_len + 1 if not symmetric else window_len\n    N = length - 1\n\n    # Sample indices\n    n = np.arange(length, dtype=float)\n\n    # Blackman\u2013Harris formula\n    w = (a0\n         - a1 * np.cos(2.0 * np.pi * n / N)\n         + a2 * np.cos(4.0 * np.pi * n / N)\n         - a3 * np.cos(6.0 * np.pi * n / N))\n\n    # Drop last sample for periodic form\n    if not symmetric:\n        w = w[:-1]\n\n    # Round to 4 decimal places and return as a Python list\n    w_rounded = np.round(w, 4).tolist()\n    return w_rounded\n```\n\nExplanation of key points:\n- We handle the special cases `window_len <= 0` and `window_len == 1` explicitly.\n- For the *symmetric* variant, we use `N = window_len - 1` and compute exactly `window_len` samples.\n- For the *periodic* variant, we compute one extra sample (`length = window_len + 1`) with `N = window_len`, then discard the last sample so that the sequence of `window_len` values wraps seamlessly.\n- Final window values are rounded to four decimal places and converted to a standard Python `list` of `float`."}
{"task_id": 313, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    def digamma(x):\n        # Use recurrence to shift x up to at least 6\n        result = 0.0\n        while x < 6.0:\n            result -= 1.0 / x\n            x += 1.0\n        # Asymptotic expansion for large x\n        inv = 1.0 / x\n        inv2 = inv * inv\n        inv4 = inv2 * inv2\n        inv6 = inv4 * inv2\n        return (result\n                + math.log(x)\n                - 0.5 * inv\n                - (1.0 / 12.0) * inv2\n                + (1.0 / 120.0) * inv4\n                - (1.0 / 252.0) * inv6)\n\n    # Ensure we have a NumPy array of floats\n    gamma_arr = np.asarray(gamma, dtype=float)\n    row = gamma_arr[d]\n    a_dt = row[t]\n    sum_row = row.sum()\n\n    val = digamma(a_dt) - digamma(sum_row)\n    # Round to four decimal places and return as float\n    return round(val, 4)\n```\n\nExplanation of the key steps:\n1. We define an inner `digamma(x)` that\n   - Recursively shifts `x` upward (using \u03c8(x) = \u03c8(x+1) \u2212 1/x) until `x >= 6`.\n   - Applies the asymptotic expansion \n     \u03c8(x) \u2248 ln x \u2212 1/(2x) \u2212 1/(12x\u00b2) + 1/(120x\u2074) \u2212 1/(252x\u2076).\n2. We convert the input `gamma` to a NumPy array of floats, pick out the `d`-th row,\n   and compute `a_dt = gamma[d,t]` and `sum_row = \u2211\u2096 gamma[d,k]`.\n3. We evaluate \u03c8(a_dt) \u2212 \u03c8(sum_row), round the result to four decimal places, and return it."}
{"task_id": 317, "completion_id": 0, "completion": "```python\nfrom typing import List, Tuple, Dict\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = -> str:\n   Return a formatted multi-line debug string comparing predictions to golds.\n\n    See the task description for the exact required format.\n\n    Args:\n        params: A list where every element is a tuple (prediction, label).\n        golds:  A dictionary mapping each label to its gold/expected string.\n        ix:     The index inside `params` for which the detailed comparison\n                 is built.\n        warn_str: Optional extra text appended before the footer.\n\n    Returns:\n        A string that follows the specified layout exactly.\n       # determine previous index (floor at 0)\n    prev_ix = max(ix - 1, 0)\n\n    prev_pred, prev_label = params[prev_ix]\n    curr_pred, curr_label = params[ix]\n\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n\n    header = * 25 + DEBUG + * 25\n    footer = * 23 + END DEBUG + * 23\n\n    lines = [\n        header,\n        fMine (prev) [{prev_label}]:        prev_pred,\n               firs (prev) [{prev_label}]:        prev_gold,\n         # first blank line after prev block\n         # second blank line after prev block\n        fMine [{curr_label}]:        curr_pred,\n               firs [{curr_label}]:        curr_gold + warn_str,\n        footer\n    ]\n\n    # Join with newline, no extra newline at end\n    returnnjoin(lines)\n```"}
{"task_id": 318, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n   Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples containing the training labels. Each label\n        is either -1 or 1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds (weak learners) to use.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 or 1) for each sample in X_test.\n       n_samples, n_features = X_train.shape\n    # Initialize sample weights uniformly\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    # To store the parameters of each weak learner\n    stumps = []  # each element: (feature_index, threshold, polarity, alpha)\n\n    eps = 1e-10  # to avoid division by zero in alpha calc\n\n    # Boosting rounds\n    for m in range(n_estimators):\n        best_err = float('inf')\n        best_j = None\n        best_t = None\n        best_p = None\n        best_pred = None\n\n        # Find the best decision stump\n        for j in range(n_features):\n            x_j = X_train[:, j]\n            thresholds = np.unique(x_j)\n            for t in thresholds:\n                # Polarity +1 and -1\n                for p in [1, -1]:\n                    # stump prediction: p * sign(x_j - t)\n                    pred = p * np.where(x_j - t < 0, -1, 1)\n                    # weighted error\n                    err = np.sum(w * (pred != y_train))\n                    if err < best_err:\n                        best_err = err\n                        best_j = j\n                        best_t = t\n                        best_p = p\n                        best_pred = pred\n\n        # Clamp error to avoid division by zero or log of zero\n        best_err = max(best_err, eps)\n        best_err = min(best_err, 1 - eps)\n\n        # Compute alpha\n        alpha = 0.5 * math.log((1 - best_err) / best_err)\n\n        # Save the stump parameters\n        stumps.append((best_j, best_t, best_p, alpha))\n\n        # Update sample weights\n        # w_i <- w_i * exp(-alpha * y_i * h_m(x_i))\n        w *= np.exp(-alpha * y_train * best_pred)\n        # Normalize\n        w_sum = np.sum(w)\n        if w_sum <= 0:\n            # all weights have underflowed; reinitialize uniformly\n            w = np.full(n_samples, 1.0 / n_samples)\n        else:\n            w /= w_sum\n\n    # Now predict on X_test\n    n_test = X_test.shape[0]\n    F = np.zeros(n_test)  # aggregated score\n\n    for (j, t, p, alpha) in stumps:\n        x_j_test = X_test[:, j]\n        pred = p * np.where(x_j_test - t < 0, -1, 1)\n        F += alpha * pred\n\n    # Final prediction: sign(F)\n    y_pred = np.where(F < 0, -1, 1)\n\n    return y_pred.tolist()\n```\n\nExplanation of key steps:\n1. Initialize sample weights `w` uniformly.\n2. For each boosting round:\n   - Loop over features `j`, candidate thresholds `t` (unique values), and polarities `p = \u00b11`.\n   - Compute the stump prediction `h(x) = p * sign(x_j - t)`.\n   - Measure weighted error `\u03b5 = \u03a3 w\u1d62 [h(x\u1d62) \u2260 y\u1d62]`.\n   - Keep the stump `(j, t, p)` that gives minimal `\u03b5`.\n   - Compute its coefficient `\u03b1 = \u00bd ln((1\u2212\u03b5)/\u03b5)`.\n   - Update sample weights:  \n     w\u1d62 \u2190 w\u1d62 \u00b7 exp(\u2212\u03b1 \u00b7 y\u1d62 \u00b7 h(x\u1d62)), then normalize.\n3. For prediction, sum the weighted stump outputs and take the sign.\n\nThis implementation uses only `numpy` and `math` and returns a plain Python list of `-1` or `1` as required."}
{"task_id": 329, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When *True* return only the non-negative frequency bins; when\n        *False* return all *N* bins.  Default is *True*.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.  An empty\n        list is returned when *N* or *fs* is non-positive.\n    \"\"\"\n    # guard against invalid inputs\n    if N <= 0 or fs <= 0:\n        return []\n\n    # frequency resolution\n    df = fs / N\n\n    if positive_only:\n        # non-negative bins from k=0 to floor(N/2)\n        k = np.arange(0, N//2 + 1)\n        freqs = k * df\n    else:\n        # full spectrum as in numpy.fft.fftfreq\n        # d=1/fs makes the returned frequencies in Hz\n        freqs = np.fft.fftfreq(N, d=1/fs)\n\n    # round to 4 decimal places and convert to plain Python list\n    freqs_rounded = np.round(freqs, 4)\n    return freqs_rounded.tolist()\n```\n\nExplanation of key points:\n- If `N` or `fs` is non\u2010positive, return an empty list immediately.\n- In the `positive_only` case, we build the array of bin indices `k = 0,1,\u2026,\u230aN/2\u230b` and multiply by the resolution `df = fs/N`.\n- Otherwise, we defer to NumPy\u2019s `fftfreq` with `d=1/fs` so that the output frequencies come out in Hz and in the required order.\n- Finally we round every element to four decimal places and call `.tolist()` to satisfy the \u201cplain Python list\u201d requirement."}
{"task_id": 331, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n   Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the\n        OOB estimate cannot be computed.\n       # Convert inputs to numpy arrays\n    y_true = np.asarray(y_true, dtype=float)\n    preds = np.asarray(predictions, dtype=float)\n    mask = np.asarray(oob_mask)\n    \n    # Check dimensions\n    if y_true.ndim != 1:\n        raise ValueErrory_true must be a 1D array of shape (n_samples,    if preds.ndim != 2 or mask.ndim != 2:\n        raise ValueErrorpredictions and oob_mask must be 2D arrays                        n_samples, n_estimators    if preds.shape != mask.shape:\n        raise ValueErrorpredictions and oob_mask must have the same shape    n_samples, _ = preds.shape\n    if y_true.shape[0] != n_samples:\n        raise ValueErrorThe length of y_true must match the number of rows in predictions    \n    # Ensure mask is boolean\n    mask = mask.astype(bool)\n    \n    # Count the number of OOB predictions per sample\n    oob_counts = mask.sum(axis=1)  # shape (n_samples,)\n    \n    # Identify samples with at least one OOB prediction\n    valid = oob_counts > 0\n    if not np.any(valid):\n        return -1.0\n    \n    # Sum the OOB predictions for each sample\n    # (preds * mask) zeroes out in-bag predictions, summing gives total OOB sum\n    oob_sum = (preds * mask).sum(axis=1)\n    \n    # Compute OOB average per sample (only for valid samples)\n    oob_avg = oob_sum[valid] / oob_counts[valid]\n    \n    # Compute squared errors and then mean\n    sq_errors = (oob_avg - y_true[valid]) ** 2\n    mse = sq_errors.mean()\n    \n    # Round to 4 decimal places\n    return round(float(mse), 4)\n```\n\nExplanation of key steps:\n1. Convert inputs to NumPy arrays for easy vectorized operations.  \n2. Validate that dimensions match:  \n   - `y_true` is 1D of length `n_samples`.  \n   - `predictions` and `oob_mask` are 2D of shape `(n_samples, n_estimators)`.  \n3. Build a boolean mask (`mask`) marking out-of-bag predictions.  \n4. For each sample, count how many OOB predictions it has (`oob_counts`).  \n5. Select only those samples with at least one OOB prediction.  \n6. Sum their OOB predictions (`oob_sum`), divide by the OOB count to get the per-sample average (`oob_avg`), then compute squared errors and average them.  \n7. If no sample has an OOB prediction, return `-1.0`. Otherwise, return the MSE rounded to four decimal places."}
{"task_id": 332, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of transactions an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node contains two keys:\n        \"support\" and \"children\" (the latter mapping item \u2192 child-node).\n    \"\"\"\n    # 1. Count support of each item across distinct transactions\n    item_counter = Counter()\n    for trans in transactions:\n        # count each item only once per transaction\n        unique_items = set(trans)\n        item_counter.update(unique_items)\n\n    # 2. Filter out infrequent items\n    frequent_items = {item for item, sup in item_counter.items() if sup >= min_support}\n    # If no items are frequent, return an empty root\n    if not frequent_items:\n        return {\"support\": 0, \"children\": {}}\n\n    # 3. Global ordering: by descending support, then alphabetically\n    #    We'll map item -> its rank in the sorted list\n    sorted_items = sorted(\n        frequent_items,\n        key=lambda item: (-item_counter[item], item)\n    )\n    # Create a lookup for ordering\n    order_lookup = {item: idx for idx, item in enumerate(sorted_items)}\n\n    # 4. Initialize the root of the FP-tree\n    root = {\"support\": 0, \"children\": {}}\n\n    # 5. Process each transaction\n    for trans in transactions:\n        # Keep only frequent items, remove duplicates, and sort by global order\n        filtered = set(trans) & frequent_items\n        if not filtered:\n            continue\n        ordered_trans = sorted(filtered, key=lambda item: order_lookup[item])\n\n        # 6. Walk the tree, updating/inserting nodes\n        current_node = root\n        for item in ordered_trans:\n            # If the child doesn't exist, create it\n            if item not in current_node[\"children\"]:\n                current_node[\"children\"][item] = {\"support\": 0, \"children\": {}}\n            # Move to the child node\n            child = current_node[\"children\"][item]\n            # Increment its support\n            child[\"support\"] += 1\n            # Continue down the path\n            current_node = child\n\n    return root\n```\n\nExplanation of the main steps:\n\n1. We use `Counter` on the set of each transaction so repeated items in one transaction count only once.  \n2. We drop items whose total support is below `min_support`.  \n3. We sort the remaining items by descending support (and alphabetically to break ties), then store their positions in `order_lookup`.  \n4. We initialize the root node as a dictionary with `\"support\": 0` and an empty `\"children\"` dict.  \n5. For each transaction, we:\n   - Filter out infrequent items and remove duplicates by converting to a `set`.\n   - Sort items in the transaction according to the global ordering.\n   - Traverse or extend the path in the FP-tree, creating child nodes if needed, and incrementing each visited node\u2019s `\"support\"` by 1."}
{"task_id": 336, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_kernel(\n    X: list[list[int | float]] | \"np.ndarray\",\n    Y: list[list[int | float]] | \"np.ndarray\" | None = None,\n    c0: int | float = 0\n) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function sets\n        Y = X.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y\n        have incompatible shapes (different number of columns).\n        If X has zero rows (N=0), returns an empty list.\n    \"\"\"\n    # Convert X to a 2-D float NumPy array\n    X_arr = np.array(X, dtype=float)\n    # Special-case: entirely empty X (no rows at all)\n    # In Python lists this is X = [], which becomes a 1-D array of size 0\n    if X_arr.ndim == 1 and X_arr.size == 0:\n        return []\n\n    # Check that X is truly 2-D\n    if X_arr.ndim != 2:\n        return -1\n\n    # If Y is not provided, use X (square, symmetric kernel matrix)\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        Y_arr = np.array(Y, dtype=float)\n        # If Y is malformed (not 2-D), reject\n        if Y_arr.ndim != 2:\n            return -1\n\n    # Shapes\n    N, Cx = X_arr.shape\n    M, Cy = Y_arr.shape\n\n    # Rule 1: number of columns must match\n    if Cx != Cy:\n        return -1\n\n    # If X has zero rows, return empty list\n    if N == 0:\n        return []\n\n    # Compute the dot products (N x M) and add c0\n    K = X_arr.dot(Y_arr.T) + float(c0)\n\n    # Round all entries to 4 decimal places\n    K = np.round(K, 4)\n\n    # Convert to a pure Python list of lists of floats\n    return K.tolist()\n```\n\nExplanation of key points:\n1. We first convert `X` (and `Y`, if given) to NumPy arrays of dtype `float`.\n2. If `X` is the empty list `[]`, it becomes a 1-D array of size 0; we detect that and return `[]` immediately.\n3. We enforce that after conversion both `X_arr` and `Y_arr` are 2-D.  Any deviation (other than the empty\u2010`X` case) leads to returning `-1`.\n4. We check the number of columns match; a mismatch returns `-1`.\n5. If `X` has zero rows (`N == 0`) but is well\u2010formed 2-D (e.g. shape `(0, C)`), we return `[]` as well.\n6. Otherwise we compute `X\u22c5Y\u1d40 + c0`, round to 4 decimal places, and return the result as a list of lists of Python `float`."}
{"task_id": 340, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n   Perform one Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter starting at 1.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_param, new_m, new_v) as python lists,\n        each rounded to 8 decimal places.\n       # 1) Update biased first moment estimate\n    m_t = beta1 * m + (1 - beta1) * grad\n\n    # 2) Update biased second raw moment estimate\n    v_t = beta2 * v + (1 - beta2) * (grad ** 2)\n\n    # 3) Compute bias-corrected first moment\n    m_hat = m_t / (1 - beta1 ** t)\n\n    # 4) Compute bias-corrected second raw moment\n    v_hat = v_t / (1 - beta2 ** t)\n\n    # 5) Parameter update\n    step = lr * m_hat / (np.sqrt(v_hat) + eps)\n    new_param = param - step\n\n    # 6) If gradient is exactly zero, keep parameter unchanged\n    zero_mask = (grad == 0)\n    if np.any(zero_mask):\n        new_param = new_param.copy()\n        new_param[zero_mask] = param[zero_mask]\n\n    # 7) Round results to 8 decimal places and convert to Python lists\n    new_param_list = np.round(new_param, 8).tolist()\n    m_t_list        = np.round(m_t, 8).tolist()\n    v_t_list        = np.round(v_t, 8).tolist()\n\n    return new_param_list, m_t_list, v_t_list\n```"}
{"task_id": 343, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                         rewards: list[float] | np.ndarray,\n                         retain_prcnt: float\n                         ) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    # Ensure rewards is an array\n    rewards_arr = np.asarray(rewards)\n    \n    # Number of samples (N)\n    N = theta_samples.shape[0]\n    # Determine how many to retain (floor of retain_prcnt * N)\n    k = int(retain_prcnt * N)\n    \n    # Get indices of the top-k rewards (descending order)\n    topk_indices = np.argsort(rewards_arr)[::-1][:k]\n    \n    # Select the elite samples\n    elite_samples = theta_samples[topk_indices, :]\n    \n    # Compute per-dimension mean and variance\n    mu = np.mean(elite_samples, axis=0)\n    var = np.var(elite_samples, axis=0)\n    \n    # Round to four decimal places and convert to Python lists\n    mu_rounded = np.round(mu, 4).tolist()\n    var_rounded = np.round(var, 4).tolist()\n    \n    return mu_rounded, var_rounded\n```\n\nExplanation of key steps:\n\n1. We convert `rewards` into a NumPy array for easy sorting.  \n2. We compute `k = int(retain_prcnt * N)`, taking the floor if necessary.  \n3. We sort the reward array in descending order (via `argsort()[::-1]`) and slice out the top-k indices.  \n4. We index into `theta_samples` to extract those elite parameter vectors.  \n5. We compute the per-dimension mean and variance of the elites, round each to four decimal places, and return them as plain Python lists."}
{"task_id": 353, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    # Convert to a concrete sequence so we can get length and iterate multiple times\n    labels_list = list(labels)\n    n = len(labels_list)\n    if n == 0:\n        return 0.0\n\n    counts = Counter(labels_list)\n    H = 0.0\n    for cnt in counts.values():\n        p = cnt / n\n        # only add if p > 0, but counts from Counter are always >0\n        H -= p * math.log2(p)\n\n    return round(H, 5)\n```\n\nExplanation:\n1. We first coerce the input `labels` into a list so we can determine its length and iterate over it multiple times.\n2. If it's empty (`n == 0`), we immediately return `0.0`.\n3. We count the occurrences of each distinct label using `collections.Counter`.\n4. For each label probability `p = count / n`, we accumulate `-p * log2(p)`.  By convention there is no need to handle `p == 0` explicitly since a zero\u2010count label never appears in the `Counter`.\n5. Finally we round the result to five decimal places with `round(H, 5)` and return it."}
{"task_id": 354, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"Performs a 2-D convolution (cross-correlation).\n\n    Args:\n        X: NumPy array of shape (n_ex, in_rows, in_cols, in_ch) representing the input batch.\n        W: NumPy array of shape (kernel_rows, kernel_cols, in_ch, out_ch) containing the kernels.\n        stride: Stride of the convolution.\n        pad: Padding specification \u2013 integer, tuple or the string 'same'.\n        dilation: Optional dilation factor. 0 corresponds to a normal convolution.\n\n    Returns:\n        The convolution result as a Python list obtained through .tolist().\n    \"\"\"\n    # input dims\n    n_ex, in_rows, in_cols, in_ch = X.shape\n    kR, kC, k_in_ch, out_ch = W.shape\n    assert k_in_ch == in_ch, \"Input channels of X and W must match\"\n    s = stride\n    d = dilation\n\n    # effective kernel size with dilation\n    # if d=0, eff = k; else eff = k + (k-1)*d\n    eff_kR = kR + (kR - 1) * d\n    eff_kC = kC + (kC - 1) * d\n\n    # parse pad\n    if isinstance(pad, str) and pad.lower() == 'same':\n        # smallest symmetric padding so that output spatial dims == input dims\n        # out_rows = in_rows  =>  (in_rows + pad_rows - eff_kR)/s + 1 = in_rows\n        # => pad_rows = (in_rows - 1)*s + eff_kR - in_rows\n        pad_rows = max((in_rows - 1) * s + eff_kR - in_rows, 0)\n        pad_cols = max((in_cols - 1) * s + eff_kC - in_cols, 0)\n        pr_top = pad_rows // 2\n        pr_bot = pad_rows - pr_top\n        pc_left = pad_cols // 2\n        pc_right = pad_cols - pc_left\n    else:\n        # numeric or tuple\n        if isinstance(pad, int):\n            pr_top = pr_bot = pad\n            pc_left = pc_right = pad\n        else:\n            # tuple of length 2 or 4\n            if len(pad) == 2:\n                pr_top = pr_bot = pad[0]\n                pc_left = pc_right = pad[1]\n            elif len(pad) == 4:\n                pr_top, pr_bot, pc_left, pc_right = pad\n            else:\n                raise ValueError(\"Invalid pad specification\")\n    # pad X: only spatial dims\n    X_pad = np.pad(\n        X,\n        ((0, 0),\n         (pr_top, pr_bot),\n         (pc_left, pc_right),\n         (0, 0)),\n        mode='constant',\n        constant_values=0\n    )\n    # new spatial dims\n    H_pad = in_rows + pr_top + pr_bot\n    W_pad = in_cols + pc_left + pc_right\n    # output spatial dims\n    out_rows = (H_pad - eff_kR) // s + 1\n    out_cols = (W_pad - eff_kC) // s + 1\n\n    # =========================\n    # im2col\n    # =========================\n    # 1) compute the offsets for the kR*kC positions in the dilated kernel\n    # row offsets: [0, d+1, 2*(d+1), ..., (kR-1)*(d+1)]\n    row_offsets = np.repeat(np.arange(kR) * (d + 1), kC)\n    col_offsets = np.tile(np.arange(kC) * (d + 1), kR)\n    # 2) compute the starting points for each sliding window\n    out_row_positions = np.arange(0, H_pad - eff_kR + 1, s)\n    out_col_positions = np.arange(0, W_pad - eff_kC + 1, s)\n    # 3) build 3D grids of indices\n    # shapes: (kR*kC, out_rows, out_cols)\n    rows_idx = row_offsets[:, None, None] + out_row_positions[None, :, None]\n    cols_idx = col_offsets[:, None, None] + out_col_positions[None, None, :]\n\n    # 4) gather\n    # transpose X_pad to (n_ex, in_ch, H_pad, W_pad) to index spatial dims 2&3\n    Xp = X_pad.transpose(0, 3, 1, 2)\n    # X_cols: shape (n_ex, in_ch, kR*kC, out_rows, out_cols)\n    X_cols = Xp[:, :, rows_idx, cols_idx]\n\n    # reshape X_cols -> (n_ex*out_rows*out_cols, in_ch*kR*kC)\n    # first reorder to (n_ex, out_rows, out_cols, in_ch, kR*kC)\n    X_cols = X_cols.transpose(0, 3, 4, 1, 2)\n    NOL = n_ex * out_rows * out_cols\n    X_matrix = X_cols.reshape(NOL, in_ch * (kR * kC))\n\n    # =========================\n    # prepare W as matrix\n    # =========================\n    # W: (kR, kC, in_ch, out_ch)\n    # we want W_col of shape (in_ch*kR*kC, out_ch)\n    # so that X_matrix @ W_col = (NOL, out_ch)\n    W_col = W.transpose(2, 0, 1, 3).reshape(in_ch * (kR * kC), out_ch)\n\n    # =========================\n    # GEMM and reshape\n    # =========================\n    out_matrix = X_matrix.dot(W_col)                # shape (NOL, out_ch)\n    out = out_matrix.reshape(n_ex, out_rows, out_cols, out_ch)\n\n    return out.tolist()\n```\n\nExplanation of the key steps:\n\n1. **Padding computation**  \n   - If `pad='same'`, we solve for the total padding so that  \n     `(in_rows + pad_rows - eff_kR)/stride + 1 == in_rows`,  \n     and similarly for the columns, then split it symmetrically (floor/ceil).  \n   - Otherwise we handle the integer / 2-tuple / 4-tuple cases as specified.\n\n2. **Effective kernel size with dilation**  \n   - A dilation of `d` means we insert `d` zeros between consecutive kernel elements.  \n   - Hence the \u201ceffective\u201d filter height is `kR + (kR-1)*d`.\n\n3. **im2col indexing**  \n   - We build `rows_idx` and `cols_idx`, each of shape `(kR*kC, out_rows, out_cols)`, which tell us, for each kernel position and each output location, which input-pixel indices to pick.  \n   - By transposing `X_pad` to `(n_ex, in_ch, H_pad, W_pad)` and doing `Xp[:, :, rows_idx, cols_idx]`, NumPy\u2019s advanced indexing broadcasts out to `(n_ex, in_ch, kR*kC, out_rows, out_cols)`.  \n   - We then reorder axes to `(n_ex, out_rows, out_cols, in_ch, kR*kC)` and flatten the last two dims into `(in_ch*kR*kC)`.\n\n4. **Matrix multiply**  \n   - We reshape the filters `W` into `(in_ch*kR*kC, out_ch)` in exactly the same patch-major order as we laid out `X_matrix`.  \n   - A single `dot` gives us the result patches for all examples and all output positions at once.\n\n5. **Reshape and `tolist()`**  \n   - The final result is reshaped back to `(n_ex, out_rows, out_cols, out_ch)` and converted to a nested Python list."}
{"task_id": 355, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array in which rows correspond to samples and columns to features.\n    n_components : int\n        Number of principal components to retain (must be \u22651).\n    solver : str, optional (default=\"svd\")\n        ``\"svd\"`` to use singular value decomposition or ``\"eigen\"`` to use\n        eigen-decomposition of the covariance matrix.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 The projected data (scores) as a list of lists \u2013 each inner list is a\n          sample represented in the new sub-space.\n        \u2022 The list of explained variance ratios corresponding to the kept\n          components.\n    \"\"\"\n    # 1. Mean\u2010centre the data\n    X = np.asarray(data, dtype=float)\n    n_samples, n_features = X.shape\n    mean = X.mean(axis=0)\n    X_centered = X - mean\n\n    # 2. Obtain principal directions\n    if solver == \"svd\":\n        # SVD on centered data\n        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n        # Principal directions are the rows of Vt; shape: (k, n_features)\n        # Eigenvalues of covariance are S^2/(n_samples-1)\n        eigvals = (S**2) / (n_samples - 1)\n        # Make directions shape (n_features, k)\n        directions = Vt.T\n    elif solver == \"eigen\":\n        # Covariance matrix from centered data\n        cov = (X_centered.T @ X_centered) / (n_samples - 1)\n        eigvals_all, eigvecs_all = np.linalg.eigh(cov)\n        # Sort descending\n        idx = np.argsort(eigvals_all)[::-1]\n        eigvals = eigvals_all[idx]\n        directions = eigvecs_all[:, idx]\n    else:\n        raise ValueError(\"solver must be 'svd' or 'eigen'\")\n\n    # 3. Select top components\n    # limit number of components to available ones\n    k_available = eigvals.shape[0]\n    k = min(n_components, k_available)\n    eigvals_sel = eigvals[:k]\n    directions_sel = directions[:, :k]  # shape (n_features, k)\n\n    # 4. Project data\n    scores = X_centered @ directions_sel  # shape (n_samples, k)\n\n    # 5. Deterministic sign\u2010fix for each component\n    tol = 1e-12\n    for j in range(k):\n        # find first loading that is not (almost) zero\n        col = directions_sel[:, j]\n        for i in range(n_features):\n            if abs(col[i]) > tol:\n                if col[i] < 0:\n                    # flip sign\n                    directions_sel[:, j] = -directions_sel[:, j]\n                    scores[:, j] = -scores[:, j]\n                break\n\n    # 6. Explained variance ratio\n    total_var = eigvals.sum()\n    explained_variance_ratio = eigvals_sel / total_var\n\n    # 7. Round final results\n    scores_rounded = np.round(scores, 4).tolist()\n    evr_rounded = np.round(explained_variance_ratio, 4).tolist()\n\n    return scores_rounded, evr_rounded\n```\n\nExplanation of key steps:\n\n1. We subtract the column\u2010wise means (`mean`) to centre the data.\n2. For `\"svd\"`, we perform `np.linalg.svd` on the centred matrix `X_centered` (with `full_matrices=False`), extract singular values `S` and right singular vectors `Vt`.  The variances (eigenvalues of the covariance matrix) are `S**2/(n_samples-1)`, and the principal directions are the rows of `Vt`.\n3. For `\"eigen\"`, we build the covariance matrix directly from the centred data, perform `np.linalg.eigh`, and sort eigenvalues and eigenvectors descendingly.\n4. We keep `k = min(n_components, available_components)` directions.\n5. We project (`scores`) by `X_centered @ directions_sel`.\n6. To resolve the sign ambiguity, for each component we look at its first non\u2010zero loading; if it\u2019s negative, we flip the sign of that component\u2019s direction and the corresponding column in `scores`.\n7. Finally, we compute the explained variance ratio, round both `scores` and `explained_variance_ratio` to 4 decimal places, and convert them to built\u2010in Python lists as required."}
{"task_id": 356, "completion_id": 0, "completion": "```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        # leaf.value is expected to be a sequence of probabilities.\n        probs = leaf.value\n        # Find the index of the largest probability; ties go to the first (smallest index).\n        # max over indices with key=probs.__getitem__ achieves that.\n        idx = max(range(len(probs)), key=probs.__getitem__)\n        return int(idx)\n    else:\n        # Regression: just return the stored scalar (could be int or float).\n        return leaf.value\n```\n\nExplanation:\n\n1. When `classifier=True`, we assume `leaf.value` is a sequence (list/tuple) of probabilities.  \n2. `max(range(len(probs)), key=probs.__getitem__)` finds the index whose probability is largest; because `max` returns the first maximal index on ties, this matches the requirement of favoring smaller indices.  \n3. We then cast that index to `int` and return it.  \n4. When `classifier=False`, we simply return the raw scalar stored in the leaf."}
{"task_id": 357, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    # Number of samples and dimensionality\n    n_samples, n_features = data.shape\n\n    # Initialize centroids: first K samples\n    centroids = data[:K].astype(float, copy=True)\n\n    tol = 1e-6\n    for _ in range(max_iters):\n        # 1) Assign each sample to the closest centroid (using squared Euclidean distance).\n        #    distances shape: (n_samples, K)\n        diffs = data[:, None, :] - centroids[None, :, :]  # shape: (n_samples, K, n_features)\n        dists_sq = np.sum(diffs * diffs, axis=2)          # shape: (n_samples, K)\n        assignments = np.argmin(dists_sq, axis=1)         # shape: (n_samples,)\n\n        # 2) Recompute centroids as the mean of assigned points\n        new_centroids = centroids.copy()\n        for j in range(K):\n            pts = data[assignments == j]\n            if pts.shape[0] > 0:\n                new_centroids[j] = pts.mean(axis=0)\n            # else: leave centroid unchanged\n\n        # 3) Check for convergence\n        shift = np.abs(new_centroids - centroids).max()\n        centroids = new_centroids\n        if shift <= tol:\n            break\n\n    # Round to 4 decimals and sort lexicographically\n    centroids_rounded = np.round(centroids, 4).tolist()\n    centroids_rounded.sort()\n\n    return centroids_rounded\n```\n\nExplanation of key steps:\n\n1. **Initialization**  \n   We take the first `K` rows of `data` as our initial centroids, cast to float.\n\n2. **Assignment**  \n   For each point, we compute the squared Euclidean distance to all centroids in a vectorized manner:\n   ```\n   diffs = data[:, None, :] - centroids[None, :, :]\n   dists_sq = (diffs**2).sum(axis=2)\n   assignments = argmin(dists_sq, axis=1)\n   ```\n   This gives us an array of cluster indices for each sample.\n\n3. **Update**  \n   We loop over each centroid index `j`, select all points assigned to it, and recompute the centroid as their mean. If no points are assigned to centroid `j`, we leave it unchanged.\n\n4. **Convergence Check**  \n   We measure the maximum absolute shift of any centroid. If it\u2019s below `1e-6`, we stop early.\n\n5. **Final Processing**  \n   We round each coordinate of the centroids to 4 decimal places, convert to a Python list of lists, and sort lexicographically before returning.\n\nThis implementation works for any number of features (dimensions) \u2265 1, handles the `K=1` case naturally, and ensures determinism by using the first `K` samples as the initial centroids."}
{"task_id": 362, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef row_stochastic_matrix(\n    data: list[list[int | float]] | \"np.ndarray\"\n) -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    # 1. Convert to float NumPy array\n    arr = np.array(data, dtype=float)\n\n    #  Ensure it's 2-D\n    if arr.ndim != 2:\n        raise ValueError(\"Input must be a 2-D structure\")\n\n    # 2. Compute the sum of every row\n    row_sums = arr.sum(axis=1)\n\n    # 3. If any row sum is zero, return -1\n    if np.any(row_sums == 0):\n        return -1\n\n    # 4. Divide each row by its sum\n    #    We reshape row_sums to (n_rows, 1) so broadcasting works\n    normalized = arr / row_sums[:, np.newaxis]\n\n    # 5. Round to 4 decimal places and return as Python list\n    rounded = np.round(normalized, 4)\n    return rounded.tolist()\n```\n\nExample usage:\n\n```python\n>>> row_stochastic_matrix([[1, 1, 2], [0.5, 0.5, 0]])\n[[0.2000, 0.2000, 0.6000], [0.5000, 0.5000, 0.0000]]\n\n>>> row_stochastic_matrix([[0, 0, 0], [1, 2, 3]])\n-1\n```"}
{"task_id": 363, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    def _check(space: Space, is_composite: bool) -> bool:\n        # If not composite, just check for Box\n        if not is_composite:\n            return isinstance(space, Box)\n        # If composite, unpack sub-spaces and ensure each is a Box\n        if isinstance(space, Tuple):\n            subs = space.spaces\n        elif isinstance(space, Dict):\n            subs = space.spaces.values()\n        else:\n            # In case the flag is wrong, fall back to atomic check\n            return isinstance(space, Box)\n        return all(isinstance(sub, Box) for sub in subs)\n\n    cont_action = _check(env.action_space, tuple_action)\n    cont_obs    = _check(env.observation_space, tuple_obs)\n    return (cont_action, cont_obs)\n```\n\nExplanation:\n\n1. We define a helper `_check(space, is_composite)`:\n   - If `is_composite` is `False`, we simply return whether `space` is a `Box`.\n   - If `is_composite` is `True`, we first see if `space` is a `Tuple` or `Dict`.  \n     - For `Tuple`, we pull out `space.spaces` (an iterable of sub-spaces).  \n     - For `Dict`, we use `space.spaces.values()`.  \n     - We then verify that *every* sub-space is an instance of `Box`.  \n   - If somehow the flag says composite but the space is neither `Tuple` nor `Dict`, we defensively fall back to an atomic `Box` check.\n\n2. We call `_check` for both `action_space` (with `tuple_action`) and `observation_space` (with `tuple_obs`) and return the pair of booleans."}
{"task_id": 369, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray],\n                              learning_rate: float,\n                              regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        raise ValueError(\"The 'updates' list must contain at least one array.\")\n    \n    # Initialize running prediction with zeros, same shape as one update\n    running_pred = np.zeros_like(updates[0], dtype=float)\n    \n    # Aggregate each tree's contribution\n    for u in updates:\n        running_pred -= learning_rate * u\n\n    if regression:\n        # Round to 4 decimal places for regression\n        return np.round(running_pred, 4)\n    else:\n        # Multi-class classification:\n        # running_pred shape = (n_samples, n_classes)\n        # Compute softmax in a numerically stable way\n        # 1) shift by the row-wise max\n        row_max = np.max(running_pred, axis=1, keepdims=True)\n        exp_shifted = np.exp(running_pred - row_max)\n        probs = exp_shifted / np.sum(exp_shifted, axis=1, keepdims=True)\n        # 2) predicted class is the argmax over classes\n        return np.argmax(probs, axis=1)\n```\n\nExplanation:\n\n1. We start a zero `running_pred` of the same shape as one tree\u2019s output.\n2. For each tree\u2019s update array `u`, we subtract `learning_rate * u` from `running_pred`.\n3. If `regression` is `True`, we round the final predictions to 4 decimals and return them.\n4. Otherwise we treat `running_pred` as logits, apply a stable row-wise softmax to get class probabilities, and then return the index of the maximum probability for each sample."}
{"task_id": 371, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\n\n    The function must support batches, multiple input/output channels, padding\n    (integer, tuple or \"same\"), arbitrary stride and dilation.  It returns\n    the output volume as a list via ndarray.tolist().\n\n    Args:\n        X: ndarray of shape (n_examples, signal_length, in_channels)\n        W: ndarray of shape (kernel_width, in_channels, out_channels)\n        stride: positive integer, the step size of the sliding window\n        pad:  int, 2-tuple, or the string \"same\"\n        dilation: non-negative integer, number of zeros inserted between\n                  neighbouring kernel elements (0 \u21d2 standard convolution)\n\n    Returns:\n        A Python list representing the convolved volume with shape\n        (n_examples, output_length, out_channels).\n    \"\"\"\n    # Validate inputs\n    if not isinstance(X, np.ndarray) or X.ndim != 3:\n        raise ValueError(\"X must be a 3D numpy array of shape (n, L, in_channels).\")\n    if not isinstance(W, np.ndarray) or W.ndim != 3:\n        raise ValueError(\"W must be a 3D numpy array of shape (kernel_width, in_channels, out_channels).\")\n    if not (isinstance(stride, int) and stride > 0):\n        raise ValueError(\"stride must be a positive integer.\")\n    if not (isinstance(dilation, int) and dilation >= 0):\n        raise ValueError(\"dilation must be a non-negative integer.\")\n\n    n_examples, L, in_ch = X.shape\n    kw, in_ch_W, out_ch = W.shape\n    if in_ch_W != in_ch:\n        raise ValueError(\"The in_channels dimension of W must match X.\")\n\n    # Compute effective kernel size with dilation\n    step = dilation + 1\n    effective_kernel = (kw - 1) * step + 1\n\n    # Determine padding\n    if isinstance(pad, str):\n        if pad.lower() != \"same\":\n            raise ValueError('pad string must be \"same\"')\n        # SAME padding\n        out_len = math.ceil(L / stride)\n        total_pad = max(0, (out_len - 1) * stride + effective_kernel - L)\n        pad_left = total_pad // 2\n        pad_right = total_pad - pad_left\n    elif isinstance(pad, int):\n        pad_left = pad_right = pad\n    elif (isinstance(pad, tuple) or isinstance(pad, list)) and len(pad) == 2:\n        pad_left, pad_right = pad\n    else:\n        raise ValueError(\"pad must be int, 2-tuple, or 'same'\")\n\n    # Pad the input on the length dimension\n    X_padded = np.pad(\n        X,\n        pad_width=((0, 0), (pad_left, pad_right), (0, 0)),\n        mode=\"constant\",\n        constant_values=0\n    )\n    L_padded = X_padded.shape[1]\n\n    # Output length (if not SAME, compute here)\n    if not (isinstance(pad, str) and pad.lower() == \"same\"):\n        out_len = (L_padded - effective_kernel) // stride + 1\n        if out_len < 0:\n            out_len = 0\n\n    # Prepare output\n    Y = np.zeros((n_examples, out_len, out_ch), dtype=X.dtype)\n\n    # Perform cross-correlation\n    for n in range(n_examples):\n        for t in range(out_len):\n            start = t * stride\n            for k in range(kw):\n                idx = start + k * step\n                # slice: shape (in_ch,)\n                x_slice = X_padded[n, idx, :]  # shape (in_ch,)\n                # W[k, :, :] has shape (in_ch, out_ch)\n                # outer product/ multiply-accumulate\n                # x_slice[:, None] * W[k, :, :] => shape (in_ch, out_ch)\n                # sum over in_ch axis\n                Y[n, t, :] += x_slice.dot(W[k, :, :])\n\n    return Y.tolist()\n```\n\nExplanation of key steps:\n\n1. **Input validation**  \n   - Check shapes and types of `X`, `W`, `stride`, and `dilation`.\n\n2. **Effective kernel size**  \n   - With dilation, the distance between consecutive kernel taps is `dilation + 1`.  \n   - `effective_kernel = (kernel_width - 1)*(dilation + 1) + 1`.\n\n3. **Padding**  \n   - If `pad=\"same\"`, use TensorFlow\u2019s SAME padding rule:  \n     ```\n     out_len = ceil(L / stride)\n     total_pad = max(0, (out_len - 1)*stride + effective_kernel - L)\n     pad_left = floor(total_pad/2)\n     pad_right = total_pad - pad_left\n     ```  \n   - If `pad` is an integer, use it both sides; if a 2-tuple, unpack.\n\n4. **Convolution loop**  \n   - After padding `X`, slide the window by `stride` steps.  \n   - For each output time-step `t`, and each kernel index `k`, multiply the slice of `X_padded` by the `k`th kernel plane in `W` and accumulate.\n\n5. **Return**  \n   - Convert the final NumPy array to a nested Python list via `tolist()`."}
{"task_id": 373, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini(y: list[int] |np.ndarray -> float:\n   Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n       # Convert input to a NumPy array for uniform handling\n    y_arr = np.asarray(y)\n    N = y_arr.size\n    if N == 0:\n        return 0.0\n\n    # Count occurrences of each class\n    classes, counts = np.unique(y_arr, return_counts=True)\n    probs = counts / N\n\n    # Gini impurity formula\n    gini_val = 1.0 - np.sum(probs ** 2)\n    return float(np.round(gini_val, 4))\n```\n\nExplanation of steps:\n1. We first convert the input `y` to a NumPy array so we can uniformly handle both lists and arrays.\n2. If the sequence is empty (`N == 0`), we return 0.0 by convention.\n3. Otherwise, we compute the counts of each distinct class via `np.unique(..., return_counts=True)`.\n4. We form the class probabilities `p_c = counts / N`, then compute `1 - sum(p_c^2)`.\n5. Finally, we round the result to 4 decimal places and return it as a Python float."}
{"task_id": 374, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gd(\n    X: np.ndarray,\n    y: np.ndarray,\n    learning_rate: float = 0.01,\n    n_iterations: int = 10000\n) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\n\n    Parameters:\n    - X: Feature matrix of shape (m, n)\n    - y: Target vector of shape (m,)\n    - learning_rate: Step size for gradient descent\n    - n_iterations: Number of iterations to run gradient descent\n\n    Returns:\n    - (weights_list, bias) where:\n      * weights_list is a list of n floats (\u03b8 rounded to 4 decimals)\n      * bias is a float (b rounded to 4 decimals)\n    - If X and y have incompatible sample sizes, returns -1\n    \"\"\"\n    # Ensure X is 2D and y is 1D\n    X = np.asarray(X)\n    y = np.asarray(y).flatten()\n\n    # Number of samples (m) and features (n)\n    if X.ndim != 2:\n        return -1\n    m, n = X.shape\n\n    # Check consistency\n    if y.shape[0] != m or m == 0:\n        return -1\n\n    # Initialize parameters\n    theta = np.zeros(n, dtype=float)\n    b = 0.0\n\n    # Batch Gradient Descent\n    for _ in range(n_iterations):\n        # Predictions\n        y_hat = X.dot(theta) + b\n\n        # Errors\n        errors = y_hat - y\n\n        # Gradients\n        dw = (2.0 / m) * X.T.dot(errors)\n        db = (2.0 / m) * errors.sum()\n\n        # Parameter updates\n        theta -= learning_rate * dw\n        b -= learning_rate * db\n\n    # Round the results to 4 decimal places\n    theta_rounded = np.round(theta, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n\n    return theta_rounded, b_rounded\n```\n\nExplanation of key steps:\n1. **Input validation**: We convert inputs to NumPy arrays, ensure `X` is 2D, flatten `y` to 1D, and check `X.shape[0] == y.shape[0]`. If they mismatch (or if there are zero samples), we return `-1`.\n2. **Parameter initialization**: `theta` is an n-dimensional zero vector, and `b` is initialized to `0.0`.\n3. **Gradient descent loop** for `n_iterations`:\n   - Compute predictions `y_hat = X\u00b7\u03b8 + b`.\n   - Compute errors `errors = y_hat - y`.\n   - Compute gradients  \n     `dw = (2/m) * X\u1d40\u00b7errors` and  \n     `db = (2/m) * sum(errors)`.\n   - Update parameters:  \n     `\u03b8 \u2190 \u03b8 \u2212 lr\u00b7dw`, `b \u2190 b \u2212 lr\u00b7db`.\n4. **Rounding**: We round each weight in `\u03b8` and the bias `b` to 4 decimal places before returning.\n\nThe function returns a tuple `(weights_list, bias)` as specified."}
{"task_id": 376, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose,\n    up to a tolerance for floating\u2010point comparisons.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Number of rows\n    n = len(X)\n    # Quick check: it must be square, so each row must have length n\n    for row in X:\n        if len(row) != n:\n            return False\n\n    # Convert to a NumPy array of floats\n    try:\n        arr = np.array(X, dtype=float)\n    except (ValueError, TypeError):\n        # In case the data cannot cleanly be cast to float\n        return False\n\n    # Must be 2D and square\n    if arr.ndim != 2 or arr.shape[0] != arr.shape[1]:\n        return False\n\n    # Compare to its transpose within absolute tolerance 1e-8\n    return bool(np.allclose(arr, arr.T, atol=1e-8, rtol=0.0))\n```\n\nExplanation of key steps:\n- We first ensure the input is square by checking that each of the `n` rows has exactly `n` columns.\n- We convert the list-of-lists to a NumPy float array; if that fails, we bail out with `False`.\n- We verify the array is truly 2-dimensional and square.\n- Finally, we use `np.allclose` with `atol=1e-8, rtol=0.0` so that two entries are considered equal if their absolute difference is at most 1\u00d710\u207b\u2078."}
{"task_id": 377, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Compute the negative gradient for the logistic (binomial deviance) loss used in\n    gradient boosting for binary classification.\n\n    Parameters\n    ----------\n    y : list of int\n        True binary labels, each element must be 0 or 1.\n    f : list of float\n        Current prediction scores for each sample (real-valued).\n\n    Returns\n    -------\n    List[float]\n        The negative gradient g_i for each sample, rounded to 4 decimal places.\n        If `y` contains values other than 0 or 1, returns -1.\n\n    Notes\n    -----\n    Internally, labels y_i are converted to y'_i = 2*y_i - 1 in {-1, +1}, and the\n    logistic loss is\n        L(y', f) = log(1 + exp(-2 * y' * f))\n    Its negative gradient w.r.t. f is\n        g_i = y'_i / (1 + exp(y'_i * f_i))\n    \"\"\"\n    # Convert to numpy arrays for vectorized operations\n    y_arr = np.array(y)\n    f_arr = np.array(f, dtype=float)\n\n    # Check that y only contains 0 or 1\n    unique_labels = np.unique(y_arr)\n    if not set(unique_labels).issubset({0, 1}):\n        return -1\n\n    # Convert labels to {-1, +1}\n    y_prime = 2 * y_arr - 1\n\n    # Compute negative gradient: g_i = y'_i / (1 + exp(y'_i * f_i))\n    denom = 1.0 + np.exp(y_prime * f_arr)\n    gradients = y_prime / denom\n\n    # Round to 4 decimal places and return as Python list\n    return np.round(gradients, 4).tolist()\n```"}
{"task_id": 380, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n   Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n       # Constants from the SELU paper\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n\n    # Convert input to numpy array\n    arr = np.array(x, dtype=float)\n    # Prepare an output array of the same shape\n    res = np.empty_like(arr)\n\n    # Boolean mask for positive vs non-positive entries\n    pos = arr > 0\n\n    if order == 0:\n        # SELU activation\n        res[pos] = scale * arr[pos]\n        res[~pos] = scale * alpha * (np.exp(arr[~pos]) - 1)\n    elif order == 1:\n        # First derivative\n        res[pos] = scale\n        res[~pos] = scale * alpha * np.exp(arr[~pos])\n    elif order == 2:\n        # Second derivative\n        res[pos] = 0.0\n        res[~pos] = scale * alpha * np.exp(arr[~pos])\n    else:\n        raise ValueErrororder must be 0, 1, or 2    # Round to 4 decimals and convert to Python list (or scalar)\n    return np.round(res, 4).tolist()\n```\n\nExplanation of the piecewise definitions:\n\n- For x > 0:\n  - SELU(x) = scale * x\n  - d/dx SELU(x) = scale\n  - d\u00b2/dx\u00b2 SELU(x) = 0\n- For x \u2264 0:\n  - SELU(x) = scale * \u03b1 (e\u02e3 \u2013 1)\n  - d/dx SELU(x) = scale * \u03b1 e\u02e3\n  - d\u00b2/dx\u00b2 SELU(x) = scale * \u03b1 e\u02e3\n\nThe result is rounded to the fourth decimal place and returned via `tolist()`, preserving scalar or array structure."}
{"task_id": 387, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n   Train a 1-D gradient boosting regressor made of decision stumps.\n\n    Args:\n        x: A list with a single numeric feature for every sample.\n        y: Target values.\n        n_estimators: Number of boosting rounds.\n        learning_rate: Shrinkage applied to every stump (\u03b7).\n\n    Returns:\n        In-sample predictions rounded to 4 decimals and converted to a Python list.\n       # Convert to numpy arrays\n    x_arr = np.array(x, dtype=float)\n    y_arr = np.array(y, dtype=float)\n    N = x_arr.shape[0]\n    \n    # Initialize predictions to the global mean\n    y_pred = np.full(N, y_arr.mean(), dtype=float)\n    \n    # Pre-sort x once (we'll refer to this order for splits)\n    sort_idx = np.argsort(x_arr)\n    x_sorted = x_arr[sort_idx]\n    \n    for _ in range(n_estimators):\n        # Compute residuals\n        residuals = y_arr - y_pred\n        \n        # Check if we can split at all\n        unique_x = np.unique(x_sorted)\n        if unique_x.size == 1:\n            # No split possible, stump predicts global mean residual\n            mean_res = residuals.mean()\n            y_pred += learning_rate * mean_res\n            continue\n        \n        # Sort residuals in the same order as x_sorted\n        r_sorted = residuals[sort_idx]\n        \n        # Precompute cumulative sums for fast SSE calculation\n        csum_r = np.cumsum(r_sorted)\n        csum_r2 = np.cumsum(r_sorted ** 2)\n        total_r = csum_r[-1]\n        total_r2 = csum_r2[-1]\n        \n        best_sse = np.inf\n        best_i = None\n        \n        # Consider splits between positions i and i+1 where x changes\n        for i in range(N - 1):\n            if x_sorted[i] == x_sorted[i + 1]:\n                continue\n            # Left partition size and sums\n            n_L = i + 1\n            sum_r_L = csum_r[i]\n            sum_r2_L = csum_r2[i]\n            mean_L = sum_r_L / n_L\n            sse_L = sum_r2_L - (sum_r_L ** 2) / n_L\n            \n            # Right partition\n            n_R = N - n_L\n            sum_r_R = total_r - sum_r_L\n            sum_r2_R = total_r2 - sum_r2_L\n            mean_R = sum_r_R / n_R\n            sse_R = sum_r2_R - (sum_r_R ** 2) / n_R\n            \n            sse = sse_L + sse_R\n            if sse < best_sse:\n                best_sse = sse\n                best_i = i\n        \n        # Determine optimal split threshold and predictions\n        t_star = (x_sorted[best_i] + x_sorted[best_i + 1]) / 2.0\n        # Compute means for this best split\n        # Left side\n        n_L = best_i + 1\n        sum_r_L = csum_r[best_i]\n        mean_L = sum_r_L / n_L\n        # Right side\n        sum_r_R = total_r - sum_r_L\n        n_R = N - n_L\n        mean_R = sum_r_R / n_R\n        \n        # Form stump prediction for each original sample\n        stump_pred = np.where(x_arr <= t_star, mean_L, mean_R)\n        \n        # Update ensemble prediction\n        y_pred += learning_rate * stump_pred\n    \n    # Round to 4 decimals and return as Python list\n    return np.round(y_pred, 4).tolist()\n```"}
{"task_id": 394, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n   Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n       eps = 1e-8\n\n    # 1) Initialize Eg if it's None\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n\n    # 2) Update running average of squared gradients\n    Eg_next = rho * Eg + (1 - rho) * np.square(grad)\n\n    # 3) Compute parameter update\n    w_next = w - learning_rate * grad / np.sqrt(Eg_next + eps)\n\n    # 4) Round to 4 decimal places and convert to Python lists\n    w_next_rounded = np.round(w_next, 4).tolist()\n    Eg_next_rounded = np.round(Eg_next, 4).tolist()\n\n    return w_next_rounded, Eg_next_rounded\n```\n\nExplanation of key steps:\n\n1. If `Eg` is `None`, we create a zero array of the same shape as `grad`.\n2. We compute the new running average `Eg_next` using the formula  \n   `Eg_next = rho * Eg + (1 - rho) * grad**2`.\n3. We update the parameters as  \n   `w_next = w - \u03b1 * grad / sqrt(Eg_next + \u03b5)`.\n4. Finally, both outputs are rounded to 4 decimal places and converted to standard Python lists before returning."}
{"task_id": 398, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If True, return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    # Convert input to a NumPy array of floats\n    x_arr = np.array(x, dtype=float)\n\n    if not derivative:\n        # ELU activation: x if x>=0, else alpha*(exp(x)-1)\n        out = np.where(x_arr >= 0,\n                       x_arr,\n                       alpha * (np.exp(x_arr) - 1.0))\n    else:\n        # ELU derivative: 1 if x>=0, else ELU(x) + alpha\n        # First compute ELU(x) for the negative branch\n        neg_elu = alpha * (np.exp(x_arr) - 1.0)\n        out = np.where(x_arr >= 0,\n                       1.0,\n                       neg_elu + alpha)\n\n    # Round to 4 decimal places and convert to Python list\n    return np.round(out, 4).tolist()\n```\n\nExplanation of key steps:\n1. We first cast the input `x` to a NumPy array of floats, so we can use vectorized operations.\n2. If `derivative` is False, we apply the ELU formula:\n   - For elements `>= 0`, output is the element itself.\n   - For elements `< 0`, output is `alpha * (exp(x) - 1)`.\n3. If `derivative` is True, we compute:\n   - For elements `>= 0`, the derivative is `1`.\n   - For elements `< 0`, the derivative is `ELU(x) + alpha`, which simplifies to `alpha * exp(x)`, but we compute it as shown for clarity.\n4. Finally, we round each result to 4 decimal places and return as a regular Python list."}
{"task_id": 411, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numbers\n\ndef env_stats(actions: list, observations: list) -> dict:\n   Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n       def _compute(data):\n        stats = {}\n\n        # 1) Detect if any element is an iterable of scalars (tuple/list/ndarray)\n        tuple_flag = any(isinstance(x, (list, tuple, np.ndarray)) for x in data)\n        stats['tuple'] = tuple_flag\n\n        # 2) Determine dimension: if iterable take its length, else 1\n        if tuple_flag:\n            # find first iterable element\n            for x in data:\n                if isinstance(x, (list, tuple, np.ndarray)):\n                    dim = len(x)\n                    break\n        else:\n            dim = 1\n        stats['dim'] = dim\n        stats['multidim'] = (dim > 1)\n\n        # 3) Build a 2D list of shape (N, dim) with native Python scalars\n        rows = []\n        for x in data:\n            if dim == 1:\n                elems = [x]\n            else:\n                elems = list(x)\n            # convert numpy scalars to native Python types\n            clean = []\n            for v in elems:\n                if isinstance(v, np.generic):\n                    clean.append(v.item())\n                else:\n                    clean.append(v)\n            rows.append(clean)\n\n        # 4) Detect if any value is a non-integer float => continuous\n        cont = False\n        for row in rows:\n            for v in row:\n                # check python floats\n                if isinstance(v, float):\n                    if not v.is_integer():\n                        cont = True\n                        break\n                # check numpy floats\n                elif isinstance(v, np.floating):\n                    if not float(v).is_integer():\n                        cont = True\n                        break\n            if cont:\n                break\n        stats['continuous'] = cont\n\n        # 5) For each dimension, collect unique sorted values\n        ids = []\n        n_per_dim = []\n        for d in range(dim):\n            col = [row[d] for row in rows]\n            unique = set(col)\n            # sort numerically\n            sorted_u = sorted(unique, key=lambda x: (isinstance(x, str), x))\n            ids.append(sorted_u)\n            n_per_dim.append(len(sorted_u))\n\n        stats['ids'] = ids\n        stats['n_per_dim'] = n_per_dim\n\n        return stats\n\n    # compute stats for actions and observations\n    a_stats = _compute(actions)\n    o_stats = _compute(observations)\n\n    # assemble final dictionary with required key names\n    return {\n        'tuple_actions':           a_stats['tuple'],\n        'tuple_observations':      o_stats['tuple'],\n        'multidim_actions':        a_stats['multidim'],\n        'multidim_observations':   o_stats['multidim'],\n        'continuous_actions':      a_stats['continuous'],\n        'continuous_observations': o_stats['continuous'],\n        'n_actions_per_dim':       a_stats['n_per_dim'],\n        'action_dim':              a_stats['dim'],\n        'n_obs_per_dim':           o_stats['n_per_dim'],\n        'obs_dim':                 o_stats['dim'],\n        'action_ids':              a_stats['ids'],\n        'obs_ids':                 o_stats['ids'],\n    }\n```\n\nExplanation of key steps:\n\n1. We define an inner helper `_compute` that gathers stats for one list (`actions` or `observations`).\n2. `tuple_flag` checks if any element is a list/tuple/ndarray.\n3. The dimensionality `dim` is the length of those iterables (or 1 if all scalars).\n4. We normalize every element into a length\u2010`dim` Python list, converting any NumPy scalar to a native Python type.\n5. We scan all values to see if there is any *non\u2010integer* float (`float` or `np.floating`) to flag `continuous`.\n6. For each dimension index `d`, we collect all values, deduplicate them with a `set`, sort them numerically, and record both the sorted unique values (`ids`) and their count (`n_per_dim`).\n7. Finally we map these generic stats into the exact key names requested by the problem statement."}
{"task_id": 413, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass TreeNode:\n       A node in a regression tree.\n       def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):\n        # for internal nodes\n        self.feature_index = feature_index\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        # for leaf nodes\n        self.value = value\n\n    def predict(self, X):\n               Predict target for each row in X by traversing the tree.\n               # If leaf node, return constant value\n        if self.value is not None:\n            return np.full(X.shape[0], self.value)\n        \n        # Otherwise, split and recurse\n        fidx = self.feature_index\n        thr = self.threshold\n        left_mask = X[:, fidx] <= thr\n        right_mask = ~left_mask\n        \n        y_pred = np.empty(X.shape[0], dtype=float)\n        # left subtree predictions\n        if np.any(left_mask):\n            y_pred[left_mask] = self.left.predict(X[left_mask])\n        # right subtree predictions\n        if np.any(right_mask):\n            y_pred[right_mask] = self.right.predict(X[right_mask])\n        return y_pred\n\ndef build_tree(X, y, depth, max_depth):\n       Recursively build a CART regression tree to fit y from X.\n    Splitting criterion: minimize sum of squared errors (SSE).\n       n_samples, n_features = X.shape\n    \n    # Compute current SSE\n    y_mean = y.mean()\n    sse_parent = np.sum((y - y_mean) ** 2)\n    \n    # Stop conditions\n    # 1) all y are identical -> no further split possible\n    # 2) reached maximum depth\n    if depth >= max_depth or np.all(y == y[0]):\n        return TreeNode(value=y_mean)\n    \n    best_sse = np.inf\n    best_feature = None\n    best_threshold = None\n    \n    # Try splitting on every feature\n    for feature in range(n_features):\n        X_f = X[:, feature]\n        # unique sorted values\n        unique_vals = np.unique(X_f)\n        if unique_vals.shape[0] < 2:\n            continue  # cannot split on constant feature\n        \n        # candidate thresholds: midpoints\n        thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n        \n        for thr in thresholds:\n            left_mask = X_f <= thr\n            right_mask = ~left_mask\n            if not np.any(left_mask) or not np.any(right_mask):\n                continue\n            \n            y_left = y[left_mask]\n            y_right = y[right_mask]\n            \n            # SSE for children\n            left_mean = y_left.mean()\n            right_mean = y_right.mean()\n            sse_left = np.sum((y_left - left_mean) ** 2)\n            sse_right = np.sum((y_right - right_mean) ** 2)\n            sse_total = sse_left + sse_right\n            \n            # Keep the split if it reduces SSE\n            if sse_total < best_sse:\n                best_sse = sse_total\n                best_feature = feature\n                best_threshold = thr\n    \n    # If no split improved SSE, make leaf\n    if best_feature is None or best_sse >= sse_parent:\n        return TreeNode(value=y_mean)\n    \n    # Otherwise, create internal node and recurse\n    left_mask = X[:, best_feature] <= best_threshold\n    right_mask = ~left_mask\n    left_subtree = build_tree(X[left_mask], y[left_mask], depth + 1, max_depth)\n    right_subtree = build_tree(X[right_mask], y[right_mask], depth + 1, max_depth)\n    \n    return TreeNode(\n        feature_index=best_feature,\n        threshold=best_threshold,\n        left=left_subtree,\n        right=right_subtree\n    )\n\ndef gbdt_regression(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    X_test: np.ndarray,\n                    n_estimators: int = 10,\n                    learning_rate: float = 0.1,\n                    max_depth: int = 3) -> list[float]:\n   Gradient Boosting Decision Tree (GBDT) regressor    \n    # Initial prediction: mean of y_train\n    init_pred = np.mean(y_train)\n    # Running predictions on training set\n    F_train = np.full(y_train.shape, init_pred, dtype=float)\n    \n    # Store the sequence of fitted trees\n    trees = []\n    \n    for _ in range(n_estimators):\n        # Compute negative gradient (residuals)\n        residuals = y_train - F_train\n        \n        # Fit a regression tree to the residuals\n        tree = build_tree(X_train, residuals, depth=0, max_depth=max_depth)\n        trees.append(tree)\n        \n        # Update the running predictions\n        update = tree.predict(X_train)\n        F_train += learning_rate * update\n    \n    # Predict on test set\n    F_test = np.full(X_test.shape[0], init_pred, dtype=float)\n    for tree in trees:\n        F_test += learning_rate * tree.predict(X_test)\n    \n    # Round to 4 decimal places and convert to Python list\n    F_test = np.round(F_test, 4).tolist()\n    return F_test\n```\n\nExplanation of key parts:\n1. **TreeNode class**: holds either a leaf value or a split (feature index, threshold) plus left/right children.\n2. **build_tree**: recursively finds the best split by minimizing sum of squared errors (SSE).  \n   - Stops when depth reaches `max_depth` or no split reduces error.\n   - Leaves store the mean target of their node.\n3. **GBDT loop**:\n   - Start with constant prediction = mean of `y_train`.\n   - For each iteration, compute residuals (`y_train - current_pred`), fit a tree to those residuals, and update predictions with `learning_rate * tree_predict`.\n   - At the end, apply the same additive model to `X_test`.\n4. Outputs are rounded to 4 decimal places and returned as a standard Python list."}
{"task_id": 416, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n       Compute the multivariate Gaussian (normal) PDF for each sample in X.\n    \n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features) or (n_samples,)\n        Data points at which to evaluate the PDF. If one-dimensional,\n        it will be treated as (n_samples, 1).\n    mean : array-like, shape (n_features,)\n        Mean vector of the Gaussian distribution.\n    cov : array-like, shape (n_features, n_features)\n        Covariance matrix (must be positive-definite).\n    \n    Returns\n    -------\n    pdf_values : list of float\n        List of PDF values for each row in X, rounded to 4 decimal places.\n       # Convert inputs to numpy arrays\n    X = np.asarray(X, dtype=float)\n    mean = np.asarray(mean, dtype=float)\n    cov = np.asarray(cov, dtype=float)\n\n    # Ensure X is two-dimensional: (n_samples, n_features)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n\n    n_samples, n_features = X.shape\n\n    # Ensure mean is one-dimensional of length n_features\n    mean = mean.flatten()\n    if mean.size != n_features:\n        raise ValueErrorMean vector length must match number of features    # Compute determinant and inverse of covariance matrix\n    det_cov = np.linalg.det(cov)\n    if det_cov <= 0:\n        raise ValueErrorCovariance matrix must be positive-definite    inv_cov = np.linalg.inv(cov)\n\n    # Normalization constant\n    denom = math.sqrt((2 * math.pi) ** n_features * det_cov)\n\n    pdf_list = []\n    for i in range(n_samples):\n        x = X[i]\n        diff = x - mean\n        # exponent term: -0.5 * diff^T * inv_cov * diff\n        exponent = -0.5 * float(diff.T.dot(inv_cov).dot(diff))\n        pdf_val = (1.0 / denom) * math.exp(exponent)\n        # round to 4 decimal places\n        pdf_list.append(round(pdf_val, 4))\n\n    return pdf_list\n```\n\nExplanation of key steps:\n1. We convert all inputs to NumPy arrays and ensure `X` has shape `(n_samples, n_features)`.  \n2. We flatten the mean and check its length matches `n_features`.  \n3. We compute the determinant and inverse of the covariance matrix. A non\u2013positive determinant indicates a non\u2013invertible or non\u2013positive\u2013definite matrix, which we guard against.  \n4. The normalization constant is \\(\\sqrt{(2\\pi)^d \\det(\\Sigma)}\\).  \n5. For each sample, we compute the quadratic form \\((x-\\mu)^\\top \\Sigma^{-1}(x-\\mu)\\) and then the PDF value.  \n6. Finally, each PDF value is rounded to 4 decimal places and returned in a Python list."}
{"task_id": 419, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,           # ignored, since it cancels in the MAP\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    Returns the MAP predictive mean for each row in X_new, rounded to 4 decimals.\n    \"\"\"\n    # 1) Optionally add an intercept column to X and X_new\n    if fit_intercept:\n        ones_train = np.ones((X.shape[0], 1))\n        X = np.hstack((ones_train, X))\n        ones_new = np.ones((X_new.shape[0], 1))\n        X_new = np.hstack((ones_new, X_new))\n\n    # Number of parameters\n    M = X.shape[1]\n\n    # 2) Build prior precision V^{-1}\n    if V is None:\n        # V = I  ==>  V^{-1} = I\n        V_inv = np.eye(M)\n    else:\n        # V is given\n        if np.isscalar(V):\n            # V = v * I  ==>  V^{-1} = (1/v) * I\n            V_inv = np.eye(M) / V\n        else:\n            V_arr = np.array(V)\n            if V_arr.ndim == 1:\n                # diagonal prior variances\n                if V_arr.shape[0] != M:\n                    raise ValueError(f\"Length of V ({V_arr.shape[0]}) != number of features ({M})\")\n                V_inv = np.diag(1.0 / V_arr)\n            elif V_arr.ndim == 2:\n                # full matrix\n                if V_arr.shape != (M, M):\n                    raise ValueError(f\"Shape of V {V_arr.shape} != ({M},{M})\")\n                V_inv = np.linalg.inv(V_arr)\n            else:\n                raise ValueError(\"V must be scalar, 1D array or 2D square matrix\")\n\n    # 2b) Build prior mean vector\n    if np.isscalar(mu):\n        mu_vec = np.full(M, mu, dtype=float)\n    else:\n        mu_arr = np.array(mu, dtype=float)\n        if mu_arr.ndim != 1 or mu_arr.shape[0] != M:\n            raise ValueError(f\"mu must be scalar or length-{M} vector\")\n        mu_vec = mu_arr\n\n    # 3) Compute posterior covariance \u03a3_p and mean \u03bc_p (MAP)\n    # \u03a3_p = (V^{-1} + X^T X)^(-1)\n    A = V_inv + X.T @ X\n    Sigma_p = np.linalg.inv(A)\n\n    # \u03bc_p = \u03a3_p (V^{-1} \u03bc + X^T y)\n    b = V_inv @ mu_vec + X.T @ y\n    mu_p = Sigma_p @ b\n\n    # 4) Predictive mean for X_new\n    y_pred = X_new @ mu_p\n\n    # Round to 4 decimals and return as Python list\n    return [round(float(val), 4) for val in y_pred]\n```\n\nExplanation of key steps:\n1. If `fit_intercept=True`, a column of ones is prepended to both `X` and `X_new`.\n2. The prior precision matrix `V\u207b\u00b9` is constructed from `V`, which may be a scalar, a 1D array (diagonal), or a full matrix. If `V` is `None`, it defaults to the identity.\n3. The posterior covariance and mean are computed via the standard conjugate\u2010Gaussian update:\n   - \u03a3\u209a = (V\u207b\u00b9 + X\u1d40X)\u207b\u00b9  \n   - \u03bc\u209a = \u03a3\u209a( V\u207b\u00b9 \u03bc + X\u1d40y )\n4. The MAP predictive mean for each row of `X_new` is `X_new @ \u03bc\u209a`, rounded to four decimal places."}
{"task_id": 423, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Stub for the RMSLE metric.  Complete the body of the function.\"\"\"\n    # 1. Check lengths\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1\n\n    # 2. Check that all entries are non\u2010negative numbers\n    for a, p in zip(actual, predicted):\n        if not (isinstance(a, (int, float)) and isinstance(p, (int, float))):\n            return -1\n        if a < 0 or p < 0:\n            return -1\n\n    # 3. Compute RMSLE\n    act_arr = np.array(actual, dtype=float)\n    pred_arr = np.array(predicted, dtype=float)\n\n    # log1p = log(1 + x)\n    log_diff = np.log1p(pred_arr) - np.log1p(act_arr)\n    msle = np.mean(log_diff ** 2)\n    rmsle = np.sqrt(msle)\n\n    # 4. Round to 4 decimal places\n    return round(rmsle, 4)\n```\n\nExplanation of steps:\n1. We first ensure both lists have the same non-zero length.\n2. We iterate through both lists simultaneously to confirm every element is an int/float and \u2265 0.\n3. We convert them to NumPy arrays (dtype=float), compute `log1p` of each, take their difference, square it, average over all entries, and finally take the square root.\n4. We round the final RMSLE to 4 decimal places as required."}
{"task_id": 428, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    # Special cases\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n\n    # Decide the length N for building the window:\n    # - symmetric: build exactly window_len points\n    # - periodic: build window_len+1 points, then drop the last one\n    N = window_len if symmetric else window_len + 1\n    denom = N - 1  # will be >= 1 here\n\n    # Compute the Hann window\n    window = []\n    for n in range(N):\n        val = 0.5 - 0.5 * math.cos(2 * math.pi * n / denom)\n        # Round to 4 decimal places\n        window.append(round(val, 4))\n\n    # If periodic, drop the last sample to get 'window_len' points\n    if not symmetric:\n        window = window[:-1]\n\n    # Ensure returning a plain Python list\n    return list(window)\n```\n\nExplanation of key steps:\n\n1. Handle the two special cases:\n   - `window_len \u2264 0`: return an empty list.\n   - `window_len = 1`: return `[1.0]`.\n2. For the main computation:\n   - If `symmetric` is `True`, set `N = window_len`.\n   - If `symmetric` is `False` (periodic), set `N = window_len + 1`.\n   - Compute each sample by the formula  \n     hann(n) = 0.5 \u2013 0.5 * cos(2\u03c0n / (N\u20131)),  \n     for n = 0,1,\u2026,N\u20131.\n   - Round each value to 4 decimal places.\n3. In the periodic case, discard the last sample to produce the final list of length `window_len`.\n4. Return a standard Python list of floats."}
{"task_id": 433, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Your task is to implement this function following the rules described in\n    the problem statement.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    # 4-D convolutional kernel case\n    if len(shape) == 4:\n        out_channels, in_channels, kernel_height, kernel_width = shape\n        receptive_field_size = float(kernel_height * kernel_width)\n        fan_in = float(in_channels) * receptive_field_size\n        fan_out = float(out_channels) * receptive_field_size\n    # all other cases (dense, embedding, higher-D, etc.)\n    else:\n        fan_in = float(shape[0])\n        fan_out = float(shape[1])\n\n    return fan_in, fan_out\n```\n\nExplanation:\n\n1. If the shape has exactly 4 dimensions, we unpack it into `(out_channels, in_channels, kernel_height, kernel_width)`.  \n2. We compute the receptive field size as the product of `kernel_height` and `kernel_width`.  \n3. We then compute  \n   - `fan_in  = in_channels  \u00d7 receptive_field_size`  \n   - `fan_out = out_channels \u00d7 receptive_field_size`  \n4. For any other tensor shape (dense layers, embeddings, etc.), we simply take the first two dimensions as `(fan_in, fan_out)`.  \n5. Finally, we cast everything to `float` and return the tuple `(fan_in, fan_out)`."}
{"task_id": 435, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value is rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    # 1. Convert to numpy arrays\n    X_arr = np.array(X, dtype=float)\n    y_arr = np.array(y, dtype=float)\n\n    # Number of samples\n    m = y_arr.shape[0]\n\n    # Compute target mean\n    y_mean = y_arr.mean() if m > 0 else 0.0\n\n    # Special case: no boosting if n_estimators <= 0 or learning_rate == 0\n    if n_estimators <= 0 or learning_rate == 0.0:\n        return [round(y_mean, 4)] * m\n\n    # 2. Initialize predictions to the mean of y\n    y_pred = np.full(m, y_mean, dtype=float)\n\n    # Pre\u2010build the design matrix for OLS (adds an intercept column)\n    # shape: (m, d+1)\n    ones = np.ones((m, 1), dtype=float)\n    X_design = np.hstack((ones, X_arr))\n\n    # 3. Boosting iterations\n    for _ in range(n_estimators):\n        # 3a. Compute residuals\n        residuals = y_arr - y_pred\n\n        # 3b. Fit OLS to predict residuals:\n        #     beta = (X_design^+)* residuals\n        #     where X_design^+ is the Moore\u2013Penrose pseudoinverse\n        beta = np.linalg.pinv(X_design).dot(residuals)\n\n        # 3c. Weak learner prediction\n        h = X_design.dot(beta)\n\n        # 3d. Update ensemble prediction\n        y_pred += learning_rate * h\n\n    # 4. Round to 4 dec places and return as Python list\n    y_pred_rounded = np.round(y_pred, 4)\n    return y_pred_rounded.tolist()\n```\n\nExplanation of key steps:\n\n1. **Data prep**: Convert inputs to NumPy arrays of type `float`.  \n2. **Special cases**: If no estimators or zero learning rate, return the constant mean of `y`.  \n3. **Initialization**: Set all predictions equal to `y`\u2019s mean.  \n4. **Design matrix**: Prepend a column of ones to `X` so that our OLS fit includes an intercept.  \n5. **Boosting loop**:  \n   - Compute residuals \\(r = y - \\hat y\\).  \n   - Fit an OLS model via the pseudoinverse: \\(\\beta = X^+\\,r\\).  \n   - Predict the residuals on the same `X`.  \n   - Update the model\u2019s prediction by adding `learning_rate * h`.  \n6. **Output**: Round each predicted value to four decimal places and return a plain Python list."}
{"task_id": 437, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    # Check valid order\n    if order not in (0, 1, 2):\n        return -1\n\n    # Detect if the original input is a Python scalar\n    is_scalar = isinstance(x, (int, float))\n\n    # Convert input to a NumPy array for vectorized computations\n    arr = np.array(x, dtype=float)\n\n    # Compute the sigmoid\n    sigma = 1.0 / (1.0 + np.exp(-arr))\n\n    # Choose formula by order\n    if order == 0:\n        result = sigma\n    elif order == 1:\n        result = sigma * (1.0 - sigma)\n    else:  # order == 2\n        result = sigma * (1.0 - sigma) * (1.0 - 2.0 * sigma)\n\n    # Round to 4 decimal places\n    result = np.round(result, 4)\n\n    # Return as Python float for scalar input, else as a list\n    if is_scalar:\n        return float(result)\n    else:\n        return result.tolist()\n```\n\nExplanation of key steps:\n\u2022 We first validate the `order` argument and return `-1` immediately if it\u2019s invalid.  \n\u2022 We detect whether the input `x` is a Python scalar (`int` or `float`) so we know whether to return a float or a list.  \n\u2022 We convert `x` into a NumPy array `arr` to leverage vectorized operations.  \n\u2022 We compute the logistic sigmoid \u03c3(x) and then select the appropriate derivative formula based on `order`.  \n\u2022 We round the results to four decimal places with `np.round(...)`.  \n\u2022 Finally, if the original input was a scalar, we cast the single output back to a Python `float`; otherwise we call `.tolist()` to return a Python list preserving the original shape."}
{"task_id": 438, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    # Convert input to a 1-D integer array\n    y_arr = np.asarray(y, dtype=int).ravel()\n    \n    # Handle empty input\n    if y_arr.size == 0:\n        return []\n    \n    # Check for negative labels\n    if np.any(y_arr < 0):\n        raise ValueError(\"Class indices must be non-negative integers.\")\n    \n    max_label = int(y_arr.max())\n    \n    # Infer num_classes if needed\n    if num_classes is None:\n        num_classes = max_label + 1\n    # If provided num_classes is too small, signal failure\n    elif num_classes < max_label + 1:\n        return -1\n    \n    # Build one-hot matrix\n    one_hot = np.zeros((y_arr.shape[0], num_classes), dtype=int)\n    one_hot[np.arange(y_arr.shape[0]), y_arr] = 1\n    \n    # Convert to plain Python list of lists and return\n    return one_hot.tolist()\n```\n\nExplanation of key steps:\n1. We first coerce `y` into a flat NumPy integer array.\n2. If `y` is empty, we immediately return an empty list.\n3. We check for negatives (since only non-negative labels are allowed).\n4. We compute `max_label` and infer or validate `num_classes`.\n5. If the provided `num_classes` is too small, we return `-1`.\n6. Otherwise, we allocate a zero matrix of shape `(len(y), num_classes)` and place 1\u2019s in the appropriate columns.\n7. Finally, we convert the NumPy array to a Python list of lists and return it."}
{"task_id": 439, "completion_id": 0, "completion": "```python\nimport itertools\n\ndef apriori_frequent_itemsets(transactions: list[list[int]], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        List of transactions; each transaction is itself a list of items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction (>0 and \u22641).\n\n    Returns\n    -------\n    list[tuple]\n        Frequent itemsets ordered by length and then lexicographically.\n    \"\"\"\n    # Number of transactions\n    n_trans = len(transactions)\n    if n_trans == 0:\n        return []\n\n    # Convert each transaction to a set for faster subset checks\n    tsets = [set(t) for t in transactions]\n\n    # 1) Generate L1, the frequent 1-itemsets\n    # Count each item\n    item_counts = {}\n    for t in tsets:\n        for item in t:\n            item_counts[item] = item_counts.get(item, 0) + 1\n\n    # Keep those meeting min_sup\n    L1 = []\n    for item, cnt in item_counts.items():\n        if cnt / n_trans >= min_sup:\n            L1.append((item,))\n    # Sort lexicographically\n    L1 = sorted(L1)\n\n    # This will hold all frequent itemsets\n    frequent_itemsets = L1.copy()\n\n    # L_prev holds the frequent (k-1)-itemsets in the previous iteration\n    L_prev = L1\n    k = 2\n\n    # Loop until no more candidates\n    while L_prev:\n        # 2) Candidate generation by self-join of L_prev\n        Ck = []\n        len_prev = len(L_prev)\n        for i in range(len_prev):\n            for j in range(i + 1, len_prev):\n                a = L_prev[i]\n                b = L_prev[j]\n                # join only if first k-2 items are equal\n                if a[:k-2] == b[:k-2]:\n                    # union and sort to get a candidate k-itemset\n                    cand = tuple(sorted(set(a) | set(b)))\n                    if len(cand) == k:\n                        Ck.append(cand)\n\n        # deduplicate\n        Ck = sorted(set(Ck))\n\n        # 3) Prune candidates that have an infrequent (k-1)-subset\n        L_prev_set = set(L_prev)\n        pruned = []\n        for cand in Ck:\n            # all (k-1)-subsets\n            all_subsets_frequent = True\n            for subset in itertools.combinations(cand, k-1):\n                if subset not in L_prev_set:\n                    all_subsets_frequent = False\n                    break\n            if all_subsets_frequent:\n                pruned.append(cand)\n\n        Ck = pruned\n\n        # 4) Count supports of candidates Ck\n        counts = {cand: 0 for cand in Ck}\n        for t in tsets:\n            for cand in Ck:\n                # if cand is subset of transaction\n                if set(cand) <= t:\n                    counts[cand] += 1\n\n        # 5) Form Lk by thresholding\n        Lk = []\n        for cand, cnt in counts.items():\n            if cnt / n_trans >= min_sup:\n                Lk.append(cand)\n\n        if not Lk:\n            break\n\n        # sort lexicographically\n        Lk = sorted(Lk)\n\n        # collect them and iterate\n        frequent_itemsets.extend(Lk)\n        L_prev = Lk\n        k += 1\n\n    # The collected frequent_itemsets are already in order of increasing size\n    # and within each size block they are lex-sorted.\n    return frequent_itemsets\n```\n\nExplanation of key steps:\n\n1. **L1 generation**: Count each singleton item\u2019s support and keep those \u2265 `min_sup`.\n2. **Candidate generation (self-join)**: From the frequent (k\u22121)-itemsets `L_prev`, join pairs that share the first k\u22122 items to form k-item candidates.\n3. **Pruning**: Discard any k-item candidate that has at least one (k\u22121)-subset not in `L_prev`.\n4. **Support counting**: Scan all transactions and count how many contain each candidate.\n5. **Thresholding**: Keep only those with support \u2265 `min_sup`.\n\nWe repeat until no new frequent itemsets of size k are found. The final list is naturally ordered by itemset size, and within each size block we keep lexicographic order."}
{"task_id": 440, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Tuple\n\ndef aggregate_predictions(\n    predictions: List[List[List[float]]]\n) -> Tuple[List[List[float]], List[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    # Convert to numpy array of shape (n_estimators, n_samples, n_classes)\n    arr = np.array(predictions, dtype=float)\n    \n    # Check dimensions\n    if arr.ndim != 3:\n        raise ValueError(f\"Expected a 3D list, got array with shape {arr.shape}\")\n    \n    n_estimators, n_samples, n_classes = arr.shape\n    \n    # Average over the first axis (estimators)\n    avg_probs = arr.mean(axis=0)  # shape: (n_samples, n_classes)\n    \n    # Round to 4 decimal places\n    avg_probs_rounded = np.round(avg_probs, 4)\n    \n    # Determine predicted class per sample\n    # In case of ties, numpy.argmax returns the first (smallest) index\n    preds = np.argmax(avg_probs_rounded, axis=1)\n    \n    # Convert back to pure Python lists\n    avg_probs_list = avg_probs_rounded.tolist()\n    preds_list = preds.tolist()\n    \n    return avg_probs_list, preds_list\n```\n\nExplanation of key steps:\n1. **Conversion to NumPy**: We turn the nested lists into a NumPy array for easy manipulation.\n2. **Averaging**: We average over the estimator dimension (`axis=0`).\n3. **Rounding**: We round each averaged probability to four decimal places.\n4. **Predictions**: We use `np.argmax` along the class dimension (`axis=1`) to get the class index with the highest probability for each sample. In the event of ties, `argmax` picks the smallest index.\n5. **Return types**: We convert the results back to native Python lists as required."}
{"task_id": 444, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function follows the specifications:\n    - X: N\u00d7C list of vectors\n    - Y: M\u00d7C list of vectors (if None, Y = X)\n    - sigma: float (isotropic), list of C floats (anisotropic), or None\n             (defaults to sqrt(C/2))\n    Returns:\n    - Nested Python list of shape N\u00d7M of kernel values rounded to 4 decimals\n    - Or -1 on invalid input\n    \"\"\"\n    # 1) Validate X\n    if not isinstance(X, list) or len(X) == 0:\n        return -1\n    N = len(X)\n    # All rows must be lists of same length C\n    try:\n        C = len(X[0])\n    except Exception:\n        return -1\n    if C == 0:\n        return -1\n    for row in X:\n        if not isinstance(row, (list, tuple)) or len(row) != C:\n            return -1\n\n    # 2) If Y is None, set Y = X\n    if Y is None:\n        Y = X\n    # Validate Y\n    if not isinstance(Y, list) or len(Y) == 0:\n        return -1\n    M = len(Y)\n    for row in Y:\n        if not isinstance(row, (list, tuple)) or len(row) != C:\n            return -1\n\n    # 3) Process sigma\n    # Default sigma if None\n    if sigma is None:\n        sigma_arr = np.full((C,), np.sqrt(C / 2.0), dtype=float)\n    # Isotropic: single float or int\n    elif isinstance(sigma, (int, float)):\n        if sigma <= 0:\n            return -1\n        sigma_arr = np.full((C,), float(sigma), dtype=float)\n    # Anisotropic: list of length C\n    elif isinstance(sigma, (list, tuple, np.ndarray)):\n        if len(sigma) != C:\n            return -1\n        try:\n            sigma_arr = np.array(sigma, dtype=float)\n        except Exception:\n            return -1\n        if np.any(sigma_arr <= 0):\n            return -1\n    else:\n        # invalid sigma type\n        return -1\n\n    # 4) Convert X, Y to numpy arrays\n    try:\n        X_np = np.array(X, dtype=float)  # shape (N, C)\n        Y_np = np.array(Y, dtype=float)  # shape (M, C)\n    except Exception:\n        return -1\n\n    # 5) Scale features by sigma\n    # broadcasting sigma_arr over rows\n    Xs = X_np / sigma_arr\n    Ys = Y_np / sigma_arr\n\n    # 6) Compute squared distances efficiently\n    # ||x - y||^2 = ||x||^2 + ||y||^2 - 2 x\u00b7y\n    x_norm2 = np.sum(Xs * Xs, axis=1)        # shape (N,)\n    y_norm2 = np.sum(Ys * Ys, axis=1)        # shape (M,)\n    cross_term = Xs.dot(Ys.T)                # shape (N, M)\n    d2 = x_norm2[:, None] + y_norm2[None, :] - 2.0 * cross_term\n    # Numerical errors might make tiny negatives; clip to zero\n    d2 = np.maximum(d2, 0.0)\n\n    # 7) Compute the RBF kernel: exp(-0.5 * d2)\n    K = np.exp(-0.5 * d2)\n\n    # 8) Round to 4 decimal places\n    K_round = np.round(K, 4)\n\n    # 9) Convert to nested Python list and return\n    return K_round.tolist()\n```\n\nExplanation of key steps:\n\n1. **Input validation**  \n   - Ensure `X` and `Y` are non-empty lists of equal-length feature vectors.  \n   - If `Y` is `None`, we simply set `Y = X` to compute the square matrix.\n\n2. **Sigma handling**  \n   - When `sigma` is `None`, we default to the conventional \u221a(C/2).  \n   - A single positive float \u21d2 isotropic kernel (same bandwidth on all features).  \n   - A list of length C with all positive entries \u21d2 anisotropic (per-feature bandwidth).  \n   - In any other invalid case (non-positive \u03c3, wrong list\u2010length, bad type), we return `-1`.\n\n3. **Vectorized computation**  \n   - We scale the data by `\u03c3` (`Xs = X / \u03c3`, etc.).  \n   - We compute pairwise squared distances using the expansion  \n     \u2016x\u2212y\u2016\u00b2 = \u2016x\u2016\u00b2 + \u2016y\u2016\u00b2 \u2212 2x\u22c5y, clipped to \u22650.  \n   - Finally, we apply `exp(\u22120.5 * d\u00b2)`.\n\n4. **Rounding and output**  \n   - We round each kernel entry to 4 decimal places via `np.round`.  \n   - Convert the NumPy array back to a nested Python list.  \n\nThis fulfills all requirements: input checks, correct RBF definition, default \u03c3, and 4-decimal rounding."}
{"task_id": 446, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\n\n    A *decision stump* is a one-level decision tree: it chooses one feature and\n    one threshold to split the dataset into two parts. This function must find\n    the split that maximises the decrease of entropy (information gain) and\n    return a summarising tuple. Read the detailed task description for exact\n    requirements, tie-breaking rules and the expected return value.\n\n    Args:\n        X: 2-D list (or array-like) of shape (n_samples, n_features) containing\n           only numeric values.\n        y: 1-D list (or array-like) with the class label of every sample.\n\n    Returns:\n        \u2022 (best_feature_index, best_threshold, left_majority_label,\n           right_majority_label)\n        \u2022 If no split can improve information gain, returns\n          (-1, None, majority_label, majority_label).\n    \"\"\"\n    # Convert to numpy arrays for convenience\n    X_arr = np.array(X, dtype=float)\n    y_arr = np.array(y, dtype=int)\n    n_samples, n_features = X_arr.shape\n\n    # Helper: compute entropy of a 1-D array of labels\n    def entropy(labels: np.ndarray) -> float:\n        if labels.size == 0:\n            return 0.0\n        vals, counts = np.unique(labels, return_counts=True)\n        probs = counts / counts.sum()\n        return -np.sum(probs * np.log2(probs))\n\n    # Helper: find the majority label (ties broken by smallest label)\n    def majority_label(labels: np.ndarray) -> int:\n        vals, counts = np.unique(labels, return_counts=True)\n        max_count = counts.max()\n        # select all labels with the max count, then pick the smallest\n        candidates = vals[counts == max_count]\n        return int(candidates.min())\n\n    # Compute parent entropy\n    H_parent = entropy(y_arr)\n\n    best_gain = 0.0\n    best_feature = -1\n    best_threshold = None\n\n    # Iterate over each feature\n    for j in range(n_features):\n        col = X_arr[:, j]\n        # get sorted unique values\n        uniq_vals = np.unique(col)\n        if uniq_vals.size < 2:\n            continue  # no possible split on this feature\n\n        # possible thresholds are midpoints between successive unique values\n        thresholds = (uniq_vals[:-1] + uniq_vals[1:]) / 2.0\n\n        # evaluate each threshold\n        for t in thresholds:\n            left_mask = (col <= t)\n            right_mask = ~left_mask\n            n_left = left_mask.sum()\n            n_right = right_mask.sum()\n            # skip if one side is empty\n            if n_left == 0 or n_right == 0:\n                continue\n\n            y_left = y_arr[left_mask]\n            y_right = y_arr[right_mask]\n\n            H_left = entropy(y_left)\n            H_right = entropy(y_right)\n\n            p_left = n_left / n_samples\n            p_right = n_right / n_samples\n\n            gain = H_parent - (p_left * H_left + p_right * H_right)\n\n            # check for a strictly better gain\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = j\n                best_threshold = t\n            # tie-breaking: same gain? pick smaller feature index first,\n            # then smaller threshold\n            elif np.isclose(gain, best_gain) and gain > 0:\n                if j < best_feature:\n                    best_feature = j\n                    best_threshold = t\n                elif j == best_feature and t < best_threshold:\n                    best_threshold = t\n\n    # If no positive gain found, return the default stump\n    if best_feature == -1:\n        maj = majority_label(y_arr)\n        return (-1, None, maj, maj)\n\n    # Otherwise, compute the majority labels on each side\n    t_rounded = round(best_threshold, 4)\n    col_best = X_arr[:, best_feature]\n    left_mask = (col_best <= best_threshold)\n    right_mask = ~left_mask\n\n    left_maj = majority_label(y_arr[left_mask])\n    right_maj = majority_label(y_arr[right_mask])\n\n    return (best_feature, t_rounded, left_maj, right_maj)\n```\n\nExplanation of key steps:\n1. **Entropy calculation**: `entropy(labels)` computes Shannon entropy of the given label array.\n2. **Majority label**: `majority_label(labels)` returns the most frequent label, breaking ties by choosing the smallest label.\n3. **Threshold candidates**: For each feature, we sort its unique values and form midpoints between successive distinct values.\n4. **Information gain**: For each threshold, we split the data, compute the child entropies and weighted sum, and subtract from parent entropy.\n5. **Tie-breaking**: We update our best split only if we find a strictly higher gain, or if the gain is equal (within floating\u2010point tolerance) and the candidate feature index or threshold is smaller according to the rules.\n6. **No\u2010split case**: If no split yields positive gain, we return `(-1, None, maj, maj)` where `maj` is the overall majority label."}
{"task_id": 452, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Union\n\ndef divide_on_feature(\n    X: Union[np.ndarray, List[list]], feature_i: int, threshold\n) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\n\n    The function must create two disjoint subsets:\n    1. For a numeric threshold (int or float) the first subset contains every\n       sample whose value in column *feature_i* is **greater than or equal** to\n       the threshold.\n    2. For any other type of threshold the first subset contains every sample\n       whose value in column *feature_i* is **exactly equal** to the threshold.\n\n    The second subset always contains the remaining samples.  Both subsets must\n    keep the original order of appearance in *X*.\n\n    Args:\n        X: 2-D iterable (list of lists or ndarray) where each element is a sample.\n        feature_i: Index of the feature column used for the split.\n        threshold: Value that determines how the split is performed.\n\n    Returns:\n        A list [subset1, subset2], each a regular Python list of samples.\n    \"\"\"\n    # If X is an ndarray, convert it to a list of lists:\n    if isinstance(X, np.ndarray):\n        samples = X.tolist()\n    else:\n        # assume it's already a list of lists\n        samples = X\n\n    # Decide if the threshold is numeric\n    is_numeric = isinstance(threshold, (int, float))\n\n    subset1 = []\n    subset2 = []\n\n    for sample in samples:\n        val = sample[feature_i]\n        if is_numeric:\n            # for numeric thresholds: >=\n            if val >= threshold:\n                subset1.append(sample)\n            else:\n                subset2.append(sample)\n        else:\n            # for categorical thresholds: ==\n            if val == threshold:\n                subset1.append(sample)\n            else:\n                subset2.append(sample)\n\n    return [subset1, subset2]\n```\n\nExplanation:\n\n1. We first normalize `X` to a plain Python list of samples via `tolist()` if it's an ndarray.\n2. We check whether `threshold` is numeric (an `int` or `float`).\n3. We iterate over each sample, pick out the feature value `val = sample[feature_i]`, and decide\n   - For numeric thresholds, we test `val >= threshold`.\n   - For non-numeric (categorical) thresholds, we test `val == threshold`.\n4. We append the sample to the first subset if it meets the criterion, otherwise to the second subset.\n5. Finally, we return a two-element list containing the two subsets (each already a Python list)."}
{"task_id": 453, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # Validate gamma\n    if not isinstance(gamma, (float, int)) or gamma < 0 or gamma > 1:\n        return -1\n\n    # Validate Q and Q_next dimensions\n    if not isinstance(Q, np.ndarray) or not isinstance(Q_next, np.ndarray):\n        return -1\n    if Q.shape != Q_next.shape:\n        return -1\n\n    batch_size, n_actions = Q.shape\n\n    # Validate lengths of lists\n    if len(actions) != batch_size or len(rewards) != batch_size or len(dones) != batch_size:\n        return -1\n\n    # Copy Q to targets\n    y = Q.copy().astype(float)\n\n    for i in range(batch_size):\n        a = actions[i]\n        # Validate action indices\n        if not (0 <= a < n_actions):\n            return -1\n\n        r = rewards[i]\n        done = dones[i]\n\n        if done:\n            target = r\n        else:\n            target = r + gamma * np.max(Q_next[i])\n\n        y[i, a] = target\n\n    # Round to 4 decimals and convert to nested list\n    y = np.round(y, 4)\n    return y.tolist()\n```\n\nExplanation of key steps:\n\n1. **Validation**  \n   - Check that `gamma` is a number in \\[0,1\\].  \n   - Ensure `Q` and `Q_next` are NumPy arrays of the same shape `(batch, n_actions)`.  \n   - Verify that `actions`, `rewards`, and `dones` all have length equal to the batch size.  \n   - Confirm each action index is in the valid range `0 <= action < n_actions`.\n\n2. **Target Computation**  \n   - Copy the original `Q` into `y`.  \n   - For each sample `i`, compute  \n     - `target = rewards[i]` if `dones[i]` is True,  \n     - otherwise `target = rewards[i] + gamma * max(Q_next[i])`.  \n   - Replace `y[i, actions[i]]` with this `target`.\n\n3. **Rounding & Output**  \n   - Round the entire `y` array to 4 decimal places.  \n   - Convert to a nested Python list and return.  \n   - If any validation fails, return `-1`."}
{"task_id": 458, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n   Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n       # Step 1: pull out Python ints and validate\n    try:\n        labels_iter = list(labels)\n    except Exception:\n        return -1\n\n    labels_int = []\n    for x in labels_iter:\n        # try to convert to an integer\n        try:\n            xi = int(x)\n        except Exception:\n            return -1\n        # ensure it was actually integral\n        if x != xi:\n            return -1\n        if xi < 0:\n            return -1\n        labels_int.append(xi)\n\n    n = len(labels_int)\n    # Step 2: infer num_classes if needed\n    if num_classes is None:\n        if n == 0:\n            num_classes = 0\n        else:\n            num_classes = max(labels_int) + 1\n    else:\n        # if user gave a non-int num_classes or negative, reject\n        try:\n            nc = int(num_classes)\n        except Exception:\n            return -1\n        if nc != num_classes or nc < 0:\n            return -1\n        num_classes = nc\n\n    # Step 3: check that num_classes is sufficient\n    if n > 0 and num_classes <= max(labels_int):\n        return -1\n\n    # Step 4: build one-hot matrix\n    # If there are no labels, just return an empty list\n    if n == 0:\n        return []\n\n    one_hot = []\n    for lbl in labels_int:\n        row = [0] * num_classes\n        row[lbl] = 1\n        one_hot.append(row)\n\n    return one_hot\n```\n\nExplanation of key steps:\n\n1. We first attempt to make a list out of `labels`. If that fails, we immediately return -1.\n2. We iterate through each element, converting it to a Python `int`, checking that there\u2019s no loss of information (to reject floats like `1.2`), and verifying it\u2019s non\u2010negative.\n3. If `num_classes` is not provided, we infer it as `max(labels) + 1` (or `0` if the label list is empty). If it is provided, we check that it\u2019s a non\u2010negative integer.\n4. We ensure `num_classes` is at least `max(labels) + 1`. Otherwise we return -1.\n5. Finally, we construct the list of one-hot rows and return it (or an empty list if there are no labels).\n"}
{"task_id": 461, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\n\n    Parameters\n    ----------\n    x : list[float]\n        Feature values (one-dimensional).\n    y : list[float]\n        Target values.\n    n_estimators : int, default 200\n        Number of boosting iterations.\n    learning_rate : float, default 0.5\n        Shrinkage applied to each weak learner.\n\n    Returns\n    -------\n    list[float]\n        Final predictions for the training data, rounded to four decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    x_arr = np.array(x, dtype=float)\n    y_arr = np.array(y, dtype=float)\n    n_samples = y_arr.shape[0]\n\n    # Special case: no boosting rounds\n    if n_estimators <= 0:\n        init_pred = np.full(n_samples, y_arr.mean())\n        return np.round(init_pred, 4).tolist()\n\n    # Initialize prediction to the mean of y\n    y_pred = np.full(n_samples, y_arr.mean())\n\n    # Unique sorted x-values for potential splits\n    unique_vals = np.unique(x_arr)\n\n    for _ in range(n_estimators):\n        # Compute residuals\n        residuals = y_arr - y_pred\n\n        # Stop early if residuals are all exactly zero\n        if np.all(residuals == 0):\n            break\n\n        best_sse = None\n        best_t = None\n        best_vL = None\n        best_vR = None\n\n        # Try every split threshold (all unique x except the largest)\n        for t in unique_vals[:-1]:\n            left_mask = x_arr <= t\n            right_mask = ~left_mask\n\n            # Skip invalid splits\n            if not np.any(left_mask) or not np.any(right_mask):\n                continue\n\n            r_left = residuals[left_mask]\n            r_right = residuals[right_mask]\n\n            vL = r_left.mean()\n            vR = r_right.mean()\n\n            # Sum of squared errors for this stump\n            sse_left = ((r_left - vL) ** 2).sum()\n            sse_right = ((r_right - vR) ** 2).sum()\n            sse = sse_left + sse_right\n\n            # Keep the best split (first in case of ties)\n            if best_sse is None or sse < best_sse:\n                best_sse = sse\n                best_t = t\n                best_vL = vL\n                best_vR = vR\n\n        # Build the stump's prediction of the residuals\n        if best_sse is None:\n            # No valid split found => constant stump\n            const_pred = residuals.mean()\n            stump_pred = np.full(n_samples, const_pred)\n        else:\n            stump_pred = np.where(x_arr <= best_t, best_vL, best_vR)\n\n        # Update the ensemble prediction\n        y_pred += learning_rate * stump_pred\n\n    # Round final predictions to 4 decimals and return as list\n    return np.round(y_pred, 4).tolist()\n```"}
{"task_id": 471, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One step Expected\u2013SARSA TD(0) update for a tabular Q function.\n\n    Parameters\n    ----------\n    q_table : list[list[float]]\n        Current Q-table; q_table[s][a] is Q(s,a).\n    state : int\n        Index of the state *s* where the action was taken.\n    action : int\n        Index of the action *a* taken in state *s*.\n    reward : float\n        Immediate reward received after executing the action.\n    next_state : int | None\n        The successor state *s'*.  Use ``None`` if the transition ended the\n        episode.\n    epsilon : float\n        \u03b5 in the \u03b5-soft policy used to compute the expectation.\n    lr : float\n        Learning-rate \u03b7.\n    gamma : float\n        Discount factor \u03b3.\n\n    Returns\n    -------\n    list[list[float]]\n        The updated Q-table (all entries rounded to four decimals).\n    \"\"\"\n    # make a copy so we don't modify in-place\n    new_q = [row.copy() for row in q_table]\n    old_value = new_q[state][action]\n\n    # compute expected Q for the next state under the \u03b5-soft policy\n    if next_state is None:\n        expected_q = 0.0\n    else:\n        row = new_q[next_state]\n        n_actions = len(row)\n        # find the greedy action (first max if tie)\n        greedy_idx = int(np.argmax(row))\n        expected_q = 0.0\n        for a_idx, qval in enumerate(row):\n            if a_idx == greedy_idx:\n                prob = (1 - epsilon) + epsilon / n_actions\n            else:\n                prob = epsilon / n_actions\n            expected_q += prob * qval\n\n    # compute TD target\n    target = reward + gamma * expected_q\n\n    # update\n    new_q[state][action] = old_value + lr * (target - old_value)\n\n    # round all entries to 4 decimals and return\n    rounded_q = [\n        [round(val, 4) for val in row]\n        for row in new_q\n    ]\n    return rounded_q\n```\n\nExplanation of key steps:\n\n1. We copy `q_table` into `new_q` so the original isn't mutated.\n2. If `next_state` is `None`, the expected future value is 0.\n3. Otherwise, we identify the greedy action in `next_state` (ties go to the first max).\n4. We compute the probability of each action under the \u03b5-soft policy:\n   - greedy action: `1 - \u03b5 + \u03b5/|A|`\n   - others: `\u03b5/|A|`\n5. We form the expected Q by summing `\u03c0(a|s') * Q[s',a]`.\n6. We form the TD target `r + \u03b3 * expected_q` and update `Q[state][action]`.\n7. Finally, we round every Q\u2010value to 4 decimal places and return."}
{"task_id": 474, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None\n                  ) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n   Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes (>= 2).\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                   of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where\n            X_train: shape (m * n_train, 2)\n            X_val:   shape (m * n_val,   2)\n            Y_train: shape (m * n_train,)\n            Y_val:   shape (m * n_val,)\n        All coordinates are rounded to 4 decimal places.\n       if seed is not None:\n        random.seed(seed)\n\n    X_train_list = []\n    Y_train_list = []\n    X_val_list = []\n    Y_val_list = []\n\n    for i in range(m):\n        # Compute the x-range for class i\n        col_index = i // 2\n        x_min = (col_index + 0.1) * 10.0\n        x_max = (col_index + 0.9) * 10.0\n\n        # Compute the y-range for class i\n        row_index = i % 2\n        y_min = (row_index * interval + 0.1) * 10.0\n        y_max = (row_index * interval + 0.9) * 10.0\n\n        # Label: (i - 0.5) * 2  \u2192  -1 for i=0, +1 for i=1, -1 for i=2, etc.\n        label = (i - 0.5) * 2\n\n        # Generate training samples\n        for _ in range(n_train):\n            x = random.uniform(x_min, x_max)\n            y = random.uniform(y_min, y_max)\n            X_train_list.append([x, y])\n            Y_train_list.append(label)\n\n        # Generate validation samples\n        for _ in range(n_val):\n            x = random.uniform(x_min, x_max)\n            y = random.uniform(y_min, y_max)\n            X_val_list.append([x, y])\n            Y_val_list.append(label)\n\n    # Convert to NumPy arrays\n    X_train = np.array(X_train_list, dtype=float)\n    X_val   = np.array(X_val_list,   dtype=float)\n    Y_train = np.array(Y_train_list, dtype=float)\n    Y_val   = np.array(Y_val_list,   dtype=float)\n\n    # Round coordinates to 4 decimal places\n    X_train = np.round(X_train, 4)\n    X_val   = np.round(X_val,   4)\n\n    return X_train, X_val, Y_train, Y_val\n```\n\nExplanation of key steps:\n\u2022 We seed Python\u2019s built-in `random` module if `seed` is provided so that calls to `random.uniform` are reproducible.  \n\u2022 For each class index `i`, we compute its column in the grid as `i//2` and its row (0 or 1) as `i%2`.  \n\u2022 The x-range and y-range are computed via the formulas  \n    x \u223c U([ (\u230ai/2\u230b + 0.1)\u00b710, (\u230ai/2\u230b + 0.9)\u00b710 ])  \n    y \u223c U([ ((i mod 2)*interval + 0.1)\u00b710, ((i mod 2)*interval + 0.9)\u00b710 ])  \n\u2022 The label is `(i - 0.5) * 2`, which yields \u20131 for even `i` and +1 for odd `i`.  \n\u2022 We collect `n_train` and `n_val` points per class, convert to NumPy arrays, and round all coordinates to four decimal places."}
{"task_id": 475, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t,\n                learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n   Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current parameter value(s).\n    grad : float | np.ndarray\n        Gradient w.r.t. w.\n    m_prev : float | np.ndarray | None\n        Previous first moment estimate. If None, treated as zero.\n    v_prev : float | np.ndarray | None\n        Previous second moment estimate. If None, treated as zero.\n    t : int\n        Time step, must be >= 1.\n    learning_rate : float\n        Step size \u03b1.\n    b1 : float\n        Exponential decay rate for first moment.\n    b2 : float\n        Exponential decay rate for second moment.\n    eps : float\n        Small epsilon for numerical stability.\n\n    Returns\n    -------\n    w_new : float | np.ndarray\n        Updated parameters.\n    m_t : float | np.ndarray\n        Updated first moment.\n    v_t : float | np.ndarray\n        Updated second moment.\n       # Convert inputs to numpy arrays for unified operations\n    w_arr = np.asarray(w)\n    g_arr = np.asarray(grad)\n\n    # Initialize moments if this is the first step\n    if m_prev is None:\n        m_prev_arr = np.zeros_like(g_arr)\n    else:\n        m_prev_arr = np.asarray(m_prev)\n\n    if v_prev is None:\n        v_prev_arr = np.zeros_like(g_arr)\n    else:\n        v_prev_arr = np.asarray(v_prev)\n\n    # Update biased first and second moment estimates\n    m_t = b1 * m_prev_arr + (1.0 - b1) * g_arr\n    v_t = b2 * v_prev_arr + (1.0 - b2) * (g_arr * g_arr)\n\n    # Compute bias-corrected estimates\n    m_hat = m_t / (1.0 - b1**t)\n    v_hat = v_t / (1.0 - b2**t)\n\n    # Update parameters\n    w_new = w_arr - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n\n    return w_new, m_t, v_t\n```\n\nExplanation of the steps:\n\n1. **Initialization**  \n   If `m_prev` or `v_prev` is `None`, we create zero-arrays of the same shape as `grad`.\n\n2. **First and second moment updates**  \n   m_t = \u03b2\u2081\u00b7m_{t\u22121} + (1\u2212\u03b2\u2081)\u00b7grad  \n   v_t = \u03b2\u2082\u00b7v_{t\u22121} + (1\u2212\u03b2\u2082)\u00b7grad\u00b2  \n\n3. **Bias correction**  \n   m\u0302_t = m_t / (1\u2212\u03b2\u2081\u1d57)  \n   v\u0302_t = v_t / (1\u2212\u03b2\u2082\u1d57)  \n\n4. **Parameter update**  \n   w_new = w \u2212 \u03b1\u00b7m\u0302_t / (\u221av\u0302_t + \u03b5)\n\nThis works for both scalars and arbitrary-shaped NumPy arrays."}
{"task_id": 479, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n   Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where ``rewards[t][i]`` is the reward for arm *i* at time-step\n        *t*.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n       # If there are no time-steps, nothing to do\n    if not rewards:\n        return []\n    \n    # Optionally seed numpy's RNG for reproducibility\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Number of arms\n    N = len(rewards[0])\n    \n    # Initialize value estimates and pull counts\n    V = [ev_prior] * N\n    counts = [0] * N\n    \n    # Simulate over each time-step\n    for t_step in rewards:\n        # Decide whether to explore or exploit\n        if np.random.random() < epsilon:\n            # Explore: pick random arm\n            arm = np.random.randint(0, N)\n        else:\n            # Exploit: pick arm with highest estimated value\n            # np.argmax breaks ties by smallest index\n            arm = int(np.argmax(V))\n        \n        # Observe reward for chosen arm at this time-step\n        r = t_step[arm]\n        \n        # Update pull count\n        counts[arm] += 1\n        \n        # Incremental update of sample average\n        V[arm] += (r - V[arm]) / counts[arm]\n    \n    # Round final value estimates to 4 decimal places\n    return [round(v, 4) for v in V]\n```\n\nExplanation of key steps:\n1. We seed NumPy's RNG if `seed` is provided.\n2. We initialize each arm's estimate to `ev_prior` and track how many times each arm is pulled.\n3. At each time-step, with probability `epsilon` we choose an arm uniformly at random; otherwise we pick the arm with the highest current estimate (`np.argmax` breaks ties by the smallest index).\n4. After observing the reward, we update the chosen arm's estimate using the incremental sample\u2010mean formula.\n5. Finally, we return the list of value estimates rounded to four decimal places."}
{"task_id": 481, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n   Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n       # Number of samples\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n\n    # Initialize all labels to -1 (noise)\n    labels = [-1] * n_samples\n    # Track which points have been visited\n    visited = [False] * n_samples\n    # The next cluster ID to assign\n    cluster_id = 0\n\n    def region_query(idx: int) -> list[int]:\n       Return indices of all points within eps of data[idx].        # Compute Euclidean distances from data[idx] to all points\n        diffs = data - data[idx]             # shape (n_samples, n_features)\n        dists = np.linalg.norm(diffs, axis=1)\n        # Neighbors are those with distance <= eps\n        neighbors = np.where(dists <= eps)[0]\n        return neighbors.tolist()\n\n    # Main loop: for each point, if unvisited, try to grow a cluster\n    for point_idx in range(n_samples):\n        if visited[point_idx]:\n            continue\n\n        visited[point_idx] = True\n        # Find all neighbors of point_idx\n        neighbors = region_query(point_idx)\n\n        # If not enough points to form a core point, mark as noise (-1)\n        if len(neighbors) < min_samples:\n            labels[point_idx] = -1\n        else:\n            # Start a new cluster\n            labels[point_idx] = cluster_id\n            # Seeds is the list of points we will try to expand\n            seeds = neighbors.copy()\n            i = 0\n            while i < len(seeds):\n                current = seeds[i]\n                if not visited[current]:\n                    visited[current] = True\n                    current_neighbors = region_query(current)\n                    # If current is also a core point, add its neighbors\n                    if len(current_neighbors) >= min_samples:\n                        for nbr in current_neighbors:\n                            if nbr not in seeds:\n                                seeds.append(nbr)\n                # If current is not yet assigned to any cluster (i.e., noise)\n                if labels[current] == -1:\n                    labels[current] = cluster_id\n                i += 1\n\n            # Finished expanding this cluster\n            cluster_id += 1\n\n    return labels\n```\n\nExplanation of the key steps:\n\n1. **Initialization**  \n   - `labels` is prefilled with `-1` for all points (marking them as noise initially).  \n   - `visited` keeps track of points we've already processed.  \n   - `cluster_id` will be assigned to new clusters as we discover them (starting at 0).\n\n2. **region_query**  \n   - Given an index `idx`, compute Euclidean distances from `data[idx]` to all other points in `data`.  \n   - Return the list of indices whose distance is \u2264 `eps`.\n\n3. **Main Loop**  \n   - For each point (in index order), if it's not yet visited, we mark it visited and retrieve its neighbors.  \n   - If there are fewer than `min_samples` neighbors, we leave it labeled as noise (`-1`).  \n   - Otherwise, we start a new cluster (using the current `cluster_id`), label the point, and attempt to expand the cluster:  \n     - Maintain a growing list `seeds` of points to examine.  \n     - For each point in `seeds`, if it hasn't been visited, mark it visited and get its neighbors;  \n       if it is also a core point, append its neighbors to `seeds`.  \n     - If a point was noise (`-1`), we re-label it to the current `cluster_id`.  \n   - Once expansion finishes, increment `cluster_id`.\n\n4. **Return**  \n   - The final `labels` list, where noise points remain `-1` and clusters are labeled `0, 1, 2, \u2026` in discovery order.\n"}
{"task_id": 482, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n\n    Raises:\n        ValueError: If any label is negative, not an integer, or\n                    not less than num_classes (when num_classes is provided).\n    \"\"\"\n    # Convert input to numpy array\n    y_arr = np.array(y, copy=False)\n\n    # Check that it's 1-D\n    if y_arr.ndim != 1:\n        raise ValueError(f\"y must be a 1-D array or sequence, got array with shape {y_arr.shape}\")\n\n    # If empty input\n    n = y_arr.shape[0]\n    if n == 0:\n        # Determine num_classes for empty input\n        if num_classes is None:\n            num_classes = 0\n        return np.zeros((0, num_classes), dtype=int)\n\n    # Check integer type\n    if not np.issubdtype(y_arr.dtype, np.integer):\n        # If it's float but all values are integers, we can cast\n        if np.issubdtype(y_arr.dtype, np.floating) and np.all(np.floor(y_arr) == y_arr):\n            y_arr = y_arr.astype(int)\n        else:\n            raise ValueError(\"y must contain integer values only\")\n\n    # Check non-negative\n    if np.any(y_arr < 0):\n        raise ValueError(\"All labels must be non-negative\")\n\n    # Infer num_classes if needed\n    if num_classes is None:\n        num_classes = int(y_arr.max()) + 1\n    else:\n        if not isinstance(num_classes, int) or num_classes <= 0:\n            raise ValueError(\"num_classes must be a positive integer or None\")\n\n    # Check label bounds\n    if np.any(y_arr >= num_classes):\n        raise ValueError(\"All labels must be less than num_classes\")\n\n    # Build the one-hot matrix\n    one_hot = np.zeros((n, num_classes), dtype=int)\n    rows = np.arange(n)\n    one_hot[rows, y_arr] = 1\n\n    return one_hot\n```\n\nExplanation of the key steps:\n1. **Input normalization**: Convert `y` to a NumPy array and enforce that it is 1-D.\n2. **Empty input**: If `y` is empty, we return an array of shape `(0, num_classes)` (or `(0, 0)` if `num_classes` is also `None`).\n3. **Type checks**:  \n   - Ensure the data are integers (or floats that are integer-valued), otherwise raise a `ValueError`.  \n   - Ensure no negative labels.\n4. **Determine `num_classes`**: Use the provided value or infer it as `max(y) + 1`.\n5. **Bounds check**: All labels must satisfy `0 <= label < num_classes`.\n6. **One-hot encoding**: Create a zero matrix of shape `(len(y), num_classes)` and place 1's at the appropriate column positions."}
{"task_id": 485, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n   Fit a univariate regression tree of depth *max_depth* and predict.\n\n    Parameters\n    ----------\n    X_train : list[float | int]\n        1-D feature values for the training set.\n    y_train : list[float | int]\n        Continuous target values for the training set.\n    X_test : list[float | int]\n        1-D feature values for which predictions are required.\n    max_depth : int, default 3\n        Maximum depth of the binary tree (root has depth 0).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every element in *X_test*.\n       # If no training data, nothing to fit; return empty list\n    if len(X_train) == 0:\n        return []\n\n    # Convert training data to numpy arrays for convenience\n    X_train_arr = np.array(X_train, dtype=float)\n    y_train_arr = np.array(y_train, dtype=float)\n\n    def build_tree(X: np.ndarray, y: np.ndarray, depth: int):\n        # Create a leaf if depth limit reached or fewer than 2 samples\n        if depth >= max_depth or X.shape[0] < 2:\n            return {\n                'split': None,\n                'value': float(np.mean(y)),\n                'left': None,\n                'right': None\n            }\n\n        # Sort by X to speed up searching for best split\n        sort_idx = np.argsort(X)\n        Xs = X[sort_idx]\n        ys = y[sort_idx]\n        n = Xs.shape[0]\n\n        # Precompute prefix sums and squared sums of targets\n        cumsum_y = np.cumsum(ys)\n        cumsum_yy = np.cumsum(ys * ys)\n        total_y = cumsum_y[-1]\n        total_yy = cumsum_yy[-1]\n\n        best_sse = np.inf\n        best_split = None\n        best_i = None\n\n        # Try splits between unique consecutive Xs\n        for i in range(1, n):\n            # skip if no change in feature\n            if Xs[i] == Xs[i - 1]:\n                continue\n            # Left side stats\n            nL = i\n            sumL = cumsum_y[i - 1]\n            sqL = cumsum_yy[i - 1]\n            # Right side stats\n            nR = n - i\n            sumR = total_y - sumL\n            sqR = total_yy - sqL\n            # Compute SSE = sum(yi^2) - sum(yi)^2 / n\n            sseL = sqL - (sumL * sumL) / nL\n            sseR = sqR - (sumR * sumR) / nR\n            sse = sseL + sseR\n            if sse < best_sse:\n                best_sse = sse\n                best_i = i\n                # midpoint between Xs[i-1] and Xs[i]\n                best_split = 0.5 * (Xs[i - 1] + Xs[i])\n\n        # If no valid split found (e.g. all Xs identical), make a leaf\n        if best_split is None:\n            return {\n                'split': None,\n                'value': float(np.mean(y)),\n                'left': None,\n                'right': None\n            }\n\n        # Partition data according to the best split\n        left_mask = X < best_split\n        right_mask = ~left_mask\n\n        left_tree = build_tree(X[left_mask], y[left_mask], depth + 1)\n        right_tree = build_tree(X[right_mask], y[right_mask], depth + 1)\n\n        return {\n            'split': best_split,\n            'value': None,\n            'left': left_tree,\n            'right': right_tree\n        }\n\n    # Build the tree from the training data\n    tree = build_tree(X_train_arr, y_train_arr, depth=0)\n\n    # Prediction helper: traverse the tree for a single x\n    def predict_one(x, node):\n        if node['split'] is None:\n            return node['value']\n        if x < node['split']:\n            return predict_one(x, node['left'])\n        else:\n            return predict_one(x, node['right'])\n\n    # Apply to all test points\n    preds = [predict_one(float(x), tree) for x in X_test]\n    return preds\n```\n\nExplanation of key steps:\n\n1. **Base cases**:\n   - If the training set is empty, immediately return an empty list.\n   - In recursion, if the node\u2019s depth reaches `max_depth` or it has fewer than 2 samples, make it a leaf storing the mean of the node\u2019s target values.\n\n2. **Finding the best split**:\n   - Sort the data by the feature.\n   - Use prefix sums of `y` and `y^2` to compute the SSE on the left and right of each candidate split in O(1) time per split.\n   - Only consider splits between distinct feature values.\n   - Pick the split that minimizes the combined SSE.\n\n3. **Recursion**:\n   - Partition the (X, y) pairs by the best split and recurse on each child, increasing the depth.\n\n4. **Prediction**:\n   - For each `x` in `X_test`, start at the root and follow left/right child pointers based on `< split` until a leaf is reached, returning the leaf\u2019s mean value."}
{"task_id": 490, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    # Convert input to a NumPy array\n    arr = np.asarray(x)\n\n    # Ensure it is one-dimensional\n    if arr.ndim != 1:\n        raise ValueError(\"Input must be one-dimensional\")\n\n    # If empty, return empty list\n    if arr.size == 0:\n        return []\n\n    # Build a diagonal matrix and convert it to a list of lists\n    diag_mat = np.diag(arr)\n    return diag_mat.tolist()\n```\n\nExplanation of the steps:\n1. We use `np.asarray(x)` to convert whatever was passed in (list, tuple, or ndarray) into a NumPy array.\n2. We check that the result is one-dimensional. If not, we raise a `ValueError`.\n3. If the array is empty (`size == 0`), we immediately return `[]`.\n4. Otherwise we call `np.diag(arr)`, which produces a square matrix with `arr` on its main diagonal and zeros elsewhere.\n5. Finally, we convert that matrix to a native Python list of lists with `.tolist()`."}
{"task_id": 491, "completion_id": 0, "completion": "```python\nfrom collections import Counter, defaultdict\nimport itertools\n\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\n\n    Parameters\n    ----------\n    transactions : Iterable[Iterable[Hashable]]\n        A collection of transactions.  Each transaction is an iterable containing hashable items.\n    min_sup : int\n        Minimum number of occurrences an item-set must have to be considered frequent.\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets sorted 1) by length, 2) lexicographically.  Every\n        tuple itself is sorted lexicographically.\n    \"\"\"\n\n    # 1) First scan: count item frequencies\n    freq = Counter()\n    for t in transactions:\n        freq.update(t)\n    # keep only globally frequent items\n    freq = {item: cnt for item, cnt in freq.items() if cnt >= min_sup}\n    if not freq:\n        return []\n\n    # Node definition for the FP-tree\n    class Node:\n        __slots__ = (\"item\", \"count\", \"parent\", \"children\", \"link\")\n        def __init__(self, item, count, parent):\n            self.item = item\n            self.count = count\n            self.parent = parent\n            self.children = {}\n            self.link = None\n\n    # Build the main FP-tree\n    # Header table: item -> [support, head, tail]\n    header = {item: [cnt, None, None] for item, cnt in freq.items()}\n\n    root = Node(None, 0, None)\n\n    def insert_tree(items, node, count):\n        \"\"\"Insert a sorted list of items into the FP-tree rooted at node.\"\"\"\n        if not items:\n            return\n        first = items[0]\n        # either extend existing child or create a new one\n        if first in node.children:\n            child = node.children[first]\n            child.count += count\n        else:\n            child = Node(first, count, node)\n            node.children[first] = child\n            # hook into header table's linked list\n            h = header[first]\n            if h[1] is None:       # no head yet\n                h[1] = child\n                h[2] = child\n            else:\n                h[2].link = child\n                h[2] = child\n        # recurse for the tail of items\n        insert_tree(items[1:], child, count)\n\n    # Prepare and insert each transaction\n    # Order each transaction's frequent items by descending global freq, then lex\n    for t in transactions:\n        filtered = [i for i in t if i in freq]\n        if not filtered:\n            continue\n        ordered = sorted(filtered, key=lambda x: (-freq[x], x))\n        insert_tree(ordered, root, 1)\n\n    # This will collect all frequent itemsets as tuples\n    patterns = set()\n\n    def mine_tree(header_table, prefix):\n        \"\"\"\n        Recursively mine the FP-tree whose header table is header_table,\n        using prefix as the suffix-root so far.\n        \"\"\"\n        # Order the items in the header by ascending frequency, then lex\n        items = sorted(\n            header_table.items(),\n            key=lambda kv: (kv[1][0], kv[0])\n        )\n        for item, (item_support, head, tail) in items:\n            # form new pattern = prefix U {item}\n            new_pat = tuple(sorted(prefix + (item,)))\n            patterns.add(new_pat)\n\n            # Build conditional pattern base (list of (path_items, count))\n            cond_patterns = []\n            node = head\n            while node:\n                cnt = node.count\n                # walk up to root, collecting items\n                path = []\n                parent = node.parent\n                while parent and parent.item is not None:\n                    path.append(parent.item)\n                    parent = parent.parent\n                if path:\n                    cond_patterns.append((path, cnt))\n                node = node.link\n\n            # Count conditional frequencies\n            cond_freq = Counter()\n            for path, cnt in cond_patterns:\n                cond_freq.update({itm: cnt for itm in path})\n            # keep only those meeting min_sup\n            cond_freq = {itm: c for itm, c in cond_freq.items() if c >= min_sup}\n            if not cond_freq:\n                continue\n\n            # Build conditional FP-tree\n            # new header for conditional tree\n            cond_header = {itm: [c, None, None] for itm, c in cond_freq.items()}\n            cond_root = Node(None, 0, None)\n\n            def insert_cond(items, node, count):\n                if not items:\n                    return\n                first = items[0]\n                if first in node.children:\n                    child = node.children[first]\n                    child.count += count\n                else:\n                    child = Node(first, count, node)\n                    node.children[first] = child\n                    h = cond_header[first]\n                    if h[1] is None:\n                        h[1] = child\n                        h[2] = child\n                    else:\n                        h[2].link = child\n                        h[2] = child\n                insert_cond(items[1:], child, count)\n\n            # Insert each pattern base path into conditional tree\n            for path, cnt in cond_patterns:\n                # filter path by cond_freq and order by descending cond_freq, then lex\n                filtered = [i for i in path if i in cond_freq]\n                if not filtered:\n                    continue\n                ordered = sorted(filtered, key=lambda x: (-cond_freq[x], x))\n                insert_cond(ordered, cond_root, cnt)\n\n            # Check if the conditional tree is a single path\n            # i.e., every node (except root) has at most one child\n            def is_single_path(node):\n                if len(node.children) > 1:\n                    return False\n                if not node.children:\n                    return True\n                # go deeper\n                child = next(iter(node.children.values()))\n                return is_single_path(child)\n\n            if is_single_path(cond_root):\n                # collect the single path of items\n                single_path = []\n                cur = cond_root\n                while len(cur.children) == 1:\n                    cur = next(iter(cur.children.values()))\n                    single_path.append(cur.item)\n                # enumerate all non-empty combinations of items in single_path\n                for r in range(1, len(single_path) + 1):\n                    for comb in itertools.combinations(single_path, r):\n                        newset = tuple(sorted(prefix + (item,) + comb))\n                        patterns.add(newset)\n            else:\n                # recursively mine the conditional FP-tree\n                mine_tree(cond_header, prefix + (item,))\n\n    # Kick off mining with the main header table and empty prefix\n    mine_tree(header, ())\n\n    # Prepare final sorted list of frequent itemsets\n    # 1) by length, then 2) lexicographically\n    result = sorted(patterns, key=lambda t: (len(t), t))\n    return result\n```\n\nExplanation of key steps:\n\n1. **First pass**: We count item frequencies and drop those below `min_sup`.\n2. **FP-tree construction**:  \n   - We sort each transaction\u2019s frequent items in descending global frequency (tie\u2010breaking lexicographically) and insert them into the tree.  \n   - Each node has a `link` pointer; the `header` table links all nodes of the same item in a singly-linked list for easy pattern base extraction.\n3. **Mining recursively**:\n   - For each item in the header table (in ascending support order), we  \n     a. Output the pattern = current prefix \u222a {item},  \n     b. Build its conditional pattern base by following the item\u2019s node links and collecting prefix paths,  \n     c. Build a conditional FP-tree from those paths (filtering by `min_sup` again),  \n     d. If that conditional tree is a single path, emit **all** combinations of its items at once, each appended to the new prefix; otherwise recurse.\n4. **Formatting**: We collect all patterns in a `set` to avoid duplicates, then sort them first by length, then lexicographically as required."}
{"task_id": 492, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to a given degree.\n\n    The function should take a 2-D list *X* whose rows correspond to\n    observations and columns correspond to original features.  It must\n    return a new list of lists containing, for every observation, all\n    monomials of the original features whose total degree does not exceed\n    the supplied *degree*.  The very first element in every row should be\n    1 (the degree-0 term).\n\n    If *degree* is negative the function must return -1.\n\n    Args:\n        X: Input data of shape (n_samples, n_features).\n        degree: Maximum total degree of the generated polynomial terms.\n\n    Returns:\n        A 2-D Python list with the enriched feature matrix, or \u20111 when\n        *degree* is negative.\n    \"\"\"\n    # Handle the negative-degree case\n    if degree < 0:\n        return -1\n\n    # Convert input to numpy array for vectorized operations\n    X_arr = np.array(X, dtype=float)\n    n_samples, n_features = X_arr.shape\n\n    # Generate all index\u2010multisets (combinations with replacement)\n    # of total degree 0, 1, 2, ..., degree\n    all_combos = []\n    for total_deg in range(degree + 1):\n        # combinations_with_replacement gives tuples of length total_deg\n        # each tuple is non-decreasing, which enforces lexicographic order\n        for combo in combinations_with_replacement(range(n_features), total_deg):\n            all_combos.append(combo)\n\n    # For each combination, compute the product of the corresponding columns\n    # X_arr[:, combo] has shape (n_samples, total_deg). np.prod over axis=1\n    # yields an array of length n_samples.  If combo is empty (degree 0),\n    # np.prod of an empty slice gives 1 -> the constant term.\n    feature_cols = [np.prod(X_arr[:, combo], axis=1) for combo in all_combos]\n\n    # Stack them horizontally: result shape = (n_samples, n_output_features)\n    poly_matrix = np.stack(feature_cols, axis=1)\n\n    # Convert back to a plain Python list of lists and return\n    return poly_matrix.tolist()\n```\n\nExplanation of the core steps:\n\n1. **Negative degree**: Return `-1` immediately as specified.  \n2. **Input conversion**: Cast `X` to a NumPy array of floats for easy slicing and product computations.  \n3. **Index multiset generation**:  \n   - Loop `total_deg` from `0` up to `degree`.  \n   - For each, use `itertools.combinations_with_replacement(range(n_features), total_deg)`.  \n   - This produces all ways to pick `total_deg` indices (with repetition allowed), in non\u2010decreasing (lexicographic) order.  \n4. **Column construction**: For each index\u2010multiset `combo`, compute  \n   ```\n   np.prod(X_arr[:, combo], axis=1)\n   ```  \n   - If `combo` is empty (degree 0), NumPy returns an array of ones, giving the constant term.  \n5. **Assembly**: Stack these column arrays side by side and convert to a list of lists.\n\nThis precisely matches the requirements: all monomials up to total degree `d`, sorted first by degree and then lexicographically, with the first column being all 1\u2019s."}
{"task_id": 493, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse(y):\n   Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    returns -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n       # Convert to numpy array of float\n    arr = np.asarray(y, dtype=float)\n    # Check for empty input\n    if arr.size == 0:\n        return -1\n    # Compute mean\n    mu = arr.mean()\n    # Compute mean of squared deviations\n    mse_val = ((arr - mu) ** 2).mean()\n    # Round to 4 decimal places and return as Python float\n    return round(float(mse_val), 4)\n```\n\nExplanation:\n1. We first coerce the input `y` into a NumPy array of floats, which handles both lists and existing arrays uniformly.  \n2. If `arr.size` is zero, we immediately return -1.  \n3. Otherwise we compute the sample mean `mu`, then take the average of `(arr - mu)**2`.  \n4. Finally, we round the result to four decimal places and cast to a native Python `float` before returning."}
{"task_id": 496, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values (or a scalar).\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Ensure x is at least a 1D array\n    arr = np.array(x, dtype=float, copy=False)\n    arr = np.atleast_1d(arr)\n    \n    # Compute the affine activation\n    y = slope * arr + intercept\n    \n    # First derivative is constant = slope\n    grad = np.full_like(arr, slope, dtype=float)\n    \n    # Second derivative is constant = 0\n    grad2 = np.zeros_like(arr, dtype=float)\n    \n    # Round each element to 4 decimal places and convert to Python lists\n    y_rounded     = [round(float(v), 4) for v in y]\n    grad_rounded  = [round(float(v), 4) for v in grad]\n    grad2_rounded = [round(float(v), 4) for v in grad2]\n    \n    return y_rounded, grad_rounded, grad2_rounded\n```\n\nExplanation of key steps:\n1. We convert the input `x` into a NumPy array and use `np.atleast_1d` so that even scalars become length-1 arrays.\n2. We compute the affine mapping element-wise: `y = slope * x + intercept`.\n3. The first derivative of a linear (affine) function is the constant `slope`; the second derivative is always `0`.\n4. We round every element in the outputs to 4 decimal places and return them as plain Python lists."}
{"task_id": 499, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places. If *k* is outside the\n        valid range, return \u22121.\n    \"\"\"\n    # Check dimensions and k\n    if data.ndim != 2:\n        raise ValueError(\"data must be a 2-D array\")\n    n_samples, n_features = data.shape\n    if not (1 <= k <= n_features):\n        return -1\n\n    # 1. Standardise each feature (zero mean, unit population variance)\n    means = np.mean(data, axis=0)\n    vars_ = np.var(data, axis=0)  # population variance (ddof=0)\n    # Avoid division by zero: for zero-variance features, keep denom=1\n    denom = np.sqrt(vars_)\n    denom[denom == 0] = 1.0\n\n    X_centered = data - means\n    X_std = X_centered / denom\n\n    # 2. Compute sample covariance matrix (divide by n_samples - 1)\n    cov = (X_std.T @ X_std) / (n_samples - 1)\n\n    # 3. Eigen-decomposition (cov is symmetric)\n    eigvals, eigvecs = np.linalg.eigh(cov)\n\n    # 4. Sort eigenvalues descending and reorder eigenvectors\n    idx = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[idx]\n    eigvecs = eigvecs[:, idx]\n\n    # 5. Fix signs: make the largest-abs entry in each eigenvector positive\n    for j in range(eigvecs.shape[1]):\n        vec = eigvecs[:, j]\n        max_idx = np.argmax(np.abs(vec))\n        if vec[max_idx] < 0:\n            eigvecs[:, j] = -vec\n\n    # 6. Project the standardized data onto the first k eigenvectors\n    top_vecs = eigvecs[:, :k]\n    projected = X_std @ top_vecs\n\n    # Round and convert to list of lists\n    projected_rounded = np.round(projected, 4)\n    return projected_rounded.tolist()\n```"}
{"task_id": 500, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_classifier(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    n_estimators: int = 20,\n    learning_rate: float = 0.1\n) -> list[int]:\n   Train a tiny Gradient Boosting model using decision stumps and predict labels.\n\n    The ensemble minimises the squared-error on the *binary* targets and turns the\n    final regression score into a class label by thresholding at 0.5.\n\n    Args:\n        X_train: Training samples, each sample being a list of feature values.\n        y_train: Binary labels (0 or 1) for the training samples.\n        X_test: Samples to predict \u2013 identical structure to X_train.\n        n_estimators: Number of boosting stages to perform.\n        learning_rate: Shrinkage factor (\u03b7) applied to each weak learner.\n\n    Returns:\n        A list containing the predicted class (0 or 1) for every sample in\n        X_test.\n       # Convert inputs to numpy arrays\n    X_tr = np.array(X_train, dtype=float)\n    y_tr = np.array(y_train, dtype=float)\n    X_te = np.array(X_test, dtype=float)\n\n    n_samples, n_features = X_tr.shape\n\n    # Initial constant prediction c = mean(y)\n    c0 = y_tr.mean()\n    # F_train holds the current ensemble prediction for each train sample\n    F_train = np.full(n_samples, c0, dtype=float)\n\n    # Store the stumps as tuples: (feature_index, threshold, a_left, a_right)\n    stumps = []\n\n    # Boosting iterations\n    for m in range(n_estimators):\n        # 1) Compute residuals\n        residuals = y_tr - F_train\n        S_total = residuals.sum()\n        SS_total = np.dot(residuals, residuals)\n\n        best_sse = np.inf\n        best_stump = None  # will hold (j, threshold, a_left, a_right)\n\n        # 2) Find the best decision stump for these residuals\n        for j in range(n_features):\n            x_j = X_tr[:, j]\n            # sort feature values and corresponding residuals\n            sorted_idx = np.argsort(x_j)\n            x_sorted = x_j[sorted_idx]\n            r_sorted = residuals[sorted_idx]\n\n            # prefix sums for fast SSE calc\n            csum_r = np.cumsum(r_sorted)\n            csum_r2 = np.cumsum(r_sorted * r_sorted)\n\n            # try every split between distinct consecutive x's\n            for i in range(n_samples - 1):\n                if x_sorted[i] == x_sorted[i + 1]:\n                    continue  # no meaningful split here\n                nL = i + 1\n                nR = n_samples - nL\n\n                sumL = csum_r[i]\n                sumR = S_total - sumL\n                sumL2 = csum_r2[i]\n                sumR2 = SS_total - sumL2\n\n                # SSE in left and right\n                sseL = sumL2 - (sumL * sumL) / nL\n                sseR = sumR2 - (sumR * sumR) / nR\n                sse_total = sseL + sseR\n\n                if sse_total < best_sse:\n                    best_sse = sse_total\n                    thresh = 0.5 * (x_sorted[i] + x_sorted[i + 1])\n                    aL = sumL / nL\n                    aR = sumR / nR\n                    best_stump = (j, thresh, aL, aR)\n\n        # 3) If no valid split found (all X's constant), fit a constant stump\n        if best_stump is None:\n            # stump predicts the mean residual everywhere\n            mean_res = residuals.mean()\n            stumps.append((0, np.inf, mean_res, mean_res))\n            F_train += learning_rate * mean_res\n        else:\n            j_star, thresh_star, aL_star, aR_star = best_stump\n            stumps.append(best_stump)\n            # update F_train with this stump's predictions\n            pred = np.where(X_tr[:, j_star] <= thresh_star, aL_star, aR_star)\n            F_train += learning_rate * pred\n\n    # 4) Predict on X_test\n    n_test = X_te.shape[0]\n    F_test = np.full(n_test, c0, dtype=float)\n    for (j, thresh, aL, aR) in stumps:\n        pred_te = np.where(X_te[:, j] <= thresh, aL, aR)\n        F_test += learning_rate * pred_te\n\n    # 5) Threshold final scores at 0.5 -> class labels\n    y_pred = (F_test >= 0.5).astype(int).tolist()\n    return y_pred\n```\n\nExplanation of the main steps:\n\n1. **Initialization**  \n   - Start with a constant model \\(F_0(x)=c_0=\\bar y\\).\n\n2. **Boosting Loop** (repeated `n_estimators` times)  \n   a. Compute residuals \\(r_i=y_i-F_{m-1}(x_i)\\).  \n   b. For each feature, sort the data by that feature and use prefix sums to quickly evaluate every possible threshold.  \n   c. For each split, compute the best\u2010fit constants \\(a_L,a_R\\) on left/right and the resulting sum of squared errors.  \n   d. Pick the split that minimizes SSE, record it as a stump, and update \\(F_m(x)=F_{m-1}(x)+\\eta\\,h_m(x)\\).  \n\n3. **Prediction**  \n   - Evaluate the sum of all stumps plus the initial constant on each test point, then threshold at 0.5 to get binary outputs.\n\nThis approach uses only one-dimensional splits (stumps) and the squared\u2013error loss, treating the binary labels as regression targets in the boosting process."}
{"task_id": 505, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n   Perform one CD-k weight update for a binary RBM.\n\n    Args:\n        X: A 2-D NumPy array of shape (batch_size, n_visible) containing the\n           mini-batch of visible units. Values are expected to be in [0,1].\n        W: Weight matrix of shape (n_visible, n_hidden).\n        hbias: 1-D NumPy array (length n_hidden) with hidden unit biases.\n        vbias: 1-D NumPy array (length n_visible) with visible unit biases.\n        learning_rate: Scalar learning-rate used for the update.\n        k: Number of Gibbs sampling steps to run (CD-k).\n\n    Returns:\n        The updated weight matrix rounded to 4 decimal places and converted\n        to a regular Python list of lists.\n        If X is empty, returns an empty list.\n       # If the mini-batch is empty, return empty list\n    m = X.shape[0]\n    if m == 0:\n        return []\n\n    def sigmoid(x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    # Positive phase: compute hidden probabilities given data\n    # h0_prob shape: (m, n_hidden)\n    h0_lin = X.dot(W) + hbias  # broadcast hbias over rows\n    h0_prob = sigmoid(h0_lin)\n\n    # Start Gibbs chain from h0_prob, but using probabilities (no sampling)\n    h_prob = h0_prob.copy()\n    for _ in range(k):\n        # Reconstruct visible probabilities\n        v_lin = h_prob.dot(W.T) + vbias  # shape (m, n_visible)\n        v_prob = sigmoid(v_lin)\n        # Recompute hidden probabilities\n        h_lin = v_prob.dot(W) + hbias   # shape (m, n_hidden)\n        h_prob = sigmoid(h_lin)\n\n    v_k_prob = v_prob   # from last iteration\n    h_k_prob = h_prob\n\n    # Compute gradients\n    # pos_grad shape: (n_visible, n_hidden)\n    pos_grad = X.T.dot(h0_prob)\n    # neg_grad shape: (n_visible, n_hidden)\n    neg_grad = v_k_prob.T.dot(h_k_prob)\n\n    # Update weights\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n\n    # Round to 4 decimal places and convert to list of lists\n    W_rounded = np.round(W_new, 4)\n    return W_rounded.tolist()\n```\n\nExplanation of key steps:\n\n1. **Empty batch check**  \n   If `X.shape[0] == 0`, we immediately return `[]` to satisfy the requirement for an empty mini-batch.\n\n2. **Positive phase**  \n   - Compute the pre-activation `h0_lin = X\u00b7W + hbias`  \n   - Apply the sigmoid to get hidden probabilities `h0_prob`.\n\n3. **Gibbs chain for CD-k**  \n   We start from `h0_prob` and for each of the `k` steps:\n   - Reconstruct visible probabilities `v_prob = sigmoid(h_prob\u00b7W\u1d40 + vbias)`\n   - Recompute hidden probabilities `h_prob = sigmoid(v_prob\u00b7W + hbias)`\n\n4. **Gradient estimation**  \n   - Positive gradient: `pos_grad = X\u1d40 \u00b7 h0_prob`  \n   - Negative gradient: `neg_grad = v_k_prob\u1d40 \u00b7 h_k_prob`\n\n5. **Weight update**  \n   `W_new = W + learning_rate * (pos_grad - neg_grad) / m`\n\n6. **Formatting**  \n   The updated weight matrix is rounded to 4 decimal places and converted to a plain Python list of lists before returning."}
{"task_id": 509, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Union\n\ndef chebyshev(x: Union[list[float], \"np.ndarray\"],\n              y: Union[list[float], \"np.ndarray\"]) -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors x and y must be one-dimensional and of identical\n    length. If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    # We only expect lists or 1-D numpy arrays\n    # First, check lengths\n    try:\n        len_x = len(x)\n        len_y = len(y)\n    except Exception:\n        # If objects don't support len(), convert to numpy 1-D\n        x = np.asarray(x).ravel()\n        y = np.asarray(y).ravel()\n        len_x = x.shape[0]\n        len_y = y.shape[0]\n\n    if len_x != len_y:\n        return -1\n\n    # If both are numpy arrays, flatten them to 1-D views\n    if isinstance(x, np.ndarray):\n        x_iter = x.ravel()\n    else:\n        x_iter = x\n    if isinstance(y, np.ndarray):\n        y_iter = y.ravel()\n    else:\n        y_iter = y\n\n    # Edge case: empty vectors\n    if len_x == 0:\n        return 0.0\n\n    # Compute the component-wise absolute differences and take the maximum\n    max_diff = max(abs(a - b) for a, b in zip(x_iter, y_iter))\n\n    # Round to four decimal places and ensure a Python float is returned\n    return float(round(max_diff, 4))\n```\n\nExplanation of key steps:\n\n1. **Length check**  \n   We want to quickly reject mismatched lengths. The built-in `len()` works on both Python lists and NumPy 1-D arrays. If they differ, we return `-1`.\n\n2. **Flattening**  \n   If the inputs are NumPy arrays (of any shape), we `.ravel()` them to a one-dimensional view so that `zip` will iterate component-wise correctly.\n\n3. **Empty vectors**  \n   By convention, we define the Chebyshev distance between two empty vectors to be `0.0`.\n\n4. **Core computation**  \n   We use a generator expression to compute `abs(a - b)` for each pair of components, then take the `max` of those differences.\n\n5. **Rounding and type conversion**  \n   Finally, we round the result to 4 decimal places with Python\u2019s built-in `round(...)` and wrap it in `float(...)` to guarantee a standard Python float (not a NumPy scalar).\n"}
{"task_id": 510, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function returns the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as y.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as t_mean.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    # Numerical stability epsilon\n    eps = np.finfo(float).eps\n\n    # Clip predictions to avoid log(0)\n    y_pred_clipped = np.clip(y_pred, eps, 1.0 - eps)\n\n    # Reconstruction loss (binary cross-entropy), summed over all elements\n    recon = - (y * np.log(y_pred_clipped) +\n               (1 - y) * np.log(1 - y_pred_clipped))\n    recon_loss = np.sum(recon)\n\n    # KL divergence term, summed over batch and latent dims\n    # KL = -0.5 * sum(1 + log_var - mean^2 - exp(log_var))\n    kl = -0.5 * np.sum(1 + t_log_var - np.square(t_mean) - np.exp(t_log_var))\n\n    # Average over batch\n    batch_size = y.shape[0]\n    total_loss = (recon_loss + kl) / batch_size\n\n    # Round to 6 decimal places and return as Python float\n    return round(float(total_loss), 6)\n```\n\nExplanation of steps:\n1. We clip `y_pred` into `(eps, 1 - eps)` to ensure numerical stability when taking logs.\n2. The reconstruction loss is the element\u2010wise binary cross\u2010entropy between `y` and `y_pred`, summed over all samples and features.\n3. The KL divergence is computed analytically for a diagonal Gaussian approximate posterior against the standard normal prior, summed over batch and latent dimensions.\n4. We divide the total by `batch_size` to get the average loss per example.\n5. Finally, we round to six decimal places as required."}
{"task_id": 513, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    X_arr = np.array(X, dtype=float)      # shape (n_samples, n_features)\n    w_arr = np.array(w, dtype=float)      # shape (n_features,)\n    V_arr = np.array(V, dtype=float)      # shape (n_features, k)\n\n    # Linear term: \u03a3_j w_j * x_j\n    linear_term = X_arr.dot(w_arr)        # shape (n_samples,)\n\n    # Interaction term:\n    #  \u00bd \u03a3_f [ (\u03a3_j V_{j,f} x_j)^2 \u2212 \u03a3_j V_{j,f}^2 x_j^2 ]\n    # First part: (X * V) summed over features, then squared\n    XV = X_arr.dot(V_arr)                 # shape (n_samples, k)\n    sum_squared = np.sum(XV * XV, axis=1) # shape (n_samples,)\n\n    # Second part: sum over j of (V_{j,f}^2 * x_j^2)\n    X_squared = X_arr * X_arr             # shape (n_samples, n_features)\n    V_squared = V_arr * V_arr             # shape (n_features, k)\n    squared_sum = np.sum(X_squared.dot(V_squared), axis=1)  # shape (n_samples,)\n\n    interaction_term = 0.5 * (sum_squared - squared_sum)    # shape (n_samples,)\n\n    # Final prediction\n    y = w0 + linear_term + interaction_term  # shape (n_samples,)\n\n    # Round to 4 decimal places and convert to Python list\n    y_rounded = np.round(y, 4).tolist()\n    return y_rounded\n```\n\nExplanation of key steps:\n1. Convert inputs to NumPy arrays for vectorized operations.  \n2. Compute the linear part `X_arr.dot(w_arr)`.  \n3. For the interaction part:\n   - Compute `XV = X_arr.dot(V_arr)` which is the matrix of summed latent interactions per factor.\n   - Square it and sum over latent factors to get the first term of the interaction.\n   - Compute the second term by squaring `X_arr` and `V_arr` elementwise, then taking the dot\u2010product and summing over factors.\n   - Combine both with the 0.5 factor.\n4. Sum bias, linear, and interaction terms to form the final prediction.  \n5. Round to 4 decimals and return as a list of floats."}
{"task_id": 517, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n   Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n       # Detect whether the original input is a scalar\n    is_scalar_input = np.isscalar(z)\n\n    # Convert input to a NumPy array of floats (this handles scalars, lists, tuples)\n    z_arr = np.array(z, dtype=float)\n\n    # Numerically stable sigmoid:\n    #   for z >= 0: 1 / (1 + exp(-z))\n    #   for z <  0: exp(z) / (1 + exp(z))\n    out = np.where(\n        z_arr >= 0,\n        1.0 / (1.0 + np.exp(-z_arr)),\n        np.exp(z_arr) / (1.0 + np.exp(z_arr))\n    )\n\n    # Round to 4 decimal places\n    out = np.round(out, 4)\n\n    # Return a Python float if the input was scalar, else return a nested list\n    if is_scalar_input:\n        return out.item()\n    else:\n        return out.tolist()\n```\n\nExplanation of key points:\n1. We first check `np.isscalar(z)` to remember if the user passed in a true scalar (Python int/float or NumPy scalar).  \n2. By converting `z` to a NumPy array `z_arr`, we get uniform handling of lists, tuples, and arrays.  \n3. We implement the \u201ctwo\u2010branch\u201d sigmoid for numerical stability:\n   - When `z >= 0`, computing `exp(-z)` is safe even for large positive `z`.\n   - When `z < 0`, computing `exp(z)` is safe even for large negative `z`.  \n4. We round the result with `np.round(..., 4)`.  \n5. Finally, we return either a Python float (via `.item()`) or a nested Python list (via `.tolist()`).  \n\nTest:\n```python\n>>> sigmoid([-1, 0, 1])\n[0.2689, 0.5, 0.7311]\n```"}
{"task_id": 518, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Callable\nfrom functools import wraps\n\ndef unhot(function: Callable) -> Callable:\n   Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n       @wraps(function)\n    def wrapper(actual: np.ndarray, predicted: np.ndarray):\n        # Convert actual if it's one-hot\n        if actual.ndim == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        # Convert predicted if it's one-hot\n        if predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        # Call the original metric and return its result\n        return function(actual, predicted)\n\n    return wrapper\n```\n\nExample usage:\n\n```python\nimport numpy as np\n\n@unhot\ndef accuracy(actual: np.ndarray, predicted: np.ndarray) -> float:\n    return float(np.round(np.mean(actual == predicted), 4))\n\n# One-hot inputs\nactual    = np.array([[0,1,0], [1,0,0], [0,0,1]])\npredicted = np.array([[0,1,0], [0,1,0], [0,0,1]])\n\nprint(accuracy(actual, predicted))  # 0.6667\n```"}
{"task_id": 520, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float, nested list, or ndarray) with each value\n        replaced by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    # Case 1: NumPy ndarray\n    if isinstance(z, np.ndarray):\n        return np.round(z * (1 - z), 4)\n\n    # Case 2: Python list (possibly nested)\n    elif isinstance(z, list):\n        # Empty list -> empty list\n        if len(z) == 0:\n            return []\n        # Recurse for each element\n        return [sigmoid_grad(elem) for elem in z]\n\n    # Case 3: scalar (int, float, or numpy scalar)\n    else:\n        grad_val = z * (1 - z)\n        # Round and return a Python float\n        return float(np.round(grad_val, 4))\n```\n\nExample usage and outputs:\n\n```python\n>>> sigmoid_grad(0.5)\n0.25\n>>> sigmoid_grad([0.2, 0.5, 1.0])\n[0.16, 0.25, 0.0]\n>>> sigmoid_grad([[0.2, 0.5], [0.0, 1.0]])\n[[0.16, 0.25], [0.0, 0.0]]\n>>> arr = np.array([[0.2, 0.5], [0.0, 1.0]])\n>>> sigmoid_grad(arr)\narray([[0.16, 0.25],\n       [0.0 , 0.0 ]])\n>>> sigmoid_grad([])\n[]\n```"}
{"task_id": 528, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    # Convert X to a NumPy array of shape (n, 2)\n    X_arr = np.array(X, dtype=float)\n    if X_arr.ndim != 2 or X_arr.shape[1] != 2:\n        raise ValueError(\"X must be a list of [x1, x2] pairs.\")\n    \n    # Extract weight components\n    w1, w2 = W\n\n    # Determine the plotting ranges\n    x1_min, x1_max = X_arr[:, 0].min(), X_arr[:, 0].max()\n    x2_min, x2_max = X_arr[:, 1].min(), X_arr[:, 1].max()\n    \n    # Build equally spaced 1D arrays for each axis\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    \n    # Create the meshgrid\n    X1_grid, X2_grid = np.meshgrid(x1_plot, x2_plot)\n    \n    # Compute the raw scores for each point: w1*x1 + w2*x2 + b\n    scores = w1 * X1_grid + w2 * X2_grid + b\n    \n    # Apply the sign rule: >= 0 -> 1, else -> -1\n    labels = np.where(scores >= 0, 1, -1).astype(int)\n    \n    # Convert to a nested Python list and return\n    return labels.tolist()\n```\n\nExplanation of the main steps:\n\n1. **Convert** the input `X` to a NumPy array and verify it has shape `(n, 2)`.\n2. **Find** the minimum and maximum of each column to determine the grid bounds.\n3. **Generate** two 1D arrays (`x1_plot`, `x2_plot`) of length `grid_n` spanning those bounds.\n4. **Form** the 2D mesh with `np.meshgrid`.\n5. **Compute** the classifier\u2019s linear score `w1\u00b7x1 + w2\u00b7x2 + b` at each grid point.\n6. **Label** each point with `1` if `score \u2265 0` and `-1` otherwise.\n7. **Return** the result as a nested Python list of shape `(grid_n \u00d7 grid_n)` containing only `1` and `-1`.\n"}
{"task_id": 537, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n   Gaussian Naive Bayes binary classifier.\n\n    Training phase: estimate mean, standard deviation and class prior for\n    every feature of both classes.\n    Prediction phase: compute the posterior probability for each class\n    using the Gaussian likelihood (with feature independence assumption)\n    and return the most probable class (0 or 1) for every test sample.\n\n    Args:\n        X_train: 2-D NumPy array of shape (n_samples, n_features) containing\n                  the training features.\n        y_train: 1-D NumPy array of length n_samples with binary labels\n                  (each entry is 0 or 1).\n        X_test:  2-D NumPy array of shape (m_samples, n_features) containing\n                  the data to classify.\n\n    Returns:\n        A Python list of length m_samples where each element is the\n        predicted class label (0 or 1).\n       # Small constant to avoid division by zero\n    eps = 1e-9\n\n    # Separate the training data by class\n    X0 = X_train[y_train == 0]\n    X1 = X_train[y_train == 1]\n\n    # Number of samples per class\n    n0 = X0.shape[0]\n    n1 = X1.shape[0]\n    n_total = n0 + n1\n\n    # Compute class priors\n    prior0 = n0 / n_total\n    prior1 = n1 / n_total\n\n    # Compute per-feature mean and std for each class (MLE, ddof=0)\n    mu0 = X0.mean(axis=0)\n    mu1 = X1.mean(axis=0)\n    sigma0 = X0.std(axis=0, ddof=0) + eps\n    sigma1 = X1.std(axis=0, ddof=0) + eps\n\n    # Precompute constants for Gaussian log-likelihood\n    # log(1/(sqrt(2*pi)*sigma)) = -0.5*log(2*pi) - log(sigma)\n    const0 = -0.5 * np.log(2 * np.pi) - np.log(sigma0)\n    const1 = -0.5 * np.log(2 * np.pi) - np.log(sigma1)\n\n    # For numerical stability, work in log-space\n    log_prior0 = np.log(prior0)\n    log_prior1 = np.log(prior1)\n\n    y_pred = []\n    for x in X_test:\n        # Compute log-likelihood for class 0\n        diff0 = x - mu0\n        log_lik0 = const0 - (diff0**2) / (2 * sigma0**2)\n        log_prob0 = log_prior0 + log_lik0.sum()\n\n        # Compute log-likelihood for class 1\n        diff1 = x - mu1\n        log_lik1 = const1 - (diff1**2) / (2 * sigma1**2)\n        log_prob1 = log_prior1 + log_lik1.sum()\n\n        # Predict the class with higher posterior log-probability\n        if log_prob1 > log_prob0:\n            y_pred.append(1)\n        else:\n            y_pred.append(0)\n\n    return y_pred\n```"}
{"task_id": 539, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n       Compute the binary cross\u2010entropy cost.\n\n    Arguments:\n    AL -- Predicted probabilities, shape (m,) or (1, m)\n    Y  -- True labels (0 or 1), shape same as AL\n\n    Returns:\n    cost -- Python float, the cross\u2010entropy cost\n       # Clip AL to avoid log(0)\n    eps = 1e-15\n    AL_clipped = np.clip(AL, eps, 1 - eps)\n\n    # Number of examples\n    m = Y.size\n\n    # Compute cross\u2010entropy\n    loss_terms = Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped)\n    cost = - np.sum(loss_terms) / m\n\n    # Ensure it's a native Python float and round to 4 decimal places\n    return round(float(cost), 4)\n```\n\nExplanation of key steps:\n1. We clip `AL` into \\[1e-15, 1\u22121e-15\\] to avoid taking `log(0)`.\n2. We compute the sum of `y\u00b7ln(a) + (1\u2212y)\u00b7ln(1\u2212a)` across all m examples.\n3. We multiply by `-1/m` to get the average loss.\n4. We convert to a Python float and round to 4 decimal places for the final output."}
{"task_id": 552, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation uses the forward-backward algorithm and returns the\n    resulting vector rounded to four decimal places.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'A' and 'B'.\n                    - pi: 1D array of shape (N,) initial state distribution\n                    - A:  2D array of shape (N, N) state transition matrix\n                    - B:  2D array of shape (N, M) emission probability matrix\n        observations (list[int]): Sequence of observation indices (each in [0, M-1]).\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t,\n                     rounded to 4 decimal places. Empty list if t is out of range.\n    \"\"\"\n    # number of time steps\n    T = len(observations)\n    # check time index\n    if t < 0 or t >= T:\n        return []\n\n    # unpack HMM parameters and ensure numpy arrays of float\n    pi = np.array(hmm['pi'], dtype=float)           # shape (N,)\n    A  = np.array(hmm['A'],  dtype=float)           # shape (N, N)\n    B  = np.array(hmm['B'],  dtype=float)           # shape (N, M)\n\n    N = pi.shape[0]  # number of hidden states\n\n    # -- Forward pass --\n    # alpha[u, i] = P(O[0..u], q_u = i | lambda)\n    alpha = np.zeros((T, N), dtype=float)\n    # initialization\n    o0 = observations[0]\n    alpha[0, :] = pi * B[:, o0]\n\n    # induction\n    for u in range(1, T):\n        o_u = observations[u]\n        # vectorized: for each state j:\n        # alpha[u, j] = [sum_i alpha[u-1, i] * A[i, j]] * B[j, o_u]\n        alpha[u, :] = (alpha[u - 1, :].reshape(-1, 1) * A).sum(axis=0) * B[:, o_u]\n\n    # -- Backward pass --\n    # beta[u, i] = P(O[u+1..T-1] | q_u = i, lambda)\n    beta = np.zeros((T, N), dtype=float)\n    # initialization\n    beta[T - 1, :] = 1.0\n\n    # induction\n    for u in range(T - 2, -1, -1):\n        o_next = observations[u + 1]\n        # for each state i:\n        # beta[u, i] = sum_j A[i, j] * B[j, o_next] * beta[u+1, j]\n        beta[u, :] = (A * (B[:, o_next] * beta[u + 1, :])).sum(axis=1)\n\n    # -- Compute gamma at time t --\n    numer = alpha[t, :] * beta[t, :]\n    denom = numer.sum()\n    # normalize\n    gamma_t = numer / denom\n\n    # round to 4 decimal places and convert to list\n    gamma_rounded = np.round(gamma_t, 4).tolist()\n    return gamma_rounded\n```\n\nExplanation of the main steps:\n\n1. **Checks**  \n   If the requested time step `t` is outside `[0, T-1]`, we immediately return an empty list.\n\n2. **Forward pass** (`alpha`)  \n   - Initialize  \n     \u03b1\u2080(i) = \u03c0_i \u00b7 b_i(o\u2080)  \n   - Recursion  \n     \u03b1_u(j) = [\u2211\u1d62 \u03b1_{u\u22121}(i) \u00b7 a_{ij}] \u00b7 b_j(o_u)\n\n3. **Backward pass** (`beta`)  \n   - Initialize  \n     \u03b2_{T\u22121}(i) = 1  \n   - Recursion  \n     \u03b2_u(i) = \u2211\u2c7c a_{ij} \u00b7 b_j(o_{u+1}) \u00b7 \u03b2_{u+1}(j)\n\n4. **Posterior** (`gamma`)  \n   \u03b3\u209c(i) = \u03b1\u209c(i) \u00b7 \u03b2\u209c(i) / \u2211\u2096 [\u03b1\u209c(k) \u00b7 \u03b2\u209c(k)]  \n   Finally we round each component to four decimal places.\n\nThis satisfies all requirements: it uses the forward\u2013backward algorithm, normalizes correctly, and returns a Python list of floats rounded to four decimal places."}
{"task_id": 555, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample.  Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    n_samples = data.shape[0]\n    # 1. Compute full pairwise distance matrix\n    #    (n_samples x n_samples)\n    diff = data[:, np.newaxis, :] - data[np.newaxis, :, :]\n    dist_matrix = np.sqrt(np.sum(diff**2, axis=2))\n\n    # 2. Build neighborhood list for each sample\n    neighborhoods = [\n        list(np.where(dist_matrix[i] <= eps)[0])\n        for i in range(n_samples)\n    ]\n\n    # 3. Identify core points\n    is_core = np.array([len(neigh) >= min_samples for neigh in neighborhoods])\n\n    # Prepare labels: 0 = unvisited/unlabeled, >0 = cluster IDs, -1 = noise\n    labels = np.zeros(n_samples, dtype=int)\n    cluster_id = 0\n\n    # 4. Expand clusters from each core point\n    for i in range(n_samples):\n        if labels[i] != 0:\n            continue  # already labeled or marked noise\n        if not is_core[i]:\n            continue  # not a core point, can't start a cluster\n\n        # Start a new cluster\n        cluster_id += 1\n        labels[i] = cluster_id\n        queue = deque([i])\n\n        while queue:\n            point = queue.popleft()\n            for neighbor in neighborhoods[point]:\n                if labels[neighbor] == 0:\n                    # Assign to this cluster\n                    labels[neighbor] = cluster_id\n                    # If neighbor is also a core, it can expand the cluster\n                    if is_core[neighbor]:\n                        queue.append(neighbor)\n\n    # 5. Label all remaining unlabeled points as noise (-1)\n    labels[labels == 0] = -1\n\n    # Return as a Python list of ints\n    return labels.tolist()\n```\n\nExplanation of key steps:\n\n1. **Distance Matrix**  \n   We use NumPy broadcasting to compute an `n\u00d7n` matrix `dist_matrix` where `dist_matrix[i, j]` is the Euclidean distance between point `i` and `j`.\n\n2. **Neighborhoods**  \n   For each point `i`, we collect all indices `j` such that `dist_matrix[i, j] \u2264 eps`. This is a Python list of lists.\n\n3. **Core Points**  \n   A point `i` is a *core* point if `len(neighborhoods[i]) \u2265 min_samples`.\n\n4. **Cluster Expansion (BFS)**  \n   We iterate over each point. If it\u2019s an unvisited core point, we start a new cluster, label it, and do a breadth\u2010first search using a queue:  \n   \u2013 Pop a point, look at its neighbors.  \n   \u2013 If a neighbor is still unlabeled (`labels[neighbor] == 0`), label it with the current cluster ID.  \n   \u2013 If that neighbor is itself a core point, enqueue it so its neighbors will be visited too.\n\n5. **Noise Assignment**  \n   After all core points and their density\u2010reachable points have been assigned to clusters, any point still labeled `0` is *noise*, so we set its label to `-1`.\n\nClusters are numbered in discovery order (1, 2, 3, \u2026), and noise points carry the label -1."}
{"task_id": 561, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef gmm_em(data: np.ndarray,\n           k: int,\n           max_iter: int = 200,\n           epsilon: float = 1e-4) -> list[int]:\n    \"\"\"\n    Fit a Gaussian Mixture Model (GMM) with k components to the data using the\n    Expectation\u2013Maximization (EM) algorithm, then return hard cluster assignments.\n    \n    Parameters\n    ----------\n    data : np.ndarray, shape (N, D)\n        The input data: N samples with D features each.\n    k : int\n        The number of Gaussian components (clusters).\n    max_iter : int\n        Maximum number of EM iterations.\n    epsilon : float\n        Convergence threshold on the change in mixture weights.\n    \n    Returns\n    -------\n    labels : list[int], length N\n        Hard cluster assignments (0 through k-1) for each sample.\n        If k==1, returns all zeros.\n    \"\"\"\n    N, D = data.shape\n    \n    # Trivial case\n    if k == 1:\n        return [0] * N\n    \n    # For reproducibility across calls\n    rng = np.random.RandomState(0)\n    \n    # --- Initialization via a random soft assignment, then M-step ---\n    # Random responsibilities\n    resp = rng.rand(N, k)\n    resp = resp / resp.sum(axis=1, keepdims=True)    # normalize per sample\n    \n    # M-step to get initial pi, means, covariances\n    def m_step(resp):\n        \"\"\"Given resp (N x k), compute pi (k,), means (k x D), covs (k x D x D).\"\"\"\n        Nk = resp.sum(axis=0)                # shape (k,)\n        pi = Nk / N                          # mixture weights\n        means = (resp.T @ data) / Nk[:, None]  # shape (k, D)\n        covs = np.zeros((k, D, D))\n        for j in range(k):\n            diff = data - means[j]           # (N, D)\n            # weighted outer products\n            w = resp[:, j][:, None]          # (N, 1)\n            covs[j] = (w * diff).T @ diff / Nk[j]\n            # regularize\n            covs[j] += np.eye(D) * 1e-6\n        return pi, means, covs\n    \n    pi, means, covs = m_step(resp)\n    \n    # Pre-allocate storage for responsibilities\n    resp = np.zeros((N, k))\n    \n    # Pre-compute constant\n    const_coeff = (2 * math.pi) ** (-D / 2)\n    \n    # EM loop\n    for iteration in range(max_iter):\n        pi_old = pi.copy()\n        \n        # E-step: compute responsibilities\n        for j in range(k):\n            # compute multivariate normal pdf for component j\n            cov_j = covs[j]\n            # inverse and determinant\n            inv_cov = np.linalg.inv(cov_j)\n            det_cov = np.linalg.det(cov_j)\n            coef = const_coeff * (det_cov ** (-0.5))\n            diff = data - means[j]             # (N, D)\n            # Mahalanobis term\n            exponent = -0.5 * np.sum(diff @ inv_cov * diff, axis=1)\n            pdf = coef * np.exp(exponent)      # shape (N,)\n            resp[:, j] = pi[j] * pdf\n        \n        # normalize responsibilities over j\n        row_sums = resp.sum(axis=1, keepdims=True)\n        # Avoid division by zero (shouldn't happen with reg)\n        row_sums[row_sums == 0] = np.finfo(float).eps\n        resp /= row_sums\n        \n        # M-step: update parameters\n        pi, means, covs = m_step(resp)\n        \n        # Check convergence on mixture weights\n        if np.max(np.abs(pi - pi_old)) < epsilon:\n            break\n    \n    # Final hard assignments: pick the component with highest responsibility\n    hard_labels = np.argmax(resp, axis=1)  # shape (N,)\n    \n    # To make labeling deterministic, sort components by means[:,0]\n    order = np.argsort(means[:, 0])  # old indices in ascending order\n    # Build a mapping old_index -> new_label\n    new_label = np.empty(k, dtype=int)\n    for new_idx, old_idx in enumerate(order):\n        new_label[old_idx] = new_idx\n    \n    # Apply mapping to hard_labels\n    final_labels = [int(new_label[lab]) for lab in hard_labels]\n    \n    return final_labels\n```\n\nExplanation of key steps:\n\n1. **Initialization**  \n   We start with a random soft assignment (`resp`) of each of the N points to the K components, normalize so each row sums to 1, and perform one M-step to obtain initial mixture weights `pi`, means `means`, and full covariance matrices `covs`. A fixed random seed (`RandomState(0)`) ensures deterministic behavior across runs.\n\n2. **E-step**  \n   For each component j we compute the multivariate Gaussian density at all data points, multiply by the mixture weight \u03c0_j, and then normalize across all components to get the responsibilities `resp[i, j]`.\n\n3. **M-step**  \n   Given responsibilities `resp`, we re-estimate  \n   - Nk = \u2211\u1d62 resp[i, j]  \n   - \u03c0_j = Nk / N  \n   - \u03bc_j = (1 / Nk) \u2211\u1d62 resp[i, j] \u00b7 x\u1d62  \n   - \u03a3_j = (1 / Nk) \u2211\u1d62 resp[i, j] \u00b7 (x\u1d62 - \u03bc_j)(x\u1d62 - \u03bc_j)\u1d40 + small_identity  \n\n4. **Convergence**  \n   We repeat E and M until either we reach `max_iter` or the maximum change in any \u03c0_j is less than `epsilon`.\n\n5. **Final hard labels and sorting**  \n   We assign each point to the component with the highest posterior responsibility.  \n   To ensure a consistent labeling (0, 1, \u2026, K\u22121) that\u2019s independent of the arbitrary internal ordering, we sort the components by the first coordinate of their mean, then relabel accordingly."}
{"task_id": 562, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A list of length N where the i-th element is an integer label identifying the\n        cluster assignment of the i-th sample (labels are 0-based).\n    \"\"\"\n    N, d = data.shape\n    if n_cluster <= 0:\n        raise ValueError(\"n_cluster must be >= 1\")\n    if n_cluster == 1:\n        return [0] * N\n\n    # 1) Build the similarity matrix W using RBF/Gaussian kernel\n    #    W_ij = exp(-gamma * ||x_i - x_j||^2), set diagonal to zero\n    # Compute pairwise squared distances\n    sq_norms = np.sum(data * data, axis=1)  # shape (N,)\n    dist2 = sq_norms[:, None] + sq_norms[None, :] - 2.0 * data.dot(data.T)\n    # Numerical safety\n    dist2 = np.maximum(dist2, 0.0)\n    W = np.exp(-gamma * dist2)\n    np.fill_diagonal(W, 0.0)\n\n    # 2) Construct Laplacian\n    degrees = np.sum(W, axis=1)\n    D = np.diag(degrees)\n    L = D - W\n    if method == 'normalized':\n        # Symmetric normalized Laplacian: L_sym = D^{-1/2} L D^{-1/2}\n        # Compute D^{-1/2}\n        with np.errstate(divide='ignore'):\n            d_inv_sqrt = 1.0 / np.sqrt(degrees)\n        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0\n        D_inv_sqrt = np.diag(d_inv_sqrt)\n        L = D_inv_sqrt @ L @ D_inv_sqrt\n    elif method != 'unnormalized':\n        raise ValueError(\"method must be 'unnormalized' or 'normalized'\")\n\n    # 3) Eigen-decomposition: get the first n_cluster eigenvectors\n    #    of the (normalized) Laplacian corresponding to the smallest eigenvalues\n    # Since L is symmetric, use eigh\n    eigvals, eigvecs = np.linalg.eigh(L)\n    # eigh returns them in ascending order\n    U = eigvecs[:, :n_cluster]  # shape (N, n_cluster)\n\n    # If normalized variant, row-normalize U\n    if method == 'normalized':\n        row_norms = np.linalg.norm(U, axis=1)\n        # avoid division by zero\n        nonzero = row_norms > 0\n        U[nonzero] = U[nonzero] / row_norms[nonzero, None]\n\n    # 4) Run k-means in the U-space with a deterministic init:\n    #    initial centroids are the first n_cluster rows of U\n    X = U.copy()\n    K = n_cluster\n    centroids = X[:K].copy()  # shape (K, n_cluster)\n    labels = np.full(N, -1, dtype=int)\n\n    for _ in range(100):  # max 100 iters\n        # Compute squared distances to centroids\n        # dist2[i,j] = ||X[i] - centroids[j]||^2\n        X_sq = np.sum(X * X, axis=1).reshape(-1, 1)    # (N,1)\n        C_sq = np.sum(centroids * centroids, axis=1).reshape(1, -1)  # (1,K)\n        d2 = X_sq + C_sq - 2.0 * X.dot(centroids.T)  # (N,K)\n        new_labels = np.argmin(d2, axis=1)\n\n        # If labels didn't change, we are done\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # Recompute centroids\n        for j in range(K):\n            members = X[labels == j]\n            if len(members) > 0:\n                centroids[j] = np.mean(members, axis=0)\n            # else: leave centroid unchanged\n\n    # 5) Relabel clusters so that cluster containing smallest original index is 0, next is 1, ...\n    # Find for each cluster the min index of a data point in it\n    min_idx = []\n    for j in range(K):\n        idxs = np.where(labels == j)[0]\n        if idxs.size > 0:\n            min_idx.append((j, idxs.min()))\n        else:\n            # If a cluster is empty, give it a large sentinel\n            min_idx.append((j, N + j))\n    # Sort clusters by that min index\n    min_idx.sort(key=lambda x: x[1])\n    # Build a mapping old_label -> new_label\n    new_label_map = {old: new for new, (old, _) in enumerate(min_idx)}\n    # Apply mapping\n    final_labels = [new_label_map[int(l)] for l in labels]\n\n    return final_labels\n```\n\nExplanation of key steps:\n\n1. **Similarity graph**: We compute all pairwise squared distances efficiently via  \n   `dist2 = |x|\u00b2 + |x|\u00b2^T - 2\u00b7X\u00b7X^T`, then apply the RBF kernel  \n   `W[i,j] = exp(-gamma * dist2[i,j])`, zeroing the diagonal to avoid self\u2010loops.\n\n2. **Laplacian**:  \n   - Unnormalized: `L = D \u2212 W`.  \n   - Normalized: `L_sym = D^(\u22121/2) \u00b7 (D\u2212W) \u00b7 D^(\u22121/2)`.\n\n3. **Eigen-decomposition**: Since `L` (or `L_sym`) is symmetric, `np.linalg.eigh` gives sorted eigenvalues/eigenvectors. We take the first `k` eigenvectors.\n\n4. **Row-normalization** (only for the normalized method): each sample\u2019s eigenvector row is scaled to unit length.\n\n5. **K-means**:  \n   - Deterministic initialization using the first `k` rows of the eigenvector matrix.  \n   - Iteratively assign each point to its closest centroid (by Euclidean distance) and recompute centroids.  \n   - Stop when labels stabilize or after 100 iterations.\n\n6. **Relabeling**: We ensure the cluster with the globally smallest sample index becomes label `0`, the next becomes `1`, etc., so labels are consistently ordered.\n\nThe function finally returns a plain Python list of integer labels of length `N`."}
{"task_id": 563, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data:   2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data:   1-D NumPy array of length N with labels \u22121 or 1.\n        eta:      Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    # Number of samples (N) and feature-dimension (d)\n    N, d = X_data.shape\n\n    # Initialize dual coefficients and bias\n    alpha = np.zeros(N, dtype=float)\n    b = 0.0\n\n    # Precompute Gram matrix G where G[i, j] = X_i \u00b7 X_j\n    G = X_data @ X_data.T\n\n    # Dual perceptron updates\n    for _ in range(max_iter):\n        any_update = False\n        for i in range(N):\n            # Compute g = sum_j alpha_j * y_j * <x_j, x_i>\n            g = np.dot(alpha * y_data, G[:, i])\n            # Check the margin condition\n            if y_data[i] * (g + b) <= 0:\n                # Perform the update\n                alpha[i] += eta\n                b += eta * y_data[i]\n                any_update = True\n        # If no update in this pass, we've converged\n        if not any_update:\n            break\n\n    # Recover the primal weight vector: w = sum_i alpha_i * y_i * x_i\n    w = np.zeros(d, dtype=float)\n    for i in range(N):\n        w += alpha[i] * y_data[i] * X_data[i]\n\n    # Round w and b to 4 decimal places\n    w = np.round(w, 4)\n    b = round(b, 4)\n\n    return w.tolist(), b\n```"}
{"task_id": 564, "completion_id": 0, "completion": "```python\ndef predict_sample(root:Node features: list | tuple, task_type: str):\n   Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n       # Helper to check for leaf: leaf nodes store y\n    def is_leaf(node):\n        return hasattr(node,y    if task_type == 'classification':\n        # Classification: match feature value to child.x, default to second child\n        node = root\n        while not is_leaf(node):\n            feat_val = features[node.label]\n            # try to find a matching child\n            next_node = None\n            for child in node.child:\n                if child.x == feat_val:\n                    next_node = child\n                    break\n            # if no match, default to second child\n            if next_node is None:\n                next_node = node.child[1]\n            node = next_node\n        return node.y\n\n    else:  # task_type == 'regression'\n        # Regression: compare feature value to split s\n        node = root\n        while not is_leaf(node):\n            feat_val = features[node.label]\n            # go left if <= s, otherwise right\n            if feat_val <= node.s:\n                node = node.child[0]\n            else:\n                node = node.child[1]\n        return node.y\n```"}
{"task_id": 565, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # Compute entropy H(y)\n    # Count label frequencies\n    label_counts = np.bincount(y)\n    probs = label_counts[label_counts > 0] / n_samples\n    H_y = -np.sum(probs * np.log2(probs))\n\n    best_feature = -1\n    best_ig = -1.0\n\n    # For each feature, compute information gain\n    for feature_idx in range(n_features):\n        col = X[:, feature_idx]\n        # Unique values of this feature and their counts\n        values, value_counts = np.unique(col, return_counts=True)\n\n        # Compute conditional entropy H(y | A)\n        H_y_given_A = 0.0\n        for val, count_val in zip(values, value_counts):\n            # Probability of this value\n            p_val = count_val / n_samples\n            # Subset of y where feature == val\n            subset_labels = y[col == val]\n            # Count frequencies in this subset\n            sub_counts = np.bincount(subset_labels)\n            sub_probs = sub_counts[sub_counts > 0] / count_val\n            H_sub = -np.sum(sub_probs * np.log2(sub_probs))\n            H_y_given_A += p_val * H_sub\n\n        ig = H_y - H_y_given_A\n\n        # Track the best feature (tie-breaking on smaller index)\n        if ig > best_ig or (math.isclose(ig, best_ig) and feature_idx < best_feature):\n            best_ig = ig\n            best_feature = feature_idx\n\n    # Round the information gain to 6 decimal places\n    best_ig = round(best_ig, 6)\n\n    return best_feature, best_ig\n```\n\nExplanation of key steps:\n1. Compute the overall label entropy H(y) using `-\u2211 p * log2(p)`.\n2. For each feature column:\n   - Find its unique values and their frequencies.\n   - For each value, compute the conditional entropy of y restricted to the samples with that feature value.\n   - Weight these entropies by the probability of each feature value to get H(y|A).\n3. Information gain for the feature is `IG = H(y) - H(y|A)`.\n4. Keep track of the feature with the highest IG (breaking ties by smaller index).\n5. Return the best feature index and the IG rounded to 6 decimal places."}
{"task_id": 566, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation.\n\n    Args:\n        data: 2-D NumPy array where each row is an observation and each column a feature.\n        n_trees: Number of random isolation trees that will be grown.\n        sample_size: Number of samples drawn (without replacement) to grow every tree.\n        epsilon: Fraction (0 \u2264 \u03b5 < 1) of observations that are considered outliers.\n\n    Returns:\n        A list with the zero-based indices of the detected outliers, sorted increasingly.\n    \"\"\"\n    # reproducibility\n    np.random.seed(42)\n\n    # trivial cases\n    n_samples, n_features = data.shape\n    if epsilon <= 0.0 or n_samples <= 1:\n        return []\n\n    # effective sample size for each tree\n    sample_size_eff = min(sample_size, n_samples)\n    # height limit for trees\n    height_limit = math.ceil(math.log2(sample_size_eff)) if sample_size_eff > 0 else 0\n\n    # Euler-Mascheroni constant\n    _gamma = 0.5772156649\n\n    # expected path length of unsuccessful search in BST, used to correct leaf path\n    def c(n: int) -> float:\n        if n <= 1:\n            return 0.0\n        return 2.0 * math.log(n - 1) + _gamma - 2.0 * (n - 1) / n\n\n    # build one isolation tree on the array X (rows = samples)\n    def build_tree(X: np.ndarray, current_depth: int = 0) -> dict:\n        n = X.shape[0]\n        # stop criteria: depth limit or one-or-zero samples\n        if current_depth >= height_limit or n <= 1:\n            # leaf node\n            return {'size': n}\n        # choose a random feature\n        f = np.random.randint(0, n_features)\n        col = X[:, f]\n        min_val, max_val = col.min(), col.max()\n        # if no spread, make leaf\n        if min_val == max_val:\n            return {'size': n}\n        # random threshold in [min, max)\n        thresh = np.random.uniform(min_val, max_val)\n        # partition\n        left_mask = col < thresh\n        X_left = X[left_mask]\n        X_right = X[~left_mask]\n        # recursively build children\n        left = build_tree(X_left, current_depth + 1)\n        right = build_tree(X_right, current_depth + 1)\n        return {'feature': f, 'threshold': thresh, 'left': left, 'right': right}\n\n    # compute the path length of point x in a given tree\n    def path_length(x: np.ndarray, node: dict, current_depth: int = 0) -> float:\n        # if leaf\n        if 'size' in node:\n            return current_depth + c(node['size'])\n        # internal node\n        f = node['feature']\n        if x[f] < node['threshold']:\n            return path_length(x, node['left'], current_depth + 1)\n        else:\n            return path_length(x, node['right'], current_depth + 1)\n\n    # grow forest\n    trees = []\n    for _ in range(n_trees):\n        # sample without replacement\n        if sample_size_eff < n_samples:\n            idx = np.random.choice(n_samples, size=sample_size_eff, replace=False)\n            X_sub = data[idx]\n        else:\n            X_sub = data.copy()\n        tree = build_tree(X_sub, current_depth=0)\n        trees.append(tree)\n\n    # accumulate path lengths for every observation\n    path_sums = np.zeros(n_samples, dtype=float)\n    for t in trees:\n        for i in range(n_samples):\n            path_sums[i] += path_length(data[i], t, 0)\n\n    # average path length\n    h_bar = path_sums / n_trees\n\n    # normalization constant phi using effective sample size\n    if sample_size_eff > 1:\n        phi = 2.0 * math.log(sample_size_eff - 1) - 2.0 * (sample_size_eff - 1) / sample_size_eff\n    else:\n        # degenerate case: no variation\n        return []\n\n    # anomaly scores\n    scores = 2.0 ** ( - h_bar / phi )\n\n    # select outliers: those with scores among the largest epsilon fraction\n    # compute the (1 - epsilon) quantile\n    thresh = np.quantile(scores, 1.0 - epsilon)\n    outlier_idx = np.where(scores >= thresh)[0]\n\n    return sorted(outlier_idx.tolist())\n```"}
{"task_id": 568, "completion_id": 0, "completion": "Here is a working GIS\u2010based MaxEnt classifier. It only uses Python\u2019s standard library plus numpy and math. The main steps are:\n\n1. Build a sparse feature set F = { (i, v, y) } only for combinations seen in the training data.  \n2. Compute empirical expectations E_emp[j] = (# times feature j occurs in (x_n,y_n)) / N.  \n3. Iteratively update weights w_j by  \n   \u03b4_j = (1/C) \u00b7 ln( E_emp[j] / E_model[j] ),  \n   where  \n     E_model[j] = (1/N) \u03a3_n \u03a3_y P(y|x_n) f_j(x_n,y),  \n     C = max number of features active for any (x,y).  \n4. Stop when max|\u03b4_j| < \u03b5 or we hit the iteration limit.  \n5. For each test x, predict the y with highest \u03a3_{j active} w_j.\n\n```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test:  np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"\n    Maximum entropy classifier with Generalized Iterative Scaling (GIS).\n    Returns a list of predicted labels for X_test.\n    \"\"\"\n    # 1) Basic shapes & uniques\n    N, d = X_train.shape\n    labels = np.unique(y_train)\n    L = len(labels)\n    \n    # 2) Build only those features (i, value, y) that occur in training\n    feat_count = defaultdict(int)\n    for x, y in zip(X_train, y_train):\n        for i, v in enumerate(x):\n            feat_count[(i, v, y)] += 1\n    \n    # Map each observed feature to an index j\n    j2feat = list(feat_count.keys())\n    feat2j = {f: j for j, f in enumerate(j2feat)}\n    M = len(j2feat)\n    \n    # 3) Empirical expectations E_emp[j]\n    E_emp = np.zeros(M, dtype=float)\n    for j, f in enumerate(j2feat):\n        E_emp[j] = feat_count[f] / N\n    \n    # 4) Prepare for GIS updates\n    w = np.zeros(M, dtype=float)\n    # C = maximum sum_j f_j(x,y) over j for any (x,y).  Here each (x,y) can only\n    # activate at most one feature per attribute i, so C = d\n    C = d\n    \n    # Helper: label -> its index\n    label2idx = {y: idx for idx, y in enumerate(labels)}\n    \n    # 5) GIS main loop\n    for iteration in range(n_iter):\n        # Model expectations\n        E_model = np.zeros(M, dtype=float)\n        \n        # For every training sample\n        for x in X_train:\n            # compute unnormalized scores for each label\n            scores = np.zeros(L, dtype=float)\n            for idx_y, y in enumerate(labels):\n                s = 0.0\n                # sum weights of features active under (x,y)\n                # i.e. for each attribute i, see if (i, x[i], y) in feat2j\n                for i, v in enumerate(x):\n                    key = (i, v, y)\n                    j = feat2j.get(key)\n                    if j is not None:\n                        s += w[j]\n                # store exp(score)\n                scores[idx_y] = math.exp(s)\n            \n            Z = scores.sum()\n            # normalized P(y|x)\n            probs = scores / Z\n            \n            # accumulate model expectations\n            for idx_y, y in enumerate(labels):\n                p = probs[idx_y]\n                for i, v in enumerate(x):\n                    j = feat2j.get((i, v, y))\n                    if j is not None:\n                        E_model[j] += p / N\n        \n        # 6) Weight updates\n        delta = np.zeros(M, dtype=float)\n        for j in range(M):\n            # GIS update: \u03b4_j = (1/C) * ln( E_emp[j] / E_model[j] )\n            # both E_emp[j], E_model[j] > 0 by construction\n            delta[j] = (1.0 / C) * math.log(E_emp[j] / E_model[j])\n        \n        # check convergence\n        max_update = np.max(np.abs(delta))\n        w += delta\n        if max_update < epsilon:\n            # print(f\"Converged at iter {iteration}, max_delta={max_update}\")\n            break\n    \n    # 7) Predict on X_test\n    preds = []\n    for x in X_test:\n        best_score = -1e300\n        best_label = None\n        # we compare \"unnormalized\" scores since Z(x) is common across y\n        for y in labels:\n            s = 0.0\n            for i, v in enumerate(x):\n                j = feat2j.get((i, v, y))\n                if j is not None:\n                    s += w[j]\n            if s > best_score:\n                best_score = s\n                best_label = y\n        preds.append(int(best_label))\n    \n    return preds\n```\n\nExplanation of the key parts:  \n- We only build features `(i, value, y)` that actually appear in the training set (so their empirical expectation is non\u2010zero).  \n- `E_emp[j]` is simply the fraction of times feature j fired with the true label.  \n- In each GIS iteration we compute `E_model[j]` by going over each training instance, computing `P(y|x)` via the current weights, and accumulating `P(y|x)\u00b7f_j(x,y)`.  \n- We then update all weights by \u03b4 = (1/C)\u00b7ln(E_emp/E_model).  \n- Finally, for test points we pick the label y maximizing the sum of relevant w_j\u2019s."}
{"task_id": 569, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio is rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                  ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of (word, ratio) pairs and (2) a set of the most\n        frequent words as described above.\n    \"\"\"\n    total_docs = len(data)\n    # If there are no documents, return empty results\n    if total_docs == 0:\n        return [], set()\n\n    # Count the number of documents each word appears in\n    doc_counter: Counter[str] = Counter()\n    for _, words in data:\n        unique_words = set(words)\n        doc_counter.update(unique_words)\n\n    # Build the list of (word, ratio) rounded to 4 decimal places\n    freq_list: List[Tuple[str, float]] = []\n    for word, doc_count in doc_counter.items():\n        ratio = round(doc_count / total_docs, 4)\n        freq_list.append((word, ratio))\n\n    # Sort primarily by ratio descending; ties in ratio can be in any order\n    freq_list.sort(key=lambda x: x[1], reverse=True)\n\n    # Determine the set of top-k words\n    if cut_off is None:\n        top_words = set(doc_counter.keys())\n    elif cut_off == 0:\n        top_words = set()\n    else:\n        # Take the first `cut_off` entries (or fewer if not enough words)\n        top_k = freq_list[:cut_off]\n        top_words = {word for word, _ in top_k}\n\n    return freq_list, top_words\n```\n\nExplanation of key steps:\n\n1. **Handle empty input**: If `data` is empty, there are no documents to process, so we immediately return an empty list and an empty set.\n2. **Count per\u2010document appearances**: For each document we convert its word list into a `set` to ensure each word is counted only once per document, then update a `Counter`.\n3. **Compute ratios**: Divide each word\u2019s document count by the total number of documents and round to 4 decimal places.\n4. **Sort by frequency**: We sort descending by ratio. Words with the same ratio may appear in any order relative to each other.\n5. **Select top\u2010k words**: Depending on `cut_off`, we either take all words, none, or the first `cut_off` words from the sorted list."}
{"task_id": 571, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    # Compute elementwise product \u03b1_i * y_i\n    ay = alpha * y\n\n    # Compute weight vector w = X^T (\u03b1 \u2299 y)\n    w = X.T @ ay  # shape: (n_features,)\n\n    # Compute bias b = (1/n) * sum_i [ y_i \u2212 w^T x_i ]\n    residuals = y - (X @ w)\n    b = np.mean(residuals)\n\n    # Round to 4 decimal places\n    w_rounded = np.round(w, 4)\n    b_rounded = round(float(b), 4)\n\n    # Convert w to a Python list\n    w_list = w_rounded.tolist()\n\n    return w_list, b_rounded\n```\n\nExplanation of key steps:\n1. We form the elementwise product `ay = alpha * y`.\n2. We compute the weight vector `w = X^T @ (alpha \u2299 y)`.\n3. We compute the bias term `b` as the average of `(y_i \u2212 w^T x_i)` over all samples.\n4. Finally, we round each component of `w` and the scalar `b` to four decimal places and return them."}
{"task_id": 572, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n       Compute the Local Outlier Factor (LOF) for each sample in `data`\n    and return the LOF scores along with the indices of samples\n    whose LOF score is strictly greater than `epsilon`.\n    \n    Parameters\n    ----------\n    data : array-like, shape (n_samples, n_features)\n        The input data.\n    k : int\n        Number of neighbors to use for LOF. Must satisfy 1 \u2264 k < n_samples.\n    epsilon : float, optional (default=1.5)\n        Threshold for declaring an outlier.\n    \n    Returns\n    -------\n    (lof_scores, outlier_indices) : tuple\n        lof_scores : list of float, length n_samples\n            The LOF score for each sample, rounded to 4 decimals.\n        outlier_indices : list of int\n            Sorted indices of samples whose LOF score > epsilon.\n       X = np.asarray(data, dtype=float)\n    n_samples = X.shape[0]\n    \n    # 1. Pairwise Euclidean distance matrix\n    #    Use the identity: ||x - y||^2 = ||x||^2 + ||y||^2 - 2\u27e8x,y\u27e9\n    sum_sq = np.sum(X**2, axis=1)\n    dist_sq = sum_sq[:, None] + sum_sq[None, :] - 2 * X.dot(X.T)\n    dist_sq = np.maximum(dist_sq, 0.0)   # numerical stability\n    distances = np.sqrt(dist_sq)\n\n    # Exclude self\u2010distance by setting diagonal to infinity\n    np.fill_diagonal(distances, np.inf)\n\n    # 2. For each point, find its k nearest neighbors\n    #    neighbor_indices[i] is an array of length k\n    neighbor_indices = np.argsort(distances, axis=1)[:, :k]\n\n    # k\u2010distance of each point = distance to its k-th nearest neighbor\n    # The k-th nearest neighbor for point i is at position k-1 in the sorted list\n    idx = np.arange(n_samples)\n    k_distances = distances[idx, neighbor_indices[:, k-1]]\n\n    # 3. Compute reachability distance matrix for each point\u2010neighbor pair\n    #    reach_dist[i, j] = max(k-distance(neighbor_j), dist(i, neighbor_j))\n    #    First, get dist(i, neighbor_j) as dist_pq\n    dist_pq = distances[idx[:, None], neighbor_indices]\n    #    Next, get k-distance(neighbor_j) via advanced indexing\n    kdist_of_neighbors = k_distances[neighbor_indices]\n    reach_dist = np.maximum(kdist_of_neighbors, dist_pq)\n\n    # 4. Local Reachability Density (LRD) of each point\n    #    LRD[i] = k / sum_j reach_dist(i, neighbor_j)\n    lrd = k / np.sum(reach_dist, axis=1)\n\n    # 5. Local Outlier Factor (LOF) of each point\n    #    LOF[i] = (sum_j LRD(neighbor_j) / k) / LRD[i]\n    lrd_neighbors_sum = np.sum(lrd[neighbor_indices], axis=1)\n    lof = (lrd_neighbors_sum / k) / lrd\n\n    # Round LOF scores to 4 decimal places\n    lof_rounded = np.round(lof, 4)\n\n    # Find indices of outliers: LOF > epsilon\n    outliers = [i for i, score in enumerate(lof_rounded) if score > epsilon]\n\n    return lof_rounded.tolist(), outliers\n```\n\nExplanation of the main steps:\n\n1. **Distance matrix**: We compute the full \\(n\\times n\\) Euclidean distance matrix efficiently via vectorized linear algebra.\n2. **k-nearest neighbours**: By setting self-distances to +\u221e, we can call `argsort` on each row to pick the closest \\(k\\) other points.\n3. **Reachability distance**: For each point \\(p\\) and its neighbour \\(q\\),  \n   reach-dist\\(_k(p,q)=\\max\\bigl(k\\)-distance\\((q),\\,dist(p,q)\\bigr)\\).\n4. **Local Reachability Density** \\( \\mathrm{LRD}_k(p)\\):  \n   \\( \\displaystyle \\frac{k}{\\sum_{q\\in N_k(p)} \\mathrm{reach\\_dist}_k(p,q)} \\).\n5. **Local Outlier Factor** \\( \\mathrm{LOF}_k(p)\\):  \n   \\( \\displaystyle \\frac{\\tfrac{1}{k}\\sum_{q\\in N_k(p)} \\mathrm{LRD}_k(q)}{\\mathrm{LRD}_k(p)} \\).\n\nFinally we round each LOF to 4 decimals and collect indices of points whose LOF exceeds `epsilon`."}
{"task_id": 574, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    X_train_list = []\n    y_train_list = []\n    X_val_list = []\n    y_val_list = []\n\n    for c in range(m):\n        base = 10 * (c // 2)\n        low, high = base + 1, base + 9\n\n        # generate training points for class c\n        for _ in range(n_train):\n            x = round(random.uniform(low, high), 4)\n            y = round(random.uniform(low, high), 4)\n            X_train_list.append([x, y])\n            y_train_list.append(c)\n\n        # generate validation points for class c\n        for _ in range(n_val):\n            x = round(random.uniform(low, high), 4)\n            y = round(random.uniform(low, high), 4)\n            X_val_list.append([x, y])\n            y_val_list.append(c)\n\n    X_train = np.array(X_train_list, dtype=float)\n    X_val   = np.array(X_val_list,   dtype=float)\n    y_train = np.array(y_train_list, dtype=int)\n    y_val   = np.array(y_val_list,   dtype=int)\n\n    return X_train, X_val, y_train, y_val\n```"}
{"task_id": 579, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres.  Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached.  The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n    \"\"\"\n    m, d = X.shape\n    # Initialize centres as the first k points (make a float copy for means)\n    centres = X[:k].astype(float).copy()\n    prev_labels = None\n\n    for _ in range(max_iters):\n        # ASSIGNMENT STEP\n        # Compute squared Euclidean distances (m x k)\n        # using broadcasting: X[:, None, :] - centres[None, :, :]\n        diff = X[:, None, :] - centres[None, :, :]\n        dists = np.linalg.norm(diff, axis=2)  # shape (m, k)\n        labels = np.argmin(dists, axis=1)     # picks smallest index on ties\n\n        # Check for convergence\n        if prev_labels is not None and np.array_equal(labels, prev_labels):\n            break\n        prev_labels = labels.copy()\n\n        # UPDATE STEP\n        for j in range(k):\n            assigned_pts = X[labels == j]\n            if assigned_pts.size > 0:\n                centres[j] = assigned_pts.mean(axis=0)\n            # if no points assigned, keep centre[j] unchanged\n\n    # Round centres to 4 decimal places\n    centres_rounded = np.round(centres, 4)\n\n    # Convert outputs to lists\n    return labels.tolist(), centres_rounded.tolist()\n```"}
{"task_id": 581, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n   Fits a linear model with Elastic-Net regularisation.\n\n    Your task is to complete this function so that it performs batch gradient\n    descent for the given number of iterations and returns the learned weight\n    vector and bias.  All returned values must be rounded to 4 decimal places.\n\n    Args:\n        X: A 2-D NumPy array of shape (m, n) containing the input features.\n        y: A 1-D NumPy array of length m containing the target values.\n        learning_rate: Step size for gradient descent (\u03b1).\n        iterations: Number of optimisation steps to perform.\n        l1_penalty: L1 regularisation strength (\u03bb\u2081).\n        l2_penalty: L2 regularisation strength (\u03bb\u2082).\n\n    Returns:\n        Tuple (weights, bias) where weights is a list of length n and bias is a\n        float.  Every number must be rounded to four decimal places.\n       m, n = X.shape\n\n    # If no training will happen, return zeros immediately\n    if learning_rate == 0 or iterations == 0:\n        return [0.0] * n, 0.0\n\n    # Initialize parameters\n    w = np.zeros(n, dtype=float)\n    b = 0.0\n\n    for _ in range(iterations):\n        # 1) Predictions\n        y_pred = X.dot(w) + b\n\n        # 2) Residuals\n        r = y - y_pred  # shape (m,)\n\n        # 3) Compute sign(w) with sign(0) = -1\n        #    np.where(w > 0, 1, -1) -> gives 1 if w>0, else -1 (so w=0 => -1)\n        sign_w = np.where(w > 0, 1.0, -1.0)\n\n        # 4) Gradients\n        #    \u2202L/\u2202w = -2 * X^T \u00b7 r + \u03bb1 * sign(w) + 2 * \u03bb2 * w\n        grad_w = (-2.0 * X.T.dot(r) + l1_penalty * sign_w + 2.0 * l2_penalty * w) / m\n\n        #    \u2202L/\u2202b = -2 * sum(r)\n        grad_b = (-2.0 * np.sum(r)) / m\n\n        # 5) Parameter updates\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n    # Round the results to 4 decimal places\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(round(b, 4))\n\n    return w_rounded, b_rounded\n```"}
{"task_id": 591, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Perform a forward pass through a fully-connected ReLU neural network.\n\n    The network topology and parameters are encoded in *weights* where:\n      \u2022 *weights[L]* \u2013 weight matrix of layer L,\n      \u2022 each *weights[L][i]* \u2013 list of weights of neuron i of layer L,\n      \u2022 the first weight of every neuron is its bias weight.\n\n    All hidden layers use ReLU, the last layer is linear.\n\n    Args:\n        weights: A three-level nested list containing the network\u2019s weights.\n        x:       Input feature vector (without the bias term).\n\n    Returns:\n        The prediction rounded to four decimals (float if the output layer has\n        one neuron, otherwise list of floats).\n    \"\"\"\n    # Convert the input to a NumPy array for vectorized computations\n    activation = np.array(x, dtype=float)\n\n    # Forward pass through each layer\n    n_layers = len(weights)\n    for L, layer in enumerate(weights):\n        # Stack the weight lists into a 2D NumPy array of shape (n_neurons, prev_layer_size + 1)\n        W = np.array(layer, dtype=float)\n\n        # Prepend bias input = 1.0 to the current activation vector\n        with_bias = np.concatenate(([1.0], activation))\n\n        # Compute the layer output\n        z = W @ with_bias  # shape = (n_neurons,)\n\n        # Apply ReLU on all but the last layer\n        if L < n_layers - 1:\n            activation = np.maximum(z, 0.0)\n        else:\n            activation = z\n\n    # Round the final outputs to 4 decimals\n    activation_rounded = np.round(activation, 4)\n\n    # Return a float if there's exactly one output, else a list of floats\n    if activation_rounded.size == 1:\n        return float(activation_rounded[0])\n    else:\n        return activation_rounded.tolist()\n```\n\nExplanation of key steps:\n1. We keep `activation` as a NumPy array representing the current layer\u2019s outputs (starting with the input `x`).\n2. For each layer:\n   - We stack its neurons\u2019 weight lists into a 2D array `W` of shape `(n_neurons, prev_size + 1)`.\n   - We prepend the bias input `1.0` to the activation vector.\n   - We compute the pre\u2010activation `z = W @ [1.0, activation]`.\n   - We apply ReLU (`max(z, 0)`) if it\u2019s not the final layer.\n3. After the last layer, we simply take the linear output.\n4. We round every output to four decimal places and return either a single float or a list of floats, per the specification."}
{"task_id": 595, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans_centroids(X:np.ndarray k: int, max_iters: int = 100) -> list[list[float]]:\n       Perform K-Means clustering on X and return the centroids only.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n_samples, n_features)\n        The data to cluster.\n    k : int\n        The number of clusters.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations.\n\n    Returns\n    -------\n    centroids : list of list of float\n        The final centroids, sorted lexicographically and rounded to 4 decimals.\n       # Initialize centroids to the first k points\n    centroids = X[:k].astype(float, copy=True)  # shape (k, n_features)\n\n    for _ in range(max_iters):\n        # 1) Assign each sample to the nearest centroid (squared Euclidean)\n        # distances: shape (n_samples, k)\n        # broadcasting: X[:, None, :] - centroids[None, :, :]\n        diffs = X[:, None, :] - centroids[None, :, :]  # (n_samples, k, n_features)\n        dists = np.sum(diffs**2, axis=2)               # (n_samples, k)\n        labels = np.argmin(dists, axis=1)              # (n_samples,)\n\n        # 2) Update centroids\n        new_centroids = np.zeros_like(centroids)\n        for i in range(k):\n            members = (labels == i)\n            if np.any(members):\n                # mean over all members\n                new_centroids[i] = X[members].mean(axis=0)\n            else:\n                # empty cluster: keep old centroid\n                new_centroids[i] = centroids[i]\n\n        # 3) Check for convergence (max centroid movement < 1e-4)\n        shifts = np.linalg.norm(new_centroids - centroids, axis=1)\n        if np.max(shifts) < 1e-4:\n            centroids = new_centroids\n            break\n\n        centroids = new_centroids\n\n    # 4) Round to 4 decimals and convert to Python lists\n    rounded = [list(np.round(c, 4)) for c in centroids]\n\n    # 5) Sort lexicographically\n    rounded.sort()\n    return rounded\n```\n\nExplanation of key steps:\n\n1. **Initialization**: Centroids are set to the first `k` rows of `X` (deterministic).\n2. **Assignment**: Compute squared Euclidean distances from each point to each centroid, then assign to the nearest.\n3. **Update**: For each cluster, compute the mean of assigned points; if no points are assigned, keep the old centroid.\n4. **Convergence check**: Stop early if the largest centroid shift is below `1e-4`.\n5. **Post-processing**: Round each centroid coordinate to 4 decimals, convert to nested Python lists, then sort lexicographically before returning."}
