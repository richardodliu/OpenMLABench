{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a) == 0 or len(b) == 0:\n        return -1\n    num_cols = len(a[0])\n    if num_cols != len(b):\n        return -1\n    result = []\n    for row in a:\n        row_product = sum((row[i] * b[i] for i in range(num_cols)))\n        result.append(row_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    rows = len(a)\n    cols = len(a[0]) if rows > 0 else 1\n    result = [[0] * cols for _ in range(rows)]\n    for i in range(rows):\n        for j in range(cols):\n            result[i][j] = a[j][i]\n    return result"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    if not isinstance(a, list) or not all((isinstance(row, list) for row in a)):\n        return []\n    if not isinstance(new_shape, tuple) or len(new_shape) != 2:\n        return []\n    if new_shape[0] == 0 or new_shape[1] == 0:\n        return []\n    try:\n        reshaped = np.array(a).reshape(new_shape)\n    except Exception as e:\n        return []\n    return reshaped.tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column, based on a given mode.\n\n    Args:\n        matrix: A list of lists representing the matrix.\n        mode: 'row' or 'column', indicating whether to compute the mean per row or per column.\n\n    Returns:\n        A list containing the computed means for rows (if 'row') or columns (if 'column').\n    \"\"\"\n    if not isinstance(matrix, list):\n        raise TypeError('Input must be a list of lists.')\n    if len(matrix) == 0:\n        return []\n    if mode == 'row':\n        row_means = [sum(row) / len(row) for row in matrix]\n        return row_means\n    elif mode == 'column':\n        column_means = []\n        for col_index in range(len(matrix[0])):\n            total = sum((matrix[row][col_index] for row in range(len(matrix))))\n            column_means.append(total / len(matrix))\n        return column_means\n    else:\n        raise ValueError(f\"Invalid mode: {mode}. Please use 'row' or 'column'.\")\nmatrix = [[1, 2, 3], [4, 5, 6]]\nmode = 'row'"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"Multiply each element in the matrix by a scalar.\"\"\"\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n    \n    Args:\n        matrix: A list of lists containing a 2x2 matrix.\n        \n    Returns:\n        A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2:\n        raise ValueError('Input matrix must be 2x2.')\n    (a, b) = (matrix[0][0], matrix[0][1])\n    (c, d) = (matrix[1][0], matrix[1][1])\n    det = a * d - b * c\n    trace = a + d\n    eigenvalues = [(trace + det ** 0.5) / 2, (trace - det ** 0.5) / 2]\n    eigenvalues.sort(reverse=True)\n    return eigenvalues"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        det_T = np.linalg.det(T)\n        det_S = np.linalg.det(S)\n        if not (np.isclose(det_T, 0) and np.isclose(det_S, 0)):\n            return -1\n        result = np.dot(T, np.dot(S, A))\n        rounded_result = np.round(result, 4)\n        return rounded_result.reshape(A[0].shape)\n    except Exception as e:\n        print(f'Error in transformation: {e}')\n        return -1"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    adj = [[matrix[1][1], -matrix[0][1]], [-matrix[1][0], matrix[0][0]]]\n    inv_matrix = [[adj[0][0] / det, adj[0][1] / det], [adj[1][0] / det, adj[1][1] / det]]\n    return inv_matrix"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b[0]):\n        return [-1]\n    result = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            sum_val = 0\n            for k in range(len(b)):\n                sum_val += a[i][k] * b[k][j]\n            row.append(sum_val)\n        result.append(row)\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n        vectors: A list of lists, where each inner list represents a feature with its observations.\n\n    Returns:\n        A list of lists representing the covariance matrix.\n    \"\"\"\n    if not vectors or not vectors[0]:\n        return []\n    n = len(vectors[0])\n    cov_matrix = [[0.0 for _ in range(n)] for _ in range(n)]\n    for i in range(n):\n        mean_i = sum((vectors[i][j] for j in range(n))) / n\n        for j in range(n):\n            cov_matrix[i][j] = sum(vectors[i][j] - mean_i) ** 2 / n\n    return cov_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve a system of linear equations using the Jacobi method.\n    \n    Args:\n        A: The coefficient matrix of the system (n x m).\n        b: The constant vector of the system (m x 1).\n        n: The number of iterations.\n        \n    Returns:\n        A list containing the approximate solution x.\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square for Jacobi method.')\n    if A.shape[1] != b.shape[0]:\n        raise ValueError('A must have same number of columns as b')\n    x = np.zeros(A.shape[1])\n    for _ in range(n):\n        for i in range(A.shape[1]):\n            x[i] = (b[i] * A[i, 0] + A[i, 1] * x[i]) / A[i, 1]\n        x = round(x, 4)\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion method.\n\n    Args:\n        matrix: A list of lists representing a 4x4 matrix, where each element is an integer or float.\n\n    Returns:\n        float: The determinant of the matrix.\n    \"\"\"\n    if len(matrix) == 3:\n        return determinant_3x3(matrix)\n\n    def laplacian(matrix):\n        n = len(matrix)\n        det = 0.0\n        for i in range(n):\n            sign = (-1) ** (i + 1)\n            minor = [[matrix[j][k] for j in range(n)] for k in range(n - 1)]\n            det += sign * matrix[i][k] * laplacian(minor)\n        return det\n    return laplacian(matrix)"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    coefficients = X.T @ X @ (y.T / np.linalg.norm(X.T) ** 2)\n    rounded_coefficients = round(coefficients, 4)\n    return rounded_coefficients"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    X = X.reshape(-1, 1)\n    y = y.reshape(-1, 1)\n    b0 = X @ np.linalg.inv(X.T @ X) @ X\n    for _ in range(iterations):\n        X_grad = X.T @ X @ np.linalg.inv(X.T @ X) - X.T @ b0 * X.T @ X\n        b0 -= alpha * X_grad\n    return b0.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform feature scaling using standardization and min-max normalization on a 2D dataset.\n    \n    Args:\n        data: A 2D NumPy array representing the dataset where each row is a sample and each column is a feature.\n        \n    Returns:\n        A tuple of two lists:\n            - The first list is scaled by standardization.\n            - The second list is scaled by min-max normalization.\n    \"\"\"\n    scaled_standardized = np.std(data, axis=0) / np.sqrt(np.var(data, axis=0))\n    scaled_standardized = np.round(scaled_standardized, 4).tolist()\n    scaled_min_max = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    scaled_min_max = np.round(scaled_min_max, 4).tolist()\n    return (scaled_standardized, scaled_min_max)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    X = np.array(points)\n    centroids = np.array(initial_centroids)\n    if len(centroids) == 0:\n        return []\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(X - centroids, axis=1)\n        assigned_points = np.argmin(distances, axis=0)\n        new_centroids = np.mean(X[assigned_points], axis=0)\n        rounded_centroids = round(new_centroids, 4)\n        if np.allclose(centroids, rounded_centroids, atol=1e-06):\n            break\n        centroids = rounded_centroids\n    return [round(c, 4) for c in centroids]"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n        X (np.ndarray): The input dataset with shape (n_samples, n_features)\n        y (np.ndarray): The target variable with shape (n_samples, 1)\n        k (int): Number of folds (default: 5)\n        shuffle (bool): Whether to shuffle the data (default: True)\n        random_seed (int): Random seed for reproducibility (optional)\n\n    Returns:\n        list: A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError('X and y must be numpy arrays')\n    if X.shape[1] != len(y):\n        raise ValueError('X should have the same number of columns as y')\n    np.random.seed(random_seed) if random_seed else None\n    folds = []\n    n_samples = X.shape[0]\n    n_splits = k\n    if n_samples == 0:\n        return []\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    group_size = n_samples // k\n    remaining = n_samples % k\n    for i in range(k):\n        start_index = i * group_size\n        end_index = start_index + group_size if remaining > 0 else start_index + group_size - 1\n        current_folds = [indices[start_index:end_index], indices[end_index:]]\n        folds.append(current_folds)\n    return folds"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) from scratch.\n\n    Parameters:\n        data (np.ndarray): A 2D array where each row is a data sample and each column represents a feature.\n        k (int): The number of principal components to return.\n\n    Returns:\n        list[list[float]]: The principal components in the order of the largest eigenvalues (eigenvectors).\n    \"\"\"\n    if len(data.shape) != 2 or len(data.shape[1]) < k:\n        raise ValueError(\"Input data must have at least 'k' features.\")\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(cov_matrix)\n    sorted_eigenvalues = np.sort(eigenvalues, reverse=True)\n    sorted_eigenvectors = eigenvectors[:k]\n    rounded_eigenvectors = [round(e, 4) for e in sorted_eigenvectors]\n    return rounded_eigenvectors"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom itertools import combinations\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    train_set = []\n    test_set = []\n    for example in examples:\n        if 'test' in example:\n            test_set.append(example)\n        else:\n            train_set.append(example)\n    if not train_set or not test_set:\n        return {}\n    num_attributes = len(attributes)\n    unique_attributes = set(attributes)\n    info_gain = 0\n    best_attribute = None\n    for attr in unique_attributes:\n        entropy = -sum((math.log2(count / len(train_set)) for count in Counter(train_set[attr]).values()))\n        info_gain = sum((entropy * (1 - count / len(train_set)) for count in Counter(train_set[attr]).values()))\n        if info_gain > info_gain:\n            best_attribute = attr\n\n    def build_tree(attribute, current_depth):\n        if not train_set:\n            return {}\n        if attribute == target_attr:\n            return {'value': train_set[0][attribute]}\n        splits = combinations([i for i in range(len(attributes)) if i != best_attribute], 2)\n        for split_idx in splits:\n            left_count = 0\n            right_count = 0\n            for example in train_set:\n                if example[split_idx[0]] == train_set[0][split_idx[0]]:\n                    left_count += 1\n                elif example[split_idx[1]] == train_set[0][split_idx[1]]:\n                    right_count += 1\n            left_child = build_tree(attributes[split_idx[0]], current_depth + 1)\n            right_child = build_tree(attributes[split_idx[1]], current_depth + 1)\n            combined = {'attribute': attributes[split_idx[0]], 'left': left_child, 'right': right_child, 'value': train_set[0][attributes[split_idx[0]]]}\n            return combined\n    result = build_tree(attributes[best_attribute], 0)\n    return result"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier.\n\n    Parameters:\n        data (np.ndarray): A 2D NumPy array where each row is a data sample and each column is a feature.\n        labels (np.ndarray): A 1D NumPy array where each entry corresponds to the label of the sample.\n        kernel (str): 'linear' or 'rbf'\n        lambda_val (float): Regularization parameter for linear kernel\n        iterations (int): Number of iterations\n        sigma (float): Standard deviation for RBF kernel\n\n    Returns:\n        list: A list containing the model's alpha coefficients and bias, rounded to the nearest 4th decimal place.\n    \"\"\"\n    X = np.array(data)\n    y = np.array(labels)\n    alpha = np.zeros(X.shape[1])\n    bias = 0.0\n\n    def cost(theta, y):\n        return np.sum((theta - y) ** 2) / (2 * X.shape[1]) + lambda_val * np.sum(alpha * theta)\n\n    def grad_cost(theta, y):\n        return 2 * (theta - y) / X.shape[1] + lambda_val * alpha\n    for _ in range(iterations):\n        grad = grad_cost(alpha, y)\n        alpha -= grad * sigma\n        bias += grad * sigma\n    result = np.round(np.array([alpha, bias]), 4).tolist()\n    return result"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"Compute the output of the sigmoid activation function.\"\"\"\n    return 1 / (1 + math.exp(-z))"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"Compute the softmax activation for a given list of scores.\n\n    Args:\n        scores: A list of float values representing the input scores.\n\n    Returns:\n        A list of floats representing the softmax values.\n    \"\"\"\n    normalized = [math.exp(score) / sum(math.exp(score)) for score in scores]\n    rounded = [round(value, 4) for value in normalized]\n    return rounded"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    features_array = np.array(features)\n    labels_array = np.array(labels)\n    reshaped_features = np.reshape(features_array, (len(features), len(weights)))\n    predicted_probabilities = 1 / (1 + np.exp(-np.dot(reshaped_features, weights) - bias))\n    mse = np.mean(labels_array * predicted_probabilities - labels_array)\n    predictions = np.round(predicted_probabilities, 4).tolist()\n    loss = np.round(mse, 4)\n    return (predictions, loss)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        predicted = sigmoid(weights @ features + bias)\n        current_mse = np.mean((labels - predicted) ** 2)\n        mse_values.append(current_mse)\n        grad_weights = predicted * (1 - predicted) * np.array([features[0] for features in features])\n        grad_bias = predicted * (1 - predicted) * np.array([1.0])\n        weights -= learning_rate * grad_weights / len(features)\n        bias -= learning_rate * grad_bias\n    return (np.round(weights, 4), np.round(bias, 4), np.round(mse_values, 4).tolist())\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def backward(self, inputs):\n        if self._op == 'add':\n            grad = 1.0\n        elif self._op == 'mul':\n            grad = 1.0\n        elif self._op == 'relu':\n            grad = 1.0 if self.data > 0 else 0.0\n        else:\n            raise ValueError('Unknown operation')\n        self.grad += grad * self.data\n        for prev in self._prev:\n            inputs[prev].grad += grad\n        return self"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_np = np.array(B)\n    C_np = np.array(C)\n    P = np.dot(C_np, B_np.T)\n    P_rounded = P.round(4)\n    result = np.tolist(P_rounded)\n    return result"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    (eigenvalues, eigenvectors) = np.linalg.eig(A)\n    singular_values = np.sqrt(eigenvalues)\n    U = np.random.rand(len(A), len(A))\n    S = singular_values.reshape(-1, len(A))\n    V = np.random.rand(len(A), len(A))\n    return (U, S, V)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    data = np.array([x, y])\n    shuffled_indices = np.random.permutation(np.arange(len(data)))\n    shuffled_X_y = data[shuffled_indices]\n    return shuffled_X_y.tolist()"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generate batches of data from a numpy array X and an optional numpy array y.\n    \n    Args:\n        X: A numpy array containing the input data (can be 2D).\n        y: Optional numpy array representing labels for each row in X. If not provided, use None.\n        batch_size: The size of each batch to generate (default is 64).\n    \n    Returns:\n        A list of tuples where each tuple contains a batch of shape (batch_size, ...), with `y` included if available.\n    \"\"\"\n    batches = []\n    while X.shape[0] > 0:\n        current_batch = X[:batch_size]\n        if y is not None:\n            current_batch_with_y = np.column_stack((current_batch, y))\n        else:\n            current_batch_with_y = current_batch\n        batches.append(current_batch_with_y)\n        X = X[batch_size:]\n    return [batch.tolist() for batch in batches]"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    subset1 = X[X[feature_i] >= threshold]\n    subset2 = X[X[feature_i] < threshold]\n    return (tolist(subset1), tolist(subset2))"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    X = X.reshape(-1, len(X[0])) if not isinstance(X, np.ndarray) else X\n    features = []\n    for i in range(degree + 1):\n        for combo in combinations_with_replacement(X.columns, i + 1):\n            feature = np.array([X[col] for col in combo])\n            features.append(feature)\n    return np.array(features).tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a dataset.\n\n    Parameters:\n    X (np.ndarray): The input 2D numpy array.\n    y (np.ndarray): The input 1D numpy array.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to allow replacement in subsets (default: True).\n    seed (int): Random seed for reproducibility (default: 42).\n\n    Returns:\n    list: A list of tuples containing the subset (X_subset, y_subset) and their reshaped form.\n    \"\"\"\n    if X.shape != y.shape:\n        raise ValueError('X and y must have the same shape.')\n    X_subset = X.reshape((n_subsets, len(X)))\n    y_subset = y.reshape((n_subsets, 1))\n    result = []\n    for i in range(n_subsets):\n        indices = np.random.permutation(np.arange(len(X)))\n        X_subset[i] = X[indices]\n        y_subset[i] = y[indices]\n        result.append((X_subset[i], y_subset[i]))\n    return result"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of nominal values in a 1D array.\n\n    Parameters:\n        x (np.ndarray): A 1D array of integer values.\n        n_col (int, optional): The number of columns for the one-hot encoded array. If not provided, it is determined from the input array.\n\n    Returns:\n        list: A list of reshaped and one-hot encoded arrays.\n    \"\"\"\n    if n_col is None:\n        n_col = len(x)\n    x_list = x.tolist()\n    result = np.array([np.eye(n_col)[x[i]] for i in range(len(x))])\n    return result.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = x.reshape((len(x), 1))\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (list or numpy array): The true labels.\n    y_pred (list or numpy array): The predicted labels.\n\n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal place.\n    \"\"\"\n    y_true_array = np.array(y_true)\n    y_pred_array = np.array(y_pred)\n    correct = np.sum(y_true_array == y_pred_array)\n    total_samples = len(y_true_array)\n    accuracy = correct / total_samples\n    return round(accuracy * 10000) / 10000.0"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Parameters:\n    X (np.ndarray): A 2D numpy array representing the data.\n    Y (np.ndarray, optional): An optional 2D numpy array to compute the correlation matrix with.\n\n    Returns:\n    np.ndarray: The correlation matrix returned as a 2D numpy array.\n    \"\"\"\n    if Y is None:\n        corr_matrix = np.corrcoef(X)\n    else:\n        corr_matrix = np.corrcoef(X, y=Y)\n    return corr_matrix.tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    classifiers = []\n    weights = np.ones(len(y))\n    for i in range(n_clf):\n        threshold = 0.5\n        error = compute_error(X, y, weights)\n        new_weights = weights * (1 - error)\n        classifiers.append((threshold, new_weights))\n    rounded_classifiers = [round(x[0], 4), round(x[1], 4)]\n    return rounded_classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    softmax_scores = np.exp(scores_array) / np.max(np.abs(scores_array))\n    log_softmax_scores = np.log(softmax_scores)\n    reshaped = np.reshape(log_softmax_scores, (len(scores), 1))\n    rounded = np.round(reshaped, 4)\n    return rounded.tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int) -> list:\n    if not input_matrix.ndim == 3 or not input_matrix.shape[0] == 2 or (not input_matrix.shape[1] == 2):\n        raise ValueError('Input matrix must be 2D with shape (h, w, c)')\n    if not kernel.ndim == 3 or not kernel.shape[0] == 2 or (not kernel.shape[1] == 2):\n        raise ValueError('Kernel must be 2D with shape (h, w, c)')\n    input_list = input_matrix.tolist()\n    (h, w, c) = input_matrix.shape\n    padded_input = np.pad(input_list, ((padding, padding), (padding, padding)), mode='constant')\n    output = np.convolve(padded_input, kernel, mode='same')\n    output = output.reshape(input_matrix.shape)\n    rounded_output = round(output, 4)\n    return rounded_output.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    return max(z, 0.0)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Implements the Ridge Regression loss function.\n\n    Args:\n        X: 2D numpy array of shape (n_samples, n_features)\n        w: 1D numpy array of shape (n_features, 1)\n        y_true: 1D numpy array of shape (n_samples, 1)\n        alpha: Regularization parameter for Ridge regression\n\n    Returns:\n        float: Ridge loss value, which combines Mean Squared Error (MSE) and a regularization term.\n    \"\"\"\n    mse = np.mean((y_true - np.dot(X, w)) ** 2)\n    reg_term = alpha * np.sum(np.abs(w))\n    return mse + reg_term"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"Implement the Leaky ReLU activation function.\n\n    Args:\n        z: Input value.\n        alpha: Slope for negative inputs (optional, default 0.01).\n\n    Returns:\n        The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"Computes the linear kernel between two input vectors.\n\n    The linear kernel is defined as the dot product (inner product) of two vectors.\n    \n    Args:\n        x1: First input vector\n        x2: Second input vector\n\n    Returns:\n        The dot product of x1 and x2, representing the linear kernel.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"Calculate the precision metric between two binary labels arrays.\n\n    Args:\n        y_true: array of binary true labels (0 or 1).\n        y_pred: array of binary predicted labels (0 or 1).\n\n    Returns:\n        float: Precision score.\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) | (y_pred == 0))\n    precision = true_positives / (true_positives + false_positives)\n    return precision"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs three variants of gradient descent using MSE loss.\n    \n    Parameters:\n    X : numpy array - input features\n    y : numpy array - target values\n    weights : numpy array - model weights\n    learning_rate : float - learning rate for gradient descent\n    n_iterations : int - number of iterations to perform\n    batch_size : int (optional) - size of the batch used in mini-batch methods\n    method : str (optional) - 'batch' or 'mini_batch' for variant\n    \n    Returns:\n    list: a list containing the trained model weights and the results after reshaping\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    weights = np.array(weights)\n    if method == 'batch':\n        results = []\n        for i in range(n_iterations):\n            grads = np.gradient(weights, X.shape[1])\n            weights += learning_rate * grads\n            results.append([weights.tolist(), i + 1])\n        return results\n    elif method == 'mini_batch':\n        results = []\n        for i in range(n_iterations):\n            grads = np.gradient(weights, X.shape[1], batch_size)\n            weights += learning_rate * grads\n            results.append([weights.tolist(), i + 1])\n        return results\n    else:\n        raise ValueError(\"Invalid method. Please choose either 'batch' or 'mini_batch'.\")"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix)\n    matrix = np.tolist(matrix)\n    (n_rows, n_cols) = matrix.shape\n    for i in range(n_rows):\n        pivot_col = i\n        if not np.any(matrix[i]):\n            break\n        pivot_row = np.where(matrix[i] != 0)[0][0]\n        matrix[[i], pivot_col] = matrix[pivot_row, [i], pivot_col]\n        for j in range(i + 1, n_rows):\n            if matrix[j, pivot_col] != 0:\n                matrix[j, pivot_col] = 0\n    return matrix.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Implements the Adam optimization algorithm for a given objective function.\n\n    Parameters:\n        f: Objective function to optimize (a single value)\n        grad: Gradient of `f` (function that returns the gradient)\n        x0: Initial parameter values\n        learning_rate: Step size (default: 0.001)\n        beta1: Exponential decay rate for first moment (default: 0.9)\n        beta2: Exponential decay rate for second moment (default: 0.999)\n        epsilon: Small constant for numerical stability (default: 1e-8)\n        num_iterations: Number of iterations (default: 10)\n\n    Returns:\n        A list containing the updated parameters in the form [x1, x2, ...]\n    \"\"\"\n    x = np.copy(x0)\n    for _ in range(num_iterations):\n        m_hat_1 = beta1 * x + (1 - beta1) * (np.sum(grad(x)) / len(x))\n        m_hat_2 = beta2 * x + (1 - beta2) * (np.sum(grad(x) ** 2 / len(x)) if len(x) > 1 else 0)\n        x = x * learning_rate - m_hat_1 * learning_rate ** 2\n        x = x * beta1 + (1 - beta1) * (m_hat_1 - x) / (beta1 - 1)\n        x = x * beta2 + (1 - beta2) * (m_hat_2 - x) / (beta2 - 1)\n        if epsilon != 0:\n            x = x / (1 - beta1 - beta2) if beta1 + beta2 < 1 else x\n    return np.round(x).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    w = np.zeros(len(X[0]))\n    b = 0.0\n    for _ in range(max_iter):\n        grad_w = X.T @ (y - (w @ X + b)) * X / len(y)\n        grad_b = (y - (w @ X + b)) / len(y)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n        if np.allclose(w @ X + b, y, atol=tol):\n            break\n    return (round(w, 4).tolist(), round(b, 4).tolist())"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    if m == 0 or n == 0:\n        return 0\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            dp[i][j] = min(dp[i - 1][j], dp[i][j - 1])\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] += 0\n            else:\n                dp[i][j] += 1\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 1]):\n                dp[i][j] += 1\n    return dp[m][n]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n\n    Args:\n        y_true: A list of true binary labels (0 or 1).\n        y_pred: A list of predicted binary labels (0 or 1).\n\n    Returns:\n        The recall metric rounded to three decimal places.\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 0) & (y_pred != 1))\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    return round(tp / denominator, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    X = X.astype(np.float32)\n    qk = np.dot(W_q, X)\n    qk = qk / np.sqrt(len(W_q) * len(W_k))\n    attn_weights = np.softmax(qk, axis=0)\n    output = np.dot(attn_weights, W_v)\n    output = output.reshape(X.shape)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    if not input_sequence:\n        return []\n    num_input_units = len(Wx[0])\n    num_hidden_units = len(Wh[0])\n    output_shape = (len(input_sequence), num_hidden_units)\n    input_tensor = np.reshape(np.array([input_sequence]), input_tensor.shape)\n    output_layer = np.zeros(output_shape)\n    for i in range(len(input_tensor)):\n        current_input = input_tensor[i]\n        output = np.dot(current_input, Wx) + b\n        hidden_state = np.tanh(output)\n        output_layer[i] = hidden_state\n    output_layer = np.reshape(output_layer, output_shape)\n    rounded_output = np.round(output_layer, 4)\n    return rounded_output.tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points)\n    translated_points = np.add(points_array, np.array([tx, ty]))\n    return np.tolist(translated_points)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Args:\n        mu_p: Mean of the first distribution\n        sigma_p: Variance of the first distribution\n        mu_q: Mean of the second distribution\n        sigma_q: Variance of the second distribution\n        \n    Returns:\n        The KL divergence between the two distributions as a floating-point number.\n    \"\"\"\n    numerator = 0.5 * (sigma_p * (mu_p - mu_q) + sigma_q * (mu_q - mu_p))\n    denominator = 2 * np.sqrt(sigma_p ** 2 + sigma_q ** 2)\n    return numerator / denominator"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implements the Gauss-Seidel method for solving a system of linear equations.\n\n    Args:\n        A: Square matrix of coefficients (n x n)\n        b: Right-hand side vector (n elements)\n        n: Number of iterations\n        x_ini: Optional initial guess (list or numpy array) - if not provided, uses zeros.\n\n    Returns:\n        A list of floats representing the approximated solution vector after n iterations.\n    \"\"\"\n    x = x_ini if x_ini is not None else np.zeros_like(b)\n    for _ in range(n):\n        for i in range(n):\n            x[i] = b[i] - A[i][0] * x[0] - A[i][1] * x[1] - ...\n    return x.tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Perform Gaussian Elimination to solve a linear system Ax = b.\n    \n    Args:\n        A: A square matrix (n x n)\n        b: A vector of size n\n    \n    Returns:\n        A list of floats representing the solution vector x.\n    \"\"\"\n    if not isinstance(A, np.ndarray) or len(A.shape) != 2:\n        raise ValueError('Input A must be a 2D numpy array.')\n    n = A.shape[0]\n    m = b.shape[0]\n    A = np.array(A)\n    b = np.array(b)\n    for i in range(n):\n        if A[i, i] == 0:\n            if m > 1:\n                raise ValueError(f'Row {i} has all zero elements and there are {m} non-zero elements.')\n    A[:, :] = A[:, :].copy()\n    for j in range(n - 1):\n        if A[j, j] == 0:\n            idx = j\n            break\n        else:\n            A[[j, idx], :] = A[[idx, j], :]\n    x = [0.0] * n\n    for i in range(n):\n        x[i] = b[i] / A[i, i]\n        for j in range(i + 1, n):\n            x[i] -= A[j, i] * x[j]\n    return x.tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states at each time step, as well as the final hidden state and cell state.\n        \"\"\"\n        output = []\n        h_prev = initial_hidden_state.copy()\n        c_prev = initial_cell_state.copy()\n        for i in range(len(x)):\n            z_f = np.dot(self.Wf, np.hstack([h_prev, c_prev]))\n            a_f = np.tanh(z_f + self.bf)\n            z_i = np.dot(self.Wi, np.hstack([h_prev, c_prev]))\n            a_i = np.tanh(z_i + self.bi)\n            z_c = np.dot(self.Wc, np.hstack([h_prev, c_prev]))\n            a_c = np.tanh(z_c + self.bc)\n            z_o = np.dot(self.Wo, np.hstack([h_prev, c_prev]))\n            a_o = np.tanh(z_o + self.bo)\n            h_new = a_i * (a_f + a_c) / 2\n            c_new = a_c * (a_f + a_o) / 2\n            output.append([h_new, c_new])\n            (h_prev, c_prev) = (h_new, c_new)\n        hidden_states = [state[0] for state in output]\n        final_hidden = np.array(hidden_states).reshape(-1, self.hidden_size)\n        final_cell = np.array([state[1] for state in output]).reshape(-1, self.hidden_size)\n        return (hidden_states, final_hidden, final_cell)"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a given corpus of documents.\n    \n    Args:\n        corpus: A list of documents, where each document is a list of words.\n        query: A list of words for which you want to compute the TF-IDF scores.\n        \n    Returns:\n        A list of lists containing the TF-IDF scores for the query words in each document,\n        rounded to five decimal places.\n    \"\"\"\n    if not corpus or not query:\n        raise ValueError('Corpus and query must be non-empty.')\n    query_words = [word.lower() for word in query]\n    tf_matrix = np.zeros((len(query), len(corpus)))\n    for (i, doc) in enumerate(corpus):\n        for word in doc:\n            tf_matrix[i][query_words.index(word)] += 1\n    idf_matrix = np.zeros((len(corpus), len(query_words)))\n    for (doc_index, doc_words) in enumerate(corpus):\n        doc_words_freq = [count for (count, word) in enumerate(doc_words)]\n        for (idx, word) in enumerate(query_words):\n            if word in doc_words_freq:\n                idf_matrix[doc_index][idx] = 1 / (np.sum(doc_words_freq) + 1)\n    tfidf_scores = np.dot(tf_matrix, idf_matrix.T)\n    return round(tfidf_scores, 4).tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    precision = 1.0 * (np.sum(y_pred == y_true) + np.sum(y_pred != y_true)) / (np.sum(y_pred) + np.sum(y_true))\n    recall = 1.0 * (np.sum(y_pred == y_true) - np.sum(y_pred != y_true)) / (np.sum(y_pred) + np.sum(y_true))\n    if beta < 1:\n        return beta / (1 + beta * (precision + recall)) * (precision + recall)\n    else:\n        return beta / (1 + beta * (precision + recall)) * (precision + recall)\n    return round(f_score, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        Returns: outputs, last_inputs, last_hiddens\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        for i in range(len(x)):\n            current_input = x[i]\n            h_next = self._forward_step(h_prev, current_input)\n            h_prev = h_next\n            outputs.append(h_next)\n        return (outputs, x[-1], h_prev)\n\n    def _forward_step(self, h_prev, current_input):\n        \"\"\"\n        Forward step in the RNN for one time step.\n        Returns: new hidden state, previous hidden state\n        \"\"\"\n        W_hh = self.W_hh\n        W_xh = self.W_xh\n        h_current = W_xh @ current_input + W_hh @ h_prev\n        b_h = self.b_h\n        return (h_current, h_prev)\n\n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        loss = 0.0\n        for t in range(len(input_sequence)):\n            e_t = expected_output[t] - outputs[t]\n            grad_h = W_hh @ (e_t * last_hiddens[t]) + W_xh @ (e_t * last_inputs[t])\n            grad_y = W_hy @ (e_t * last_hiddens[t])\n            loss += grad_h * learning_rate / len(input_sequence)\n            loss += grad_y * learning_rate / len(input_sequence)\n            b_h += grad_h * learning_rate / len(input_sequence)\n            b_y += grad_y * learning_rate / len(input_sequence)\n        return loss"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric, positive-definite matrix representing the linear system.\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    x = np.zeros_like(b) if x0 is None else x0.copy()\n    r = b - A @ x\n    p = np.dot(A, r) / np.linalg.norm(r)\n    for _ in range(n):\n        x += p\n        r -= A @ p\n        p = np.dot(A, r) / np.linalg.norm(r)\n        if np.linalg.norm(r) <= tol:\n            break\n    x = round(x, 8).tolist()\n    return x"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    y = np.array(y)\n    counts = np.bincount(y)\n    gini = -np.sum(counts * (1 - counts) / 2)\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointer = []\n    for row in range(len(dense_matrix)):\n        current_row = dense_matrix[row]\n        non_zero_count = sum((1 for element in current_row if element != 0))\n        row_pointer.append(non_zero_count)\n        for (index, value) in enumerate(current_row):\n            if value == 0:\n                continue\n            values.append(value)\n            col_indices.append(index)\n            row_pointer.append(row_pointer[-1] + 1)\n    return (values, col_indices, row_pointer)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    if L:\n        L = L / norm(L)\n    proj = L * (v.dot(L) / norm(L) ** 2)\n    return round(proj, 3)"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not isinstance(dense_matrix, list):\n        raise ValueError('Input must be a list of lists')\n    rows = len(dense_matrix)\n    cols = len(dense_matrix[0]) if rows > 0 else 0\n    values = []\n    row_indices = []\n    column_pointer = []\n    for i in range(rows):\n        for j in range(cols):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                row_indices.append(i)\n                column_pointer.append(j)\n    return (values, row_indices, column_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Returns the column space of a matrix A as a list of basis vectors.\n\n    Args:\n        A (np.ndarray): The input matrix.\n\n    Returns:\n        list: A list of tuples representing the basis vectors in column space.\n    \"\"\"\n    columns = [col.tolist() for col in A]\n    rank = 0\n    pivot_columns = []\n    for (i, column) in enumerate(columns):\n        if all((column[j] == 0 for j in range(len(column)))):\n            rank += 1\n            pivot_columns.append(i)\n    basis = []\n    for i in range(len(columns)):\n        if i not in pivot_columns:\n            basis.append(columns[i])\n    return basis"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"Calculate the R-squared value for regression analysis.\"\"\"\n    squared_diff = (y_true - y_pred) ** 2\n    sum_sq_pred = np.sum(y_pred ** 2)\n    sum_sq_true = np.sum(y_true ** 2)\n    r_squared_value = (sum_sq_pred - sum_sq_true / len(y_true)) / (sum_sq_true / len(y_true))\n    return round(r_squared_value, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Args:\n        img: A 2D list representing the grayscale image (each element is between 0 and 255).\n\n    Returns:\n        The average brightness of the image, rounded to two decimal places.\n        -1 if any edge case occurs (empty matrix, inconsistent row lengths, invalid pixel values).\n    \"\"\"\n    if not img or len(img[0]) == 0:\n        return -1\n    max_pixel = max((row[-1] for row in img))\n    min_pixel = min((row[0] for row in img))\n    valid_pixels = all((pixel >= 0 and pixel <= 255 for row in img for pixel in row))\n    if not valid_pixels:\n        return -1\n    average_brightness = sum((pixel for row in img for pixel in row)) / len(img[0])\n    return round(average_brightness * 100) / 100.0"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate Root Mean Square Error (RMSE) between the actual values and predicted values.\n    \n    Parameters:\n    - y_true: array of actual values\n    - y_pred: array of predicted values\n    \n    Returns:\n    - The RMSE value rounded to three decimal places.\n    \"\"\"\n    if not isinstance(y_true, list) or not isinstance(y_pred, list):\n        raise ValueError('Both inputs must be lists')\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input arrays must have the same length')\n    n = len(y_true)\n    squared_error = sum(((y_true[i] - y_pred[i]) ** 2 for i in range(n)))\n    rmse_value = np.sqrt(squared_error / n)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true_list = list(y_true)\n    y_pred_list = list(y_pred)\n    intersection = 0\n    union = 0\n    for (true_val, pred_val) in zip(y_true_list, y_pred_list):\n        if true_val == pred_val:\n            intersection += 1\n            union += 1\n    if intersection == 0:\n        return 0.0\n    elif all((v == 0 for v in y_true_list)) and all((v == 0 for v in y_pred_list)):\n        return 0.0\n    else:\n        return round(intersection / union, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n\n    Args:\n        y_true: Binary array of length N (true labels)\n        y_pred: Binary array of length N (predicted labels)\n\n    Returns:\n        float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(y_true) + np.sum(y_pred) - intersection\n    if union == 0:\n        return 0.0\n    else:\n        return 2 * intersection / union * 100"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    hypervectors = []\n    for (name, value) in row.items():\n        seed = random_seeds.get(name)\n        if seed is None:\n            raise ValueError(f\"Seed not found for feature '{name}'\")\n        hypervector = np.array([value] * dim)\n        hypervectors.append(hypervector)\n    composite_hv = np.vstack(hypervectors)\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for binary classification.\n\n    Args:\n        data: A list of lists, where each inner list contains a pair (y_true, y_pred).\n\n    Returns:\n        A 2x2 list of lists representing the confusion matrix.\n    \"\"\"\n    y_true = [y for (y, _) in data]\n    y_pred = [p for (_, p) in data]\n    tp = sum((1 for (y, p) in data if y == p))\n    fp = sum((1 for (y, p) in data if y != p))\n    fn = sum((1 for (y, p) in data if y != p))\n    tn = len(y_true) - tp - fp - fn\n    return [[tp, fp], [fn, tn]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Parameters:\n    v1, v2: Numpy arrays representing the input vectors.\n\n    Returns:\n    float: The cosine similarity value, rounded to three decimal places.\n    \"\"\"\n    if not np.array_equal(v1.shape, v2.shape):\n        raise ValueError('Vectors must have the same shape.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cosine_similarity_value = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_similarity_value, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculate and return performance metrics for a binary classification model.\n\n    Parameters:\n    actual (list of int): Actual class labels (0 or 1).\n    predicted (list of int): Predicted class labels from the model.\n\n    Returns:\n    tuple: A tuple containing:\n        - confusion_matrix: 2x2 matrix.\n        - accuracy: Accuracy as a float with three decimal places.\n        - f1_score: F1 score as a float with three decimal places.\n        - specificity: Specificity as a float with three decimal places.\n        - negative_predictive_value: Negative predictive value as a float with three decimal places.\n    \"\"\"\n    conf_mat = [[0, 0], [0, 0]]\n    for i in range(2):\n        for j in range(2):\n            if actual[i] == predicted[j]:\n                conf_mat[i][j] = 1\n            else:\n                conf_mat[i][j] = 0\n    accuracy = sum(conf_mat[0][0] + conf_mat[1][1]) / len(actual)\n    f1 = 2 * (conf_mat[0][0] * conf_mat[1][1]) / (conf_mat[0][0] + conf_mat[1][1] + conf_mat[0][1] + conf_mat[1][0])\n    specificity = (conf_mat[0][0] + conf_mat[1][1]) / len(actual)\n    n_p = len(actual)\n    n_n = len(predicted) - len(actual)\n    n_np = n_p * (n_n - len(predicted)) / n_p\n    negative_predictive_value = n_np / (n_n - len(predicted))\n    accuracy_rounded = round(accuracy, 3)\n    f1_score_rounded = round(f1, 3)\n    specificity_rounded = round(specificity, 3)\n    negative_predictive_value_rounded = round(negative_predictive_value, 3)\n    return (conf_mat, accuracy_rounded, f1_score_rounded, specificity_rounded, negative_predictive_value_rounded)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics for a given dataset.\n\n    Parameters:\n        data (list or NumPy array): A list of numerical values.\n\n    Returns:\n        dict: A dictionary containing the computed statistics, rounded to four decimal places.\n    \"\"\"\n    if not isinstance(data, list):\n        data = [data]\n    data = [float(x) for x in data]\n    if len(data) == 0:\n        return {}\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = np.unique(data).mode()\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    interquartile_range = percentile_75 - percentile_25\n    return {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode[0], 4), 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(interquartile_range, 4)}"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    \n    Args:\n        n (int): Total number of trials\n        k (int): Number of successes\n        p (float): Probability of success on each trial\n    \n    Returns:\n        float: The probability of exactly k successes in n trials, rounded to five decimal places.\n    \"\"\"\n    probability = math.comb(n, k) * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    Args:\n        x: The value at which the PDF is evaluated.\n        mean: The mean (\u03bc) of the distribution.\n        std_dev: The standard deviation (\u03c3) of the distribution.\n    \n    Returns:\n        The value of the normal PDF at the given point, rounded to 5 decimal places.\n    \"\"\"\n    pdf_value = 1 / (std_dev * math.sqrt(2 * math.pi)) * math.exp(-0.5 * (x - mean) ** 2 / std_dev ** 2)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    \n    Parameters:\n        k (int): Number of events (non-negative integer)\n        lam (float): The average rate of occurrences in a fixed interval\n    \n    Returns:\n        float: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam < 0:\n        return 0.0\n    if k > 0 and lam > 0:\n        result = math.exp(-lam) * lam ** k / math.factorial(k)\n        return round(result, 5)\n    else:\n        return 0.0"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    min_val = np.min(img)\n    max_val = np.max(img)\n    contrast = max_val - min_val\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list of lists: A nested list where each inner list contains the transformed features of the corresponding data point. Rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    n_features = len(data_array) + 1\n    transformed = np.zeros((len(data_array), n_features))\n    for i in range(len(data_array)):\n        x = data_array[i]\n        poly = np.zeros(n_features)\n        for j in range(degree + 1):\n            poly[j] = x ** j\n        transformed[i] = poly\n    rounded_transformed = np.round(transformed, 8)\n    return rounded_transformed.tolist()"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates positional encodings for a sequence length `position` and model dimensionality `d_model`\n    \n    Args:\n        position (int): The number of positions to encode.\n        d_model (int): The dimensionality of the model.\n\n    Returns:\n        list: Positional encoding array in float16 dtype, converted to a list.\n        -1 if position is 0 or d_model <= 0.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return [-1]\n    return [np.sin(2 * np.pi * i / (d_model * 2 ** (i // 2))) / 2 ** (i // 2) for i in range(position)]"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    \n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if abs(training_accuracy - test_accuracy) > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    \n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    new_m = learning_rate * m - beta1 * m * (1 - beta1) * (parameter - parameter * beta1)\n    new_v = learning_rate * v - beta2 * v * (1 - beta1) * (parameter - parameter * beta1) + beta2 * grad ** 2\n    if beta1 > 0:\n        new_m = new_m / (1 - beta1) * (1 - beta1) + beta1 * (parameter - parameter * beta1)\n    else:\n        new_m = new_m\n    if beta1 > 0:\n        new_m = new_m / (1 - beta1) * (1 - beta1) + beta1 * (parameter - parameter * beta1)\n    updated_m = new_m\n    updated_v = new_v\n    updated_values = [round(x, 5) for x in [updated_m, updated_v]]\n    return np.array(updated_values).tolist()"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        values = np.array(values)\n        values &= np.greater_equal(values, 0)\n        log_values = np.log(values)\n        return np.exp(log_values) / np.sum(log_values)\n    attention_scores = softmax(crystal_values)\n    rounded_attention_scores = np.round(attention_scores, 4)\n    return rounded_attention_scores.tolist()"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a given query on a corpus of documents.\n\n    Args:\n        corpus: list of documents (strings)\n        query: the query string to search for\n        k1: weight for the reciprocal term frequency (default 1.5)\n        b: weight for the document length penalty (default 0.75)\n\n    Returns:\n        List of float values representing document scores (rounded to three decimal places)\n    \"\"\"\n    terms = set(query.split())\n    term_counts = {}\n    for doc in corpus:\n        for term in doc.split():\n            if term in term_counts:\n                term_counts[term] += 1\n            else:\n                term_counts[term] = 1\n    doc_lengths = [len(doc) for doc in corpus]\n    total_length = sum(doc_lengths)\n    penalty_weight = b * total_length / len(doc_lengths)\n    scores = []\n    for doc in corpus:\n        doc_terms = Counter(doc.split())\n        term_freq = []\n        for term in doc_terms:\n            term_freq.append(term_counts.get(term, 0))\n        score = (k1 + b) * sum(term_freq) / sum(doc_terms.values()) - penalty_weight * (sum(term_freq) - sum(doc_terms.values())) ** 2\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    precision = 2 * (y_pred == y_true) / (len(y_pred) + len(y_true))\n    recall = (y_pred[y_true.index(y_pred[0])] if y_pred[0] in y_true else 0) / (len(y_true) + len(y_pred))\n    f1 = 2 * precision * recall / (precision + recall)\n    return round(f1, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\ndef power_grid_forecast(consumption_data):\n    detrended_data = [round(consumption_data[i] - 10 * math.sin(2 * math.pi * i / 10), 2) for i in range(10)]\n    (slope, intercept) = linear_regression(detrended_data)\n    predicted_consumption = intercept + slope * 15\n    final_consumption = round(predicted_consumption + 5 * (predicted_consumption - 10 * math.sin(2 * math.pi * 15 / 10)), 2)\n    return final_consumption"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_error = abs(y_pred - y_true)\n    mean_absolute_error = np.mean(absolute_error)\n    rounded_mae = round(mean_absolute_error, 3)\n    return rounded_mae"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    result = [self_attention(Q, K, V, n_heads) for _ in range(n_heads)]\n    return np.array(result).tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    count_x = sum(x)\n    count_y = sum(y)\n    numerator = count_x * count_y\n    denominator = (count_x + count_y) * (count_x + count_y - 2 * count_x - 2 * count_y)\n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": ""}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x < 0:\n        return alpha * (x - alpha)\n    else:\n        return x * (1 - alpha) + alpha"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        result = x * (1 + alpha * (x - 0))\n    else:\n        result = x * (1 + alpha * (x - 0))\n    return result"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x < -math.inf:\n        return 0.0\n    if x > math.inf:\n        return 0.0\n    exp_x = math.exp(x)\n    result = 1 + exp_x\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    sign = 1.0 if x > 0 else -1.0\n    softsign_value = abs(x) / (1 + abs(x))\n    return round(softsign_value, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    mean_diff = np.mean(pi_theta_ref - pi_theta_old)\n    var_diff = np.var(pi_theta_ref - pi_theta_old)\n    clipped_diff = np.clip(pi_theta_ref - pi_theta_old, -epsilon, epsilon)\n    mean_clipped = np.mean(clipped_diff)\n    var_clipped = np.var(clipped_diff)\n    kl_penalty = beta * np.log(mean_clipped) + beta * np.log(var_clipped)\n    grpo_value = mean_diff ** 2 + kl_penalty\n    grpo_value_rounded = round(grpo_value, 6)\n    return grpo_value_rounded"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * (1 + math.exp(-math.log2(x)))"}
{"task_id": 103, "completion_id": 0, "solution": "import math\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x < 0:\n        return alpha * (x / (scale - x))\n    else:\n        return alpha * (x + scale) / (1 + math.exp(-x))"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_output = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_output))\n    return (probabilities > 0.5).astype(np.int8).tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    B = np.zeros((X.shape[1], X.shape[0]))\n    losses = []\n\n    def cross_entropy_loss(y_true, y_pred):\n        return -np.sum(y_true * np.log(y_pred + 1e-12)) / len(y_true)\n    for _ in range(iterations):\n        dLdB = 1.0 / len(X) * (np.dot(X, B.T) - y)\n        dLdB /= learning_rate\n        B += dLdB\n        loss = cross_entropy_loss(y, B)\n        losses.append(loss)\n    B_rounded = np.round(B, 4)\n    losses_rounded = np.round(losses, 4)\n    return (B_rounded.tolist(), losses_rounded.tolist())"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Args:\n        X: The input dataset (n_samples x n_features)\n        y: The target variable (n_samples x 1)\n        learning_rate: Learning rate for gradient descent\n        iterations: Number of iterations to perform\n        \n    Returns:\n        A tuple containing the optimized coefficients and a list of loss values over iterations.\n    \"\"\"\n    theta = np.zeros((X.shape[1], 1))\n    loss_history = []\n    for _ in range(iterations):\n        grad_theta = np.dot(X.T, (y - np.dot(theta, X)) / len(y)) + learning_rate * np.sum(np.dot(X, theta) * (1 - y) + y * theta)\n        theta -= learning_rate * grad_theta\n        current_loss = np.mean(np.dot(theta, X) * (1 - y) + y * theta)\n        loss_history.append(current_loss)\n    return (theta.tolist(), loss_history)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    masked_Q = Q[mask > 0]\n    masked_K = K[mask > 0]\n    masked_V = V[mask > 0]\n    masked_Q = masked_Q.reshape(-1, *masked_Q.shape[1:])\n    masked_K = masked_K.reshape(-1, *masked_K.shape[1:])\n    masked_V = masked_V.reshape(-1, *masked_V.shape[1:])\n    return np.stack((masked_Q, masked_K, masked_V))"}
{"task_id": 108, "completion_id": 0, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The function returns the disorder value as a float, rounded to the nearest 4th decimal place.\n    \"\"\"\n    from collections import Counter\n    color_counts = Counter(apples)\n    disorder_score = 0.0\n    total = sum(color_counts.values())\n    sorted_colors = sorted(color_counts.keys())\n    sorted_colors.sort()\n    n = len(sorted_colors)\n    if n % 2 == 1:\n        median = sorted_colors[n // 2]\n    else:\n        median = (sorted_colors[n // 2 - 1] + sorted_colors[n // 2]) / 2\n    sum_abs_diff = sum((abs(color - median) for color in sorted_colors))\n    disorder_score = sum_abs_diff * 100.0 / total\n    return round(disorder_score, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on a sequence of input tensors.\n\n    Parameters:\n        X (np.ndarray): Input tensor with shape (batch_size, sequence_length, feature_dim)\n        gamma (np.ndarray): Scaling parameter for normalization\n        beta (np.ndarray): Shifting parameter for normalization\n        epsilon (float): Small epsilon to avoid division by zero (default: 1e-5)\n\n    Returns:\n        np.ndarray: Normalized tensor with rounded values and converted to list.\n    \"\"\"\n    if X.shape != (X.shape[0], X.shape[1], X.shape[2]):\n        raise ValueError('Input tensor must have shape (batch_size, sequence_length, feature_dim)')\n    mean = X.mean(axis=0)\n    std = np.std(X, axis=0, ddof=1)\n    normalized_X = (X - mean) * gamma / std\n    normalized_X += beta\n    return np.round(normalized_X).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Parameters:\n    reference (str): Reference translation.\n    candidate (str): Candidate translation.\n    alpha (float, optional): Weight for unigram matches (default: 0.9).\n    beta (float, optional): Weight for precision (default: 3).\n    gamma (float, optional): Weight for F-mean (default: 0.5).\n\n    Returns:\n    float: METEOR score, rounded to 3 decimal places.\n    \"\"\"\n    tokens_ref = reference.split()\n    tokens_cand = candidate.split()\n    token_count_ref = Counter(tokens_ref)\n    token_count_cand = Counter(tokens_cand)\n    match_unigrams = alpha * sum((token_count_ref[t] * token_count_cand[t] for t in token_count_ref))\n    precision = beta * sum((1 / len(tokens_ref) for _ in range(len(tokens_ref))))\n    recall = (len(set(tokens_ref)) - len(set(tokens_cand))) / len(tokens_ref)\n    f_mean = (gamma * precision + beta * recall) / (1 + (gamma - beta) * precision)\n    score = match_unigrams + precision + recall + f_mean\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) for two events.\n\n    Args:\n        joint_counts: A 2D array representing the joint occurrence count of two events.\n        total_counts_x: A 1D array representing the individual counts of event X.\n        total_counts_y: A 1D array representing the individual counts of event Y.\n        total_samples: An integer representing the total number of samples.\n\n    Returns:\n        A float rounded to three decimal places.\n    \"\"\"\n    chi_squared = np.sum((joint_counts - np.outer(total_counts_x, total_counts_y)) ** 2 / total_samples)\n    pmis = np.log(chi_squared) / np.log(2)\n    return round(pmis, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    scaled = (x - min_val) / (max_val - min_val)\n    scaled = round(scaled, 4)\n    return scaled"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Creates a simple residual block using NumPy with a shortcut connection.\n\n    Parameters:\n    x (np.ndarray): Input 1D array.\n    w1 (np.ndarray): First weight matrix for the first layer.\n    w2 (np.ndarray): Second weight matrix for the second layer.\n\n    Returns:\n    list: The output of the residual block, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    x = np.dot(x, w1)\n    x = np.dot(x, w2)\n    x = np.relu(x)\n    x += x\n    return x.tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"Perform Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer.\"\"\"\n    pooled = np.mean(x, axis=(0, 1))\n    return np.squeeze(pooled)"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Batch Normalization on a BCHW input format.\n\n    Args:\n        X: Input array (batch, channels, height, width)\n        gamma: Scaling factor for the output\n        beta: Shift factor for the output\n        epsilon: Numerical stability parameter (default is 1e-5)\n\n    Returns:\n        A list of arrays representing the normalized outputs with 4 decimal places.\n    \"\"\"\n    if X.ndim != 4 or X.shape[0] != X.shape[1]:\n        raise ValueError('Input must be 4D array with shape (batch, channels, height, width)')\n    batch_size = X.shape[0]\n    channels = X.shape[1]\n    height = X.shape[2]\n    width = X.shape[3]\n    mean = X.mean(axis=(0, 1))\n    var = X.var(axis=(0, 1))\n    X_normalized = (X - mean) * gamma / np.sqrt(var + epsilon) + beta\n    return round(X_normalized.tolist(), 4)"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    return c * n * x ** (n - 1)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    Args:\n        vectors: List of 2D vectors (each vector is a list of two floats).\n        tol: Tolerance to determine linear independence, defaults to 1e-10.\n    \n    Returns:\n        A list of orthonormal vectors (unit length and orthogonal to each other) that span the same subspace.\n    \"\"\"\n    n = len(vectors)\n    result = []\n    for i in range(n):\n        v = vectors[i]\n        norm_sq = np.sum(v ** 2)\n        if norm_sq == 0:\n            result.append(v / np.linalg.norm(v))\n            continue\n        new_v = v - (v @ result[i - 1] + ...)\n        result.append(new_v)\n    return [np.round(v, 4).tolist() for v in result]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    a_array = np.array(a)\n    b_array = np.array(b)\n    result = [a_array[1] * b_array[2] - a_array[2] * b_array[1], a_array[2] * b_array[0] - a_array[0] * b_array[2], a_array[0] * b_array[1] - a_array[1] * b_array[0]]\n    return round(result, 4).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    if len(A) != len(b):\n        return -1\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = np.array([np.linalg.solve(A, b) for _ in range(len(A))])\n    x_rounded = np.round(x, 4)\n    result = x_rounded.tolist()\n    return result"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p_array = np.array(p)\n    q_array = np.array(q)\n    dist = np.sum(p_array * q_array) / (np.sqrt(np.sum(p_array ** 2)) * np.sqrt(np.sum(q_array ** 2)))\n    rounded_dist = round(dist, 4)\n    return rounded_dist"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Returns a new list representing the resulting sum if the vectors are of the same length.\n    Otherwise, returns -1 to indicate incompatible dimensions.\n    \n    Parameters:\n    a (list): First vector containing integers or floats\n    b (list): Second vector containing integers or floats\n    \n    Returns:\n    list: Resulting vector or [-1] if dimensions differ\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    log_prob = np.exp(theta @ episodes[i][0] @ theta.T)\n    return_list = []\n    for episode in episodes:\n        (current_state, action, reward) = (episode[0], episode[1], episode[2])\n        current_return = reward + np.dot(log_prob[current_state, action], episodes[i + 1])\n        return_list.append(current_return)\n    result = np.array(return_list).tolist()\n    return result"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out * 2 * n_experts\n    flops_moe = d_in * d_out * k_active * 2\n    savings_percentage = (flops_moe - flops_dense) / flops_dense * 100\n    rounded_savings = round(savings_percentage, 1)\n    return f'{rounded_savings:.1f}%'"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    X = X.reshape((-1, len(X[0])))\n    W_g = W_g.reshape((-1, len(W_g[0])))\n    W_noise = W_noise.reshape((-1, len(W_noise[0])))\n    N = N.reshape((-1, len(N[0])))\n    sliding_window_size = len(X)\n    top_k_values = []\n    current_sum = 0.0\n    for i in range(len(X)):\n        current_sum += X[i] * W_g[i]\n        if i >= k:\n            top_k_values.append(current_sum / (sliding_window_size + k))\n            current_sum = 0.0\n    noise_contribution = np.zeros_like(top_k_values)\n    for i in range(len(top_k_values)):\n        noise_contribution[i] = W_noise[i] * (top_k_values[i] - 1.0)\n    final_gating = top_k_values + noise_contribution\n    rounded_gating = np.round(final_gating, 4).tolist()\n    return rounded_gating"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    if x.ndim != 2:\n        raise ValueError('Input tensor should have shape (batch_size, sequence_length)')\n    if x.shape[1] != We.shape[0]:\n        raise ValueError('Weight matrix should have the same number of features')\n    if x.shape[1] != Wg.shape[1]:\n        raise ValueError('Gating weight matrix should have the same number of features')\n    if n_experts <= 0 or top_k <= 0:\n        raise ValueError('Number of experts and top-k must be positive')\n    p = np.einsum('..., ...', x, We) / np.max(np.abs(Wg))\n    selected_indices = np.argsort(p, axis=1)[:top_k]\n    output = np.dot(x, We[selected_indices]) * Wg[selected_indices]\n    result = np.sum(output * p, axis=0) / np.sum(p, axis=0)\n    rounded_result = round(result, 4)\n    return np.round(result, 4).tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    if X.ndim != 4:\n        raise ValueError('Input tensor must be a 4D tensor')\n    (B, C, H, W) = X.shape\n    normalized_X = (X - np.mean(X, axis=(0, 2, 3))) / (np.std(X, axis=(0, 2, 3)) + epsilon)\n    normalized_X = gamma * normalized_X + beta\n    result = normalized_X.reshape(X.shape)\n    rounded_result = np.round(result, 4)\n    return rounded_result.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Finds the minimum of the function f(x) = x^4 - 3x^3 + 2 using a numerical optimization method.\n    \n    Parameters:\n    start_x (float): Initial guess for the root (starting point).\n    learning_rate (float): Learning rate for the optimization algorithm.\n    tolerance (float): Tolerance for stopping the optimization.\n    max_iters (int): Maximum number of iterations to perform.\n    \n    Returns:\n    float: The value of x where f(x) reaches its minimum.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        f = x ** 4 - 3 * x ** 3 + 2\n        df_dx = 4 * x ** 3 - 9 * x ** 2\n        x = x - f / df_dx\n        if abs(f) < tolerance:\n            break\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    x = x.reshape((x.shape[0], x.shape[1] * x.shape[2]))\n    y = (alpha * x + beta) / (gamma + 1)\n    result = y.reshape(x.shape)\n    result = np.round(result, 4).tolist()\n    return result"}
