{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Compute the dot product of a matrix and a vector.\n\n    Args:\n    a (list[list[int|float]]): A matrix (a list of lists).\n    b (list[int|float]): A vector (a list).\n\n    Returns:\n    list[int|float]: A list representing the resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Compute the transpose of a given matrix.\n\n    Args:\n    a (list[list[int|float]]): The input matrix.\n\n    Returns:\n    list[list[int|float]]: The transpose of the input matrix.\n    \"\"\"\n    if not a:\n        return []\n    num_rows = len(a)\n    num_cols = len(a[0])\n    transposed_matrix = [[0.0 for _ in range(num_rows)] for _ in range(num_cols)]\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transposed_matrix[j][i] = a[i][j]\n    return transposed_matrix\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given matrix into a specified shape.\n    \n    Args:\n    a (list[list[int|float]]): The input matrix.\n    new_shape (tuple[int, int]): The desired shape of the output matrix.\n    \n    Returns:\n    list[list[int|float]]: The reshaped matrix if possible, otherwise an empty list.\n    \"\"\"\n    a = np.array(a)\n    try:\n        reshaped_array = a.reshape(new_shape)\n        reshaped_list = reshaped_array.tolist()\n        return reshaped_list\n    except ValueError:\n        return []\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Args:\n        matrix (list[list[float]]): A 2D list of floating point numbers.\n        mode (str): Either 'row' or 'column' to specify the calculation mode.\n\n    Returns:\n        list[float]: A list of means according to the specified mode.\n    \"\"\"\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Invalid mode. Mode should be either 'row' or 'column'.\")\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        means = [sum(col) / len(matrix) for col in zip(*matrix)]\n    return means\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar.\n\n    Args:\n        matrix (list[list[int|float]]): The input matrix.\n        scalar (int|float): The scalar value.\n\n    Returns:\n        list[list[int|float]]: The result of the scalar multiplication.\n    \"\"\"\n    if not all((isinstance(row, list) for row in matrix)):\n        raise ValueError('Input matrix must be a list of lists')\n    if not all((isinstance(element, (int, float)) for row in matrix for element in row)):\n        raise ValueError('All elements in the matrix must be numbers')\n    if not isinstance(scalar, (int, float)):\n        raise ValueError('Scalar must be a number')\n    result = [[element * scalar for element in row] for row in matrix]\n    return result\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'tolist'", "solution": "\nimport numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float|int]]): A 2x2 matrix.\n\n    Returns:\n    list[float]: A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    matrix_array = np.array(matrix)\n    if matrix_array.shape != (2, 2):\n        raise ValueError('Input matrix must be a 2x2 matrix')\n    eigenvalues = np.linalg.eigvals(matrix_array)\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    return sorted_eigenvalues.tolist()\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function transforms a given matrix A using the operation T^-1AS, \n    where T and S are invertible matrices. It first validates if the matrices T and S are invertible, \n    and then performs the transformation. In cases where there is no solution, it returns -1.\n    \n    Parameters:\n    A (list[list[int|float]]): The input matrix A\n    T (list[list[int|float]]): The invertible matrix T\n    S (list[list[int|float]]): The invertible matrix S\n    \n    Returns:\n    list[list[int|float]]: The transformed matrix, or -1 if there is no solution\n    \"\"\"\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    transformed_matrix = np.dot(np.linalg.inv(T), np.dot(A, S))\n    transformed_matrix = np.round(transformed_matrix, 4)\n    transformed_matrix = transformed_matrix.tolist()\n    return transformed_matrix\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nimport numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n\n    Args:\n    matrix (list[list[float]]): A 2x2 matrix.\n\n    Returns:\n    list[list[float]]: The inverse of the matrix. Returns 'None' if the matrix is not invertible.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2:\n        return None\n    matrix = np.array(matrix)\n    determinant = np.linalg.det(matrix)\n    if determinant == 0:\n        return None\n    inverse = np.linalg.inv(matrix)\n    inverse = inverse.tolist()\n    return inverse\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function multiplies two matrices. If the matrices cannot be multiplied, it returns -1.\n    \n    Args:\n        a (list[list[int|float]]): The first matrix.\n        b (list[list[int|float]]): The second matrix.\n    \n    Returns:\n        list[list[int|float]]: The product of the two matrices, or -1 if they cannot be multiplied.\n    \"\"\"\n    if not a or not b:\n        return -1\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0.0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nimport numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n    vectors (list[list[float]]): A list of lists, where each inner list represents a feature with its observations.\n\n    Returns:\n    list[list[float]]: The covariance matrix as a list of lists.\n    \"\"\"\n    vectors_array = np.array(vectors)\n    means = np.mean(vectors_array, axis=0)\n    centered_vectors = vectors_array - means\n    covariance_matrix = np.cov(centered_vectors, rowvar=False)\n    covariance_matrix_list = covariance_matrix.tolist()\n    return covariance_matrix_list\nvectors = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi method.\n\n    Args:\n    A (np.ndarray): The coefficient matrix A.\n    b (np.ndarray): The right-hand side vector b.\n    n (int): The number of iterations.\n\n    Returns:\n    list: The approximate solution x.\n    \"\"\"\n    (num_equations, num_variables) = A.shape\n    x = np.zeros(num_variables)\n    for _ in range(n):\n        diagonal = np.diag(A)\n        x_new = (b - np.dot(A - np.diag(diagonal), x)) / diagonal\n        x_new = np.round(x_new, 4)\n        x = x_new\n    return x.tolist()\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) of a 2x2 matrix A using the Jacobian method.\n\n    Args:\n        A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n        tuple: A tuple containing the singular values and the right and left singular vectors.\n    \"\"\"\n    assert A.shape == (2, 2), 'Input matrix must be a 2x2 matrix'\n    AA_T = np.dot(A, A.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(AA_T)\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    V = np.dot(A, eigenvectors)\n    U = np.dot(A.T, V) / singular_values[0]\n    singular_values = np.round(singular_values, 4)\n    U = np.round(U, 4)\n    V = np.round(V, 4)\n    return (singular_values, U, V)\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    This function calculates the determinant of a 4x4 matrix using Laplace's Expansion method.\n\n    Args:\n        matrix (list[list[int|float]]): A 4x4 matrix represented as a list of lists.\n\n    Returns:\n        float: The determinant of the matrix.\n    \"\"\"\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input matrix must be a 4x4 matrix')\n    det = 0\n    for i in range(4):\n        minor = [row[:i] + row[i + 1:] for row in matrix[1:]]\n        cofactor = (-1) ** i * matrix[0][i] * determinant_3x3(minor)\n        det += cofactor\n    return det\ndef determinant_3x3(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    This function calculates the determinant of a 3x3 matrix using the formula:\n    det(A) = a11*(a22*a33 - a23*a32) - a12*(a21*a33 - a23*a31) + a13*(a21*a32 - a22*a31)\n\n    Args:\n        matrix (list[list[int|float]]): A 3x3 matrix represented as a list of lists.\n\n    Returns:\n        float: The determinant of the matrix.\n    \"\"\"\n    if len(matrix) != 3 or any((len(row) != 3 for row in matrix)):\n        raise ValueError('Input matrix must be a 3x3 matrix')\n    det = matrix[0][0] * (matrix[1][1] * matrix[2][2] - matrix[1][2] * matrix[2][1]) - matrix[0][1] * (matrix[1][0] * matrix[2][2] - matrix[1][2] * matrix[2][0]) + matrix[0][2] * (matrix[1][0] * matrix[2][1] - matrix[1][1] * matrix[2][0])\n    return det\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": false, "result": "failed: Singular matrix", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    This function performs linear regression using the normal equation.\n\n    Parameters:\n    X (list[list[float]]): A 2D list of features.\n    y (list[float]): A 1D list of target values.\n\n    Returns:\n    list[float]: The coefficients of the linear regression model.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n    theta = np.round(theta, 4)\n    return theta.tolist()\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    This function performs linear regression using gradient descent.\n\n    Parameters:\n    X (np.ndarray): Features with a column of ones for the intercept.\n    y (np.ndarray): Target variable.\n    alpha (float): Learning rate.\n    iterations (int): Number of iterations.\n\n    Returns:\n    np.ndarray: Coefficients of the linear regression model as a list.\n    \"\"\"\n    coefficients = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = np.dot(X, coefficients)\n        residuals = predictions - y\n        gradient = 2 * np.dot(X.T, residuals) / X.shape[0]\n        coefficients -= alpha * gradient\n    coefficients = np.round(coefficients, 4)\n    return coefficients.tolist()\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    This function performs feature scaling on a dataset using both standardization and min-max normalization.\n\n    Args:\n        data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n\n    Returns:\n        tuple: Two 2D lists, one scaled by standardization and one by min-max normalization.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    min_values = np.min(data, axis=0)\n    max_values = np.max(data, axis=0)\n    normalized_data = (data - min_values) / (max_values - min_values)\n    standardized_data = np.round(standardized_data, 4).tolist()\n    normalized_data = np.round(normalized_data, 4).tolist()\n    return (standardized_data, normalized_data)\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": false, "result": "failed: tuple expected at most 1 argument, got 2", "solution": "import numpy as np\nimport numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    This function implements the k-Means clustering algorithm.\n\n    Args:\n    - points (list[tuple[float, float]]): A list of points, where each point is a tuple of coordinates.\n    - k (int): An integer representing the number of clusters to form.\n    - initial_centroids (list[tuple[float, float]]): A list of initial centroid points, each a tuple of coordinates.\n    - max_iterations (int): An integer representing the maximum number of iterations to perform.\n\n    Returns:\n    - A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n    \"\"\"\n    centroids = initial_centroids.copy()\n    for _ in range(max_iterations):\n        labels = np.argmin(np.linalg.norm(np.array(points)[:, np.newaxis] - np.array(centroids), axis=2), axis=1)\n        new_centroids = []\n        for i in range(k):\n            points_in_cluster = [points[j] for j in range(len(points)) if labels[j] == i]\n            if points_in_cluster:\n                new_centroid = tuple(np.mean([point[0] for point in points_in_cluster], dtype=np.float64) + 1e-09, np.mean([point[1] for point in points_in_cluster], dtype=np.float64) + 1e-09)\n                new_centroids.append(new_centroid)\n            else:\n                new_centroids.append(centroids[i])\n        if np.allclose(centroids, new_centroids, atol=0.0001):\n            break\n        centroids = new_centroids\n    final_centroids = [(round(x, 4), round(y, 4)) for (x, y) in centroids]\n    return final_centroids\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "import numpy as np\nimport numpy as np\nfrom sklearn.model_selection import KFold\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    X (np.ndarray): Feature data.\n    y (np.ndarray): Target data.\n    k (int): Number of folds. Default is 5.\n    shuffle (bool): Whether to shuffle the data before splitting. Default is True.\n    random_seed (int): Random seed for reproducibility. Default is None.\n\n    Returns:\n    list: A list of train-test indices for each fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    kf = KFold(n_splits=k, shuffle=shuffle, random_state=random_seed)\n    train_test_indices = []\n    for (train_index, test_index) in kf.split(X):\n        train_test_indices.append((train_index, test_index))\n    return train_test_indices\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on a 2D NumPy array.\n\n    Args:\n    - data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - k (int): The number of principal components to return.\n\n    Returns:\n    - A list of lists, where each sublist contains the k principal components (eigenvectors corresponding to the largest eigenvalues).\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(-eigenvalues)\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    principal_components = principal_components.tolist()\n    return principal_components\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": false, "result": "failed: local variable 'target_values' referenced before assignment", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree using recursive binary splitting based on entropy and information gain.\n\n    Args:\n    - examples (list[dict]): A list of examples, where each example is a dictionary of attribute-value pairs.\n    - attributes (list[str]): A list of attribute names.\n    - target_attr (str): The name of the target attribute.\n\n    Returns:\n    - A nested dictionary representing the decision tree.\n    \"\"\"\n    if all((example[target_attr] == examples[0][target_attr] for example in examples)):\n        return {target_attr: examples[0][target_attr]}\n    if not attributes:\n        target_values = [example[target_attr] for example in examples]\n        return {target_attr: Counter(target_values).most_common(1)[0][0]}\n    initial_entropy = calculate_entropy(target_values)\n    best_attr = None\n    best_info_gain = 0\n    for attr in attributes:\n        if attr == target_attr:\n            continue\n        info_gain = calculate_info_gain(examples, attr, target_attr, initial_entropy)\n        if info_gain > best_info_gain:\n            best_attr = attr\n            best_info_gain = info_gain\n    if best_info_gain == 0:\n        target_values = [example[target_attr] for example in examples]\n        return {target_attr: Counter(target_values).most_common(1)[0][0]}\n    examples_left = [example for example in examples if example[best_attr] == examples[0][best_attr]]\n    examples_right = [example for example in examples if example[best_attr] != examples[0][best_attr]]\n    left_tree = learn_decision_tree(examples_left, attributes[:], target_attr)\n    right_tree = learn_decision_tree(examples_right, attributes[:], target_attr)\n    return {best_attr: {True: left_tree, False: right_tree}}\ndef calculate_entropy(target_values: list[str]) -> float:\n    \"\"\"\n    Calculates the entropy of a list of target values.\n\n    Args:\n    - target_values (list[str]): A list of target values.\n\n    Returns:\n    - The entropy of the target values.\n    \"\"\"\n    target_counts = Counter(target_values)\n    entropy = 0\n    for count in target_counts.values():\n        prob = count / len(target_values)\n        entropy -= prob * math.log2(prob)\n    return entropy\ndef calculate_info_gain(examples: list[dict], attr: str, target_attr: str, initial_entropy: float) -> float:\n    \"\"\"\n    Calculates the information gain for a given attribute.\n\n    Args:\n    - examples (list[dict]): A list of examples.\n    - attr (str): The attribute to calculate the information gain for.\n    - target_attr (str): The target attribute.\n    - initial_entropy (float): The initial entropy of the examples.\n\n    Returns:\n    - The information gain for the attribute.\n    \"\"\"\n    attr_values = [example[attr] for example in examples]\n    unique_values = set(attr_values)\n    info_gain = initial_entropy\n    for value in unique_values:\n        examples_value = [example for example in examples if example[attr] == value]\n        value_entropy = calculate_entropy([example[target_attr] for example in examples_value])\n        prob = len(examples_value) / len(examples)\n        info_gain -= prob * value_entropy\n    return info_gain\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: shapes (4,2) and (4,) not aligned: 2 (dim 1) != 4 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    This function implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier from scratch.\n\n    Parameters:\n    data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    labels (np.ndarray): A 1D NumPy array where each entry corresponds to the label of the sample.\n    kernel (str): The choice of kernel. It can be either 'linear' or 'rbf'. Default is 'linear'.\n    lambda_val (float): The regularization parameter. Default is 0.01.\n    iterations (int): The number of iterations. Default is 100.\n    sigma (float): The standard deviation for the RBF kernel. Default is 1.0.\n\n    Returns:\n    list: A list containing the model's alpha coefficients and bias.\n    \"\"\"\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    if kernel == 'linear':\n        kernel_matrix = data\n    elif kernel == 'rbf':\n        kernel_matrix = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                kernel_matrix[i, j] = np.exp(-np.sum((data[i] - data[j]) ** 2) / (2 * sigma ** 2))\n    else:\n        raise ValueError(\"Invalid kernel. It can be either 'linear' or 'rbf'.\")\n    for _ in range(iterations):\n        gradient_alpha = (np.dot(kernel_matrix, alpha) + bias * np.ones(n_samples) - labels) * kernel_matrix\n        alpha = np.maximum(0, alpha - lambda_val * gradient_alpha / (np.sum(kernel_matrix, axis=1) + lambda_val))\n        bias = np.mean(labels - np.dot(kernel_matrix, alpha))\n    return [round(x, 4) for x in alpha.tolist()] + [round(bias, 4)]\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function given an input value z.\n    \n    Args:\n        z (float): The input value to the sigmoid function.\n    \n    Returns:\n        float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    sigmoid_output = 1 / (1 + math.exp(-z))\n    sigmoid_output = round(sigmoid_output, 4)\n    return sigmoid_output\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax activation for a given list of scores.\n\n    Args:\n    scores (list[float]): A list of scores.\n\n    Returns:\n    list[float]: The softmax values as a list, each rounded to four decimal places.\n    \"\"\"\n    max_score = max(scores)\n    scores = [score - max_score for score in scores]\n    sum_exponentials = sum((math.exp(score) for score in scores))\n    softmax_values = [math.exp(score) / sum_exponentials for score in scores]\n    softmax_values = [round(value, 4) for value in softmax_values]\n    return softmax_values\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Args:\n    features (list[list[float]]): A list of feature vectors, where each vector represents multiple features for an example.\n    labels (list[int]): Associated true binary labels.\n    weights (list[float]): The neuron's weights, one for each feature.\n    bias (float): The neuron's bias.\n\n    Returns:\n    list[float]: The predicted probabilities after sigmoid activation, rounded to four decimal places.\n    float: The mean squared error between the predicted probabilities and the true labels, rounded to four decimal places.\n    \"\"\"\n    features_array = np.array(features)\n    weighted_sum = np.sum(features_array * np.array(weights), axis=1) + bias\n    predicted_probabilities = 1 / (1 + np.exp(-weighted_sum))\n    mse = np.mean((predicted_probabilities - np.array(labels)) ** 2)\n    predicted_probabilities = np.round(predicted_probabilities, 4).tolist()\n    mse = np.round(mse, 4)\n    return (predicted_probabilities, mse)\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (3,) (3,2) ", "solution": "import numpy as np\nimport numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Train a single neuron with sigmoid activation using backpropagation.\n\n    Parameters:\n    features (np.ndarray): A 2D array of feature vectors.\n    labels (np.ndarray): A 1D array of true binary labels.\n    initial_weights (np.ndarray): A 1D array of initial weights.\n    initial_bias (float): The initial bias value.\n    learning_rate (float): The learning rate for gradient descent.\n    epochs (int): The number of epochs to train the neuron.\n\n    Returns:\n    A tuple containing the updated weights, bias, and a list of MSE values for each epoch.\n    \"\"\"\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        predictions = 1 / (1 + np.exp(-np.dot(features, weights) - bias))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n        d_loss_dw = 2 * (predictions - labels) * predictions * (1 - predictions) * features\n        d_loss_db = 2 * (predictions - labels) * predictions * (1 - predictions)\n        weights -= learning_rate * np.mean(d_loss_dw, axis=0)\n        bias -= learning_rate * np.mean(d_loss_db)\n    return (weights, bias, mse_values)\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": false, "result": "failed: '>=' not supported between instances of 'Value' and 'int'", "solution": "\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def backward(self, grad=1):\n        if self.grad == 0:\n            return\n        for child in self._prev:\n            child.backward(grad * child.grad)\n\n    def add(self, other):\n        other = Value(other, _children=(self,), _op='add')\n        return other\n\n    def mul(self, other):\n        other = Value(other, _children=(self,), _op='mul')\n        return other\n\n    def relu(self):\n        return self.where(self >= 0)\n\n    def where(self, condition):\n        mask = Value(condition, _children=(self,), _op='where')\n        return mask * self\n\n    def __add__(self, other):\n        return self.add(other)\n\n    def __mul__(self, other):\n        return self.mul(other)\n\n    def __truediv__(self, other):\n        return self.div(other)\n\n    def div(self, other):\n        other = Value(other, _children=(self,), _op='div')\n        return self.mul(other.inv())\n\n    def inv(self):\n        return self.pow(-1)\n\n    def pow(self, exponent):\n        return self.pow_op(exponent)\n\n    def pow_op(self, exponent):\n        return Value(self.data ** exponent, _children=(self,), _op='pow')\n\n    def __radd__(self, other):\n        return self.add(other)\n\n    def __rmul__(self, other):\n        return self.mul(other)\n\n    def __rtruediv__(self, other):\n        return self.div(other)\n\n    def __neg__(self):\n        return self.mul(-1)\n\n    def __pos__(self):\n        return self\n\n    def __abs__(self):\n        return self.relu()\n\n    def __str__(self):\n        return f'Value({self.data})'\n\n    def __repr__(self):\n        return f'Value({self.data})'\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P from basis B to C.\n\n    Parameters:\n    B (list[list[int]]): Basis vectors in basis B\n    C (list[list[int]]): Basis vectors in basis C\n\n    Returns:\n    list[list[float]]: Transformation matrix P from basis B to C\n    \"\"\"\n    B = np.array(B)\n    C = np.array(C)\n    C_inv = np.linalg.inv(C)\n    P = np.dot(C_inv, B)\n    P = np.round(P, 4)\n    P = P.tolist()\n    return P\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n\n    Parameters:\n    A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n    tuple: A tuple containing the matrices U, S, and V such that A = U * S * V.\n    \"\"\"\n    (eigenvalues, eigenvectors) = np.linalg.eig(np.dot(A.T, A))\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    s = np.sqrt(eigenvalues)\n    u1 = A @ eigenvectors[:, 0] / s[0]\n    u2 = A @ eigenvectors[:, 1] / s[1]\n    v1 = eigenvectors[:, 0]\n    v2 = eigenvectors[:, 1]\n    U = np.array([u1, u2]).T\n    S = np.array([[s[0], 0], [0, s[1]]])\n    V = np.array([v1, v2]).T\n    return (U.tolist(), S.tolist(), V.tolist())\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle the samples in two numpy arrays, X and y, \n    while maintaining the corresponding order between them.\n\n    Parameters:\n    X (numpy array): The input data.\n    y (numpy array): The target data.\n    seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n    list: A list of tuples, where each tuple contains a shuffled sample from X and y.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    shuffled_data = np.column_stack((X_shuffled, y_shuffled)).tolist()\n    return shuffled_data\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Batch iterator for dataset.\n\n    Args:\n        X (numpy array): Input data.\n        y (numpy array, optional): Target data. Defaults to None.\n        batch_size (int, optional): Batch size. Defaults to 64.\n\n    Yields:\n        list: Batches of (X, y) pairs or X only.\n    \"\"\"\n    if y is not None:\n        assert X.shape[0] == y.shape[0], 'X and y must have the same number of samples'\n        for i in range(0, X.shape[0], batch_size):\n            batch_X = X[i:i + batch_size]\n            batch_y = y[i:i + batch_size]\n            yield (batch_X.tolist(), batch_y.tolist())\n    else:\n        for i in range(0, X.shape[0], batch_size):\n            batch_X = X[i:i + batch_size]\n            yield batch_X.tolist()\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide a dataset into two subsets based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n    Parameters:\n    X (numpy array): The input dataset.\n    feature_i (int): The index of the feature to divide on.\n    threshold (float): The threshold value.\n\n    Returns:\n    tuple: Two subsets of the dataset: one with samples that meet the condition and another with samples that do not.\n    \"\"\"\n    idx_above_threshold = np.where(X[:, feature_i] >= threshold)[0]\n    idx_below_threshold = np.where(X[:, feature_i] < threshold)[0]\n    above_threshold = X[idx_above_threshold]\n    below_threshold = X[idx_below_threshold]\n    return (above_threshold.tolist(), below_threshold.tolist())\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n\n    Parameters:\n    X (2D numpy array): Input dataset.\n    degree (int): Degree of polynomial features to generate.\n\n    Returns:\n    2D numpy array: New dataset with polynomial features up to the specified degree.\n    \"\"\"\n    out = np.hstack((X, np.ones((X.shape[0], 1))))\n    for d in range(2, degree + 1):\n        combos = list(combinations_with_replacement(range(X.shape[1]), d))\n        for combo in combos:\n            poly_feature = np.prod(X[:, combo], axis=1)\n            out = np.hstack((out, poly_feature[:, np.newaxis]))\n    return out.tolist()\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (5,2) (2,2) ", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (2D numpy array): The feature dataset.\n    y (1D numpy array): The target variable.\n    n_subsets (int): The number of random subsets to generate.\n    replacements (bool, optional): Whether to create subsets with replacements. Defaults to True.\n    seed (int, optional): The random seed. Defaults to 42.\n\n    Returns:\n    list: A list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            idx = np.random.randint(0, n_samples, size=X.shape[0])\n        else:\n            idx = np.random.choice(n_samples, size=X.shape[0], replace=False)\n        X_subset = X[idx]\n        y_subset = y[idx]\n        subsets.append((X_subset, y_subset))\n    return [tuple(subset) for subset in subsets]\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of nominal values.\n\n    Parameters:\n    x (numpy array): 1D array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. If not provided, it will be automatically determined from the input array.\n\n    Returns:\n    list: One-hot encoded array as a list.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    one_hot = one_hot.reshape(-1, n_col)\n    one_hot = one_hot.tolist()\n    return one_hot\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n\n    Parameters:\n    x (numpy.ndarray): The input 1D numpy array.\n\n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.zeros((len(x), len(x)))\n    np.fill_diagonal(diagonal_matrix, x)\n    return diagonal_matrix.tolist()\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels.\n    y_pred (numpy.ndarray): 1D array of predicted labels.\n\n    Returns:\n    float: Accuracy score, ranging from 0 to 1.\n    \"\"\"\n    assert y_true.shape == y_pred.shape, 'y_true and y_pred must have the same shape'\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = y_true.shape[0]\n    accuracy = correct_predictions / total_predictions\n    return accuracy\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Parameters:\n    X (2D numpy array): The input dataset.\n    Y (2D numpy array, optional): The second dataset. If not provided, the function calculates the correlation matrix of X with itself.\n\n    Returns:\n    list: The correlation matrix as a 2D list.\n    \"\"\"\n    if Y is None:\n        correlation_matrix = np.corrcoef(X.T)\n    else:\n        correlation_matrix = np.corrcoef(X, Y.T)\n    correlation_matrix = np.round(correlation_matrix, 4)\n    correlation_matrix = correlation_matrix.tolist()\n    return correlation_matrix\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    This function implements the fit method for an AdaBoost classifier.\n\n    Parameters:\n    X (2D numpy array): The dataset of shape (n_samples, n_features)\n    y (1D numpy array): The labels of shape (n_samples,)\n    n_clf (int): The number of classifiers\n\n    Returns:\n    list: A list of classifiers with their parameters\n    \"\"\"\n    n_samples = X.shape[0]\n    weights = np.full(n_samples, 1.0 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        best_feature = None\n        best_threshold = None\n        best_error = 1.0\n        for feature in range(X.shape[1]):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                predictions = np.where(X[:, feature] < threshold, 0, 1)\n                error = np.sum(weights * (predictions != y)) / np.sum(weights)\n                if error < best_error:\n                    best_feature = feature\n                    best_threshold = threshold\n                    best_error = error\n        predictions = np.where(X[:, best_feature] < best_threshold, 0, 1)\n        error = np.sum(weights * (predictions != y)) / np.sum(weights)\n        alpha = 0.5 * math.log((1 - error) / error)\n        weights *= np.exp(-alpha * y * predictions)\n        weights /= np.sum(weights)\n        classifiers.append({'feature': best_feature, 'threshold': best_threshold, 'alpha': alpha})\n    return classifiers\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a 1D numpy array of scores.\n\n    Args:\n        scores (list): A 1D list of scores.\n\n    Returns:\n        list: The log-softmax of the input scores, rounded to the nearest 4th decimal.\n    \"\"\"\n    scores = np.array(scores)\n    log_softmax_scores = np.log(scores / np.sum(np.exp(scores)))\n    log_softmax_scores = log_softmax_scores.tolist()\n    log_softmax_scores = [round(x, 4) for x in log_softmax_scores]\n    return log_softmax_scores\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: Dense.initialize() takes 1 positional argument but 2 were given", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nimport numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        super().__init__()\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self):\n        if self.input_shape is None:\n            raise ValueError('Input shape must be provided for Dense layer')\n        self.W = np.random.uniform(-1 / math.sqrt(self.input_shape[0]), 1 / math.sqrt(self.input_shape[0]), (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        dW = np.dot(self.layer_input.T, accum_grad)\n        dw0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.optimizer_W is not None:\n            self.optimizer_W.update(self.W, dW)\n            self.optimizer_w0.update(self.w0, dw0)\n        return np.dot(accum_grad, self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)\n\n    def set_optimizer(self, optimizer_W, optimizer_w0):\n        self.optimizer_W = optimizer_W\n        self.optimizer_w0 = optimizer_w0\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (2,0) (2,2) ", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    This function implements a simple 2D convolutional layer.\n    \n    Parameters:\n    input_matrix (np.ndarray): The input matrix to be convolved.\n    kernel (np.ndarray): The convolutional kernel.\n    padding (int): The amount of padding to be applied to the input matrix.\n    stride (int): The stride of the convolutional operation.\n    \n    Returns:\n    list: The output of the convolutional operation, reshaped into a list.\n    \"\"\"\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            top = i * stride\n            left = j * stride\n            sub_matrix = input_matrix[top:top + kernel_height, left:left + kernel_width]\n            output_matrix[i, j] = np.sum(sub_matrix * kernel)\n    if padding > 0:\n        output_matrix = np.pad(output_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    output_list = output_matrix.tolist()\n    return output_list\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef relu(z: float) -> float:\n    \"\"\"\n    Rectified Linear Unit (ReLU) activation function.\n\n    Args:\n        z (float): Input value.\n\n    Returns:\n        float: The output of the ReLU function.\n    \"\"\"\n    if z > 0:\n        return z\n    else:\n        return 0\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss.\n\n    Parameters:\n    X (np.ndarray): 2D numpy array representing the feature matrix.\n    w (np.ndarray): 1D numpy array representing the coefficients.\n    y_true (np.ndarray): 1D numpy array representing the true labels.\n    alpha (float): regularization parameter.\n\n    Returns:\n    float: Ridge loss.\n    \"\"\"\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_pred - y_true) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    loss = mse + reg_term\n    return round(loss, 4)\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Args:\n        z (float): Input value.\n        alpha (float, optional): Slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        float|int: The value after applying the Leaky ReLU function.\n    \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n\n    The linear kernel is defined as the dot product (inner product) of two vectors.\n\n    Parameters:\n    x1 (numpy array): The first input vector.\n    x2 (numpy array): The second input vector.\n\n    Returns:\n    float: The linear kernel value between x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given two numpy arrays: y_true and y_pred.\n\n    Precision is defined as the ratio of true positives to the sum of true positives and false positives.\n\n    Parameters:\n    y_true (numpy array): The true binary labels.\n    y_pred (numpy array): The predicted binary labels.\n\n    Returns:\n    float: The precision metric.\n    \"\"\"\n    assert y_true.shape == y_pred.shape, 'Input arrays must have the same shape'\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    precision_metric = tp / (tp + fp)\n    return precision_metric\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'tolist'", "solution": "import numpy as np\nimport numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    This function performs three variants of gradient descent (SGD, Batch GD, Mini-Batch GD) \n    using Mean Squared Error (MSE) as the loss function.\n\n    Parameters:\n    X (numpy array): Input features\n    y (numpy array): Target variable\n    weights (numpy array): Initial weights\n    learning_rate (float): Learning rate for gradient descent\n    n_iterations (int): Number of iterations\n    batch_size (int, optional): Batch size for Mini-Batch GD. Defaults to 1.\n    method (str, optional): Method to use. Can be 'sgd', 'batch', or 'mini-batch'. Defaults to 'batch'.\n\n    Returns:\n    list: List of weights after each iteration, rounded to the nearest 4th decimal.\n    \"\"\"\n    weights_history = [weights]\n    for _ in range(n_iterations):\n        predictions = np.dot(X, weights)\n        loss = np.mean((predictions - y) ** 2)\n        gradients = 2 * np.dot(X.T, predictions - y) / X.shape[0]\n        if method == 'sgd':\n            weights -= learning_rate * gradients\n        elif method == 'batch':\n            weights -= learning_rate * gradients\n        elif method == 'mini-batch':\n            mini_batches = np.array_split(X, batch_size)\n            mini_batch_gradients = []\n            for mini_batch in mini_batches:\n                mini_batch_gradients.append(2 * np.dot(mini_batch.T, np.dot(mini_batch, weights) - y[:len(mini_batch)]) / len(mini_batch))\n            gradients = np.mean(mini_batch_gradients, axis=0)\n            weights -= learning_rate * gradients\n        else:\n            raise ValueError(\"Invalid method. Please choose 'sgd', 'batch', or 'mini-batch'.\")\n        weights_history.append(weights)\n    weights_history = [np.round(weight, 4) for weight in weights_history]\n    return weights_history.tolist()\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: attempt to get argmax of an empty sequence", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert a given matrix into its Reduced Row Echelon Form (RREF).\n\n    Args:\n        matrix (numpy.ndarray): The input matrix.\n\n    Returns:\n        list: The RREF of the input matrix.\n    \"\"\"\n    matrix = np.array(matrix, dtype=float)\n    (num_rows, num_cols) = matrix.shape\n    current_row = 0\n    current_col = 0\n    while current_col < num_cols:\n        max_row = np.argmax(np.abs(matrix[current_row:, current_col])) + current_row\n        if max_row != current_row:\n            matrix[[current_row, max_row]] = matrix[[max_row, current_row]]\n        if not np.all(matrix[current_row, current_col:] == 0):\n            matrix[current_row, current_col] = 1\n            for i in range(num_rows):\n                if i != current_row:\n                    matrix[i, current_col:] -= matrix[i, current_col] * matrix[current_row, current_col:]\n            current_row += 1\n            current_col += 1\n        else:\n            current_col += 1\n    matrix = matrix[~np.all(matrix == 0, axis=1)]\n    return matrix.tolist()\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Adam optimization algorithm.\n\n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): A function that computes the gradient of f.\n    x0 (numpy array): Initial parameter values.\n    learning_rate (float, optional): The step size. Defaults to 0.001.\n    beta1 (float, optional): Exponential decay rate for the first moment estimates. Defaults to 0.9.\n    beta2 (float, optional): Exponential decay rate for the second moment estimates. Defaults to 0.999.\n    epsilon (float, optional): A small constant for numerical stability. Defaults to 1e-8.\n    num_iterations (int, optional): Number of iterations to run the optimizer. Defaults to 1000.\n\n    Returns:\n    numpy array: The optimized parameters.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    t = 0\n    for _ in range(num_iterations):\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** (t + 1))\n        v_hat = v / (1 - beta2 ** (t + 1))\n        x0 = x0 - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n        t += 1\n    return x0.tolist()\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Lasso Regression using Gradient Descent.\n\n    Parameters:\n    X (np.array): Feature matrix.\n    y (np.array): Target vector.\n    alpha (float): Regularization parameter. Default is 0.1.\n    learning_rate (float): Learning rate for gradient descent. Default is 0.01.\n    max_iter (int): Maximum number of iterations. Default is 1000.\n    tol (float): Tolerance for convergence. Default is 1e-4.\n\n    Returns:\n    tuple: A tuple containing the optimized weights and bias.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    for _ in range(max_iter):\n        predictions = np.dot(X, weights) + bias\n        residuals = predictions - y\n        gradient_weights = 2 * np.dot(X.T, residuals) / n_samples + alpha * np.sign(weights)\n        gradient_bias = 2 * np.sum(residuals) / n_samples\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n        if np.linalg.norm(gradient_weights) < tol and np.abs(gradient_bias) < tol:\n            break\n    weights = np.round(weights, 4).tolist()\n    bias = np.round(bias, 4)\n    return (weights, bias)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n    \n    The OSA distance represents the minimum number of edits required to transform one string into another.\n    The allowed edit operations are:\n    - Insert a character\n    - Delete a character\n    - Substitute a character\n    - Transpose two adjacent characters\n    \n    Each of these operations costs 1 unit.\n    \n    Parameters:\n    source (str): The source string\n    target (str): The target string\n    \n    Returns:\n    int: The minimum number of edits needed to convert the source string into the target string\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i, 0] = i\n    for j in range(n + 1):\n        dp[0, j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i, j] = dp[i - 1, j - 1]\n            else:\n                dp[i, j] = 1 + min(dp[i - 1, j], dp[i, j - 1], dp[i - 1, j - 1])\n    return dp[m, n]\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric in binary classification.\n\n    Parameters:\n    y_true (list): A list of true binary labels (0 or 1) for the dataset.\n    y_pred (list): A list of predicted binary labels (0 or 1) from the model.\n\n    Returns:\n    float: The recall value rounded to three decimal places.\n    \"\"\"\n    tp = np.sum(np.logical_and(y_true, y_pred))\n    fn = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    recall_value = tp / denominator\n    return round(recall_value, 3)\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": false, "result": "failed: self_attention() takes 3 positional arguments but 4 were given", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute the self-attention output.\n\n    Parameters:\n    Q (numpy array): Query matrix (n_samples, n_features)\n    K (numpy array): Key matrix (n_samples, n_features)\n    V (numpy array): Value matrix (n_samples, n_features)\n\n    Returns:\n    numpy array: Self-attention output (n_samples, n_features)\n    \"\"\"\n    scores = np.dot(Q, K.T)\n    scores_softmax = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    output = np.dot(scores_softmax, V)\n    return output.tolist()\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    This function implements a simple Recurrent Neural Network (RNN) cell.\n    \n    Parameters:\n    input_sequence (list[list[float]]): A list of input vectors, where each vector is a list of floats.\n    initial_hidden_state (list[float]): The initial hidden state of the RNN.\n    Wx (list[list[float]]): The weight matrix for input-to-hidden connections.\n    Wh (list[list[float]]): The weight matrix for hidden-to-hidden connections.\n    b (list[float]): The bias vector.\n    \n    Returns:\n    list[float]: The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_state = np.array(initial_hidden_state)\n    for input_vector in input_sequence:\n        input_to_hidden = np.dot(input_vector, Wx) + np.dot(hidden_state, Wh) + b\n        hidden_state = np.tanh(input_to_hidden)\n    return hidden_state.tolist()\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": false, "result": "failed: shapes (3,3) and (2,3) not aligned: 3 (dim 1) != 2 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Args:\n        points (list): A list of [x, y] coordinates.\n        tx (float): The translation distance in the x direction.\n        ty (float): The translation distance in the y direction.\n\n    Returns:\n        list: A new list of points after applying the translation matrix.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    translated_points = np.dot(translation_matrix, points_array.T).T\n    translated_points_list = translated_points.tolist()\n    return translated_points_list\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first normal distribution.\n    sigma_p (float): Standard deviation of the first normal distribution.\n    mu_q (float): Mean of the second normal distribution.\n    sigma_q (float): Standard deviation of the second normal distribution.\n\n    Returns:\n    float: KL divergence between the two normal distributions.\n    \"\"\"\n    log_likelihood_ratio = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    kl_divergence = np.log(sigma_p / sigma_q) + (sigma_q ** 2 + (mu_q - mu_p) ** 2) / (2 * sigma_p ** 2) - 0.5\n    return kl_divergence\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Gauss-Seidel Method for Solving Linear Systems\n\n    Parameters:\n    A (numpy array): Square matrix of coefficients\n    b (numpy array): Right-hand side vector\n    n (int): Number of iterations\n    x_ini (numpy array, optional): Initial guess for x (default is a vector of zeros)\n\n    Returns:\n    x (list): Approximated solution vector after n iterations\n    \"\"\"\n    assert A.shape[0] == A.shape[1], 'A must be a square matrix'\n    assert A.shape[0] == b.shape[0], 'A and b must have the same number of rows'\n    if x_ini is None:\n        x = np.zeros(A.shape[0])\n    else:\n        x = x_ini\n    for _ in range(n):\n        x_new = np.zeros(A.shape[0])\n        for i in range(A.shape[0]):\n            sum_val = np.sum([A[i, j] * x[j] for j in range(A.shape[0]) if j != i])\n            x_new[i] = (b[i] - sum_val) / A[i, i]\n        x = x_new\n    return x.tolist()\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Gaussian Elimination with partial pivoting to solve the system (Ax = b).\n\n    Parameters:\n    A (numpy array): Coefficient matrix\n    b (numpy array): Right-hand side vector\n\n    Returns:\n    x (list): Solution vector\n    \"\"\"\n    n = len(b)\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    augmented_matrix = np.hstack((A, b[:, None]))\n    for i in range(n):\n        max_row = np.argmax(np.abs(augmented_matrix[i:, i])) + i\n        if max_row != i:\n            augmented_matrix[[i, max_row]] = augmented_matrix[[max_row, i]]\n        pivot = augmented_matrix[i, i]\n        augmented_matrix[i] = augmented_matrix[i] / pivot\n        for j in range(n):\n            if j != i:\n                factor = augmented_matrix[j, i]\n                augmented_matrix[j] = augmented_matrix[j] - factor * augmented_matrix[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = augmented_matrix[i, n] / augmented_matrix[i, i]\n        for j in range(i - 1, -1, -1):\n            augmented_matrix[j, n] -= augmented_matrix[j, i] * x[i]\n    x = np.round(x, 4).tolist()\n    return x\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": false, "result": "failed: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)", "solution": "import numpy as np\nimport numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        cell_states = []\n        final_hidden_state = initial_hidden_state\n        final_cell_state = initial_cell_state\n        for i in range(x.shape[0]):\n            input_t = x[i]\n            forget_gate = self.sigmoid(np.dot(self.Wf, np.concatenate((input_t, final_hidden_state))) + self.bf)\n            input_gate = self.sigmoid(np.dot(self.Wi, np.concatenate((input_t, final_hidden_state))) + self.bi)\n            candidate_cell_state = self.tanh(np.dot(self.Wc, np.concatenate((input_t, final_hidden_state))) + self.bc)\n            output_gate = self.sigmoid(np.dot(self.Wo, np.concatenate((input_t, final_hidden_state))) + self.bo)\n            cell_state = forget_gate * final_cell_state + input_gate * candidate_cell_state\n            hidden_state = output_gate * self.tanh(cell_state)\n            hidden_states.append(hidden_state)\n            cell_states.append(cell_state)\n            final_hidden_state = hidden_state\n            final_cell_state = cell_state\n        hidden_states = np.array(hidden_states).T\n        cell_states = np.array(cell_states).T\n        return [hidden_states.tolist(), cell_states.tolist(), final_hidden_state.tolist(), final_cell_state.tolist()]\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a given corpus of documents.\n\n    Parameters:\n    corpus (list): A list of documents, where each document is a list of words.\n    query (list): A list of words for which you want to compute the TF-IDF scores.\n\n    Returns:\n    list: A list of lists containing the TF-IDF scores for the query words in each document, rounded to five decimal places.\n    \"\"\"\n    if not corpus:\n        raise ValueError('Corpus cannot be empty')\n    num_docs = len(corpus)\n    df = {}\n    for doc in corpus:\n        for term in set(doc):\n            if term not in df:\n                df[term] = 0\n            df[term] += 1\n    idf = {}\n    for term in query:\n        if term not in df:\n            idf[term] = np.log((num_docs + 1) / 1)\n        else:\n            idf[term] = np.log(num_docs / (df[term] + 1))\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_tf_idf = []\n        for term in query:\n            tf = doc.count(term) / len(doc)\n            tf_idf = tf * idf[term]\n            doc_tf_idf.append(tf_idf)\n        tf_idf_scores.append(doc_tf_idf)\n    return [np.round(np.array(doc_tf_idf).tolist(), 5) for doc_tf_idf in tf_idf_scores]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    precision = TP / (TP + FP) if TP + FP != 0 else 0\n    recall = TP / (TP + FN) if TP + FN != 0 else 0\n    f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: name 'SimpleRNN' is not defined", "solution": "import numpy as np\nimport numpy as np\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    assert np.allclose(A, A.T), 'Matrix A must be symmetric'\n    assert np.all(np.linalg.eigvals(A) > 0), 'Matrix A must be positive-definite'\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    r = b - np.dot(A, x)\n    p = r\n    i = 0\n    while i < n:\n        r_dot_r = np.dot(r, r)\n        p_dot_Ap = np.dot(p, np.dot(A, p))\n        alpha = r_dot_r / p_dot_Ap\n        x += alpha * p\n        r -= alpha * np.dot(A, p)\n        r_dot_r_new = np.dot(r, r)\n        if r_dot_r_new < tol * r_dot_r:\n            break\n        p_dot_Ap_new = np.dot(r, np.dot(A, r))\n        p = r + r_dot_r_new / p_dot_Ap_new * p\n        i += 1\n    return x.tolist()\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    n = len(y)\n    gini = 0\n    for label in set(y):\n        p = np.sum(np.array(y) == label) / n\n        gini += p * (1 - p)\n    return round(gini, 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    column_indices = []\n    row_pointers = [0] * num_rows\n    for i in range(num_rows):\n        for j in range(num_cols):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                column_indices.append(j)\n                row_pointers[i] += 1\n    return (values, column_indices, row_pointers)\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    dot_product = np.dot(v, L)\n    magnitude_squared = np.dot(L, L)\n    coefficient = dot_product / magnitude_squared\n    projection = coefficient * L\n    projection = np.round(projection, 3)\n    return projection.tolist()\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer[col + 1] = column_pointer[col] + len(values) - column_pointer[col]\n    return (values, row_indices, column_pointer)\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": false, "result": "failed: type list doesn't define __round__ method", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Compute the column space of a matrix A.\n\n    The column space, also known as the image or span, consists of all linear combinations of the columns of A.\n    This function identifies independent columns that span the matrix's image.\n\n    Parameters:\n    A (numpy array): The input matrix.\n\n    Returns:\n    list: A list of basis vectors that span the column space of A.\n    \"\"\"\n    A_row_echelon = np.array(np.linalg.lstsq(A, np.eye(A.shape[1]), rcond=None)[0]).T\n    independent_columns = A_row_echelon[:, np.where(np.all(A_row_echelon, axis=0))[0]]\n    basis_vectors = [round(vector.tolist(), 8) for vector in independent_columns]\n    return basis_vectors\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for regression analysis.\n\n    Parameters:\n    y_true (array-like): Array of true values.\n    y_pred (array-like): Array of predicted values.\n\n    Returns:\n    float: R-squared value rounded to three decimal places.\n    \"\"\"\n    mean_y_true = np.mean(y_true)\n    ss_tot = np.sum((y_true - mean_y_true) ** 2)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Args:\n        img (list of lists): A 2D matrix representing the image, where each element is a pixel value between 0 (black) and 255 (white).\n\n    Returns:\n        float: The average brightness of the image rounded to two decimal places. Returns -1 for edge cases.\n    \"\"\"\n    if not img:\n        return -1\n    row_lengths = [len(row) for row in img]\n    if len(set(row_lengths)) > 1:\n        return -1\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n    total_brightness = sum((sum(row) for row in img))\n    average_brightness = total_brightness / (len(img) * len(img[0]))\n    return round(average_brightness, 2)\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Parameters:\n    y_true (array-like): Actual values.\n    y_pred (array-like): Predicted values.\n\n    Returns:\n    float: RMSE value rounded to three decimal places.\n\n    Raises:\n    ValueError: If input arrays have mismatched shapes or are empty.\n    TypeError: If input arrays are not of type numpy.ndarray.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Input arrays must be of type numpy.ndarray.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    differences = y_true - y_pred\n    squared_differences = differences ** 2\n    mean_squared_error = np.mean(squared_differences)\n    rmse_value = np.sqrt(mean_squared_error)\n    rmse_value = round(rmse_value, 3)\n    return rmse_value\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): True labels.\n    y_pred (numpy.ndarray): Predicted labels.\n\n    Returns:\n    float: The Jaccard Index, rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if not np.all(np.logical_or(y_true == 0, y_true == 1)) or not np.all(np.logical_or(y_pred == 0, y_pred == 1)):\n        raise ValueError('Inputs must be binary arrays')\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        jaccard_index = 1.0\n    else:\n        jaccard_index = intersection / union\n    return round(jaccard_index, 3)\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": false, "result": "failed: Inputs must be binary arrays", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n\n    Parameters:\n    y_true (numpy array): True labels.\n    y_pred (numpy array): Predicted labels.\n\n    Returns:\n    float: Dice Score as a float value rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if not (np.all(y_true == 0) or np.all(y_true == 1) or np.all(y_pred == 0) or np.all(y_pred == 1)):\n        raise ValueError('Inputs must be binary arrays')\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    elif sum_true == 0 or sum_pred == 0:\n        return 0.0\n    dice = 2 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for +: 'int' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Args:\n    - row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n\n    Returns:\n    - A composite hypervector representing the entire row, converted to a list using tolist().\n    \"\"\"\n    feature_hvs = []\n    for (feature, value) in row.items():\n        feature_seed = random_seeds.get(feature)\n        if feature_seed is None:\n            feature_seed = np.random.randint(0, 2 ** 32)\n        feature_name_hv = np.zeros(dim)\n        feature_name_hv[feature_seed % dim] = 1\n        feature_value_hv = np.zeros(dim)\n        feature_value_hv[(feature_seed + value) % dim] = 1\n        feature_hv = np.logical_or(feature_name_hv, feature_value_hv).astype(int)\n        feature_hvs.append(feature_hv)\n    row_hv = np.concatenate(feature_hvs)\n    return row_hv.tolist()\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for a binary classification problem.\n\n    Args:\n        data (list of lists): A list of pairs [y_true, y_pred] for each observation.\n\n    Returns:\n        list of lists: A 2x2 confusion matrix.\n    \"\"\"\n    (tp, tn, fp, fn) = (0, 0, 0, 0)\n    for (y_true, y_pred) in data:\n        if y_true == y_pred == 1:\n            tp += 1\n        elif y_true == y_pred == 0:\n            tn += 1\n        elif y_true == 1 and y_pred == 0:\n            fn += 1\n        elif y_true == 0 and y_pred == 1:\n            fp += 1\n    matrix = [[tp, fp], [fn, tn]]\n    return matrix\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Args:\n    v1 (numpy.ndarray): The first input vector.\n    v2 (numpy.ndarray): The second input vector.\n\n    Returns:\n    float: The cosine similarity between the two input vectors, rounded to three decimal places.\n\n    Raises:\n    ValueError: If the input vectors have different shapes or are empty or have zero magnitude.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    cosine_similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    cosine_similarity = round(cosine_similarity, 3)\n    return cosine_similarity\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple:\n    \"\"\"\n    Calculate performance metrics for a binary classification model.\n\n    Args:\n    - actual (List[int]): The actual class labels (1 for positive, 0 for negative).\n    - predicted (List[int]): The predicted class labels from the model.\n\n    Returns:\n    - A tuple containing:\n        - confusion_matrix (List[List[int]]): A 2x2 matrix.\n        - accuracy (float): The accuracy of the model.\n        - f1_score (float): The F1 score of the model.\n        - specificity (float): The specificity of the model.\n        - negative_predictive_value (float): The negative predictive value.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('The actual and predicted lists must have the same length.')\n    if not all((x in [0, 1] for x in actual)) or not all((x in [0, 1] for x in predicted)):\n        raise ValueError('All elements in the actual and predicted lists must be either 0 or 1.')\n    confusion_matrix = [[0, 0], [0, 0]]\n    for (actual_value, predicted_value) in zip(actual, predicted):\n        confusion_matrix[actual_value][predicted_value] += 1\n    accuracy = (confusion_matrix[0][0] + confusion_matrix[1][1]) / len(actual)\n    true_positives = confusion_matrix[1][1]\n    false_positives = confusion_matrix[1][0]\n    false_negatives = confusion_matrix[0][1]\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    specificity = confusion_matrix[0][0] / (confusion_matrix[0][0] + confusion_matrix[0][1]) if confusion_matrix[0][0] + confusion_matrix[0][1] > 0 else 0\n    negative_predictive_value = confusion_matrix[0][0] / (confusion_matrix[0][0] + false_negatives) if confusion_matrix[0][0] + false_negatives > 0 else 0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n\n    Parameters:\n    data (list or NumPy array): A list or NumPy array of numerical values.\n\n    Returns:\n    dict: A dictionary containing mean, median, mode, variance, standard deviation,\n          percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = np.bincount(data.astype(np.int64)).argmax()\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentiles = np.percentile(data, [25, 50, 75])\n    iqr = np.subtract(*np.percentile(data, [75, 25]))\n    result = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentiles[0], 4), '50th_percentile': round(percentiles[1], 4), '75th_percentile': round(percentiles[2], 4), 'interquartile_range': round(iqr, 4)}\n    return result\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    if not (isinstance(n, int) and isinstance(k, int) and isinstance(p, (int, float))):\n        raise ValueError('Input values must be integers or floats')\n    if n < 0 or k < 0 or p < 0 or (p > 1):\n        raise ValueError('Input values must be non-negative and p must be between 0 and 1')\n    if k > n:\n        raise ValueError('k cannot be greater than n')\n    probability = math.comb(n, k) * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf = coefficient * math.exp(exponent)\n    pdf = round(pdf, 5)\n    return pdf\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError('k must be a non-negative integer')\n    if lam < 0:\n        raise ValueError('lam must be a non-negative number')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or img.ndim != 2:\n        raise ValueError('Input must be a 2D numpy array')\n    if np.any(img < 0) or np.any(img > 255):\n        raise ValueError('Image pixel values must be between 0 and 255')\n    contrast = np.max(img) - np.min(img)\n    return contrast\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two input vectors.\n    \"\"\"\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise ValueError('Both inputs must be 1D NumPy arrays')\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D arrays')\n    if len(vec1) != len(vec2):\n        raise ValueError('Both vectors must have the same length')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    result = [[round(x, 8) for x in data]]\n    for i in range(1, degree + 1):\n        new_features = []\n        for j in range(len(data)):\n            new_features.append([round(data[j] ** k, 8) for k in range(i + 1)])\n        result.append(new_features)\n    result = list(map(list, zip(*result)))\n    return result\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate positional encodings for a sequence length and model dimensionality.\n\n    Args:\n    position (int): The sequence length.\n    d_model (int): The model dimensionality.\n\n    Returns:\n    list: A list of positional encodings of dtype np.float16.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return [-1]\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for i in range(position):\n        for j in range(d_model // 2):\n            pe[i, 2 * j] = np.sin(i / 10000 ** (2 * j / d_model))\n            pe[i, 2 * j + 1] = np.cos(i / 10000 ** (2 * j / d_model))\n    return pe.tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m_hat = beta1 * m + (1 - beta1) * grad\n    v_hat = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat_corrected = m_hat / (1 - beta1 ** t)\n    v_hat_corrected = v_hat / (1 - beta2 ** t)\n    update_step = learning_rate * m_hat_corrected / (np.sqrt(v_hat_corrected) + epsilon)\n    updated_parameter = parameter - update_step\n    updated_m = beta1 * m + (1 - beta1) * grad\n    updated_v = beta2 * v + (1 - beta2) * grad ** 2\n    return (np.round(updated_parameter, 5).tolist(), np.round(updated_m, 5).tolist(), np.round(updated_v, 5).tolist())\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: shapes (1,) and (3,10) not aligned: 1 (dim 0) != 3 (dim 0)", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\nimport numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    token_embeddings = np.dot(token_ids, params['wte'])\n    position_ids = np.arange(len(token_ids))\n    position_embeddings = np.dot(position_ids, params['wpe'])\n    embeddings = token_embeddings + position_embeddings\n    attention_weights = np.random.rand(len(token_ids), len(token_ids))\n    attention_output = np.dot(attention_weights, embeddings)\n    output = np.dot(attention_output, params['wpe'].T)\n    output = (output - np.mean(output, axis=0)) / np.std(output, axis=0)\n    new_token_ids = []\n    for _ in range(n_tokens_to_generate):\n        next_token_probabilities = np.dot(output, params['wte'].T)\n        next_token_id = np.argmax(next_token_probabilities)\n        new_token_ids.append(next_token_id)\n        output = np.dot(np.eye(len(token_ids))[next_token_id], output)\n    generated_text = encoder.decode(new_token_ids)\n    return generated_text\nnp.random.seed(42)\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nnp.random.seed(42)\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nnp.random.seed(42)\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    This function calculates the self-attention scores for each crystal in the sequence\n    and returns the final weighted pattern for each crystal.\n\n    Parameters:\n    n (int): The number of crystals.\n    crystal_values (list): A list of unique numeric values for each crystal.\n    dimension (int): The dimension of the crystal values.\n\n    Returns:\n    list: A list of floats representing the final weighted pattern for each crystal.\n    \"\"\"\n\n    def softmax(values):\n        \"\"\"\n        This function calculates the softmax of a given list of values.\n\n        Parameters:\n        values (list): A list of numeric values.\n\n        Returns:\n        list: A list of softmax values.\n        \"\"\"\n        max_value = np.max(values)\n        shifted_values = [value - max_value for value in values]\n        exp_values = np.exp(shifted_values)\n        sum_exp_values = np.sum(exp_values)\n        softmax_values = [value / sum_exp_values for value in exp_values]\n        return softmax_values\n    attention_scores = []\n    for i in range(n):\n        crystal_attention_scores = []\n        for j in range(n):\n            if i == j:\n                continue\n            dot_product = np.dot(crystal_values[i], crystal_values[j])\n            attention_score = softmax([dot_product])[0]\n            crystal_attention_scores.append(attention_score)\n        attention_scores.append(crystal_attention_scores)\n    final_pattern = np.array(attention_scores).T.tolist()\n    final_pattern = [[round(score, 4) for score in pattern] for pattern in final_pattern]\n    return final_pattern\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'split'", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a query in a corpus.\n\n    Parameters:\n    corpus (list of str): A list of documents, where each document is a string.\n    query (str): The query string.\n    k1 (float, optional): The term frequency saturation parameter. Defaults to 1.5.\n    b (float, optional): The document length normalization parameter. Defaults to 0.75.\n\n    Returns:\n    list: A list of BM25 scores for each document in the corpus, rounded to three decimal places.\n    \"\"\"\n    query_terms = set(query.split())\n    corpus_terms = [set(doc.split()) for doc in corpus]\n    df = {term: sum((1 for doc_terms in corpus_terms if term in doc_terms)) for term in query_terms}\n    N = len(corpus)\n    n = {term: sum((1 for doc_terms in corpus_terms if term in doc_terms)) for term in set.union(*corpus_terms)}\n    scores = []\n    for doc in corpus:\n        doc_terms = set(doc.split())\n        doc_len = len(doc_terms)\n        score = 0\n        for term in query_terms:\n            if term in doc_terms:\n                tf = doc_terms.count(term)\n                idf = np.log((N - df[term] + 0.5) / (df[term] + 0.5))\n                score += tf * (k1 + 1) / (tf + k1 * (1 - b + b * doc_len / N)) * idf\n        scores.append(round(score, 3))\n    return scores\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    f1 = round(f1, 3)\n    return f1\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nPI = 3.14159\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = [x - 10 * math.sin(2 * PI * i / 10) for (i, x) in enumerate(consumption_data, start=1)]\n    import numpy as np\n    coefficients = np.polyfit(range(1, len(detrended_data) + 1), detrended_data, 1)\n    day_15_base_consumption = coefficients[0] * 15 + coefficients[1]\n    day_15_consumption = day_15_base_consumption + 10 * math.sin(2 * PI * 15 / 10)\n    final_consumption = math.ceil(day_15_consumption * 1.05)\n    return final_consumption\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    assert y_true.shape == y_pred.shape, 'Input arrays must have the same shape'\n    absolute_errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(absolute_errors)\n    mae_value = round(mae_value, 3)\n    return mae_value\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: multi_head_attention() takes 4 positional arguments but 5 were given", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(Q, K, V, n_heads):\n    \"\"\"\n    Compute self-attention from Q, K, V.\n\n    Args:\n        Q (numpy array): Query matrix with shape (m, n, n_heads)\n        K (numpy array): Key matrix with shape (m, n, n_heads)\n        V (numpy array): Value matrix with shape (m, n, n_heads)\n        n_heads (int): Number of attention heads\n\n    Returns:\n        output (numpy array): Output of self-attention with shape (m, n, n_heads)\n    \"\"\"\n    (m, n, _) = Q.shape\n    scores = np.matmul(Q, K.T) / np.sqrt(n)\n    scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    scores = scores / np.sum(scores, axis=-1, keepdims=True)\n    output = np.matmul(scores, V)\n    return output\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Compute multi-head attention from Q, K, V.\n\n    Args:\n        Q (numpy array): Query matrix with shape (m, n, n_heads)\n        K (numpy array): Key matrix with shape (m, n, n_heads)\n        V (numpy array): Value matrix with shape (m, n, n_heads)\n        n_heads (int): Number of attention heads\n\n    Returns:\n        output (numpy array): Output of multi-head attention with shape (m, n, n_heads)\n    \"\"\"\n    output = []\n    for i in range(n_heads):\n        Q_head = Q[:, :, i]\n        K_head = K[:, :, i]\n        V_head = V[:, :, i]\n        output_head = self_attention(Q_head, K_head, V_head, 1)\n        output.append(output_head)\n    output = np.concatenate(output, axis=-1)\n    return output\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length')\n    n = len(x)\n    x_ones = sum(x)\n    y_ones = sum(y)\n    xy_ones = sum((1 for (i, j) in zip(x, y) if i == 1 and j == 1))\n    phi = (n * xy_ones - x_ones * y_ones) / math.sqrt((n - x_ones) * (n - y_ones) * x_ones * y_ones)\n    return round(phi, 4)\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0\n    elif x > 2.5:\n        return 1\n    else:\n        return 0.2 * x + 0.5\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x < 0:\n        return alpha * x\n    else:\n        return x\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x >= 100:\n        return math.log(1 + math.exp(x - 100)) + 100\n    if x <= -100:\n        return math.log(1 + math.exp(-100 + x))\n    return math.log(1 + math.exp(x))\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    clipped_A = np.clip(A, -epsilon, epsilon)\n    kl_penalty = beta * np.sum(np.log(clipped_rhos) * pi_theta_old)\n    grpo = np.sum(clipped_A * clipped_rhos) - kl_penalty\n    return round(grpo, 6)\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": false, "result": "failed: test case failed: swish(1)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * np.tanh(np.log(1 + np.exp(x)))\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": false, "result": "failed: test case failed: selu(1.0)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_combination = np.dot(X, weights)\n    linear_combination += bias\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    predictions = np.where(probabilities >= 0.5, 1, 0)\n    predictions = predictions.tolist()\n    return predictions\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: shapes (10,5) and (10,5) not aligned: 5 (dim 1) != 10 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (C, M) = X.shape\n    B = np.zeros((C, M))\n    losses = []\n    for _ in range(iterations):\n        Z = np.dot(X, B)\n        exp_Z = np.exp(Z)\n        A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n        loss = -np.mean(np.log(A[np.arange(C), y]))\n        losses.append(round(loss, 4))\n        dZ = A.copy()\n        dZ[np.arange(C), y] -= 1\n        dZ /= C\n        dB = np.dot(X.T, dZ)\n        B -= learning_rate * dB\n    return (B.tolist(), losses)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n\n    Args:\n    X (np.ndarray): Feature matrix\n    y (np.ndarray): Target vector\n    learning_rate (float): Learning rate for gradient descent\n    iterations (int): Number of iterations for gradient descent\n\n    Returns:\n    tuple[list[float], list[float]]: Optimized coefficients and collected loss values over iterations\n    \"\"\"\n    coefficients = np.zeros(X.shape[1])\n    loss_values = []\n    for _ in range(iterations):\n        predictions = 1 / (1 + np.exp(-np.dot(X, coefficients)))\n        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n        loss_values.append(round(loss, 4))\n        gradients = np.dot(X.T, predictions - y) / X.shape[0]\n        coefficients -= learning_rate * gradients\n    return (coefficients.tolist(), loss_values)\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    attention_scores = np.dot(Q, K.T) / np.sqrt(Q.shape[1])\n    attention_scores = attention_scores * mask\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    output = output.tolist()\n    return output\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([1,1,0,0])", "solution": "\nimport math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    Args:\n    apples (list): A list of integers representing the colors of the apples.\n    \n    Returns:\n    float: A measure of disorder in the basket of apples.\n    \"\"\"\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    disorder_value = 0\n    for count in color_counts.values():\n        probability = count / total_apples\n        disorder_value -= probability * math.log2(probability)\n    disorder_value = round(disorder_value, 4)\n    return disorder_value\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": false, "result": "failed: type list doesn't define __round__ method", "solution": "import numpy as np\nimport numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    This function performs Layer Normalization on an input tensor.\n\n    Args:\n    X (np.ndarray): Input tensor with shape (batch_size, sequence_length, feature_dim)\n    gamma (np.ndarray): Scaling parameters with shape (feature_dim,)\n    beta (np.ndarray): Shifting parameters with shape (feature_dim,)\n    epsilon (float, optional): A small value to prevent division by zero. Defaults to 1e-5.\n\n    Returns:\n    list: The normalized tensor with shape (batch_size, sequence_length, feature_dim) rounded to 5 decimal places and converted to a list.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    std = np.sqrt(variance + epsilon)\n    normalized_X = (X - mean) / std\n    normalized_X = gamma * normalized_X + beta\n    normalized_X = normalized_X.tolist()\n    normalized_X = [[round(x, 5) for x in sequence] for sequence in normalized_X]\n    return normalized_X\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Parameters:\n    reference (str): The reference translation.\n    candidate (str): The candidate translation.\n    alpha (float, optional): The weight for unigram matches. Defaults to 0.9.\n    beta (float, optional): The weight for precision. Defaults to 3.\n    gamma (float, optional): The weight for word order fragmentation. Defaults to 0.5.\n\n    Returns:\n    float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    unigram_matches = len(set(ref_words) & set(cand_words))\n    total_ref_unigrams = len(ref_words)\n    total_cand_unigrams = len(cand_words)\n    precision = unigram_matches / total_cand_unigrams if total_cand_unigrams > 0 else 0\n    recall = unigram_matches / total_ref_unigrams if total_ref_unigrams > 0 else 0\n    f_mean = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n    fragmentation_penalty = 1 - (1 - gamma) * (1 - f_mean)\n    meteor_score = alpha * (unigram_matches / total_ref_unigrams) + beta * f_mean + gamma * fragmentation_penalty\n    return round(meteor_score, 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n\n    Parameters:\n    joint_counts (int): The joint occurrence count of two events.\n    total_counts_x (int): The total count of event X.\n    total_counts_y (int): The total count of event Y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The Pointwise Mutual Information (PMI) value rounded to 3 decimal places.\n    \"\"\"\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    This function performs Min-Max Normalization on a list of integers, \n    scaling all values to the range [0, 1].\n    \n    Args:\n    x (list[int]): A list of integers to be normalized.\n    \n    Returns:\n    list[float]: A list of floats representing the normalized values, \n    rounded to 4 decimal places.\n    \"\"\"\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    normalized = [round(val, 4) for val in normalized]\n    return normalized\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    This function implements a simple residual block using NumPy.\n    \n    Args:\n    x (np.ndarray): The input 1D array.\n    w1 (np.ndarray): The weights for the first weight layer.\n    w2 (np.ndarray): The weights for the second weight layer.\n    \n    Returns:\n    list: The final output of the residual block, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    z = np.matmul(x, w1)\n    z = np.maximum(z, 0)\n    z = np.matmul(z, w2)\n    z = z + x\n    z = np.maximum(z, 0)\n    z = z.tolist()\n    z = [round(i, 4) for i in z]\n    return z\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array representing feature maps from a convolutional layer.\n\n    Args:\n        x (np.ndarray): A 3D NumPy array of shape (height, width, channels) representing feature maps.\n\n    Returns:\n        np.ndarray: A 1D array of shape (channels,) where each element is the average of all values in the corresponding feature map.\n    \"\"\"\n    sum_along_height_width = np.sum(x, axis=(0, 1))\n    count_along_height_width = np.prod(x.shape[:2])\n    avg_along_height_width = sum_along_height_width / count_along_height_width\n    return avg_along_height_width\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": false, "result": "failed: type list doesn't define __round__ method", "solution": "import numpy as np\nimport numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format.\n\n    Args:\n    X (np.ndarray): Input 4D NumPy array in BCHW format (batch, channels, height, width).\n    gamma (np.ndarray): Scale (gamma) parameters.\n    beta (np.ndarray): Shift (beta) parameters.\n    epsilon (float, optional): Epsilon value for numerical stability. Defaults to 1e-5.\n\n    Returns:\n    list: Normalized output with scale and shift applied, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized = gamma * X_normalized + beta\n    output = X_normalized.tolist()\n    output = [[round(x, 4) for x in channel] for channel in output]\n    return output\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    This function computes the derivative of a polynomial term of the form c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which the derivative is to be computed.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative of the polynomial term at the given point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_coefficient = n * c\n    derivative_value = x ** (n - 1)\n    derivative = derivative_coefficient * derivative_value\n    derivative = round(derivative, 4)\n    return derivative\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n\n    Args:\n    vectors (list[list[float]]): A list of 2D vectors.\n    tol (float, optional): Tolerance value to determine linear independence. Defaults to 1e-10.\n\n    Returns:\n    list[list[float]]: A list of orthonormal vectors (unit length and orthogonal to each other) that span the same subspace.\n    \"\"\"\n    vectors = np.array(vectors)\n    basis = [vectors[0] / np.linalg.norm(vectors[0])]\n    for i in range(1, len(vectors)):\n        projection = np.sum([np.dot(vectors[i], basis[j]) * basis[j] for j in range(i)], axis=0)\n        component = vectors[i] - projection\n        if np.linalg.norm(component) > tol:\n            basis.append(component / np.linalg.norm(component))\n    basis = [np.round(vector, 4).tolist() for vector in basis]\n    return basis\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n\n    Args:\n        a (list or numpy array): The first 3D vector.\n        b (list or numpy array): The second 3D vector.\n\n    Returns:\n        list: The cross product of the two input vectors, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError('Both vectors must have 3 dimensions')\n    cross_product_vector = np.cross(a, b)\n    result = cross_product_vector.tolist()\n    result = [round(component, 4) for component in result]\n    return result\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'shape'", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations Ax = b using Cramer's Rule.\n\n    Parameters:\n    A (numpy array): Square coefficient matrix\n    b (numpy array): Constant vector\n\n    Returns:\n    x (list): Solution vector, rounded to the nearest 4th decimal\n    \"\"\"\n    assert A.shape[0] == A.shape[1], 'A must be a square matrix'\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = np.zeros(A.shape[1])\n    for i in range(A.shape[1]):\n        A_minor = A.copy()\n        A_minor[:, i] = b\n        det_minor = np.linalg.det(A_minor)\n        x[i] = det_minor / det_A\n    x = np.round(x, 4)\n    return x.tolist()\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two discrete probability distributions.\n\n    Args:\n        p (list[float]): The first probability distribution.\n        q (list[float]): The second probability distribution.\n\n    Returns:\n        float: The Bhattacharyya distance between the two distributions, rounded to 4 decimal places.\n    \"\"\"\n    if len(p) != len(q):\n        return 0.0\n    if not p or not q:\n        return 0.0\n    bhattacharyya_dist = -np.sum(np.log(np.sqrt(np.array(p) * np.array(q))))\n    bhattacharyya_dist = round(bhattacharyya_dist, 4)\n    return bhattacharyya_dist\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Compute the element-wise sum of two vectors.\n\n    Args:\n        a (list[int|float]): The first vector.\n        b (list[int|float]): The second vector.\n\n    Returns:\n        list[int|float]: A new vector representing the resulting sum if the operation is valid,\n                         or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> list[float]:\n    \"\"\"\n    Compute the average gradient of the log-policy multiplied by the return at each time step.\n\n    Args:\n    theta (np.ndarray): A 2D NumPy array of shape (num_states, num_actions) parameterizing the policy.\n    episodes (list[list[tuple[int, int, float]]]): A list of episodes, where each episode is a list of (state, action, reward) tuples.\n\n    Returns:\n    list[float]: A list of gradients, rounded to the nearest 4th decimal.\n    \"\"\"\n    total_gradient = np.zeros(theta.shape)\n    for episode in episodes:\n        cumulative_reward = 0\n        for (state, action, reward) in episode:\n            log_policy = np.log(np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :])))\n            cumulative_reward += reward\n            total_gradient += cumulative_reward * log_policy\n    average_gradient = total_gradient / len(episodes)\n    average_gradient = np.round(average_gradient, 4)\n    return average_gradient.tolist()\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Parameters:\n    n_experts (int): Number of experts in the MoE layer.\n    k_active (int): Number of active experts.\n    d_in (int): Input dimension.\n    d_out (int): Output dimension.\n\n    Returns:\n    tuple: (flops_moe, flops_dense, savings_percentage)\n    \"\"\"\n    flops_moe = n_experts * d_in * d_out + k_active * d_in * d_out + k_active * d_out * d_out\n    flops_dense = d_in * d_out\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100\n    flops_moe = round(flops_moe, 1)\n    flops_dense = round(flops_dense, 1)\n    savings_percentage = round(savings_percentage, 1)\n    return (flops_moe, flops_dense, savings_percentage)\nn_experts = 10\nk_active = 5\nd_in = 100\nd_out = 50\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: module 'numpy' has no attribute 'topk'", "solution": "import numpy as np\nimport numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n    Compute the final gating probabilities matrix using the Noisy Top-K gating mechanism.\n\n    Args:\n    X (np.ndarray): Input matrix.\n    W_g (np.ndarray): Weight matrix for the gating network.\n    W_noise (np.ndarray): Pre-sampled noise matrix.\n    N (np.ndarray): Noise distribution.\n    k (int): Sparsity constraint.\n\n    Returns:\n    list: Final gating probabilities matrix.\n    \"\"\"\n    noisy_scores = np.dot(X, W_g) + W_noise\n    topk_scores = np.topk(noisy_scores, k, axis=1, return_indices=False)\n    gating_probabilities = np.exp(topk_scores) / np.sum(np.exp(topk_scores), axis=1, keepdims=True)\n    gating_probabilities = np.round(gating_probabilities, 4)\n    return gating_probabilities.tolist()\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": false, "result": "failed: shape mismatch: indexing arrays could not be broadcast together with shapes (4,1,1) (1,2,1) (2,3,2) ", "solution": "import numpy as np\nimport numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n\n    Parameters:\n    x (np.ndarray): Input tensor of shape (batch_size, sequence_length, embedding_dim)\n    We (np.ndarray): Weight matrices of shape (n_experts, embedding_dim, embedding_dim)\n    Wg (np.ndarray): Gating weight matrix of shape (n_experts, embedding_dim)\n    n_experts (int): Number of experts\n    top_k (int): Number of top experts to select per token\n\n    Returns:\n    np.ndarray: Final MoE output of shape (batch_size, sequence_length, embedding_dim)\n    \"\"\"\n    gating_probabilities = np.dot(x, Wg)\n    gating_probabilities = np.exp(gating_probabilities)\n    gating_probabilities = gating_probabilities / np.sum(gating_probabilities, axis=-1, keepdims=True)\n    top_k_experts = np.argsort(gating_probabilities, axis=-1)[:, :, -top_k:]\n    expert_outputs = np.take_along_axis(We, top_k_experts, axis=-1)\n    expert_outputs = np.sum(expert_outputs, axis=-2)\n    moe_output = np.sum(expert_outputs * gating_probabilities, axis=-1)\n    return moe_output.tolist()\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Group Normalization on a 4D input tensor.\n\n    Args:\n    X (np.ndarray): Input tensor with shape (B, C, H, W)\n    gamma (np.ndarray): Learned scale parameters with shape (C,)\n    beta (np.ndarray): Learned shift parameters with shape (C,)\n    num_groups (int): Number of groups to normalize over\n    epsilon (float, optional): Small value to prevent division by zero. Defaults to 1e-5.\n\n    Returns:\n    np.ndarray: Normalized tensor with shape (B, C, H, W)\n    \"\"\"\n    X_reshaped = X.reshape(X.shape[0], num_groups, -1, X.shape[2], X.shape[3])\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(X.shape)\n    X_normalized = gamma * X_normalized + beta\n    X_normalized = np.round(X_normalized, 4)\n    return X_normalized.tolist()\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    This function finds the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum, \n    starting from any random initial position.\n\n    Args:\n    start_x (float): The initial position to start the search from.\n    learning_rate (float, optional): The step size for each iteration. Defaults to 0.1.\n    tolerance (float, optional): The minimum difference between the current and previous values to stop the iteration. Defaults to 1e-6.\n    max_iters (int, optional): The maximum number of iterations. Defaults to 10000.\n\n    Returns:\n    float: The value of x where f(x) reaches its minimum.\n    \"\"\"\n\n    def f(x: float) -> float:\n        return x ** 4 - 3 * x ** 3 + 2\n    current_x = start_x\n    previous_x = current_x\n    for _ in range(max_iters):\n        derivative = 4 * current_x ** 3 - 9 * current_x ** 2\n        current_x = current_x - learning_rate * derivative\n        if abs(current_x - previous_x) < tolerance:\n            break\n        previous_x = current_x\n    return round(current_x, 4)\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function, a normalization-free transformation inspired by the Tanh function.\n    \n    Args:\n    x (np.ndarray): Input array.\n    alpha (float): Hyperparameter for DyT.\n    gamma (float): Hyperparameter for DyT.\n    beta (float): Hyperparameter for DyT.\n    \n    Returns:\n    list[float]: DyT transformed array.\n    \"\"\"\n    dyt = (1 + np.tanh(alpha * x)) / (1 + gamma * np.tanh(alpha * x)) + beta\n    dyt = np.round(dyt, 4)\n    dyt = dyt.tolist()\n    return dyt\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
